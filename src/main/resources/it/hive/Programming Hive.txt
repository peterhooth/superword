Programming Hive, the image of a hornet’s hive, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
Programming Hive introduces Hive, an essential tool in the Hadoop ecosystem that provides an SQL (Structured Query Language) dialect for querying data stored in the Hadoop Distributed Filesystem (HDFS), other filesystems that integrate with Hadoop, such as MapR-FS and Amazon’s S3 and databases like HBase (the Hadoop database) and Cassandra.
Most data warehouse applications are implemented using relational databases that use SQL as the query language.
Hive lowers the barrier for moving these applications to Hadoop.
Without Hive, these users must learn new languages and tools to become productive again.
Similarly, Hive makes it easier for developers to port SQL-based applications to Hadoop, compared to other tool options.
Without Hive, developers would face a daunting challenge when porting their SQL applications to Hadoop.
Still, there are aspects of Hive that are different from other SQL-based environments.
Documentation for Hive users and Hadoop developers has been sparse.
We decided to write this book to fill that gap.
We provide a pragmatic, comprehensive introduction to Hive that is suitable for SQL experts, such as database designers and business analysts.
We also cover the in-depth technical details that Hadoop developers require for tuning and customizing Hive.
You can learn more at the book’s catalog page (http://oreil.ly/Programming_Hive)
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Definitions of most terms can be found in the Glossary.
Constant width Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Technology professionals, software developers, web designers, and business and creative professionals use Safari Books Online as their primary resource for research, problem solving, learning, and certification training.
Safari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals.
For more information about Safari Books Online, please visit us online.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
For more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.
Find us on Facebook: http://facebook.com/oreilly Follow us on Twitter: http://twitter.com/oreillymedia Watch us on YouTube: http://www.youtube.com/oreillymedia.
What Brought Us to Hive? The three of us arrived here from different directions.
Edward Capriolo When I first became involved with Hadoop, I saw the distributed filesystem and MapReduce as a great way to tackle computer-intensive problems.
However, programming in the MapReduce model was a paradigm shift for me.
Hive offered a fast and simple way to take advantage of MapReduce in an SQL-like world I was comfortable in.
This approach also made it easy to prototype proof-of-concept applications and also to.
Even though I am now very familiar with Hadoop internals, Hive is still my primary method of working with Hadoop.
Being a Hive Committer and a member of the Apache Software Foundation is my most valued accolade.
Dean Wampler As a “big data” consultant at Think Big Analytics, I work with experienced “data people” who eat and breathe SQL.
For them, Hive is a necessary and sufficient condition for Hadoop to be a viable tool to leverage their investment in SQL and open up new opportunities for data analytics.
I suggested to my previous editor at O’Reilly, Mike Loukides, that a Hive book was needed by the community.
My career has involved an array of technologies including search, Hadoop, mobile, cryptography, and natural language processing.
Hive is the ultimate way to build a data warehouse using open technologies on any amount of data.
He is a contributor to the Apache Hive project and is active helping others on the Hive IRC channel.
David Ha and Rumit Patel, at M6D, contributed the case study and code on the Rank function.
The ability to do Rank in Hive is a significant feature.
Ori Stitelman, at M6D, contributed the case study, Data Science using Hive and R, which demonstrates how Hive can be used to make first pass on large data sets and produce results to be used by a second R process.
David Funk contributed three use cases on in-site referrer identification, sessionization, and counting unique visitors.
David’s techniques show how rewriting and optimizing Hive queries can make large scale map reduce data analysis more efficient.
Ian Robertson read the entire first draft of the book and provided very helpful feedback on it.
We’re grateful to him for providing that feedback on short notice and a tight schedule.
John was also instrumental in driving through some of the newer features in Hive like StorageHandlers and Indexing Support.
He has been actively growing and supporting the Hive community.
Nanda Vijaydev contributed the chapter on how Karmasphere offers productized enhancements for Hive.
From the early days of the Internet’s mainstream breakout, the major search engines and ecommerce companies wrestled with ever-growing quantities of data.
Today, many organizations realize that the data they gather is a valuable resource for understanding their customers, the performance of their business in the marketplace, and the effectiveness of their infrastructure.
The Hadoop ecosystem  emerged as a cost-effective way of working with such large data sets.
It imposes a particular programming model, called MapReduce, for breaking up computation tasks into units that can be distributed around a cluster of commodity, server class hardware, thereby providing cost-effective, horizontal scalability.
Underneath this computation model is a distributed file system called the Hadoop Distributed Filesystem (HDFS)
Although the filesystem is “pluggable,” there are now several commercial and open source alternatives.
However, a challenge remains; how do you move an existing data infrastructure to Hadoop, when that infrastructure is based on traditional relational databases and the Structured Query Language (SQL)? What about the large base of SQL users, both expert database designers and administrators, as well as casual users who use SQL to extract information from their data warehouses? This is where Hive comes in.
Hive provides an SQL dialect, called Hive Query Language (abbreviated HiveQL or just HQL) for querying data stored in a Hadoop cluster.
Mapping these familiar data operations to the low-level MapReduce Java API can be daunting, even for experienced Java developers.
Hive does this dirty work for you, so you can focus on the query itself.
Hive translates most queries to MapReduce jobs, thereby exploiting the scalability of Hadoop, while presenting a familiar SQL abstraction.
If you don’t believe us, see “Java Versus Hive: The Word Count Algorithm” on page 10 later in this chapter.
Hive is most suited for data warehouse applications, where relatively static data is analyzed, fast response times are not required, and when the data is not changing rapidly.
The design constraints and limitations of Hadoop and HDFS impose limits on what Hive can do.
The biggest limitation is that Hive does not provide record-level update, insert, nor delete.
You can generate new tables from queries or output query results to files.
Also, because Hadoop is a batch-oriented system, Hive queries have higher latency, due to the start-up overhead for MapReduce jobs.
Queries that would finish in seconds for a traditional database take longer for Hive, even for relatively small data sets.1 Finally, Hive does not provide transactions.
It’s closer to being an OLAP tool, Online Analytic Processing, but as we’ll see, Hive isn’t ideal for satisfying the “online” part of OLAP, at least today, since there can be significant latency between issuing a query and receiving a reply, both due to the overhead of Hadoop and due to the size of the data sets Hadoop was designed to serve.
If you need OLTP features for large-scale data, you should consider using a NoSQL database.
So, Hive is best suited for data warehouse applications, where a large data set is maintained and mined for insights, reports, etc.
Because most data warehouse applications are implemented using SQL-based relational databases, Hive lowers the barrier for moving these applications to Hadoop.
Without Hive, these users would need to learn new languages and tools to be productive again.
Similarly, Hive makes it easier for developers to port SQL-based applications to Hadoop, compared with other Hadoop languages and tools.
However, like most SQL dialects, HiveQL does not conform to the ANSI SQL standard and it differs in various ways from the familiar SQL dialects provided by Oracle, MySQL, and SQL Server.
However, for the big data sets Hive is designed for, this start-up overhead is trivial compared to the actual processing time.
First, it provides a comprehensive, example-driven introduction to HiveQL for all users, from developers, database administrators and architects, to less technical users, such as business analysts.
Second, the book provides the in-depth technical details required by developers and Hadoop administrators to tune Hive query performance and to customize Hive with user-defined functions, custom data formats, etc.
We wrote this book out of frustration that Hive lacked good documentation, especially for new users who aren’t developers and aren’t accustomed to browsing project artifacts like bug and feature databases, source code, etc., to get the information they need.
The Hive Wiki5 is an invaluable source of information, but its explanations are sometimes sparse and not always up to date.
We hope this book remedies those issues, providing a single, comprehensive guide to all the essential features of Hive and how to use them effectively.6
An Overview of Hadoop and MapReduce If you’re already familiar with Hadoop and the MapReduce computing model, you can skip this section.
While you don’t need an intimate knowledge of MapReduce to use Hive, understanding the basic principles of MapReduce will help you understand what Hive is doing behind the scenes and how you can use Hive more effectively.
We provide a brief overview of Hadoop and MapReduce here.
MapReduce MapReduce is a computing model that decomposes large data manipulation jobs into individual tasks that can be executed in parallel across a cluster of servers.
The results of the tasks can be joined together to compute the final results.
Both papers inspired the creation of Hadoop by Doug Cutting.
A map operation converts the elements of a collection from one form to another.
In this case, input key-value pairs are converted to zero-to-many.
In MapReduce, all the key-pairs for a given key are sent to the same reduce operation.
Specifically, the key and a collection of the values are passed to the reducer.
The goal of “reduction” is to convert the collection to a value, such as summing or averaging a collection of numbers, or to another collection.
Again, the input versus output keys and values may be different.
Note that if the job requires no reduction step, then it can be skipped.
An implementation infrastructure like the one provided by Hadoop handles most of the chores required to make jobs run successfully.
For example, Hadoop determines how to decompose the submitted job into individual map and reduce tasks to run, it schedules those tasks given the available resources, it decides where to send a particular task in the cluster (usually where the corresponding data is located, when possible, to minimize network overhead), it monitors each task to ensure successful completion, and it restarts tasks that fail.
The Hadoop Distributed Filesystem, HDFS, or a similar distributed filesystem, manages data across the cluster.
Each block is replicated several times (three copies is the usual default), so that no single hard drive or server failure results in data loss.
Also, because the goal is to optimize the processing of very large data sets, HDFS and similar filesystems use very large block sizes, typically 64 MB or multiples thereof.
Such large blocks can be stored contiguously on hard drives so they can be written and read with minimal seeking of the drive heads, thereby maximizing write and read performance.
To make MapReduce more clear, let’s walk through a simple example, the Word Count algorithm that has become the “Hello World” of MapReduce.7 Word Count returns a list of all the words that appear in a corpus (one or more documents) and the count of how many times each word appears.
The output shows each word found and its count, one per line.
By common convention, the word (output key) and count (output value) are usually separated by a tab separator.
There is a lot going on here, so let’s walk through it from left to right.
Each Input box on the left-hand side of Figure 1-1 is a separate document.
Here are four documents, the third of which is empty and the others contain just a few words, to keep things simple.
By default, a separate Mapper process is invoked to process each document.
In real scenarios, large documents might be split and each split would be sent to a separate Mapper.
Also, there are techniques for combining many small documents into a single split for a Mapper.
If you’re not a developer, a “Hello World” program is the traditional first program you write when learning a new language or tool set.
The fundamental data structure for input and output in MapReduce is the key-value pair.
After each Mapper is started, it is called repeatedly for each line of text from the document.
For each call, the key passed to the mapper is the character offset into the document at the start of the line.
The value, the line of text, is tokenized into words, using one of several possible techniques (e.g., splitting on whitespace is the simplest, but it can leave in undesirable punctuation)
We’ll also assume that the Mapper converts each word to lowercase, so for example, “FUN” and “fun” will be counted as the same word.
Finally, for each word in the line, the mapper outputs a key-value pair, with the word as the key and the number 1 as the value (i.e., the count of “one occurrence”)
Note that the output types of the keys and values are different from the input types.
Part of Hadoop’s magic is the Sort and Shuffle phase that comes next.
Hadoop sorts the key-value pairs by key and it “shuffles” all pairs with the same key to the same Reducer.
There are several possible techniques that can be used to decide which reducer gets which range of keys.
We won’t worry about that here, but for illustrative purposes, we have assumed in the figure that a particular alphanumeric partitioning was used.
For the mapper to simply output a count of 1 every time a word is seen is a bit wasteful of network and disk I/O used in the sort and shuffle.
It does minimize the memory used in the Mappers, however.
One optimization is to keep track of the count for each word and then output only one count for each word when the Mapper finishes.
The inputs to each Reducer are again key-value pairs, but this time, each key will be one of the words found by the mappers and the value will be a collection of all the counts emitted by all the mappers for that word.
Note that the type of the key and the type of the value collection elements are the same as the types used in the Mapper’s output.
That is, the key type is a character string and the value collection element type is an integer.
To finish the algorithm, all the reducer has to do is add up all the counts in the value collection and write a final key-value pair consisting of each word and the count for that word.
The data it produces is used in spell checkers, language detection and translation systems, and other applications.
Hive in the Hadoop Ecosystem The Word Count algorithm, like most that you might implement with Hadoop, is a little involved.
When you actually implement such algorithms using the Hadoop Java API, there are even more low-level details you have to manage yourself.
It’s a job that’s only suitable for an experienced Java developer, potentially putting Hadoop out of reach of users who aren’t programmers, even when they understand the algorithm they want to use.
In fact, many of those low-level details are actually quite repetitive from one job to the next, from low-level chores like wiring together Mappers and Reducers to certain data manipulation constructs, like filtering for just the data you want and performing SQLlike joins on data sets.
There’s a real opportunity to eliminate reinventing these idioms by letting “higher-level” tools handle them automatically.
It not only provides a familiar programming model for people who know SQL, it also eliminates lots of boilerplate and sometimes-tricky coding you would have to do in Java.
This is why Hive is so important to Hadoop, whether you are a DBA or a Java developer.
Hive lets you complete a lot of work with relatively little effort.
Figure 1-2 shows the major “modules” of Hive and how they work with Hadoop.
In this book, we will mostly focus on the CLI, command-line interface.
Bundled with the Hive distribution is the CLI, a simple web interface called Hive web interface (HWI), and programmatic access through JDBC, ODBC, and a Thrift server (see Chapter 16)
All commands and queries go to the Driver, which compiles the input, optimizes the computation required, and executes the required steps, usually with MapReduce jobs.
Instead, it uses built-in, generic Mapper and Reducer modules that are driven by an XML file representing the “job plan.” In other words, these generic modules function like mini language interpreters and the “language” to drive the computation is encoded in XML.
Hive communicates with the JobTracker to initiate the MapReduce job.
Hive does not have to be running on the same master node with the JobTracker.
In larger clusters, it’s common to have edge nodes where tools like Hive run.
They communicate remotely with the JobTracker on the master node to execute jobs.
Usually, the data files to be processed are in HDFS, which is managed by the NameNode.
The Metastore is a separate relational database (usually a MySQL instance) where Hive persists table schemas and other system metadata.
While this is a book about Hive, it’s worth mentioning other higher-level tools that you should consider for your needs.
Hive is best suited for data warehouse applications, where real-time responsiveness to queries and record-level inserts, updates, and deletes.
Of course, Hive is also very nice for people who know SQL already.
However, some of your work may be easier to accomplish with alternative tools.
Pig The best known alternative to Hive is Pig (see http://pig.apache.org), which was developed at Yahoo! about the same time Facebook was developing Hive.
Pig is also now a top-level Apache project that is closely associated with Hadoop.
Suppose you have one or more sources of input data and you need to perform a complex set of transformations to generate one or more collections of output data.
Using Hive, you might be able to do this with nested queries (as we’ll see), but at some point it will be necessary to resort to temporary tables (which you have to manage yourself) to manage the complexity.
Pig is described as a data flow language, rather than a query language.
In Pig, you write a series of declarative statements that define relations from other relations, where each new relation performs some new data transformation.
Pig looks at these declarations and then builds up a sequence of MapReduce jobs to perform the transformations until the final results are computed the way that you want.
This step-by-step “flow” of data can be more intuitive than a complex set of queries.
For this reason, Pig is often used as part of ETL (Extract, Transform, and Load) processes used to ingest external data into a Hadoop cluster and transform it into a more desirable form.
A drawback of Pig is that it uses a custom language not based on SQL.
This is appropriate, since it is not designed as a query language, but it also means that Pig is less suitable for porting over SQL applications and experienced SQL users will have a larger learning curve with Pig.
Nevertheless, it’s common for Hadoop teams to use a combination of Hive and Pig, selecting the appropriate tool for particular jobs.
HBase What if you need the database features that Hive doesn’t provide, like row-level updates, rapid query response times, and transactions? HBase is a distributed and scalable data store that supports row-level updates, rapid queries, and row-level transactions (but not multirow transactions)
HBase is inspired by Google’s Big Table, although it doesn’t implement all Big Table features.
One of the important features HBase supports is column-oriented storage, where columns can be organized into column families.
Rather than reading entire rows and discarding most of the columns, you read only the columns you need.
HBase can be used like a key-value store, where a single key is used for each row to provide very fast reads and writes of the row’s columns or column families.
HBase also keeps a configurable number of versions of each column’s values (marked by timestamps), so it’s possible to go “back in time” to previous values, when needed.
Finally, what is the relationship between HBase and Hadoop? HBase uses HDFS (or one of the other distributed filesystems) for durable file storage of data.
To provide row-level updates and fast queries, HBase also uses in-memory caching of data and local files for the append log of updates.
Periodically, the durable files are updated with all the append log updates, etc.
HBase doesn’t provide a query language like SQL, but Hive is now integrated with HBase.
Cascading, Crunch, and Others There are several other “high-level” languages that have emerged outside of the Apache Hadoop umbrella, which also provide nice abstractions on top of Hadoop to reduce the amount of low-level boilerplate code required for typical jobs.
All are JVM (Java Virtual Machine) libraries that can be used from programming languages like Java, Clojure, Scala, JRuby, Groovy, and Jython, as opposed to tools with their own languages, like Hive and Pig.
Using one of these programming languages has advantages and disadvantages.
It makes these tools less attractive to nonprogrammers who already know SQL.
However, for developers, these tools provide the full power of a Turing complete programming language.
We’ll learn how to extend Hive with Java code when we need additional functionality that Hive doesn’t provide (Table 1-1)
A Clojure DSL for Cascading that provides additional functionality inspired by Datalog for data processing and query abstractions.
Because Hadoop is a batch-oriented system, there are tools with different distributed computing models that are better suited for event stream processing, where closer to “real-time” responsiveness is required.
Here we list several of the many alternatives (Table 1-2)
Spark http://www.spark-project.org/ A distributed computing framework based on the idea of distributed data sets with a Scala API.
It can work with HDFS files and it offers notable performance improvements over Hadoop MapReduce for many computations.
There is also a project to port Hive to Spark, called Shark (http://shark.cs.berkeley.edu/)
Finally, it’s important to consider when you don’t need a full cluster (e.g., for smaller data sets or when the time to perform a computation is less critical)
Also, many alternative tools are easier to use when prototyping algorithms or doing exploration with a subset of data.
Some of the more popular options are listed in Table 1-3
It’s not a distributed system, so the data sizes it can handle are limited.
A commercial system for data analysis and numerical methods that is popular with engineers and scientists.
A commercial data analysis, symbolic manipulation, and numerical methods system that is also popular with scientists and engineers.
SciPy, NumPy http://scipy.org Extensive software package for scientific programming in Python, which is widely used by data scientists.
If you are a Java programmer, you might be reading this book because you’ll need to support the Hive users in your organization.
You might be skeptical about using Hive for your own work.
If so, consider the following example that implements the Word.
Count algorithm we discussed above, first using the Java MapReduce API and then using Hive.
It’s very common to use Word Count as the first Java MapReduce program that people write, because the algorithm is simple to understand, so you can focus on the API.
Hence, it has become the “Hello World” of the Hadoop world.
The following Java implementation is included in the Apache Hadoop distribution.8 If you don’t know Java (and you’re still reading this section), don’t worry, we’re only showing you the code for the size comparison:
In both examples, the files were tokenized into words using the simplest possible approach; splitting on whitespace boundaries.
This approach doesn’t properly handle punctuation, it doesn’t recognize that singular and plural forms of words are the same word, etc.
However, it’s good enough for our purposes here.10 The virtue of the Java API is the ability to customize and fine-tune every detail of an algorithm implementation.
However, most of the time, you just don’t need that level of control and it slows you down considerably when you have to manage all those details.
If you’re not a programmer, then writing Java MapReduce code is out of reach.
However, if you already know SQL, learning Hive is relatively straightforward and many applications are quick and easy to implement.
What’s Next We described the important role that Hive plays in the Hadoop ecosystem.
The Hive query hardcodes a path to the data, while the Java code takes the path as an argument.
In Chapter 2, we’ll learn how to use Hive variables in scripts to avoid hardcoding such details.
This is a convenient way to learn and experiment with Hadoop.
Then we’ll discuss how to configure Hive for use on Hadoop clusters.
If you already use Amazon Web Services, the fastest path to setting up Hive for learning is to run a Hive-configured job flow on Amazon Elastic MapReduce (EMR)
VirtualBox is free for all these platforms, and also Solaris.
The virtual machines use Linux as the operating system, which is currently the only recommended operating system for running Hadoop in production.3
Using a virtual machine is currently the only way to run Hadoop on Windows systems, even when Cygwin or similar Unix-like software is installed.
Most of the preconfigured virtual machines (VMs) available are only designed for VMWare, but if you prefer VirtualBox you may find instructions on the Web that explain how to import a particular VM into VirtualBox.
You can download preconfigured virtual machines from one of the websites given in Table 2-1.4 Follow the instructions on these web sites for loading the VM into VMWare.
Detailed Installation While using a preconfigured virtual machine may be an easy way to run Hive, installing Hadoop and Hive yourself will give you valuable insights into how these tools work, especially if you are a developer.
The instructions that follow describe the minimum necessary Hadoop and Hive installation steps for your personal Linux or Mac OS X workstation.
For production installations, consult the recommended installation procedures for your Hadoop distributor.
Although the JRE (Java Runtime Environment) is all you need to run Hive, you will need the full JDK (Java Development Kit) to build examples in this book that demonstrate how to extend Hive with Java code.
However, if you are not a programmer, the companion source code distribution for this book (see the Preface) contains prebuilt examples.
These are the current URLs at the time of this writing.
After the installation is complete, you’ll need to ensure that Java is in your path and the JAVA_HOME environment variable is set.
On Linux systems, the following instructions set up a bash file in the /etc/profile.d/ directory that defines JAVA_HOME for all users.
Changing environmental settings in this folder requires root access and affects all users of the system.
If you’ve never used sudo (“super user do something”) before to run a command as a “privileged” user, as in two of the commands, just type your normal password when you’re asked for it.
If you’re on a personal machine, your user account probably has “sudo rights.” If not, ask your administrator to run those commands.
Mac OS X systems don’t have the /etc/profile.d directory and they are typically single-user systems, so it’s best to put the environment variable definitions in your $HOME/.bashrc.
The Java paths are different, too, and they may be in one of several places.5 Here are a few examples.
You’ll need to determine where Java is installed on your Mac and adjust the definitions accordingly.
This discrepancy may actually reflect the fact that stewardship of the Mac OS X Java port is transitioning from Apple to Oracle as of Java 1.7
Hadoop is an active open source project with many releases and branches.
Also, many commercial software companies are now producing their own distributions of Hadoop, sometimes with custom enhancements or replacements for some components.
This situation promotes innovation, but also potential confusion and compatibility issues.
Keeping software up to date lets you exploit the latest performance enhancements and bug fixes.
So, for this book, we’ll show you how to install the Apache Hadoop release v0.20.2
This edition is not the most recent stable release, but it has been the reliable gold standard for some time for performance and compatibility.
Note that the bundled Cloudera, MapR, and planned Hortonworks distributions all include a Hive release.
While this release will bring significant enhancements to the Hadoop ecosystem, it is too new for our purposes.
To install Hadoop on a Linux system, run the following commands.
Note that we wrapped the long line for the wget command:
To install Hadoop on a Mac OS X system, run the following commands.
Note that we wrapped the long line for the curl command:
In what follows, we will assume that you added $HADOOP_HOME/bin to your path, as in the previous commands.
This will allow you to simply type the hadoop command without the path prefix.
We mentioned above that the default mode is local mode, where filesystem references use the local filesystem.
Also in local mode, when Hadoop jobs are executed (including most Hive queries), the Map and Reduce tasks are run as part of the same process.
Actual clusters are configured in distributed mode, where all filesystem references that aren’t full URIs default to the distributed filesystem (usually HDFS) and jobs are managed by the JobTracker service, with individual tasks executed in separate processes.
A dilemma for developers working on personal machines is the fact that local mode doesn’t closely resemble the behavior of a real cluster, which is important to remember when testing applications.
To address this need, a single machine can be configured to run in pseudodistributed mode, where the behavior is identical to distributed mode, namely filesystem references default to the distributed filesystem and jobs are managed by the JobTracker service, but there is just a single machine.
Hence, for example, HDFS file block replication is limited to one copy.
Because Hive uses Hadoop jobs for most of its work, its behavior reflects the Hadoop mode you’re using.
However, even when running in distributed mode, Hive can decide on a per-query basis whether or not it can perform the query using just local mode, where it reads the data files and manages the MapReduce tasks itself, providing faster turnaround.
Hence, the distinction between the different modes is more of an execution style for Hive than a deployment style, as it is for Hadoop.
For most of the book, it won’t matter which mode you’re using.
We’ll assume you’re working on a personal machine in local mode and we’ll discuss the cases where the mode matters.
When working with small data sets, using local mode execution will make Hive queries much faster.
Testing Hadoop Assuming you’re using local mode, let’s look at the local filesystem two different ways.
The following output of the Linux ls command shows the typical contents of the “root” directory of a Linux system:
Hadoop provides a dfs tool that offers basic filesystem functionality like ls for the default filesystem.
Since we’re using local mode, the default filesystem is the local filesystem:6
If you find yourself using the hadoop dfs command frequently, it’s convenient to define an alias for it (e.g., alias hdfs="hadoop dfs")
Let’s run it! Start by creating an input directory (inside your current working directory) with files to be processed by Hadoop:
Use the hadoop command to launch the Word Count application on the input directory we just created.
Note that it’s conventional to always specify directories for input and output, not individual files, since there will often be multiple input and/or output files per directory, a consequence of the parallelism of the system.
Unfortunately, the dfs -ls command only provides a “long listing” format.
There is no short format, like the default for the Linux ls command.
If you are running these commands on your local installation that was configured to use local mode, the hadoop command will launch the MapReduce components in the same process.
If you are running on a cluster or on a single machine using pseudodistributed mode, the hadoop command will launch one or more separate processes using the JobTracker service (and the output below will be slightly different)
Also, if you are running with a different version of Hadoop, change the name of the examples.jar as needed:
The results of the Word count application can be viewed through local filesystem commands:
They can also be viewed by the equivalent dfs command (again, because we assume you are running in local mode):
For very big files, if you want to view just the first or last parts, there is no -more, -head, nor -tail subcommand.
Instead, just pipe the output of the -cat command through the shell’s more, head, or tail.
Now that we have installed and tested an installation of Hadoop, we can install Hive.
We will download and extract a tarball for Hive, which does not include an embedded version of Hadoop.
A single Hive binary is designed to work with multiple versions of Hadoop.
This means it’s often easier and less risky to upgrade to newer Hive releases than it is to upgrade to newer Hadoop releases.
Hive uses the environment variable HADOOP_HOME to locate the Hadoop JARs and configuration files.
So, make sure you set that variable as discussed above before proceeding.
As you can infer from these commands, we are using the latest stable release of Hive at the time of this writing, v0.9.0
We’ll call out the differences as we come to them.
You’ll want to add the hive command to your path, like we did for the hadoop command.
We’ll assume it’s defined for some examples later in the book.
Each JAR file implements a particular subset of Hive’s functionality, but the details don’t concern us now.
The $HIVE_HOME/bin directory contains executable scripts that launch various Hive services, including the hive command-line interface (CLI)
The CLI is the most popular way to use Hive.
We will use hive (in lowercase, with a fixed-width font) to refer to the CLI, except where noted.
The CLI can be used interactively to type in statements one at a time or it can be used to run “scripts” of Hive statements, as we’ll see.
All Hive installations require a metastore service, which Hive uses to store table schemas and other metadata.
It is typically implemented using tables in a relational database.
By default, Hive uses a built-in Derby SQL server, which provides limited, singleprocess storage.
For example, when using Derby, you can’t run two simultaneous instances of the Hive CLI.
However, this is fine for learning Hive on a personal machine.
For clusters, MySQL or a similar relational database is required.
Finally, a simple web interface, called Hive Web Interface (HWI), provides remote access to Hive.
Hive has a number of configuration properties that we will discuss as needed.
These properties control features such as the metastore (where data is stored), various optimizations, and “safety controls,” etc.
Starting Hive Let’s finally start the Hive command-line interface (CLI) and run a few commands! We’ll briefly comment on what’s happening, but save the details for discussion later.
Substitute the directory where Hive is installed on your system whenever $HIVE_HOME is listed in the following script.
Or, if you added $HIVE_HOME/bin to your PATH, you can just type hive to run the command.
We’ll make that assumption for the rest of the book.
Here is a sample session, where we have added a blank line after the output of each command, for clarity:
The first line printed by the CLI is the local filesystem location where the CLI writes log data about the commands and queries you execute.
Throughout the book, we will follow the SQL convention of showing Hive keywords in uppercase (e.g., CREATE, TABLE, SELECT and FROM), even though case is ignored by Hive, following SQL conventions.
Going forward, we’ll usually add the blank line after the command output for all sessions.
Also, when starting a session, we’ll omit the line about the logfile.
For individual commands and queries, we’ll omit the OK and Time taken:...
At the successive prompts, we create a simple table named x with a single INT (4-byte integer) column named a, then query it twice, the second time showing how queries and commands can spread across multiple lines.
If you are running with the default Derby database for the metastore, you’ll notice that your current working directory now contains a new subdirectory called metastore_db that was created by Derby during the short hive session you just executed.
If you are running one of the VMs, it’s possible it has configured different behavior, as we’ll discuss later.
Creating a metastore_db subdirectory under whatever working directory you happen to be in is not convenient, as Derby “forgets” about previous metastores when you change to a new working directory! In the next section, we’ll see how to configure a permanent location for the metastore database, as well as make other changes.
Configuring Your Hadoop Environment Let’s dive a little deeper into the different Hadoop modes and discuss more configuration issues relevant to Hive.
You can skip this section if you’re using Hadoop on an existing cluster or you are using a virtual machine instance.
If you are a developer or you installed Hadoop and Hive yourself, you’ll want to understand the rest of this section.
Local Mode Configuration Recall that in local mode, all references to files go to your local filesystem, not the distributed filesystem.
Instead, your jobs run all tasks in a single JVM instance.
Figure 2-1 illustrates a Hadoop job running in local mode.
If you plan to use the local mode regularly, it’s worth configuring a standard location for the Derby metastore_db, where Hive stores metadata about your tables, etc.
Changes to your configuration are done by editing the hive-site.xml file.
Here is an example configuration file where we set several properties for local mode execution (Example 2-1)
You can use any directory path you want for the value.
Note that this directory will not be used to store the table metadata, which goes in the separate metastore.
This property controls whether to connect to a remote metastore server or open a new metastore server as part of the Hive Client JVM.
This setting is almost always set to true and JDBC is used to communicate directly to a relational database.
This property tells Hive how to connect to the metastore server.
By default, it uses the current working directory for the databaseName part of the value string.
This change eliminates the problem of Hive dropping the metastore_db directory in the current working directory every time we start a new Hive session.
Now, we’ll always have access to all our metadata, no matter what directory we are working in.
Distributed and Pseudodistributed Mode Configuration In distributed mode, several services run in the cluster.
The JobTracker manages jobs and the NameNode is the HDFS master.
Worker nodes run individual job tasks, managed by a TaskTracker service on each node, and then hold blocks for files in the distributed filesystem, managed by DataNode services.
Figure 2-2 shows a typical distributed mode configuration for a Hadoop cluster.
We’re using the convention that *.domain.pvt is our DNS naming convention for the cluster’s private, internal network.
Pseudodistributed mode is nearly identical; it’s effectively a one-node cluster.
Here, we’ll focus on the unique configuration steps required by Hive.
Specifying a different value here allows each user to define their own warehouse directory, so they don’t affect other system users.
Hence, each user might use the following statement to define their own warehouse directory:
It’s tedious to type this each time you start the Hive CLI or to remember to add it to every Hive script.
Of course, it’s also easy to forget to define this property.
See “The .hiverc File” on page 36 for more details.
Metastore Using JDBC Hive requires only one extra component that Hadoop does not already have; the metastore component.
The metastore stores metadata such as table schema and partition information that you specify when you run commands such as create table x..., or alter table y..., etc.
Because multiple users and systems are likely to need concurrent access to the metastore, the default embedded database is not suitable for production.
If you are using a single node in pseudodistributed mode, you may not find it useful to set up a full relational database for the metastore.
It is straightforward to adapt this information to other JDBC-compliant databases.
The information required for table schema, partition information, etc., is small, typically much smaller than the large quantity of data stored in Hive.
As a result, you typically don’t need a powerful dedicated database server for the metastore.
However because it represents a Single Point of Failure (SPOF), it is strongly recommended that you replicate and back up this database using the standard techniques you would normally use with other relational database instances.
For our MySQL configuration, we need to know the host and port the service is running on.
Finally, we will assume that hive_db is the name of our catalog.
You may have noticed the ConnectionURL property starts with a prefix of jdbc:mysql.
For Hive to be able to connect to MySQL, we need to place the JDBC driver in our classpath.
The driver can be placed in the Hive library path, $HIVE_HOME/lib.
Some teams put all such support libraries in their Hadoop lib directory.
With the driver and the configuration settings in place, Hive will store its metastore information in MySQL.
We’ll also assume that you have added $HIVE_HOME/bin to your environment’s PATH so you can type hive at the shell prompt and your shell environment (e.g., bash) will find the command.
Command Options If you run the following command, you’ll see a brief list of the options for the hive command.
For help on a particular service: ./hive --service serviceName --help Debug help:  ./hive --debug --help.
There are several services available, including the CLI that we will spend most of our time using.
You can invoke a service using the --service name option, although there are shorthand invocations for some of the services, as well.
It is the default service if no other service is specified.
The --auxpath option lets you specify a colon-separated list of “auxiliary” Java archive (JAR) files that contain custom extensions, etc., that you might require.
The --config directory is mostly useful if you have to override the default configuration properties in $HIVE_HOME/conf in a new directory.
The Command-Line Interface The command-line interface or CLI is the most common way to interact with Hive.
Using the CLI, you can create tables, inspect schema and query tables, etc.
However, that’s technically an unsupported option, but it produces the help output with an additional line that complains about Missing argument for option: h.
For Hive v0.7.X, the -d, --hivevar, and -p options are not supported.
Variables and Properties The --define key=value option is effectively equivalent to the --hivevar key=value option.
Both let you define on the command line custom variables that you can reference in Hive scripts to customize execution.
This feature is only supported in Hive v0.8.0 and later versions.
When you use this feature, Hive puts the key-value pair in the hivevar “namespace” to distinguish these definitions from three other built-in namespaces, hiveconf, system, and env.
The terms variable or property are used in different contexts, but they function the same way in most cases.
You can reference variables in queries; Hive replaces the reference with the variable’s value before sending the query to the query processor.
Inside the CLI, variables are displayed and changed using the SET command.
For example, the following session shows the value for one variable, in the env namespace, and then all variable definitions! Here is a Hive session where some output has been omitted and we have added a blank line after the output of each command for clarity:
Without the -v flag, set prints all the variables in the namespaces hivevar, hiveconf, system, and env.
With the -v option, it also prints all the properties defined by Hadoop, such as properties controlling HDFS and MapReduce.
The set command is also used to set new values for variables.
Let’s look specifically at the hivevar namespace and a variable that is defined for it on the command line:
The --hivevar flag is the same as the --define flag.
Variable references in queries are replaced in the CLI before the query is sent to the query processor.
Let’s look at the --hiveconf option, which is supported in Hive v0.7.X.
It is used for all properties that configure Hive behavior.
It turns on printing of the current working database name in the CLI prompt.
See “Databases in Hive” on page 49 for more on Hive databases.
We can even add new hiveconf entries, which is the only supported option for Hive versions earlier than v0.8.0:
It’s also useful to know about the system namespace, which provides read-write access to Java system properties, and the env namespace, which provides read-only access to environment variables:
Unlike hivevar variables, you have to use the system: or env: prefix with system properties and environment variables.
The env namespace is useful as an alternative way to pass variable definitions to Hive, especially for Hive v0.7.X.
The query processor will see the literal number 2012 in the WHERE clause.
If you are using Hive v0.7.X, some of the examples in this book that use parameters and variables may not work as written.
If so, replace the variable reference with the corresponding value.
Hive “One Shot” Commands The user may wish to run one or more queries (semicolon separated) and then have the hive CLI exit immediately after completion.
The CLI accepts a -e command argument that enables this feature.
If mytable has a string and integer column, we might see the following output:
A quick and dirty technique is to use this feature to output the query results to a file.
Adding the -S for silent mode removes the OK and Time taken ...
Note that hive wrote the output to the standard output and the shell command redirected that output to the local filesystem, not to HDFS.
Finally, here is a useful trick for finding a property name that you can’t quite remember, without having to scroll through the list of the set output.
Suppose you can’t remember the name of the property that specifies the “warehouse” location for managed tables:
Executing Hive Queries from Files Hive can execute one or more queries that were saved to a file using the -f file argument.
By convention, saved Hive query files use the .q or .hql extension.
If you are already inside the Hive shell you can use the SOURCE command to execute a script file.
By the way, we’ll occasionally use the name src (“source”) for tables in queries when the name of the table is irrelevant for the example.
This convention is taken from the unit tests in Hive’s source code; first create a src table before all tests.
For example, when experimenting with a built-in function, it’s convenient to write a “query” that passes literal arguments to the function, as in the following example taken from later in the book, “XPath-Related Functions” on page 207:
The details for xpath don’t concern us here, but note that we pass string literals to the xpath function and use FROM src LIMIT 1 to specify the required FROM clause and to limit the output.
Substitute src with the name of a table you have already created or create a dummy table named src:
The .hiverc File The last CLI option we’ll discuss is the -i file option, which lets you specify a file of commands for the CLI to run as it starts, before showing you the prompt.
Hive automatically looks for a file named .hiverc in your HOME directory and runs the commands it contains, if any.
The following shows an example of a typical $HOME/.hiverc file:
The first line adds a JAR file to the Hadoop distributed cache.
The last line “encourages” Hive to be more aggressive about using local-mode execution when possible, even when Hadoop is running in distributed or pseudo-distributed mode, which speeds up queries for small data sets.
An easy mistake to make is to forget the semicolon at the end of lines like this.
When you make this mistake, the definition of the property will include all the text from all the subsequent lines in the file until the next semicolon.
More on Using the Hive CLI The CLI supports a number of other useful features.
If you start typing and hit the Tab key, the CLI will autocomplete possible keywords and function names.
For example, if you type SELE and then the Tab key, the CLI will complete the word SELECT.
If you type the Tab key at the prompt, you’ll get this reply:
If you enter y, you’ll get a long list of all the keywords and built-in functions.
A common source of error and confusion when pasting statements into the CLI occurs where some lines begin with a tab.
You’ll get the prompt about displaying all possibilities, and subsequent characters in the stream will get misinterpreted as answers to the prompt, causing the command to fail.
Command History You can use the up and down arrow keys to scroll through previous commands.
Actually, each previous line of input is shown separately; the CLI does not combine multiline commands and queries into a single history entry.
If you want to repeat a previous command, scroll to it and hit Enter.
If you want to edit the line before entering it, use the left and right arrow keys to navigate to the point where changes are required and edit the line.
You can hit Return to submit it without returning to the end of the line.
Most navigation keystrokes using the Control key work as they do for the bash shell (e.g., Control-A goes to the beginning of the line and Control-E goes to the end of the line)
However, similar “meta,” Option, or Escape keys don’t work (e.g., Option-F to move forward a word at a time)
Similarly, the Delete key will delete the character to the left of the cursor, but the Forward Delete key doesn’t delete the character under the cursor.
Shell Execution You don’t need to leave the hive CLI to run simple bash shell commands.
Simply type ! followed by the command and terminate the line with a semicolon (;):
Hadoop dfs Commands from Inside Hive You can run the hadoop dfs ...
This method of accessing hadoop commands is actually more efficient than using the hadoop dfs ...
You can see a full listing of help on the options supported by dfs using this command:
See also http://hadoop.apache.org/common/docs/r0.20.205.0/file_system_shell.html or similar documentation for your Hadoop distribution.
Comments in Hive Scripts As of Hive v0.8.0, you can embed lines of comments that start with the string --, for example:
If you paste them into the CLI, you’ll get errors.
They only work when used in scripts executed with hive -f script_name.
Query Column Headers As a final example that pulls together a few things we’ve learned, let’s tell the CLI to print column headers, which is disabled by default.
If you always prefer seeing the headers, put the first line in your $HOME/.hiverc file.
Hive supports many of the primitive data types you find in relational databases, as well as three collection data types that are rarely found in relational databases, for reasons we’ll discuss shortly.
A related concern is how these types are represented in text files, as well as alternatives to text storage that address various performance and other concerns.
A unique feature of Hive, compared to most databases, is that it provides great flexibility in how data is encoded in files.
Most databases take total control of the data, both how it is persisted to disk and its life cycle.
By letting you control all these aspects, Hive makes it easier to manage and process data with a variety of tools.
Primitive Data Types Hive supports several sizes of integer and floating-point types, a Boolean type, and character strings of arbitrary length.
As for other SQL dialects, the case of these names is ignored.
It’s useful to remember that each of these types is implemented in Java, so the particular behavior details will be exactly what you would expect from the corresponding Java types.
For example, STRING is implemented by the Java String, FLOAT is implemented by Java float, etc.
Note that Hive does not support “character arrays” (strings) with maximum-allowed lengths, as is common in other SQL dialects.
Relational databases offer this feature as a performance optimization; fixed-length records are easier to index, scan, etc.
In the “looser” world in which Hive lives, where it may not own the data files and has to be flexible on file format, Hive relies on the presence of delimiters to separate fields.
Also, Hadoop and Hive emphasize optimizing disk reading and writing performance, where fixing the lengths of column values is relatively unimportant.
The BINARY type is similar to the VARBINARY type found in many relational databases.
It’s not like a BLOB type, since BINARY columns are stored within the record, not separately like BLOBs.
Note that you don’t need BINARY if your goal is to ignore the tail end of each record.
If a table schema specifies three columns and the data files contain five values for each record, the last two will be ignored by Hive.
What if you run a query that wants to compare a float column to a double column or compare a value of one integer type with a value of a different integer type? Hive will implicitly cast any integer to the larger of the two integer types, cast FLOAT to DOUBLE, and cast any integer value to DOUBLE, as needed, so it is comparing identical types.
What if you run a query that wants to interpret a string column as a number? You can explicitly cast one type to another as in the following example, where s is a string column that holds a value representing an integer:
To be clear, the AS INT are keywords, so lowercase would be fine.
Collection Data Types Hive supports columns that are structs, maps, and arrays.
Note that the literal syntax examples in Table 3-2 are actually calls to built-in functions.
For example, if a column name is of type ARRAY of strings with the value ['John', 'Doe'], then the second element can be referenced using name[1]
As for simple types, the case of the type name is ignored.
Most relational databases don’t support such collection types, because using them tends to break normal form.
For example, in traditional data models, structs might be captured in separate tables, with foreign key relations between the tables, as appropriate.
A practical problem with breaking normal form is the greater risk of data duplication, leading to unnecessary disk space consumption and potential data inconsistencies, as duplicate copies can grow out of sync as changes are made.
However, in Big Data systems, a benefit of sacrificing normal form is higher processing throughput.
Scanning data off hard disks with minimal “head seeks” is essential when processing terabytes to petabytes of data.
Embedding collections in records makes retrieval faster with minimal seeks.
Navigating each foreign key relationship requires seeking across the disk, with significant performance overhead.
Here is a table declaration that demonstrates how to use these types, an employees table in a fictitious Human Resources application:
The name is a simple string and for most employees, a float is large enough for the salary.
The list of subordinates is an array of string values, where we treat the name as a “primary key,” so each element in subordinates would reference another record in the table.
In a traditional model, the relationship would go the other way, from an employee to his or her manager.
We’re not arguing that our model is better for Hive; it’s just a contrived example to illustrate the use of arrays.
The deductions is a map that holds a key-value pair for every deduction that will be subtracted from the employee’s salary when paychecks are produced.
The key is the name of the deduction (e.g., “Federal Taxes”), and the key would either be a percentage value or an absolute number.
In a traditional data model, there might be separate tables for deduction type (each key in our map), where the rows contain particular deduction values and a foreign key pointing back to the corresponding employee record.
Finally, the home address of each employee is represented as a struct, where each field is named and has a particular type.
Note that Java syntax conventions for generics are followed for the collection types.
For an ARRAY<STRING>, every item in the array will be a STRING.
STRUCTs can mix different types, but the locations are fixed to the declared position in the STRUCT.
Text File Encoding of Data Values Let’s begin our exploration of file formats by looking at the simplest example, text files.
You are no doubt familiar with text files delimited with commas or tabs, the so-called comma-separated values (CSVs) or tab-separated values (TSVs), respectively.
Hive can use those formats if you want and we’ll show you how shortly.
However, there is a drawback to both formats; you have to be careful about commas or tabs embedded in text and not intended as field or column delimiters.
For this reason, Hive uses various control characters by default, which are less likely to appear in value strings.
Hive uses the term field when overriding the default delimiter, as we’ll see shortly.
Written using the octal code \001 when explicitly specified in CREATE TABLE statements.
Written using the octal code \002 when explicitly specified in CREATE TABLE statements.
Written using the octal code \003 when explicitly specified in CREATE TABLE statements.
Records for the employees table declared in the previous section would look like the following example, where we use ^ A, etc., to represent the field delimiters.
A text editor like Emacs will show the delimiters this way.
Note that the lines have been wrapped in the example because they are too long for the printed page.
Let’s walk through the first line to understand the structure.
You’ll note that maps and structs are effectively the same thing in JSON.
Now, here’s how the first line of the text file breaks down:
This might be necessary if another application writes the data using a different convention.
Here is the same table declaration again, this time with all the format defaults explicitly specified:
The ROW FORMAT DELIMITED sequence of keywords must appear before any of the other clauses, with the exception of the STORED AS … clause.
You can override the field, collection, and key-value separators and still use the default text file format, so the clause STORED AS TEXTFILE is rarely used.
For most of this book, we will use the default TEXTFILE file format.
So, while you can specify all these clauses explicitly, using the default separators most of the time, you normally only provide the clauses for explicit overrides.
These specifications only affect what Hive expects to see when it reads files.
Except in a few limited cases, it’s up to you to write the data files in the correct format.
For example, here is a table definition where the data will contain comma-delimited fields.
This example does not properly handle the general case of files in CSV (comma-separated values) and TSV (tab-separated values) formats.
They can include a header row with column names and column string values might be quoted and they might contain embedded commas or tabs, respectively.
See Chapter 15 for details on handling these file types more generally.
This powerful customization feature makes it much easier to use Hive with files created by other tools and various ETL (extract, transform, and load) processes.
Schema on Read When you write data to a traditional database, either through loading external data, writing the output of a query, doing UPDATE statements, etc., the database has total control over the storage.
The database is the “gatekeeper.” An important implication of this control is that the database can enforce the schema as data is written.
There are many ways to create, modify, and even damage the data that Hive will query.
So what if the schema doesn’t match the file contents? Hive does the best that it can to read the data.
You will get lots of null values if there aren’t enough fields in each record to match the schema.
If some fields are numbers and Hive encounters nonnumeric strings, it will return nulls for those fields.
Above all else, Hive tries to recover from all errors as best it can.
Like all SQL dialects in widespread use, it doesn’t fully conform to any particular revision of the ANSI SQL standard.
It is perhaps closest to MySQL’s dialect, but with significant differences.
Hive offers no support for rowlevel inserts, updates, and deletes.
Hive adds extensions to provide better performance in the context of Hadoop and to integrate with custom extensions and even external programs.
This chapter and the ones that follow discuss the features of HiveQL using representative examples.
In some cases, we will briefly mention details for completeness, then explore them more fully in later chapters.
This chapter starts with the so-called data definition language parts of HiveQL, which are used for creating, altering, and dropping databases, tables, views, functions, and indexes.
We’ll also discuss the SHOW and DESCRIBE commands for listing and describing items as we go.
Subsequent chapters explore the data manipulation language parts of HiveQL that are used to put data into Hive tables and to extract data to the filesystem, and how to explore and manipulate data with queries, grouping, filtering, joining, etc.
Databases in Hive The Hive concept of a database is essentially just a catalog or namespace of tables.
However, they are very useful for larger clusters with multiple teams and users, as a way of avoiding table name collisions.
It’s also common to use databases to organize production tables into logical groups.
If you don’t specify a database, the default database is used.
The simplest syntax for creating a database is shown in the following example:
While normally you might like to be warned if a database of the same name already exists, the IF NOT EXISTS clause is useful for scripts that should create a database onthe-fly, if necessary, before proceeding.
You can also use the keyword SCHEMA instead of DATABASE in all the database-related commands.
At any time, you can see the databases that already exist as follows:
If you have a lot of databases, you can restrict the ones listed using a regular expression, a concept we’ll explain in “LIKE and RLIKE” on page 96, if it is new to you.
The following example lists only those databases that start with the letter h and end with any other characters (the .* part):
Tables in that database will be stored in subdirectories of the database directory.
The exception is tables in the default database, which doesn’t have its own directory.
You can override this default location for the new directory as shown in this example:
You can add a descriptive comment to the database, which will be shown by the DESCRIBE DATABASE <database> command.
Note that DESCRIBE DATABASE also shows the directory location for the database.
In the output of DESCRIBE DATABASE, we’re showing master-server to indicate the URI authority, in this case a DNS name and optional port number (i.e., server:port) for the “master node” of the filesystem (i.e., where the NameNode service is running for HDFS)
If you are running in pseudo-distributed mode, then the master server will be localhost.
However, if you are running in local mode, your current working directory is used as the parent of some/relative/path.
For script portability, it’s typical to omit the authority, only specifying it when referring to another distributed filesystem instance (including S3 buckets)
Lastly, you can associate key-value properties with the database, although their only function currently is to provide a way of adding information to the output of DESCRIBE DATABASE EXTENDED <database>:
The USE command sets a database as your working database, analogous to changing working directories in a filesystem:
Now, commands such as SHOW TABLES; will list the tables in this database.
Unfortunately, there is no command to show you which database is your current working database! Fortunately, it’s always safe to repeat the USE … command; there is no concept in Hive of nesting of databases.
The IF EXISTS is optional and suppresses warnings if financials doesn’t exist.
By default, Hive won’t permit you to drop a database if it contains tables.
You can either drop the tables first or append the CASCADE keyword to the command, which will cause the Hive to drop the tables in the database first:
Using the RESTRICT keyword instead of CASCADE is equivalent to the default behavior, where existing tables must be dropped before dropping the database.
When a database is dropped, its directory is also deleted.
Alter Database You can set key-value pairs in the DBPROPERTIES associated with a database using the ALTER DATABASE command.
No other metadata about the database can be changed, including its name and directory location:
There is no way to delete or “unset” a DBPROPERTY.
Creating Tables The CREATE TABLE statement follows SQL conventions, but Hive’s version offers significant extensions to support a wide range of flexibility where the data files for tables are stored, the formats used, etc.
In this section, we describe the other options available for the CREATE TABLE statement, adapting the employees table declaration we used previously in “Collection Data Types” on page 43:
First, note that you can prefix a database name, mydb in this case, if you’re not currently working in the target database.
If you add the option IF NOT EXISTS, Hive will silently ignore the statement if the table already exists.
This is useful in scripts that should create a table the first time they run.
If the schema specified differs from the schema in the table that already exists, Hive won’t warn you.
If your intention is for this table to have the new schema, you’ll have to drop the old table, losing your data, and then re-create it.
Consider if you should use one or more ALTER TABLE statements to change the existing table schema instead.
If you use IF NOT EXISTS and the existing table has a different schema than the schema in the CREATE TABLE statement, Hive will ignore the discrepancy.
You can add a comment to any column, after the type.
Like databases, you can attach a comment to the table itself and you can define one or more table properties.
In most cases, the primary benefit of TBLPROPERTIES is to add additional documentation in a key-value format.
However, when we examine Hive’s integration with databases such as DynamoDB (see “DynamoDB” on page 225), we’ll see that the TBLPROPERTIES can be used to express essential metadata about the database connection.
Finally, you can optionally specify a location for the table data (as opposed to metadata, which the metastore will always hold)
By default, Hive always creates the table’s directory under the directory for the enclosing database.
To avoid potential confusion, it’s usually better to use an external table if you don’t want to use the default location table.
You can also copy the schema (but not the data) of an existing table: CREATE TABLE IF NOT EXISTS mydb.employees2 LIKE mydb.employees;
This version also accepts the optional LOCATION clause, but note that no other properties, including the schema, can be defined; they are determined from the original table.
With no additional arguments, it shows the tables in the current working database.
If we aren’t in the same database, we can still list the tables in that database: hive> USE default;
If we have a lot of tables, we can limit the ones listed using a regular expression, a concept we’ll discuss in detail in “LIKE and RLIKE” on page 96:
If you know regular expressions, it’s better to test a candidate regular expression to make sure it actually works! The regular expression in the single quote looks for all tables with names starting with empl and ending with any other characters (the .* part)
Using the IN database_name clause and a regular expression for the table names together is not supported.
We can also use the DESCRIBE EXTENDED mydb.employees command to show details about the table.
We have reformatted the output for easier reading and we have suppressed many details to focus on the items that interest us now:
Replacing EXTENDED with FORMATTED provides more readable but also more verbose output.
The first section shows the output of DESCRIBE without EXTENDED or FORMATTED (i.e., the schema including the comments for each column)
If you only want to see the schema for a particular column, append the column to the table name.
Returning to the extended output, note the line in the description that starts with location:
It shows the full URI path in HDFS to the directory where Hive will keep all the data for this table, as we discussed above.
However, they are only shown in the Detailed Table Information if a user-specified table property has also been defined!
Managed Tables The tables we have created so far are called managed tables or sometimes called internal tables, because Hive controls the lifecycle of their data (more or less)
When we drop a managed table (see “Dropping Tables” on page 66), Hive deletes the data in the table.
However, managed tables are less convenient for sharing with other tools.
For example, suppose we have data that is created and used primarily by Pig or other tools, but we want to run some queries against it, but not give Hive ownership of the data.
We can define an external table that points to that data, but doesn’t take ownership of it.
External Tables Suppose we are analyzing data from the stock markets.
Periodically, we ingest the data for NASDAQ and the NYSE from a source like Infochimps (http://infochimps.com/da tasets) and we want to study this data with many tools.
The schema we’ll use next matches the schemas of both these data sources.
Let’s assume the data files are in the distributed filesystem directory /data/stocks.
The following table declaration creates an external table that can read all the data files for this comma-delimited data in /data/stocks:
The EXTERNAL keyword tells Hive this table is external and the LOCATION … clause is required to tell Hive where it’s located.
Because it’s external, Hive does not assume it owns the data.
Therefore, dropping the table does not delete the data, although the metadata for the table will be deleted.
There are a few other small differences between managed and external tables, where some HiveQL constructs are not permitted for external tables.
However, it’s important to note that the differences between managed and external tables are smaller than they appear at first.
Even for managed tables, you know where they are located, so you can use other tools, hadoop dfs commands, etc., to modify and even delete the files in the directories for managed tables.
Hive may technically own these directories and files, but it doesn’t have full control over them! Recall, in “Schema on Read” on page 48, we said that Hive really has no control over the integrity of the files used for storage and whether or not their contents are consistent with the table schema.
Still, a general principle of good software design is to express intent.
If the data is shared between tools, then creating an external table makes this ownership explicit.
You can tell whether or not a table is managed or external using the output of DESCRIBE EXTENDED tablename.
Near the end of the Detailed Table Information output, you will see the following for managed tables:
As for managed tables, you can also copy the schema (but not the data) of an existing table:
If you omit the EXTERNAL keyword and the original table is external, the new table will also be external.
If you omit EXTERNAL and the original table is managed, the new table will also be managed.
However, if you include the EXTERNAL keyword and the original table is managed, the new table will be external.
Even in this scenario, the LOCATION clause will still be optional.
Partitioned, Managed Tables The general notion of partitioning data is an old one.
It can take many forms, but often it’s used for distributing load horizontally, moving data physically closer to its most frequent users, and other purposes.
We’ll see that they have important performance benefits, and they can help organize data in a logical fashion, such as hierarchically.
Let’s return to our employees table and imagine that we work for a very large multinational corporation.
Our HR people often run queries with WHERE clauses that restrict the results to a particular country or to a particular first-level subdivision (e.g., state in the United States or province in Canada)
There is no ambiguity in queries, since we have to use address.state to project the value inside the address.
So, let’s partition the data first by country and then by state:
If we create this table in the mydb database, there will still be an employees directory for the table:
However, Hive will now create subdirectories reflecting the partitioning structure.
The state directories will contain zero or more files for the employees in those states.
Once created, the partition keys (country and state, in this case) behave like regular columns.
There is one known exception, due to a bug (see “Aggregate functions” on page 85)
In fact, users of the table don’t need to care if these “columns” are partitions or not, except when they want to optimize query performance.
For example, the following query selects all employees in the state of Illinois in the United States:
Note that because the country and state values are encoded in directory names, there is no reason to have this data in the data files themselves.
In fact, the data just gets in the way in the files, since you have to account for it in the table schema, and this data wastes space.
Perhaps the most important reason to partition data is for faster queries.
In the previous query, which limits the results to employees in Illinois, it is only necessary to scan the contents of one directory.
Even if we have thousands of country and state directories, all but one can be ignored.
For very large data sets, partitioning can dramatically improve query performance, but only if the partitioning scheme reflects common range filtering (e.g., by locations, timestamp ranges)
When we add predicates to WHERE clauses that filter on partition values, these predicates are called partition filters.
Of course, if you need to do a query for all employees around the globe, you can still do it.
Hive will have to read every directory, but hopefully these broader disk scans will be relatively rare.
However, a query across all partitions could trigger an enormous MapReduce job if the table data and number of partitions are large.
A highly suggested safety measure is putting Hive into “strict” mode, which prohibits queries of partitioned tables without a WHERE clause that filters on partitions.
You can set the mode to “nonstrict,” as in the following session:
You can see the partitions that exist with the SHOW PARTITIONS command: hive> SHOW PARTITIONS employees; ...
If you have a lot of partitions and you want to see if partitions have been defined for particular partition keys, you can further restrict the command with an optional PARTI TION clause that specifies one or more of the partitions with specific values:
The DESCRIBE EXTENDED employees command shows the partition keys: hive> DESCRIBE EXTENDED employees; name         string, salary       float, ...
The schema part of the output lists the country and state with the other columns, because they are columns as far as queries are concerned.
The Detailed Table Infor mation includes the country and state as partition keys.
The comments for both of these keys are null; we could have added comments just as for regular columns.
You create partitions in managed tables by loading data into them.
Notice how we reference the HOME environment variable in HiveQL:
See “Loading Data into Managed Tables” on page 71 for more information on populating tables.
External Partitioned Tables You can use partitioning with external tables.
In fact, you may find that this is your most common scenario for managing large production data sets.
The combination gives you a way to “share” data with other tools, while still optimizing query performance.
You also have more flexibility in the directory structure used, as you define it yourself.
Let’s consider a new example that fits this scenario well: logfile analysis.
Most organizations use a standard format for log messages, recording a timestamp, severity (e.g., ERROR, WARNING, INFO), perhaps a server name and process ID, and then an arbitrary text message.
Suppose our Extract, Transform, and Load (ETL) process ingests and aggregates logfiles in our environment, converting each log message to a tab-delimited record and also decomposing the timestamp into separate year, month, and day fields, and a combined hms field for the remaining hour, minute, and second parts of the timestamp, for reasons that will become clear in a moment.
You could do this parsing of log messages using the string parsing functions built into Hive or Pig, for example.
Alternatively, we could use smaller integer types for some of the timestamp-related fields to conserve space.
We’re assuming that a day’s worth of log data is about the correct size for a useful partition and finer grain queries over a day’s data will be fast enough.
Recall that when we created the nonpartitioned external stocks table, a LOCATION … clause was required.
Instead, an ALTER TABLE statement is used to add each partition separately.
It must specify a value for each partition key, the year, month, and day, in this case (see “Alter Table” on page 66 for more details on this feature)
The directory convention we use is completely up to us.
Here, we follow a hierarchical directory structure, because it’s a logical way to organize our data, but there is no requirement to do so.
An interesting benefit of this flexibility is that we can archive old data on inexpensive storage, like Amazon’s S3, while keeping newer, more “interesting” data in HDFS.
For example, each day we might use the following procedure to move data older than a month to S3:
Copy the data for the partition being moved to S3
You don’t have to be an Amazon Elastic MapReduce user to use S3 this way.
You can still query this data, even queries that cross the month-old “boundary,” where some data is read from HDFS and some data is read from S3! By the way, Hive doesn’t care if a partition directory doesn’t exist for a partition or if it has no files.
In both cases, you’ll just get no results for a query that filters for the partition.
This is convenient when you want to set up partitions before a separate process starts writing data to them.
As soon as data is there, queries will return results from that data.
This feature illustrates another benefit: new data can be written to a dedicated directory with a clear distinction from older data in other directories.
Also, whether you move old data to an “archive” location or delete it outright, the risk of tampering with newer data is reduced since the data subsets are in separate directories.
As for nonpartitioned external tables, Hive does not own the data and it does not delete the data if the table is dropped.
As for managed partitioned tables, you can see an external table’s partitions with SHOW PARTITIONS:
Similarly, the DESCRIBE EXTENDED log_messages shows the partition keys both as part of the schema and in the list of partitionKeys:
This output is missing a useful bit of information, the actual location of the partition data.
There is a location field, but it only shows Hive’s default directory that would be used if the table were a managed table.
We frequently use external partitioned tables because of the many benefits they provide, such as logical data management, performant queries, etc.
You can use it with managed tables, too, when you have (or will have) data for partitions in directories created outside of the LOAD and INSERT options we discussed above.
You’ll need to remember that not all of the table’s data will be under the usual Hive “warehouse” directory, and this data won’t be deleted when you drop the managed table! Hence, from a “sanity” perspective, it’s questionable whether you should dare to use this feature with managed tables.
Customizing Table Storage Formats In “Text File Encoding of Data Values” on page 45, we discussed that Hive defaults to a text file format, which is indicated by the optional clause STORED AS TEXTFILE, and you can overload the default values for the various delimiters when creating the table.
Here we repeat the definition of the employees table we used in that discussion:
When TEXTFILE is used, each line is considered a separate record.
You can replace TEXTFILE with one of the other built-in file formats supported by Hive, including SEQUENCEFILE and RCFILE, both of which optimize disk space usage and I/O bandwidth performance using binary encoding and optional compression.
Hive draws a distinction between how records are encoded into files and how columns are encoded into records.
If you are unfamiliar with Java, the dotted name syntax indicates a hierarchical namespace tree of packages that actually corresponds to the directory structure for the Java code.
The last name, TextInputFormat, is a class in the lowest-level package mapred.
For completeness, there is also an output format that Hive uses for writing the output of queries to files and to the console.
Hive uses an input format to split input streams into records, an output format to format records into output streams (i.e., the output of queries), and a SerDe to parse records into columns, when reading, and encodes columns into records, when writing.
Third-party input and output formats and SerDes can be specified, a feature which permits users to customize Hive for a wide range of file formats not supported natively.
Here is a complete example that uses a custom SerDe, input format, and output format for files accessible through the Avro protocol, which we will discuss in detail in “Avro Hive SerDe” on page 209:
Hive provides the WITH SERDEPRO PERTIES feature that allows users to pass configuration information to the SerDe.
Note that the name and value of each property must be a quoted string.
If you specify one of these formats, you are required to specify both of them.
Note that the DESCRIBE EXTENDED table command lists the input and output formats, the SerDe, and any SerDe properties in the DETAILED TABLE INFORMATION.
Finally, there are a few additional CREATE TABLE clauses that describe more details about how the data is supposed to be stored.
Let’s extend our previous stocks table example from “External Tables” on page 56:
If not used and the table doesn’t exist, Hive returns an error.
For managed tables, the table metadata and data are deleted.
While it’s not guaranteed to work for all versions of all distributed filesystems, if you accidentally drop a managed table with important data, you may be able to re-create the table, re-create any partitions, and then move the files from .Trash to the correct directories (using the filesystem commands) to restore the data.
For external tables, the metadata is deleted but the data is not.
Alter Table Most table properties can be altered with ALTER TABLE statements, which change metadata about the table but not the data itself.
These statements can be used to fix mistakes in schema, move partition locations (as we saw in “External Partitioned Tables” on page 61), and do other operations.
It’s up to you to ensure that any modifications are consistent with the actual data.
Renaming a Table Use this statement to rename the table log_messages to logmsgs:
Adding, Modifying, and Dropping a Table Partition As we saw previously, ALTER TABLE table ADD PARTITION … is used to add a new partition to a table (usually an external table)
Here we repeat the same command shown previously with the additional options available:
Multiple partitions can be added in the same query when using Hive v0.8.0 and later.
As always, IF NOT EXISTS is optional and has the usual meaning.
Hive v0.7.X allows you to use the syntax with multiple partition specifications, but it actually uses just the first partition specification, silently ignoring the others! Instead, use a separate ALTER STATEMENT statement for each partition.
This command does not move the data from the old location, nor does it delete the old data.
For managed tables, the data for the partition is deleted, along with the metadata, even if the partition was created using ALTER TABLE … ADD PARTITION.
Changing Columns You can rename a column, change its position, type, or comment:
You have to specify the old name, a new name, and the type, even if the name or type is not changing.
The keyword COLUMN is optional as is the COMMENT clause.
If you aren’t moving the column, the AFTER other_column clause is not necessary.
In the example shown, we move the column after the severity column.
If you want to move the column to the first position, use FIRST instead of AFTER other_column.
If you are moving columns, the data must already match the new schema or you must change it to match by some other means.
Adding Columns You can add new columns to the end of the existing columns, before any partition columns.
If any of the new columns are in the wrong position, use an ALTER COLUMN table CHANGE COLUMN statement for each one to move it to the correct position.
Deleting or Replacing Columns The following example removes all the existing columns and replaces them with the new columns specified:
This statement effectively renames the original hms column and removes the server and process_id columns from the original schema definition.
As for all ALTER statements, only the table metadata is changed.
Recall that the SerDe determines how records are parsed into columns (deserialization) and how a record’s columns are written to storage (serialization)
Alter Table Properties You can add additional table properties or modify existing properties, but not remove them:
The PARTITION clause is required if the table is partitioned.
You can specify a new SerDe along with SerDe properties or change the properties for the existing SerDe.
The SERDEPROPERTIES feature is a convenient mechanism that SerDe implementations can exploit to permit user customization.
The following example demonstrates how to add new SERDEPROPERTIES for the current SerDe:
You can alter the storage properties that we discussed in “Creating Tables” on page 53:
See also “Bucketing Table Data Storage” on page 125 for information on the use of data bucketing.
Miscellaneous Alter Table Statements In “Execution Hooks” on page 158, we’ll discuss a technique for adding execution “hooks” for various operations.
The ALTER TABLE … TOUCH statement is used to trigger these hooks:
A typical scenario for this statement is to trigger execution of the hooks when table storage files have been modified outside of Hive.
This statement won’t create the table or partition if it doesn’t already exist.
The ALTER TABLE … ARCHIVE PARTITION statement captures the partition files into a Hadoop archive (HAR) file.
This only reduces the number of files in the filesystem, reducing the load on the NameNode, but doesn’t provide any space savings (e.g., through compression):
This feature is only available for individual partitions of partitioned tables.
The following statements prevent the partition from being dropped and queried:
This chapter continues our discussion of HiveQL, the Hive query language, focusing on the data manipulation language parts that are used to put data into tables and to extract data from tables to the filesystem.
If they are, please refer to Chapter 6 for details.
Loading Data into Managed Tables Since Hive has no row-level insert, update, and delete operations, the only way to put data into an table is to use one of the “bulk” load operations.
Or you can just write files in the correct directories by other means.
We saw an example of how to load data into a managed table in “Partitioned, Managed Tables” on page 58, which we repeat here with an addition, the use of the OVERWRITE keyword:
This command will first create the directory for the partition, if it doesn’t already exist, then copy the data to it.
If the target table is not partitioned, you omit the PARTITION clause.
It is conventional practice to specify a path that is a directory, rather than an individual file.
Hive will copy all the files in the directory, which give you the flexibility of organizing the data into multiple files and changing the file naming convention, without.
Either way, the files will be copied to the appropriate location for the table and the names will be the same.
If the LOCAL keyword is used, the path is assumed to be in the local filesystem.
If LOCAL is omitted, the path is assumed to be in the distributed filesystem.
In this case, the data is moved from the path to the final location.
The rationale for this inconsistency is the assumption that you usually don’t want duplicate copies of your data files in the distributed filesystem.
Also, because files are moved in this case, Hive requires the source and target files and directories to be in the same filesystem.
For example, you can’t use LOAD DATA to load (move) data from one HDFS cluster to another.
It is more robust to specify a full path, but relative paths can be used.
When running in local mode, the relative path is interpreted relative to the user’s working directory when the Hive CLI was started.
For distributed or pseudo-distributed mode, the path is interpreted relative to the user’s home directory in the distributed filesystem, which is /user/$USER by default in HDFS and MapRFS.
If you specify the OVERWRITE keyword, any data already present in the target directory will be deleted first.
Without the keyword, the new files are simply added to the target directory.
However, if files already exist in the target directory that match filenames being loaded, the old files are overwritten.
Versions of Hive before v0.9.0 had the following bug: when the OVER WRITE keyword was not used, an existing data file in the target directory would be overwritten if its name matched the name of a data file being written to the directory.
The PARTITION clause is required if the table is partitioned and you must specify a value for each partition key.
In the example, the data will now exist in the following directory:
Another limit on the file path used, the INPATH clause, is that it cannot contain any directories.
Hive does not verify that the data you are loading matches the schema for the table.
However, it will verify that the file format matches the table definition.
For example, if the table was created with SEQUENCEFILE storage, the loaded files must be sequence files.
Inserting Data into Tables from Queries The INSERT statement lets you load data into a table from a query.
Reusing our employ ees example from the previous chapter, here is an example for the state of Oregon, where we presume the data is already in another table called staged_employees.
For reasons we’ll discuss shortly, let’s use different names for the country and state fields in staged_employees, calling them cnty and st, respectively:
With OVERWRITE, any previous contents of the partition (or whole table if not partitioned) are replaced.
If you drop the keyword OVERWRITE or replace it with INTO, Hive appends the data rather than replaces it.
This feature is only available in Hive v0.8.0 or later.
This example suggests one common scenario where this feature is useful: data has been staged in a directory, exposed to Hive as an external table, and now you want to put it into the final, partitioned table.
A workflow like this is also useful if you want the target table to have a different record format than the source table (e.g., a different field delimiter)
The following example shows this feature for creating the employees partitions for three states:
We have used indentation to make it clearer how the clauses group together.
In fact, by using this construct, some records from the source table can be written to multiple partitions of the destination table or none of them.
To be clear, each INSERT clause can insert into a different table, when desired, and some of those tables could be partitioned while others aren’t.
Hence, some records from the input might get written to multiple output locations and others might get dropped! You can mix INSERT OVERWRITE clauses and INSERT INTO clauses, as well.
Dynamic Partition Inserts There’s still one problem with this syntax: if you have a lot of partitions to create, you have to write a lot of SQL! Fortunately, Hive also supports a dynamic partition feature, where it can infer the partitions to create based on query parameters.
By comparison, up until now we have considered only static partitions.
Hive determines the values of the partition keys, country and state, from the last two columns in the SELECT clause.
This is why we used different names in staged_employ ees, to emphasize that the relationship between the source column values and the output partition values is by position only and not by matching on names.
After running this query, employees will have 100 partitions! You can also mix dynamic and static partitions.
This variation of the previous query specifies a static value for the country (US) and a dynamic value for the state:
The static partition keys must come before the dynamic partition keys.
When it is enabled, it works in “strict” mode by default, where it expects at least some columns to be static.
This helps protect against a badly designed query that generates a gigantic number of partitions.
For example, you partition by timestamp and generate a separate partition for each second! Perhaps you meant to partition by day or maybe hour instead.
Several other properties are also used to limit excess resource utilization.
The maximum number of dynamic partitions that can be created by each mapper or reducer.
Raises a fatal error if one mapper or reducer attempts to create more than the threshold.
The total number of dynamic partitions that can be created by one statement with dynamic partitioning.
The maximum total number of files that can be created globally.
A Hadoop counter is used to track the number of files created.
So, for example, our first example using dynamic partitioning for all partitions might actually look this, where we set the desired properties just before use:
Creating Tables and Loading Them in One Query You can also create a table and insert query results into it in one statement:
This table contains just the name, salary, and address columns from the employee table records for employees in California.
The schema for the new table is taken from the SELECT clause.
A common use for this feature is to extract a convenient subset of data from a larger, more unwieldy table.
Recall that “populating” a partition for an external table is done with an ALTER TABLE statement, where we aren’t “loading” data, per se, but pointing metadata to a location where the data can be found.
Exporting Data How do we get data out of tables? If the data files are already formatted the way you want, then it’s simple enough to copy the directories or files:
One or more files will be written to /tmp/ca_employees, depending on the number of reducers invoked.
Independent of how the data is actually stored in the source table, it is written to files with all fields serialized as strings.
Hive uses the same encoding in the generated output files as it uses for the tables internal storage.
As a reminder, we can look at the results from within the hive CLI:
If there were two or more reducers writing output, we would have additional files with similar names (e.g., 000001_0)
Just like inserting data to tables, you can specify multiple inserts to directories:
There are some limited options for customizing the output of the data (other than writing a custom OUTPUTFORMAT, as discussed in “Customizing Table Storage Formats” on page 63)
To format columns, the built-in functions include those for formatting strings, such as converting case, padding output, and more.
See “Other built-in functions” on page 88 for more details.
If you export table data frequently, it might be appropriate to use comma or tab delimiters.
Another workaround is to define a “temporary” table with the storage configured to match the desired output format (e.g., tab-delimited fields)
Then write a query result to that table and use INSERT OVERWRITE DIRECTORY, selecting from the temporary table.
Unlike many relational databases, there is no temporary table feature in Hive.
You have to manually drop any tables you create that aren’t intended to be permanent.
After learning the many ways we can define and format tables, let’s learn how to run queries.
Of course, we have assumed all along that you have some prior knowledge of SQL.
We’ll move quickly through details that are familiar to users with prior SQL experience and focus on what’s unique to HiveQL, including syntax and feature differences, as well as performance implications.
The FROM clause identifies from which table, view, or nested query we select records (see Chapter 7)
For a given record, SELECT specifies the columns to keep, as well as the outputs of function calls on one or more columns (e.g., the aggregation functions like count(*))
Let’s assume we have the same contents we showed in “Text File Encoding of Data Values” on page 45 for four employees in the US state of Illinois (abbreviated IL)
Here are queries of this table and the output they produce:
The second version uses a table alias e, which is not very useful in this query, but becomes necessary in queries with JOINs (see “JOIN Statements” on page 98) where several different tables are used:
When you select columns that are one of the collection types, Hive uses JSON (JavaScript Object Notation) syntax for the output.
First, let’s select the subordinates, an ARRAY, where a comma-separated list surrounded with […] is used.
Note that STRING elements of the collection are quoted, while the primitive STRING name column is not:
Here is a query that selects the first element of the subordinates array:
Also, the extracted STRING values are no longer quoted! To reference a MAP element, you also use ARRAY[...] syntax, but with key values instead of integer indices:
Specify Columns with Regular Expressions We can even use regular expressions to select the columns we want.
The following query selects the symbol column and all columns from stocks whose names start with the prefix price:1
Computing with Column Values Not only can you select columns in a table, but you can manipulate column values using function calls and arithmetic expressions.
For example, let’s select the employees’ names converted to uppercase, their salaries, federal taxes percentage, and the value that results if we subtract the federal taxes portion from their salaries and round to the nearest integer.
We could call a built-in function map_values to extract all the values from the deductions map and then add them up with the built-in sum function.
The following query is long enough that we’ll split it over two lines.
Note the secondary prompt that Hive uses, an indented greater-than sign (>):
At the time of this writing, the Hive Wiki shows an incorrect syntax for specifying columns using regular expressions.
Let’s discuss arithmetic operators and then discuss the use of functions in expressions.
If the operands are integer types, the quotient of the division is returned.
No type coercion is performed if the two operands are of the same numeric type.
Otherwise, if the types differ, then the value of the smaller of the two types is promoted to wider type of the other value.
Wider in the sense that a type with more bytes can hold a wider range of values.
For example, for INT and BIGINT operands, the INT is promoted to BIGINT.
Since the deductions are FLOATS, the 1 was promoted to FLOAT.
You have to be careful about data overflow or underflow when doing arithmetic.
Hive follows the rules for the underlying Java types, where no attempt is made to automatically convert a result to a wider type if one exists, when overflow or underflow will occur.
Multiplication and division are most likely to trigger this problem.
It pays to be aware of the ranges of your numeric data values, whether or not those values approach the upper or lower range limits of the types you are using in the corresponding schema, and what kinds of calculations people might do with the data.
If you are concerned about overflow or underflow, consider using wider types in the schema.
The drawback is the extra memory each data value will occupy.
You can also convert values to wider types in specific expressions, called casting.
Finally, it is sometimes useful to scale data values, such as dividing by powers of 10, using log values, and so on.
Scaling can also improve the accuracy and numerical stability of algorithms used in certain machine learning calculations, for example.
Using Functions Our tax-deduction example also uses a built-in mathematical function, round(), for finding the nearest integer for a DOUBLE value.
Passing in an integer seed makes the return value deterministic.
Note the functions floor, round, and ceil (“ceiling”) for converting DOUBLE to BIGINT, which is floating-point numbers to integer numbers.
These functions are the preferred technique, rather than using the cast operator we mentioned above.
Also, there are functions for converting integers to strings in different bases (e.g., hexadecimal)
A special kind of function is the aggregate function that returns a single value resulting from some computation over many rows.
Perhaps the two best known examples are count, which counts the number of rows (or values for a specific column), and avg, which returns the average value of the specified column values.
Here is a query that counts the number of our example employees and averages their salaries:
You can usually improve the performance of aggregation by setting the following property to true, hive.map.aggr, as shown here:
This setting will attempt to do “top-level” aggregation in the map phase, as in this example.
An aggregation that isn’t top-level would be aggregation after performing a GROUP BY.
For example, we could count the unique stock symbols this way:
Wait, zero?? There is a bug when trying to use count(DISTINCT col) when col is a partition column.
Note that the Hive wiki currently claims that you can’t use more than one function(DIS TINCT …) expression in a query.
For example, the following is supposed to be disallowed, but it actually works:
The “inverse” of aggregate functions are so-called table generating functions, which take single columns and expand them to multiple columns or rows.
We will discuss them extensively in “Table Generating Functions” on page 165, but to complete the contents of this section, we will discuss them briefly now and list the few built-in table generating functions available in Hive.
To explain by way of an example, the following query converts the subordinate array in each employees record into zero or more new records.
If an employee record has an empty subordinates array, then no new records are generated.
We used a column alias, sub, defined using the AS sub clause.
When using table generating functions, column aliases are required by Hive.
There are many other particular details that you must understand to use these functions correctly.
We’ll wait until “Table Generating Functions” on page 165 to discuss the details.
Like get_json_object, but it takes multiple names and returns a tuple.
All the input parameters and output column types are STRING.
It takes a URL and the part names to extract, returning a tuple.
All the input parameters and output column types are STRING.
You can pass an arbitrary number of string arguments and the result will contain all of them joined together.
Returns the substring for the index’s match using the regex_pattern.
The optional key is used for the last QUERY:<key> request.
A NULL is returned if the conversion does not succeed.
Extract the JSON object from a JSON string based on the given JSON path, and return the JSON string of the.
Returns the index of the comma-separated string where s is found, or NULL if it is not found.
The lang and country arguments are optional; if omitted, the default locale is used.
Like ngrams, but looks for n-grams that begin with the second array of words in each outer array.
Note that the time-related functions (near the end of the table) take integer or string arguments.
As of Hive v0.8.0, these functions also take TIMESTAMP arguments, but they will continue to take integer or string arguments for backwards compatibility.
The LIMIT clause puts an upper limit on the number of rows returned:
Column Aliases You can think of the previous example query as returning a new relation with new columns, some of which are anonymous results of manipulating columns in employees.
It’s sometimes useful to give those anonymous columns a name, called a column alias.
Nested SELECT Statements The column alias feature is especially useful in nested select statements.
We’ll cover WHERE clauses in “WHERE Clauses” on page 92 below.
When Hive Can Avoid MapReduce If you have been running the queries in this book so far, you have probably noticed that a MapReduce job is started in most cases.
Hive implements some kinds of queries without using MapReduce, in so-called local mode, for example:
In this case, Hive can simply read the records from employees and dump the formatted output to the console.
This even works for WHERE clauses that only filter on partition keys, with or without LIMIT clauses:
Like SELECT clauses, we have already used many simple examples of WHERE clauses before defining the clause, on the assumption you have seen them before.
Several predicate expressions can be joined with AND and OR clauses.
When the predicate expressions evaluate to true, the corresponding rows are retained in the output.
We just used the following example that restricts the results to employees in the state of California:
The predicates can reference the same variety of computations over column values that can be used in SELECT clauses.
Here we adapt our previously used query involving Federal Taxes, filtering for those rows where the salary minus the federal taxes is greater than 70,000:
This query is a bit ugly, because the complex expression on the second line is duplicated in the WHERE clause.
The following variation eliminates the duplication, using a column alias, but unfortunately it’s not valid:
As the error message says, we can’t reference column aliases in the WHERE clause.
A <= B Primitive types NULL if A or B is NULL; true if A is less than or equal to B; false otherwise.
A >= B Primitive types NULL if A or B is NULL; true if A is greater than or equal to B; false otherwise.
Matching is done by the JDK regular expression library and hence it follows the rules of that library.
For example, the regular expression must match the entire string A, not just a subset.
We’ll discuss LIKE and RLIKE in detail below (“LIKE and RLIKE” on page 96)
First, let’s point out an issue with comparing floating-point numbers that you should understand.
Gotchas with Floating-Point Comparisons A common gotcha arises when you compare floating-point numbers of different types (i.e., FLOAT versus DOUBLE)
When you write a floating-point literal value like 0.2, Hive uses a DOUBLE to hold the value.
We defined the deductions map values to be FLOAT, which means that Hive will implicitly convert the tax deduction value to DOUBLE to do the comparison.
The number 0.2 can’t be represented exactly in a FLOAT or DOUBLE.
See http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg .html for an in-depth discussion of floating-point number issues.
In this particular case, the closest exact value is just slightly greater than 0.2, with a few nonzero bits at the least significant end of the number.
Rather, it’s a general problem for all systems that use the IEEE standard for encoding floating-point numbers! However, there are two workarounds we can use in Hive.
We could use DOUBLE instead of FLOAT in our schema.
However, this change will increase the memory footprint of our queries.
Also, we can’t simply change the schema like this if the data file is a binary file format like SEQUENCEFILE (discussed in Chapter 15)
The second workaround is to explicitly cast the 0.2 literal value to FLOAT.
Java has a nice way of doing this: you append the letter f or F to the end of the number (e.g., 0.2f)
Unfortunately, Hive doesn’t support this syntax; we have to use the cast operator.
Here is a modified query that casts the 0.2 literal value to FLOAT.
With this change, the expected results are returned by the query:
Note the syntax inside the cast operator: number AS FLOAT.
Actually, there is also a third solution: avoid floating-point numbers for anything involving money.
You have probably seen LIKE before, a standard SQL operator.
It lets us match on strings that begin with or end with a particular substring, or when the substring appears anywhere within the string.
For example, the following three queries select the employee names and addresses where the street ends with Ave., the city begins with O, and the street contains Chicago:
A Hive extension is the RLIKE clause, which lets us use Java regular expressions, a more powerful minilanguage for specifying matches.
The rich details of regular expression syntax and features are beyond the scope of this book.
The entry for RLIKE in Table 6-6 provides links to resources with more details on regular expressions.
Here, we demonstrate their use with an example, which finds all the employees whose street contains the word Chicago or Ontario:
The string after the RLIKE keyword has the following interpretation.
Hence, there might be no characters before “Chicago” or “Ontario” and there might be no characters after them.
Of course, we could have written this particular example with two LIKE clauses:
General regular expression matches will let us express much richer matching criteria that would become very unwieldy with joined LIKE clauses such as these.
The following query groups stock records for Apple by year, then averages the closing price for each year:
Here’s the previous query with an additional HAVING clause that limits the results to years where the average closing price was greater than $50.0:
Inner JOIN In an inner JOIN, records are discarded unless join criteria finds matching records in every table being joined.
For example, the following query compares Apple (symbol AAPL) and IBM (symbol IBM)
The stocks table is joined against itself, a self-join, where the dates, ymd (year-month-day) values must be equal in both tables.
We say that the ymd columns are the join keys in this query:
The ON clause specifies the conditions for joining records between the two tables.
The WHERE clause limits the lefthand table to AAPL records and the righthand table to IBM records.
You can also see that using table aliases for the two occurrences of stocks is essential in this query.
As you may know, IBM is an older company than Apple.
It has been a publicly traded stock for much longer than Apple.
However, since this is an inner JOIN, no IBM records.
This is not valid in Hive, primarily because it is difficult to implement these kinds of joins in MapReduce.
It turns out that Pig offers a cross product feature that makes it possible to implement this join, even though Pig’s native join feature doesn’t support it, either.
Also, Hive does not currently support using OR between predicates in ON clauses.
To see a nonself join, let’s introduce the corresponding dividends data, also available from infochimps.org, as described in “External Tables” on page 56:
Here is an inner JOIN between stocks and dividends for Apple, where we use the ymd and symbol columns as join keys:
Yes, Apple paid a dividend years ago and only recently announced it would start doing so again! Note that because we have an inner JOIN, we only see records approximately every three months, the typical schedule of dividend payments, which are announced when reporting quarterly results.
Most of the time, Hive will use a separate MapReduce job for each pair of things to join.
In this example, it would use one job for tables a and b, then a second job to join the output of the first join with c.
Why not join b and c first? Hive goes from left to right.
However, this example actually benefits from an optimization we’ll discuss next.
Join Optimizations In the previous example, every ON clause uses a.ymd as one of the join keys.
In this case, Hive can apply an optimization where it joins all three tables in a single MapReduce job.
The optimization would also be used if b.ymd were used in both ON clauses.
When joining three or more tables, if every ON clause uses the same join key, a single MapReduce job will be used.
Hive also assumes that the last table in the query is the largest.
It attempts to buffer the other tables and then stream the last table through, while performing joins on individual records.
Therefore, you should structure your join queries so the largest table is last.
We actually made the mistake of using the smaller dividends table last:
We should switch the positions of stocks and dividends: SELECT s.ymd, s.symbol, s.price_close, d.dividend FROM dividends d JOIN stocks s ON s.ymd = d.ymd AND s.symbol = d.symbol WHERE s.symbol = 'AAPL';
It turns out that these data sets are too small to see a noticeable performance difference, but for larger data sets, you’ll want to exploit this optimization.
Fortunately, you don’t have to put the largest table last in the query.
Hive also provides a “hint” mechanism to tell the query optimizer which table should be streamed:
Now Hive will attempt to stream the stocks table, even though it’s not the last table in the query.
In this join, all the records from the lefthand table that match the WHERE clause are returned.
If the righthand table doesn’t have a record that matches the ON criteria, NULL is used for each column selected from the righthand table.
Recall what we said previously about speeding up queries by adding partition filters in the WHERE clause.
To speed up our previous query, we might choose to add predicates that select on the exchange in both tables:
However, the output has changed, even though we thought we were just adding an optimization! We’re back to having approximately four stock records per year and we have non-NULL entries for all the dividend values.
In other words, we are back to the original inner join! This is actually common behavior for all outer joins in most SQL implementations.
It occurs because the JOIN clause is evaluated first, then the results are passed through the WHERE clause.
By the time the WHERE clause is reached, d.exchange is NULL most of the time, so the “optimization” actually filters out all records except those on the day of dividend payments.
One solution is straightforward; remove the clauses in the WHERE clause that reference the dividends table:
You might wonder if you can move the predicates from the WHERE clause into the ON clause, at least the partition filters.
However, using such filter predicates in ON clauses for inner joins does work! Fortunately, there is solution that works for all joins; use nested SELECT statements:
The nested SELECT statement performs the required “push down” to apply the partition filters before data is joined.
Also, contrary to Hive documentation, partition filters don’t work in ON clauses for OUTER JOINS, although they do work for INNER JOINS!
Here we switch the places of stocks and dividends and perform a righthand join, but leave the SELECT statement unchanged:
If we convert the previous query to a full-outer join, we’ll actually get the same results, since there is never a case where a dividend record exists without a matching stock record:
Note that the SELECT and WHERE clauses can’t reference columns from the righthand table.
The reason semi-joins are more efficient than the more general inner join is as follows.
For a given record in the lefthand table, Hive can stop looking for matching records in the righthand table as soon as any match is found.
At that point, the selected columns from the lefthand table record can be projected.
Cartesian Product JOINs A Cartesian product is a join where all the tuples in the left side of the join are paired with all the tuples of the right table.
Using the table of stocks and dividends, it is hard to find a reason for a join of this type, as the dividend of one stock is not usually paired with another.
Unlike other join types, Cartesian products are not executed in parallel, and they are not optimized in any way using MapReduce.
It is critical to point out that using the wrong join syntax will cause a long, slow-running Cartesian product query.
For example, the following query will be optimized to an inner join in many databases, but not in Hive:
In Hive, this query computes the full Cartesian product before applying the WHERE clause.
For example, suppose there is a table of user preferences, a table of news articles, and an algorithm that predicts which articles a user would like to read.
A Cartesian product is required to generate the set of all users and all pages.
Map-side Joins If all but one table is small, the largest table can be streamed through the mappers while the small tables are cached in memory.
Hive can do all the joining map-side, since it can look up every possible match against the small tables in memory, thereby eliminating the reduce step required in the more common join scenarios.
Even on smaller data sets, this optimization is noticeably faster than the normal join.
Not only does it eliminate reduce steps, it sometimes reduces the number of map steps, too.
The joins between stocks and dividends can exploit this optimization, as the dividends data set is small enough to be cached.
Before Hive v0.7, it was necessary to add a hint to the query to enable this optimization.
The hint still works, but it’s now deprecated as of Hive v0.7
Note that you can also configure the threshold size for table files considered small enough to use this optimization.
Here is the default definition of the property (in bytes):
If you always want Hive to attempt this optimization, set one or both of these properties in your $HOME/.hiverc file.
Hive does not support the optimization for right- and full-outer joins.
Briefly, the data must be bucketed on the keys used in the ON clause and the number of buckets for one table must be a multiple of the number of buckets for the other table.
When these conditions are met, Hive can join individual buckets between tables in the map phase, because it does not need to fetch the entire contents of one table to match against each bucket in the other table.
If the bucketed tables actually have the same number of buckets and the data is sorted by the join/bucket keys, then Hive can perform an even faster sort-merge join.
Once again, properties must be set to enable the optimization:
It performs a total ordering of the query result set.
This means that all the data is passed through a single reducer, which may take an unacceptably long time to execute for larger data sets.
Hive adds an alternative, SORT BY, that orders the data only within each reducer, thereby performing a local ordering, where each reducer’s output will be sorted.
In both cases, the syntax differs only by the use of the ORDER or SORT keyword.
You can specify any columns you wish and specify whether or not the columns are ascending using the ASC keyword (the default) or descending using the DESC keyword.
The two queries look almost identical, but if more than one reducer is invoked, the output will be sorted differently.
While each reducer’s output files will be sorted, the data will probably overlap with the output of other reducers.
All data that flows through a MapReduce job is organized into key-value pairs.
Hive must use this feature internally when it converts your queries to MapReduce jobs.
There is one other scenario where these clauses are useful.
By default, MapReduce computes a hash on the keys output by mappers and tries to evenly distribute the key-value pairs among the available reducers using the hash values.
Unfortunately, this means that when we use SORT BY, the contents of one reducer’s output will overlap significantly with the output of the other reducers, as far as sorted order is concerned, even though the data is sorted within each reducer’s output.
Say we want the data for each stock symbol to be captured together.
We can use DISTRIBUTE BY to ensure that the records for each stock symbol go to the same reducer, then use SORT BY to order the data the way we want.
Of course, the ASC keywords could have been omitted as they are the defaults.
The ASC keyword is placed here for reasons that will be described shortly.
Note that Hive requires that the DISTRIBUTE BY clause come before the SORT BY clause.
Suppose that the same columns are used in both clauses and all columns are sorted by ascending order (the default)
In this case, the CLUSTER BY clause is a shor-hand way of expressing the same query.
For example, let’s modify the previous query to drop sorting by s.ymd and use CLUSTER BY on s.symbol:
Because the sort requirements are removed for the s.ymd, the output reflects the original order of the stock data, which is sorted descending.
Casting We briefly mentioned in “Primitive Data Types” on page 41 that Hive will perform some implicit conversions, called casts, of numeric data types, as needed.
For example, when doing comparisons between two numbers of different types.
Here we discuss the cast() function that allows you to explicitly convert a value of one type to another.
Recall our employees table uses a FLOAT for the salary column.
Now, imagine for a moment that STRING was used for that column instead.
How could we work with the values as FLOATS? The following example casts the values to FLOAT before performing a comparison:
The syntax of the cast function is cast(value AS TYPE)
What would happen in the example if a salary value was not a valid string for a floating-point number? In this case, Hive returns NULL.
Note that the preferred way to convert floating-point numbers to integers is to use the round() or floor() functions listed in Table 6-2, rather than to use the cast operator.
However, if you know the value is a number, you can nest cast() invocations, as in this example where column b is a BINARY column:
If we bucket on a column instead of rand(), then identical results are returned on multiple runs:
The denominator in the bucket clause represents the number of buckets into which data will be hashed.
Block Sampling Hive offers another syntax for sampling a percentage of blocks of an input path as an alternative to sampling based on rows:
This sampling is not known to work with all file formats.
Also, the smallest unit of sampling is a single HDFS block.
Hence, for tables less than the typical block size of 128 MB, all rows will be retuned.
Percentage-based sampling offers a variable to control the seed information for blockbased tuning.
Input Pruning for Bucket Tables From a first look at the TABLESAMPLE syntax, an astute user might come to the conclusion that the following query would be equivalent to the TABLESAMPLE operation:
It is true that for most table types, sampling scans through the entire table and selects every Nth row.
However, if the columns specified in the TABLESAMPLE clause match the columns in the CLUSTERED BY clause, TABLESAMPLE queries only scan the required hash partitions of the table:
Because this table is clustered into three buckets, the following query can be used to sample only one of the buckets efficiently:
Each subquery of the union query must produce the same number of columns, and for each column, its type must match all the column types in the same position.
For example, if the second column is a FLOAT, then the second column of all the other query results must be a FLOAT.
Logically, the same results could be achieved with a single SELECT and WHERE clause.
This technique increases readability by breaking up a long complex WHERE clause into two or more UNION queries.
However, unless the source table is indexed, the query will have to make multiple passes over the same source data.
A view allows a query to be saved and treated like a table.
It is a logical construct, as it does not store data like a table.
In other words, materialized views are not currently supported by Hive.
When a query references a view, the information in its definition is combined with the rest of the query by Hive’s query planner.
Logically, you can imagine that Hive executes the view and then uses the results in the rest of the query.
Views to Reduce Query Complexity When a query becomes long or complicated, a view may be used to hide the complexity by dividing the query into smaller, more manageable pieces; similar to writing a function in a programming language or the concept of layered design in software.
Encapsulating the complexity makes it easier for end users to construct complex queries from reusable parts.
For example, consider the following query with a nested subquery:
It is common for Hive queries to have many levels of nesting.
In the following example, the nested portion of the query is turned into a view:
In this query we added a WHERE clause to the SELECT statement.
Views that Restrict Data Based on Conditions A common use case for views is restricting the result rows based on the value of one or more columns.
Some databases allow a view to be used as a security mechanism.
Rather than give the user access to the raw table with sensitive data, the user is given access to a view with a WHERE clause that restricts the data.
Hive does not currently support this feature, as the user must have access to the entire underlying raw table for the view to work.
However, the concept of a view created to limit data access can be used to protect information from the casual query:
Here is another example where a view is used to restrict data based on a WHERE clause.
In this case, we wish to provide a view on an employee table that only exposes employees from a specific department:
These datatypes are not common in traditional databases as they break first normal form.
Hive’s ability to treat a line of text as a map, rather than a fixed set of columns, combined with the view feature, allows you to define multiple logical tables over one physical table.
For example, consider the following sample data file that treats an entire row as a map rather than a list of fixed columns.
Because there is only one field per row, the FIELDS TERMINATED BY value actually has no effect.
Now we can create a view that extracts only rows with type equal to requests and get the city, state, and part into a view called orders:
This view returns the time and part column from rows where the type is response:
View Odds and Ends We said that Hive evaluates the view and then uses the results to evaluate the query.
However, as part of Hive’s query optimization, the clauses of both the query and view may be combined together into a single actual query.
Nevertheless, the conceptual view still applies when the view and a query that uses it both contain an ORDER BY clause or a LIMIT clause.
The view’s clauses are evaluated before the using query’s clauses.
While defining a view doesn’t “materialize” any data, the view is frozen to any subsequent changes to any tables and columns that the view uses.
Hence, a query using a view can fail if the referenced tables or columns no longer exist.
There are a few other clauses you can use when creating views.
As for tables, the IF NOT EXISTS and COMMENT … clauses are optional, and have the same meaning they have for tables.
A view’s name must be unique compared to all other table and view names in the same database.
You can also add a COMMENT for any or all of the new column names.
The comments are not “inherited” from the definition of the original table.
The view definition will fail if the AS SELECT clause is invalid.
Before the AS SELECT clause, you can also define TBLPROPERTIES, just like for tables.
In the example, we defined a property for the “creator” of the view.
You can also use the optional EXTERNAL keyword and LOCATION … clause, as before.
The behavior of this statement is different as of Hive v0.8.0 and previous versions of Hive.
For v0.8.0, the command creates a new table, not a new view.
A view is dropped in the same way as a table: DROP VIEW IF EXISTS shipments;
A view will be shown using SHOW TABLES (there is no SHOW VIEWS), however DROP TABLE cannot be used to delete a view.
As for tables, DESCRIBE shipments and DESCRIBE EXTENDED shipments displays the usual data for the shipment view.
With the latter, there will be a tableType value in the Detailed Table Information indicating the “table” is a VIRTUAL_VIEW.
You cannot use a view as a target of an INSERT or LOAD command.
You can only alter the metadata TBLPROPERTIES for a view:
There are no keys in the usual relational database sense, but you can build an index on columns to speed some operations.
The index data for a table is stored in another table.
Also, the feature is relatively new, so it doesn’t have a lot of options yet.
However, the indexing process is designed to be customizable with plug-in Java code, so teams can extend the feature to meet their needs.
Indexing is also a good alternative to partitioning when the logical partitions would actually be too numerous and small to be useful.
Indexing can aid in pruning some blocks from a table as input for a MapReduce job.
Not all queries can benefit from an index—the EXPLAIN syntax and Hive can be used to determine if a given query is aided by an index.
Indexes in Hive, like those in relational databases, need to be evaluated carefully.
Maintaining an index requires extra disk space and building an index has a processing cost.
The user must weigh these costs against the benefits they offer when querying a table.
Here is the table definition we used previously, for reference:
In this case, we did not partition the index table to the same level of granularity as the original table.
If we omitted the PARTITIONED BY clause completely, the index would span all partitions of the original table.
Third-party implementations can optimize certain scenarios, support specific file formats, and more.
We’ll discuss the meaning of WITH DEFERRED REBUILD in the next section.
It’s not a requirement for the index handler to save its data in a new table, but if it does, the IN TABLE ...
It supports many of the options available when creating other tables.
Currently, indexing external tables and views is supported except for data residing in S3
Bitmap Indexes Hive v0.8.0 adds a built-in bitmap index handler.
Bitmap indexes are commonly used for columns with few distinct values.
Here is our previous example rewritten to use the bitmap index handler:
At any time, the index can be built the first time or rebuilt using the ALTER INDEX statement:
If the PARTITION clause is omitted, the index is rebuilt for all partitions.
There is no built-in mechanism to trigger an automatic rebuild of the index if the underlying table or a particular partition changes.
However, if you have a workflow that updates table partitions with data, one where you might already use the ALTER TABLE ...
The rebuild is atomic in the sense that if the rebuild fails, the index is left in the previous state before the rebuild was started.
Showing an Index The following command will show all the indexes defined for any column in the indexed table:
It causes column titles to be added to the output.
You can also replace INDEX with INDEXES, as the output may list multiple indexes.
Dropping an Index Dropping an index also drops the index table, if any:
Hive won’t let you attempt to drop the index table directly with DROP TABLE.
As always, IF EXISTS is optional and serves to suppress errors if the index doesn’t exist.
If the table that was indexed is dropped, the index itself and its table is dropped.
Similarly, if a partition of the original table is dropped, the corresponding partition index is also dropped.
When the index is created, the Java code you implement for the index handler has to do some initial validation and define the schema for the index table, if used.
It also has to implement the rebuilding process where it reads the table to be indexed and writes to the index storage (e.g., the index table)
The handler must clean up any nontable storage it uses for the index when the index is dropped, relying on Hive to drop the index table, as needed.
Users have a familiar nomenclature such as tables and columns, as well as a query language that is remarkably similar to SQL dialects they have used before.
However, Hive is implemented and used in ways that are very different from conventional relational databases.
Often, users try to carry over paradigms from the relational world that are actually Hive anti-patterns.
This section highlights some Hive patterns you should use and some anti-patterns you should avoid.
Table-by-day is an anti-pattern in the database world, but due to common implementation challenges of ever-growing data sets, it is still widely used:
Hive uses expressions in the WHERE clause to select input only from the partitions needed for the query.
This query will run efficiently, and it is clean and easy on the eyes:
Over Partitioning The partitioning feature is very useful in Hive.
This is because Hive typically performs full scans over all input to satisfy a query (we’ll leave Hive’s indexing out for this discussion)
However, a design that creates too many partitions may optimize some queries, but be detrimental for other important queries:
The first drawback of having too many partitions is the large number of Hadoop files and directories that are created unnecessarily.
Each partition corresponds to a directory that usually contains multiple files.
If a given table contains thousands of partitions, it may have tens of thousands of files, possibly created every day.
If the retention of this table is multiplied over years, it will eventually exhaust the capacity of the NameNode to manage the filesystem metadata.
The NameNode must keep all metadata for the filesystem in memory.
While each file requires a small number of bytes for its metadata (approximately 150 bytes/file), the net effect is to impose an upper limit on the total number of files that can be managed in an HDFS installation.
Other filesystems, like MapR and Amazon S3 don’t have this limitation.
In the default case, each task is a new JVM instance, requiring the overhead of start up and tear down.
For small files, a separate task will be used for each file.
In pathological scenarios, the overhead of JVM start up and tear down can exceed the actual processing time! Hence, an ideal partition scheme should not result in too many partitions and their directories, and the files in each directory should be large, some multiple of the filesystem block size.
A good strategy for time-range partitioning, for example, is to determine the approximate size of your data accumulation over different granularities of time, and start with the granularity that results in “modest” growth in the number of partitions over time, while each partition contains files at least on the order of the filesystem block size or multiples thereof.
This balancing keeps the partitions large, which optimizes throughput for the general case query.
Another solution is to use two levels of partitions along different dimensions.
For example, the first partition might be by day and the second-level partition might be by geographic region, like the state:
However, since some states will probably result in lots more data than others, you could see imbalanced map tasks, as processing the larger states takes a lot longer than processing the smaller states.
Unique Keys and Normalization Relational databases typically use unique keys, indexes, and normalization to store data sets that fit into memory or mostly into memory.
Hive, however, does not have the concept of primary keys or automatic, sequence-based key generation.
Joins should be avoided in favor of denormalized data, when feasible.
The complex types, Array, Map, and Struct, help by allowing the storage of one-to-many data inside a single row.
This is not to say normalization should never be utilized, but star-schema type designs are nonoptimal.
The primary reason to avoid normalization is to minimize disk seeks, such as those typically required to navigate foreign key relations.
Denormalizing data permits it to be scanned from or written to large, contiguous sections of disk drives, which optimizes I/O performance.
However, you pay the penalty of denormalization, data duplication and the greater risk of inconsistent data.
The data model of this example breaks the traditional design rules in a few ways.
First, we are informally using name as the primary key, although we all know that names are often not unique! Ignoring that issue for now, a relational model would have a single foreign key relation from an employee record to the manager record, using the name key.
We represented this relation the other way around: each employee has an ARRAY of names of subordinates.
Second, the value for each deduction is unique to the employee, but the map keys are duplicated data, even if you substitute “flags” (say, integers) for the actual key strings.
A normal relational model would have a separate, two-column table for the deduction name (or flag) and value, with a one-to-many relationship between the employees and this deductions table.
Finally, chances are that at least some employees live at the same address, but we are duplicating the address for each employee, rather than using a one-to-one relationship to an addresses table.
It’s up to us to manage referential integrity (or deal with the consequences), and to fix the duplicates of a particular piece of data that has changed.
Hive does not give us a convenient way to UPDATE single records.
Still, when you have 10s of terabytes to many petabytes of data, optimizing speed makes these limitations worth accepting.
Making Multiple Passes over the Same Data Hive has a special syntax for producing multiple aggregations from a single pass through a source of data, rather than rescanning it for each aggregation.
This change can save considerable processing time for large input data sets.
For example, each of the following two queries creates a table from the same source table, history:
The following rewrite achieves the same thing, but using a single pass through the source history table:
Each step may produce one or more temporary tables that are only needed until the end of the next job.
However, imagine a scenario where a mistake in step’s query or raw data forces a rerun of the ETL process for several days of input.
You will likely need to run the catch-up process a day at a time in order to make sure that one job does not overwrite the temporary table before other tasks have completed.
This approach works, however computing a single day causes the record of the previous day to be removed via the INSERT OVERWRITE clause.
If two instances of this process are run at once for different days they could stomp on each others’ results.
A more robust approach is to carry the partition information all the way through the process.
Also, as a side effect, this approach allows you to compare the intermediate data day over day:
A drawback of this approach is that you will need to manage the intermediate table and delete older partitions, but these tasks are easy to automate.
Bucketing Table Data Storage Partitions offer a convenient way to segregate data and to optimize queries.
However, not all data sets lead to sensible partitioning, especially given the concerns raised earlier about appropriate sizing.
Bucketing is another technique for decomposing data sets into more manageable parts.
For example, suppose a table using the date dt as the top-level partition and the user_id as the second-level partition leads to too many small partitions.
Recall that if you use dynamic partitioning to create these partitions, by default Hive limits the maximum number of dynamic partitions that may be created to prevent the extreme case where so many partitions are created they overwhelm the filesystem’s ability to manage them and other problems.
Instead, if we bucket the weblog table and use user_id as the bucketing column, the value of this column will be hashed by a user-defined number into buckets.
Records with the same user_id will always be stored in the same bucket.
Assuming the number of users is much greater than the number of buckets, each bucket will have many users:
However, it is up to you to insert data correctly into the table! The specification in CREATE TABLE only defines metadata, but has no effect on commands that actually populate the table.
This is how to populate the table correctly, when using an INSERT … TABLE statement.
First, we set a property that forces Hive to choose the correct number of reducers corresponding to the target table’s bucketing setup.
Then the INSERT query would require a CLUSTER BY clause after the SELECT clause.
As for all table metadata, specifying bucketing doesn’t ensure that the table is properly populated.
Follow the previous example to ensure that you correctly populate bucketed tables.
The number of buckets is fixed so it does not fluctuate with data.
Adding Columns to a Table Hive allows the definition of a schema over raw data files, unlike many databases that force the conversion and importation of data following a specific format.
A benefit of this separation of concerns is the ability to adapt a table definition easily when new columns are added to the data files.
Hive offers the SerDe abstraction, which enables the extraction of data from input.
The SerDe also enables the output of data, though the output feature is not used as frequently because Hive is used primarily as a query mechanism.
A SerDe usually parses from left to right, splitting rows by specified delimiters into columns.
For example, if a row has fewer columns than expected, the missing columns will be returned as null.
If the row has more columns than expected, they will be ignored.
Adding new columns to the schema involves a single ALTER TABLE ADD COL UMN command.
This is very useful as log formats tend to only add more information to a message:
Over time a new column may be added to the underlying data.
In the following example the column user_id is added to the data.
Note that some older raw data files may not have this column:
Note that with this approach, columns cannot be added in the beginning or the middle.
Using Columnar Tables Hive typically uses row-oriented storage, however Hive also has a columnar SerDe that stores information in a hybrid row-column orientated form.
While this format can be used for any type of data there are some data sets that it is optimal for.
Repeated Data Given enough rows, fields like state and age will have the same data repeated many times.
Many Columns The table below has a large number of columns.
Queries typically only use a single column or a small set of columns.
You can reference the section “RCFile” on page 202 to see how to use this format.
The only compelling reason to not use compression is when the data produced.
MapReduce jobs tend to be I/O bound, so the extra CPU overhead is usually not a problem.
However, for workflows that are CPU intensive, such as some machine-learning algorithms, compression may actually reduce performance by stealing valuable CPU resources from more essential operations.
See Chapter 11 for more on how to use compression.
HiveQL is a declarative language where users issue declarative queries and Hive figures out how to translate them into MapReduce jobs.
Most of the time, you don’t need to understand how Hive works, freeing you to focus on the problem at hand.
While the sophisticated process of query parsing, planning, optimization, and execution is the result of many years of hard engineering work by the Hive team, most of the time you can remain oblivious to it.
However, as you become more experienced with Hive, learning about the theory behind Hive, and the low-level implementation details, will let you use Hive more effectively, especially where performance optimizations are concerned.
This chapter covers several different topics related to tuning Hive performance.
Some tuning involves adjusting numeric configuration parameters (“turning the knobs”), while other tuning steps involve enabling or disabling specific features.
Using EXPLAIN The first step to learning how Hive works (after reading this book…) is to use the EXPLAIN feature to learn how Hive translates queries into MapReduce jobs.
Now, put the EXPLAIN keyword in front of the last query to see the query plan and other information.
This shows how Hive parsed the query into tokens and literals, as part of the first step in turning the query into the ultimate result:
The indentation of the actual output was changed to fit the page.
For those not familiar with parsers and tokenizers, this can look overwhelming.
However, even if you are a novice in this area, you can study the output to get a sense for what Hive is doing with the SQL statement.
Even though our query will write its output to the console, Hive will actually write the output to a temporary file first, as shown by this part of the output:
Next, we can see references to our column name number, our table name onecol, and the sum function.
A Hive job consists of one or more stages, with dependencies between different stages.
As you might expect, more complex queries will usually involve more stages and more stages usually requires more processing time to complete.
A stage could be a MapReduce job, a sampling stage, a merge stage, a limit stage, or a stage for some other task Hive needs to do.
Some stages will be short, like those that move files around.
Other stages may also finish quickly if they have little data to process, even though they require a map or reduce task:
Stage-1 is the bulk of the processing for this job and happens via a MapReduce job.
A TableScan takes the input of the table and produces a single output column number.
The Group By Operator applies the sum(number) and produces an output column _col0 (a synthesized name for an anonymous result)
All this is happening on the map side of the job, under the Map Operator Tree:
On the reduce side, under the Reduce Operator Tree, we see the same Group by Opera tor but this time it is applying sum on _col0
Understanding the intricate details of how Hive parses and plans every query is not useful all of the time.
However, it is a nice to have for analyzing complex or poorly performing queries, especially as we try various tuning steps.
We can observe what effect these changes have at the “logical” level, in tandem with performance measurements.
In an effort to “go green,” we won’t show the entire output, but we will show you the Reduce Operator Tree to demonstrate the different output:
We encourage you to compare the two outputs for the Reduce Operator Tree.
Limit Tuning The LIMIT clause is commonly used, often by people working with the CLI.
However, in many cases a LIMIT clause still executes the entire query, then only returns a handful.
Because this behavior is generally wasteful, it should be avoided when possible.
Hive has a configuration property to enable sampling of source data for use with LIMIT:
A drawback of this feature is the risk that useful input data will never get processed.
For example, any query that requires a reduce step, such as most JOIN and GROUP BY operations, most calls to aggregate functions, etc., will have very different results.
Perhaps this difference is okay in many cases, but it’s important to understand.
If all but one table is small enough, typically to fit in memory, then Hive can perform a map-side join, eliminating the need for reduce tasks and even some map tasks.
Sometimes even tables that do not fit in memory are good candidates because removing the reduce phase outweighs the cost of bringing semi-large tables into each map tasks.
Local Mode Many Hadoop jobs need the full scalability benefits of Hadoop to process large data sets.
However, there are times when the input to Hive is very small.
In many of these cases, Hive can leverage the lighter weight of the local mode to perform all the tasks for the job on a single machine and sometimes in the same process.
The reduction in execution times can be dramatic for small data sets.
You can explicitly enable local mode temporarily, as in this example:
Parallel Execution Hive converts a query into one or more stages.
Stages could be a MapReduce stage, a sampling stage, a merge stage, a limit stage, or other possible tasks Hive needs to do.
By default, Hive executes these stages one at a time.
However, a particular job may consist of some stages that are not dependent on each other and could be executed in parallel, possibly allowing the overall job to complete more quickly.
However, if more stages are run simultaneously, the job may complete much faster.
If a job is running more stages in parallel, it will increase its cluster utilization:
Strict Mode Strict mode is a setting in Hive that prevents users from issuing queries that could have unintended and undesirable effects.
First, queries on partitioned tables are not permitted unless they include a partition filter in the WHERE clause, limiting their scope.
In other words, you’re prevented from queries that will scan all partitions.
The rationale for this limitation is that partitioned tables often hold very large data sets that may be growing rapidly.
An unrestricted partition could consume unacceptably large resources over such a large table:
The following enhancement adds a partition filter—the table partitions—to the WHERE clause:
The second type of restricted query are those with ORDER BY clauses, but no LIMIT clause.
Because ORDER BY sends all results to a single reducer to perform the ordering, forcing the user to specify a LIMIT clause prevents the reducer from executing for an extended period of time:
The third and final type of query prevented is a Cartesian product.
Users coming from the relational database world may expect that queries that perform a JOIN not with an ON clause but with a WHERE clause will have the query optimized by the query planner, effectively converting the WHERE clause into an ON clause.
Unfortunately, Hive does not perform this optimization, so a runaway query will occur if the tables are large:
Here is a properly constructed query with JOIN and ON clauses:
Tuning the Number of Mappers and Reducers Hive is able to parallelize queries by breaking the query into one or more MapReduce jobs.
Each of which might have multiple mapper and reducer tasks, at least some of which can run in parallel.
Determining the optimal number of mappers and reducers depends on many variables, such as the size of the input and the operation being performed on the data.
Having too many mapper or reducer tasks causes excessive overhead in starting, scheduling, and running the job, while too few tasks means the inherent parallelism of the cluster is underutilized.
When running a Hive query that has a reduce phase, the CLI prints information about how the number of reducers can be tuned.
Let’s see an example that uses a GROUP BY query, because they always require a reduce phase.
In contrast, many other queries are converted into map-only jobs:
Hive is determining the number of reducers from the input size.
This can be confirmed using the dfs -count command, which works something like the Linux du -s command; it computes a total size for all the data under a given directory:
We’ve reformatted the output and elided some details for space.
Changing this value to 750 MB causes Hive to estimate four reducers for this job:
However, there are cases where a query’s map phase will create significantly more data than the input size.
In the case of excessive map phase data, the input size of the default might be selecting too few reducers.
Likewise the map function might filter a large portion of the data from the data set and then fewer reducers may be justified.
A quick way to experiment is by setting the number of reducers to a fixed size, rather than allowing Hive to calculate the value.
If you remember, the Hive default estimate is three reducers.
Remember that benchmarking like this is complicated by external factors such as other users running jobs simultaneously.
Hadoop has a few seconds overhead to start up and schedule map and reduce tasks.
When executing performance tests, it’s important to keep these factors in mind, especially if the jobs are small.
A Hadoop cluster has a fixed number of map and reduce “slots” to allocate to tasks.
One large job could reserve all of the slots and block other jobs from starting.
A suggested formula is to set the value to the result of this calculation:
The 1.5 multiplier is a fudge factor to prevent underutilization of the cluster.
The default configuration of Hadoop will typically launch map or reduce tasks in a forked JVM.
The JVM start-up may create significant overhead, especially when launching jobs with hundreds or thousands of tasks.
Reuse allows a JVM instance to be reused up to N times for the same job.
A drawback of this feature is that JVM reuse will keep reserved task slots open until the job completes, in case they are needed for reuse.
If an “unbalanced” job has some reduce tasks that run considerably longer than the others, the reserved slots will sit idle, unavailable for other jobs, until the last task completes.
Indexes Indexes may be used to accelerate the calculation speed of a GROUP BY query.
The main use case for bitmap indexes is when there are comparatively few values for a given column.
Dynamic Partition Tuning As explained in “Dynamic Partition Inserts” on page 74, dynamic partition INSERT statements enable a succinct SELECT statement to create many new partitions for insertion into a partitioned table.
This is a very powerful feature, however if the number of partitions is high, a large number of output handles must be created on the system.
This is a somewhat uncommon use case for Hadoop, which typically creates a few files at once and streams large amounts of data to them.
Out of the box, Hive is configured to prevent dynamic partition inserts from creating more than 1,000 or so partitions.
While it can be bad for a table to have too many partitions, it is generally better to tune this setting to the larger value and allow these queries to work.
When strict mode is on, at least one partition has to be static, as demonstrated in “Dynamic Partition Inserts” on page 74:
Then, increase the other relevant properties to allow queries that will create a large number of dynamic partitions, for example:
Another setting controls how many files a DataNode will allow to be open at once.
The value affects the number of maximum threads and resources, so setting it to a very high number is not recommended.
Note also that in Hadoop v0.20.2, changing this variable requires restarting the DataNode to take effect:
Speculative Execution Speculative execution is a feature of Hadoop that launches a certain number of duplicate tasks.
While this consumes more resources computing duplicate copies of data that may be discarded, the goal of this feature is to improve overall job progress by getting individual task results faster, and detecting then black-listing slow-running TaskTrackers.
However, Hive provides its own variable to control reduce-side speculative execution:
It is hard to give a concrete recommendation about tuning these speculative execution variables.
If you are very sensitive to deviations in runtime, you may wish to turn these features on.
However, if you have long-running map or reduce tasks due to large amounts of input, the waste could be significant.
Single MapReduce MultiGROUP BY Another special optimization attempts to combine multiple GROUP BY operations in a query into a single MapReduce job.
For this optimization to work, a common set of GROUP BY keys is required:
Virtual Columns Hive provides two virtual columns: one for the input filename for split and the other for the block offset in the file.
These are helpful when diagnosing queries where Hive is producing unexpected or null results.
By projecting these “columns,” you can see which file and row is causing problems:
We wrapped the long output and put a blank line between the two output rows.
A third virtual column provides the row offset of the file.
One of Hive’s unique features is that Hive does not force data to be converted to a specific format.
Hive leverages Hadoop’s InputFormat APIs to read data from a variety of sources, such as text files, sequence files, or even custom formats.
Likewise, the OutputFormat API is used to write data to various formats.
While Hadoop offers linear scalability in file storage for uncompressed data, storing data in compressed form has many benefits.
Compression typically saves significant disk storage; for example, text-based files may compress 40% or more.
This may seem counterintuitive because compressing and decompressing data incurs extra CPU overhead, however, the I/O savings resulting from moving fewer bytes into memory can result in a net performance gain.
Hadoop jobs tend to be I/O bound, rather than CPU bound.
However, if your jobs are CPU bound, then compression will probably lower your performance.
The only way to really know is to experiment with different options and measure the results.
Determining Installed Codecs Based on your Hadoop version, different codecs will be available to you.
The set feature in Hive can be used to display the value of hiveconf or Hadoop configuration values.
Choosing a Compression Codec Using compression has the advantage of minimizing the disk space required for files and the overhead of disk and network I/O.
Therefore, compression is best used for I/O-bound jobs, where there is extra CPU capacity, or when disk space is at a premium.
All recent versions of Hadoop have built-in support for the GZip and BZip2 compression schemes, including native Linux libraries that accelerate compression and decompression for these formats.
BZip2 creates the smallest compressed output, but with the highest CPU overhead.
GZip is next in terms of compressed size versus speed.
Hence, if disk space utilization and I/O overhead are concerns, both are attractive choices.
They are good choices if disk space and I/O overhead are less important than rapid decompression of frequently read data.
Another important consideration is whether or not the compression format is splittable.
MapReduce wants to split very large input files into splits (often one split per filesystem block, i.e., a multiple of 64 MB), where each split is sent to a separate map process.
This can only work if Hadoop knows the record boundaries in the file.
In text files, each line is a record, but these boundaries are obscured by GZip and Snappy.
However, BZip2 and LZO provide block-level compression, where each block has complete records, so Hadoop can split these files on block boundaries.
The desire for splittable files doesn’t rule out GZip and Snappy.
When you create your data files, you could partition them so that they are approximately the desired size.
Typically the number of output files is equal to the number of reducers.
If you are using N reducers you typically get N output files.
Be careful, if you have a large nonsplittable file, a single task will have to read the entire file beginning to end.
There’s much more we could say about compression, but instead we’ll refer you to Hadoop: The Definitive Guide by Tom White (O’Reilly) for more details, and we’ll focus now on how to tell Hive what format you’re using.
From Hive’s point of view, there are two aspects to the file format.
One aspect is how the file is delimited into rows (records)
Text files use \n (linefeed) as the default row delimiter.
When you aren’t using the default text file format, you tell Hive the name of.
Actually, you will specify the names of Java classes that implement these formats.
The InputFormat knows how to read splits and partition them into records, and the OutputFormat knows how to write these splits back to files or console output.
The second aspect is how records are partitioned into fields (or columns)
Hive uses ^A by default to separate fields in text files.
This time you will specify a single Java class that performs both jobs.
All this information is specified as part of the table definition when you create the table.
After creation, you query the table as you normally would, agnostic to the underlying format.
Hence, if you’re a user of Hive, but not a Java developer, don’t worry about the Java aspects.
The developers on your team will help you specify this information when needed, after which you’ll work as you normally do.
Enabling Intermediate Compression Intermediate compression shrinks the data shuffled between the map and reduce tasks for a job.
For intermediate compression, choosing a codec that has lower CPU cost is typically more important than choosing a codec that results in the most compression.
SnappyCodec is a good choice for intermediate compression because it combines good compression performance with low CPU cost:
Final Output Compression When Hive writes output to a table, that content can also be compressed.
You may wish to leave this value set to false in the global configuration file, so that the default output is uncompressed clear text.
Users can turn on final compression by setting the property to true on a query-by-query basis or in their scripts:
GZip compression is a good choice for output compression because it typically reduces the size of files significantly, but remember that GZipped files aren’t splittable by subsequent MapReduce jobs:
Sequence Files Compressing files results in space savings but one of the downsides of storing raw compressed files in Hadoop is that often these files are not splittable.
Splittable files can be broken up and processed in parts by multiple mappers in parallel.
Most compressed files are not splittable because you can only start reading from the beginning.
The sequence file format supported by Hadoop breaks a file into blocks and then optionally compresses the blocks in a splittable way.
Sequence files have three different compression options: NONE, RECORD, and BLOCK.
However, BLOCK compression is usually more efficient and it still provides the desired splittability.
Like many other compression properties, this one is not Hive-specific.
It can be defined in Hadoop’s mapred-site.xml file, in Hive’s hivesite.xml, or as needed in scripts or before individual queries:
Let’s use these properties in some examples and show what they produce.
Remember that variables set by the CLI persist across the rest of the queries in the session, so between examples you should revert the settings or simply restart the Hive session:
This won’t affect the final output, however the job counters will show less physical data transferred for the job, since the shuffle sort data was compressed:
As expected, intermediate compression did not affect the final output, which remains uncompressed:
We can also chose an intermediate compression codec other then the default codec.
In this case we chose GZIP, although Snappy is normally a better option.
We can also see the output file is named .deflate.
Trying to cat the file is not suggested, as you get binary output.
This ability to seamlessly work with compressed files is not Hive-specific; Hadoop’s TextInputFormat is at work here.
While the name is confusing in this case, TextInput Format understands file extensions such as .deflate or .gz and decompresses these files on the fly.
Hive is unaware if the underlying files are uncompressed or compressed using any of the supported compression schemes.
Let’s change the codec used by output compression to see the results (another line wrap for space):
As you can see, the output folder now contains zero or more .gz files.
Hive has a quick hack to execute local commands like zcat from inside the Hive shell.
The ! tells Hive to fork and run the external command and block until the system returns a result.
Using output compression like this results in binary compressed files that are small and, as a result, operations on them are very fast.
However, recall that the number of output files is a side effect of how many mappers or reducers processed the data.
In the worst case scenario, you can end up with one large binary file in a directory that is not splittable.
This means that subsequent steps that have to read this data cannot work in parallel.
The answer to this problem is to use sequence files:
But it is a nice exercise to see the header.
To confirm the results are what was intended (output wrapped):
Because of the meta-information embedded in the sequence file and in the Hive metastore, Hive can query the table without any specific settings.
Hadoop also offers the dfs -text command to strip the header and compression away from sequence files and return the raw result:
Finally, let’s use intermediate and output compression at the same time and set different compression codecs for each while saving the final output to sequence files! These settings are commonly done for production environments where data sets are large and such settings improve performance:
Archive Partition Hadoop has a format for storage known as HAR, which stands for Hadoop ARchive.
A HAR file is like a TAR file that lives in the HDFS filesystem as a single file.
In some use cases, older directories and files are less commonly accessed than newer files.
If a particular partition contains thousands of files it will require significant overhead to manage it in the HDFS.
By archiving the partition it is stored as a single, large file, but it can still be accessed by hive.
The trade-off is that HAR files will be less efficient to query.
Also, HAR files are not compressed, so they don’t save any space.
In the following example, we’ll use Hive’s own documentation as data.
First, create a partitioned table and load it with the text data from the Hive package:
Some versions of Hadoop, such as Hadoop v0.20.2, will require the JAR containing the Hadoop archive tools to be placed on the Hive auxlib:
Take a look at the underlying structure of the table, before we archive it.
Note the location of the table’s data partition, since it’s a managed, partitioned table:
We reformatted the output slightly so it would fit, and used ...
The underlying table has gone from two files to one Hadoop archive (HAR file):
Compression: Wrapping Up Hive’s ability to read and write different types of compressed files is a big performance win as it saves disk space and processing overhead.
This flexibility also aids in integration with other tools, as Hive can query many native file types without the need to write custom “adapters” in Java.
This chapter explores working with the Hive source code itself, including the new Plugin Developer Kit introduced in Hive v0.8.0
These files do not need to be present inside the Hive installation because the default properties come built inside the Hive JARs.
In fact, the actual files in the conf directory have the .template extension, so they are ignored by default.
To use either of them, copy it with a name that removes the .template extension and edit it to taste:
It is also possible to change the logging configuration of Hive temporarily without copying and editing the Log4J files.
The hiveconf switch can be specified on start-up with definitions of any properties in the log4.properties file.
For example, here we set the default logger to the DEBUG level and send output to the console appender:
Connecting a Java Debugger to Hive When enabling more verbose output does not help find the solution to the problem you are troubleshooting, attaching a Java debugger will give you the ability to step through the Hive code and hopefully find the problem.
Remote debugging is a feature of Java that is manually enabled by setting specific command-line properties for the JVM.
The Hive shell script provides a switch and help screen that makes it easy to set these properties (some output truncated for space):
Default: y port=<port_number>  Port on which main JVM listens for debug connection.
Building Hive from Source Running Apache releases is usually a good idea, however you may wish to use features that are not part of a release, or have an internal branch with nonpublic customizations.
Hive also contains components such as Thrift-generated classes that are not built by default.
Hive does have traditional JUnit tests, however the majority of the testing happens by running queries saved in .q files, then comparing the results with a previous run saved in Hive source.1 There are multiple.
That is, they are more like feature or acceptance tests.
An example of a positive test is a well-formed query.
An example of a negative test is a query that is malformed or tries doing something that is not allowed by HiveQL:
The first thing you should know is that a src table is the first table automatically created in the test process.
It is a table with two columns, key and value, where key is an INT and value is a STRING.
Because Hive does not currently have the ability to do a SELECT without a FROM clause, selecting a single row from the src table is the trick used to test out functions that don’t really need to retrieve table data; inputs can be “hard-coded” instead.
As you can see in the following example queries, the src table is never referenced in the SELECT clauses:
The result file is large and printing the complete results inline will kill too many trees.
This command invokes a positive and a negative test case for the Hive client:
They are now deprecated in favor of clientpositive and clientnegative.
You can also run multiple tests in one ant invocation to save time (the last -Dqfile=… string was wrapped for space; it’s all one string):
Execution Hooks PreHooks and PostHooks are utilities that allow user code to hook into parts of Hive and execute custom code.
Hive’s testing framework uses hooks to echo commands that produce no output, so that the results show up inside tests:
The following steps allow you to use Eclipse to work with the Hive source code:
Hive in a Maven Project You can set up Hive as a dependency in Maven builds.
Here is the top-level dependency definition for Hive v0.9.0, not including the tree of transitive dependencies, which is quite deep:
Unit Testing in Hive with hive_test The optimal way to write applications to work with Hive is to access Hive with Thrift through the HiveService.
However, the Thrift service was traditionally difficult to bring up in an embedded environment due to Hive’s many JAR dependencies and the metastore component.
Hive_test fetches all the Hive dependencies from Maven, sets up the metastore and Thrift service locally, and provides test classes to make unit testing easier.
Also, because it is very lightweight and unit tests run quickly, this is in contrast to the elaborate test targets inside Hive, which have to rebuild the entire project to execute any unit test.
Hive_test is ideal for testing code such as UDFs, input formats, SerDes, or any component that only adds a pluggable feature for the language.
It is not useful for internal Hive development because all the Hive components are pulled from Maven and are external to the project.
In your Maven project, create a pom.xml and include hive_test as a dependency, as shown here:
Unlike a normal hive-site.xml, this version should not save any data to a permanent place.
This is because unit tests are not supposed to create or preserve any permanent state.
HiveTestService set up the environment, cleared out the warehouse directory, and launched a metastore and HiveService in-process.
Its intent is to allow developers to build and test plug-ins without the Hive source.
The PDK is relatively new and has some subtle bugs of its own that can make it difficult to use.
User-Defined Functions (UDFs) are a powerful feature that allow users to extend HiveQL.
As we’ll see, you implement them in Java and once you add them to your session (interactive or driven by a script), they work just like built-in functions, even the online help.
Hive has several types of user-defined functions, each of which performs a particular “class” of transformations on input data.
In an ETL workload, a process might have several processing steps.
The Hive language has multiple ways to pipeline the output from one step to the next and produce multiple outputs during a single query.
Users also have the ability to create their own functions for custom processing.
Without this feature a process might have to include a custom MapReduce step or move the data into another system to apply the changes.
Interconnecting systems add complexity and increase the chance of misconfigurations or other errors.
Moving data between systems is time consuming when dealing with gigabyteor terabyte-sized data sets.
In contrast, UDFs run in the same processes as the tasks for your Hive queries, so they work efficiently and eliminate the complexity of integration with other systems.
This chapter covers best practices associated with creating and using UDFs.
Discovering and Describing Functions Before writing custom UDFs, let’s familiarize ourselves with the ones that are already part of Hive.
Note that it’s common in the Hive community to use “UDF” to refer to any function, user-defined or built-in.
The SHOW FUNCTIONS command lists the functions currently loaded in the Hive session, both built-in and any user-defined functions that have been loaded using the techniques we will discuss shortly:
Functions may also contain extended documentation that can be accessed by adding the EXTENDED keyword:
Calling Functions To use a function, simply call it by name in a query, passing in any required arguments.
Some functions take a specific number of arguments and argument types, while other functions accept a variable number of arguments with variable types.
Just like keywords, the case of function names is ignored:
Standard Functions The term user-defined function (UDF) is also used in a narrower sense to refer to any function that takes a row argument or one or more columns from a row and returns a single value.
Examples include many of the mathematical functions, like round() and floor(), for converting DOUBLES to BIGINTS, and abs(), for taking the absolute value of a number.
Other examples include string manipulation functions, like ucase(), which converts the string to upper case; reverse(), which reverses a string; and concat(), which joins multiple input strings into one output string.
Note that these UDFs can return a complex object, such as an array, map, or struct.
Aggregate Functions Another type of function is an aggregate function.
All aggregate functions, user-defined and built-in, are referred to generically as user-defined aggregate functions (UDAFs)
An aggregate function takes one or more columns from zero to many rows and returns a single result.
Examples include the math functions: sum(), which returns a sum of all.
We saw this example in “GROUP BY Clauses” on page 97:
Table Generating Functions A third type of function supported by Hive is a table generating function.
As for the other function kinds, all table generating functions, user-defined and built-in, are often referred to generically as user-defined table generating functions (UDTFs)
Table generating functions take zero or more inputs and produce multiple columns or rows of output.
The array function takes a list of arguments and returns the list as a single array type.
The explode() function is a UDTF that takes an array of input and iterates through the list, returning each element from the list in a separate row.
However, Hive only allows table generating functions to be used in limited ways.
For example, we can’t project out any other columns from the table, a significant limitation.
Here is a query we would like to write with the employees table we have used before.
However, Hive offers a LATERAL VIEW feature to allow this kind of query:
Note that there are no output rows for employees who aren’t managers (i.e., who have no subordinates), namely Bill King and Todd Jones.
The LATERAL VIEW wraps the output of the explode call.
A view alias and column alias are required, subView and sub, respectively, in this case.
Imagine we have a table with each user’s birth date stored as a column of a table.
With that information, we would like to determine the user’s Zodiac sign.
This process can be implemented with a standard function (UDF in the most restrictive sense)
Specifically, we assume we have a discrete input either as a date formatted as a string or as a month and a day.
The function must return a discrete single column of output.
The input for the function will be a date and the output will be a string representing the user’s Zodiac sign.
Here is a Java implementation of the UDF we need:
To write a UDF, start by extending the UDF class and implements and the evaluate() function.
During query processing, an instance of the class is instantiated for each usage of the function in a query.
Hive will pick the method that matches in a similar way to Java method overloading.
This is how function documentation is defined and you should use these annotations to document your own UDFs.
When a user invokes DESCRIBE FUNCTION ..., the _FUNC_ strings will be replaced with the function name the user picks when defining a “temporary” function, as discussed below.
The arguments and return types of the UDF’s evaluate() function can only be types that Hive can serialize.
For example, if you are working with whole numbers, a UDF can take as input a primitive int, an Inte ger wrapper object, or an IntWritable, which is the Hadoop wrapper for integers.
You do not have to worry specifically about what the caller is sending because Hive will convert the types for you if they do not match.
Remember that null is valid for any type in Hive, but in Java primitives are not objects and cannot be null.
To use the UDF inside Hive, compile the Java code and package the UDF bytecode class file into a JAR file.
Then, in your Hive session, add the JAR to the classpath and use a CREATE FUNCTION statement to define a function that uses the Java class:
Note that quotes are not required around the JAR file path and currently it needs to be a full path to the file on a local filesystem.
Hive not only adds this JAR to the classpath, it puts the JAR file in the distributed cache so it’s available around the cluster.
Now the Zodiac UDF can be used like any other function.
Functions declared will only be available in the current session.
You will have to add the JAR and create the function in each session.
However, if you use the same JAR files and functions frequently, you can add these statements to your $HOME/.hiverc file:
To recap, our UDF allows us to do custom transformations inside the Hive language.
Hive can now convert the user’s birthday to the corresponding Zodiac sign while it is doing any other aggregations and transformations.
If we’re finished with the function, we can drop it:
GenericUDF is a more complex abstraction, but it offers support for better null handling and makes it possible to handle some types of operations programmatically that a standard UDF cannot support.
An example of a generic UDF is the Hive CASE ...
We will demonstrate how to use the GenericUDF class to write a user-defined function, called nvl(), which returns a default value if null is passed in.
If the first argument is null, the second argument is returned.
The GenericUDF framework is a good fit for this problem.
A standard UDF could be used as a solution but it would be cumbersome because it requires overloading the evaluate method to handle many different input types.
GenericUDF will detect the type of input to the function programmatically and provide an appropriate response.
We begin with the usual laundry list of import statements:
Now the class extends GenericUDF, a requirement to exploit the generic handling we want.
The initialize() method is called and passed an ObjectInspector for each argument.
The goal of this method is to determine the return type from the arguments.
The user can also throw an Exception to signal that bad types are being sent to the method.
The returnOIResolver is a built-in class that determines the return type by finding the type of non-null variables and using that type:
The evaluate method has access to the values passed to the method stored in an array of DeferredObject values.
The returnOIResolver created in the initialize method is used to get values from the DeferredObjects.
In this case, the function returns the first non-null value:
To test the  generic nature of the UDF, it is called several times, each time passing values of different types, as shown the following example:
Permanent Functions Until this point we have bundled our code into JAR files, then used ADD JAR and CREATE TEMPORARY FUNCTION to make use of them.
Your function may also be added permanently to Hive, however this requires a small modification to a Hive Java file and then rebuilding Hive.
Then you rebuild Hive following the instructions that come with the source distribution.
Here is an example change to FunctionRegistry where the new nvl() function is added to Hive’s list of built-in functions:
User-Defined Aggregate Functions Users are able to define aggregate functions, too.
Depending on the transformation the UDAF performs, the types returned by each phase could be different.
For example, a sum() UDAF could accept primitive integer input, create integer PARTIAL data, and produce a final integer result.
However, an aggregate like median() could take primitive integer input, have an intermediate list of integers as PARTIAL data, and then produce a final integer as the result.
Aggregations execute inside the context of a map or reduce task, which is a Java process with memory limitations.
Therefore, storing large structures inside an aggregate may exceed available heap space.
The min() UDAF only requires a single element be stored in memory for comparison.
The collectset() UDAF uses a set internally to deduplicate data in order to limit memory usage.
It is important to keep memory usage in mind when writing a UDAF.
Below is an example MySQL query that shows how to use its version of this function:
We can do the same transformation in Hive without the need for additional grammar in the language.
First, we need an aggregate function that builds a list of all input to the aggregate.
Hive already has a UDAF called collect_set that adds all input into a java.util.Set collection.
Sets automatically de-duplicate entries on insertion, which is undesirable for GROUP CONCAT.
To build collect, we will take the code in col lect_set and replace instances of Set with instances of ArrayList.
The result of the aggregate will be a single array of all values.
It is important to remember that the computation of your aggregation must be arbitrarily divisible over the data.
Think of it as writing a divide-and-conquer algorithm where the partitioning of the data is completely out of your control and handled by Hive.
More formally, given any subset of the input rows, you should be able to compute a partial result, and also be able to merge any pair of partial results into another partial result.
All the input to the aggregation must be primitive types.
Table 13-1 describes the methods that are part of the base class.
Here, persistable means the return value can only be built up in terms of Java primitives, arrays, primitive wrappers (e.g., Double), Hadoop Writables, Lists, and Maps.
Do NOT use your own classes (even if they implement java.io .Serializable)
In the init method, the object inspectors for the result type are set, after determining what mode the evaluator is in.
The iterate() and terminatePartial() methods are used on the map side, while ter minate() and merge() are used on the reduce side to produce the final result.
You may have noticed that Hive tends to avoid allocating objects with new whenever possible.
Hadoop and Hive use this pattern to create fewer temporary objects and thus less work for the JVM’s Garbage Collec tion algorithms.
Keep this in mind when writing UDFs, because references are typically reused.
The remaining arguments can be string types or arrays of strings.
The returned result contains the argument joined together by the delimiter.
Hence, we have converted the array into a single commaseparated string:
User-Defined Table Generating Functions While UDFs can be used be return arrays or structures, they cannot return multiple columns or multiple rows.
User-Defined Table Generating Functions, or UDTFs, address this need by providing a programmatic interface to return multiple columns and even multiple rows.
UDTFs that Produce Multiple Rows We have already used the explode method in several examples.
Explode takes an array as input and outputs one row for each element in the array.
An alternative way to do this would have the UDTF generate the rows based on some input.
We will demonstrate this with a UDTF that works like a for loop.
The function receives user inputs of the start and stop values and then outputs N rows:
We declare three integer variables for the start, end, and increment.
The forwardObj array will be used to return result rows:
Because the arguments to this function are constant, the value can be determined in the initialize method.
Nonconstant values are typically not available until the evaluate method.
The third argument for increment is optional, as it defaults to 1:
This function returns only a single column and its type is always an integer.
We need to give it a name, but the user can always override this later:
This is because UDTF can forward zero or more rows, unlike a UDF, which has a single return.
In this case the call to the forward method is nested inside a for loop, which causes it to forward a row for each iteration:
UDTFs that Produce a Single Row with Multiple Columns An example of a UDTF that returns multiple columns but only one row is the parse_url_tuple function, which is a built-in Hive function.
It takes as input a parameter that is a URL and one or more constants that specify the parts of the URL the user wants returned:
The benefit of this type of UDFT is the URL only needs to be parsed once, then returns multiple columns—a clear performance win.
The alternative, using UDFs, involves writing several UDFs to extract specific parts of the URL.
Using UDFs requires writing more code as well as more processing time because the URL is parsed multiple times.
UDTFs that Simulate Complex Types A UDTF can be used as a technique for adding more complex types to Hive.
For example, a complex type can be serialized as an encoded string and a UDTF will deserialize the complex type when needed.
Hive cannot work with this datatype directly, however a Book could be encoded to and decoded from a string format:
Imagine we have a flat text file with books in this format.
For now lets assume we could not use a delimited SerDe to split on | and ,:
This HiveQL works correctly, however it could be made easier for the end user.
For example, writing this type of query may require consulting documentation regarding which fields and types are used, remembering casting conversion rules, and so forth.
By contrast, a UDTF makes this HiveQL simpler and more readable.
The function parse_book() allows Hive to return multiple columns of different types representing the fields of a book:
The function will return three properties and ISBN as an integer, a title as a string, and authors as an array of strings.
Notice that we can return nested types with all UDFs, for example we can return an array of array of strings:
However, each element in the object array will be bound to a specific variable:
We have followed the call to the book UDTF with AS, which allows the result columns to be named by the user.
They can then be used in other parts of the query without having to parse information from the book again:
Accessing the Distributed Cache from a UDF UDFs may access files inside the distributed cache, the local filesystem, or even the distributed filesystem.
This access should be used cautiously as the overhead is significant.
A common usage of Hive is the analyzing of web logs.
A popular operation is determining the geolocation of web traffic based on the IP address.
Maxmind makes a GeoIP database available and a Java API to search this database.
By wrapping a UDF around this API, location information may be looked up about an IP address from within a Hive query.
This is ideal for showing the functionality of accessing a distributed cache file from a UDF.
Finally, the temporary function must be defined as the final step before performing queries:
The two examples returned include an IP address in the United States and a private IP address that has no fixed address.
The first call to the UDF (which triggers the first call to the evaluate Java function in the implementation) will instantiate a LookupService object that uses the file located in the distributed cache.
The lookup service is saved in a reference so it only needs to be initialized once in the lifetime of a map or reduce task that initializes it.
An if statement in evaluate determines which data the method should return.
Annotations for Use with Functions In this chapter we mentioned the Description annotation and how it is used to provide documentation for Hive methods at runtime.
Other annotations exist for UDFs that can make functions easier to use and even increase the performance of some Hive queries:
Deterministic By default, deterministic is automatically turned on for most queries because they are inherently deterministic by nature.
If a UDF is not deterministic, it is not included in the partition pruner.
An example of a nondeterministic query using rand() is the following:
If rand() were deterministic, the result would only be calculated a single time in the computation state.
Because a query with rand() is nondeterministic, the result of rand() is recomputed for each row.
Stateful Almost all the UDFs are stateful by default; a UDF that is not stateful is rand() because it returns a different value for each invocation.
The Stateful annotation may be used under the following conditions:
If stateful is set to true, the UDF should also be treated as nondeterministic (even if the deterministic annotation explicitly returns true)
DistinctLike Used for cases where the function behaves like DISTINCT even when applied to a nondistinct column of values.
Examples include min and max functions that return a distinct value even though the underlying numeric data can have repeating values.
Macros Macros provide the ability to define functions in HiveQL that call other functions and operators.
When appropriate for the particular situation, macros are a convenient alternative to writing UDFs in Java or using Hive streaming, because they require no external code or scripts.
To define a macro, use the CREATE TEMPORARY MACRO syntax.
Here is an example that creates a SIGMOID function calculator:
These components are all Java components, but Hive hides the complexity of implementing and using these components by letting the user work with SQL abstractions, rather than Java code.
During a streaming job, the Hadoop Streaming API opens an I/O pipe to an external process.
Data is then passed to the process, which operates on the data it reads from the standard input and writes the results out through the standard output, and back to the Streaming API job.
While Hive does not leverage the Hadoop streaming API directly, it works in a very similar way.
This pipeline computing model is familiar to users of Unix operating systems and their descendants, like Linux and Mac OS X.
Streaming is usually less efficient than coding the comparable UDFs or InputFormat objects.
Serializing and deserializing data to pass it in and out of the pipe is relatively inefficient.
It is also harder to debug the whole program in a unified manner.
However, it is useful for fast prototyping and for leveraging existing code that is not written in Java.
For Hive users who don’t want to write Java code, it can be a very effective approach.
Hive provides several clauses to use streaming: MAP(), REDUCE(), and TRANSFORM()
An important point to note is that MAP() does not actually force streaming during the map phase nor does reduce force streaming to happen in the reduce phase.
For this reason, the functionally equivalent yet more generic TRANSFORM() clause is suggested to avoid misleading the reader of the query.
Identity Transformation The most basic streaming job is an identity operation.
The /bin/cat command echoes the data sent to it and meets the requirements.
In this example, /bin/cat is assumed to be installed on all TaskTracker nodes.
Any Linux system should have it! Later, we will show how Hive can “ship” applications with the job when they aren’t already installed around the cluster:
Changing Types The return columns from TRANSFORM are typed as strings, by default.
There is an alternative syntax that casts the results to different types.
Projecting Transformation The cut command can be used with streaming to extract or project specific fields.
Note that the query attempts to read more columns than are actually returned from the external process, so newB is always NULL.
By default, TRANSFORM assumes two columns but there can be any number of them:
Manipulative Transformations The /bin/sed program (or /usr/bin/sed on Mac OS X systems) is a stream editor.
It takes the input stream, edits it according to the user’s specification, and then writes the results to the output stream.
Using the Distributed Cache All of the streaming examples thus far have used applications such as cat and sed that are core parts of Unix operating systems and their derivatives.
When a query requires files that are not already installed on every TaskTracker, users can use the distributed cache to transmit data or program files across the cluster that will be cleaned up when the job is complete.
This is helpful, because installing (and sometimes removing) lots of little components across large clusters can be a burden.
Also, the cache keeps one job’s cached files separate from those files belonging to other jobs.
The following example is a bash shell script that converts degrees in Celsius to degrees in Fahrenheit:
Then enter another number and the program returns another result.
You can continue entering numbers or use Control-D to end the input.
Hive’s ADD FILE feature adds files to the distributed cache.
The added file is put in the current working directory of each task.
This allows the transform task to use the script without needing to know where to find it:
Producing Multiple Rows from a Single Row The examples shown thus far have taken one row of input and produced one row of output.
Streaming can be used to produce multiple rows of output for each input row.
This functionality produces output similar to the EXPLODE() UDF and the LATERAL VIEW syntax1
This will allow the rows to be processed by familiar HiveQL operators:
The entire table is defined as a single string column.
The row format does not need to be configured because the streaming script will do all the tokenization of the fields:
Calculating Aggregates with Streaming Streaming can also be used to do aggregating operations like Hive’s built-in SUM function.
This is possible because streaming processes can return zero or more rows of output for every given input.
To accomplish aggregation in an external application, declare an accumulator before the loop that reads from the input stream and output the sum after the completion of the input:
Add the streaming program to the distributed cache and use it in a TRANSFORM query.
The process returns a single row, which is the sum of the input:
Unfortunately, it is not possible to do multiple TRANSFORMs in a single query like the UDAF SUM() can do.
Also, without using CLUSTER BY or DISTRIBUTE BY for the intermediate data, this job may run single, very long map and reduce tasks.
While not all operations can be done in parallel, many can.
The next section discusses how to do streaming in parallel, when possible.
These features can be used on most queries, but are particularly useful when doing streaming processes.
For example, data for the same key may need to be sent to the same processing node, or data may need to be sorted by a specific column, or by a function.
The first way to control this behavior is the CLUSTER BY clause, which ensures like data is routed to the same reduce task and sorted.
Now, we will use the TRANSFORM feature and two Python scripts, one to tokenize lines of text into words, and the second to accept a stream of word occurrences and an intermediate count of the words (mostly the number “1”) and then sum up the counts for each word.
Without explaining all the Python syntax, this script imports common functions from a sys module, then it loops over each line on the “standard input,” stdin, splits each line on whitespace into a collection of words, then iterates over the word and writes each word, followed by a tab, \t, and the “count” of one.2 Before we show the second Python script, let’s discuss the data that’s passed to it.
We’ll use CLUSTER BY for the words output from the first Python script in our TRANSFORM Hive query.
This will have the effect of causing all occurrences of the word\t1 “pairs” for a give, word to be grouped together, one pair per line:
Hence, the second Python script will be more complex, because it needs to cache the word it’s currently processing and the count of occurrences seen so far.
When the word changes, the script must output the count for the previous word and reset its caches.
We’ll assume that both Python scripts are in your home directory.
Finally, here is the Hive query that glues it all together.
Any text file could serve as the data for this table.
We could cache the counts of words seen and then write the final count.
That would be faster, by minimizing I/O overhead, but it would also be more complex to implement.
It will have two columns, the word and count, and data will be tabdelimited.
Finally, we show the TRANSFORM query that glues it all together:
The USING clauses specify an absolute path to the Python scripts.
This is used in the general case when you wish to partition the data by one column and sort it by another.
The following version of the TRANSFORM query outputs the word count results in reverse order:
Without these directives, Hive may not be able to parallelize the job properly.
All the data might be sent to a single reducer, which would extend the job processing time.
GenericMR Tools for Streaming to Java Typically, streaming is used to integrate non-Java code into Hive.
Streaming works with applications written in essentially any language, as we saw.
It is possible to use Java for streaming, and Hive includes a GenericMR API that attempts to give the feel of the Hadoop MapReduce API to streaming:
To understand how the IdentityMapper is written, we can take a look at the interfaces GenericMR provides.
The Mapper interface is implemented to build custom Mapper implementations.
It provides a map method where the column data is sent as a string array, String []:
The IdentityMapper makes no changes to the input data and passes it to the collector.
This is functionally equivalent to the identity streaming done with /bin/cat earlier in the chapter:
The Reducer interface provides the first column as a String, and the remaining columns are available through the record Iterator.
Each iteration returns a pair of Strings, where the 0th element is the key repeated and the next element is the value.
The output object is the same one used to emit results:
WordCountReduce has an accumulator that is added by each element taken from the records Iterator.
When all the records have been counted, a single two-element array of the key and the count is emitted:
Calculating Cogroups It’s common in MapReduce applications to join together records from multiple data sets and then stream them through a final TRANSFORM step.
Suppose we have several sources of logfiles, with similar schema, that we wish to bring together and analyze with a reduce_script:
Finally, you can customize the file and record formats, which we discuss now.
File Versus Record Formats Hive draws a clear distinction between the file format, how records are encoded in a file, the record format, and how the stream of bytes for a given record are encoded in the record.
In this book we have been using text files, with the default STORED AS TEXTFILE in CREATE TABLE statements (see “Text File Encoding of Data Values” on page 45), where each line in the file is a record.
Most of the time those records have used the default separators, with occasional examples of data that use commas or tabs as field separators.
However, a text file could contain JSON or XML “documents.” For Hive, the file format choice is orthogonal to the record format.
We’ll first discuss options for file formats, then we’ll discuss different record formats and how to use them in Hive.
Demystifying CREATE TABLE Statements Throughout the book we have shown examples of creating tables.
You may have noticed that CREATE TABLE has a variety of syntax.
This chapter will cover much of this syntax and give examples, but as a preface note that some syntax.
Let’s create some tables and use DESCRIBE TABLE EXTENDED to peel away the sugar and expose the internals.
First, we will create and then describe a simple table (we have formatted the output here, as Hive otherwise would not have indented the output):
Unless you have been blinded by Hive’s awesomeness, you would have picked up on the difference between these two tables.
Hive uses the InputFormat when reading data from the table, and it uses the OutputFor mat when writing data to the table.
InputFormat reads key-value pairs from files; Hive currently ignores the key and works only with the data found in the value by default.
The reason for this is that the key, which comes from TextInputFormat, is a long integer that represents the byte offset in the block (which is not user data)
The rest of the chapter describes other aspects of the table metadata.
File Formats We discussed in “Text File Encoding of Data Values” on page 45 that the simplest data format to use is the text format, with whatever delimiters you prefer.
It is also the default format, equivalent to creating a table with the clause STORED AS TEXTFILE.
The text file format is convenient for sharing data with other tools, such as Pig, Unix text tools like grep, sed, and awk, etc.
However, the text format is not space efficient compared to binary formats.
We can use compression, as we discussed in Chapter 11, but we can also gain more efficient usage of disk space and better disk I/O performance by using binary file formats.
SequenceFile The first alternative is the SequenceFile format, which we can specify using the STORED AS SEQUENCEFILE clause during table creation.
Sequence files are flat files consisting of binary key-value pairs.
When Hive converts queries to MapReduce jobs, it decides on the appropriate key-value pairs to use for a given record.
The sequence file is a standard format supported by Hadoop itself, so it is an acceptable choice when sharing files between Hive and other Hadoop-related tools.
It’s less suitable for use with tools outside the Hadoop ecosystem.
As we discussed in Chapter 11, sequence files can be compressed at the block and record level, which is very useful for optimizing disk space utilization and I/O, while still supporting the ability to split files on block boundaries for parallel processing.
Another efficient binary format that is supported natively by Hive is RCFile.
A powerful aspect of Hive is that converting data between different formats is simple.
When a query SELECTs from one table and INSERTs into another, Hive uses the metadata about the tables and handles the conversion automatically.
This makes for easy evaluation of the different options without writing one-off programs to convert data between the different formats.
RCFile’s cannot be opened with the tools that open typical sequence files.
However, Hive provides an rcfilecat tool to display the contents of RCFiles:
This can be used to perform simple calculations, such as SELECT 1+2
If Hive did not allow this type of query, then a user would instead select from an existing table and limit the results to a single row.
Or the user may create a table with a single row.
Some databases provide a table named dual, which is a single row table to be used in this manner.
The TextInputFormat calculates zero or more splits for the input.
Splits are opened by the framework and a RecordReader is used to read the data.
To create an input format that works with a dual table, we need to create an input format that returns one split with one row, regardless of the input path specified.1 In the example below, DualInputFormat returns a single split:
After the first invocation of next(), its value is set to false.
Thus, this record reader returns a single row and then is finished with virtual input:
Selecting from the table confirms that it returns a single empty row.
Input Formats should be placed inside the Hadoop lib directory or preferably inside the Hive auxlib directory.
A SerDe encapsulates the logic for converting the unstructured bytes in a record, which is stored as part of a file, into a record that Hive can use.
Hive comes with several built-in SerDes and many other third-party SerDes are available.
Internally, the Hive engine uses the defined InputFormat to read a record of data.
A lazy SerDe does not fully materialize an object until individual attributes are necessary.
The following example uses a RegexSerDe to parse a standard formatted Apache web log.
The RegexSerDe is included as a standard feature as a part of the Hive distribution:
However, this simplistic approach doesn’t handle strings with embedded commas or tabs, nor does it handle other common conventions, like whether or not to quote all or no strings, or the optional presence of a “column header” row as the first line in each file.
First, it’s generally safer to remove the column header row, if present.
Then one of several third-party SerDes are available for properly parsing CSV or TSV files.
While TSV support should be similar, there are no comparable third-party TSV SerDes available at the time of this writing.
ObjectInspector Underneath the covers, Hive uses what is known as an ObjectInspector to transform raw records into objects that Hive can access.
Using the JavaBeans model for introspection, any “property” on objects that are exposed through get methods or as public member variables may be referenced in queries.
One of the reasons Hadoop is ideal as an XML database platform is the complexity and resource consumption to parse and process potentially large XML documents.
Because Hadoop parallelizes processing of XML documents, Hive becomes a perfect tool for accelerating XML-related data solutions.
Additionally, HiveQL natively enables access to XML’s nested elements and values, then goes further by allowing joins on any of the nested fields, values, and attributes.
XPath (XML Path Language) is a global standard created by the W3C for addressing parts of an XML document.
Using XPath as an expressive XML query language, Hive becomes extremely useful for extracting data from XML documents and into the Hive subsystem.
XPath models an XML document as a tree of nodes.
Basic facilities are provided for access to primitive types, such as string, numeric, and Boolean types.
While commercial solutions such as Oracle XML DB and MarkLogic provide native XML database solutions, open source Hive leverages the advantages provided by the parallel petabyte processing of the Hadoop infrastructure to enable widely effective XML database vivification.
Here are some examples where these functions are run on string literals:
There is a third-party JSON SerDe that started as a Google “Summer of Code” project and was subsequently cloned and forked by other contributors.
Think Big Analytics created its own fork and added an enhancement we’ll go over in the discussion that follows.
In the following example, this SerDe is used to extract a few fields from JSON data for a fictitious messaging system.
Those that are exposed become available as columns in the table:
The WITH SERDEPROPERTIES is a Hive feature that allows the user to define properties that will be passed to the SerDe.
In this case, the properties are used to map fields in the JSON documents to columns in the table.
This value for the id is used as the value for the user_id column.
Once defined, the user runs queries as always, blissfully unaware that the queries are actually getting data from JSON!
Avro Hive SerDe Avro is a serialization systemit’s main feature is an evolvable schema-driven binary data format.
Initially, Avro’s goals appeared to be in conflict with Hive since both wish to provide schema or metadata information.
However Hive and the Hive metastore have pluggable design and can defer to the Avro support to infer the schema.
The Hive Avro SerDe system was created by LinkedIn and has the following features:
Infers the schema of the Hive table from the Avro schema.
Reads all Avro files within a table against a specified schema, taking advantage of.
The schema specifies three columns: number as int, firstname as string, and lastname as string.
When the DESCRIBE command is run, Hive shows the name and types of the columns.
In the output below you will notice that the third column of output states from deser ializer.
This shows that the SerDe itself returned the information from the column rather than static values stored in the metastore:
Defining a Schema from a URI It is also possible to provide the schema as a URI.
This can be a path to a file in HDFS or a URL to an HTTP server.
The schema can also be stored on an HTTP server: TBLPROPERTIES ('avro.schema.url'='http://site.com/path/to.schema')
Evolving Schema Over time fields may be added or deprecated from data sets.
It also allows for default values to be returned if the column is not defined in the data file.
For example, if the Avro schema is changed and a field added, the default field supplies a value if the column is not found:
We have already seen compression of files, sequence files (compressed or not), and related file types.
Sometimes, it’s also useful to read and write streams of bytes.
For example, you may have tools that expect a stream of bytes, without field separators of any kind, and you either use Hive to generate suitable files for those tools or you want to query such files with Hive.
You may also want the benefits of storing numbers in compact binary forms instead of strings like “5034223,” which consume more space.
A common example is to query the output of the tcpdump command to analyze network behavior.
The following table expects its own files to be in text format, but it writes query results as binary streams:
Here’s a SELECT TRANSFORM query that reads binary data from a src table, streams it through the shell cat command and overwrites the contents of a destination1 table:
Hive has an optional component known as HiveServer or HiveThrift that allows access to Hive over a single port.
Thrift is a software framework for scalable cross-language services development.
Thrift allows clients using languages including Java, C++, Ruby, and many others, to programmatically access Hive remotely.
The CLI is the most common way to access Hive.
However, the design of the CLI can make it difficult to use programmatically.
The CLI is a fat client; it requires a local copy of all the Hive components and configuration as well as a copy of a Hadoop client and its configuration.
Additionally, it works as an HDFS client, a MapReduce client, and a JDBC client (to access the metastore)
Even with the proper client installation, having all of the correct network access can be difficult, especially across subnets or datacenters.
Starting the Thrift Server To Get started with the HiveServer, start it in the background using the service knob for hive:
A quick way to ensure the HiveServer is running is to use the netstat command to determine if port 10,000 is open and listening for connections:
With the interface, the Thrift compiler generates code that creates network RPC clients for many languages.
Because Hive is written in Java, and Java bytecode is cross-platform, the clients for the Thrift server are included in the Hive.
One way to use these clients is by starting a Java project with an IDE and including these libraries or fetching them through Maven.
Groovy is an agile and dynamic language for the Java Virtual Machine.
This will allow Groovy to communicate with Hive without having to manually load JAR files each session:
Groovy has an @grab annotation that can fetch JAR files from Maven web repositories, but currently some packaging issues with Hive prevent this from working correctly.
Groovy provides a shell found inside the distribution at bin/groovysh.
Groovy code is similar to Java code, although it does have other forms including closures.
For the most part, you can write Groovy as you would write Java.
These classes are used to connect to Hive and create an instance of HiveClient.
HiveClient has the methods users will typically use to interact with Hive:
This can be used to collect performance metrics and can also be used to wait for a lull to launch a job:
Result Set Schema After executing a query, you can get the schema of the result set using the get Schema() method.
If you call this method before a query, it may return a null schema:
Fetching Results After a query is run, you can fetch results with the fetchOne() method.
Retrieving large result sets with the Thrift interface is not suggested.
However, it does offer several methods to retrieve data using a one-way cursor.
Instead of retrieving rows one at a time, the entire result set can be retrieved as a string array using the fetchAll() method:
Also available is fetchN, which fetches N rows at a time.
Retrieving Query Plan After a query is started, the getQueryPlan() method is used to retrieve status information about the query.
The information includes information on counters and the state of the job:
Generally, users should not call metastore methods that modify directly and should only interact with Hive via the HiveQL language.
Users should utilize the read-only methods that provide meta-information about tables.
It is important to remember that while the metastore API is relatively stable in terms of changes, the methods inside, including their signatures and purpose, can change between releases.
Hive tries to maintain compatibility in the HiveQL language, which masks changes at these levels.
Example Table Checker The ability to access the metastore programmatically provides the capacity to monitor and enforce conditions across your deployment.
For example, a check can be written to ensure that all tables use compression, or that tables with names that start with zz should not exist longer than 10 days.
These small “Hive-lets” can be written quickly and executed remotely, if necessary.
Usually, external tables do not use this directory, but there is nothing that prevents you from putting them there.
The Table object returned from get_table(data base,table) has all the information about the table in the metastore.
We determine the location of the table and check that the type matches the string MANAGED_TABLE.
Administrating HiveServer The Hive CLI creates local artifacts like the .hivehistory file along with entries in /tmp and hadoop.tmp.dir.
Because the HiveService becomes the place where Hadoop jobs launch from, there are some considerations when deploying it.
Productionizing HiveService HiveService is a good alternative to having the entire Hive client install local to the machine that launches the job.
Using it in production does bring up some added issues that need to be addressed.
The work that used to be done on the client machine, in planning and managing the tasks, now happens on the server.
If you are launching many clients simultaneously, this could cause too much load for a single HiveService.
A simple solution is to use a TCP load balancer or proxy to alternate connections between a pool of backend servers.
There are several ways to do TCP load balancing and you should consult your network administrator for the best solution.
We suggest a simple solution that uses the hap roxy tool to balance connections between backend ThriftServers.
Depending on your operating system and distribution these steps may be different.
After you have confirmed it is working, add it to the default system start-up with chkconfig:
Setting it to true will cause the service to clean up its scratch directory on restart:
Hive ThriftMetastore Typically, a Hive session connects directly to a JDBC database, which it uses as a metastore.
In this setup, the Hive client connects to the ThriftMetastore, which in turn communicates to the JDBC Metastore.
It is useful for deployments that have non-Java clients that need access to information in the metastore.
ThriftMetastore Configuration The ThriftMetastore should be set up to communicate with the actual metastore using JDBC.
Client Configuration Clients like the CLI should communicate with the metastore directory:
Although, there are some nuances with Hadoop Security and the metastore having to do work as the user.
Storage Handlers are a combination of InputFormat, OutputFormat, SerDe, and specific code that Hive uses to treat an external entity as a standard Hive table.
This allows the user to issue queries seamlessly whether the table represents a text file stored in Hadoop or a column family stored in a NoSQL database such as Apache HBase, Apache Cassandra, and Amazon DynamoDB.
Storage handlers are not only limited to NoSQL databases, a storage handler could be designed for many different kinds of data stores.
A specific storage handler may only implement some of the capabilities.
For example, a given storage handler may allow read-only access or impose some other restriction.
For example, a Hive query could be run that selects a data table that is backed by sequence files, however it could output.
Storage Handler Background Hadoop has an abstraction known as InputFormat that allows data from different sources and formats to be used as input for a job.
It works by providing Hadoop with information on how to split a given path into multiple tasks, and it provides a RecordReader that provides methods for reading data from each split.
Hadoop also has an abstraction known as OutputFormat, which takes the output from a job and outputs it to an entity.
It works by persisting output to a file which could be stored on HDFS or locally.
Input and output that represent physical files are common in Hadoop, however Input Format and OutputFormat abstractions can be used to load and persist data from other.
Hive’s abstractions such as tables, types, row format, and other metadata are used by Hive to understand the source data.
Once Hive understands the source data, the query engine can process the data using familiar HiveQL operators.
Many NoSQL databases have implemented Hive connectors using custom adapters.
HiveStorageHandler HiveStorageHandler is the primary interface Hive uses to connect with NoSQL stores such as HBase, Cassandra, and others.
An examination of the interface shows that a custom InputFormat, OutputFormat, and SerDe must be defined.
The storage handler enables both reading from and writing to the underlying storage subsystem.
This translates into writing SELECT queries against the data system, as well as writing into the data system for actions such as reports.
When executing Hive queries over NoSQL databases, the performance is less than normal Hive and MapReduce jobs on HDFS due to the overhead of the NoSQL system.
Some of the reasons include the socket connection to the server and the merging of multiple underlying files, whereas typical access from HDFS is completely sequential I/O.
A common technique for combining NoSQL databases with Hadoop in an overall system architecture is to use the NoSQL database cluster for real-time work, and utilize the Hadoop cluster for batch-oriented work.
If the NoSQL system is the master data store, and that data needs to be queried on using batch jobs with Hadoop, bulk exporting is an efficient way to convert the NoSQL data into HDFS files.
Once the HDFS files are created via an export, batch Hadoop jobs may be executed with a maximum efficiency.
HBase The following creates a Hive table and an HBase table using HiveQL:
To create a Hive table that points to an existing HBase table, the CREATE EXTERNAL TABLE HiveQL statement must be used:
Instead of scanning the entire HBase table for a given Hive query, filter pushdowns will constrain the row data returned to Hive.
Examples of the types of predicates that are converted into pushdowns are:
Any other more complex types of predicates will be ignored and not utilize the pushdown feature.
The following is an example of creating a simple table and a query that will use the filter pushdown feature.
Note the pushdown is always on the HBase key, and not the column values of a column family:
The following query will not result in a pushdown because it contains an OR on the predicate:
Hive with HBase supports joins on HBase tables to HBase tables, and HBase tables to non-HBase tables.
By default, pushdowns are turned on, however they may be turned off with the following:
It is important to note when inserting data into HBase from Hive that HBase requires unique keys, whereas Hive has no such constraint.
There is no way to access the HBase row timestamp, and only the latest version of a row is returned.
Cassandra Cassandra has implemented the HiveStorageHandler interface in a similar way to that of HBase.
The implementation was originally performed by Datastax on the Brisk project.
The model is fairly straightforward, a Cassandra column family maps to a Hive table.
In turn, Cassandra column names map directly to Hive column names.
Static Column Mapping Static column mapping is useful when the user has specific columns inside Cassandra which they wish to map to Hive columns.
The following is an example of creating an external Hive table that maps to an existing Cassandra keyspace and column family:
This use case is where a given column family does not have fixed, named columns, but rather the columns of a row key represent some piece of data.
This is often used in time series data where the column name represents a time and the column value represents the value at that time.
This is also useful if the column names are not known or you wish to retrieve all of them:
DynamoDB Amazon’s Dynamo was one of the first NoSQL databases.
Its design influenced many other databases, including Cassandra and HBase.
Despite its influence, Dynamo was restricted to internal use by Amazon until recently.
Amazon released another database influenced by the original Dynamo called DynamoDB.
In DynamoDB, tables are a collection of items and they are required to have a primary key.
An item consists of a key and an arbitrary number of attributes.
The set of attributes can vary from item to item.
You can query a table with Hive and you can move data to and from S3
Here is another example of a Hive table for stocks that is backed by a DynamoDB table:
To understand Hive security, we have to backtrack and understand Hadoop security and the history of Hadoop.
At that time and through its early formative years, features were prioritized over security.
Security is more complex in a distributed system because multiple components across different machines need to communicate with each other.
Unsecured Hadoop like the versions before the v0.20.205 release derived the username by forking a call to the whoami program.
Users are free to change this parameter by setting the hadoop.job.ugi property for FSShell (filesystem) commands.
Map and reduce tasks all run under the same system user (usually hadoop or mapred) on TaskTracker nodes.
Also, Hadoop components are typically listening on ports with high numbers.
They are also typically launched by nonprivileged users (i.e., users other than root)
The recent efforts to secure Hadoop involved several changes, primarily the incorporation of Kerberos authorization support, but also other changes to close vulnerabilities.
A client’s request for a ticket is passed along with a request.
Tasks on the TaskTracker are run as the user who launched the job.
Users are no longer able to impersonate other users by setting the hadoop.job.ugi property.
For this to work, all Hadoop components must use Kerberos security from end to end.
Hive was created before any of this Kerberos support was added to Hadoop, and Hive is not yet fully compliant with the Hadoop security changes.
For example, the connection to the Hive metastore may use a direct connection to a JDBC database or it may go through Thrift, which will have to take actions on behalf of the user.
Components like the Thrift-based HiveService also have to impersonate other users.
The file ownership model of Hadoop, where one owner and group own a file, is different than the model many databases have implemented where access is granted and revoked on a table in a row- or column-based manner.
This chapter attempts to highlight components of Hive that operate differently between secure and nonsecure Hadoop.
Security support in Hadoop is still relatively new and evolving.
Some parts of Hive are not yet compliant with Hadoop security support.
The discussion in this section summarizes the current state of Hive security, but it is not meant to be definitive.
Also, more than in any other chapter in this book, we’ll occasionally refer you to Hive JIRA entries for more information.
User privileges can be granted and revoked, as we’ll discuss below.
There are still several known security gaps involving Thrift and other components, as listed on the security wiki page.
Authentication with Hive When files and directories are owned by different users, the permissions set on the files become important.
The HDFS permissions system is very similar to the Unix model, where there are three entities: user, group, and others.
The default value for this property is false, but it should be set to true:
This needs to be set to true to enable authentication:
So, we also gave table creators subsequent access to their tables!
Currently it is possible for users to use the set command to disable authentication by setting this property to false.
Users, Groups, and Roles Privileges are granted or revoked to a user, a group, or a role.
We will walk through granting privileges to each of these entities:
Already we can see that our user does not have the privilege to create tables in the default database.
The first entity is a user: the user in Hive is your system user.
We can determine the user and then grant that user permission to create tables in the default database:
Granting permissions on a per-user basis becomes an administrative burden quickly with many users and many tables.
A better option is to grant permissions based on groups.
A group in Hive is equivalent to the user’s primary POSIX group:
When user and group permissions are not flexible enough, roles can be used.
Users are placed into roles and then roles can be granted privileges.
Roles are very flexible, because unlike groups that are controlled externally by the system, roles are controlled from inside Hive:
Privileges to Grant and Revoke Table 18-1 lists the available privileges that can be configured.
The syntax GRANT SELECT(COLUMN) is currently accepted but does nothing.
Partition-Level Privileges It is very common for Hive tables to be partitioned.
Automatic Grants Regular users will want to create tables and not bother with granting privileges to themselves to perform subsequent queries, etc.
Earlier, we showed that you might want to grant ALL privileges, by default, but you can narrow the allowed privileges instead.
In the following example, rather than granting ALL privileges, the users are automatically granted SELECT and DROP privileges for their own tables:
Similarly, specific users can be granted automatic privileges on tables as they are created.
Similar properties exist to automatically grant privileges to groups and roles.
The values of the properties follow the same format just shown.
While HiveQL is an SQL dialect, Hive lacks the traditional support for locking on a column, row, or query, as typically used with update or insert queries.
Files in Hadoop are traditionally write-once (although Hadoop does support limited append semantics)
Because of the write-once nature and the streaming style of MapReduce, access to fine-grained locking is unnecessary.
However, since Hadoop and Hive are multi-user systems, locking and coordination are valuable in some situations.
For example, if one user wishes to lock a table, because an INSERT OVERWRITE query is changing its content, and a second user attempts to issue a query against the table at the same time, the query could fail or yield invalid results.
Hive can be thought of as a fat client, in the sense that each Hive CLI, Thrift server, or web interface instance is completely independent of the other instances.
Because of this independence, locking must be coordinated by a separate system.
Other than some additional setup and configuration steps, Zookeeper is invisible to Hive users.
To set up Zookeeper, designate one or more servers to run its server processes.
Three Zookeeper nodes is a typical minimum size, to provide a quorum and to provide sufficient redundancy.
In the following commands, we will install Zookeeper in the /opt directory, requiring sudo access (a later version of Zookeeper, if any, will probably work fine, too):
Make a directory for Zookeeper to store its data: $ sudo mkdir /var/zookeeper.
On each server, create a myid file and ensure the contents of the file matches the ID from the configuration.
For example, for the file on the zk1.site.pvt node, you could use the following command to create the file:
We are starting the process as root, which is generally not recommended for most processes.
You could use any standard techniques to run this file as a different user.
Once the Zookeeper nodes are in communication with each other, it will be possible to create data on one Zookeeper node and read it from the other.
Then, run this session on a different node or a different terminal window on the first node:
Now we need to configure Hive so it can use these Zookeeper nodes to enable the concurrency support.
With these settings configured, Hive automatically starts acquiring locks for certain queries.
You can see all current locks with the SHOW LOCKS command:
The following more focused queries are also supported, where the ellipsis would be replaced with an appropriate partition specification, assuming that places is partitioned:
There are two types of locks provided by Hive, and they are enabled automatically when the concurrency feature is enabled.
A shared lock is acquired when a table is read.
An exclusive lock is required for all other operations that modify the table in some way.
They not only freeze out other table-mutating operations, they also prevent queries by other processes.
When the table is partitioned, acquiring an exclusive lock on a partition causes a shared lock to be acquired on the table itself to prevent incompatible concurrent changes from occurring, such as attempting to drop the table while a partition is being modified.
Of course, an exclusive lock on the table globally affects all partitions.
For example, suppose one Hive session creates an exclusive lock on table people:
The table can be unlocked using the UNLOCK TABLE statement, after which queries from other sessions will work again:
You may have noticed Hive has its own internal workflow system.
Hive converts a query into one or more stages, such as a map reduce stage or a move task stage.
If a stage fails, Hive cleans up the process and reports the errors.
If a stage succeeds, Hive executes subsequent stages until the entire job is done.
Also, multiple Hive statements can be placed inside an HQL file and Hive will execute each query in sequence until the file is completely processed.
Hive’s system of workflow management is excellent for single jobs or jobs that run one after the next.
For example, a user may want to have a process in which step one is a custom MapReduce job, step two uses the output of step one and processes it using Hive, and finally step three uses distcp to copy the output from step 2 to a remote cluster.
These kinds of workflows are candidates for management as Oozie Workflows.
Oozie Coordinator jobs are recurrent Oozie Workflow jobs triggered by time (frequency) and data availability.
An important feature of Oozie is that the state of the workflow is detached from the client who launches the job.
This detached (fire and forget) job launching is useful; normally a Hive job is attached to the console that submitted it.
The user supplies the MapperClass, the ReducerClass, and sets conf variables Shell.
A shell command with arguments is run as an action.
Java action A Java class with a main method is launched with optional arguments.
DistCp Run a distcp command to copy data to or from another HDFS cluster.
Hive Thrift Service Action The built-in Hive action works well but it has some drawbacks.
Most of the Hive distributions, including JARs and configuration files, need to be copied into the workflow directory.
When Oozie launches an action, it will launch from a random TaskTracker node.
There may be a problem reaching the metastore if you have your metastore setup to only allow access from specific hosts.
Since Hive can leave artifacts like the hive-history file or some /tmp entries if a job fails, make sure to clean up across your pool of TaskTrackers.
The fat-client challenges of Hive have been solved (mostly) by using Hive Thrift Service (see Chapter 16)
This has the benefits of funneling all the Hive operations to a predefined set of nodes running Hive service:
A Two-Query Workflow A workflow is created by setting up a specific directory hierarchy with required JAR files, a job.properties file and a workflow.xml file.
This hierarchy has to be stored in HDFS, but it is best to assemble the folder locally and then copy it to HDFS:
The job.properties sets the name of the filesystem and the JobTracker.
Also, additional properties can be set here to be used as Hadoop Job Configuration properties: The job.properties file:
Oozie launches each action inside a map task and captures all the input and output.
Oozie does a good job presenting this information as well as providing links to job status pages found on the Hadoop JobTracker web console.
Variables in Workflows A workflow based on completely static queries is useful but not overly practical.
Most of the use cases for Oozie run a series of processes against files for today or this week.
In the previous workflow, you may have noticed the KILL tag and the interpolated variable inside of it:
Key-value pairs defined in job.properties can be referenced this way.
Capturing Output Oozie also has a tag <captureOutput/> that can be placed inside an action.
Output captured can be emailed with an error or sent to another process.
Oozie sets a Java property in each action that can be used as a filename to write output to.
Capturing Output to Variables We have discussed both capturing output and Oozie variables; using them together provides what you need for daily workflows.
Looking at our previous example, we see that we are selecting data from a hardcoded day FROM BCO WHERE dt=20120426
We would like to run this workflow every day so we need to substitute the hardcoded dt=20120426 with a date:
There are many more things you can do with Oozie, including integrating Hive jobs with jobs implemented with other tools, such as Pig, Java MapReduce, etc.
With EMR comes the ability to spin up a cluster of nodes on demand.
These clusters come with Hadoop and Hive installed and configured.
You can also configure the clusters with Pig and other tools.
You can then run your Hive queries and terminate the cluster when you are done, only paying for the time you used the cluster.
This section describes how to use Elastic MapReduce, some best practices, and wraps up with pros and cons of using EMR versus other options.
This chapter won’t cover all the details of using Amazon EMR with Hive.
It is designed to provide an overview and discuss some practical details.
Why Elastic MapReduce? Small teams and start-ups often don’t have the resources to set up their own cluster.
An in-house cluster is a fixed cost of initial investment.
It requires effort to set up and servers and switches as well as maintaining a Hadoop and Hive installation.
On the other hand, Elastic MapReduce comes with a variable cost, plus the installation and maintenance is Amazon’s responsibility.
This is a huge benefit for teams that can’t or don’t want to invest in their own clusters, and even for larger teams that need a test bed to try out new tools and ideas without affecting their production clusters.
Instances An Amazon cluster is comprised of one or more instances.
Instances come in various sizes, with different RAM, compute power, disk drive, platform, and I/O performance.
It can be hard to determine what size would work the best for your use case.
You will also need to create an Amazon S3 bucket for storing your input data and retrieving the output results of your Hive processing.
When you set up your AWS account, make sure that all your Amazon EC2 instances, key pairs, security groups, and EMR jobflows are located in the same region to avoid cross-region transfer costs.
Try to locate your Amazon S3 buckets and EMR jobflows in the same availability zone for better performance.
Although Amazon EMR supports several versions of Hadoop and Hive, only some combinations of versions of Hadoop and Hive are supported.
See the Amazon EMR documentation to find out the supported version combinations of Hadoop and Hive.
Managing Your EMR Hive Cluster Amazon provides multiple ways to bring up, terminate, and modify a Hive cluster.
Currently, there are three ways you can manage your EMR Hive cluster: EMR AWS Management Console (web-based frontend)
This is the easiest way to bring up a cluster and requires no setup.
However, as you start to scale, it is best to move to one of the other methods.
The Amazon EMR online documentation describes how to install and use this CLI.
Details on downloading and using the SDK are available in the Amazon EMR documentation.
A drawback of an SDK is that sometimes particular SDK wrapper implementations lag behind the latest version of the AWS API.
It is common to use more than one way to manage Hive clusters.
Here is an example that uses the Ruby elastic-mapreduce CLI to start up a single-node Amazon EMR cluster with Hive configured.
It also sets up the cluster for interactive use, rather than for running a job and exiting.
If you also want Pig available, add the --pig-interface option.
Next you would log in to this cluster as described in the Amazon EMR documentation.
However, in the Amazon Hive installation, this port number depends on the version of Hive being used.
This change was implemented in order to allow users to install and support concurrent versions of Hive.
These port numbers are expected to change as newer versions of Hive get ported to Amazon EMR.
Each of these nodes can fit into one of the following three instance groups: Master Instance Group.
This instance group contains exactly one node, which is called the master node.
The master node performs the same duties as the conventional Hadoop master node.
It runs the namenode and jobtracker daemons, but it also has Hive installed on it.
In addition, it has a MySQL server installed, which is configured to serve as the metastore for the EMR Hive installation.
The embedded Derby metastore that is used as the default metastore in Apache Hive installations is not used.
There is also an instance controller that runs on the master node.
It is responsible for launching and managing other instances from the other two instance groups.
Note that this instance controller also uses the MySQL server on the master node.
If the MySQL server becomes unavailable, the instance controller will be unable to launch and manage instances.
Core Instance Group The nodes in the core instance group have the same function as Hadoop slave nodes that run both the datanode and tasktracker daemons.
These nodes are used for MapReduce jobs and for the ephemeral storage on these nodes that is used for HDFS.
Once a cluster has been started, the number of nodes in this instance group can only be increased but not decreased.
It is important to note that ephemeral storage will be lost if the cluster is terminated.
The nodes in this group also function as Hadoop slave nodes.
Hence, these nodes are used for MapReduce tasks, but not for storing HDFS blocks.
Once the cluster has been started, the number of nodes in the task instance group can be increased or decreased.
The task instance group is convenient when you want to increase cluster capacity during hours of peak demand and bring it back to normal afterwards.
It is also useful when using spot instances (discussed below) for lower costs without risking the loss of data when a node gets removed from the cluster.
If you are running a cluster with just a single node, the node would be a master node and a core node at the same time.
Configuring Your EMR Cluster You will often want to deploy your own configuration files when launching an EMR cluster.
The most common files to customize are hive-site.xml, .hiverc, hadoop-env.sh.
Deploying hive-site.xml For overriding hive-site.xml, upload your custom hive-site.xml to S3
If you are starting you cluster via the elastic-mapreduce Ruby client, use a command like the following to spin up your cluster with your custom hive-site.xml:
If you are using the SDK to spin up a cluster, use the appropriate method to override the hive-site.xml file.
After the bootstrap actions, you would need two config steps, one for installing Hive and another for deploying hive-site.xml.
The first step of installing Hive is to call --install-hive along with --hive-versions flag followed by a commaseparated list of Hive versions you would like to install on your EMR cluster.
Deploying a .hiverc Script For .hiverc, you must first upload to S3 the file you want to install.
Then you can either use a config step or a bootstrap action to deploy the file to your cluster.
Note that .hiverc can be placed in the user’s home directory or in the bin directory of the Hive installation.
However, it is fairly straightforward to extend the Amazon-provided hive-script to enable installation of .hiverc, if you are comfortable modifying Ruby code.
After implementing this change to hive-script, upload it to S3 and use that version instead of the original Amazon version.
Have your modified script install .hiverc to the user’s home directory or to the bin directory of the Hive installation.
Alternatively, you can create a custom bootstrap script that transfers .hiverc from S3 to the user’s home directory or Hive’s bin directory of the master node.
Then, simply use a command such as the following to download the file from S3 and deploy it in the home directory:
Then use a bootstrap action to call this script during the cluster creation process, just like you would any other bootstrap action.
Setting Up a Memory-Intensive Configuration If you are running a memory-intensive job, Amazon provides some predefined bootstrap actions that can be used to fine tune the Hadoop configuration parameters.
For example, to use the memory-intensive bootstrap action when spinning up your cluster, use the following flag in your elastic-mapreduce --create command (wrapped for space):
Persistence and the Metastore on EMR An EMR cluster comes with a MySQL server installed on the master node of the cluster.
By default, EMR Hive uses this MySQL server as its metastore.
However, all data stored on the cluster nodes are deleted once you terminate your cluster.
This includes the data stored on the master node metastore, as well! This is usually unacceptable because you would like to retain your table schemas, etc., in a persistent metastore.
You can use one of the following methods to work around this limitation: Use a persistent metastore external to your EMR cluster.
You can use the Amazon RDS (Relational Data Service), which is based on MySQL, or another, in-house database server as a metastore.
This is the best choice if you want to use the same metastore for multiple EMR clusters or the same EMR cluster running more than one version of Hive.
Leverage a start-up script If you don’t intend to use an external database server for your metastore, you can still use the master node metastore in conjunction with your start-up script.
You can place your create table statements in startup.q, as follows:
It is important to include the IF NOT EXISTS clause in your create statement to ensure that the script doesn’t try to re-create the table on the master node metastore if it was previously created by a prior invocation of startup.q.
At this point, we have our table definitions in the master node metastore but we haven’t yet imported the partitioning metadata.
To do so, include a line like the following in your startup.q file after the create table statement:
This will populate all the partitioning related metadata in the metastore.
Instead of your custom start-up script, you could use .hiverc, which will be sourced automatically when Hive CLI starts up.
The benefit of using .hiverc is that it provides automatic invocation.
The disadvantage is that it gets executed on every invocation of the Hive CLI, which leads to unnecessary overhead on subsequent invocations.
The advantage of using your custom start-up script is that you can more strictly control when it gets executed in the lifecycle of your workflow.
In any case, a side benefit of using a file to store Hive queries for initialization is that you can track the changes to your DDL via version control.
As your meta information gets larger with more tables and more partitions, the start-up time using this system will take longer and longer.
This solution is not suggested if you have more than a few tables or partitions.
MySQL dump on S3 Another, albeit cumbersome, alternative is to back up your metastore before you terminate the cluster and restore it at the beginning of the next workflow.
S3 is a good place to persist the backup while the cluster is not in use.
Note that this metastore is not shared amongst different versions of Hive running on your EMR cluster.
If you would like to share the metadata between different Hive versions, you will have to use an external persistent metastore.
All the data stored on the cluster nodes is deleted once the cluster is terminated.
Since HDFS is formed by ephemeral storage of the nodes in the core instance group, the data stored on HDFS is lost after cluster termination.
S3, on the other hand, provides a persistent storage for data associated with the EMR cluster.
Therefore, intermediate results of processing should be stored in HDFS, with only the final results saved to S3 that need to persist.
Please note that as a side effect of using S3 as a source for input data, you lose the Hadoop data locality optimization, which may be significant.
If this optimization is crucial for your analysis, you should consider importing “hot” data from S3 onto HDFS before processing it.
This initial overhead will allow you to make use of Hadoop’s data locality optimization in your subsequent processing.
Since EMR Hive and Hadoop installations natively understand S3 paths, it is straightforward to work with these files in subsequent Hadoop jobs.
For example, you can add the following lines in .hiverc without any errors:
These include logs from bootstrap actions of the cluster and the logs from running daemon processes on the various cluster nodes.
The log-uri field can be set in the credentials.json file found in the installation directory of the elastic-mapreduce Ruby client.
It can also be specified or overridden explicitly when spinning up the cluster using elastic-mapreduce by using the --log-uri flag.
However, if this field is not set, those logs will not be available on S3
If your workflow is configured to terminate if your job encounters an error, any logs on the cluster will be lost after the cluster termination.
If your log-uri field is set, these logs will be available at the specified location on S3 even after the cluster has been terminated.
They can be an essential aid in debugging the issues that caused the failure.
However, if you store logs on S3, remember to purge unwanted logs on a frequent basis to save yourself from unnecessary storage costs!
Spot Instances Spot instances allows users to bid on unused Amazon capacity to get instances at cheaper rates compared to on-demand prices.
Depending on your use case, you might want instances in all three instance groups to be spot instances.
In this case, your entire cluster could terminate at any stage during the workflow, resulting in a loss of intermediate data.
If it’s “cheap” to repeat the calculation, this might not be a serious issue.
An alternative is to persist intermediate data periodically to S3, as long as your jobs can start again from those snapshots.
Another option is to only include the nodes in the task instance group as spot nodes.
If these spot nodes get taken out of the cluster because of unavailability or because the spot prices increased, the workflow will continue with the master and core nodes, but.
When spot nodes get added to the cluster again, MapReduce tasks can be delegated to them, speeding up the workflow.
Using the elastic-mapreduce Ruby client, spot instances can be ordered by using the --bid-price option along with a bid price.
The following example shows a command to create a cluster with one master node, two core nodes and two spot nodes (in the task instance group) with a bid price of 10 cents:
If you are spinning up a similar cluster using the Java SDK, use the following Instance GroupConfig variables for master, core, and task instance groups:
If a map or reduce task fails, Hadoop will have to start them from the beginning.
If you rely on too many spot instances, your job times may be unpredictable or fail entirely by TaskTrackers getting removed from the cluster.
You can use ssh tunneling or a dynamic SOCKS proxy to view them.
In order to be able to view these from a browser on your client machine (outside of the Amazon network), you need to modify the Elastic MapReduce master security group via your AWS Web Console.
This approach gives you more control over the version and configuration of Hive and Hadoop.
For example, you can experiment with new releases of tools before they are made available through EMR.
The drawback of this approach is that customizations available through EMR may not be available in the Apache Hive release.
There is also an optimization for reducing start-up time for Amazon S3 queries, which is only available in EMR Hive.
This optimization is enabled by adding the following snippet in your hive-site.xml:
Another example is a command that allows the user to recover partitions if they exist in the correct directory structure on HDFS or S3
This is convenient when an external process is populating the contents of the Hive table in appropriate partitions.
In order to track these partitions in the metastore, one could run the following command, where emr_table is the name of the table:
Wrapping Up Amazon EMR provides an elastic, scalable, easy-to-set-up way to bring up a cluster with Hadoop and Hive ready to run queries as soon as it boots.
While much of the configuration is done for you, it is flexible enough to allow users to have their own custom configurations.
Introduction Using Hive for data processing on Hadoop has several nice features beyond the ability to use an SQL-like language.
It’s ability to store metadata means that users do not need to remember the schema of the data.
It also means they do not need to know where the data is stored, or what format it is stored in.
Data producers can add a new column to the data without breaking their consumers’ data-reading applications.
Administrators can relocate data to change the format it is stored in without requiring changes on the part of the producers or consumers.
The majority of heavy Hadoop users do not use a single tool for data production and consumption.
Often, users will begin with a single tool: Hive, Pig, MapReduce, or another tool.
As their use of Hadoop deepens they will discover that the tool they chose is not optimal for the new tasks they are taking on.
Users who start with analytics queries with Hive discover they would like to use Pig for ETL processing or constructing their data models.
Users who start with Pig discover they would like to use Hive for analytics type queries.
While tools such as Pig and MapReduce do not require metadata, they can benefit from it when it is present.
Sharing a metadata store also enables users across tools to share data more easily.
A workflow where data is loaded and normalized using MapReduce or Pig and then analyzed via Hive is very common.
When all these tools share one metastore, users of each tool have immediate access to data created with another tool.
It makes the Hive metastore available to users of other tools on Hadoop.
It provides connectors for MapReduce and Pig so that users of those tools can read data from and write data to Hive’s warehouse.
It also provides a notification service so that workflow tools, such as Oozie, can be notified when new data becomes available in the warehouse.
HCatalog is a separate Apache project from Hive, and is part of the Apache Incubator.
It helps those involved with the project build a community around the project and learn the way Apache software is developed.
InputFormat implementations also exist to read data from HBase, Cassandra, and other data sources.
First, it determines how data is split into sections so that it can be processed in parallel by MapReduce’s map tasks.
Second, it provides a RecordReader, a class that MapReduce uses to read records from its input source and convert them to keys and values for the map task to operate on.
HCatalog provides HCatInputFormat to enable MapReduce users to read data stored in Hive’s data warehouse.
It allows users to read only the partitions of tables and columns that they need.
And it provides the records in a convenient list format so that users do not need to parse them.
This is because it requires some features added in the MapReduce (0.20) API.
This means that a MapReduce user will need to use this interface to interact with HCatalog.
However, Hive requires that the underlying InputFormat used to read data from disk be a mapred implementation.
So if you have data formats you are currently using with a MapReduce InputFormat, you can use it with HCatalog.
InputFormat is a class in the mapreduce API and an interface in the mapred API, hence it was referred to as a class above.
When initializing HCatInputFormat, the first thing to do is specify the table to be read.
This is done by creating an InputJobInfo class and specifying the database, table, and partition filter to use.
If this is null then the default database will be used.
This must be non-null and refer to a valid table in Hive.
If it is left null then the entire table will be read.
Care should be used here, as reading all the partitions of a large table can result in scanning a large volume of data.
For example, if the table to be read is partitioned on a column called datestamp, the filter might look like datestamp = "2012-05-26"
This does carry some risks, depending on how you deploy Hive.
This InputJobInfo instance is then passed to HCatInputFormat via the method setIn put along with the instance of Job being used to configure the MapReduce job:
The map task will need to specify HCatRecord as a value type.
The key type is not important, as HCatalog does not provide keys to the map task.
For example, a map task that reads data via HCatalog might look like:
HCatRecord is the class that HCatalog provides for interacting with records.
When requesting columns by name, the schema must be provided, as each individual HCatRe cord does not keep a reference to the schema.
Since Java does not support overloading of functions by return type, different instances of get and set are provided for each data type.
There are also implementations of get and set that work with Java Objects:
Often a program will not want to read all of the columns in an input.
In this case it makes sense to trim out the extra columns as quickly as possible.
This is particularly true in columnar formats like RCFile, where trimming columns early means reading less data from disk.
This can be achieved by passing a schema that describes the desired columns.
The following example will configure the user’s job to read only two columns named user and url:
Writing Data Similar to reading data, when writing data, the database and table to be written to need to be specified.
If the data is being written to a partitioned table and only one partition is being written, then the partition to be written needs to be specified as well:
The databaseName name indicates the Hive database (or schema) the table is in.
If this is null then the default database will be used.
The tableName is the table that will be written to.
This must be non-null and refer to a valid table in Hive.
If only one partition is to be written, the map must uniquely identify a partition.
For example, if the table is partitioned by two columns, entries for both columns must be in the map.
When working with tables that are not partitioned, this field can be left null.
When the partition is explicitly specified in this manner, the partition column need not be present in the data.
If it is, it will be removed by HCatalog before writing the data to the Hive warehouse, as Hive does not store partition columns with the data.
It is possible to write to more than one partition at a time.
This is referred to as dynamic partitioning, because the records are partitioned dynamically at runtime.
For dynamic partitioning to be used, the values of the partition column(s) must be present in the data.
For example, if a table is partitioned by a column “datestamp,” that column must appear in the data collected in the reducer.
This is because HCatalog will read the partition column(s) to determine which partition to write the data to.
As part of writing the data, the partition column(s) will be removed.
Once an OutputJobInfo has been created, it is then passed to HCatOutputFormat via the static method setOutput:
When writing with HCatOutputFormat, the output key type is not important.
Records can be written from the reducer, or in map only jobs from the map task.
Putting all this together in an example, the following code will read a partition with a datestamp of 20120531 from the table rawevents, count the number of events for each user, and write the result to a table cntd:
Command Line Since HCatalog utilizes Hive’s metastore, Hive users do not need an additional tool to interact with it.
However, for HCatalog users that are not also Hive users, a command-line tool hcat is provided.
The biggest difference is that it only accepts commands that do not result in a MapReduce job being spawned.
This means that the vast majority of DDL (Data Definition Language, or operations that define the data, such as creating tables) are supported:
The SQL operations that HCatalog’s command line does not support are:
Security Model HCatalog does not make use of Hive’s authorization model.
Since it is possible to go directly to the filesystem and access the underlying data, authorization in Hive is limited.
This can be resolved by having all files and directories that contain Hive’s data be owned by the user running Hive jobs.
This way other users can be prevented from reading or writing data, except through Hive.
However, this has the side effect that all UDFs in Hive will then run as a super user, since they will be running in the Hive process.
Consequently, they will have read and write access to all files in the warehouse.
The only way around this in the short term is to declare UDFs to be a privileged operation and only allow those with proper access to create UDFs, though there is no mechanism to enforce this currently.
This may be acceptable in the Hive context, but in Pig and MapReduce where user-generated code is the rule rather than the exception, this is clearly not acceptable.
To resolve these issues, HCatalog instead delegates authorization to the storage layer.
In the case of data stored in HDFS, this means that HCatalog looks at the directories and files containing data to see if a user has access to the data.
If so, he will be given identical access to the metadata.
For example, if a user has permission to write to a directory that contains a table’s partitions, she will also have permission to write to that table.
It is not possible to subvert the system by changing abstraction levels.
The disadvantage is that the security model supported by HDFS is much poorer than is traditional for databases.
In particular, features such as column-level permissions are not possible.
Also, users can only be given permission to a table by being added to a filesystem group that owns that file.
Architecture As explained above, HCatalog presents itself to MapReduce and Pig using their standard input and output mechanisms.
HCatLoader and HCatStorer are fairly simple since they sit atop HCatInputFormat and HCatOutputFormat, respectively.
These two MapReduce classes do a fair amount of work to integrate MapReduce with Hive’s metastore.
HCatInputFormat communicates with Hive’s metastore to obtain information about the table and partition(s) to be read.
This includes finding the table schema as well as schema for each partition.
For each partition it must also determine the actual Input Format and SerDe to use to read the partition.
These are then collected together and the splits from all the partitions returned as the list of InputSplits.
Similarly, the RecordReaders from each underlying InputFormat are used to decode the partitions.
The HCatRecordReader then converts the values from the underlying Record Reader to HCatRecords via the SerDe associated with the partition.
That is, when the table schema contains columns that the partition schema does not, columns with null values must be added to the HCatRecord.
Also, if the user has indicated that only certain columns are needed, then the extra columns are trimmed out at this point.
HCatOutputFormat also communicates with the Hive metastore to determine the proper file format and schema for writing.
Since HCatalog only supports writing data in the format currently specified for the table, there is no need to open different OutputFor mats per partition.
A RecordWriter is then created per partition that wraps the underlying RecordWriter, while the indicated SerDe is used to write data into these new records.
When all of the partitions have been written, HCatalog uses an OutputCommitter to commit the data to the metastore.
Hive is in use at a multitude of companies and organizations around the world.
This case studies chapter details interesting and unique use cases, the problems that were present, and how those issues were solved using Hive as a unique data warehousing tool for petabytes of data.
Our role is to create machine learning algorithms that are specifically tailored toward finding the best new prospects for an advertising campaign.
These algorithms are layered on top of a delivery engine that is tied directly into a myriad of real time bidding exchanges that provide a means to purchase locations on websites to display banner advertisements on behalf of our clients.
The m6d display advertising engine is involved in billions of auctions a day and tens of millions of advertisements daily.
Naturally, such a system produces an immense amount of data.
A large portion of the records that are generated by our company’s display advertising delivery system are housed in m6d’s Hadoop cluster and, as a result, Hive is the primary tool our data science team uses to interact with the these logs.
Hive gives our data science team a way to extract and manipulate large amounts of data.
In fact, it allows us to extract samples and summarize data that prior to using Hive could not be analyzed as efficiently, or at all, because of the immense size.
Despite the fact that Hive allows us access to huge amounts of data at rates many times faster than before, it does not change the fact that most of the tools that we were previously familiar with as data scientists are not always able to analyze data samples of the size.
In summary, Hive provides us a great tool to extract huge amounts of data; however, the toolbox of data science, or statistical learning, methods that we as data scientists are used to using cannot easily accommodate the new larger data sets without substantial changes.
Many different software packages have been developed or are under development for both supervised and unsupervised learning on large data sets.
Some of these software packages are stand alone software implementations, such as Vowpal Wabbit and BBR, while others are implementations within a larger infrastructure such as Mahout for Hadoop or the multitude of “large data” packages for R.
A portion of these algorithms take advantage of parallel programing approaches while others rely on different methods to achieve scalability.
The primary tool for statistical learning for several of the data scientists in our team is R.
It provides a large array of packages that are able to perform many statistical learning methods.
More importantly, we have a lot of experience with it, know how its packages perform, understand their features, and are very familiar with its documentation.
However, one major drawback of R is that by default it loads the entire data set into memory.
This is a major limitation considering that the majority of the data sets that we extract from Hive and are able to analyze today are much larger than what can fit in memory.
Moreover, once the data in R is larger than what is able to fit in memory, the system will start swapping, which leads to the system thrashing and massive decreases in processing speed.1 In no way are we advocating ignoring the new tools that are available.
Obviously, it is important to take advantage of the best of these scalable technologies, but only so much time can be spent investigating and testing new technology.
So now we are left with a choice of either using the new tools that are available for large data sets or downsampling our data to fit into the tools that we are more familiar with.
If we decide to use the new tools, we can gain signal by letting our data learn off of more data, and as a result the variance in our estimates will decrease.
This is particularly appealing in situations where the outcome is very rare.
However, learning these new tools takes time and there is an opportunity cost of using that time to learn new tools rather than answering other questions that have particular value to the company.
Alternatively, we can downsample the data to obtain something that can fit in the old tools we have at our disposal, but must deal with a loss of signal and increased variance in our estimates.
However, this allows us to deal with tools with which we are familiar and the features that they provide.
Thus, we are able to retain the functionality of our current toolbox at the price of losing some signal.
In this case study, we highlight a way that we can both retain the functionality of the current toolbox as well as gain signal, or decrease variance, by using a larger sample, or all, of the data available to us.
Figure 23-1 shows the probability of converting versus the score from an algorithm designed to rank prospects for an advertising campaign.
This plot clearly shows that the top individuals are converting at a lower rate than some of the lower scoring browsers.
Considering that some campaigns only target a very small percentage of the overall population, it is important the best prospects are among the top scorers.
For the purpose of this case study it can be thought of as a black box that produces for each score a prediction of the conversion rate.
For more details on the use of generalized additive models (GAM), see Hastie et al.
The R package used to implement the GAM for the purpose of the analysis presented here is the mgcv package available at http://cran.r-project.org/
The new ranking can be generated in the following way.
First, extract the scores for each individual browser and then follow them for some designated period of time, say five days, and record if they took the desired action, and thus converted.
Consider a Hive table called scoretable that has the following information and is partitioned on date and subpartitioned by offer.
The following query can then be used to extract a set of data from scoretable for use in R to estimate the GAM line that predicts conversion for different levels of score like in the preceding table:
This data is then loaded into R and the following code is used to create the predicted conversion probability versus score, as in the preceding table:
The issue with this approach is that it only can be used for a limited number of days of data because the data set gets too large and R begins thrashing for any more than three days of data.
Moreover, it takes approximately 10 minutes of time for each campaign to do this for about three days of data.
By simply extracting the data from Hive in a slightly different way and making use of the feature of the gam function in mgcv that allows for frequency weights, the same.
This is done by selecting the data from Hive by rounding the score to the nearest hundredth and getting frequency weights for each rounded score, convert combination by using a GROUP BY query.
This is a very common approach for dealing with large data sets and in the case of these scores there should be no loss of signal due to rounding because there is no reason to believe that individuals with scores that differ by less than 0.001 are any different from each other.
The resulting data set is significantly smaller than the original approach presented that does not use frequency weights.
In fact, the initial data set for each offer consisted of millions of records, and this new data set consists of approximately 6,500 rows per offer.
The new data is then loaded into R and the following code may be used to generate the new GAM results:
This increase in speed was also realized while using more than twice the amount of data resulting in more precise estimates of the predicted conversion probabilities.
In summary, the frequency weights allowed for a more precise estimate of the GAM in significantly less time.
In the presented case study, we showed how by rounding the continuous variables and grouping like variables with frequency weights, we were both able to get more precise estimates by using more data and fewer computational resources, resulting in quicker estimates.
The example shown was for a model with a single feature, score.
In general, this is an approach that will work well for a low number of features or a larger number of sparse features.
The above approach may be extended to higher dimensional problems as well using some other small tricks.
One way this can be done for a larger number of variables is by bucketing the variables, or features, into binary variables and then using GROUP BY queries and frequency weights for those features.
You order the whole data set by some criteria and limit the result set to N.
But there are times when you need to group like elements together and find the top N elements within that group only.
Several database platforms define a rank() function that can support these scenarios, but until Hive provides an implementation, we can create a user-defined function to produce the results we want.
We will call this function p_rank() for psuedorank, leaving the name rank() for the Hive implementation.
Say we have the following product sales data and we want to see the top three items per category and country:
To achieve the same result using HiveQL, the first step is partitioning the data into groups, which we can achieve using the DISTRIBUTE BY clause.
We must ensure that all rows with the same category and country are sent to the same reducer:
The next step is ordering the data in each group by descending sales using the SORT BY clause.
While ORDER BY effects a total ordering across all data, SORT BY affects the ordering of data on a specific reducer.
You must repeat the partition columns named in the DISTRIBUTE BY clause:
The subquery t1 organizes the data so that all data belonging to the same category and country are sorted by descending sales count.
The outermost query filters the rank to be in the top three:
The function remembers the previous arguments, and so long as the successive arguments match, it increments and returns the rank.
Whenever the arguments do not match, the function resets the rank back to 1 and starts over.
This is just one simple example of how p_rank() can be used.
Or, if you precalculate the counts of products in each category and country, you can use p_rank() to calculate percentiles using a join.
Please know that p_rank() is not a direct substitute for rank() because there will be differences in some circumstances.
It is public domain so feel free to use, improve, and modify it to suit your needs:
At M6D, we have such requirements, for example we have hourly and daily process reports using Hadoop and Hive that are business critical and must complete in a timely manner.
However our systems also support data science and sales engineers that periodically run ad hoc reporting.
While using the fair share scheduler and capacity scheduler meets many of our requirements, we need more isolation than schedulers can provide.
Also, because HDFS has no snapshot or incremental backup type features, we require a solution that will prevent an accidental delete or drop table operations from destroying data.
Data can have a replication factor of two or three on the primary deployment and additionally be replicated to a second deployment.
This decision allows us to have guaranteed resources dedicated to our time-sensitive production process as well as our ad hoc users.
Additionally, we protected against any accidental drop tables or data deletes.
This design does incur some overhead in having to administer two deployments and setup and administer the replication processes, but this overhead is justified in our case.
A given table zz_mid_set exists on Production and we wish to be able to query it from Research without having to transfer the data between clusters using distcp.
Generally, we try to avoid this because it breaks our isolation design but it is nice to know that this can be done.
Use the describe extended command to determine the columns of a table as well as its location:
On the second cluster, craft a second CREATE TABLE statement with the same columns.
Create the second table as EXTERNAL, in this way if the table is dropped on the second cluster the files are not deleted on the first cluster.
Notice that for the location we specified a full URI.
In fact, when you specify a location as a relative URI, Hive stores it as a full URI:
It is important to note that this cross-deployment access works because both clusters have network access to each other.
The TaskTrackers of the deployment we submit the job to will have to be able to access the NameNode and DataNodes of the other deployment.
This is done by scheduling tasks to run on nodes where the data is located.
Which means a general performance decrease and network usage increase.
Replicating Hadoop and Hive data is easier than replicating a traditional database.
Unlike a database running multiple transactions that change the underlying data frequently, Hadoop and Hive data is typically “write once.” Adding new partitions does not change the existing ones, and typically new partitions are added on time-based intervals.
Early iterations of replication systems were standalone systems that used distcp and generated Hive statements to add partitions on an interval.
When we wanted to replicate a new table, we could copy an existing program and make changes for different tables and partitions.
Over time we worked out a system that could do this in a more automated manner without having to design a new process for each table to replicate.
The process that creates the partition also creates an empty HDFS file named:
If it finds a file, it looks up the table and partition in Hive’s metadata.
First, we do some checking to make sure the table is defined in the source and destination metastores:
Using the database and table name we can look up the location information inside the metastore.
We then do a sanity check to ensure the information does not already exist:
Hadoop DistCP is not necessarily made to be run programmatically.
However, we can pass a string array identical to command-line arguments to its main function.
After, we check to confirm the returned result was a 0:
Outbrain by David Funk Outbrain is the leading content-discovery platform.
In-Site Referrer Identification Sometimes, when you’re trying to aggregate your traffic, it can be tricky to tell where it’s actually coming from, especially for traffic coming from elsewhere in your site.
If you have a site with a lot of URLs with different structures, you can’t simply check that the referrer URLs match the landing page.
What we want is to correctly group each referrer as either In-site, Direct, or Other.
That way you can tell your internal traffic apart from Google searches to your site, and so on and so forth.
If the referrer is blank or null, we’ll label it as Direct.
From here on out, we’ll assume that all our URLs are already parsed down to the host or domain, whatever level of granularity you’re aiming for.
Personally, I like using the domain because it’s a little simpler.
That said, Hive only has a host function, but not domain.
If you just have the raw URLs, there are a couple of options.
The important thing is that we’re going to be using these to look for matches, so just make your choice based on your own criteria.
Now, we can convert each pageview’s URL to the appropriate class.
But what if we’re an ad network? What if we have hundreds of sites? What if each of the sites could have any number of URL structures? If that’s the case, we probably also have a table that has each URL, as well as what site it belongs to.
What we want to do is go through each referrer URL and see if it matches with anything of the same site ID.
We use the outer join in this case, because we expect there to be some external referrers that won’t match, and this will let them through.
Then, we just catch any cases that did match, and if there were any, we know they came from somewhere in the site.
Counting Uniques Let’s say you want to calculate the number of unique visitors you have to your site/ network/whatever.
We’ll use a ridiculously simple schema for our hypothetical table, daily_users:
However, if you have too many users and not enough machines in your cluster, it might begin to have trouble counting users over a month:
In all likelihood, your cluster is probably able to make it through the map phase without too much problems, but starts having issues around the reduce phase.
The problem is that it’s able to access all the records but it can’t count them all at once.
Of course, you can’t count them day by day, either, because there might be some redundancies.
Counting uniques is O(n), where n is the number of records, but it has a high constant factor.
We could maybe come up with some clever way to cut that down a little bit, but it’s much easier to cut down your n.
While it’s never good to have a high O(n), most of the real problems happen further along.
So, if each day has m entries, and an average of x redundancies, our first query would have n= 31*m.
We can reduce this to n=31*(m–x) by building a temp table to save deduped versions for each day.
Then we write a template version of a query to run over each day, and update it to our temp table.
I like to refer to these as “metajobs,” so let’s call this mj_01.sql:
Next, we write a script that marks this file up, runs it, and repeats it for every date in a range.
Run the script, and you’ve got a table with a n=31*(m-x)
Now, you can query the deduped table without as big a reduce step to get through.
If that’s not enough, you can then dedupe sets of dates, maybe two at a time, whatever the interval that works for you.
If you still have trouble, you could hash your user IDs into different classes, maybe based on the first character, to shrink n even further.
The basic idea remains, if you limit the size of your n, a high O(n) isn’t as big of a deal.
Sessionization For analyzing web traffic, we often want to be able to measure engagement based on various criteria.
One way is to break up user behavior into sessions, chunks of activity that represent a single “use.” A user might come to your site several times a day, a few days a month, but each visit is certainly not the same.
So, what is a session? One definition is a string of activity, not separated by more than 30 minutes.
That is, if you go to your first page, wait five minutes, go to the second page, it’s the same session.
Wait 30 minutes exactly until the third page, still the same session.
Wait 31 minutes until that fourth page, and the session will be broken; rather than the fourth pageview, it would be the first page of the second session.
Once we’ve got these broken out, we can look at properties of the session to see what happened.
The ubiquitous case is to compare referrers to your page by session length.
So, we might want to find out if Google or Facebook give better engagement on your site, which we might measure by session length.
At first glance, this seems perfect for an iterative process.
For each pageview, keep counting backwards until you find the page that was first.
Identify which pageviews are the session starters, or “origin” pages.
For every pageview, bucket it in with the correct origin page.
Label each origin page, then calculate engagement for each session.
This leaves a table where each row represents a full session, which you can then query for whatever you want to find out.
Most of this is pretty straightforward, though I will mention that st_pageview_id is basically a unique ID to represent each transaction, in this case a pageview.
Otherwise, it could be confusing if you happened to have multiple views of the same page.
For the purposes of this example, the timestamp will be in terms of seconds.
What we want to do is count up how many times, for each pageview.
Then, anything with a count of zero must be an origin case.
In order to do this, we need to compare every pageview that could precede it.
This is a pretty expensive move, as it requires performing a cross-product.
To prevent this from blowing up to unmanageable size, we should group everything on criteria that limits it as much as possible.
In this case, it’s just the user ID, but if you have a large network of independent sites, you might also want to group based on each source, as well:
That’s a bit much, isn’t it? The important part is to count the flags that are not of a session origin, which is where we define c_nonorigin_flags.
Basically, counting up how many reasons why it isn’t the session starter.
Plus, since this is a cross-product, it prevents a false positive by using the candidate as its own qualifier.
Which is probably a good reason to start on step two, finding which pageview belongs to which origin.
It’s pretty simple to do this, every pageview’s origin must be the one immediately prior to it.
For this, we take another big join to check for the minimum difference between a pageview’s timestamp and all the potential origin pageviews:
Again, we use the idea of qualifiers and candidates, in this case b are the candidates for every qualifier a.
An origin candidate can’t come later than the pageview, so for every case like that, we want to find the absolute latest origin that meets that criteria.
The null is irrelevant, because we are guaranteed to have a minimum, because there is always at least one possible origin (even if it’s itself)
This doesn’t give us the origin, but it gives us the timestamp, which we can use as a fingerprint for what the origin should be.
From here, it’s just a matter of matching up this timestamp with all the other potential origins, and we know which origin each pageview belongs to.
It’s worth mentioning that this isn’t the only way to identify the origin pageviews.
You could do it based on the referrer, labeling any external referrer, homepage URL, or blank referrer (indicating direct traffic) as a session origin.
You could base it on an action, only measuring activity after a click.
There are plenty of options, but the important thing is simply to identify what the session origins are.
Step three, where we aggregate on origins, is really, really simple.
For each origin, count up how many pageviews match to it:
Now, this last step we could have avoided by keeping all the qualitative info about a pageview, particularly the origins, in one of the earlier steps.
However, if you have a lot of details you want to pay attention to, it can sometimes be easier to add it in at the end.
Now, with our final table, we can do whatever we want.
Let’s say we want to check the number of sessions, average pageviews per session, weighted average pageviews per session, and the max or min.
We could pick whatever criteria we want, or none at all, but in this case, let’s do it by referrer URL so we can find out the answer to which traffic source gives the best engagement.
And, just for kicks, let’s also check who gives us the most unique users:
We could check which page URL gives the best engagement, figure out who the power users are, whatever.
Once we’ve got it all in a temp table, especially with a more complete set of qualitative attributes, we can answer all sorts of questions about user engagement.
Facilitate the evaluation and analysis of regional climate model simulation outputs via the availability of the reference data sets of quality-controlled observations and assimilations especially from spaceborne sensors, an efficient database structure, a collection of computational tools for calculating the metrics for model evaluation metrics and diagnostics, and relocatable and friendly user interfaces.
Easily bring together a number of complex, and heterogeneous software tools and capability for data access, representation, regridding, reformatting, and visualization so that the end product such as a bias plot can be easily delivered to the end user.
Support regional assessments of climate variability, and impacts, needed to inform decision makers (e.g., local governments, agriculture, state government, hydrologists) so that they can make critical decisions with large financial and societal impact.
Elastically scaling up, performing a regional study that requires specific remote sensing data, and climate model output data, performing a series of analyses, and then destroying that particular instance of the system.
Reading the diagram from left to right, available reference data sets from observations and assimilations, especially from satellite-based remote sensing, enters the system according to the desired climate parameters useful for climate model evaluation.
Those parameters are stored in various mission data sets, and those data sets are housed in several external repositories, eventually fed into the database component (RCMED: Regional Climate Model Evaluation Database) of RCMES.
As an example, AIRS is NASA’s Atmospheric Infrared Sounder and provides parameters including surface air temperature, temperature, and geopotential; MODIS is NASA’s Moderate Imaging Spectroradiometer and provides parameters including cloud fraction; and TRMM is NASA’s Tropical Rainfall Measurement Mission and provides parameters including monthly precipitation.
Data sets are loaded into the RCMED using the Apache OODT extractor framework and the desired parameters, their values, spatial and temporal constraints (and optionally height) are loaded and potentially transformed (e.g., normalized, put on same coordinate system, converted from unit values) into a MySQL database.
The data loaded into that MySQL database, RCMED, is exposed to external clients via a Space/ Time query and subsetting web service; the description of which is a topic of a separate.
For all intents and purposes, it provides the same capabilities that the OPeNDAP technology does.
It provides users with the ability to take in the reference data from RCMED and climate model output data produced elsewhere and to re-grid these datasets in order to match them spatially and temporally in preparation for the comparison of the reference and model data for the evaluation of model output against the user-selected reference data.
At that point, the system allows for seasonal cycle compositing (e.g., all Januaries, or all summer months for N years), and for preparing the data for eventual metrics calculation, that is, comparison of the values of the model output against the remote sensing data observation parameters and their values.
The system supports several metrics, such as bias computation, Root Mean Squared Error (RMSE), and the generation of relevant visualizations, including traditional plots and Taylor diagrams for science use/decision making.
Our Experience: Why Hive? So, where does Hive come in to play? After loading 6 billion rows of (latitude, longitude, time, data point value, height) tuples into MySQL, the system fell down and.
We did notice a high I/O wait on the nodes no matter how many task trackers we ran.
The numbers suggested by the community member suggested that we needed roughly 16 mappers in the Hadoop job to match with the I/O performance of a local non-Hadoop task.
Based on the above feedback, we were able to tune our Hive cluster for RCMES to respond to a count query benchmark, and to a space/time query from RCMET for billions of rows in under 15 seconds, using the above-mentioned resources, making Hive a viable and great choice for our system architecture.
We leveraged Hive during a case study wherein we wanted to explore cloud-based technology alternatives to MySQL, and configuration requirements needed to make it scale to the level of tens of billions of rows, and to elastically destroy and re-create the data stored within.
Hive did a great job of meeting our system needs and we are actively looking for more ways to closely integrate it into the RCMES system.
Photobucket Photobucket is the largest dedicated photo-hosting service on the Internet.
Started in 2003 by Alex Welch and Darren Crystal, Photobucket quickly became one of the most popular sites on the Internet and attracted over one hundred million users and billions of stored and shared media.
User and system data is spread across hundreds of MySQL instances, thousands of web servers, and petabytes of filesystem.
Big Data at Photobucket Prior to 2008, Photobucket didn’t have a dedicated analytics system in-house.
Questions from the business users were run across hundreds of MySQL instances and the results aggregated manually in Excel.
In 2008, Photobucket embarked on implementing its first data warehouse dedicated to answering the increasingly complex data questions being asked by a fast-growing company.
The first iteration of the data warehouse was built using an open source system with a Java SQL optimizer and a set of underlying PostGreSQL databases.
The previous system worked well into 2009, but the shortcomings in the architecture became quickly evident.
Working data sets quickly became larger than the available memory; coupled.
In 2009, we started to investigate systems that would allow us to scale out, as the amount of data continued to grow and still meet our SLA with the business users.
Hadoop quickly became the favorite for consuming and analyzing the terabytes of data generated daily by the system, but the difficulty of writing MapReduce programs for simple ad hoc questions became a negative factor for full implementation.
Thankfully, Facebook open sourced Hive a few weeks later and the barriers to efficiently answering ad hoc business questions were quickly smashed.
Here are a few examples of why we chose Hadoop and Hive:
What’s in Hive? The primary goal of Hive at Photobucket is to provide answers about business functions, system performance, and user activity.
To meet these needs, we store nightly dumps of MySQL data sets from across hundreds of servers, terabytes of logfiles from web servers and custom log formats ingested through Flume.
This data helps support many groups throughout the company, such as executive management, advertising, customer support, product development, and operations just to name a few.
For historical data, we keep the partition of all data created on the first day of the month for MySQL data and 30+ days of log files.
Photobucket uses a custom ETL framework for migrating MySQL data into Hive.
Log file data is streamed into HDFS using Flume and picked up by scheduled Hive processes.
Who Does It Support? Executive management relies on Hadoop to provide reports surrounding the general health of the business.
Hive allows us to parse structured database data and unstructured click stream data and distill the data into a format requested by the business stakeholder.
Advertising operations uses Hive to sift through historical data for forecast and define quotas for ad targeting.
Product development is far and away the group generating the largest number of ad hoc queries.
As with any user base, segments change and evolve over time.
Hive is important because it allows us to run A/B tests across current and historical data to gauge relevancy of new products in a quickly changing user environment.
Providing our users with a best-in-class system is the most important goal at Photobucket.
From an operations perspective, Hive is used to generate rollup data partitioned across multiple dimensions.
Knowing the most popular media, users, and referring domains is important for many levels across the company.
A single user can quickly consume large amounts of system resources, significantly increasing monthly expenditures.
Hive is used to identify and analyze rogue users; to determine which ones are within our Terms of Service and which are not.
Operations also uses Hive to run A/B tests defining new hardware requirements and generating ROI calculations.
Hive’s ability to abstract users from underlying MapReduce code means questions can be answered in hours or days instead of weeks.
SimpleReach by Eric Lubow At SimpleReach, we use Cassandra to store our raw data from all of our social network polling.
The format of the row key is an account ID (which is a MongoDB ObjectId) and a content item ID (witha MD5 hash of the URL of the content item being tracked) separated by an underscore which we split on to provide that data in the result set.
The columns in the row are composite columns that look like the ones below:
In order for us to be able to query on composite columns, we need to know the hex value of the column name.
Once the column name is converted to hexadecimal format, Hive queries are run against it.
The first part of the query is the LEFT SEMI JOIN, which is used to mimic a SQL subselect.
All the references to SUBSTR and INSTR are to handle the case of composite columns.
The SUBSTR is used for matching as part of the Ruby function.
This means that the passed in values can be matched to a part of the column name.
The goal of this query is to export our data from Cassandra into a CSV file to give aggregated data dumps to our publishers.
It is done via a Resque (offline) job that is kicked off through our Rails stack.
Having a full CSV file means that all columns in the header must be accounted for in the Hive query (meaning that zeros need to be put to fill places where there is no data)
We do that by pivoting our wide rows into fixed column tables using the CASE statement.
The output of the query is a comma-separated value (CSV) file, an example of which is below (wrapped for length with a blank line between each record for clarity):
Introduction For over 18 months, Karmasphere has been engaged with a fast-growing number of companies who adopted Hadoop and immediately gravitated towards Hive as the optimal way for teams of analysts and business users to use existing SQL skills with the Hadoop environment.
The first part of this chapter provides use case techniques that we’ve seen used repeatedly in customer environments to advance Hive-based analytics.
As companies we’ve worked with plan for and move into production use of Hive, they look for incremental capabilities that make Hive-based access to Hadoop even easier to use, more productive, more powerful, and available to more people in their organization.
When they wire Hadoop and Hive into their existing data architectures, they also want to enable results from Hive queries to be systematized, shared and integrated with other data stores, spreadsheets, BI tools, and reporting systems.
Easier ways to ingest data, detect raw formats, and create metadata.
Finer-grain control over data, table, and column security, and compartmentalized.
Scheduling of queries for automated result generation and export to non-Hadoop.
Integration with Microsoft Excel, Tableau, Spotfire, and other spreadsheet, reporting systems, dashboards, and BI tools.
Ability to manage Hive-based assets including queries, results, visualizations, and.
One recurring question from many Hive users revolves around the format of their data and how to make that available in Hive.
Many data formats are supported out-of-the-box in Hive but some custom proprietary formats are not.
And some formats that are supported raise questions for Hive users about how to extract individual components from within a row of data.
Sometimes, writing a standard Hive SerDe that supports a custom data format is the optimal approach.
In other cases, using existing Hive delimiters and exploiting Hive UDFs is the most convenient solution.
One representative case we worked on was with a company using Hadoop and Hive to provide personalization services from the analysis of multiple input data streams.
They were receiving logfiles from one of their data providers.
They were trying to figure out a way to parse the data and run queries without writing a custom SerDe.
The data had top header level information and multiple detailed information.
The detailed section was a JSON nested within the top level object, similar to the data set below:
After talking with the customer, we realized they were interested in splitting individual columns of the detailed information that was tagged with “data” identifier in the above sample.
To help them proceed, we used existing Hive function get_json_object as shown below: First step is to create a table using the sample data:
Then using Hive functions such as get_json_object, we could get to the nested JSON element and parse it using UDFs:
Extract the nested JSON object identified by data in the inner query as col0
Then the JSON object is split into appropriate columns using their names in the.
Using partitions with data being streamed or regularly added to Hadoop is a use case we see repeatedly, and a powerful and valuable way of harnessing Hadoop and Hive to analyze various kinds of rapidly additive data sets.
Web, application, product, and sensor logs are just some of the types of data that Hive users often want to perform ad hoc, repeated, and scheduled queries on.
Hive partitions, when set up correctly, allow users to query data only in specific partitions and hence improves performance significantly.
To set up partitions for a table, files should be located in directories as given in this example:
With the above structure, tables can be set up with partition by year, month, and day.
Queries can use yr, mon, and day as columns and restrict the data accessed to specific values during query time.
If you notice the folder names, partitioned folders have identifiers such as yr= , mon=, and day=
Working with one high tech company, we discovered that their folders did not have this explicit partition naming and they couldn’t change their existing directory structure.
In this case, we can still add partitions by explicitly adding the location of the absolute path to the table using ALTER TABLE statements.
The output of the script is a set of Hive SQL statements generated using the existing directory structure and captured into a simple text file.
When these statements are executed in Hive, the data in the specified directories automatically become available under defined logical partitions created using ALTER TABLE statements.
You should make sure that your table is created with PARTITIONED BY columns for year, month, and day.
Many companies we work with have text analytics use cases which vary from simple to complex.
Understanding and using Hive regex functions, n-gram functions and other string functions can address a number of those use cases.
One large manufacturing customer we worked with had lot of machine-generated compressed text data being ingested into Hadoop.
Multiple rows of data in each file and a number of such files in time-partitioned buckets.
Within each row there were a number of segments separated by /r/n (carriage return and line feed)
Each segment was in the form of a “name: value” pair.
Read each row and separate individual segments as name-value pairs.
Zero in on specific segments and look for word counts and word patterns for analyzing keywords and specific messages.
The sample below illustrates this customer’s data (text elided for space):
Within each row there are three subsections, including “name,” “description,” and “type” separated by /r/n.
First step is to create the initial table with this sample data:
In the following, we run a series of queries, starting with a simple query and adding functions as we iterate.
Note that the requirement can be met with queries written in several different ways.
The purpose of the queries shown below is to demonstrate some of the key capabilities in Hive around text parsing.
First, we use a split function to separate each section of data into an array of individual elements:
Next, we explode the splits (array) into individual lines using the LATERAL VIEW EXPLODE function.
Results of this query will have name-value pairs separated into individual rows.
The function LTRIM is also used to remove left spaces.
Now we separate the description line into name-value pair and select only the value data.
We use split by : and choose the value parameter:
Notice the use of temporary identifiers tmp1 for the inner query.
This is required when you use the output of a subquery as the input to outer query.
At the end of step three, we have the value of the description segment within each row.
Notice that we have used functions such as lower to convert to lowercase and sentences to tokenize each word in the text.
Hive adoption continues to grow, as outlined by the use cases defined above.
Companies across different industry segments and various sizes have benefited immensely by leveraging Hive in their Hadoop environments.
A strong and active community of contributors and significant investments in Hive R&D efforts by leading Hadoop vendors ensures that Hive, already the SQL-based standard for Hadoop, will become the SQL-based standard within organizations that are leveraging Hadoop for Big Data analysis.
As companies invest significant resources and time in understanding and building Hive resources, in many cases we find they look for additional capabilities that enable them to build on their initial use of Hive and extend its reach faster and more broadly within their organizations.
From working with these customers looking to take Hive to the next level, a common set of requirements have emerged.
These requirements include: Collaborative multiuser environments Hadoop enables new classes of analysis that were prohibitive computationally and economically with traditional RDBMS technologies.
Hadoop empowers organizations to break down the data and people silos, performing analysis on every byte of data they can get their hands on, doing this all in a way that enables them to share their queries, results, and insights with other individuals, teams, and systems in the organization.
This model implies that users with deep understanding of these different data sets need to collaborate in discovery, in the sharing of insights, and the availability of all Hivebased analytic assets across the organization.
Productivity enhancements The current implementation of Hive offers a serial batch environment on Hadoop to run queries.
This implies that once a user submits a query for job execution to the Hadoop cluster, they have to wait for the query to complete execution before they can execute another query against the cluster.
One major reason for companies adopting Hive is that it enables their SQL-skilled data professionals to move faster and more easily to working with Hadoop.
These users are usually familiar with graphical SQL editors in tools and BI products.
They are looking for similar productivity enhancements like syntax highlighting and code completion.
Managing Hive assets A recent McKinsey report predicted significant shortage of skilled workers to enable organizations to profit from their data.
However, organizations are realizing that just having Hive available to their users is not enough.
They need to be able to manage Hive assets like queries (history and versions), UDFs, SerDes for later share and reuse.
Organizations would like to build this living knowledge repository of Hive assets that is easily searchable by users.
Extending Hive for advanced analytics Many companies are looking to re-create analysis they perform in the traditional RDBMS world in Hadoop.
While not all capabilities in the SQL environment easily translate into Hive functions, due to inherent limitations of how data is stored, there are some advanced analytics functions like RANKING, etc., that are Hadoop-able.
In addition, organizations have spent tremendous resources and time in building analytical models using traditional tools like SAS and SPSS and would like the ability to score these models on Hadoop via Hive queries.
Extending Hive beyond the SQL skill set As Hadoop is gaining momentum in organizations and becoming a key fabric of data processing and analytics within IT infrastructure, it is gaining popularity amongst users with different skill sets and capabilities.
While Hive is easily adopted by users with SQL skill sets, other less SQL savvy users are also looking for drag-and-drop capabilities like those available in traditional BI tools to perform analysis on Hadoop using Hive.
The ability to support interactive forms on top of Hive, where a user is prompted to provide column values via simple web-based forms is an often-asked for capability.
Data exploration capabilities Traditional database technologies provide data exploration capabilities; for example, a user can view min, max values for an integer column.
In addition, users can also view visualizations of these columns to understand the data distribution before they perform analysis on the data.
As Hadoop stores hundreds of terabytes of data, and often petabytes, similar capabilities are being requested by customers for specific use cases.
Schedule and operationalize Hive queries As companies find insights using Hive on Hadoop, they are also looking to operationalize these insights and schedule them to run on a regular interval.
While open source alternatives are currently available, these sometimes fall short when companies also want to manage the output of Hive queries; for example, moving result sets into a traditional RDBMS system or BI stack.
To manage certain use cases, companies often have to manually string together various different open source tools or rely on poor performing JDBC connectors.
Karmasphere is a software company, based in Silicon Valley California, focused exclusively on bringing native Hadoop Big Data Analytics capabilities to teams of analysts and business users.
Their flagship product, Karmasphere 2.0, is based on Apache Hive, extending it in a multi-user graphical workspace to enable:
Social, project-based big data analytics for teams of analysts and business users.
Heuristic-based recognition and table creation of many popular data formats.
Easy integration with traditional spreadsheets, reporting, dashboard, and BI tools.
Hive features survey We’d like to get feedback on the importance of these needs and share them back with the growing Hive community.
If you are interested in seeing what others think and would like to participate, please visit: http://karmasphere.com/hive-features-survey.html.
Avro Avro is a new serialization format developed to address some of the common problems associated with evolving other serialization formats.
Some of the benefits are: rich data structures, fast binary format, support for remote procedure calls, and built-in schema evolution.
Bash The “Bourne-Again Shell” that is the default interactive command shell for Linux and Mac OS X systems.
A user may have many buckets, analogous to the root of a physical hard drive.
Command-Line Interface The command-line interface (CLI) can run “scripts” of Hive statements or all the user to enter statements interactively.
Data Warehouse A repository of structured data suitable for analysis for reports, trends, etc.
Warehouses are batch mode or offline, as opposed to providing real-time responsiveness for online activity, like ecommerce.
Derby A lightweight SQL database that can be embedded in Java applications.
It runs in the same process and saves its data to local files.
It is used as the default SQL data store for Hive’s metastore.
Dynamic Partitions A HiveQL extension to SQL that allows you to insert query results into table partitions where you leave one or more partition column values unspecified and they are determined dynamically from the query results themselves.
This technique is convenient for partitioning a query result into a potentially large number of partitions in a new table, without having to write a separate query for each partition column value.
Ephemeral Storage In the nodes for a virtual Amazon EC2 cluster, the on-node disk storage is called ephemeral because it will vanish when the cluster is shut down, in contrast to a physical cluster that is shut down.
ExternalTable A table using a storage location and contents that are outside of Hive’s control.
It is convenient for sharing data with other tools, but it is up to other processes to manage the life cycle of the data.
That is, when an external table is created, Hive does not create the.
Hadoop Distributed File System (HDFS) A distributed, resilient file system for data storage that is optimized for scanning large contiguous blocks of data on hard disks.
Distribution across a cluster provides horizontal scaling of data storage.
Blocks of HDFS files are replicated across the cluster (by default, three times) to prevent data loss when hard drives or whole servers fail.
HBase The NoSQL database that uses HDFS for durable storage of table data.
HBase is a column-oriented, key-value store designed to provide traditional responsiveness for queries and row-level updates and insertions.
Column oriented means that the data storage is organized on disk by groups of columns, called column families, rather than by row.
Key-value means that rows are stored and fetched by a unique key and the value is the entire row.
HBase does not provide an SQL dialect, but Hive can be used to query HBase tables.
Hive A data warehouse tool that provides table abstractions on top of data resident in HDFS, HBase tables, and other stores.
Input Format The input format determines how input streams, usually from files, are split into records.
A custom input format can be specified when creating a table using the INPUTFORMAT clause.
The input format for the default STORED AS TEXTFILE specification is.
Job In the Hadoop context, a job is a self-contained workflow submitted to MapReduce.
It encompasses all the work required to perform a complete calculation, from reading input to generating output.
The MapReduce JobTracker will decompose the job into one or more tasks for distribution and execution around the cluster.
JobTracker The top-level controller of all jobs using Hadoop’s MapReduce.
The JobTracker accepts job submissions, determines what tasks to run and where to run them, monitors their execution, restarts failed tasks as needed, and provides a web console for monitoring job and task execution, viewing logs, etc.
Job Flow A term used in Amazon Elastic MapReduce (EMR) for the sequence of jobs executed on a temporary EMR cluster to accomplish a particular goal.
Map The mapping phase of a MapReduce process where an input set of key-value pairs are converted into a new set of key-value pairs.
For each input key-value pair, there can be zero-to-many output key-value pairs.
The input and output keys and the input and output values can be completely different.
MapReduce A computation paradigm invented at Google and based loosely on the common data operations of mapping a collection of data elements from one form to another (the map phase) and reducing a collection to a single value or a smaller collection (the reduce phase)
MapReduce is designed to scale computation horizontally by decomposing map and reduce steps into tasks and distributing those tasks across a cluster.
The MapReduce runtime provided by Hadoop handles decomposition of a job into tasks, distribution around the cluster, movement of a particular task to the machine that holds the data for the task, movement of data to tasks (as needed), and automated reexecution of failed tasks and other error recovery and logging services.
Metastore The service that maintains “metadata” information, such as table schemas.
By default, it uses a built-in Derby SQL server, which provides limited, single-process SQL support.
Production systems must use a fullservice relational database, such as MySQL.
NoSQL An umbrella term for data stores that don’t support the relational model for data management, dialects of the structured query language, and features like transactional updates.
These data stores trade off these features in order to provide more cost-effective scalability, higher availability, etc.
Output Format The output format determines how records are written to output streams, usually to files.
A custom output format can be specified when creating a table using the OUTPUTFORMAT clause.
Partition A subset of a table’s data set where one column has the same value for all records in the subset.
In Hive, as in most databases that support partitioning, each partition is stored in a physically separate location—in Hive’s case, in a subdirectory of the root directory for the table.
The column value corresponding to a partition doesn’t have to be repeated in every record in the partition, saving space, and queries with WHERE clauses that restrict the result set to specific values for the partition columns can perform more quickly, because they avoid scanning the directories of nonmatching partition values.
Reduce The reduction phase of a MapReduce process where the key-value pairs from the map phase are processed.
A crucial feature of MapReduce is that all the key-value pairs from all the map tasks that have the same key will be sent together to the same reduce task, so that the collection of values can be “reduced” as appropriate.
For example, a collection of integers might be added or averaged together, a collection of strings might have all duplicates removed, etc.
Relational Model The most common model for database management systems, it is based on a logical model of data organization and manipulation.
A declarative specification of the data structure and how it should be manipulated is supplied by the user, most typically using the Structured Query Language.
It can be used with or instead of HDFS when running MapReduce jobs.
It is also used to create those record bytes (i.e., serialization)
In contrast, the Input Format is used to split an input stream into records and the Output Format is used to write records to an output stream.
A SerDe can be specified when a Hive table is created.
The default SerDe supports the field and collection separators discussed in “Text File Encoding of Data Values” on page 45, as well as various optimizations such as a lazy parsing.
Structured Query Language A language that implements the relational model for querying and manipulating data.
While there is an ANSI standard for SQL that has undergone periodic revisions, all SQL dialects in widespread use add their own custom extensions and variations.
Task In the MapReduce context, a task is the smallest unit of work performed on a single cluster node, as part of an overall job.
Each map and reduce invocation will have its own task.
Remote processes can send Hive statements to Hive through Thrift.
User-Defined Aggregate Functions User-defined functions that take multiple rows (or columns from multiple rows) and return a single “aggregation” of the data, such as a count of the rows, a sum or average.
User-Defined Functions Functions implemented by users of Hive to extend their behavior.
Sometimes the term is used generically to include built-in functions and sometimes the term is used for the specific case of functions that work on a single row (or columns in a row) and return a single value, (i.e., which don’t change the number of records)
See also user-defined aggregate functions and user-defined table generating functions.
User-Defined Table Generating Functions User-defined functions that  take a column from a single record and expand it into multiple rows.
Examples include the explode function that converts an array into rows of single fields and, for Hive v0.8.0 and later, converts a map into rows of key and value fields.
We’d like to hear your suggestions for improving our indexes.
About the Authors Edward Capriolo is currently System Administrator at Media6degrees, where he helps design and maintain distributed data storage systems for the Internet advertising industry.
Edward is a member of the Apache Software Foundation and a committer for the Hadoop-Hive project.
He has experience as a developer, as well as a Linux and network administrator, and enjoys the rich world of open source software.
Dean is a frequent speaker at industry and academic conferences on these topics.
Colophon The animal on the cover of Programming Hive is a European hornet (Vespa cabro) and its hive.
The European hornet is the only hornet in North America, introduced to the continent when European settlers migrated to the Americas.
This hornet can be found throughout Europe and much of Asia, adapting its hive-building techniques to different climates when necessary.
The hornet is a social insect, related to bees and ants.
The hornet’s hive consists of one queen, a few male hornets (drones), and a large quantity of sterile female workers.
The chief purpose of drones is to reproduce with the hornet queen, and they die soon after.
It is the female workers who are responsible for building the hive, carrying food, and tending to the hornet queen’s eggs.
The hornet’s nest itself is the consistency of paper, since it is constructed out of wood pulp in several layers of hexagonal cells.
The end result is a pear-shaped nest attached to its shelter by a short stem.
In colder areas, hornets will abandon the nest in the winter and take refuge in hollow logs or trees, or even human houses, where the queen and her eggs will stay until the warmer weather returns.
The eggs form the start of a new colony, and the hive can be constructed once again.
Counting Uniques Why this is a problem Load a temp table Querying the temp table.
Sessionization Setting it up Finding origin pageviews Bucketing PVs to origins Aggregating on origins Aggregating on origin type Measure engagement.
