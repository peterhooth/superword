No part of this book may be reproduced in any form or by any means, without permission in writing from the publisher.
The authors and publisher of this book have used their best efforts in preparing this book.
These efforts include the development, research, and testing of the theories and programs to determine their effectiveness.
The authors and publisher make no warranty of any kind, expressed or implied, with regard to these programs or to the documentation contained in this book.
The authors and publisher shall not be liable in any event for incidental or consequential damages in connection with, or arising out of, the furnishing, performance, or use of these programs.
No part of this book may be reproduced, in any form or by any means, without permission in writing from the publisher.
Other operating systems have an animal mascot, so we felt MINIX 3 ought to have one too.
We chose the raccoon because raccoons are small, cute, clever, agile, eat bugs, and are user-friendly—at least if you keep your garbage can well locked.
Most books on operating systems are strong on theory and weak on practice.
This one aims to provide a better balance between the two.
It covers all the fundamental principles in great detail, including processes, interprocess communication, semaphores, monitors, message passing, scheduling algorithms, input/output, deadlocks, device drivers, memory management, paging algorithms, file system design, security, and protection mechanisms.
But it also discusses one particular system—MINIX 3—a UNIX-compatible operating system in detail, and even provides a source code listing for study.
This arrangement allows the reader not only to learn the principles, but also to see how they are applied in a real operating system.
When the first edition of this book appeared in 1987, it caused something of a small revolution in the way operating systems courses were taught.
With the appearance of MINIX, many schools began to have laboratory courses in which students examined a real operating system to see how it worked inside.
We consider this trend highly desirable and hope it continues.
In addition, MINIX was ported to many other platforms, including the Macintosh, Amiga, Atari, and SPARC.
A second edition of the book, covering this system, was published in 1997 and was widely used at universities.
The popularity of MINIX has continued, as can be observed by examining the number of hits for MINIX found by Google.
This third edition of the book has many changes throughout.
Nearly all of the material on principles has been revised, and considerable new material has been added.
The design of MINIX 3 was inspired by the observation that operating systems are becoming bloated, slow, and unreliable.
They crash far more often than other electronic devices such as televisions, cell phones, and DVD players and have so many features and options that practically nobody can understand them fully or manage them well.
And of course, computer viruses, worms, spyware, spam, and other forms of malware have become epidemic.
To a large extent, many of these problems are caused by a fundamental design flaw in current operating systems: their lack of modularity.
The entire operatng system is typically millions of lines of C/C++ code compiled into a single massive executable program run in kernel mode.
A bug in any one of those millions of lines of code can cause the system to malfunction.
Getting all this code correct is impossible, especially when about 70% consists of device drivers, written by third parties, and outside the purview of the people maintaining the operating system.
With MINIX 3, we demonstrate that this monolithic design is not the only possibility.
The rest of the system, including all the device drivers (except the clock driver), is a collection of small, modular, user-mode processes, each of which is tightly restricted in what it can do and with which other processes it may communicate.
In any event, this design makes it much easier for students to learn how an operating system works than attempting to study a huge monolithic system.
The CD-ROM that is included in this book is a live CD.
You can put it in your CD-ROM drive, reboot the computer, and MINIX 3 will give a login prompt within a few seconds.
You can log in as root and give the system a try without first having to install it on your hard disk.
Of course, it can also be installed on the hard disk.
As suggested above, MINIX 3 is rapidly evolving, with new versions being issued frequently.
To download the current CD-ROM image file for burning, please go to the official Website: www.minix3.org.
This site also contains a large amount of new software, documentation, and news about MINIX 3 development.
For discussions about MINIX 3, or to ask questions, there is a USENET newsgroup: comp.os.minix.
People without newsreaders can follow discussions on the Web at http://groups.google.com/group/comp.os.minix.
As an alternative to installing MINIX 3 on your hard disk, it is possible to run it on any one of several PC simulators now available.
Some of these are listed on the main page of the Website.
Instructors who are using the book as the text for a university course can get the problem solutions from their local Prentice Hall representative.
We have been extremely fortunate in having the help of many people during the course of this project.
First and foremost, Ben Gras and Jorrit Herder have done most of the programming of the new version.
They did a great job under tight time constraints, including responding to e-mail well after midnight on many occasions.
They also read the manuscript and made many useful comments.
Kees Bot also helped greatly with previous versions, giving us a good base to work with.
Kees wrote large chunks of code for versions up to 2.0.4, repaired bugs, and answered numerous questions.
Philip Homburg wrote most of the networking code as well as helping out in numerous other useful ways, especially providing detailed feedback on the manuscript.
People too numerous to list contributed code to the very early versions, helping to get MINIX off the ground in the first place.
There were so many of them and their contributions have been so varied that we cannot even begin to list them all here, so the best we can do is a generic thank you to all of them.
Several people read parts of the manuscript and made suggestions.
It’s kind of getting to be routine, but the love and support is still much appreciated.
It is still a delight to have a son who understands and cares about the things that fascinate me.
Without its software, a computer is basically a useless lump of metal.
With its software, a computer can store, process, and retrieve information; play music and videos; send e-mail, search the Internet; and engage in many other valuable activities to earn its keep.
Computer software can be divided roughly into two kinds: system programs, which manage the operation of the computer itself, and application programs, which perform the actual work the user wants.
The most fundamental system program is the operating system, whose job is to control all the computer’s resources and provide a base upon which the application programs can be written.
In particular, an operating system called MINIX 3 is used as a model, to illustrate design principles and the realities of implementing a design.
A modern computer system consists of one or more processors, some main memory, disks, printers, a keyboard, a display, network interfaces, and other input/output devices.
Writing programs that keep track of all these components and use them correctly, let alone optimally, is an extremely difficult job.
If every programmer had to be concerned with how disk drives work, and with all the dozens of things that could go wrong when reading a disk block, it is unlikely that many programs could be written at all.
Many years ago it became abundantly clear that some way had to be found to shield programmers from the complexity of the hardware.
The way that has evolved gradually is to put a layer of software on top of the bare hardware, to manage all parts of the system, and present the user with an interface or virtual.
The placement of the operating system is shown in Fig.
At the bottom is the hardware, which, in many cases, is itself composed of two or more levels (or layers)
The lowest level contains physical devices, consisting of integrated circuit chips, wires, power supplies, cathode ray tubes, and similar physical devices.
How these are constructed and how they work is the province of the electrical engineer.
A computer system consists of hardware, system programs, and application programs.
Next comes the microarchitecture level, in which the physical devices are grouped together to form functional units.
Typically this level contains some registers internal to the CPU (Central Processing Unit) and a data path containing an arithmetic logic unit.
In each clock cycle, one or two operands are fetched from the registers and combined in the arithmetic logic unit (for example, by addition or Boolean AND)
On some machines, the operation of the data path is controlled by software, called the microprogram.
On other machines, it is controlled directly by hardware circuits.
The purpose of the data path is to execute some set of instructions.
Some of these can be carried out in one data path cycle; others may require multiple data path cycles.
Together, the hardware and instructions visible to an assembly language programmer form the ISA (Instruction Set Architecture) This level is often called machine language.
In this level, the input/output devices are controlled by loading values into special device registers.
For example, a disk can be commanded to read by loading the values of the disk address, main memory address, byte count, and direction (read or write) into its registers.
Furthermore, for many I/O (Input/Output) devices, timing plays an important role in the programming.
A major function of the operating system is to hide all this complexity and give the programmer a more convenient set of instructions to work with.
For example, read block from file is conceptually much simpler than having to worry about the details of moving disk heads, waiting for them to settle down, and so on.
On top of the operating system is the rest of the system software.
It is important to realize that these programs are definitely not part of the operating system, even though they are typically supplied preinstalled by the computer manufacturer, or in a package with the operating system if it is installed after purchase.
The operating system is (usually) that portion of the software that runs in kernel mode or supervisor mode.
It is protected from user tampering by the hardware (ignoring for the moment some older or low-end microprocessors that do not have hardware protection at all)
If a user does not like a particular compiler, he† is free to write his own if he so chooses; he is not free to write his own clock interrupt handler, which is part of the operating system and is normally protected by hardware against attempts by users to modify it.
This distinction, however, is sometimes blurred in embedded systems (which may not have kernel mode) or interpreted systems (such as Java-based systems that use interpretation, not hardware, to separate the components)
Still, for traditional computers, the operating system is what runs in kernel mode.
That said, in many systems there are programs that run in user mode but which help the operating system or perform privileged functions.
For example, there is often a program that allows users to change their passwords.
This program is not part of the operating system and does not run in kernel mode, but it clearly carries out a sensitive function and has to be protected in a special way.
In some systems, including MINIX 3, this idea is carried to an extreme form, and pieces of what is traditionally considered to be the operating system (such as the file system) run in user space.
In such systems, it is difficult to draw a clear boundary.
Everything running in kernel mode is clearly part of the operating system, but some programs running outside it are arguably also part of it, or at least closely associated with it.
For example, in MINIX 3, the file system is simply a big C program running in user-mode.
He’’ should be read as ‘‘he or she’’ throughout the book.
Most computer users have had some experience with an operating system, but it is difficult to pin down precisely what an operating system is.
Part of the problem is that operating systems perform two basically unrelated functions, extending the machine and managing resources, and depending on who is doing the talking, you hear mostly about one function or the other.
As mentioned earlier, the architecture (instruction set, memory organization, I/O, and bus structure) of most computers at the machine language level is primitive and awkward to program, especially for input/output.
To make this point more concrete, let us briefly look at how floppy disk I/O is done using the NEC PD765 compatible controller chips used on many Intel-based personal computers.
These commands are for reading and writing data, moving the disk arm, and formatting tracks, as well as initializing, sensing, resetting, and recalibrating the controller and the drives.
If you do not understand this mumbo jumbo, do not worry; that is precisely the point—it is rather esoteric.
As if this were not enough, the floppy disk programmer must also be constantly aware of whether the motor is on or off.
If the motor is off, it must be turned on (with a long startup delay) before data can be read or written.
The motor cannot be left on too long, however, or the floppy disk will wear out.
The programmer is thus forced to deal with the trade-off between long startup delays versus wearing out floppy disks (and losing the data on them)
Without going into the real details, it should be clear that the average programmer probably does not want to get too intimately involved with the programming of floppy disks (or hard disks, which are just as complex and quite different)
Instead, what the programmer wants is a simple, high-level abstraction to deal with.
In the case of disks, a typical abstraction would be that the disk contains a collection of named files.
Each file can be opened for reading or writing, then read or written, and finally closed.
Details such as whether or not recording should use modified frequency modulation and what the current state of the motor is should not appear in the abstraction presented to the user.
The program that hides the truth about the hardware from the programmer and presents a nice, simple view of named files that can be read and written is, of course, the operating system.
Just as the operating system shields the programmer from the disk hardware and presents a simple file-oriented interface, it also conceals a lot of unpleasant business concerning interrupts, timers, memory management, and other low-level features.
In each case, the abstraction offered by the operating system is simpler and easier to use than that offered by the underlying hardware.
In this view, the function of the operating system is to present the user with the equivalent of an extended machine or virtual machine that is easier to program than the underlying hardware.
How the operating system achieves this goal is a long story, which we will study in detail throughout this book.
To summarize it in a nutshell, the operating system provides a variety of services that programs can obtain using special instructions called system calls.
We will examine some of the more common system calls later in this chapter.
The concept of the operating system as primarily providing its users with a convenient interface is a top-down view.
An alternative, bottom-up, view holds that the operating system is there to manage all the pieces of a complex system.
Modern computers consist of processors, memories, timers, disks, mice, network interfaces, printers, and a wide variety of other devices.
In the alternative view, the job of the operating system is to provide for an orderly and controlled allocation of the processors, memories, and I/O devices among the various programs competing for them.
Imagine what would happen if three programs running on some computer all tried to print their output simultaneously on the same printer.
The operating system can bring order to the potential chaos by buffering all the output destined for the printer on the disk.
When one program is finished, the operating system can then copy its output from the disk file where it has been stored to the printer, while at the same time the other program can continue generating more output, oblivious to the fact that the output is not really going to the printer (yet)
When a computer (or network) has multiple users, the need for managing and protecting the memory, I/O devices, and other resources is even greater, since the users might otherwise interfere with one another.
In short, this view of the operating system holds that its primary task is to keep track of who is using which resource, to grant resource requests, to account for usage, and to mediate conflicting requests from different programs and users.
Resource management includes multiplexing (sharing) resources in two ways: in time and in space.
When a resource is time multiplexed, different programs or users take turns using it.
First one of them gets to use the resource, then another, and so on.
For example, with only one CPU and multiple programs that want to run on it, the operating system first allocates the CPU to one program, then after it has run long enough, another one gets to use the CPU, then another, and then eventually the first one again.
Determining how the resource is time multiplexed—who goes next and for how long—is the task of the operating system.
When multiple print jobs are queued up for printing on a single printer, a decision has to be made about which one is to be printed next.
Instead of the customers taking turns, each one gets part of the resource.
For example, main memory is normally divided up among several running programs, so each one can be resident at the same time (for example, in order to take turns using the CPU)
Assuming there is enough memory to hold multiple programs, it is more efficient to hold several programs in memory at once rather than give one of them all of it, especially if it only needs a small fraction of the total.
Of course, this raises issues of fairness, protection, and so on, and it is up to the operating system to solve them.
Another resource that is space multiplexed is the (hard) disk.
In many systems a single disk can hold files from many users at the same time.
Allocating disk space and keeping track of who is using which disk blocks is a typical operating system resource management task.
Since operating systems have historically been closely tied to the architecture of the computers on which they run, we will look at successive generations of computers to see what their operating systems were like.
This mapping of operating system generations to computer generations is crude, but it does provide some structure where there would otherwise be none.
The first true digital computer was designed by the English mathematician Charles Babbage (1792–1871)
Although Babbage spent most of his life and fortune trying to build his ‘‘analytical engine,’’ he never got it working properly because it was purely mechanical, and the technology of his day could not produce the required wheels, gears, and cogs to the high precision that he needed.
Needless to say, the analytical engine did not have an operating system.
After Babbage’s unsuccessful efforts, little progress was made in constructing.
Presper Eckert and John Mauchley at the University of Pennsylvania, and Konrad Zuse in Germany, among others, all succeeded in building calculating engines.
The first ones used mechanical relays but were very slow, with cycle times measured in seconds.
These machines were enormous, filling up entire rooms with tens of thousands of vacuum tubes, but they were still millions of times slower than even the cheapest personal computers available today.
In these early days, a single group of people designed, built, programmed, operated, and maintained each machine.
All programming was done in absolute machine language, often by wiring up plugboards to control the machine’s basic functions.
The usual mode of operation was for the programmer to sign up for a block of time on the signup sheet on the wall, then come down to the machine room, insert his or her plugboard into the computer, and spend the next few hours hoping that none of the 20,000 or so vacuum tubes would burn out during the run.
Virtually all the problems were straightforward numerical calculations, such as grinding out tables of sines, cosines, and logarithms.
By the early 1950s, the routine had improved somewhat with the introduction of punched cards.
It was now possible to write programs on cards and read them in instead of using plugboards; otherwise, the procedure was the same.
The introduction of the transistor in the mid-1950s changed the picture radically.
Computers became reliable enough that they could be manufactured and sold to paying customers with the expectation that they would continue to function long enough to get some useful work done.
For the first time, there was a clear separation between designers, builders, operators, programmers, and maintenance personnel.
These machines, now called mainframes, were locked away in specially airconditioned computer rooms, with staffs of specially-trained professional operators to run them.
Only big corporations or major government agencies or universities could afford their multimillion dollar price tags.
To run a job (i.e., a program or set of programs), a programmer would first write the program on paper (in FORTRAN or possibly even in assembly language), then punch it on cards.
He would then bring the card deck down to the input room and hand it to one of the operators and go drink coffee until the output was ready.
When the computer finished whatever job it was currently running, an operator would go over to the printer and tear off the output and carry it over to the output room, so that the programmer could collect it later.
Then he would take one of the card decks that had been brought from the input room and read it in.
If the FORTRAN compiler was needed, the operator would have to get it from a file cabinet and read it in.
Much computer time was wasted while operators were walking around the machine room.
Given the high cost of the equipment, it is not surprising that people quickly looked for ways to reduce the wasted time.
The idea behind it was to collect a tray full of jobs in the input room and then read them onto a magnetic tape using a small (relatively) inexpensive computer, such as the IBM 1401, which was very good at reading cards, copying tapes, and printing output, but not at all good at numerical calculations.
Other, much more expensive machines, such as the IBM 7094, were used for the real computing.
After about an hour of collecting a batch of jobs, the tape was rewound and brought into the machine room, where it was mounted on a tape drive.
The operator then loaded a special program (the ancestor of today’s operating system), which read the first job from tape and ran it.
The output was written onto a second tape, instead of being printed.
After each job finished, the operating system automatically read the next job from the tape and began running it.
When the whole batch was done, the operator removed the input and output tapes, replaced the input tape with the next batch, and brought the output tape to a 1401 for printing off line (i.e., not connected to the main computer)
The structure of a typical input job is shown in Fig.
It started out with a $JOB card, specifying the maximum run time in minutes, the account number to be charged, and the programmer’s name.
Then came a $FORTRAN card, telling the operating system to load the FORTRAN compiler from the system tape.
Compiled programs were often written on scratch tapes and had to be loaded explicitly.
Next came the $RUN card, telling the operating system to run the program with the data following it.
Finally, the $END card marked the end of the job.
These primitive control cards were the forerunners of modern job control languages and command interpreters.
Large second-generation computers were used mostly for scientific and engineering calculations, such as solving the partial differential equations that often occur in physics and engineering.
By the early 1960s, most computer manufacturers had two distinct, and totally.
On the one hand there were the word-oriented, large-scale scientific computers, such as the 7094, which were used for numerical calculations in science and engineering.
Developing, maintaining, and marketing two completely different product lines was an expensive proposition for the computer manufacturers.
The machines differed only in price and performance (maximum memory, processor speed, number of I/O devices permitted, and so forth)
Since all the machines had the same architecture and instruction set, programs written for one machine could run on all the others, at least in theory.
Furthermore, the 360 was designed to handle both scientific (i.e., numerical) and commercial computing.
Thus a single family of machines could satisfy the needs of all customers.
The 360 was the first major computer line to use (small-scale) Integrated Circuits (ICs), thus providing a major price/performance advantage over the secondgeneration machines, which were built up from individual transistors.
It was an immediate success, and the idea of a family of compatible computers was soon adopted by all the other major manufacturers.
The descendants of these machines are still in use at computer centers today.
Nowadays they are often used for managing huge databases (e.g., for airline reservation systems) or as servers for World Wide Web sites that must process thousands of requests per second.
The greatest strength of the ‘‘one family’’ idea was simultaneously its greatest weakness.
The intention was that all software, including the operating system, OS/360, had to work on all models.
It had to be good on systems with few peripherals and on systems with many peripherals.
It had to work in commercial environments and in scientific environments.
Above all, it had to be efficient for all of these different uses.
There was no way that IBM (or anybody else) could write a piece of software to meet all those conflicting requirements.
The result was an enormous and extraordinarily complex operating system, probably two to three orders of magnitude larger than FMS.
It consisted of millions of lines of assembly language written by thousands of programmers, and contained thousands upon thousands of bugs, which necessitated a continuous stream of new releases in an attempt to correct them.
Each new release fixed some bugs and introduced new ones, so the number of bugs probably remained constant in time.
While it would be impossible to summarize the book here, suffice it to say that the cover shows a herd of prehistoric beasts stuck in a tar pit.
Despite its enormous size and problems, OS/360 and the similar thirdgeneration operating systems produced by other computer manufacturers actually satisfied most of their customers reasonably well.
They also popularized several key techniques absent in second-generation operating systems.
On the 7094, when the current job paused to wait for a tape or other I/O operation to complete, the CPU simply sat idle until the I/O finished.
With heavily CPU-bound scientific calculations, I/O is infrequent, so this wasted time is not significant.
The solution that evolved was to partition memory into several pieces, with a different job in each partition, as shown in Fig.
While one job was waiting for I/O to complete, another job could be using the CPU.
If enough jobs could be held in main memory at once, the CPU could be kept busy nearly 100 percent of the time.
Having multiple jobs safely in memory at once requires special hardware to protect each job against snooping and mischief by the other ones, but the 360 and other third-generation systems were equipped with this hardware.
Another major feature present in third-generation operating systems was the.
Then, whenever a running job finished, the operating system could load a new job from the disk into the now-empty partition and run it.
This technique is called spooling (from Simultaneous Peripheral Operation On Line) and was also used for output.
With spooling, the 1401s were no longer needed, and much carrying of tapes disappeared.
Although third-generation operating systems were well suited for big scientific calculations and massive commercial data processing runs, they were still basically batch systems.
Many programmers pined for the first-generation days when they had the machine all to themselves for a few hours, so they could debug their programs quickly.
With third-generation systems, the time between submitting a job and getting back the output was often hours, so a single misplaced comma could cause a compilation to fail, and the programmer to waste half a day.
This desire for quick response time paved the way for timesharing, a variant of multiprogramming, in which each user has an online terminal.
After the success of the CTSS system, MIT, Bell Labs, and General Electric (then a major computer manufacturer) decided to embark on the development of a ‘‘computer utility,’’ a machine that would support hundreds of simultaneous timesharing users.
Their model was the electricity distribution system—when you need electric power, you just stick a plug in the wall, and within reason, as much power as you need will be there.
The designers of this system, known as MULTICS (MULTiplexed Information and Computing Service), envisioned one huge machine providing computing power for everyone in the Boston area.
It was designed to support hundreds of users on a machine only slightly more powerful than an Intel 80386-based PC, although it had much more I/O capacity.
This is not quite as crazy as it sounds, since people knew how to write small, efficient programs in those days, a skill that has subsequently been lost.
There were many reasons that MULTICS did not take over the world, not the least of which is that it was written in PL/I, and the PL/I compiler was years late and barely worked at all when it finally arrived.
In addition, MULTICS was enormously ambitious for its time, much like Charles Babbage’s analytical engine in the nineteenth century.
The phrase ‘‘computer utility’’ is no longer heard, but the idea has gained new life in recent years.
In its simplest form, PCs or workstations (high-end PCs) in a business or a classroom may be connected via a LAN (Local Area Network) to a file server on which all programs and data are stored.
An administrator then has to install and protect only one set of programs and data, and can easily reinstall local software on a malfunctioning PC or workstation without worrying about retrieving or preserving local data.
In more heterogeneous environments, a class of software called middleware has evolved to bridge the gap between local users and the files, programs, and databases they use on remote servers.
Middleware makes networked computers look local to individual users’ PCs or workstations and presents a consistent user interface even though there may be a wide variety of different servers, PCs, and workstations in use.
A web browser presents documents to a user in a uniform way, and a document as seen on a user’s browser can consist of text from one server and graphics from another server, presented in a format determined by a style sheet on yet another server.
Businesses and universities commonly use a web interface to access databases and run programs on a computer in another building or even another city.
Middleware appears to be the operating system of a distributed system, but it is not really an operating system at all, and is beyond the scope of this book.
For more on distributed systems see Tanenbaum and Van Steen (2002)
For certain kinds of nonnumerical work, it was almost as fast as the 7094 and gave birth to a whole new industry.
It was quickly followed by a series of other PDPs (unlike IBM’s family, all incompatible) culminating in the PDP-11
One of the computer scientists at Bell Labs who had worked on the MULTICS project, Ken Thompson, subsequently found a small PDP-7 minicomputer that no one was using and set out to write a stripped-down, one-user version of MULTICS.
This work later developed into the UNIX operating system, which became popular in the academic world, with government agencies, and with many companies.
The history of UNIX has been told elsewhere (e.g., Salus, 1994)
Because the source code was widely available, various organizations developed their own (incompatible) versions, which led to chaos.
These had minor variants as well, now including FreeBSD, OpenBSD, and NetBSD.
In fact, some other operating systems now also support the POSIX interface.
Later in this chapter, when we refer to UNIX, we mean all of these systems as well, unless stated otherwise.
While they differ internally, all of them support the POSIX standard, so to the programmer they are quite similar.
In terms of architecture, personal computers (initially called microcomputers) were not all that different from minicomputers of the PDP-11 class, but in terms of price they certainly were different.
The minicomputer made it possible for a department in a company or university to have its own computer.
The microcomputer made it possible for an individual to have his or her own computer.
Many application programs were written to run on CP/M, and it dominated the personal computing world for about 5 years.
One of these, the Apple II, became a major competitor for CP/M systems in the home and educational markets.
The CP/M cards were sold by a little company called Microsoft, which also had a market niche supplying BASIC interpreters used by a number of microcomputers running CP/M.
CP/M, MS-DOS, and the Apple DOS were all command-line systems: users typed commands at the keyboard.
Years earlier, Doug Engelbart at Stanford Research Institute had invented the GUI (Graphical User Interface), pronounced ‘‘gooey,’’ complete with windows, icons, menus, and mouse.
In 2001 Apple made a major operating system change, releasing Mac OS X, with a new version of the Macintosh GUI on top of Berkeley UNIX.
And in 2005 Apple announced that it would be switching to Intel processors.
Originally Windows was just a graphical environment on top of 16-bit MS-DOS (i.e., it was more like a shell than a true operating system)
However, current versions of Windows are descendants of Windows NT, a full 32-bit system, rewritten from scratch.
The other major contender in the personal computer world is UNIX (and its various derivatives)
It is especially popular on machines powered by high-performance RISC chips.
On Pentium-based computers, Linux is becoming a popular alternative to Windows for students and increasingly many corporate users.
Although many UNIX users, especially experienced programmers, prefer a command-based interface to a GUI, nearly all UNIX systems support a windowing system called the X Window system developed at M.I.T.
This system handles the basic window management, allowing users to create, delete, move, and resize windows using a mouse.
Often a complete GUI, such as Motif, is available to run on top of the X Window system giving UNIX a look and feel something like the Macintosh or Microsoft Windows for those UNIX users who want such a thing.
In a network operating system, the users are aware of the existence of multiple computers and can log in to remote machines and copy files from one machine to another.
Each machine runs its own local operating system and has its own local user (or users)
Network operating systems are not fundamentally different from singleprocessor operating systems.
They obviously need a network interface controller and some low-level software to drive it, as well as programs to achieve remote login and remote file access, but these additions do not change the essential structure of the operating system.
A distributed operating system, in contrast, is one that appears to its users as a traditional uniprocessor system, even though it is actually composed of multiple processors.
The users should not be aware of where their programs are being run or where their files are located; that should all be handled automatically and efficiently by the operating system.
True distributed operating systems require more than just adding a little code to a uniprocessor operating system, because distributed and centralized systems differ in critical ways.
Distributed systems, for example, often allow applications to run on several processors at the same time, thus requiring more complex processor scheduling algorithms in order to optimize the amount of parallelism.
Communication delays within the network often mean that these (and other) algorithms must run with incomplete, outdated, or even incorrect information.
This situation is radically different from a single-processor system in which the operating system has complete information about the system state.
John Lions, of the University of New South Wales in Australia, even wrote a little booklet describing its operation, line by line (Lions, 1996)
This booklet was used (with permission of AT&T) as a text in many university operating system courses.
Many universities complied by simply dropping the study of UNIX and teaching only theory.
Unfortunately, teaching only theory leaves the student with a lopsided view of what an operating system is really like.
The theoretical topics that are usually covered in great detail in courses and books on operating systems, such as scheduling algorithms, are in practice not really that important.
Subjects that really are important, such as I/O and file systems, are generally neglected because there is little theory about them.
To remedy this situation, one of the authors of this book (Tanenbaum) decided to write a new operating system from scratch that would be compatible with UNIX from the user’s point of view, but completely different on the inside.
By not using even one line of AT&T code, this system avoided the licensing restrictions, so it could be used for class or individual study.
It was called MINIX and was released in 1987 with its complete source code for anyone to study or modify.
The name MINIX stands for mini-UNIX because it is small enough that even a nonguru can understand how it works.
In addition to the advantage of eliminating the legal problems, MINIX had another advantage over UNIX.
It was written a decade after UNIX and was structured in a more modular way.
For instance, from the very first release of MINIX the file system and the memory manager were not part of the operating system at all but ran as user programs.
In the current release (MINIX 3) this modularization has been extended to the I/O device drivers, which (with the exception of the clock driver) all run as user programs.
Another difference is that UNIX was designed to be efficient; MINIX was designed to be readable (inasmuch as one can speak of any program hundreds of pages long as being readable)
The MINIX code, for example, has thousands of comments in it.
Version 7 was used as the model because of its simplicity and elegance.
It is sometimes said that Version 7 was an improvement not only over all its predecessors, but also over all its successors.
With the advent of POSIX, MINIX began evolving toward the new standard, while maintaining backward compatibility with existing programs.
This kind of evolution is common in the computer industry, as no vendor wants to introduce a new system that none of its existing customers can use without great upheaval.
The version of MINIX described in this book, MINIX 3, is based on the POSIX standard.
Like UNIX, MINIX was written in the C programming language and was intended to be easy to port to various computers.
As MINIX grew in functionality and size, it eventually got to the point that a hard disk was needed for PCs, but in keeping with the MINIX philosophy, a 200-MB partition is sufficient (for embedded applications, no hard disk is required though)
In contrast, even small Linux systems require 500-MB of disk space, and several GB will be needed to install common applications.
To the average user sitting at an IBM PC, running MINIX is similar to running UNIX.
All of the basic programs, such as cat, grep, ls, make, and the shell are present and perform the same functions as their UNIX counterparts.
Like the operating system itself, all these utility programs have been rewritten completely from scratch by the author, his students, and some other dedicated people, with no AT&T or other proprietary code.
Development continued slowly but systematically until 2004, when Tanenbaum became convinced that software was getting too bloated and unreliable and decided to pick up the slightly-dormant MINIX thread again.
Together with his students and programmers at the Vrije Universiteit in Amsterdam, he produced MINIX 3, a major redesign of the system, greatly restructuring the kernel, reducing its size, and emphasizing modularity and reliability.
The new version was intended both for PCs and embedded systems, where compactness, modularity, and reliability are crucial.
While some people in the group called for a completely new name, it was eventually decided to call it MINIX 3 since the name MINIX was already well known.
By way of analogy, when Apple abandoned it own operating system, Mac OS 9 and replaced it with a variant of Berkeley UNIX, the name chosen was Mac OS X rather than APPLIX or something like that.
Similar fundamental changes have happened in the Windows family while retaining the Windows name.
Small kernel size is important because kernel bugs are far more devastating than bugs in user-mode programs and more code means more bugs.
The actual number of bugs is probably much higher since the researchers could only count reported bugs, not unreported bugs.
This is the reason the device drivers were moved out of the kernel in MINIX 3; they can do less damage in user mode.
Throughout this book MINIX 3 will be used as an example.
Most of the comments about the MINIX 3 system calls, however (as opposed to comments about the actual code), also apply to other UNIX systems.
This remark should be kept in mind when reading the text.
A few words about Linux and its relationship to MINIX may possibly be of interest to some readers.
Shortly after MINIX was released, a USENET newsgroup, comp.os.minix, was formed to discuss it.
Within weeks, it had 40,000 subscribers, most of whom wanted to add vast numbers of new features to MINIX to make it bigger and better (well, at least bigger)
Every day, several hundred of them offered suggestions, ideas, and frequently snippets of source code.
For people who thought little of MS-DOS, the existence of MINIX (with source code) as an alternative was even a reason to finally go out and buy a PC.
One of these people was a Finnish student named Linus Torvalds.
Torvalds installed MINIX on his new PC and studied the source code carefully.
Torvalds wanted to read USENET newsgroups (such as comp.os.minix) on his own PC rather than at his university, but some features he needed were lacking in MINIX, so he wrote a program to do that, but soon discovered he needed a different terminal driver, so he wrote that too.
Then he wanted to download and save postings, so he wrote a disk driver, and then a file system.
Linux has become one of the notable successes of the open source movement (which MINIX helped start)
Linux is challenging UNIX (and Windows) in many environments, partly because commodity PCs which support Linux are now available with performance that rivals the proprietary RISC systems required by some UNIX implementations.
Other open source software, notably the Apache web server and the MySQL database, work well with Linux in the commercial world.
Linux, Apache, MySQL, and the open source Perl and PHP programming languages are often used together on web servers and are sometimes referred to by the acronym LAMP.
For more on the history of Linux and open source software see DiBona et al.
The interface between the operating system and the user programs is defined by the set of ‘‘extended instructions’’ that the operating system provides.
These extended instructions have been traditionally known as system calls, although they can be implemented in several ways.
To really understand what operating systems do, we must examine this interface closely.
The calls available in the interface vary from operating system to operating system (although the underlying concepts tend to be similar)
It’s more work that way, but it gives more insight into what operating systems really do.
For simplicity’s sake, we will refer only to MINIX 3, but the corresponding UNIX and Linux system calls are based on POSIX in most cases.
Before we look at the actual system calls, however, it is worth taking a bird’s-eye view of MINIX 3, to get a general feel for what an operating system is all about.
This overview applies equally well to UNIX and Linux, as mentioned above.
The MINIX 3 system calls fall roughly in two broad categories: those dealing with processes and those dealing with the file system.
A key concept in MINIX 3, and in all operating systems, is the process.
Associated with each process is its address space, a list of memory locations from some minimum (usually 0) to some maximum, which the process can read and write.
The address space contains the executable program, the program’s data, and its stack.
Also associated with each process is some set of registers, including the program counter, stack pointer, and other hardware registers, and all the other information needed to run the program.
We will come back to the process concept in much more detail in Chap.
Periodically, the operating system decides to stop running one process and start running another, for example, because the first one has had more than its share of CPU time in the past second.
When a process is suspended temporarily like this, it must later be restarted in exactly the same state it had when it was stopped.
This means that all information about the process must be explicitly saved somewhere during the suspension.
For example, the process may have several files open for reading at once.
Associated with each of these files is a pointer giving the current position (i.e., the number of the byte or record to be read next)
When a process is temporarily suspended, all these pointers must be saved so that a read call executed after the process is restarted will read the proper data.
In many operating systems, all the information about each process, other than the contents of its own address space, is stored in an operating system table called the process table, which is an array (or linked list) of structures, one for each process currently in existence.
Thus, a (suspended) process consists of its address space, usually called the core image (in honor of the magnetic core memories used in days of yore), and its process table entry, which contains its registers, among other things.
The key process management system calls are those dealing with the creation and termination of processes.
A process called the command interpreter or shell reads commands from a terminal.
The user has just typed a command requesting that a program be compiled.
When that process has finished the compilation, it executes a system call to terminate itself.
On Windows and other operating systems that have a GUI, (double) clicking on a desktop icon launches a program in much the same way as typing its name at the command prompt.
Although we will not discuss GUIs much, they are really simple command interpreters.
If a process can create one or more other processes (usually referred to as child processes) and these processes in turn can create child processes, we quickly arrive at the process tree structure of Fig.
Related processes that are cooperating to get some job done often need to communicate with one another and synchronize their activities.
This communication is called interprocess communication, and will be addressed in detail in Chap.
Other process system calls are available to request more memory (or release unused memory), wait for a child process to terminate, and overlay its program with a different one.
Occasionally, there is a need to convey information to a running process that is not sitting around waiting for it.
For example, a process that is communicating with another process on a different computer does so by sending messages to the remote process over a network.
To guard against the possibility that a message or its reply is lost, the sender may request that its own operating system notify it after a specified number of seconds, so that it can retransmit the message if no acknowledgement has been received yet.
After setting this timer, the program may continue doing other work.
When the specified number of seconds has elapsed, the operating system sends an alarm signal to the process.
The signal causes the process to temporarily suspend whatever it was doing, save its registers on the stack, and start running a special signal handling procedure, for example, to retransmit a presumably lost message.
When the signal handler is done, the running process is restarted in the state it was in just before the signal.
They are generated by a variety of causes in addition to timers expiring.
Many traps detected by hardware, such as executing an illegal instruction or using an invalid address, are also converted into signals to the guilty process.
Each person authorized to use a MINIX 3 system is assigned a UID (User IDentification) by the system administrator.
Every process started has the UID of the person who started it.
A child process has the same UID as its parent.
Users can be members of groups, each of which has a GID (Group IDentification)
One UID, called the superuser (in UNIX), has special power and may violate many of the protection rules.
In large installations, only the system administrator knows the password needed to become superuser, but many of the ordinary users (especially students) devote considerable effort to trying to find flaws in the system that allow them to become superuser without the password.
We will study processes, interprocess communication, and related issues in Chap.
The other broad category of system calls relates to the file system.
As noted before, a major function of the operating system is to hide the peculiarities of the disks and other I/O devices and present the programmer with a nice, clean abstract model of device-independent files.
System calls are obviously needed to create files, remove files, read files, and write files.
Before a file can be read, it must be opened, and after it has been read it should be closed, so calls are provided to do these things.
To provide a place to keep files, MINIX 3 has the concept of a directory as a way of grouping files together.
A student, for example, might have one directory for each course he is taking (for the programs needed for that course), another directory for his electronic mail, and still another directory for his World Wide Web home page.
System calls are then needed to create and remove directories.
Calls are also provided to put an existing file into a directory, and to remove a file from a directory.
This model also gives rise to a hierarchy—the file system—as shown in Fig.
The process and file hierarchies both are organized as trees, but the similarity stops there.
Process hierarchies usually are not very deep (more than three levels is unusual), whereas file hierarchies are commonly four, five, or even more levels deep.
Process hierarchies are typically short-lived, generally a few minutes at most, whereas the directory hierarchy may exist for years.
Typically, only a parent process may control or even access a child process, but mechanisms nearly always exist to allow files and directories to be read by a wider group than just the owner.
Every file within the directory hierarchy can be specified by giving its path name from the top of the directory hierarchy, the root directory.
Such absolute path names consist of the list of directories that must be traversed from the root directory to get to the file, with slashes separating the components.
The leading slash indicates that the path is absolute, that is, starting at the root directory.
Throughout this book we will use the UNIX convention for paths.
At every instant, each process has a current working directory, in which path names not beginning with a slash are looked for.
Processes can change their working directory by issuing a system call specifying the new working directory.
Each field has a bit for read access, a bit for write access, and a bit for execute access.
For example, the protection code rwxr-x--x means that the owner can read, write, or execute the file, other group members can read or execute (but not write) the file, and everyone else can execute (but not read or write) the file.
A dash means that the corresponding permission is absent (the bit is zero)
Another important concept in MINIX 3 is the mounted file system.
Nearly all personal computers have one or more CD-ROM drives into which CD-ROMs can be inserted and removed.
Before the mount call, the root file system, on the hard disk, and a second file system, on a CD-ROM, are separate and unrelated.
However, the file system on the CD-ROM cannot be used, because there is no way to specify path names on it.
Instead, the mount system call allows the file system on the CD-ROM to be attached to the root file system wherever the program wants it to be.
Not being able to access these files is not as serious as it at first seems: file systems are nearly always mounted on empty directories.
If a system contains multiple hard disks, they can all be mounted into a single tree as well.
Another important concept in MINIX 3 is the special file.
Special files are provided in order to make I/O devices look like files.
That way, they can be read and written using the same system calls as are used for reading and writing files.
Two kinds of special files exist: block special files and character special files.
Block special files are normally used to model devices that consist of a collection.
By opening a block special file and reading, say, block 4, a program can directly access the fourth block on the device, without regard to the structure of the file system contained on it.
Similarly, character special files are used to model printers, modems, and other devices that accept or output a character stream.
By convention, the special files are kept in the /dev directory.
The last feature we will discuss in this overview is one that relates to both processes and files: pipes.
A pipe is a sort of pseudofile that can be used to connect two processes, as shown in Fig.
If processes A and B wish to talk using a pipe, they must set it up in advance.
When process A wants to send data to process B, it writes on the pipe as though it were an output file.
Process B can read the data by reading from the pipe as though it were an input file.
Thus, communication between processes in MINIX 3 looks very much like ordinary file reads and writes.
Stronger yet, the only way a process can discover that the output file it is writing on is not really a file, but a pipe, is by making a special system call.
The operating system is the code that carries out the system calls.
Editors, compilers, assemblers, linkers, and command interpreters definitely are not part of the operating system, even though they are important and useful.
At the risk of confusing things somewhat, in this section we will look briefly at the MINIX 3 command interpreter, called the shell.
Although it is not part of the operating system, it makes heavy use of many operating system features and thus serves as a good example of how the system calls can be used.
It is also the primary interface between a user sitting at his terminal and the operating system, unless the user is using a graphical user interface.
All of them support the functionality described below, which derives from the original shell (sh)
When any user logs in, a shell is started up.
The shell has the terminal as standard input and standard output.
It starts out by typing the prompt, a character such as a dollar sign, which tells the user that the shell is waiting to accept a command.
While the child process is running, the shell waits for it to terminate.
When the child finishes, the shell types the prompt again and tries to read the next input line.
The user can specify that standard output be redirected to a file, for example, date >file.
The output of one program can be used as the input for another program by connecting them with a pipe.
The output of sort is redirected to the file /dev/lp, typically the printer.
If a user puts an ampersand after a command, the shell does not wait for it to complete.
The shell has a number of other interesting features, which we do not have space to discuss here.
Most books for UNIX beginners are useful for MINIX 3 users who want to learn more about using the system.
Armed with our general knowledge of how MINIX 3 deals with processes and files, we can now begin to look at the interface between the operating system and its application programs, that is, the set of system calls.
Since the actual mechanics of issuing a system call are highly machine dependent, and often must be expressed in assembly code, a procedure library is provided to make it possible to make system calls from C programs.
It is useful to keep the following in mind: any single-CPU computer can execute only one instruction at a time.
If a process is running a user program in user.
The operating system then figures out what the calling process wants by inspecting the parameters.
Then it carries out the system call and returns control to the instruction following the system call.
In a sense, making a system call is like making a special kind of procedure call, only system calls enter the kernel or other privileged operating system components and procedure calls do not.
To make the system call mechanism clearer, let us take a quick look at read.
It has three parameters: the first one specifying the file, the second one specifying the buffer, and the third one specifying the number of bytes to read.
A call to read from a C program might look like this:
A few other calls exist, but they have very specialized uses so we will omit them here.
In the following sections we will briefly examine each of the calls of Fig.
To a large extent, the services offered by these calls determine most of what the operating system has to do, since the resource management on personal computers is minimal (at least compared to big machines with many users)
This is a good place to point out that the mapping of POSIX procedure calls onto system calls is not necessarily one-to-one.
The POSIX standard specifies a number of procedures that a conformant system must supply, but it does not specify whether they are system calls, library calls, or something else.
In others, several required procedures are only minor variations of one another, and one system call handles all of them.
After the fork, the original process and the copy (the parent and child) go their separate ways.
All the variables have identical values at the time of the fork, but since the parent’s data are copied to create.
Process management pid = fork() Create a child process identical to the parent pid = waitpid(pid, &statloc, opts) Wait for a child to terminate s = wait(&status) Old version of waitpid s = execve(name, argv, envp) Replace a process core image exit(status) Terminate process execution and return status size = brk(addr) Set the size of the data segment pid = getpid() Return the caller’s process id pid = getpgrp() Return the id of the caller’s process group pid = setsid() Create a new session and return its proc.
The program text, which is unchangeable, is shared between parent and child.
The fork call returns a value, which is zero in the child and equal to the child’s process identifier or PID in the parent.
Using the returned PID, the two processes can see which one is the parent process and which one is the child process.
When a command is typed, the shell forks off a new process.
It does this by using the execve system call, which causes its entire core image to be replaced by the file named in its first parameter.
Actually, the system call itself is exec, but several different library procedures call it with different parameters and slightly different names.
A highly simplified shell illustrating the use of fork, waitpid, and execve is shown in Fig.
In the most general case, execve has three parameters: the name of the file to be executed, a pointer to the argument array, and a pointer to the environment.
Various library routines, including execl, execv, execle, and execve, are provided to allow the parameters to be omitted or specified in various ways.
Throughout this book we will use the name exec to represent the system call invoked by all of these.
After the shell has forked, the child process locates and executes the file cp and passes to it the names of the source and target files.
The main program of cp (and main program of most other C programs) contains the declaration.
The second parameter, argv, is a pointer to an array.
Element i of that array is a pointer to the i-th string on the command line.
The third parameter of main, envp, is a pointer to the environment, an array of strings containing assignments of the form name=value used to pass information such as the terminal type and home directory name to a program.
If exec seems complicated, do not despair; it is (semantically) the most complex of all the POSIX system calls.
As an example of a simple one, consider exit, which processes should use when they are finished executing.
The low-order byte of status contains the termination status, with 0 being normal termination and the other values being various error conditions.
Processes in MINIX 3 have their memory divided up into three segments: the text segment (i.e., the program code), the data segment (i.e., the variables), and the stack segment.
The data segment grows upward and the stack grows downward, as shown in Fig.
The stack grows into the gap automatically, as needed, but expansion of the data.
This address may be more than the current value (data segment is growing) or less than the current value (data segment is shrinking)
The parameter must, of course, be less than the stack pointer or the data and stack segments would overlap, which is forbidden.
In this example, all three are in one address space, but separate instruction and data space is also supported.
As a convenience for programmers, a library routine sbrk is provided that also changes the size of the data segment, only its parameter is the number of bytes to add to the data segment (negative parameters make the data segment smaller)
It works by keeping track of the current size of the data segment, which is the value returned by brk, computing the new size, and making a call asking for that number of bytes.
The brk and sbrk calls, however, are not defined by the POSIX standard.
Programmers are encouraged to use the malloc library procedure for dynamically allocating storage, and the underlying implementation of malloc was not thought to be a suitable subject for standardization since few programmers use it directly.
The next process system call is also the simplest, getpid.
Remember that in fork, only the parent was given the child’s PID.
If the child wants to find out its own PID, it must use getpid.
The getpgrp call returns the PID of the caller’s process group.
Sessions are related to an optional feature of POSIX, job control, which is not supported by MINIX 3 and which will not concern us further.
The last process management system call, ptrace, is used by debugging programs to control the program being debugged.
It allows the debugger to read and write the controlled process’ memory and manage it in other ways.
Although most forms of interprocess communication are planned, situations exist in which unexpected communication is needed.
For example, if a user accidently tells a text editor to list the entire contents of a very long file, and then.
In MINIX 3, the user can hit the CTRL-C key on the keyboard, which sends a signal to the editor.
Signals can also be used to report certain traps detected by the hardware, such as illegal instruction or floating point overflow.
When a signal is sent to a process that has not announced its willingness to accept that signal, the process is simply killed without further ado.
To avoid this fate, a process can use the sigaction system call to announce that it is prepared to accept some signal type, and to provide the address of the signal handling procedure and a place to store the address of the current one.
After a sigaction call, if a signal of the relevant type is generated (e.g., by pressing CTRL-C), the state of the process is pushed onto its own stack, and then the signal handler is called.
It may run for as long as it wants to and perform any system calls it wants to.
When the signal handling procedure is done, it calls sigreturn to continue where it left off before the signal.
The sigaction call replaces the older signal call, which is now provided as a library procedure, however, for backward compatibility.
A blocked signal is held pending until it is unblocked.
The sigprocmask call allows a process to define the set of blocked signals by presenting the kernel with a bitmap.
It is also possible for a process to ask for the set of signals currently pending but not allowed to be delivered due to their being blocked.
Finally, the sigsuspend call allows a process to atomically set the bitmap of blocked signals and suspend itself.
It would be undesirable for a SIGINT signal (generated by pressing CTRL-C) to affect the background process, so after the fork but before the exec, the shell does.
Hitting CTRL-C is not the only way to send a signal.
The kill system call allows a process to signal another process (provided they have the same UIDunrelated processes cannot signal each other)
Getting back to the example of background processes used above, suppose a background process is started up, but later it is decided that the process should be terminated.
The solution is to use the kill program, which uses the kill system call to send a signal to any process.
By sending signal 9 (SIGKILL), to a background process, that process can be killed.
For many real-time applications, a process needs to be interrupted after a specific time interval to do something, such as to retransmit a potentially lost packet over an unreliable communication line.
To handle this situation, the alarm system call has been provided.
The parameter specifies an interval, in seconds, after which a SIGALRM signal is sent to the process.
A process may only have one alarm outstanding at any instant.
The first signal is canceled by the second call to alarm.
If the parameter to alarm is zero, any pending alarm signal is canceled.
If an alarm signal is not caught, the default action is taken and the signaled process is killed.
It sometimes occurs that a process has nothing to do until a signal arrives.
It displays some text on the screen and then calls alarm to signal it after 30 seconds.
While the student is reading the text, the program has nothing to do.
It could sit in a tight loop doing nothing, but that would waste CPU time that another process or user might need.
A better idea is to use pause, which tells MINIX 3 to suspend the process until the next signal.
In this section we will look at calls that operate on individual files; in the next one we will examine those that involve directories or the file system as a whole.
To create a new file, the creat call is used (why the call is creat and not create has been lost in the mists of time)
Its parameters provide the name of the file and the protection mode.
Creat not only creates a new file but also opens it for writing, regardless of the file’s mode.
The file descriptor returned, fd, can be used to write the file.
The creat call is obsolete, as open can now create new files, but it has been included for backward compatibility.
Calls to mknod fail unless the caller is the superuser.
The most heavily used calls are undoubtedly read and write.
Although most programs read and write files sequentially, for some applications programs need to be able to access any part of a file at random.
Associated with each file is a pointer that indicates the current position in the file.
When reading (writing) sequentially, it normally points to the next byte to be read (written)
The lseek call changes the value of the position pointer, so that subsequent calls to read or write can begin anywhere in the file, or even beyond the end.
The value returned by lseek is the absolute position in the file after changing the pointer.
For each file, MINIX 3 keeps track of the file mode (regular file, special file, directory, and so on), size, time of last modification, and other information.
Programs can ask to see this information via the stat and fstat system calls.
These differ only in that the former specifies the file by name, whereas the latter takes a file descriptor, making it useful for open files, especially standard input and standard output, whose names may not be known.
Both calls provide as the second parameter a pointer to a structure where the information is to be put.
When manipulating file descriptors, the dup call is occasionally helpful.
The structure used to return information for the stat and fstat system calls.
In the actual code, symbolic names are used for some of the types.
The dup call has a variant that allows an arbitrary unassigned file descriptor to be made to refer to a given open file.
Interprocess communication in MINIX 3 uses pipes, as described earlier.
Figure 1-13 depicts a skeleton procedure that creates two processes, with the output of the first one piped into the second one.
A more realistic example would do error checking and handle arguments.
First a pipe is created, and then the procedure forks, with the parent eventually becoming the first process in the pipeline and the child process becoming the second one.
The parent first closes off the file descriptor for reading from the pipe.
Then it closes standard output and does a DUP call that allows file descriptor 1 to write on the pipe.
The parameter to execl is repeated because the first one is the file to be executed and the second one is the first parameter, which most programs expect to be the file name.
The next system call, ioctl, is potentially applicable to all special files.
It is, for instance, used by block device drivers like the SCSI driver to control tape and CD-ROM devices.
Its main use, however, is with special character files, primarily terminals.
The tcgetattr and tcsetattr library functions use ioctl to change the characters used for correcting typing errors on the terminal, changing the terminal mode, and so forth.
Traditionally, there are three terminal modes, cooked, raw, and cbreak.
Cooked mode is the normal terminal mode, in which the erase and kill characters work normally, CTRL-S and CTRL-Q can be used for stopping and starting terminal output, CTRL-D means end of file, CTRL-C generates an interrupt signal, and CTRL-\ generates a quit signal to force a core dump.
In raw mode, all of these functions are disabled; consequently, every character is passed directly to the program with no special processing.
Furthermore, in raw mode, a read from the terminal will give the program any characters that have been typed, even a partial line, rather than waiting for a complete line to be typed, as in cooked mode.
The erase and kill characters for editing are disabled, as is CTRL-D, but CTRL-S, CTRL-Q, CTRL-C, and CTRL-\ are enabled.
Like raw mode, partial lines can be returned to programs (if intraline editing is turned off there is no need to wait until a whole line has been received—the user cannot change his mind and delete it, as he can in cooked mode)
In this mode there are eleven special characters defined, and input is by lines.
In noncanonical mode a minimum number of characters to accept and a time, specified in units of 1/10th of a second, determine how a read will be satisfied.
Under POSIX there is a great deal of flexibility, and various flags can be set to make noncanonical mode behave like either cbreak or raw mode.
The older terms are more descriptive, and we will continue to use them informally.
Ioctl has three parameters, for example a call to tcsetattr to set terminal parameters will result in.
The access system call is used to determine whether a certain file access is permitted by the protection system.
It is needed because some programs can run using a different user’s UID.
The rename system call is used to give a file a new name.
Finally, the fcntl call is used to control files, somewhat analogous to ioctl (i.e., both of them are horrible hacks)
It has several options, the most important of which is for advisory file locking.
Using fcntl, it is possible for a process to lock and unlock parts of files and test part of a file to see if it is locked.
In this section we will look at some system calls that relate more to directories or the file system as a whole, rather than just to one specific file as in the previous section.
The first two calls, mkdir and rmdir, create and remove empty directories, respectively.
Its purpose is to allow the same file to appear under two or more names, often in different directories.
A typical use is to allow several members of the same programming team to share a common file, with each of them having the file appear in his own directory, possibly under different names.
Sharing a file is not the same as giving every team member a private copy, because having a shared file means that changes that any member of the team makes are instantly visible to the other members—there is only one file.
When copies are made of a file, subsequent changes made to one copy do not affect the other ones.
To see how link works, consider the situation of Fig.
Here are two users, ast and jim, each having their own directories with some files.
If ast now executes a program containing the system call.
Understanding how link works will probably make it clearer what it does.
Every file in UNIX has a unique number, its i-number, that identifies it.
This inumber is an index into a table of i-nodes, one per file, telling who owns the file, where its disk blocks are, and so on.
A directory is simply a file containing a set of (i-number, ASCII name) pairs.
A more complicated structure is needed to support long file names, but conceptually a directory is still a set of (i-number, ASCII name) pairs.
What link does is simply create a new directory entry with.
If either one is later removed, using the unlink system call, the other one remains.
If both are removed, UNIX sees that no entries to the file exist (a field in the i-node keeps track of the number of directory entries pointing to the file), so the file is removed from the disk.
As we have mentioned earlier, the mount system call allows two file systems to be merged into one.
A common situation is to have the root file system containing the binary (executable) versions of the common commands and other heavily used files, on a hard disk.
The user can then insert a CD-ROM with files to be read into the CD-ROM drive.
By executing the mount system call, the CD-ROM file system can be attached to the root file system, as shown in Fig.
A typical statement in C to perform the mount is.
After the mount call, a file on CD-ROM drive 0 can be accessed by just using its path from the root directory or the working directory, without regard to which drive it is on.
In fact, second, third, and fourth drives can also be mounted.
The mount call makes it possible to integrate removable media into a single integrated file hierarchy, without having to worry about which device a file is on.
Although this example involves CD-ROMs, hard disks or portions of hard disks (often called partitions or minor devices) can also be mounted this way.
When a file system is no longer needed, it can be unmounted with the umount system call.
If a block in the cache is modified (by a write on a file) and the system crashes before the modified block is written out to disk, the file system will be damaged.
To limit the potential damage, it is important to flush the cache periodically, so that the amount of data lost by a crash will be small.
The system call sync tells MINIX 3 to write out all the cache blocks that have been modified since being read in.
Two other calls that relate to directories are chdir and chroot.
The former changes the working directory and the latter changes the root directory.
Nine of these bits are the read-write-execute bits for the owner, group, and others.
The chmod system call makes it possible to change the mode of a file.
For example, to make a file read-only by everyone except the owner, one could execute.
Creating a directory uses mknod, which is for the superuser only.
By arranging for the mkdir program to be owned by the superuser and have mode 04755, ordinary users can be given the power to execute mknod but in a highly restricted way.
When a process executes a file that has the SETUID or SETGID bit on in its mode, it acquires an effective UID or GID different from its real UID or GID.
It is sometimes important for a process to find out what its real and effective UID or GID is.
The system calls getuid and getgid have been provided to supply this information.
Each call returns both the real and effective UID or GID, so four library routines are needed to extract the proper information: getuid, getgid, geteuid, and getegid.
The first two get the real UID/GID, and the last two the effective ones.
Ordinary users cannot change their UID, except by executing programs with the SETUID bit on, but the superuser has another possibility: the setuid system call, which sets both the effective and real UIDs.
The superuser can also change the owner of a file with the chown system call.
In short, the superuser has plenty of opportunity for violating all the protection rules, which explains why so many students devote so much of their time to trying to become superuser.
The last two system calls in this category can be executed by ordinary user processes.
The first one, umask, sets an internal bit mask within the system, which is used to mask off mode bits when a file is created.
When a program owned by the root has the SETUID bit on, it can access any file, because its effective UID is the superuser.
Frequently it is useful for the program to know if the person who called the program has permission to access a given file.
If the program just tries the access, it will always succeed, and thus learn nothing.
Although the protection mechanisms of all UNIX-like operating systems are generally similar, there are some differences and inconsistencies that lead to security vulnerabilities.
Time just returns the current time in seconds, with 0 corresponding to Jan.
Of course, the system clock must be set at some point in order to allow it to be read later, so stime has been provided to let the clock be set (by the superuser)
The third time call is utime, which allows the owner of a file (or the superuser) to change the time stored in a file’s i-node.
Application of this system call is fairly limited, but a few programs need it, for example, touch, which sets the file’s time to the current time.
Finally, we have times, which returns the accounting information to a process, so it can see how much CPU time it has used directly, and how much CPU time the system itself has expended on its behalf (handling its system calls)
The total user and system times used by all of its children combined are also returned.
Now that we have seen what operating systems look like on the outside (i.e, the programmer’s interface), it is time to take a look inside.
In the following sections, we will examine five different structures that have been tried, in order to get some idea of the spectrum of possibilities.
These are by no means exhaustive, but they give an idea of some designs that have been tried in practice.
The five designs are monolithic systems, layered systems, virtual machines, exokernels, and client-server systems.
By far the most common organization, this approach might well be subtitled ‘‘The Big Mess.’’ The structure is that there is no structure.
The operating system is written as a collection of procedures, each of which can call any of the other ones whenever it needs to.
When this technique is used, each procedure in the system has a well-defined interface in terms of parameters and results, and each one is free to call any other one, if the latter provides some useful computation that the former needs.
To construct the actual object program of the operating system when this approach is used, one first compiles all the individual procedures, or files containing the procedures, and then binds them all together into a single object file using.
In terms of information hiding, there is essentially none—every procedure is visible to every other procedure (as opposed to a structure containing modules or packages, in which much of the information is hidden away inside modules, and only the officially designated entry points can be called from outside the module)
Even in monolithic systems, however, it is possible to have at least a little structure.
The services (system calls) provided by the operating system are requested by putting the parameters in well-defined places, such as in registers or on the stack, and then executing a special trap instruction known as a kernel call or supervisor call.
This instruction switches the machine from user mode to kernel mode and transfers control to the operating system.
Most CPUs have two modes: kernel mode, for the operating system, in which all instructions are allowed; and user mode, for user programs, in which I/O and certain other instructions are not allowed.
Trap to the kernel Put code for read in register.
The 11 steps in making the system call read(fd, buffer, nbytes)
This is a good time to look at how system calls are performed.
In preparation for calling the read library procedure, which actually makes the read system call, the calling program first pushes the parameters onto the stack, as shown in steps 1–3 in Fig.
The first and third parameters are called by value, but the second parameter is passed by reference, meaning that the address of the buffer (indicated by &) is passed, not the contents of the buffer.
Then comes the actual call to the library procedure (step 4)
This instruction is the normal procedure call instruction used to call all procedures.
The library procedure, possibly written in assembly language, typically puts the system call number in a place where the operating system expects it, such as a register (step 5)
Then it executes a TRAP instruction to switch from user mode to kernel mode and start execution at a fixed address within the kernel (step 6)
The kernel code that starts examines the system call number and then dispatches to the correct system call handler, usually via a table of pointers to system call handlers indexed on system call number (step 7)
At that point the system call handler runs (step 8)
Once the system call handler has completed its work, control may be returned to the user-space library procedure at the instruction following the TRAP instruction (step 9)
This procedure then returns to the user program in the usual way procedure calls return (step 10)
To finish the job, the user program has to clean up the stack, as it does after any procedure call (step 11)
Assuming the stack grows downward, as it often does, the compiled code increments the stack pointer exactly enough to remove the parameters pushed before the call to read.
The program is now free to do whatever it wants to do next.
The system call may block the caller, preventing it from continuing.
For example, if it is trying to read from the keyboard and nothing has been typed yet, the caller has to be blocked.
In this case, the operating system will look around to see if some other process can be run next.
Later, when the desired input is available, this process will get the attention of the system and steps 9–11 will occur.
A set of service procedures that carry out the system calls.
A set of utility procedures that help the service procedures.
In this model, for each system call there is one service procedure that takes care of it.
The utility procedures do things that are needed by several service procedures, such as fetching data from user programs.
This division of the procedures into three layers is shown in Fig.
The first system constructed in this way was the THE system built at the Technische Hogeschool Eindhoven in the Netherlands by E.
Layer 0 dealt with allocation of the processor, switching between processes when interrupts occurred or timers expired.
Above layer 0, the system consisted of sequential processes, each of which could be programmed without having to worry about the fact that multiple processes were running on a single processor.
In other words, layer 0 provided the basic multiprogramming of the CPU.
It allocated space for processes in main memory and on a 512K word drum used for holding parts of processes (pages) for which there was no room in main memory.
Above layer 1, processes did not have to worry about whether they were in memory or on the drum; the.
Layer 2 handled communication between each process and the operator console.
Above this layer each process effectively had its own operator console.
Layer 3 took care of managing the I/O devices and buffering the information streams to and from them.
Above layer 3 each process could deal with abstract I/O devices with nice properties, instead of real devices with many peculiarities.
They did not have to worry about process, memory, console, or I/O management.
A further generalization of the layering concept was present in the MULTICS system.
Instead of layers, MULTICS was organized as a series of concentric rings, with the inner ones being more privileged than the outer ones.
When a procedure in an outer ring wanted to call a procedure in an inner ring, it had to make the equivalent of a system call, that is, a TRAP instruction whose parameters were carefully checked for validity before allowing the call to proceed.
Although the entire operating system was part of the address space of each user process in MULTICS, the hardware made it possible to designate individual procedures (memory segments, actually) as protected against reading, writing, or executing.
Whereas the THE layering scheme was really only a design aid, because all the parts of the system were ultimately linked together into a single object program, in MULTICS, the ring mechanism was very much present at run time and enforced by the hardware.
The advantage of the ring mechanism is that it can easily be extended to structure user subsystems.
For example, a professor could write a program to test and grade student programs and run this program in ring n, with the student programs running in ring n + 1 so that they could not change their grades.
The Pentium hardware supports the MULTICS ring structure, but no major operating system uses it at present.
Nevertheless, many IBM decided to write timesharing systems for it.
The official IBM timesharing system, TSS/360, was delivered late, and when it finally arrived it was so big and slow that few sites converted over to it.
But a group at IBM’s Scientific Center in Cambridge, Massachusetts, produced a radically different system that IBM eventually accepted as a product, and which is now widely used on its mainframes.
The essence of VM/370 is to completely separate these two functions.
The heart of the system, known as the virtual machine monitor, runs on the bare hardware and does the multiprogramming, providing not one, but several virtual machines to the next layer up, as shown in Fig.
However, unlike all other operating systems, these virtual machines are not extended machines, with files and other nice features.
Instead, they are exact copies of the bare hardware, including kernel/user mode, I/O, interrupts, and everything else the real machine has.
Because each virtual machine is identical to the true hardware, each one can run any operating system that will run directly on the bare hardware.
Different virtual machines can, and frequently do, run different operating systems.
Some run one of the descendants of OS/360 for batch or transaction processing, while others run a single-user, interactive system called CMS (Conversational Monitor System) for timesharing users.
When a CMS program executes a system call, the call is trapped to the operating system in its own virtual machine, not to VM/370, just as it would if it were running on a real machine instead of a virtual one.
These I/O instructions are trapped by VM/370, which then performs them as part of its simulation of the real hardware.
By making a complete separation of the functions of multiprogramming and providing an extended machine, each of the pieces can be much simpler, more flexible, and easier to maintain.
The idea of a virtual machine is used nowadays in a different context: running old MS-DOS programs on a Pentium.
When designing the Pentium and its software, both Intel and Microsoft realized that there would be a big demand for running old software on new hardware.
For this reason, Intel provided a virtual 8086 mode on the Pentium.
This mode is used by Windows, and other operating systems for running old MS-DOS programs.
However, when a program tries to trap to the operating system to make a system call, or tries to do protected I/O directly, a trap to the virtual machine monitor occurs.
When MS-DOS later tries to do the I/O itself, that operation is caught and carried out by the virtual machine monitor.
In the other variant, the virtual machine monitor just catches the first trap and does the I/O itself, since it knows what all the MS-DOS system calls are and thus knows what each trap is supposed to do.
This variant is less pure than the first one, since it emulates only MS-DOS correctly, and not other operating systems, as the first one does.
On the other hand, it is much faster, since it saves the trouble of starting up MS-DOS to do the I/O.
A further disadvantage of actually running MS-DOS in virtual 8086 mode is that MS-DOS fiddles around with the interrupt enable/disable bit quite a lot, all of which must be emulated at considerable cost.
For companies that provide web-hosting services, it can be more economical to run multiple virtual machines on a single fast server (perhaps one with multiple CPUs) than to run many small computers, each hosting a single Web site.
VMWare and Microsoft’s Virtual PC are marketed for such installations.
These programs use large files on a host system as simulated disks for their guest systems.
To achieve efficiency they analyze guest system program binaries and allow safe code to run directly on the host hardware, trapping instructions that make operating system calls.
Most professors teaching other subjects would be very nervous about sharing laboratory computers with an operating systems course where student mistakes could corrupt or erase disk data.
Another area where virtual machines are used, but in a somewhat different way, is for running Java programs.
When Sun Microsystems invented the Java programming language, it also invented a virtual machine (i.e., a computer architecture) called the JVM (Java Virtual Machine)
The Java compiler produces code for JVM, which then typically is executed by a software JVM interpreter.
The advantage of this approach is that the JVM code can be shipped over the Internet to any computer that has a JVM interpreter and run there.
Of course, Sun could have produced a compiler that produced SPARC binaries and then distributed a SPARC interpreter, but JVM is a much simpler architecture to interpret.
Another advantage of using JVM is that if the interpreter is implemented properly, which is not completely trivial, incoming JVM programs can be checked for safety and then executed in a protected environment so they cannot steal data or do any damage.
With VM/370, each user process gets an exact copy of the actual computer.
With virtual 8086 mode on the Pentium, each user process gets an exact copy of a different computer.
At the bottom layer, running in kernel mode, is a program called the exokernel.
Its job is to allocate resources to virtual machines and then check attempts to use them to make sure no machine is trying to use somebody else’s resources.
The advantage of the exokernel scheme is that it saves a layer of mapping.
In the other designs, each virtual machine thinks it has its own disk, with blocks running from 0 to some maximum, so the virtual machine monitor must maintain tables to remap disk addresses (and all other resources)
The exokernel need only keep track of which virtual machine has been assigned which resource.
This method still has the advantage of separating the multiprogramming (in the exokernel) from the user operating system code (in user space), but with less overhead, since all the exokernel has to do is keep the virtual machines out of each other’s hair.
VM/370 gains much in simplicity by moving a large part of the traditional operating system code (implementing the extended machine) into a higher layer, CMS.
A trend in modern operating systems is to take this idea of moving code up into higher layers even further and remove as much as possible from the operating system, leaving a minimal kernel.
The usual approach is to implement most of the operating system functions in user processes.
By splitting the operating system up into parts, each of which only handles one facet of the system, such as file service, process service, terminal service, or memory service, each part becomes small and manageable.
Furthermore, because all the servers run as user-mode processes, and not in kernel mode, they do not have direct access to the hardware.
As a consequence, if a bug in the file server is triggered, the file service may crash, but this will not usually bring the whole machine down.
If a client communicates with a server by sending it messages, the client need not know whether the message is handled locally in its own machine, or whether it was sent across a network to a server on a remote machine.
As far as the client is concerned, the same thing happens in both cases: a request was sent and a reply came back.
The picture painted above of a kernel that handles only the transport of messages from clients to servers and back is not completely realistic.
Some operating system functions (such as loading commands into the physical I/O device registers) are difficult, if not impossible, to do from user-space programs.
One way is to have some critical server processes (e.g., I/O device drivers) actually run in kernel mode, with complete access to all the hardware, but still communicate with other processes using the normal message mechanism.
A variant of this mechanism was used in earlier versions of MINIX where drivers were compiled into the kernel but ran as separate processes.
The other way is to build a minimal amount of mechanism into the kernel but leave the policy decisions up to servers in user space.
For example, the kernel might recognize that a message sent to a certain special address means to take the contents of that message and load it into the I/O device registers for some disk, to start a disk read.
In this example, the kernel would not even inspect the bytes in the message to see if they were valid or meaningful; it would just blindly copy them into the disk’s device registers.
Obviously, some scheme for limiting such messages to authorized processes only must be used.
This is how MINIX 3 works, drivers are in user space and use special kernel calls to request reads and writes of I/O registers or to access kernel information.
The split between mechanism and policy is an important concept; it occurs again and again in operating systems in various contexts.
Operating systems typically have four major components: process management, I/O device management, memory management, and file management.
The next four chapters deal with these four topics, one topic per chapter.
Chapter 6 is a list of suggested readings and a bibliography.
The chapters on processes, I/O, memory management, and file systems have the same general structure.
First the general principles of the subject are laid out.
Then comes an overview of the corresponding area of MINIX 3 (which also applies to UNIX)
The implementation section may be skimmed or skipped without loss of continuity by readers just interested in the principles of operating systems and not interested in the MINIX 3 code.
Readers who are interested in finding out how a real operating system (MINIX 3) works should read all the sections.
Operating systems can be viewed from two viewpoints: resource managers.
In the resource manager view, the operating system’s job is to efficiently manage the different parts of the system.
In the extended machine view, the job of the system is to provide the users with a virtual machine that is more convenient to use than the actual machine.
Operating systems have a long history, starting from the days when they replaced the operator, to modern multiprogramming systems.
The heart of any operating system is the set of system calls that it can handle.
For MINIX 3, these calls can be divided into six groups.
The first group of system calls relates to process creation and termination.
The fifth group protects information, and the sixth group is about keeping track of time.
The most common ones are as a monolithic system, as a hierarchy of layers, as a virtual machine system, using an exokernel, and using the client-server model.
What are the two main functions of an operating system?
Which of the following instructions should be allowed only in kernel mode?
List some differences between personal computer operating systems and mainframe operating systems.
Give one reason why a closed-source proprietary operating system like Windows should have better quality than an open-source operating system like Linux.
Now give one reason why an open-source operating system like Linux should have better quality than a closed-source proprietary operating system like Windows.
In view of the fact that the mere existence of a superuser can lead to all kinds of security problems, why does such a concept exist?
All versions of UNIX support file naming using both absolute paths (relative to the root) and relative paths (relative to the working directory)
Would it be possible to dispose of one of these and just use the other? If so, which would you suggest keeping?
Why is the process table needed in a timesharing system? Is it also needed in personal computer systems in which only one process exists, that process taking over the entire machine until it is finished?
Are pipes an essential facility? Would major functionality be lost if they were not available?
To what part of a personal computer software is the command processing via the stereo or camera’s display similar to?
Windows does not have a fork system call, yet it is able to create new processes.
Make an educated guess about the semantics of the system call Windows uses to create new processes.
Which call do you think is likely to execute most quickly.
How many system calls can the computer execute per second and still have half the CPU capacity for running application code?
Does this mean that you have to be very, very careful about making nodes this way because there is no way to every remove them?
Write a program (or series of programs) to test all the MINIX 3 system calls.
You might also add some features such as redirection of input and output, pipes, and background jobs.
We are now about to embark on a detailed study of how operating systems, in general, and MINIX 3, in particular, are designed and constructed.
The most central concept in any operating system is the process: an abstraction of a running program.
Everything else hinges on this concept, and it is important that the operating system designer (and student) understand this concept well.
All modern computers can do several things at the same time.
While running a user program, a computer can also be reading from a disk and outputting text to a screen or printer.
In a multiprogramming system, the CPU also switches from program to program, running each for tens or hundreds of milliseconds.
While, strictly speaking, at any instant of time, the CPU is running only one program, in the course of 1 second, it may work on several programs, thus giving the users the illusion of parallelism.
Sometimes people speak of pseudoparallelism in this context, to contrast it with the true hardware parallelism of multiprocessor systems (which have two or more CPUs sharing the same physical memory)
Keeping track of multiple, parallel activities is hard for people to do.
Therefore, operating system designers over the years have evolved a conceptual model (sequential processes) that makes parallelism easier to deal with.
That model, its uses, and some of its consequences form the subject of this chapter.
In this model, all the runnable software on the computer, sometimes including the operating system, is organized into a number of sequential processes, or just processes for short.
A process is just an executing program, including the current values of the program counter, registers, and variables.
In reality, of course, the real CPU switches back and forth from process to process, but to understand the system, it is much easier to think about a collection of processes running in (pseudo) parallel, than to try to keep track of how the CPU switches from program to program.
This rapid switching back and forth is called multiprogramming, as we saw in Chap.
Of course, there is only one physical program counter, so when each process runs, its logical program counter is loaded into the real program counter.
When it is finished for the time being, the physical program counter is saved in the process’ logical program counter in memory.
With the CPU switching back and forth among the processes, the rate at which a process performs its computation will not be uniform, and probably not even reproducible if the same processes are run again.
Thus, processes must not be programmed with built-in assumptions about timing.
Consider, for example, an I/O process that starts a streamer tape to restore backed up files, executes an idle loop 10,000 times to let it get up to speed, and then issues a command to read the first record.
If the CPU decides to switch to another process during the idle loop, the tape process might not run again until after the first record was already.
When a process has critical real-time requirements like this, that is, particular events must occur within a specified number of milliseconds, special measures must be taken to ensure that they do occur.
Normally, however, most processes are not affected by the underlying multiprogramming of the CPU or the relative speeds of different processes.
The difference between a process and a program is subtle, but crucial.
Consider a culinary-minded computer scientist who is baking a birthday cake for his daughter.
He has a birthday cake recipe and a kitchen well stocked with the necessary input: flour, eggs, sugar, extract of vanilla, and so on.
In this analogy, the recipe is the program (i.e., an algorithm expressed in some suitable notation), the computer scientist is the processor (CPU), and the cake ingredients are the input data.
The process is the activity consisting of our baker reading the recipe, fetching the ingredients, and baking the cake.
Now imagine that the computer scientist’s son comes running in crying, saying that he has been stung by a bee.
The computer scientist records where he was in the recipe (the state of the current process is saved), gets out a first aid book, and begins following the directions in it.
When the bee sting has been taken care of, the computer scientist goes back to his cake, continuing at the point where he left off.
The key idea here is that a process is an activity of some kind.
A single processor may be shared among several processes, with some scheduling algorithm being used to determine when to stop work on one process and service a different one.
Operating systems need some way to make sure all the necessary processes exist.
In very simple systems, or in systems designed for running only a single application (e.g., controlling a device in real time), it may be possible to have all the processes that will ever be needed be present when the system comes up.
In general-purpose systems, however, some way is needed to create and terminate processes as needed during operation.
There are four principal events that cause processes to be created:
Execution of a process creation system call by a running process.
When an operating system is booted, often several processes are created.
Some of these are foreground processes, that is, processes that interact with (human) users and perform work for them.
Others are background processes, which are not associated with particular users, but instead have some specific function.
For example, a background process may be designed to accept incoming requests for web pages hosted on that machine, waking up when a request arrives to service the request.
Processes that stay in the background to handle some activity such as web pages, printing, and so on are called daemons.
In MINIX 3, the ps program can be used to list the running processes.
In addition to the processes created at boot time, new processes can be created afterward as well.
Often a running process will issue system calls to create one or more new processes to help it do its job.
Creating new processes is particularly useful when the work to be done can easily be formulated in terms of several related, but otherwise independent interacting processes.
For example, when compiling a large program, the make program invokes the C compiler to convert source files to object code, and then it invokes the install program to copy the program to its destination, set ownership and permissions, etc.
In MINIX 3, the C compiler itself is actually several different programs, which work together.
These include a preprocessor, a C language parser, an assembly language code generator, an assembler, and a linker.
In interactive systems, users can start a program by typing a command.
In MINIX 3, virtual consoles allow a user to start a program, say a compiler, and then switch to an alternate console and start another program, perhaps to edit documentation while the compiler is running.
The last situation in which processes are created applies only to the batch systems found on large mainframes.
Here users can submit batch jobs to the system (possibly remotely)
When the operating system decides that it has the resources to run another job, it creates a new process and runs the next job from the input queue in it.
Technically, in all these cases, a new process is created by having an existing process execute a process creation system call.
That process may be a running user process, a system process invoked from the keyboard or mouse, or a batch manager process.
What that process does is execute a system call to create the new process.
This system call tells the operating system to create a new process and indicates, directly or indirectly, which program to run in it.
In MINIX 3, there is only one system call to create a new process: fork.
This call creates an exact clone of the calling process.
After the fork, the two processes, the parent and the child, have the same memory image, the same environment strings, and the same open files.
Usually, the child process then executes execve or a similar system call to change its memory image and run a new program.
For example, when a user types a command, say, sort, to the shell, the shell forks off a child process and the child executes sort.
In both MINIX 3 and UNIX, after a process is created both the parent and child have their own distinct address spaces.
If either process changes a word in its address space, the change is not visible to the other process.
The child’s initial address space is a copy of the parent’s, but there are two distinct address spaces involved; no writable memory is shared (like some UNIX implementations, MINIX 3 can share the program text between the two since that cannot be modified)
It is, however, possible for a newly created process to share some of its creator’s other resources, such as open files.
After a process has been created, it starts running and does whatever its job is.
Sooner or later the new process will terminate, usually due to one of the following conditions:
When a compiler has compiled the program given to it, the compiler executes a system call to tell the operating system that it is finished.
For instance, editors always have a key combination that the user can invoke to tell the process to save the working file, remove any temporary files that are open and terminate.
The second reason for termination is that the process discovers a fatal error.
The third reason for termination is an error caused by the process, perhaps due.
Examples include executing an illegal instruction, referencing nonexistent memory, or dividing by zero.
In MINIX 3, a process can tell the operating system that it wishes to handle certain errors itself, in which case the process is signaled (interrupted) instead of terminated when one of the errors occurs.
The fourth reason a process might terminate is that one process executes a system call telling the operating system to kill some other process.
Of course, the killer must have the necessary authorization to do in.
In some systems, when a process terminates, either voluntarily or otherwise, all processes it created are immediately killed as well.
In some systems, when a process creates another process, the parent and child continue to be associated in certain ways.
The child can itself create more processes, forming a process hierarchy.
Unlike plants and animals that use sexual reproduction, a process has only one parent (but zero, one, two, or more children)
In MINIX 3, a process, its children, and further descendants together may form a process group.
When a user sends a signal from the keyboard, the signal may be delivered to all members of the process group currently associated with the keyboard (usually all processes that were created in the current window)
If a signal is sent to a group, each process can catch the signal, ignore the signal, or take the default action, which is to be killed by the signal.
As a simple example of how process trees are used, let us look at how MINIX 3 initializes itself.
Two special processes, the reincarnation server and init are present in the boot image.
The reincarnation server’s job is to (re)start drivers and servers.
It begins by blocking, waiting for a message telling it what to create.
In contrast, init executes the /etc/rc script that causes it to issue commands to the reincarnation server to start the drivers and servers not present in the boot image.
This procedure makes the drivers and servers so started children of the reincarnation server, so if any of them ever terminate, the reincarnation server will be informed and can restart (i.e., reincarnate) them again.
This mechanism is intended to allow MINIX 3 to tolerate a driver or server crash because a new one will be started automatically.
In practice, replacing a driver is much easier than replacing a server, however, since there fewer repercussions elsewhere in the system.
And, we do not say this always works perfectly; it is still work in progress.
Init forks a getty process for each one, displays a login prompt on it, and then waits for input.
When a name is typed, getty execs a login process with the name as its argument.
If the user succeeds in logging in, login will exec the user’s shell.
User commands create children of the shell, which are grandchildren of init.
This sequence of events is an example of how process trees are used.
As an aside, the code for the reincarnation server and init is not listed in this book; neither is the shell.
Although each process is an independent entity, with its own program counter registers, stack, open files, alarms, and other internal state, processes often need to interact, communicate, and synchronize with other processes.
In that case, the data needs to be moved between processes.
The second process, running grep, selects all lines containing the word ‘‘tree.’’ Depending on the relative speeds of the two processes (which depends on both the relative complexity of the programs and how much CPU time each one has had), it may happen that grep is ready to run, but there is no input waiting for it.
When a process blocks, it does so because logically it cannot continue, typically because it is waiting for input that is not yet available.
It is also possible for a process that is conceptually ready and able to run to be stopped because the operating system has decided to allocate the CPU to another process for a while.
In the first case, the suspension is inherent in the problem (you cannot process the user’s command line until it has been typed)
In the second case, it is a technicality of the system (not enough CPUs to give each process its own private processor)
In both cases the process is willing to run, only in the second one, there is temporarily no CPU available for it.
The third state is different from the first two in that the process cannot run, even if the CPU has nothing else to do.
A process can be in running, blocked, or ready state.
Four transitions are possible among these three states, as shown.
Transition 1 occurs when a process discovers that it cannot continue.
In some systems the process must execute a system call, block or pause to get into blocked state.
In other systems, including MINIX 3, when a process reads from a pipe or special file (e.g., a terminal) and there is no input available, the process is automatically moved from the running state to the blocked state.
Transition 2 occurs when the scheduler decides that the running process has run long enough, and it is time to let another process have some CPU time.
Transition 3 occurs when all the other processes have had their fair share and it is time for the first process to get the CPU to run again.
Many algorithms have been devised to try to balance the competing demands of efficiency for the system as a whole and fairness to individual processes.
We will look at scheduling and study some of these algorithms later in this chapter.
Transition 4 occurs when the external event for which a process was waiting (e.g., the arrival of some input) happens.
If no other process is running then, transition 3 will be triggered immediately, and the process will start running.
Otherwise it may have to wait in ready state for a little while until the CPU is available.
Using the process model, it becomes much easier to think about what is going on inside the system.
Some of the processes run programs that carry out commands typed in by a user.
Other processes are part of the system and handle tasks such as carrying out requests for file services or managing the details of running a disk or a tape drive.
When a disk interrupt occurs, the system may make a decision to stop running the current process and run the disk process, which was blocked waiting for that interrupt.
We say ‘‘may’’ because it depends upon relative priorities of the running process and the disk driver process.
But the point is that instead of thinking about interrupts, we can think about user processes, disk processes, terminal processes, and so on, which block when they are waiting for something to happen.
When the disk block has been read or the character typed, the process waiting for it is unblocked and is eligible to run again.
This view gives rise to the model shown in Fig.
Here the lowest level of the operating system is the scheduler, with a variety of processes on top of it.
All the interrupt handling and details of actually starting and stopping processes are hidden away in the scheduler, which is actually quite small.
The rest of the operating system is nicely structured in process form.
Of course, the ‘‘scheduler’’ is not the only thing in the lowest layer, there is also support for interrupt handling and interprocess communication.
Nevertheless, to a first approximation, it does show the basic structure.
To implement the process model, the operating system maintains a table (an array of structures), called the process table, with one entry per process.
This entry contains information about the process’ state, its program counter, stack pointer, memory allocation, the status of its open files, its accounting and scheduling information, alarms and other signals, and everything else about the process that must be saved when.
The lowest layer of a process-structured operating system handles interrupts and scheduling.
In MINIX 3, interprocess communication, memory management, and file management are each handled by separate modules within the system, so the process table is partitioned, with each module maintaining the fields that it needs.
The fields in the first column are the only ones relevant to this chapter.
The other two columns are provided just to give an idea of what information is needed elsewhere in the system.
Some of the fields of the MINIX 3 process table.
The fields are distributed over the kernel, the process manager, and the file system.
Now that we have looked at the process table, it is possible to explain a little more about how the illusion of multiple sequential processes is maintained on a machine with one CPU and many I/O devices.
What follows is technically a description of how the ‘‘scheduler’’ of Fig.
Associated with each I/O device class (e.g., floppy disks, hard disks, timers, terminals) is a data structure in a table called the interrupt descriptor table.
The most important part of each entry in this table is called the interrupt vector.
Suppose that user process 23 is running when a disk interrupt occurs.
The program counter, program status word, and possibly one or more registers are pushed onto the (current) stack by the interrupt hardware.
The computer then jumps to the address specified in the disk interrupt vector.
The interrupt service procedure starts out by saving all the registers in the process table entry for the current process.
The current process number and a pointer to its entry are kept in global variables so they can be found quickly.
Then the information deposited by the interrupt is removed from the stack, and the stack pointer is set to a temporary stack used by the process handler.
Actions such as saving the registers and setting the stack pointer cannot even be expressed in high-level languages such as C, so they are performed by a small assembly language routine.
When this routine is finished, it calls a C procedure to do the rest of the work for this specific interrupt type.
Interprocess communication in MINIX 3 is via messages, so the next step is to build a message to be sent to the disk process, which will be blocked waiting for it.
The message says that an interrupt occurred, to distinguish it from messages from user processes requesting disk blocks to be read and things like that.
The state of the disk process is now changed from blocked to ready and the scheduler is called.
In MINIX 3, different processes have different priorities, to give better service to I/O device handlers than to user processes, for example.
If the disk process is now the highest priority runnable process, it will be scheduled to run.
If the process that was interrupted is just as important or more so, then it will be scheduled to run again, and the disk process will have to wait a little while.
Either way, the C procedure called by the assembly language interrupt code now returns, and the assembly language code loads up the registers and memory map for the now-current process and starts it running.
It is worth noting that the details vary slightly from system to system.
In traditional operating systems, each process has an address space and a single thread of control.
In fact, that is almost the definition of a process.
Nevertheless, there are often situations in which it is desirable to have multiple threads of.
Skeleton of what the lowest level of the operating system does when an interrupt occurs.
These threads of control are usually just called threads, although some people call them lightweight processes.
One way of looking at a process is that it is a way to group related resources together.
A process has an address space containing program text and data, as well as other resources.
These resources may include open files, child processes, pending alarms, signal handlers, accounting information, and more.
By putting them together in the form of a process, they can be managed more easily.
The other concept a process has is a thread of execution, usually shortened to just thread.
The thread has a program counter that keeps track of which instruction to execute next.
It has a stack, which contains the execution history, with one frame for each procedure called but not yet returned from.
Although a thread must execute in some process, the thread and its process are different concepts and can be treated separately.
Processes are used to group resources together; threads are the entities scheduled for execution on the CPU.
What threads add to the process model is to allow multiple executions to take place in the same process environment, to a large degree independent of one another.
Each process has its own address space and a single thread of control.
Although in both cases we have three threads, in Fig.
As an example of where multiple threads might be used, consider a web browser process.
For each image on a web page, the browser must set up a separate connection to the page’s home site and request the image.
A great deal of time is spent establishing and releasing all these connections.
By having multiple threads within the browser, many images can be requested at the same time, greatly speeding up performance.
When multiple threads are present in the same address space, a few of the fields of Fig.
Among the per-thread items are the program counter, registers, and state.
The program counter is needed because threads, like processes, can be suspended and resumed.
The registers are needed because when threads are suspended, their registers must be saved.
Finally, threads, like processes, can be in running, ready, or blocked state.
The first column lists some items shared by all threads in a process.
The second one lists some items private to each thread.
In some systems, the operating system is not aware of the threads.
In other words, they are managed entirely in user space.
When a thread is about to block, for example, it chooses and starts its successor before stopping.
Several userlevel threads packages are in common use, including the POSIX P-threads and Mach C-threads packages.
In other systems, the operating system is aware of the existence of multiple threads per process, so when a thread blocks, the operating system chooses the next one to run, either from the same process or a different one.
To do scheduling, the kernel must have a thread table that lists all the threads in the system, analogous to the process table.
Although these two alternatives may seem equivalent, they differ considerably in performance.
Switching threads is much faster when thread management is done in user space than when a system call is needed.
This fact argues strongly for doing thread management in user space.
On the other hand, when threads are managed entirely in user space and one thread blocks (e.g., waiting for I/O or a page fault to be handled), the kernel blocks the entire process, since it is not even aware that other threads exist.
This fact as well as others argue for doing thread management in the kernel (Boehm, 2005)
As a consequence, both systems are in use, and various hybrid schemes have been proposed as well (Anderson et al., 1992)
No matter whether threads are managed by the kernel or in user space, they introduce a raft of problems that must be solved and which change the programming model appreciably.
To start with, consider the effects of the fork system call.
If the parent process has multiple threads, should the child also have them? If not, the process may not function properly, since all of them may be essential.
However, if the child process gets as many threads as the parent, what happens if a thread was blocked on a read call, say, from the keyboard? Are two threads now blocked on the keyboard? When a line is typed, do both threads get a copy of it? Only the parent? Only the child? The same problem exists with open network connections.
Another class of problems is related to the fact that threads share many data structures.
What happens if one thread closes a file while another one is still reading from it? Suppose that one thread notices that there is too little memory and starts allocating more memory.
Then, part way through, a thread switch occurs, and the new thread also notices that there is too little memory and also starts allocating more memory.
Does the allocation happen once or twice? In nearly all systems that were not designed with threads in mind, the libraries (such as the memory allocation procedure) are not reentrant, and will crash if a second call is made while the first one is still active.
In UNIX, after a system call, the status of the call is put into a global variable, errno.
What happens if a thread makes a system call, and before it is able to read errno, another thread makes a system call, wiping out the original value?
For example, if a thread calls alarm, it makes sense for the resulting signal to go to the thread that made the call.
When the kernel is aware of threads, it can usually make sure the right thread gets the signal.
When the kernel is not aware of threads, the threads package must keep track of alarms by itself.
Other signals, such as a keyboard-initiated SIGINT, are not thread specific.
Who should catch them? One designated thread? All the threads? A newly created thread? Each of these solutions has problems.
Furthermore, what happens if one thread changes the signal handlers without telling other threads?
In many systems, when stack overflow occurs, the kernel just provides more stack, automatically.
When a process has multiple threads, it must also have multiple stacks.
If the kernel is not aware of all these stacks, it cannot grow them automatically upon stack fault.
In fact, it may not even realize that a memory fault is related to stack growth.
These problems are certainly not insurmountable, but they do show that just introducing threads into an existing system without a fairly substantial system redesign is not going to work at all.
The semantics of system calls have to be redefined and libraries have to be rewritten, at the very least.
And all of these things must be done in such a way as to remain backward compatible with existing programs for the limiting case of a process with only one thread.
For example, in a shell pipeline, the output of the first process must be passed to the second process, and so on down the line.
Thus there is a need for communication between processes, preferably in a well-structured way not using interrupts.
In the following sections we will look at some of the issues related to this InterProcess Communication or IPC.
The first was alluded to above: how one process can pass information to another.
The second has to do with making sure two or more processes do not get into each other’s way when engaging in critical activities (suppose two processes each try to grab the last 1 MB of memory)
The third concerns proper sequencing when dependencies are present: if process A produces data and process B prints it, B has to wait until A has produced some data before starting to print.
We will examine all three of these issues in some detail in this section.
It is also important to mention that two of these issues apply equally well to threads.
The first one—passing information—is easy for threads since they share a common address space (threads in different address spaces that need to communicate fall under the heading of communicating processes)
However, the other two—keeping out of each other’s hair and proper sequencing—apply as well.
Below we will discuss the problem in the context of processes, but please keep in mind that the same problems and solutions also apply to threads.
In some operating systems, processes that are working together may share some common storage that each one can read and write.
The shared storage may be in main memory (possibly in a kernel data structure) or it may be a shared file; the location of the shared memory does not change the nature of the communication or the problems that arise.
To see how interprocess communication works in practice, let us consider a simple but common example, a print spooler.
When a process wants to print a file, it enters the file name in a special spooler directory.
Another process, the printer daemon, periodically checks to see if so are any files to be printed, and if so removes their names from the directory.
Also imagine that there are two shared variables, out, which points to the next file to be printed, and in, which points to the next free slot in the directory.
These two variables might well be kept in a two-word file available to all processes.
More or less simultaneously, processes A and B decide they want to queue a file for printing.
Two processes want to access shared memory at the same time.
How do we avoid race conditions? The key to preventing trouble here and in many other situations involving shared memory, shared files, and shared everything else is to find some way to prohibit more than one process from reading and writing the shared data at the same time.
Put in other words, what we need is mutual exclusion —some way of making sure that if one process is using a shared variable or file, the other processes will be excluded from doing the same thing.
The difficulty above occurred because process B started using one of the shared variables before process A was finished with it.
The choice of appropriate primitive operations for achieving mutual exclusion is a major design issue in any operating system, and a subject that we will now examine in great detail.
The problem of avoiding race conditions can also be formulated in an abstract way.
Part of the time, a process is busy doing internal computations and other things that do not lead to race conditions.
However, sometimes a process may be accessing shared memory or files.
That part of the program where the shared memory is accessed is called the critical region or critical section.
If we could arrange matters such that no two processes were ever in their critical regions at the same time, we could avoid race conditions.
Although this requirement avoids race conditions, this is not sufficient for having parallel processes cooperate correctly and efficiently using shared data.
We need four conditions to hold to have a good solution:
No two processes may be simultaneously inside their critical regions.
No assumptions may be made about speeds or the number of CPUs.
No process running outside its critical region may block other processes.
No process should have to wait forever to enter its critical region.
A little later, at time T 2 process B attempts to enter its critical region but fails because another process is already in its critical region and we allow only one at a time.
Consequently, B is temporarily suspended until time T 3 when A leaves its critical region, allowing B to enter immediately.
Eventually B leaves (at T 4) and we are back to the original situation with no processes in their critical regions.
In this section we will examine various proposals for achieving mutual exclusion, so that while one process is busy updating shared memory in its critical region, no other process will enter its critical region and cause trouble.
The simplest solution is to have each process disable all interrupts just after entering its critical region and reenable them just before leaving it.
The CPU is only switched from process to process as a result of clock or other interrupts, after all, and with interrupts turned off the CPU will not be switched to another process.
Thus, once a process has disabled interrupts, it can examine and update the shared memory without fear that any other process will intervene.
This approach is generally unattractive because it is unwise to give user processes the power to turn off interrupts.
Suppose that one of them did, and then never turned them on again? That could be the end of the system.
The other ones will continue running and can access the shared memory.
On the other hand, it is frequently convenient for the kernel itself to disable interrupts for a few instructions while it is updating variables or lists.
If an interrupt occurred while the list of ready processes, for example, was in an inconsistent state, race conditions could occur.
The conclusion is: disabling interrupts is often a useful technique within the operating system itself but is not appropriate as a general mutual exclusion mechanism for user processes.
As a second attempt, let us look for a software solution.
When a process wants to enter its critical region, it first tests the lock.
Unfortunately, this idea contains exactly the same fatal flaw that we saw in the spooler directory.
When the first process runs again, it will also set the lock to 1, and two processes will be in their critical regions at the same time.
Now you might think that we could get around this problem by first reading out the lock value, then checking it again just before storing into it, but that really does not help.
The race now occurs if the second process modifies the lock just after the first process has finished its second check.
A third approach to the mutual exclusion problem is shown in Fig.
This program fragment, like most others in this book, is written in C.
Java, for example, is not predictable because it might run out of storage at a critical moment and need to invoke the garbage collector at a most inopportune time.
This cannot happen in C because there is no garbage collection in C.
A quantitative comparison of C, C++, Java, and four other languages is given by Prechelt (2000)
In both cases, be sure to note the semicolons terminating the while statements.
Continuously testing a variable until some value appears is called busy waiting.
It should usually be avoided, since it wastes CPU time.
Only when there is a reasonable expectation that the wait will be short is busy waiting used.
A lock that uses busy waiting is called a spin lock.
At this point turn is 1 and both processes are executing in their noncritical regions.
Suddenly, process 0 finishes its noncritical region and goes back to the top of its loop.
Put differently, taking turns is not a good idea when one of the processes is much slower than the other.
In fact, this solution requires that the two processes strictly alternate in entering their critical regions, for example, in spooling files.
Neither one would be permitted to spool two in a row.
By combining the idea of taking turns with the idea of lock variables and warning variables, a Dutch mathematician, T.
Dekker, was the first one to devise a software solution to the mutual exclusion problem that does not require strict alternation.
Peterson discovered a much simpler way to achieve mutual exclusion, thus rendering Dekker’s solution obsolete.
This algorithm consists of two procedures written in ANSI C, which means that function prototypes should be supplied for all the functions defined and used.
However, to save space, we will not show the prototypes in this or subsequent examples.
When both processes come to the while statement, process 0 executes it zero times and enters its critical region.
Process 1 loops and does not enter its critical region.
Now let us look at a proposal that requires a little help from the hardware.
Many computers, especially those designed with multiple processors in mind, have an instruction.
The operations of reading the word and storing into it are guaranteed to be indivisible—no other processor can access the memory word until the instruction is finished.
The CPU executing the TSL instruction locks the memory bus to prohibit other CPUs from accessing memory until it is done.
To use the TSL instruction, we will use a shared variable, LOCK, to coordinate access to shared memory.
When it is done, the process sets LOCK back to 0 using an ordinary move instruction.
How can this instruction be used to prevent two processes from simultaneously entering their critical regions? The solution is given in Fig.
There a four-instruction subroutine in a fictitious (but typical) assembly language is shown.
If it is nonzero, the lock was already set, so the program just goes back to the beginning and tests it again.
Sooner or later it will become 0 (when the process currently in its critical region is done with its critical region), and the subroutine returns, with the lock set.
Entering and leaving a critical region using the TSL instruction.
Both Peterson’s solution and the solution using TSL are correct, but both have the defect of requiring busy waiting.
In essence, what these solutions do is this: when a process wants to enter its critical region, it checks to see if the entry is allowed.
If it is not, the process just sits in a tight loop waiting until it is.
Not only does this approach waste CPU time, but it can also have unexpected effects.
Consider a computer with two processes, H, with high priority and L, with low priority, which share a critical region.
The scheduling rules are such that H is run whenever it is in ready state.
At a certain moment, with L in its critical region, H becomes ready to run (e.g., an I/O operation completes)
This situation is sometimes referred to as the priority inversion problem.
Now let us look at some interprocess communication primitives that block instead of wasting CPU time when they are not allowed to enter their critical regions.
One of the simplest is the pair sleep and wakeup.
The wakeup call has one parameter, the process to be awakened.
Alternatively, both sleep and wakeup each have one parameter, a memory address used to match up sleeps with wakeups.
As an example of how these primitives can be used in practice, let us consider the producer-consumer problem (also known as the bounded buffer problem)
One of them, the producer, puts information into the buffer, and the other one, the consumer, takes it out.
Trouble arises when the producer wants to put a new item in the buffer, but it is already full.
The solution is for the producer to go to sleep, to be awakened when the consumer has removed one or more items.
Similarly, if the consumer wants to remove an item from the buffer and sees that the buffer is empty, it goes to sleep until the producer puts something in the buffer and wakes it up.
This approach sounds simple enough, but it leads to the same kinds of race conditions we saw earlier with the spooler directory.
To keep track of the number of items in the buffer, we will need a variable, count.
If the maximum number of items the buffer can hold is N, the producer’s code will first test to see if count is N.
If it is, the producer will go to sleep; if it is not, the producer will add an item and increment count.
If it is, go to sleep; if it is nonzero, remove an item and decrement the counter.
Each of the processes also tests to see if the other should be sleeping, and if not, wakes it up.
The code for both producer and consumer is shown in Fig.
To express system calls such as sleep and wakeup in C, we will show them as calls to library routines.
They are not part of the standard C library but presumably would be available on any system that actually had these system calls.
At that instant, the scheduler decides to stop running the consumer temporarily and start running the producer.
Reasoning that count was just 0, and thus the consumer must be sleeping, the producer calls wakeup to wake the consumer up.
Unfortunately, the consumer is not yet logically asleep, so the wakeup signal is lost.
When the consumer next runs, it will test the value of count it previously read, find it to be 0, and go to sleep.
Sooner or later the producer will fill up the buffer and also go to sleep.
The essence of the problem here is that a wakeup sent to a process that is not (yet) sleeping is lost.
A quick fix is to modify the rules to add a wakeup waiting bit to the picture.
When a wakeup is sent to a process that is still awake, this bit is set.
Later, when the process tries to go to sleep, if the wakeup waiting bit is on, it will be turned off, but the process will stay awake.
The wakeup waiting bit is a piggy bank for wakeup signals.
While the wakeup waiting bit saves the day in this simple example, it is easy to construct examples with three or more processes in which one wakeup waiting bit is insufficient.
Dijkstra (1965) suggested using an integer variable to count the number of wakeups saved for future use.
In his proposal, a new variable type, called a semaphore, was introduced.
A semaphore could have the value 0, indicating that no wakeups were saved, or some positive value if one or more wakeups were pending.
Dijkstra proposed having two operations, down and up (which are generalizations of sleep and wakeup, respectively)
If so, it decrements the value (i.e., uses up one stored wakeup) and just continues.
If the value is 0, the process is put to sleep without completing the down for the moment.
Checking the value, changing it, and possibly going to sleep is all done as a single, indivisible, atomic action.
It is guaranteed that once a semaphore operation has started, no other process can access the semaphore until the operation has completed or blocked.
This atomicity is absolutely essential to solving synchronization problems and avoiding race conditions.
The up operation increments the value of the semaphore addressed.
If one or more processes were sleeping on that semaphore, unable to complete an earlier.
Thus, after an up on a semaphore with processes sleeping on it, the semaphore will still be 0, but there will be one fewer process sleeping on it.
The operation of incrementing the semaphore and waking up one process is also indivisible.
No process ever blocks doing an up, just as no process ever blocks doing a wakeup in the earlier model.
As an aside, in Dijkstra’s original paper, he used the names p and v instead of down and up, respectively, but since these have no mnemonic significance to people who do not speak Dutch (and only marginal significance to those who do), we will use the terms down and up instead.
It is essential that they be implemented in an indivisible way.
The normal way is to implement up and down as system calls, with the operating system briefly disabling all interrupts while it is testing the semaphore, updating it, and putting the process to sleep, if necessary.
As all of these actions take only a few instructions, no harm is done in disabling interrupts.
If multiple CPUs are being used, each semaphore should be protected by a lock variable, with the TSL instruction used to make sure that only one CPU at a time examines the semaphore.
Be sure you understand that using TSL to prevent several CPUs from accessing the semaphore at the same time is quite different from busy waiting by the producer or consumer waiting for the other to empty or fill the buffer.
The semaphore operation will only take a few microseconds, whereas the producer or consumer might take arbitrarily long.
This solution uses three semaphores: one called full for counting the number of slots that are full, one called empty for counting the number of slots that are empty, and one called mutex to make sure the producer and consumer do not access the buffer at the same time.
Semaphores that are initialized to 1 and used by two or more processes to ensure that only one of them can enter its critical region at the same time are called binary semaphores.
If each process does a down just before entering its critical region and an up just after leaving it, mutual exclusion is guaranteed.
Now that we have a good interprocess communication primitive at our disposal, let us go back and look at the interrupt sequence of Fig.
In a system using semaphores, the natural way to hide interrupts is to have a semaphore, initially set to 0, associated with each I/O device.
Just after starting an I/O device, the managing process does a down on the associated semaphore, thus blocking immediately.
When the interrupt comes in, the interrupt handler then does an up on the associated semaphore, which makes the relevant process ready to run again.
Of course, if several processes are now ready, the scheduler may choose to run an even more important process next.
We will look at how scheduling is done later in this chapter.
It is designed to guarantee that only one process at a time will be reading or writing the buffer and the associated variables.
We will study mutual exclusion and how to achieve it more in the next section.
The full and empty semaphores are needed to guarantee that certain event sequences do or do not occur.
In this case, they ensure that the producer stops running when the buffer is full, and the consumer stops running when it is empty.
When the semaphore’s ability to count is not needed, a simplified version of the semaphore, called a mutex, is sometimes used.
Mutexes are good only for managing mutual exclusion to some shared resource or piece of code.
They are easy and efficient to implement, which makes them especially useful in thread packages that are implemented entirely in user space.
Look closely at the order of the downs before entering or removing items from the buffer in Fig.
Suppose that the two downs in the producer’s code were reversed in order, so mutex was decremented before empty instead of after it.
Consequently, the next time the consumer tried to access the buffer, it would do a down on mutex, now 0, and block too.
Both processes would stay blocked forever and no more work would ever be done.
This problem is pointed out to show how careful you must be when using semaphores.
One subtle error and everything comes to a grinding halt.
It is like programming in assembly language, only worse, because the errors are race conditions, deadlocks, and other forms of unpredictable and irreproducible behavior.
A monitor is a collection of procedures, variables, and data structures that are all grouped together in a special kind of module or package.
Processes may call the procedures in a monitor whenever they want to, but they cannot directly access the monitor’s internal data structures from procedures declared outside the monitor.
Figure 2-15 illustrates a monitor written in an imaginary language, Pidgin Pascal.
Monitors have a key property that makes them useful for achieving mutual exclusion: only one process can be active in a monitor at any instant.
Monitors are a programming language construct, so the compiler knows they are special and can handle calls to monitor procedures differently from other procedure calls.
Typically, when a process calls a monitor procedure, the first few instructions of the procedure will check to see if any other process is currently active within the monitor.
If so, the calling process will be suspended until the other process has left the monitor.
If no other process is using the monitor, the calling process may enter.
It is up to the compiler to implement the mutual exclusion on monitor entries, but a common way is to use a mutex or binary semaphore.
Because the compiler, not the programmer, arranges for the mutual exclusion, it is much less likely that something will go wrong.
In any event, the person writing the monitor does not have to be aware of how the compiler arranges for mutual exclusion.
It is sufficient to know that by turning all the critical regions into monitor procedures, no two processes will ever execute their critical regions at the same time.
Although monitors provide an easy way to achieve mutual exclusion, as we have seen above, that is not enough.
We also need a way for processes to block when they cannot proceed.
In the producer-consumer problem, it is easy enough to put all the tests for buffer-full and buffer-empty in monitor procedures, but how should the producer block when it finds the buffer full?
The solution lies in the introduction of condition variables, along with two operations on them, wait and signal.
When a monitor procedure discovers that it cannot continue (e.g., the producer finds the buffer full), it does a wait on some condition variable, say, full.
It also allows another process that had been previously prohibited from entering the monitor to enter now.
This other process, for example, the consumer, can wake up its sleeping partner by doing a signal on the condition variable that its partner is waiting on.
To avoid having two active processes in the monitor at the same time, we need a rule telling what happens after a signal.
Hoare proposed letting the newly awakened process run, suspending the other one.
Brinch Hansen proposed finessing the problem by requiring that a process doing a signal must exit the monitor immediately.
In other words, a signal statement may appear only as the final statement in a monitor procedure.
We will use Brinch Hansen’s proposal because it is conceptually simpler and is also easier to implement.
If a signal is done on a condition variable on which several processes are waiting, only one of them, determined by the system scheduler, is revived.
There is also a third solution, not proposed by either Hoare or Brinch Hansen.
This is to let the signaler continue to run and allow the waiting process to start running only after the signaler has exited the monitor.
They do not accumulate signals for later use the way semaphores do.
Thus if a condition variable is signaled with no one waiting on it, the signal is lost.
In other words, the wait must come before the signal.
In practice it is not a problem because it is easy to keep track of the state of each process with variables, if need be.
A process that might otherwise do a signal can see that this operation is not necessary by looking at the variables.
A skeleton of the producer-consumer problem with monitors is given in Fig.
The advantage of using Pidgin Pascal here is that it is pure and simple and follows the Hoare/Brinch Hansen model exactly.
You may be thinking that the operations wait and signal look similar to sleep and wakeup, which we saw earlier had fatal race conditions.
They are very similar, but with one crucial difference: sleep and wakeup failed because while one process was trying to go to sleep, the other one was trying to wake it up.
The automatic mutual exclusion on monitor procedures guarantees that if, say, the producer inside a monitor procedure discovers that the buffer is full, it will be able to complete the wait operation without having to worry about the possibility that the scheduler may switch to the consumer just.
The consumer will not even be let into the monitor at all until the wait is finished and the producer is marked as no longer runnable.
Although Pidgin Pascal is an imaginary language, some real programming languages also support monitors, although not always in the form designed by.
Java is an object-oriented language that supports user-level threads and also allows methods (procedures) to be grouped together into classes.
By adding the keyword synchronized to a method declaration, Java guarantees that once any thread has started executing that method, no other thread will be allowed to start executing any other synchronized method in that class.
Synchronized methods in Java differ from classical monitors in an essential way: Java does not have condition variables.
Instead, it offers two procedures, wait and notify that are the equivalent of sleep and wakeup except that when they are used inside synchronized methods, they are not subject to race conditions.
By making the mutual exclusion of critical regions automatic, monitors make parallel programming much less error-prone than with semaphores.
As we said earlier, monitors are a programming language concept.
The compiler must recognize them and arrange for the mutual exclusion somehow.
C, Pascal, and most other languages do not have monitors, so it is unreasonable to expect their compilers to enforce any mutual exclusion rules.
In fact, how could the compiler even know which procedures were in monitors and which were not?
These same languages do not have semaphores either, but adding semaphores is easy: all you need to do is add two short assembly code routines to the library to issue the up and down system calls.
The compilers do not even have to know that they exist.
Of course, the operating systems have to know about the semaphores, but at least if you have a semaphore-based operating system, you can still write the user programs for it in C or C++ (or even FORTRAN if you are masochistic enough)
With monitors, you need a language that has them built in.
Another problem with monitors, and also with semaphores, is that they were designed for solving the mutual exclusion problem on one or more CPUs that all have access to a common memory.
By putting the semaphores in the shared memory and protecting them with TSL instructions, we can avoid races.
When we go to a distributed system consisting of multiple CPUs, each with its own private memory, connected by a local area network, these primitives become inapplicable.
The conclusion is that semaphores are too low level and monitors are not usable except in a few programming languages.
Also, none of the primitives provide for information exchange between machines.
This method of interprocess communication uses two primitives, send and receive, which, like semaphores and unlike monitors, are system calls rather than language constructs.
As such, they can easily be put into library procedures, such as.
Message passing systems have many challenging problems and design issues that do not arise with semaphores or monitors, especially if the communicating processes are on different machines connected by a network.
To guard against lost messages, the sender and receiver can agree that as soon as a message has been received, the receiver will send back a special acknowledgement message.
If the sender has not received the acknowledgement within a certain time interval, it retransmits the message.
Now consider what happens if the message itself is received correctly, but the acknowledgement is lost.
The sender will retransmit the message, so the receiver will get it twice.
It is essential that the receiver can distinguish a new message from the retransmission of an old one.
Usually, this problem is solved by putting consecutive sequence numbers in each original message.
If the receiver gets a message bearing the same sequence number as the previous message, it knows that the message is a duplicate that can be ignored.
Message systems also have to deal with the question of how processes are named, so that the process specified in a send or receive call is unambiguous.
Authentication is also an issue in message systems: how can the client tell that he is communicating with the real file server, and not with an imposter?
At the other end of the spectrum, there are also design issues that are important when the sender and receiver are on the same machine.
Copying messages from one process to another is always slower than doing a semaphore operation or entering a monitor.
Cheriton (1984), for example, has suggested limiting message size to what will fit in the machine’s registers, and then doing message passing using the registers.
Now let us see how the producer-consumer problem can be solved with message passing and no shared memory.
We assume that all messages are the same size and that messages sent but not yet received are buffered automatically by the operating system.
The consumer starts out by sending N empty messages to the producer.
Whenever the producer has an item to give to the consumer, it takes an empty message and sends back a full one.
In this way, the total number of messages in the system remains constant in time, so they can be stored in a given amount of memory known in advance.
If the producer works faster than the consumer, all the messages will end up full, waiting for the consumer; the producer will be blocked, waiting for an empty to come back.
If the consumer works faster, then the reverse happens: all the messages will be empties waiting for the producer to fill them up; the consumer will be blocked, waiting for a full message.
For starters, let us look at how messages are addressed.
One way is to assign each process a unique address and have messages be addressed to processes.
A different way is to invent a new data structure, called a mailbox.
A mailbox is a place to buffer a certain number.
When mailboxes are used, the address parameters in the send and receive calls are mailboxes, not processes.
When a process tries to send to a mailbox that is full, it is suspended until a message is removed from that mailbox, making room for a new one.
For the producer-consumer problem, both the producer and consumer would create mailboxes large enough to hold N messages.
The producer would send messages containing data to the consumer’s mailbox, and the consumer would send empty messages to the producer’s mailbox.
When mailboxes are used, the buffering mechanism is clear: the destination mailbox holds messages that have been sent to the destination process but have not yet been accepted.
The other extreme from having mailboxes is to eliminate all buffering.
When this approach is followed, if the send is done before the receive, the sending process is blocked until the receive happens, at which time the message can be copied directly from the sender to the receiver, with no intermediate buffering.
Similarly, if the receive is done first, the receiver is blocked until a send happens.
It is easier to implement than a buffered message scheme but is less flexible since the sender and receiver are forced to run in lockstep.
The processes that make up the MINIX 3 operating system itself use the rendezvous method with fixed size messages for communication among themselves.
User processes also use this method to communicate with operating system components, although a programmer does not see this, since library routines mediate systems calls.
Interprocess communication between user processes in MINIX 3 (and UNIX) is via pipes, which are effectively mailboxes.
The only real difference between a message system with mailboxes and the pipe mechanism is that pipes do not preserve message boundaries.
With a true message system, each read should return only one message.
Of course, if the processes agree always to read and write fixed-size messages from the pipe, or to end each message with a special character (e.g., linefeed), no problems arise.
One well-known message-passing system, for example, is MPI (Message-Passing Interface)
For more information about it, see for example Gropp et al.
The operating systems literature is full of interprocess communication problems that have been widely discussed using a variety of synchronization methods.
In the following sections we will examine two of the better-known problems.
In 1965, Dijkstra posed and solved a synchronization problem he called the dining philosophers problem.
Since that time, everyone inventing yet another synchronization primitive has felt obligated to demonstrate how wonderful the new primitive is by showing how elegantly it solves the dining philosophers problem.
The spaghetti is so slippery that a philosopher needs two forks to eat it.
The life of a philosopher consists of alternate periods of eating and thinking.
This is something of an abstraction, even for philosophers, but the other activities are irrelevant here.
When a philosopher gets hungry, she tries to acquire her left and right fork, one at a time, in either order.
If successful in acquiring two forks, she eats for a while, then puts down the forks and continues to think.
We could modify the program so that after taking the left fork, the program checks to see if the right fork is available.
With a little bit of bad luck, all the philosophers could start the algorithm simultaneously, picking up their left forks, seeing that their right forks were not available, putting down their left forks, waiting, picking up their left forks again simultaneously, and so on, forever.
A situation like this, in which all the programs continue to run indefinitely but fail to make any progress is called starvation.
It is called starvation even when the problem does not occur in an Italian or a Chinese restaurant.
Now you might think, ‘‘If the philosophers would just wait a random time instead of the same time after failing to acquire the right-hand fork, the chance that everything would continue in lockstep for even an hour is very small.’’ This observation is true, and in nearly all applications trying again later is not a problem.
For example, in a local area network using Ethernet, a computer sends a packet only when it detects no other computer is sending one.
However, because of transmission delays, two computers separated by a length of cable may send packets that overlap—a collision.
When a collision of packets is detected each computer waits a random time and tries again; in practice this solution works fine.
However, in some applications one would prefer a solution that always works and cannot fail due to an unlikely series of random numbers.
Before starting to acquire forks, a philosopher would do a down on mutex.
After replacing the forks, she would do an up on mutex.
From a practical one, it has a performance bug: only one philosopher can be eating at any instant.
With five forks available, we should be able to allow two philosophers to eat at the same time.
It uses an array, state, to keep track of whether a philosopher is eating, thinking, or hungry (trying to acquire forks)
A philosopher may move into eating state only if neither neighbor is eating.
Philosopher i’s neighbors are defined by the macros LEFT and RIGHT.
The dining philosophers problem is useful for modeling processes that are competing for exclusive access to a limited number of resources, such as I/O devices.
Another famous problem is the readers and writers problem which models access to a database (Courtois et al., 1971)
Imagine, for example, an airline reservation system, with many competing processes wishing to read and write it.
It is acceptable to have multiple processes reading the database at the same time, but if one process is updating (writing) the database, no other process may have access to the database, not even a reader.
The question is how do you program the readers and the writers? One solution is shown in Fig.
In this solution, the first reader to get access to the data base does a down on the semaphore db.
As readers leave, they decrement the counter and the last one out does an up on the semaphore, allowing a blocked writer, if there is one, to get in.
The solution presented here implicitly contains a subtle decision that is worth commenting on.
Suppose that while a reader is using the data base, another reader comes along.
Since having two readers at the same time is not a problem, the second reader is admitted.
A third and subsequent readers can also be admitted if they come along.
The writer cannot be admitted to the data base, since writers must have exclusive access, so the writer is suspended.
As long as at least one reader is still active, subsequent readers are admitted.
As a consequence of this strategy, as long as there is a steady supply of readers, they will all get in as soon as they arrive.
The writer will be kept suspended until no reader is present.
To prevent this situation, the program could be written slightly differently: When a reader arrives and a writer is waiting, the reader is suspended behind the writer instead of being admitted immediately.
In this way, a writer has to wait for readers that were active when it arrived to finish but does not have to wait for.
The disadvantage of this solution is that it achieves less concurrency and thus lower performance.
In the examples of the previous sections, we have often had situations in which two or more processes (e.g., producer and consumer) were logically runnable.
When a computer is multiprogrammed, it frequently has multiple processes competing for the CPU at the same time.
The part of the operating system that makes the choice is called the scheduler; the algorithm it uses is called the scheduling algorithm.
Initially, we will focus on process scheduling, but later we will take a brief look at some issues specific to thread scheduling.
Back in the old days of batch systems with input in the form of card images on a magnetic tape, the scheduling algorithm was simple: just run the next job on the tape.
With timesharing systems, the scheduling algorithm became more complex, because there were generally multiple users waiting for service.
There may be one or more batch streams as well (e.g., at an insurance company, for processing claims)
On a personal computer you might think there would be only one active process.
After all, a user entering a document on a word processor is unlikely to be simultaneously compiling a program in the background.
However, there are often background jobs, such as electronic mail daemons sending or receiving e-mail.
You might also think that computers have gotten so much faster over the years that the CPU is rarely a scarce resource any more.
Processing digital photographs or watching real time video are examples.
Nearly all processes alternate bursts of computing with (disk) I/O requests, as shown in Fig.
Typically the CPU runs for a while without stopping, then a system call is made to read from a file or write to a file.
When the system call completes, the CPU computes again until it needs more data or has to write more data, and so on.
For example, when the CPU copies bits to a video RAM to update the screen, it is computing, not doing I/O, because the CPU is in use.
I/O in this sense is when a process enters the blocked state waiting for an external device to complete its work.
The former are called compute-bound; the latter are called I/O-bound.
Compute-bound processes typically have long CPU bursts and thus infrequent I/O waits, whereas I/Obound processes have short CPU bursts and thus frequent I/O waits.
Note that the key factor is the length of the CPU burst, not the length of the I/O burst.
I/Obound processes are I/O bound because they do not compute much between I/O requests, not because they have especially long I/O requests.
Bursts of CPU usage alternate with periods of waiting for I/O.
It is worth noting that as CPUs get faster, processes tend to get more I/Obound.
This effect occurs because CPUs are improving much faster than disks.
As a consequence, the scheduling of I/O-bound processes is likely to become a more important subject in the future.
The basic idea here is that if an I/O-bound process wants to run, it should get a chance quickly so it can issue its disk request and keep the disk busy.
There are a variety of situations in which scheduling may occur.
In each of these cases the process that had most recently been running becomes unready, so another must be chosen to run next.
There are three other occasions when scheduling is usually done, although logically it is not absolutely necessary at these times:
In the case of a new process, it makes sense to reevaluate priorities at this time.
In some cases the parent may be able to request a different priority for its child.
In the case of an I/O interrupt, this usually means that an I/O device has now completed its work.
So some process that was blocked waiting for I/O may now be ready to run.
In the case of a clock interrupt, this is an opportunity to decide whether the currently running process has run too long.
Scheduling algorithms can be divided into two categories with respect to how they deal with clock interrupts.
A nonpreemptive scheduling algorithm picks a process to run and then just lets it run until it blocks (either on I/O or waiting for another process) or until it voluntarily releases the CPU.
In contrast, a preemptive scheduling algorithm picks a process and lets it run for a maximum of some fixed time.
If it is still running at the end of the time interval, it is suspended and the scheduler picks another process to run (if one is available)
Doing preemptive scheduling requires having a clock interrupt occur at the end of the time interval to give control of the CPU back to the scheduler.
If no clock is available, nonpreemptive scheduling is the only option.
Not surprisingly, in different environments different scheduling algorithms are needed.
This situation arises because different application areas (and different kinds of operating systems) have different goals.
In other words, what the scheduler should optimize for is not the same in all systems.
In batch systems, there are no users impatiently waiting at their terminals for a quick response.
Consequently, nonpreemptive algorithms, or preemptive algorithms with long time periods for each process are often acceptable.
In an environment with interactive users, preemption is essential to keep one process from hogging the CPU and denying service to the others.
Even if no process intentionally ran forever, due to a program bug, one process might shut out all the others indefinitely.
In systems with real-time constraints, preemption is, oddly enough, sometimes not needed because the processes know that they may not run for long periods of time and usually do their work and block quickly.
The difference with interactive systems is that real-time systems run only programs that are intended to further the application at hand.
Interactive systems are general purpose and may run arbitrary programs that are not cooperative or even malicious.
In order to design a scheduling algorithm, it is necessary to have some idea of what a good algorithm should do.
Some goals depend on the environment (batch, interactive, or real time), but there are also some that are desirable in all cases.
All systems Fairness — giving each process a fair share of the CPU Policy enforcement — seeing that stated policy is carried out Balance — keeping all parts of the system busy.
Batch systems Throughput — maximize jobs per hour Turnaround time — minimize time between submission and termination CPU utilization — keep the CPU busy all the time.
Interactive systems Response time — respond to requests quickly Proportionality — meet users’ expectations.
Real—time systems Meeting deadlines — avoid losing data Predictability — avoid quality degradation in multimedia systems.
Giving one process much more CPU time than an equivalent one is not fair.
Of course, different categories of processes may be treated differently.
Think of safety control and doing the payroll at a nuclear reactor’s computer center.
If the local policy is that safety control processes get to run whenever they want to, even if it means the payroll is 30 sec late, the scheduler has to make sure this policy is enforced.
Another general goal is keeping all parts of the system busy when possible.
If the CPU and all the I/O devices can be kept running all the time, more work gets done per second than if some of the components are idle.
In a batch system, for example, the scheduler has control of which jobs are brought into memory to run.
Having some CPU-bound processes and some I/O-bound processes in memory together is a better idea than first loading and running all the CPU-bound jobs and then, when they are finished, loading and running all the I/O-bound jobs.
If the latter strategy is used, when the CPU-bound processes are running, they will fight.
Later, when the I/O-bound jobs come in, they will fight for the disk and the CPU will be idle.
Better to keep the whole system running at once by a careful mix of processes.
The managers of corporate computer centers that run many batch jobs (e.g., processing insurance claims) typically look at three metrics to see how well their systems are performing: throughput, turnaround time, and CPU utilization.
Throughput is the number of jobs per second that the system completes.
Turnaround time is the average time from the moment that a batch job is submitted until the moment it is completed.
It measures how long the average user has to wait for the output.
A scheduling algorithm that maximizes throughput may not necessarily minimize turnaround time.
For example, given a mix of short jobs and long jobs, a scheduler that always ran short jobs and never ran long jobs might achieve an excellent throughput (many short jobs per second) but at the expense of a terrible turnaround time for the long jobs.
If short jobs kept arriving at a steady rate, the long jobs might never run, making the mean turnaround time infinite while achieving a high throughput.
Thus computer center managers feel guilty when it is not running all the time.
What really matters is how many jobs per second come out of the system (throughput) and how long it takes to get a job back (turnaround time)
Using CPU utilization as a metric is like rating cars based on how many times per second the engine turns over.
For interactive systems, especially timesharing systems and servers, different goals apply.
The most important one is to minimize response time, that is the time between issuing a command and getting the result.
On a personal computer where a background process is running (for example, reading and storing email from the network), a user request to start a program or open a file should take precedence over the background work.
Having all interactive requests go first will be perceived as good service.
A somewhat related issue is what might be called proportionality.
Users have an inherent (but often incorrect) idea of how long things should take.
When a request that is perceived as complex takes a long time, users accept that, but when a request that is perceived as simple takes a long time, users get irritated.
For example, if clicking on a icon that calls up an Internet provider using an analog modem takes 45 seconds to establish a connection, the user will probably accept that as a fact of life.
This behavior is due to the common user perception that placing a phone call and getting a connection is supposed to take a lot longer than just hanging up.
Real-time systems have different properties than interactive systems, and thus different scheduling goals.
They are characterized by having deadlines that must or at least should be met.
For example, if a computer is controlling a device that produces data at a regular rate, failure to run the data-collection process on time may result in lost data.
Thus the foremost need in a real-time system is meeting all (or most) deadlines.
In some real-time systems, especially those involving multimedia, predictability is important.
Missing an occasional deadline is not fatal, but if the audio process runs too erratically, the sound quality will deteriorate rapidly.
Video is also an issue, but the ear is much more sensitive to jitter than the eye.
To avoid this problem, process scheduling must be highly predictable and regular.
It is now time to turn from general scheduling issues to specific scheduling algorithms.
In this section we will look at algorithms used in batch systems.
In the following ones we will examine interactive and real-time systems.
It is worth pointing out that some algorithms are used in both batch and interactive systems.
Here we will focus on algorithms that are only suitable in batch systems.
Probably the simplest of all scheduling algorithms is nonpreemptive firstcome first-served.
With this algorithm, processes are assigned the CPU in the order they request it.
When the first job enters the system from the outside in the morning, it is started immediately and allowed to run as long as it wants to.
As other jobs come in, they are put onto the end of the queue.
When the running process blocks, the first process on the queue is run next.
When a blocked process becomes ready, like a newly arrived job, it is put on the end of the queue.
The great strength of this algorithm is that it is easy to understand and equally easy to program.
It is also fair in the same sense that allocating scarce sports or concert tickets to people who are willing to stand on line starting at 2 A.M.
With this algorithm, a single linked list keeps track of all ready processes.
Picking a process to run just requires removing one from the front of the queue.
Adding a new job or unblocked process just requires attaching it to the end of the queue.
The compute-bound process runs for 1 sec, then it reads a disk block.
All the I/O processes now run and start disk reads.
When the compute-bound process gets its disk block, it runs for another 1 sec, followed by all the I/O-bound processes in quick succession.
Now let us look at another nonpreemptive batch algorithm that assumes the run times are known in advance.
In an insurance company, for example, people can predict quite accurately how long it will take to run a batch of 1000 claims, since similar work is done every day.
When several equally important jobs are sitting in the input queue waiting to be started, the scheduler picks the shortest job first.
Now let us consider running these four jobs using shortest job first, as shown in Fig.
Consider the case of four jobs, with run times of a, b, c, and d, respectively.
The first job finishes at time a, the second finishes at time a + b, and so on.
It is clear that a contributes more to the average than the other times, so it should be the shortest job, with b next, then c, and finally d as the longest as it affects only its own turnaround time.
The same argument applies equally well to any number of jobs.
It is worth pointing out that shortest job first is only optimal when all the jobs are available simultaneously.
Using shortest job first we will run the jobs in the order A, B, C, D, E, for an average wait of 4.6
However, running them in the order B, C, D, E, A has an average wait of 4.4
A preemptive version of shortest job first is shortest remaining time next.
With this algorithm, the scheduler always chooses the process whose remaining run time is the shortest.
Again here, the run time has to be known in advance.
When a new job arrives, its total time is compared to the current process’ remaining time.
If the new job needs less time to finish than the current process, the current process is suspended and the new job started.
This scheme allows new short jobs to get good service.
From a certain perspective, batch systems allow scheduling at three different levels, as illustrated in Fig.
As jobs arrive at the system, they are initially placed in an input queue stored on the disk.
The admission scheduler decides which jobs to admit to the system.
The others are kept in the input queue until they are selected.
A typical algorithm for admission control might be to look for a mix of compute-bound jobs and I/O-bound jobs.
Alternatively, short jobs could be admitted quickly whereas longer jobs would have to wait.
The admission scheduler is free to hold some jobs in the input queue and admit jobs that arrive later if it so chooses.
Once a job has been admitted to the system, a process can be created for it and it can contend for the CPU.
In that case, some of the processes have to be swapped out to disk.
The second level of scheduling is deciding which processes should be kept in memory and which ones should be kept on disk.
We will call this scheduler the memory scheduler, since it determines which processes are kept in memory and which on the disk.
This decision has to be reviewed frequently to allow the processes on disk to get some service.
However, since bringing a process in from disk is expensive, the review probably should not happen more often than once per second, maybe less often.
If the contents of main memory are shuffled too often, a large amount of disk bandwidth will be wasted, slowing down file I/O.
To optimize system performance as a whole, the memory scheduler might well want to carefully decide how many processes it wants in memory, called the degree of multiprogramming, and what kind of processes.
If it has information about which processes are compute bound and which are I/O bound, it can try to keep a mix of these process types in memory.
As a very crude approximation, if a certain class of process computes about 20% of the time, keeping five of them around is roughly the right number to keep the CPU busy.
To make its decisions, the memory scheduler periodically reviews each process on disk to decide whether or not to bring it into memory.
Among the criteria that it can use to make its decision are the following ones:
The third level of scheduling is actually picking one of the ready processes in main memory to run next.
Often this is called the CPU scheduler and is the one people usually mean when they talk about the ‘‘scheduler.’’ Any suitable algorithm can be used here, either preemptive or nonpreemptive.
These include the ones described above as well as a number of algorithms to be described in the next section.
We will now look at some algorithms that can be used in interactive systems.
All of these can also be used as the CPU scheduler in batch systems as well.
While three-level scheduling is not possible here, two-level scheduling (memory scheduler and CPU scheduler) is possible and common.
Below we will focus on the CPU scheduler and some common scheduling algorithms.
One of the oldest, simplest, fairest, and most widely used algorithms is round robin.
Each process is assigned a time interval, called its quantum, which it is allowed to run.
If the process is still running at the end of the quantum, the CPU is preempted and given to another process.
If the process has blocked or finished before the quantum has elapsed, the CPU switching is done when the process blocks, of course.
All the scheduler needs to do is maintain a list of runnable processes, as shown in Fig.
When the process uses up its quantum, it is put on the end of the list, as shown in Fig.
The only interesting issue with round robin is the length of the quantum.
Suppose that this process switch or context switch, as it is sometimes called, takes 1 msec, including switching memory maps, flushing and reloading the cache, etc.
Also suppose that the quantum is set at 4 msec.
Twenty percent of the CPU time will be wasted on administrative overhead.
To improve the CPU efficiency, we could set the quantum to, say, 100 msec.
But consider what happens on a timesharing system if ten interactive users hit the carriage return key at roughly the same time.
Ten processes will be put on the list of runnable processes.
If the CPU is idle, the first one will start immediately, the second one may not start until 100 msec later, and so on.
The unlucky last one may have to wait 1 sec before getting a chance, assuming all the others use their full quanta.
Most users will perceive a 1-sec response to a short command as sluggish.
Another factor is that if the quantum is set longer than the mean CPU burst, preemption will rarely happen.
Instead, most processes will perform a blocking operation before the quantum runs out, causing a process switch.
Eliminating preemption improves performance because process switches then only happen when.
The conclusion can be formulated as follows: setting the quantum too short causes too many process switches and lowers the CPU efficiency, but setting it too long may cause poor response to short interactive requests.
A quantum of around 20–50 msec is often a reasonable compromise.
Round-robin scheduling makes the implicit assumption that all processes are equally important.
Frequently, the people who own and operate multiuser computers have different ideas on that subject.
At a university, the pecking order may be deans first, then professors, secretaries, janitors, and finally students.
The need to take external factors into account leads to priority scheduling.
The basic idea is straightforward: Each process is assigned a priority, and the runnable process with the highest priority is allowed to run.
Even on a PC with a single owner, there may be multiple processes, some more important than others.
For example, a daemon process sending electronic mail in the background should be assigned a lower priority than a process displaying a video film on the screen in real time.
To prevent high-priority processes from running indefinitely, the scheduler may decrease the priority of the currently running process at each clock tick (i.e., at each clock interrupt)
If this action causes its priority to drop below that of the next highest process, a process switch occurs.
Alternatively, each process may be assigned a maximum time quantum that it is allowed to run.
When this quantum is used up, the next highest priority process is given a chance to run.
The UNIX system has a command, nice, which allows a user to voluntarily reduce the priority of his process, in order to be nice to the other users.
Priorities can also be assigned dynamically by the system to achieve certain system goals.
For example, some processes are highly I/O bound and spend most of their time waiting for I/O to complete.
Whenever such a process wants the CPU, it should be given the CPU immediately, to let it start its next I/O request, which can then proceed in parallel with another process actually computing.
Making the I/O-bound process wait a long time for the CPU will just mean having it around occupying memory for an unnecessarily long time.
A simple algorithm for giving good service to I/O-bound processes is to set the priority to 1/ f, where f is the fraction of the last quantum that a process used.
It is often convenient to group processes into priority classes and use priority scheduling among the classes but round-robin scheduling within each class.
The scheduling algorithm is as follows: as long as there are runnable processes in priority class 4, just run each one for one quantum, round-robin fashion, and never bother with lower priority classes.
If priorities are not adjusted occasionally, lower priority classes may all starve to death.
In MINIX 3, components of the operating system run as processes.
The initial priority of each task or service is defined at compile time; I/O from a slow device may be given lower priority than I/O from a fast device or even a server.
User processes generally have lower priority than system components, but all priorities can change during execution.
Processes in the next class were run for four quanta, and so on.
Whenever a process used up all the quanta allocated to it, it was moved down one class.
As an example, consider a process that needed to compute continuously for 100 quanta.
It would initially be given one quantum, then swapped out.
Next time it would get two quanta before being swapped out.
Furthermore, as the process sank deeper and deeper into the priority queues, it would be run less and less frequently, saving the CPU for short, interactive processes.
The following policy was adopted to prevent a process that needed to run for a long time when it first started but became interactive later, from being punished forever.
Whenever a carriage return was typed at a terminal, the process belonging to that terminal was moved to the highest priority class, on the assumption that it was about to become interactive.
One fine day, some user with a heavily CPUbound process discovered that just sitting at the terminal and typing carriage returns at random every few seconds did wonders for his response time.
Moral of the story: getting it right in practice is much harder than getting it right in principle.
Many other algorithms have been used for assigning processes to priority classes.
When a process that was waiting for terminal input was finally awakened, it went into the highest priority class (terminal)
When a process waiting for a disk block became ready, it went into the second class.
When a process was still running when its quantum ran out, it was initially placed in the third class.
However, if a process used up its quantum too many times in a row without blocking for terminal or other I/O, it was moved down to the bottom queue.
Many other systems use something similar to favor interactive users and processes over background ones.
Because shortest job first always produces the minimum average response time for batch systems, it would be nice if it could be used for interactive processes as well.
Interactive processes generally follow the pattern of wait for command, execute command, wait for command, execute command, and so on.
If we regard the execution of each command as a separate ‘‘job,’’ then we could minimize overall response time by running the shortest one first.
The only problem is figuring out which of the currently runnable processes is the shortest one.
One approach is to make estimates based on past behavior and run the process with the shortest estimated running time.
The technique of estimating the next value in a series by taking the weighted average of the current measured value and the previous estimate is sometimes called aging.
It is applicable to many situations where a prediction must be made based on previous values.
Aging is especially easy to implement when a = 1/2
A completely different approach to scheduling is to make real promises to the users about performance and then live up to them.
One promise that is realistic to make and easy to live up to is this: If there are n users logged in while you are working, you will receive about 1/n of the CPU power.
Similarly, on a singleuser system with n processes running, all things being equal, each one should get 1/n of the CPU cycles.
To make good on this promise, the system must keep track of how much CPU each process has had since its creation.
It then computes the amount of CPU each one is entitled to, namely the time since creation divided by n.
Since the amount of CPU time each process has actually had is also known, it is straightforward to compute the ratio of actual CPU time consumed to CPU time entitled.
The algorithm is then to run the process with the lowest ratio until its ratio has moved above its closest competitor.
While making promises to the users and then living up to them is a fine idea, it is difficult to implement.
However, another algorithm can be used to give similarly predictable results with a much simpler implementation.
The basic idea is to give processes lottery tickets for various system resources, such as CPU time.
Whenever a scheduling decision has to be made, a lottery ticket is chosen at random, and the process holding that ticket gets the.
To paraphrase George Orwell: ‘‘All processes are equal, but some processes are more equal.’’ More important processes can be given extra tickets, to increase their odds of winning.
In the long run, it will get about 20 percent of the CPU.
In contrast to a priority scheduler, where it is very hard to state what having a priority of 40 actually means, here the rule is clear: a process holding a fraction f of the tickets will get about a fraction f of the resource in question.
For example, if a new process shows up and is granted some tickets, at the very next lottery it will have a chance of winning in proportion to the number of tickets it holds.
For example, when a client process sends a message to a server process and then blocks, it may give all of its tickets to the server, to increase the chance of the server running next.
When the server is finished, it returns the tickets so the client can run again.
In fact, in the absence of clients, servers need no tickets at all.
Lottery scheduling can be used to solve problems that are difficult to handle with other methods.
One example is a video server in which several processes are feeding video streams to their clients, but at different frame rates.
So far we have assumed that each process is scheduled on its own, without regard to who its owner is.
To prevent this situation, some systems take into account who owns a process before scheduling it.
In this model, each user is allocated some fraction of the CPU and the scheduler picks processes in such a way as to enforce it.
Thus if two users have each been promised 50% of the CPU, they will each get that, no matter how many processes they have in existence.
As an example, consider a system with two users, each of which has been promised 50% of the CPU.
If round-robin scheduling is used, a possible scheduling sequence that meets all the constraints is this one:
Numerous other possibilities exist, of course, and can be exploited, depending on what the notion of fairness is.
A real-time system is one in which time plays an essential role.
Typically, one or more physical devices external to the computer generate stimuli, and the computer must react appropriately to them within a fixed amount of time.
For example, the computer in a compact disc player gets the bits as they come off the drive and must convert them into music within a very tight time interval.
If the calculation takes too long, the music will sound peculiar.
Other real-time systems are patient monitoring in a hospital intensive-care unit, the autopilot in an aircraft, and robot control in an automated factory.
In all these cases, having the right answer but having it too late is often just as bad as not having it at all.
Real-time systems are generally categorized as hard real time, meaning there are absolute deadlines that must be met, or else, and soft real time, meaning that missing an occasional deadline is undesirable, but nevertheless tolerable.
In both cases, real-time behavior is achieved by dividing the program into a number of processes, each of whose behavior is predictable and known in advance.
These processes are generally short lived and can run to completion in well under a second.
When an external event is detected, it is the job of the scheduler to schedule the processes in such a way that all deadlines are met.
The events that a real-time system may have to respond to can be further categorized as periodic (occurring at regular intervals) or aperiodic (occurring unpredictably)
A system may have to respond to multiple periodic event streams.
Depending on how much time each event requires for processing, it may not even be possible to handle them all.
For example, if there are m periodic events and event i occurs with period Pi and requires Ci seconds of CPU time to handle each event, then the load can only be handled if.
A real-time system that meets this criteria is said to be schedulable.
Implicit in this calculation is the assumption that the context-switching overhead is so small that it can be ignored.
The former make their scheduling decisions before the system starts running.
Static scheduling only works when there is perfect information available in advance about the work needed to be done and the deadlines that have to be met.
Up until now, we have tacitly assumed that all the processes in the system belong to different users and are thus competing for the CPU.
While this is often true, sometimes it happens that one process has many children running under its control.
For example, a database management system process may have many children.
It is entirely possible that the main process has an excellent idea of which of its children are the most important (or the most time critical) and which the least.
Unfortunately, none of the schedulers discussed above accept any input from user processes about scheduling decisions.
As a result, the scheduler rarely makes the best choice.
The solution to this problem is to separate the scheduling mechanism from the scheduling policy.
What this means is that the scheduling algorithm is parameterized in some way, but the parameters can be filled in by user processes.
Suppose that the kernel uses a priority scheduling algorithm but provides a system call by which a process can set (and change) the priorities of its children.
In this way the parent can control in detail how its children are scheduled, even though it does not do the scheduling itself.
Here the mechanism is in the kernel but policy is set by a user process.
When several processes each have multiple threads, we have two levels of parallelism present: processes and threads.
Scheduling in such systems differs substantially depending on whether user-level threads or kernel-level threads (or both) are supported.
Since the kernel is not aware of the existence of threads, it operates as it always does, picking a process, say, A, and giving A control for its quantum.
The thread scheduler inside A decides which thread to run, say A1
Since there are no clock interrupts to multiprogram threads, this thread may continue running as long as it wants to.
If it uses up the process’ entire quantum, the kernel will select another process to run.
When the process A finally runs again, thread A1 will resume running.
It will continue to consume all of A’s time until it is finished.
They will get whatever the scheduler considers their appropriate share, no matter what is going on inside process A.
Consequently, each one runs for a little while, then yields the CPU back to the thread scheduler.
The scheduling algorithm used by the run-time system can be any of the ones described above.
In practice, round-robin scheduling and priority scheduling are most common.
The only constraint is the absence of a clock to interrupt a thread that has run too long.
It does not have to take into account which process the thread belongs to, but it can if it wants to.
The thread is given a quantum and is forceably suspended if it exceeds the quantum.
A major difference between user-level threads and kernel-level threads is the performance.
Doing a thread switch with user-level threads takes a handful of machine instructions.
With kernel-level threads it requires a full context switch, changing the memory map, and invalidating the cache, which is several orders of.
On the other hand, with kernel-level threads, having a thread block on I/O does not suspend the entire process as it does with user-level threads.
Since the kernel knows that switching from a thread in process A to a thread in process B is more expensive that running a second thread in process A (due to having to change the memory map and having the memory cache spoiled), it can take this information into account when making a decision.
For example, given two threads that are otherwise equally important, with one of them belonging to the same process as a thread that just blocked and one belonging to a different process, preference could be given to the former.
For example, consider a web server which has a dispatcher thread to accept and distribute incoming requests to worker threads.
Suppose that a worker thread has just blocked and the dispatcher thread and two worker threads are ready.
Who should run next? The run-time system, knowing what all the threads do, can easily pick the dispatcher to run next, so it can start another worker running.
This strategy maximizes the amount of parallelism in an environment where workers frequently block on disk I/O.
With kernellevel threads, the kernel would never know what each thread did (although they could be assigned different priorities)
Unlike UNIX, whose kernel is a monolithic program not split up into modules, MINIX 3 itself is a collection of processes that communicate with each other and also with user processes, using a single interprocess communication primitive—message passing.
This design gives a more modular and flexible structure, making it easy, for example, to replace the entire file system by a completely different one, without having even to recompile the kernel.
Let us begin our study of MINIX 3 by taking a bird’s-eye view of the system.
The kernel in the bottom layer schedules processes and manages the transitions between the ready, running, and blocked states of Fig.
Message handling requires checking for legal destinations, locating the send and receive buffers in physical memory, and.
Only processes in the bottom layer may use privileged (kernel mode) instructions.
Also part of the kernel is support for access to I/O ports and interrupts, which on modern processors require use of privileged kernel mode instructions not available to ordinary processes.
In addition to the kernel itself, this layer contains two modules that function similarly to device drivers.
The clock task is an I/O device driver in the sense that it interacts with the hardware that generates timing signals, but it is not useraccessible like a disk or communications line driver—it interfaces only with the kernel.
One of the main functions of layer 1 is to provide a set of privileged kernel calls to the drivers and servers above it.
These include reading and writing I/O ports, copying data between address spaces, and so on.
Implementation of these calls is done by the system task.
Although the system task and the clock task are compiled into the kernel’s address space, they are scheduled as separate processes and have their own call stacks.
Most of the kernel and all of the clock and system tasks are written in C.
However, a small amount of the kernel is written in assembly language.
The assembly language parts deal with interrupt handling, the low-level mechanics of managing context switches between processes (saving and restoring registers and the like), and low-level parts of manipulating the MMU hardware.
By and large, the assembly-language code handles those parts of the kernel that deal directly with the hardware at a very low level and which cannot be expressed in C.
These parts have to be rewritten when MINIX 3 is ported to a new architecture.
The three layers above the kernel could be considered to be a single layer because the kernel fundamentally treats them all of them the same way.
Each one is limited to user mode instructions, and each is scheduled to run by the kernel.
Furthermore, none of them can access memory outside the segments allotted to it.
However, processes potentially have special privileges (such as the ability to make kernel calls)
For example, processes in layer 2, called device drivers, are allowed to request that the system task read data from or write data to I/O ports on their behalf.
A driver is needed for each device type, including disks, printers, terminals, and network interfaces.
If other I/O devices are present, a driver is needed for each one of those, as well.
Device drivers may also make other kernel calls, such as requesting that newlyread data be copied to the address space of a different process.
The third layer contains servers, processes that provide useful services to the user processes.
The process manager (PM) carries out all the MINIX 3 system calls that involve starting or stopping process execution, such as fork, exec, and exit, as well as system calls related to signals, such as alarm and kill, which can alter the execution state of a process.
The process manager also is responsible for managing memory, for instance, with the brk system call.
The file system (FS) carries out all the file system calls, such as read, mount, and chdir.
It is important to understand the difference between kernel calls and POSIX system calls.
Kernel calls are low-level functions provided by the system task to allow the drivers and servers to do their work.
Reading a hardware I/O port is a typical kernel call.
User programs contain many POSIX calls but no kernel calls.
Occasionally when we are not being careful with our language we may call a kernel call a system call.
The mechanisms used to make these calls are similar, and kernel calls can be considered a special subset of system calls.
It is safe to say that the functionality of the process manager and the file system will be found in any operating system.
The information server (IS) handles jobs such as providing debugging and status information about other drivers and servers, something that is more necessary in a system like MINIX 3, designed for experimentation, than would be the case for a commercial operating system which users cannot alter.
The reincarnation server (RS) starts, and if necessary restarts, device drivers that are not loaded into memory at the same time as the kernel.
In particular, if a driver fails during operation, the reincarnation server detects this failure, kills the driver if it is not already dead, and starts a fresh copy of the driver, making the system highly fault tolerant.
Servers cannot do I/O directly, but they can communicate with drivers to request I/O.
Servers can also communicate with the kernel via the system task.
The file system has been carefully designed as a file ‘‘server’’ and could be moved to a remote machine with few changes.
The system does not need to be recompiled to include additional servers.
The process manager and the file system can be supplemented with the network server and other servers by attaching additional servers as required when MINIX 3 starts up or later.
Device drivers, although typically started when the system is started, can also be started later.
Both device drivers and servers are compiled and stored on disk as ordinary executable files, but when properly started up they are granted access to the special privileges needed.
A user program called service provides an interface to the reincarnation server which manages this.
Although the drivers and servers are independent processes, they differ from user processes in that normally they never terminate while the system is active.
They do not belong to any user, and many if not all of them will be activated before the first user logs on.
Another difference between system processes and user processes is that system processes have higher execution priority than user processes.
In fact, normally drivers have higher execution priority than servers, but this is not automatic.
Execution priority is assigned on a case-by-case basis in MINIX 3; it is possible for a driver that services a slow device to be given lower priority than a server that must respond quickly.
Finally, layer 4 contains all the user processes—shells, editors, compilers, and user-written a.out programs.
Many user processes come and go as users log in, do work, and log out.
A running system normally has some user processes that are started when the system is booted and which run forever.
One of these is init, which we will describe in the next section.
A daemon is a background process that executes periodically or always waits for some event, such as the arrival of a packet from the network.
In a sense a daemon is a server that is started independently and runs as a user process.
Like true servers installed at startup time, it is possible to configure a daemon to have a higher priority than ordinary user processes.
A note about the terms task and device driver is needed.
In older versions of MINIX all device drivers were compiled together with the kernel, which gave them access to data structures belonging to the kernel and each other.
They were referred to as ‘‘tasks’’ to distinguish them from pure independent user-space processes.
In MINIX 3, device drivers have been implemented completely in user-space.
The only exception is the clock task, which is arguably not a device driver in the same sense as drivers that can be accessed through device files by user processes.
Within the text we have taken pains to use the term ‘‘task’’ only when referring to the clock task or the system task, both of which are compiled into the kernel to function.
However, function names, variable names, and comments in the source code have not been as carefully updated.
Processes in MINIX 3 follow the general process model described at length earlier in this chapter.
Processes can create subprocesses, which in turn can create more subprocesses, yielding a tree of processes.
Servers and drivers are a special case, of course, since some of them must be started before any user process, including init.
How does an operating system start up? We will summarize the MINIX 3 startup sequence in the next few pages.
For a look at how some other operating systems do this, see Dodge et al.
On most computers with disk devices, there is a boot disk hierarchy.
Typically, if a floppy disk is in the first floppy disk drive, it will be the boot disk.
If no floppy disk is present and a CD-ROM is present in the first CD-ROM drive, it becomes the boot disk.
If there is neither a floppy disk nor a CD-ROM present, the first hard drive becomes the boot disk.
The order of this hierarchy may be configurable by entering the BIOS immediately after powering the computer up.
Additional devices, especially other removable storage devices, may be supported as well.
When the computer is turned on, if the boot device is a diskette, the hardware reads the first sector of the first track of the boot disk into memory and executes the code it finds there.
It is very small, since it has to fit in one sector (512 bytes)
The MINIX 3 bootstrap loads a larger program, boot, which then loads the operating system itself.
A hard disk is divided into partitions, and the first sector of a hard disk contains a small program and the disk’s partition table.
Collectively these two pieces are called the master boot record.
The program part is executed to read the partition table and to select the active partition.
The active partition has a bootstrap on its first sector, which is then loaded and executed to find and start a copy of boot in the partition, exactly as is done when booting from a diskette.
CD-ROMs came along later in the history of computers than floppy disks and hard disks, and when support for booting from a CD-ROM is present it is capable.
A computer that supports booting from a CD-ROM can load a large block of data into memory immediately.
Typically what is loaded from the CD-ROM is an exact copy of a bootable floppy disk, which is placed in memory and used as a RAM disk.
After this first step control is transferred to the RAM disk and booting continues exactly as if a physical floppy disk were the boot device.
On an older computer which has a CD-ROM drive but does not support booting from a CD-ROM, the bootable floppy disk image can be copied to a floppy disk which can then be used to start the system.
The CD-ROM must be in the CD-ROM drive, of course, since the bootable floppy disk image expects that.
In any case, the MINIX 3 boot program looks for a specific multipart file on the diskette or partition and loads the individual parts into memory at the proper locations.
The most important parts are the kernel (which include the clock task and the system task), the process manager, and the file system.
Additionally, at least one disk driver must be loaded as part of the boot image.
There are several other programs loaded in the boot image.
These include the reincarnation server, the RAM disk, console, and log drivers, and init.
It should be strongly emphasized that all parts of the boot image are separate programs.
After the essential kernel, process manager and file system have been loaded many other parts could be loaded separately.
It gives ordinary processes loaded after initialization the special priorities and privileges which make them into system processes, It can also restart a crashed driver, which explains its name.
As mentioned above, at least one disk driver is essential.
If the root file system is to be copied to a RAM disk, the memory driver is also required, otherwise it could be loaded later.
The tty and log drivers are optional in the boot image.
They are loaded early just because it is useful to be able to display messages on the console and save information to a log early in the startup process.
Init could certainly be loaded later, but it controls initial configuration of the system, and it was easiest just to include it in the boot image file.
Operations that are in the realms of the disk driver and the file system must be performed by boot before these parts of the system are active.
In a later section we will detail how MINIX 3 is started.
For now, suffice it to say that once the loading operation is complete the kernel starts running.
During its initialization phase the kernel starts the system and clock tasks, and then the process manager and the file system.
The process manager and the file system then cooperate in starting other servers and drivers that are part of the boot image.
When all these have run and initialized themselves, they will block, waiting for something to do.
Only when all tasks, drivers, and servers loaded in the boot image have blocked will init, the first user process, be executed.
System components loaded with the boot image or during initialization are shown in Fig.
Others such as an Ethernet driver and the inet server may also be present.
Init is the first user process, and also the last process loaded as part of the boot image.
You might think building of a process tree such as that of Fig.
That would be true in a conventional operating system, but MINIX 3 is different.
First, there are already quite a few system processes running by the time init gets to run.
The tasks CLOCK and SYSTEM that run within the kernel are unique processes that are not visible outside of the kernel.
They receive no PIDs and are not considered part of any tree of processes.
The process manager is the first process to run in user space; it is given PID 0 and is neither a child nor a parent of any other process.
The reincarnation server is made the parent of all the other processes started from the boot image (e.g., the drivers and servers)
The logic of this is that the reincarnation server is the process that should be informed if any of these should need to be restarted.
As we will see, even after init starts running there are differences between the way a process tree is built in MINIX 3 and the conventional concept.
Like all the user space processes in the boot image (except the process manager), init is made one of the children of the reincarnation server.
As in a standard UNIX-like system, init first executes the /etc/rc shell script.
Any program started by the rc script will be a child of init.
One of the first programs run is a utility called service.
Service itself runs as a child of init, as would be expected.
The reincarnation server starts an ordinary program and converts it into a system process.
One of the actions of the reincarnation server is to adopt all system processes except the process manager as its own children.
After the cmos device driver has been started the rc script can initialize the real-time clock.
Up to this point all files needed must be found on the root device.
The servers and drivers needed initially are in the /sbin directory; other commands needed for startup are in /bin.
Once the initial startup steps have been completed other file systems such as /usr are mounted.
An important function of the rc script is to check for file system problems that might have resulted from a previous system crash.
The test is simple—when the system is shutdown correctly by executing the shutdown command an entry is written to the login history file, /usr/adm/wtmp.
The command shutdown –C checks whether the last entry in wtmp is a shutdown entry.
If not, it is assumed an abnormal shutdown occurred, and the fsck utility is run to check all file systems.
If you look at the output of a ps axl command, which shows both PIDs and parent PIDs (PPIDs), you will see that daemons such as update and usyslogd will normally be the among the first persistent processes which are children of init.
Finally init reads the file /etc/ttytab, which lists all potential terminal devices.
Those devices that can be used as login terminals (in the standard distribution, just the main console and up to three virtual consoles, but serial lines and network pseudo terminals can be added) have an entry in the getty field of /etc/ttytab, and init forks off a child process for each such terminal.
Normally, each child executes /usr/bin/getty which prints a message, then waits for a name to be typed.
If a particular terminal requires special treatment (e.g., a dial-up line) /etc/ttytab can specify a command (such as /usr/bin/stty) to be executed to initialize the line before running getty.
When a user types a name to log in, /usr/bin/login is called with the name as its argument.
Login determines if a password is required, and if so prompts for and verifies the password.
After a successful login, login executes the user’s shell (by default /bin/sh, but another shell may be specified in the /etc/passwd file)
The shell waits for commands to be typed and then forks off a new process for each command.
In this way, the shells are the children of init, the user processes are the grandchildren of init, and all the user processes in the system are part of a single tree.
In fact, except for the tasks compiled into the kernel and the process.
But unlike the process tree of a conventional UNIX system, init is not at the root of the tree, and the structure of the tree does not allow one to determine the order in which system processes were started.
The two principal MINIX 3 system calls for process management are fork and exec.
Fork is the only way to create a new process.
When a program is executed, it is allocated a portion of memory whose size is specified in the program file’s header.
It keeps this amount of memory throughout its execution, although the distribution among data segment, stack segment, and unused can vary as the process runs.
All the information about a process is kept in the process table, which is divided up among the kernel, process manager, and file system, with each one having those fields that it needs.
When a new process comes into existence (by fork), or an old process terminates (by exit or a signal), the process manager first updates its part of the process table and then sends messages to the file system and kernel telling them to do likewise.
Each task, driver or server process is allowed to exchange messages only with certain other processes.
Details of how this is enforced will be described later.
The usual flow of messages is downward in the layers of Fig 2-29, and messages can be between processes in the same layer or between processes in adjacent.
When a process sends a message to a process that is not currently waiting for a message, the sender blocks until the destination does a receive.
In other words, MINIX 3 uses the rendezvous method to avoid the problems of buffering sent, but not yet received, messages.
The advantage of this approach is that it is simple and eliminates the need for buffer management (including the possibility of running out of buffers)
In addition, because all messages are of fixed length determined at compile time, buffer overrun errors, a common source of bugs, are structurally prevented.
The basic purpose of the restrictions on exchanges of messages is that if process A is allowed to generate a send or sendrec directed to process B, then process B can be allowed to call receive with A designated as the sender, but B should not be allowed to send to A.
Obviously, if A tries to send to B and blocks, and B tries to send to A and blocks we have a deadlock.
The ‘‘resource’’ that each would need to complete the operations is not a physical resource like an I/O device, it is a call to receive by the target of the message.
We will have more to say about deadlocks in Chap.
The message mechanism is used to deliver a notification, but the information conveyed is limited.
In the general case the message contains only the identity of the sender and a timestamp added by the kernel.
In MINIX 3, function keys are used to trigger debugging dumps.
The Ethernet driver is an example of a process that generates only one kind of debug dump and never needs to get any other communication from the console driver.
In other cases a notification is not sufficient, but upon receiving a notification the target process can send a message to the originator of the notification to request more information.
Because a notify call does not block, it can be made when the recipient has not yet done a receive.
But the simplicity of the message means that a notification that cannot be received is.
Notifications are meant for use between system processes, of which there can be only a relatively small number.
Every system process has a bitmap for pending notifications, with a distinct bit for every system process.
So if process A needs to send a notification to process B at a time when process B is not blocked on a receive, the message-passing mechanism sets a bit which corresponds to A in B’s bitmap of pending notifications.
When B finally does a receive, the first step is to check its pending notifications bitmap.
It can learn of attempted notifications from multiple sources this way.
The single bit is enough to regenerate the information content of the notification.
It tells the identity of the sender, and the message passing code in the kernel adds the timestamp when it is delivered.
Timestamps are used primarily to see if timers have expired, so it does not matter that the timestamp may be for a time later than the time when the sender first tried to send the notification.
In certain cases an additional field of the notification message is used.
When the notification is generated to inform a recipient of an interrupt, a bitmap of all possible sources of interrupts is included in the message.
And when the notification is from the system task a bitmap of all pending signals for the recipient is part of the message.
The natural question at this point is, how can this additional information be stored when the notification must be sent to a process that is not trying to receive a message? The answer is that these bitmaps are in kernel data structures.
They do not need to be copied to be preserved.
If a notification must be deferred and reduced to setting a single bit, when the recipient eventually does a receive and the notification message is regenerated, knowing the origin of the notification is enough to specify which additional information needs to be included in the message.
They are less important than send, receive, sendrec, and notify.
The interrupt system is what keeps a multiprogramming operating system going.
Processes block when they make requests for input, allowing other processes to execute.
When input becomes available, the current running process is interrupted by the disk, keyboard, or other hardware.
The clock also generates interrupts that are used to make sure a running user process that has not requested input eventually relinquishes the CPU, to give other processes their chance to run.
It is the job of the lowest layer of MINIX 3 to hide these interrupts by turning them into messages.
As far as processes are concerned, when an I/O device completes.
Interrupts are also generated by software, in which case they are often called traps.
The send and receive operations that we described above are translated by the system library into software interrupt instructions which have exactly the same effect as hardware-generated interrupts—the process that executes a software interrupt is immediately blocked and the kernel is activated to process the interrupt.
User programs do not refer to send or receive directly, but any time one of the system calls listed in Fig.
Each time a process is interrupted (whether by a conventional I/O device or by the clock) or due to execution of a software interrupt instruction, there is an opportunity to redetermine which process is most deserving of an opportunity to run.
Of course, this must be done whenever a process terminates, as well, but in a system like MINIX 3 interruptions due to I/O operations or the clock or message passing occur more frequently than process termination.
Sixteen queues are defined, although recompiling to use more or fewer queues is easy.
The lowest priority queue is used only by the IDLE process which runs when there is nothing else to do.
User processes start by default in a queue several levels higher than the lowest one.
Servers are normally scheduled in queues with priorities higher than allowed for user processes, drivers in queues with priorities higher than those of servers, and the clock and system tasks are scheduled in the highest priority queue.
Not all of the sixteen available queues are likely to be in use at any time.
A process may be moved to a different priority queue by the system or (within certain limits) by a user who invokes the nice command.
The extra levels are available for experimentation, and as additional drivers are added to MINIX 3 the default settings can be adjusted for best performance.
For instance, if it were desired to add a server to stream digital audio or video to a network, such a server might be assigned a higher starting priority than current servers, or the initial priority of a current server or driver might be reduced in order for the new server to achieve better performance.
In addition to the priority determined by the queue on which a process is placed, another mechanism is used to give some processes an edge over others.
The quantum, the time interval allowed before a process is preempted, is not the same for all processes.
However, as a hedge against malfunction they are made preemptable, but are given a large quantum.
They are allowed to run for a large but finite number of clock ticks, but if they use their entire quantum they are preempted in order not to hang the system.
In such a case the timed-out process will be considered ready, and can be put on the end of its queue.
However, if a process that has used up its entire quantum is found to have.
In this case its priority is lowered by putting it on the end of a lower priority queue.
If the process times out again and another process still has not been able to run, its priority will again be lowered.
A process that has been demoted in priority can earn its way back to a higher priority queue.
If a process uses all of its quantum but is not preventing other processes from running it will be promoted to a higher priority queue, up to the maximum priority permitted for it.
Such a process apparently needs its quantum, but is not being inconsiderate of others.
Otherwise, processes are scheduled using a slightly modified round robin.
If a process has not used its entire quantum when it becomes unready, this is taken to mean that it blocked waiting for I/O, and when it becomes ready again it is put on the head of the queue, but with only the left-over part of its previous quantum.
This is intended to give user processes quick response to I/O.
A process that became unready because it used its entire quantum is placed at the end of the queue in pure round robin fashion.
With tasks normally having the highest priority, drivers next, servers below drivers, and user processes last, a user process will not run unless all system processes have nothing to do, and a system process cannot be prevented from running by a user process.
When picking a process to run, the scheduler checks to see if any processes are queued in the highest priority queue.
If one or more are ready, the one at the head of the queue is run.
If none is ready the next lower priority queue is similarly tested, and so on.
Since drivers respond to requests from servers and servers respond to requests from user processes, eventually all high priority processes should complete whatever work was requested of them.
They will then block with nothing to do until user processes get a turn to run and make more requests.
If no process is ready, the IDLE process is chosen.
This puts the CPU in a lowpower mode until the next interrupt occurs.
At each clock tick, a check is made to see if the current process has run for more than its allotted quantum.
If it has, the scheduler moves it to the end of its queue (which may require doing nothing if it is alone on the queue)
Then the next process to run is picked, as described above.
Only if there are no processes on higher-priority queues and if the previous process is alone on its queue will it get to run again immediately.
Otherwise the process at the head of the highest priority nonempty queue will run next.
Essential drivers and servers are given such large quanta that normally they are normally never preempted by the clock.
But if something goes wrong their priority can be temporarily lowered to prevent the system from coming to a total standstill.
Probably nothing useful can be done if this happens to an essential server, but it may be possible to shut the system down gracefully, preventing data loss and possibly collecting information that can help in debugging the problem.
The book and the software, both of which are continuously evolving, did not ‘‘go to press’’ on the same day, so there may be minor discrepancies between the references to the code, the printed listing, and the CD-ROM version.
Such differences generally only affect a line or two, however.
The source code printed in the book has been simplified by omitting code used to compile options that are not discussed in the book.
We will refer to all of these as Intel 32-bit processors.
The full path to the C language source code on a standard Intel-based platform is /usr/src/ (a trailing ‘‘/’’ in a path name indicates that it refers to a directory)
The source directory tree for other platforms may be in a different location.
Throughout the book, MINIX 3 source code files will be referred to using a path starting with the top src/ directory.
An important subdirectory of the source tree is src/include/, where the master copy of the C header files are located.
Each directory in the source tree contains a file named Makefile which directs the operation of the UNIX-standard make utility.
The Makefile controls compilation of files in its directory and may also direct compilation of files in one or more subdirectories.
The operation of make is complex and a full description is beyond the scope of this section, but it can be summarized by saying that make manages efficient compilation of programs involving multiple source files.
It tests previously compiled modules to see if they are up to date and recompiles any whose source files have been modified since the previous compilation.
This saves time by avoiding recompilation of files that do not need to be recompiled.
Finally, make directs the combination of separately compiled modules into an executable program and may also manage installation of the completed program.
All or part of the src/ tree can be relocated, since the Makefile in each source directory uses a relative path to C source directories.
For instance, you may want to make a source directory on the root filesystem, /src/, for speedy compilation if the root device is a RAM disk.
If you are developing a special version you can make a copy of src/ under another name.
The path to the C header files is a special case.
During compilation every Makefile expects to find header files in /usr/include/ (or the equivalent path on a non-Intel platform)
Before recompiling the system, however, the entire /usr/include/ directory tree is deleted and /usr/src/include/ is copied to /usr/include/
This was done to make it possible to keep all files needed in the development of MINIX 3 in one place.
This also makes it easy to maintain multiple copies of the entire source and headers tree for experimenting with different configurations of the MINIX 3 system.
However, if you want to edit a header file as part of such an experiment, you must be sure to edit the copy in the src/include directory and not the one in /usr/include/
This is a good place to point out for newcomers to the C language how file names are quoted in a #include statement.
Every C compiler has a default header directory where it looks for include files.
The include/ directory contains a number of POSIX standard header files.
For instance, include/arpa/ and the include/net/ directory and its subdirectory include/net/gen/ support network extensions.
These are not necessary for compiling the basic MINIX 3 system, and files in these directories are not listed in Appendix B.
In addition to src/include/, the src/ directory contains three other important subdirectories with operating system source code:
Three other source code directories are not printed or discussed in the text, but are essential to producing a working system:
The standard distribution of MINIX 3 includes many additional source files not discussed in this text.
In addition to the process manager and file system source code, the system source directory src/servers/ contains source code for the init program and the reincarnation server, rs, both of which are essential parts of a running MINIX 3 system.
Src/drivers/ has source code for device drivers not discussed in this text, including alternative disk drivers, sound cards, and network adapters.
An operating system exists, of course, to support commands (programs) that will run on it, so there is a large src/commands/ directory with source code for the utility programs (e.g., cat, cp, date, ls, pwd and more than 200 others)
Source code for some major open source applications originally developed by the GNU and BSD projects is here, too.
The ‘‘book’’ version is compiled using modified Makefiles that do not refer to unnecessary files.
A standard Makefile requires that files for optional components be present, even if not to be compiled.
Omitting these files and the conditional statements that select them makes reading the code easier.
For convenience we will usually refer to simple file names when it it is clear from the context what the complete path is.
However, be aware that some file names appear in more than one directory.
The files in a particular directory will be discussed together, so there should not be any confusion.
The files are listed in Appendix B in the order they are discussed in the text, to make it easier to follow along.
Acquisition of a couple of bookmarks might be of use at this point, so you can go back and forth between the text and the listing.
To keep the size of the listing reasonable, code for every file is not printed.
In general, those functions that are described in detail in the text are listed in Appendix B; those that are just mentioned in passing are not listed, but the complete source is on the CD-ROM and Web site, both of which also provide an index to functions, definitions, and global variables in the source code.
Appendix C contains an alphabetical list of all files described in Appendix B, divided into sections for headers, drivers, kernel, file system, and process manager.
This appendix and the Web site and CD-ROM indices reference the listed objects by line number in the source code.
The code for layer 1 is contained in the directory src/kernel/
Files in this directory support process control, the lowest layer of the MINIX 3 structure we saw in Fig.
This layer includes functions which handle system initialization, interrupts, message passing and process scheduling.
Intimately connected with these are two modules compiled into the same binary, but which run as independent processes.
These are the system task which provides an interface between kernel services and processes in higher layers, and the clock task which provides timing signals to the kernel.
There are several options, for installing MINIX 3 in different ways.
Memory layout after MINIX 3 has been loaded from the disk into memory.
The kernel, servers, and drivers are independently compiled and linked programs, listed on the left.
To install a working MINIX 3 system capable of being booted, a program called installboot (whose source is in src/boot/) adds names to kernel, pm, fs, init, and the other components of the boot image, pads each one out so that its length is.
This new file is the boot image and can be copied into the /boot/ directory or the /boot/image/ directory of a floppy disk or a hard disk partition.
Later, the boot monitor program can load the boot image and transfer control to the operating system.
Figure 2-31 shows the layout of memory after the concatenated programs are separated and loaded.
The kernel is loaded in low memory, all the other parts of the boot image are loaded above 1 MB.
When user programs are run, the available memory above the kernel will be used first.
When a new program will not fit there, it will be loaded in the high memory range, above init.
This is a modest amount; more is recommended if adequate memory is available.
On the other hand, if the size of the block cache were reduced drastically it would be possible to make the entire system fit into less than 640K of memory, with room for a few user processes as well.
It is important to realize that MINIX 3 consists of several totally independent programs that communicate only by passing messages.
A procedure called panic in the directory src/servers/fs/ does not conflict with a procedure called panic in src/servers/pm/ because they ultimately are linked into different executable files.
The only procedures that the three pieces of the operating system have in common are a few of the library routines in src/lib/
This modular structure makes it very easy to modify, say, the file system, without having these changes affect the process manager.
It also makes it straightforward to remove the file system altogether and to put it on a different machine as a file server, communicating with user machines by sending messages over a network.
As another example of the modularity of MINIX 3, adding network support makes absolutely no difference to the process manager, the file system, or the kernel.
Both an Ethernet driver and the inet server can be activated after the boot image is loaded; they would appear in Fig.
A MINIX 3 system with networking enabled can be used as a remote terminal or an ftp and web server.
The include/ directory and its subdirectories contain a collection of files defining constants, macros, and types.
The POSIX standard requires many of these definitions and specifies in which files of the main include/ directory and its subSEC.
The files in these directories are header or include files, identified by the suffix .h, and used by means of #include statements in C source files.
These statements are a built-in feature of the C language.
Headers likely to be needed for compiling user programs are mainly found in include/ whereas include/sys/ traditionally is used for files that are used primarily for compiling system programs and utilities.
The distinction is not terribly important, and a typical compilation, whether of a user program or part of the operating system, will include files from both of these directories.
We will discuss here the files that are needed to compile the standard MINIX 3 system, first treating those in include/ and then those in include/sys/
In the next section we will discuss files in the include/minix/ and include/ibm/ directories, which, as the directory names indicate, are unique to MINIX 3 and its implementation on IBM-type (really, Intel-type) computers.
The first headers to be considered are truly general purpose ones, so much so that they are not referenced directly by any of the C language source files for the MINIX 3 system.
Each master header is tailored to the needs of the corresponding part of the MINIX 3 system, but each one starts with a section like the one shown in Fig.
The master headers will be discussed again in other sections of the book.
This preview is to emphasize that headers from several directories are used together.
In this section and the next one we will mention each of the files referenced in Fig.
Part of a master header which ensures inclusion of header files needed by all C source files.
Note that two const.h files, one from the include/ tree and one from the local directory, are referenced.
Let us start with the first header in include/, ansi.h (line 0000)
This is the second header that is processed whenever any part of the MINIX 3 system is.
It is intended that MINIX 3 should be easy to port to new machines, and allowing older compilers is part of this.
Before we leave ansi.h let us mention one additional feature.
The entire file (except for initial comments) is enclosed between lines that read.
This file defines many basic sizes, both language types such as the number of bits in an integer, as well as operating system limits such as the length of a file name.
Note that for convenience, the line numbering in Appendix B is ratcheted up to the next multiple of 100 when a new file is listed.
In this way, small changes to one file will (probably) not affect subsequent files in a revised listing.
Also note that when a new file is encountered in the listing, a special three-line header consisting of a row of + signs, the file name, and another row of + signs is present (without line numbering)
Errno.h (line 0200), is also included by most of the master headers.
It contains the error numbers that are returned to user programs in the global variable errno when a system call fails.
Errno is also used to identify some internal errors, such as trying to send a message to a nonexistent task.
Internally, it would be inefficient to examine a global variable after a call to a function that might generate an error, but functions must often return other integers, for instance, the number of bytes transferred during an I/O operation.
The MINIX 3 solution is to return error numbers as negative values to mark them as error codes within the system, and then to convert them to positive values before being returned to user programs.
The trick that is used is that each error code is defined in a line like.
The next group of files to be considered are not included in all the master headers, but are nevertheless used in many source files in all parts of the MINIX 3 system.
This header defines many constants, most of which are required by POSIX.
In addition, it includes prototypes for many C functions, including all those used to access MINIX 3 system calls.
Another widely used file is string.h (line 0600), which provides prototypes for many C functions used for string manipulation.
The header signal.h (line 0700) defines the standard signal names.
Several MINIX 3-specific signals for operating system use are defined, as well.
The fact that operating systems functions are handled by independent processes rather than within a monolithic kernel requires some special signal-like communication between the system components.
As we will see when we look at the device driver layer in Chap.
Termios.h (line 1000) defines constants, macros, and function prototypes used for control of terminal-type I/O devices.
It contains flags to signal various modes of operation, variables to set input and output transmission speeds, and an array to hold special characters (e.g., the INTR and KILL characters)
This structure is required by POSIX, as are many of the macros and function prototypes defined in this file.
However, as all-encompassing as the POSIX standard is meant to be, it does not provide everything one might want, and the last part of the file, from line 1140 onward, provides extensions to POSIX.
Some of these are of obvious value, such as extensions to define standard baud rates of 57,600 baud and higher, and support for terminal display screen windows.
The POSIX standard does not forbid extensions, as no reasonable standard can ever be all-inclusive.
In this file and other files that define MINIX 3specific extensions the use of the extensions is controlled by the.
We will mention four more files in the include/ directory that are not listed in Appendix B.
Stdlib.h defines types, macros, and function prototypes that are likely to be needed in the compilation of all but the most simple of C programs.
It is one of the most frequently used headers in compiling user programs, although within the MINIX 3 system source it is referenced by only a few files in the kernel.
Stdio.h is familiar to everyone who has started to learn programming in C by writing the famous ‘‘Hello World!’’ program.
It is hardly used at all in system files, although, like stdlib.h, it is used in almost every user program.
A.out.h defines the format of the files in which executable programs are stored on disk.
An exec structure is defined here, and the information in this structure is used by the process manager to load a new program image when an exec call is made.
One other item worth mention is the section of conditional code that starts with.
Several other files in include/sys/ are widely used in the MINIX 3 system.
The file sys/sigcontext.h (line 1600) defines structures used to preserve and restore normal system operation before and after execution of a signal handling routine and is used both in the kernel and the process manager.
Sys/stat.h (line 1700) defines the structure which we saw in Fig.
It is referenced in several parts of the file system and the process manager.
Other files we will discuss in this section are not as widely referenced as the ones discussed above.
It is only referenced directly once, but this reference includes it in another header that is widely used in the file system.
It is important because, among other things, it tells how many characters a file name may contain (60)
The sys/wait.h (line 1900) header defines macros used by the wait and waitpid system calls, which are implemented in the process manager.
Several other files in include/sys/ should be mentioned, although they are not listed in Appendix B.
Sys/svrctl.h defines data structures and macros used by svrctl, which is not really a system call, but is used like one.
Svrctl is used to coordinate server-level processes as the system starts up.
We have deliberately left discussion of sys/ioctl.h and related files until last, because they cannot be fully understood without also looking at a file in the next directory, minix/ioctl.h.
The ioctl system call is used for device control operations.
The number of devices which can be interfaced with a modern computer system is ever increasing.
Many others, such as network interfaces, SCSI controllers, and sound cards, can be added.
Although this looks like a lot of work, this work is done at compile time and makes for a much more efficient interface to the system call at run time, since the parameter actually passed is the most natural data type for the host machine CPU.
It does, however, bring to mind a famous comment Ken Thompson put into the source code of an early version of UNIX:
This call is not directly invoked by programmers in many cases, since the POSIX138 PROCESSES CHAP.
In fact, the POSIX functions for control of terminal devices are converted into ioctl system calls by the library.
Files in include/minix/ are needed for an implementation of MINIX 3 on any platform, although there are platform-specific alternative definitions within some of them.
The files in include/ibm/ define structures and macros that are specific to MINIX 3 as implemented on IBM-type machines.
On many occasions, when differences in hardware or the way the operating system is intended to be used require changes in the configuration of MINIX 3, editing this file and recompiling the system is all that must be done.
We suggest that if you modify this file you should also modify the comment on line 2303 to help identify the purpose of the modifications.
Other definitions in config.h allow customization for other needs in a particular installation.
For instance, the number of buffers used by the file system for the disk cache should generally be as large as possible, but a large number of buffers.
The next file is const.h (line 2600), which illustrates another common use of header files.
Here we find a variety of constant definitions that are not likely to be changed when compiling a new kernel but that are used in a number of places.
Defining them here helps to prevent errors that could be hard to track down if inconsistent definitions were made in multiple places.
Other files named const.h can be found elsewhere in the MINIX 3 source tree, but they are for more limited use.
Global variables that are declared in header files and included in two or more files are declared EXTERN, as in.
Furthermore, the C reference manual explicitly forbids this construction (Kernighan and Ritchie, 1988)
To avoid this problem, it is necessary to have the declaration read extern int who;
Using EXTERN prevents this problem by having it expand into extern everywhere that const.h is included, except following an explicit redefinition of EXTERN as the null string.
Thus when the header files are included and expanded as part of the compilation of table.c, extern is not inserted anywhere (because EXTERN is defined as the null string within table.c) and storage for the global variables is reserved only in one place, in the object file table.o.
If you are new to C programming and do not quite understand what is going on here, fear not; the details are really not important.
This is a polite way of rephrasing Ken Thompson’s famous comment cited earlier.
Multiple inclusion of header files can cause problems for some linkers because it can lead to multiple declarations for included variables.
The EXTERN business is simply a way to make MINIX 3 more portable so it can be linked on machines whose linkers do not accept multiply defined variables.
Procedures and data that are not referenced outside the file in which they are declared are always declared as PRIVATE to prevent their names from being visible outside the file in which they are declared.
As a general rule, all variables and procedures should be declared with a local scope, if possible.
The rest of const.h defines numerical constants used throughout the system.
For instance, throughout the source code the basic unit of memory allocation is the click.
Different values for the click size may be chosen for different processor architectures.
This file also contains the macros MAX and MIN, so we can say.
Type.h (line 2800) is another file that is included in every compilation by means of the master headers.
It contains a number of key type definitions, along with related numerical values.
When a signal is caught the kernel has to arrange that the next time the signaled process gets to run it will run the signal handler, rather than continuing execution where it was interrupted.
The process manager does most of the work of managing signals; it passes a structure like this to the kernel when a signal is caught.
The process manager uses this information when it sets up its part of the process table.
In the figure four message types, the first two and the last two, seem identical.
Just in terms of size of the data elements they are identical, but many of the data types are different.
Defining seven distinct formats makes it easier to recompile MINIX 3 for a different architecture.
While discussing message formats, this is a good place to note that an operating system and its compiler often have an ‘‘understanding’’ about things like the layout of structures, and this can make the implementer’s life easier.
In MINIX 3, the int fields in messages are sometimes used to hold unsigned data types.
In some cases this could cause overflow, but the code was written using the knowledge that the MINIX 3 compiler copies unsigned types to ints and vice versa without changing the data or generating code to detect overflow.
A more compulsive approach would be to replace each int field with a union of an int and an unsigned.
The same applies to the long fields in the messages; some of them may be used to pass unsigned long data.
Are we cheating here? Perhaps a little bit, one might say, but if you wish to port MINIX 3 to a new platform, quite clearly the exact format of the messages is something to which you must pay a great deal of attention, and now you have been alerted that the behavior of the compiler is another factor that needs attention.
The sizes of message elements will vary, depending upon the architecture of the machine; this diagram illustrates sizes on CPUs with 32-bit pointers, such as those of Pentium family members.
Syslib.h contains prototypes for C library functions called from within the operating system to access other operating system services.
We do not describe details of C libraries in this text, but many library functions are standard and will be available for any C compiler.
Fortunately this is not difficult, since most of these functions simply extract the parameters of the function call and insert them into a message structure, then send the message and extract the results from the reply message.
Many of these library functions are defined in a dozen or fewer lines of C code.
A few functions which could have been defined in syslib.h are in a separate file, sysutil.h (line 3400), because their object code is compiled into a separate library.
If you have experience programming in C you will recognize that printf is a standard library function, referenced in almost all programs.
This is not the printf function you think it is, however.
The version of printf in the standard library cannot be used within system components.
Among other things, the standard printf is intended to write to standard output, and must be able to format floating point numbers.
Using standard output would require going through the file system, but for printing messages when there is a problem and a system component needs to display an error message, it is desirable to be able to do this without the assistance of any other system components.
Also, support for the full range of format specifications usable with the standard printf would bloat the code for no useful purpose.
So a simplified version of printf that does only what is needed by operating system components is compiled into the system utilities library.
When the file system, the process manager, or another part of the operating system is linked to library functions this version is found before the standard library is searched.
This is called by the system version of printf to do the work of displaying characters on the console.
There is a copy in the system utilities library, which will be the one used by default.
But several parts of the system define their own versions.
The log driver (which is not described in detail here) also defines its own version.
There is even a definition of kputc in the kernel itself, but this is a special case.
A special printing function, kprintf, is defined as part of the kernel and is used when the kernel needs to print.
One interpretation of the file name is that is stands for common, another is that it stands for communication.
This file provides common definitions used for communication between servers and device drivers.
To distinguish them from process numbers, task numbers are negative.
Note these are slot numbers in the process table; they should not be confused with process id (PID) numbers.
A few other files in include/minix/ are listed in Appendix B.
Devio.h (line 4100) defines types and constants that support user-space access to I/O ports, as well as some macros that make it easier to write code that specifies ports and values.
Dmap.h (line 4200) defines a struct and an array of that struct, both named dmap.
This table is used to relate major device numbers to the functions that support them.
Major and minor device numbers for the memory device driver and major device numbers for other important device drivers are also defined.
Include/minix/ contains several additional specialized headers that are not listed in Appendix B, but which must be present to compile the system.
The next file in this directory is interrupt.h (line 4400), which defines port address and memory locations used by the interrupt controller chip and the BIOS of PC-compatible systems.
Finally, more I/O ports are defined in ports.h (line 4500)
This file provides addresses needed to access the keyboard interface and the timer chip used by the clock chip.
Several additional files in include/ibm/ with IBM-specific data are not listed in Appendix B, but are essential and should be mentioned.
Bios.h, memory.h, and partition.h are copiously commented and are worth reading if you would like to know more about memory use or disk partition tables.
Finally, diskparm.h defines a data structure needed for formatting a floppy disk.
Kernel.h makes it possible to guarantee that all source files share a large number of important definitions by writing the single line.
Since the order of inclusion of header files is sometimes important, kernel.h also ensures that this ordering is done correctly, once and forever.
This carries to a higher level the ‘‘get it right once, then forget the details’’ technique embodied in the header file concept.
Similar master headers are provided in source directories for other system components, such as the file system and the process manager.
Now let us proceed to look at the local header files included in kernel.h.
Just as we have files const.h and type.h in the common header directory include/minix/, we also have files const.h.
The files in include/minix/ are placed there because they are needed by many parts of the system, including programs that run under the control of the system.
The files in src/kernel/ provide definitions needed only for compilation of the kernel.
The FS, PM, and other system source directories also contain const.h and type.h files to define constants and types needed only for those parts of the system.
Two of the other files included in the master header, proto.h glo.h, have no counterparts in the main include/ directories, but we will find that they, too, have counterparts used in compiling the file system and the process manager.
The last local header included in kernel.h is another ipc.h.
Some of the kernel information structures here are used at startup.
Note that these are physical addresses, that is, addresses relative to the entire address space of the processor.
Kinfo (line 5322) is also an important piece of information.
Just as the boot monitor uses aout to pass information about all processes in the boot image to the kernel, the kernel fills in the fields of kinfo with information about itself that other components of the system may need to know about.
The last few variables defined in glo.h, are declared here because they must be known throughout the kernel code, but they are declared as extern rather than as EXTERN because they are initialized variables, a feature of the C language.
The use of the EXTERN macro is not compatible with C-style initialization, since a variable can only be initialized once.
Several more kernel header files are widely used, although not so much that they are included in kernel.h.
The first of these is proc.h (line 5500), which defines the kernel’s process table.
The complete state of a process is defined by the process’ data in memory, plus the information in its process table slot.
The contents of the CPU registers are stored here when a process is not executing and then are restored when execution resumes.
This is what makes possible the illusion that multiple processes are executing simultaneously and interacting, although at any instant a single CPU can be executing instructions of only one process.
The time spent by the kernel saving and restoring the process state during each context switch is necessary, but obviously this is time during which the work of the processes themselves is suspended.
As noted in the comment at the beginning of proc.h, many routines written in assembly language also access these structures, and another header, sconst.h, defines offsets to fields in the process table for use by the assembly code.
Thus changing a definition in proc.h may necessitate a change in sconst.h.
Before going further we should mention that, because of MINIX 3’s microkernel structure, the process table we will discuss is here is paralleled by tables in PM and FS which contain per-process entries relevant to the function of these.
Together, all three of these tables are equivalent to the process table of an operating system with a monolithic structure, but for the moment when we speak of the process table we will be talking about only the kernel’s process table.
Header files are supposed to allow one to provide a single correct set of definitions and then proceed to use them in many places without devoting a lot of further attention to the details.
Obviously, duplicate definitions, like those in proc.h and sconst.h, violate that principle.
This is a special case, of course, but as such, special attention is required if changes are made to either of these files to ensure the two files remain consistent.
The priv structure is not part of the process table, rather each process table slot has a pointer to an instance of it.
Only system processes have private copies; user processes all point to the same copy.
Thus, for a user process the remaining fields of the structure are not relevant, as sharing them does not make sense.
These fields are bitmaps of pending notifications, hardware interrupts, and signals, and a timer.
It makes sense to provide these here for system processes, however.
User processes have notifications, signals, and timers managed on their behalf by the process manager.
The clock task, file server, tty, and init processes privileges are typical of tasks, servers, device drivers, and user processes, respectively.
Another header that is included in a number of different source files is protect.h (line 5800)
A detailed description of these chips is beyond the scope of this book.
Suffice it to say that they contain internal registers that point to descriptor tables in memory.
Descriptor tables define how system resources are used and prevent processes from accessing memory assigned to other processes.
The concept of privilege levels will be familiar to those who are familiar with the architecture of modern CPUs, but those who have learned computer architecture through study of the assembly language of low-end microprocessors may not have encountered such features.
The rest of table.c defines many constants related to properties of processes, such as the combinations of flag bits, call traps, and masks that define to whom messages and notifications can be sent that we saw in Fig.
Following this are masks to define the kernel calls allowed for various processes.
The process manager and file server are all allowed unique combinations.
The reincarnation server is allowed access to all kernel calls, not for its own use, but because as the parent of other system processes it can only pass to its.
Drivers are given a common set of kernel call masks, except for the RAM disk driver which needs unusual access to memory.
It has been put here rather than in a header file because the trick with EXTERN used to prevent multiple declarations does not work with initialized variables; that is, you may not say.
The image table provides details needed to initialize all of the processes that are loaded from the boot image.
This shows the size of the quantum assigned to each process.
Ordinary user processes, as children of init, get to run for 8 clock ticks.
The CLOCK and SYSTEM tasks are allowed to run for 64 clock ticks if necessary.
They are not really expected to run that long before blocking, but unlike user-space servers and drivers they cannot be demoted to a lower-priority queue if they prevent other processes from getting a chance to run.
If a new process is to be added to the boot image, a new row must be provided in the image table.
An error in matching the size of image to other constants is intolerable and cannot be permitted.
At the end of table.c tests are made for errors, using a little trick.
In each declaration the size of dummy will be impossible and will trigger a compiler error if a mistake has been made.
Since dummy is declared as extern, no space is allocated for it here (or anywhere)
Since it is not referenced anywhere else in the code, this will not bother the compiler.
Additional global storage is allocated at the end of the assembly language file mpx386.s.
Although it will require skipping ahead several pages in the listing to see this, it is appropriate to discuss this now, since we are on the subject of global variables.
A .sect bss assembler directive and the .space pseudoinstruction are also used here to reserve space for the kernel’s stack.
The .comm pseudoinstruction labels several words at the top of the stack so they may be manipulated directly.
It is almost time to start looking at the executable code—but not quite.
Before we do that, let us take a few moments to understand how MINIX 3 is loaded into memory.
It is, of course, loaded from a disk, but the process is not completely.
In particular, it depends on whether the disk is partitioned or not.
Figure 2-36 shows how diskettes and partitioned disks are laid out.
The first sector is the master boot record, also called masterboot.
When the system is started, the hardware (actually, a program in ROM) reads the first sector of the boot disk, copies it to a fixed location in memory, and executes the code found there.
On an unpartitioned MINIX 3 diskette, the first sector is a bootblock which loads the boot program, as in Fig.
Hard disks are partitioned, and the program on the first sector (called masterboot on MINIX systems) first relocates itself to a different memory region, then reads the partition table, loaded with it from the first sector.
Then it loads and executes the first sector of the active partition, as shown in Fig.
The bootblock code is the same for an unpartitioned or a partitioned disk.
Since the masterboot program relocates itself the bootblock code can be written to run at the same memory address where masterboot is originally loaded.
The actual situation can be a little more complicated than the figure shows, because a partition may contain subpartitions.
In this case the first sector of the partition will be another master boot record containing the partition table for the subpartitions.
Eventually, however, control will be passed to a boot sector, the first sector on a device that is not further subdivided.
This makes it possible for partitioned and nonpartitioned diskettes to be mounted in exactly the same way.
The main use for a partitioned floppy disk is that it provides a convenient way to divide an installation disk into a root image to be copied to a RAM disk and a mounted portion that can be dismounted when no longer needed, in order to free the diskette drive for continuing the installation process.
The MINIX 3 boot sector is modified at the time it is written to the disk by a special program called installboot which writes the boot sector and patches into it the disk address of a file named boot on its partition or subpartition.
In MINIX 3, the standard location for the boot program is in a directory of the same name, that is, /boot/boot.
But it could be anywhere—the patching of the boot sector just mentioned locates the disk sectors from which it is to be loaded.
This is necessary because previous to loading boot there is no way to use directory and file names to find a file.
It can do more than just load the operating system however, as it is a monitor program that allows the user to change, set, and save various parameters.
Boot looks in the second sector of its partition to find a set of parameters to use.
These control the boot operation, and are also passed to the operating system itself.
The default settings can also be modified to bypass the menu and start MINIX 3 immediately.
Boot is not a part of the operating system, but it is smart enough to use the file system data structures to find the actual operating system image.
Boot looks for a file with the name specified in the image= boot parameter, which by default is /boot/image.
If there is an ordinary file with this name it is loaded, but if this is the name of a directory the newest file within it is loaded.
Many operating systems have a predefined file name for the boot image.
But MINIX 3 users are encouraged to modify it and to create new versions.
It is useful to be able to select from multiple versions, in order to return to an older version if an experiment is unsuccessful.
We do not have space here to go into more detail about the boot monitor.
It is a sophisticated program, almost a miniature operating system in itself.
If you would like to know more, the MINIX 3 Web site provides a link to a detailed description of the boot monitor source code.
Note that MINIX 3 as described here is configured with just one disk driver in the boot image, but several may be present, with the active one selected by a label.
Like all binary programs, each file in the boot image includes a header that tells how much space to reserve for uninitialized data and stack after loading the executable code and initialized data, so the next program can be loaded at the proper address.
The memory regions available for loading the boot monitor and the component programs of MINIX 3 will depend upon the hardware.
Also, some architectures may require adjustment of internal addresses within executable code to correct them for the actual address where a program is loaded.
The important thing is that by one means or another the operating system is loaded into memory.
Following this, a small amount of preparation is required before MINIX 3 can be started.
Then some additional information needed to start the system is made available to the kernel.
The a.out headers of the components of the MINIX 3 image are extracted into an array within boot’s memory space, and the base address of this array is passed to the kernel.
These items are passed on the stack, as we shall see later.
Several other pieces of information, the boot parameters, must be communicated from the boot monitor to the operating system.
Some are needed by the kernel and some are not needed but are passed along for information, for instance, the name of the boot image that was loaded.
These items can all be represented as string=value pairs, and the address of a table of these pairs is passed on the stack.
In this example, an important item we will see again soon is the memory parameter; in this case it indicates that the boot monitor has determined that there are two segments of memory available for MINIX 3 to use.
This is typical of all but the most elderly PCcompatible computers.
Modern PC-compatible machines always have more memory than the original PC, but for compatibility they still have read-only memory at the same addresses as the older machines.
Boot parameters passed to the kernel at boot time in a typical MINIX 3 system.
The boot monitor loads the kernel into the low memory range and the servers, drivers, and init into the memory range above the ROM if possible.
This is primarily for the benefit of the file system, so a large block cache can be used without bumping into the read-only memory.
We should also mention here that operating systems are not universally loaded from local disks.
Diskless workstations may load their operating systems from a remote disk, over a network connection.
Although details vary from what we have described here, the elements of the process are likely to be similar.
The ROM code must be just smart enough to get an executable file over the net that can then obtain the complete operating system.
If MINIX 3 were loaded this way, very little would need to be changed in the initialization process that occurs once the operating system code is loaded into memory.
It would, of course, need a network server and a modified file system that could access files via the network.
Both of these also include assembly language support for other low-level kernel operations.
This file is so short that the entire file can be presented in Fig.
Mpx.s shows an unusual use of the C preprocessor #include statement.
Customarily the #include preprocessor directive is used to include header files, but it can also be used to select an alternate section of source code.
Not only would this be unwieldy; it would also be wasteful of disk space, since in a particular installation it is likely that one or the other of these two files will not be used at all and can be archived or deleted.
Since this is almost our first look at executable code, let us start with a few words about how we will do this throughout the book.
The multiple source files used in compiling a large C program can be hard to follow.
In general, we will keep discussions confined to a single file at a time.
The order of inclusion of the files in Appendix B is the order in which we discuss them in the text.
We will start with the entry point for each part of the MINIX 3 system, and we will follow the main line of execution.
When a call to a supporting function is encountered, we will say a few words about the purpose of the call, but normally we will not go into a detailed description of the internals of the function at that point, leaving that until we arrive at the definition of the called function.
Important subordinate functions are usually defined in the same file in which they are called, following the higher-level calling functions, but small or general-purpose functions are sometimes collected in separate files.
We do not attempt to discuss the internals of every function, and files that contain such functions may not be listed in Appendix B.
To make code easier to understand and reduce the overall size of the listings, most conditional code for.
Complete versions of all files are in the source directories on the CD-ROM and are also available on the MINIX 3 Web site.
A substantial amount of effort has been made to make the code readable by humans.
But a large program has many branches, and sometimes understanding a main function requires reading the functions it calls, so having a few slips of paper to use as bookmarks and deviating from our order of discussion to look at things in a different order may be helpful at times.
Having laid out our intended way of organizing the discussion of the code, we start by an exception.
We will describe these routines in the order that they are executed, even though that involves jumping from one file to another.
The first instruction is a jump over a few bytes of data; this includes the boot monitor flags (line 6423) mentioned earlier.
At this point the flags have already served their purpose; they were read by the monitor when it loaded the kernel into memory.
They are located here because it is an easily specified address.
Understanding the state of the stack at this point will help make sense of the following code.
The monitor passes several parameters to MINIX 3, by putting them on the stack.
First the monitor pushes the address of the variable aout, which holds the address of an array of the header information of the component programs of the boot image.
Next it pushes the size and then the address of the boot parameters.
Next come the monitor’s code segment address and the location to return to within the monitor when MINIX 3 terminates.
The assembly language code must do a substantial amount of work, setting up a stack frame to provide the proper environment for code compiled by the C.
Cstart calls another routine to initialize the Global Descriptor Table, the central data structure used by Intel 32-bit processors to oversee memory protection, and the Interrupt Descriptor Table, used to select the code to be executed for each possible interrupt type.
In the C language a[i] is just another way of writing *(a+i)
So it does not make much difference if you add a constant to a or to i.
Some C compilers generate slightly better code if you add a constant to the array instead of the index.
Whether it really makes a difference here, we cannot say.
There is one exception: the KERNEL process (also identified as HARDWARE in some places) is never considered ready, never runs as an ordinary process, and thus has no need of a stack pointer.
At line 7202, one of these headers is copied to a local exec structure, ehdr, using hdrindex as the index into the array of headers.
These processes are all compiled into the same file as the kernel, and the information about their stack requirements is in the image table.
Since a task compiled into the kernel can call code and access data located anywhere in the kernel’s space, the size of an individual task is not meaningful.
Thus the same element of the array at aout is accessed for the kernel and for each task, and the size fields for a task is filled with the sizes for the kernel itself.
The tasks get their stack information from the image table, initialized during compilation of table.c.
After all kernel processes have been processed, hdrindex is incremented on each pass through the loop (line 7196), so all the user-space system processes get the proper data from their own headers.
The processor status word for the tasks is different from that for device drivers and servers, because tasks have a higher privilege level that allows them to access I/O ports.
Following this, if the process is a user-space one, its stack pointer is initialized.
One entry in the process table does not need to be (and cannot be) scheduled.
The HARDWARE process exists only for bookkeeping purposes—it is credited.
Details of interrupt hardware are system dependent, but any system must have elements functionally equivalent to those to be described for systems with 32-bit Intel CPUs.
Interrupts generated by hardware devices are electrical signals and are handled in the first place by an interrupt controller, an integrated circuit that can sense a number of such signals and for each one generate a unique data pattern on the processor’s data bus.
This is necessary because the processor itself has only one input for sensing all these devices, and thus cannot differentiate which device needs service.
PCs using Intel 32-bit processors are normally equipped with two such controller chips.
Each can handle eight inputs, but one is a slave which feeds its output to one of the inputs of the master, so fifteen distinct external devices can be sensed by the combination, as shown in Fig.
Some of the fifteen inputs are dedicated; the clock input, IRQ 0, for instance, does not have a connection to any socket into which a new adapter can be plugged.
Others are connected to sockets and can be used for whatever device is plugged in.
In the figure, interrupt signals arrive on the various IRQ n lines shown at the right.
The connection to the CPU’s INT pin tells the processor that an interrupt has occurred.
The INTA (interrupt acknowledge) signal from the CPU causes the controller responsible for the interrupt to put data on the system data bus telling the processor which service routine to execute.
Several modes of response to interrupts are possible; in the one used by MINIX 3, the fields of most concern to us in each of the interrupt gate descriptors point to the service routine’s executable code segment and the starting address within it.
The CPU executes the code pointed to by the selected descriptor.
The result is exactly the same as execution of an.
The only difference is that in the case of a hardware interrupt the <nnn> originates from a register in the interrupt controller chip, rather than from an instruction in program memory.
The task-switching mechanism of a 32-bit Intel processor that is called into play in response to an interrupt is complex, and changing the program counter to execute another function is only a part of it.
When the CPU receives an interrupt while running a process it sets up a new stack for use during the interrupt service.
Termination of an interrupt service routine is done by switching the stack from the kernel stack back to a stackframe in the process table (but not necessarily the same one that was created by the last interrupt), explicitly popping the additional registers, and executing an iretd (return from interrupt) instruction.
Iretd restores the state that existed before an interrupt, restoring the registers that were pushed by the hardware and switching back to a stack that was in use before an interrupt.
Thus an interrupt stops a process, and completion of the interrupt service restarts a process, possibly a different one from the one that was most recently stopped.
Unlike the simpler interrupt mechanisms that are the usual subject of assembly language programming texts, nothing is stored on the interrupted process’ working stack when a user process is interrupted.
Furthermore, because the stack is created anew in a known location (determined by the TSS) after an interrupt, control of multiple processes is simplified.
To start a different process all that is necessary is to point the stack pointer to the stackframe of another process, pop the registers that were explicitly pushed, and execute an iretd instruction.
The CPU disables all interrupts when it receives an interrupt.
This guarantees that nothing can occur to cause the stackframe within a process table entry to overflow.
This is automatic, but assembly-level instructions exist to disable and enable interrupts, as well.
Interrupts remain disabled while the kernel stack, located outside the process table, is in use.
A mechanism exists to allow an exception handler (a response to an error detected by the CPU) to run when the kernel stack is in use.
An exception is similar to an interrupt and exceptions cannot be disabled.
Thus, for the sake of exceptions there must be a way to deal with what are essentially nested interrupts.
Instead, the CPU pushes the essential registers needed for resumption of the interrupted code onto the existing stack.
An exception is not supposed to occur while the kernel is running, however, and will result in a panic.
When an iretd is encountered while executing kernel code, a the return mechanism is simpler than the one used when a user process is interrupted.
The processor can determine how to handle the iretd by examining the code segment selector that is popped from the stack as part of the iretd’s action.
The privilege levels mentioned earlier control the different responses to interrupts received while a process is running and while kernel code (including interrupt service routines) is executing.
The simpler mechanism is used when the privilege level of the interrupted code is the same as the privilege level of the code to be executed in response to the interrupt.
The usual case, however, is that the interrupted code is less privileged than the interrupt service code, and in this case the more elaborate mechanism, using the TSS and a new stack, is employed.
The privilege level of a code segment is recorded in the code segment selector, and as this is one of the items stacked during an interrupt, it can be examined upon return from the interrupt to determine what the iretd instruction must do.
Another service is provided by the hardware when a new stack is created to use while servicing an interrupt.
The hardware checks to make sure the new stack is big enough for at least the minimum quantity of information that must be placed on it.
This protects the more privileged kernel code from being accidentally (or maliciously) crashed by a user process making a system call with an inadequate stack.
These mechanisms are built into the processor specifically for use in the implementation of operating systems that support multiple processes.
This behavior may be confusing if you are unfamiliar with the internal working of 32-bit Intel CPUs.
Ordinarily we try to avoid describing such details, but understanding what happens when an interrupt occurs and when an iretd instruction is executed is essential to understanding how the kernel controls the transitions to and from the ‘‘running’’ state of Fig.
The fact that the hardware handles much of the work makes life much easier for the programmer, and presumably makes the resulting system more efficient.
All this help from the hardware does, however, make it hard to understand what is happening just by reading the software.
A few words about the assembly language code used may be helpful to readers unfamiliar with assembly language programming.
The 0f is not a hexadecimal number, nor is it a normal label.
Ordinary label names are not permitted to begin with numeric characters.
The byte written on line 6526 allows the interrupt controller to resume normal operation, possibly with the line for the current interrupt disabled.
Now let us move on to look at save (line 6622), which we have already mentioned.
Its name describes one of its functions, which is to save the context of the interrupted process on the stack provided by the CPU, which is a stackframe.
In some ways documentation is harder than programming—the compiler or the program will eventually reveal errors in a program.
There is no such mechanism to correct comments in source code.
There is a rather long comment at the start of mpx386.s which is, unfortunately, incorrect.
Restart is the common point reached after system startup, interrupts, or system calls.
The most deserving process (which may be and often is a different process from the last one interrupted) runs next.
Not shown in this diagram are interrupts that occur while the kernel itself is running.
The following instruction sets the address in the next process’ process table entry to that where the stack for the next interrupt will be set up, and the following instruction stores this address into the TSS.
Finally, some data storage space is reserved at the end of the assembly language file.
The compiler puts a magic number here so boot can verify that the file it loads is a valid kernel image.
When compiling the complete system various string constants will be stored following this.
Servers and ordinary processes have stack space reserved when an executable file is linked and depend upon the kernel to properly set the stack segment descriptor and the stack pointer when they are executed.
Processes in MINIX 3 communicate by messages, using the rendezvous principle.
When a process does a send, the lowest layer of the kernel checks to see if the destination is waiting for a message from the sender (or from ANY sender)
If so, the message is copied from the sender’s buffer to the receiver’s buffer, and both processes are marked as runnable.
If the destination is not waiting for a message from the sender, the sender is marked as blocked and put onto a queue of processes waiting to send to the receiver.
When a process does a receive, the kernel checks to see if any process is queued trying to send to it.
If so, the message is copied from the blocked sender to the receiver, and both are marked as runnable.
If no process is queued trying to send to it, the receiver blocks until a message arrives.
In MINIX 3, with components of the operating system running as totally separate processes, sometimes the rendezvous method is not quite good enough.
The sender is not blocked if the destination is not waiting for a message.
The next time the destination does a receive pending notifications are delivered before ordinary messages.
Notifications can be used in situations where using ordinary messages could cause deadlocks.
Earlier we pointed out that a situation where process A blocks sending a message to process B and process B blocks sending a message to process A must be avoided.
But if one of the messages is a nonblocking notification there is no problem.
In most cases a notification informs the recipient of its origin, and little more.
Sometimes that is all that is needed, but there are two special cases where a notification conveys some additional information.
In any case, the destination process can send a message to the source of the notification to request more information.
The high-level code for interprocess communication is found in proc.c.
The kernel’s job is to translate either a hardware interrupt or a software interrupt into a message.
The former are generated by hardware and the latter are the way a request for system services, that is, a system call, is communicated to the kernel.
These cases are similar enough that they could have been handled by a single function, but it was more efficient to create specialized functions.
If the origin of a notification is HARDWARE, it carries a payload, a copy of the destination process’ bitmap of pending interrupts.
If the origin is SYSTEM, the payload is the bitmap of pending signals.
Because these bitmaps are available in the priv table slot of the destination process, they can be accessed at any time.
Notifications can be delivered later if the destination process is not blocked waiting for them at the time they are sent.
For ordinary messages this would require some kind of buffer in which an undelivered message could be stored.
To store a notification all that is required is a bitmap in which each bit corresponds to a process that can send a notification.
When a notification cannot be sent the bit corresponding to the sender is set in the recipient’s bitmap.
When a receive is done the bitmap is checked and if a bit is found to have been set the message is regenerated.
The bit tells the origin of the message, and if the origin is HARDWARE or.
The only other item needed is the timestamp, which is added when the message is regenerated.
For the purposes for which they are used, timestamps do not need to show when a notification was first attempted, the time of delivery is sufficient.
One might have thought that, because a timestamp is part of a notify message, it would convey useful information, for instance, if the recipient had been unable to do a receive for a while the timestamp would tell how long it had been undelivered.
But the notification message is generated (and timestamped) at the time it is delivered, not at the time it was sent.
There is a purpose behind constructing the notification messages at the time of delivery, however.
The code is unnecessary to save notification messages that cannot be delivered immediately.
All that is necessary is to set a bit to remember that a notification should be generated when delivery becomes possible.
You cannot get more economical storage than that: one bit per pending notification.
Processes are given initial priorities that are related to the structure shown in Fig.
The clock and system tasks in layer 1 of Fig.
The device drivers of layer 2 get lower priority, but they are not all equal.
Server processes in layer 3 get lower priorities than drivers, but some less than others.
User processes start with less priority than any of the system processes, and initially are all equal, but the nice command can raise or lower the priority of a user process.
Shown here is the initial queuing of processes as MINIX 3 starts up.
Given the queue structures just described, the scheduling algorithm is simple: find the highest priority queue that is not empty and pick the process at the head of that queue.
The IDLE process is always ready, and is in the lowest priority queue.
If all the higher priority queues are empty, IDLE is run.
One other point of interest is found in this function.
Because tasks that run in the kernel share a common hardware-defined stack area, it is a good idea to check the integrity of their stack areas occasionally.
At the beginning of dequeue a test is made to see if the process being removed from the queue is one that operates in kernel space.
Kernel processes (the clock and system tasks) are immune, but all other processes may have their priority reduced, that is, be moved to a higher-numbered queue, by adding a positive penalty.
All processes start with their maximum priority, so a negative penalty does not change anything until positive penalties have been assigned.
There is also a lower bound on priority, ordinary processes never can be put on the same queue as IDLE.
The first process on the highest priority queue is always run next.
The clock task monitors the time used by all processes.
If a user process uses up its quantum, it is put at the end of its queue, thus achieving a simple round-robin scheduling among the competing user processes.
Tasks, drivers, and servers are expected to run until they block, and are given large quanta, but if they run too long they may also be preempted.
This is not expected to happen very often, but it is a mechanism to prevent a high-priority process with a problem from locking up the system.
A process that prevents other processes from running may also be moved to a lower priority queue temporarily.
If you fully understand the MINIX 3 design goal of putting device drivers in user-space, the preceding discussion of how interrupt handlers are called will have left you slightly confused.
The interrupt handler addresses stored in the hook structures cannot be useful unless they point to functions within the kernel’s address space.
The only interrupt-driven device in the kernel’s address space is the clock.
What about device drivers that have their own address spaces?
Protect.c contains routines related to protected mode operation of Intel processors.
The GDT and IDT are pointed to by special registers within the CPU, and GDT entries point to LDTs.
The GDT is available to all processes and holds segment descriptors for memory regions used by the operating system.
Normally, there is one LDT for each process, holding segment descriptors for the memory regions used by the process.
Descriptors are 8-byte structures with a number of components, but the most important parts of a segment descriptor are the fields that describe the base address and the limit of a memory region.
The IDT is also composed of 8-byte descriptors, with the most important part being the address of the code to be executed when the corresponding interrupt is activated.
These events are processed by means of the interrupt gate.
Fortunately, we can usually leave these details to Intel’s processor designers.
For the most part, the C language allows us to avoid the details.
However, in implementing a real operating system the details must be faced at some point.
Figure 2-44 shows the internal structure of one kind of segment descriptor.
The limit is interpreted as either a number of bytes or a number of 4096-byte pages, based on the value of the G (granularity) bit.
Other descriptors, such as those used to specify how interrupts are handled, have different, but equally complex structures.
The BIOS is used to provide alternative disk drivers which are not described here.
The boot monitor also returns the number of clock ticks counted during the BIOS call.
How this is used will be seen in the discussion of the clock task.
Both addresses are absolute, that is, address 0 really means the first byte in the entire address space, and all three parameters are unsigned longs.
A consequence of making major system components independent processes outside the kernel is that they are forbidden from doing actual I/O, manipulating kernel tables and doing other things operating system functions normally do.
For example, the fork system call is handled by the process manager.
The solution to this problem is to have a kernel offer a set of services to the drivers and servers.
These services, which are not available to ordinary user processes, allow the drivers and servers to do actual I/O, access kernel tables, and do other things they need to, all without being inside the kernel.
These special services are handled by the system task, which is shown in layer 1 in Fig.
Although it is compiled into the kernel binary program, it is really a separate process and is scheduled as such.
The job of the system task is to accept all the requests for special kernel services from the drivers and servers and carry them out.
Since the system task is part of the kernel’s address space, it makes sense to study it here.
In a conventional operating system with a monolithic kernel, the term system call is used to refer to all calls for services provided by the kernel.
In a modern UNIX-like operating system the POSIX standard describes the system calls available to processes.
There may be some nonstandard extensions to POSIX, of course, and a programmer taking advantage of a system call will generally reference a function defined in the C libraries, which may provide an easy-to-use programming interface.
Also, sometimes separate library functions that appear to the programmer to be distinct ‘‘system calls’’ actually use the same access to the kernel.
In MINIX 3 the landscape is different; components of the operating system run in user space, although they have special privileges as system processes.
We will still use the name ‘‘system call’’ for any of the POSIX-defined system calls (and a few MINIX extensions) listed in Fig.
In MINIX 3 system calls by user processes are transformed into messages to server processes.
Server processes communicate with each other, with device drivers, and with the kernel by messages.
The subject of this section, the system task, receives all requests for kernel services.
Loosely speaking, we could call these requests system calls, but to be more exact we will refer to them as kernel calls.
In many cases a system call that originates with a user process results in a kernel call with a similar name being made by a server.
This is always because some part of the service being requested can only be dealt with by the kernel.
Technically speaking, a third category of calls (besides system calls and kernel calls) should be distinguished.
The message primitives used for interprocess communication such as send, receive, and notify can be thought of as systemcall-like.
We have probably called them that in various places in this book—after all, they do call the system.
But they should properly be called something different from both system calls and kernel calls.
You can think of a message primitive as being like the carrier wave in a radio communications system.
Modulation is usually needed to make a radio wave useful; the message type and other components of a message structure allow the message call to convey information.
In a few cases an unmodulated radio wave is useful; for instance, a radio beacon to guide airplanes to an airport.
This is analogous to the notify message primitive, which conveys little information other than its origin.
The system task accepts 28 kinds of messages, shown in Fig.
Each of these can be considered a kernel call, although, as we shall see, in some cases there are multiple macros defined with different names that all result in just one of the message types shown in the figure.
And in some other cases more than one of the message types in the figure are handled by a single procedure that does the work.
The main program of the system task is structured like other tasks.
It gets a message, dispatches to the appropriate service procedure, and then sends a reply.
A few general support functions are found in the main file, system.c, but the main loop dispatches to a procedure in a separate file in the kernel/system/ directory to process each kernel call.
We will see how this works and the reason for this organization when we discuss the implementation of the system task.
Any’’ means any system process; user processes cannot call the system task directly.
The system task is compiled from a header, system.h, and a C source file, system.c, in the main kernel/ directory.
In addition there is a specialized library built from source files in a subdirectory, kernel/system/
Although MINIX 3 as we describe it here is a general-purpose operating system, it is also potentially useful for special purposes, such as embedded support in a portable device.
In such cases a stripped-down version of the operating system might be adequate.
For instance, a device without a disk might not need a file system.
We saw in kernel/config.h that compilation of kernel calls can be selectively enabled and disabled.
Having the code that supports each kernel call linked from the library as the last stage of compilation makes it easier to build a customized system.
Putting support for each kernel call in a separate file simplifies maintenance of the software.
But there is some redundancy between these files, and listing all of them would add 40 pages to the length of this book.
Thus we will list in Appendix B and describe in the text only a few of the files in the kernel/system/ directory.
However, all the files are on the CD-ROM and the MINIX 3 Web site.
In any case, as we noted earlier, the system task is used to enable interrupts on behalf of user-space drivers that need to respond to interrupts, so it makes sense to have it prepare the table.
The system task is used to set up timers when synchronous alarms are requested by other system processes, so initializing the timer lists is also appropriate here.
When control returns to the main kernel directory another line in the Makefile cause this local library to be searched first when the kernel object files are linked.
At this level this is done putting the desired expiration time into the correct field of the timer structure and calling another function to do the work.
Of course, resetting the timer does not require storing a value.
We will see the functions reset and set soon, their code is in the source file.
But since the system task and the clock task are both compiled into the kernel image all functions declared PUBLIC are accessible.
As an aside, note that when we talked about the initialization of timers a few pages back (and in this section as well) we referred to synchronous alarms requested by system processes.
If that did not make complete sense at this point, and if you are wondering what is a synchronous alarm or what about timers for nonsystem processes, these questions will be dealt with in the next section, when we discuss the clock task.
There are so many interconnected parts in an operating system that it is almost impossible to order all topics in a way that does not occasionally require a reference to a part that has not been already been explained.
If we were not dealing with a real operating system we could probably avoid bringing up messy details like this.
For that matter, a totally theoretical discussion of operating system principles would probably never mention a system task.
In a theory book we could just wave our arms and ignore the problems of giving operating system components in user space limited and controlled access to privileged resources like interrupts and I/O ports.
To finish our discussion of the system task, we will look at its role in handling a typical operating service, providing data in response to a read system call.
When a user does a read call, the file system checks its cache to see if it has the.
If not, it sends a message to the appropriate disk driver to load it into the cache.
Then the file system sends a message to the system task telling it to copy the block to the user process.
In the worst case, eleven messages are needed to read a block; in the best case, four messages are needed.
These messages are a source of overhead in MINIX 3 and are the price paid for the highly modular design.
Clocks (also called timers) are essential to the operation of any timesharing system for a variety of reasons.
For example, they maintain the time of day and prevent one process from monopolizing the CPU.
The MINIX 3 clock task has some resemblance to a device driver, in that it is driven by interrupts generated by a hardware device.
However, the clock is neither a block device, like a disk, nor a character device, like a terminal.
In fact, in MINIX 3 an interface to the clock is not provided by a file in the /dev/ directory.
Furthermore, the clock task executes in kernel space and cannot be accessed directly by user-space processes.
It has access to all kernel functions and data, but user-space processes can only access it via the system task.
Two types of clocks are used in computers, and both are quite different from the clocks and watches used by people.
The other kind of clock is built out of three components: a crystal oscillator, a counter, and a holding register, as shown in Fig.
At least one such circuit is usually found in any computer, providing a synchronizing signal to the computer’s various circuits.
This signal is fed into the counter to make it count down to zero.
When the counter gets to zero, it causes a CPU interrupt.
Computers whose advertised clock rate is higher than 200 MHz normally use a slower clock and a clock multiplier circuit.
In one-shot mode, when the clock is started, it copies the value of the holding register into the counter and then decrements the counter at each pulse from the crystal.
When the counter gets to zero, it causes an interrupt and stops until it is explicitly started again by the software.
In square-wave mode, after getting to zero and causing the.
The advantage of the programmable clock is that its interrupt frequency can be controlled by software.
If a 1-MHz crystal is used, then the counter is pulsed every microsecond.
Programmable clock chips usually contain two or three independently programmable clocks and have many other options as well (e.g., counting up instead of down, interrupts disabled, and more)
To prevent the current time from being lost when the computer’s power is turned off, most computers have a battery-powered backup clock, implemented with the kind of low-power circuitry used in digital watches.
If the backup clock is not present, the software may ask the user for the current date and time.
There is also a standard protocol for a networked system to get the current time from a remote host.
In any case the time is then translated into the number of seconds since 12 A.M.
Clock ticks are counted by the running system, and every time a full second has passed the real time is incremented by one count.
Usually, utility programs are provided to manually set the system clock and the backup clock and to synchronize the two clocks.
We should mention here that all but the earliest IBM-compatible computers have a separate clock circuit that provides timing signals for the CPU, internal data busses, and other components.
This is the clock that is meant when people speak of CPU clock speeds, measured in Megahertz on the earliest personal computers, and in Gigahertz on modern systems.
The basic circuitry of quartz crystals, oscillators and counters is the same, but the requirements are so different that modern computers have independent clocks for CPU control and timekeeping.
All the clock hardware does is generate interrupts at known intervals.
Everything else involving time must be done by the software, the clock driver.
The exact duties of the clock driver vary among operating systems, but usually include most of the following:
Preventing processes from running longer than they are allowed to.
The first clock function, maintaining the time of day (also called the real time) is not difficult.
It just requires incrementing a counter at each clock tick, as mentioned before.
The only thing to watch out for is the number of bits in the time-of-day counter.
Clearly the system cannot store the real time as the number of ticks since Jan.
The first way is to use a 64-bit counter, although doing so makes maintaining the counter more expensive since it has to be done many times a second.
The second way is to maintain the time of day in seconds, rather than in ticks, using a subsidiary counter to count ticks until a whole second has been accumulated.
The third approach is to count ticks, but to do that relative to the time the system was booted, rather than relative to a fixed external moment.
When the backup clock is read or the user types in the real time, the system boot time is calculated from the current time-of-day value and stored in memory in any convenient form.
When the time of day is requested, the stored time of day is added to the counter to get the current time of day.
The second clock function is preventing processes from running too long.
Whenever a process is started, the scheduler should initialize a counter to the value of that process’ quantum in clock ticks.
When it gets to zero, the clock driver calls the scheduler to set up another process.
The most accurate way to do it is to start a second timer, distinct from the main system timer, whenever a process is started.
When that process is stopped, the timer can be read out to tell how long the process has run.
To do things right, the second timer should be saved when an interrupt occurs and restored afterward.
A less accurate, but much simpler, way to do accounting is to maintain a pointer to the process table entry for the currently running process in a global variable.
At every clock tick, a field in the current process’ entry is incremented.
In this way, every clock tick is ‘‘charged’’ to the process running at the time of the tick.
A minor problem with this strategy is that if many interrupts occur during a process’ run, it is still charged for a full tick, even though it did not get much work done.
Properly accounting for the CPU during interrupts is too expensive and is rarely done.
In MINIX 3 and many other systems, a process can request that the operating system give it a warning after a certain interval.
The warning is usually a signal, interrupt, message, or something similar.
One application requiring such warnings is networking, in which a packet not acknowledged within a certain time interval must be retransmitted.
Another application is computer-aided instruction, where a student not providing a response within a certain time is told the answer.
If the clock driver had enough clocks, it could set a separate clock for each request.
This not being the case, it must simulate multiple virtual clocks with a single physical clock.
One way is to maintain a table in which the signal time for all pending timers is kept, as well as a variable giving the time of the next one.
Whenever the time of day is updated, the driver checks to see if the closest signal has occurred.
If it has, the table is searched for the next one to occur.
If many signals are expected, it is more efficient to simulate multiple clocks by chaining all the pending clock requests together, sorted on time, in a linked list, as shown in Fig.
Each entry on the list tells how many clock ticks following the previous one to wait before causing a signal.
Note that during a clock interrupt, the clock driver has several things to do.
These things include incrementing the real time, decrementing the quantum and.
However, each of these operations has been carefully arranged to be very fast because they have to be repeated many times a second.
Parts of the operating system also need to set timers.
When we study the hard disk driver, we will see that a wakeup call is scheduled each time the disk controller is sent a command, so an attempt at recovery can be made if the command fails completely.
Floppy disk drivers use timers to wait for the disk motor to get up to speed and to shut down the motor if no activity occurs for a while.
The mechanism used by the clock driver to handle watchdog timers is the same as for user signals.
The only difference is that when a timer goes off, instead of causing a signal, the clock driver calls a procedure supplied by the caller.
This presented a problem in the design of MINIX 3, since one of the goals was to remove drivers from the kernel’s address space.
The short answer is that the system task, which is in kernel space, can set alarms on behalf of some user-space processes, and then notify them when a timer goes off.
Some operating systems provide a mechanism by which a user program can have the system build up a histogram of its program counter, so it can see where it is spending its time.
When profiling is a possibility, at every tick the driver checks to see if the current process is being profiled, and if so, computes the bin number (a range of addresses) corresponding to the current program counter.
This mechanism can also be used to profile the system itself.
The MINIX 3 clock driver is contained in the file kernel/clock.c.
First, like the device drivers that we will see in the next chapter, there is a task mechanism which runs in a loop, waiting for messages and dispatching to subroutines that perform the action requested.
However, this structure is almost vestigial in the clock task.
The message mechanism is expensive, requiring all the overhead of a context switch.
So for the clock this is used only when there is a substantial amount of work to be done.
Only one kind of message is received, there is only one subroutine to service the message, and a reply message is not sent when the job is done.
The second major part of the clock software is the interrupt handler that is activated 60 times each second.
It does basic timekeeping, updating a variable that counts clock ticks since the system was booted.
It compares this with the time for the next timer expiration.
It also updates counters that register how much of the quantum of the current process has been used and how much total time the current process has used.
If the interrupt handler detects that a process has used its quantum or that a timer has expired it generates the message that goes to the main task loop.
The strategy here is that for each clock tick the handler does as little as necessary, as fast as possible.
The costly main task is activated only when there is substantial work to do.
The third general part of the clock software is a collection of subroutines that provide general support, but which are not called in response to clock interrupts, either by the interrupt handler or by the main task loop.
One of these subroutines is coded as PRIVATE, and is called before the main task loop is entered.
It initializes the clock, which entails writing data to the clock chip to cause it to generate interrupts at the desired intervals.
The initialization routine also puts the address of the interrupt handler in the right place to be found when the clock chip triggers the IRQ 8 input to the interrupt controller chip, and then enables that input to respond.
The rest of the subroutines in clock.c are declared PUBLIC, and can be called from anywhere in the kernel binary.
In fact none of them are called from clock.c itself.
They are mostly called by the system task in order to service system calls related to time.
These subroutines do such things as reading the time-since-boot counter, for timing with clock-tick resolution, or reading a register in the clock chip itself, for timing that requires microsecond resolution.
Finally, a subroutine is provided to be called when MINIX 3 shuts down.
This one resets the hardware timer parameters to those expected by the BIOS.
The interrupt handler runs every time the counter in the clock chip reaches zero and generates an interrupt.
In MINIX 3 the time is kept using the method of Fig.
However, in clock.c only the counter for ticks since boot is maintained; records of the boot time are kept elsewhere.
The clock software supplies only the current tick count to aid a system call for the real time.
This is consistent with the MINIX 3 strategy of moving functionality to processes that run in user space.
In the interrupt handler the local counter is updated for each interrupt received.
In some cases it is possible to correct for this effect.
A global variable is available for counting lost ticks, and it is added to the main counter and then reset to zero each time the handler is activated.
In the implementation section we will see an example of how this is used.
The handler also affects variables in the process table, for billing and process control purposes.
A message is sent to the clock task only if the current time has passed the expiration time of the next scheduled timer or if the quantum of the running process has been decremented to zero.
At worst there are five additions or subtractions and six comparisons, plus a few logical operations and assignments in completing the interrupt service.
A few pages back we left hanging the question of how user-space processes can be provided with watchdog timers, which ordinarily are thought of as usersupplied procedures that are part of the user’s code and are executed when a timer expires.
But we can use a synchronous alarm to bridge the gap from the kernel to user space.
This is a good time to explain what is meant by a synchronous alarm.
A signal may arrive or a conventional watchdog may be activated without any relation to what part of a process is currently executing, so these mechanisms are asynchronous.
A synchronous alarm is delivered as a message, and thus can be received only when the recipient has executed receive.
So we say it is synchronous because it will be received only when the receiver expects it.
If the notify method is used to inform a recipient of an alarm, the sender does not have to block, and the recipient does not have to be concerned with missing the alarm.
Messages from notify are saved if the recipient is not waiting.
A bitmap is used, with each bit representing a possible source of a notification.
When a user process makes an alarm system call to ask for an alarm to be set, it is handled by the process manager, which sets up the timer and inserts it into its list of timers.
The process manager asks the system task to send it a notification when the first timer in the PM’s list of timers is scheduled to expire.
The process manager only has to ask for help when the head of its chain of timers changes, either because the first timer has expired or has been cancelled, or because a new request has been received that must go on the chain before the current head.
This is used to support the POSIX-standard alarm system call.
The procedure to execute is within the address space of the process manager.
When executed, the user process that requested the alarm is sent a signal, rather than a notification.
A procedure is provided in clock.c that provides microsecond resolution timing.
Delays as short as a few microseconds may be needed by various I/O devices.
There is no practical way to do this using alarms and the message passing interface.
The counter that is used for generating the clock interrupts can be read directly.
To be useful for I/O timing it would have to be polled by a procedure running in kernel-space, but.
Currently this function is used only as a source of randomness for the random number generator.
More use might be made of it on a very fast system, but this is a future project.
Figure 2-50 summarizes the various services provided directly or indirectly by clock.c.
There are several functions declared PUBLIC that can be called from the kernel or the system task.
All other services are available only indirectly, by system calls ultimately handled by the system task.
Other system processes can ask the system task directly, but user processes must ask the process manager, which also relies on the system task.
Int86 uses the boot monitor to manage the transfer of control to the BIOS, and the monitor returns the number of clock ticks counted while the BIOS call was busy in the ecx register just before the return to the kernel.
This works because, although the clock chip is not triggering the MINIX 3 clock interrupt handler when the BIOS request is handled, the boot monitor can keep track of the time with the help of the BIOS.
To hide the effects of interrupts, operating systems provide a conceptual model consisting of sequential processes running in parallel.
Processes can communicate with each other using interprocess communication primitives, such as semaphores, monitors, or messages.
These primitives are used to ensure that no two processes are ever in their critical sections at the same time.
A process can be running, runnable, or blocked and can change state when it or another process executes one of the interprocess communication primitives.
Interprocess communication primitives can be used to solve such problems as the producer-consumer, dining philosophers, and reader-writer.
Even with these primitives, care has to be taken to avoid errors and deadlocks.
Many scheduling algorithms are known, including round-robin, priority scheduling, multilevel queues, and policy-driven schedulers.
Messages are not buffered, so a send succeeds only when the receiver is waiting for it.
Similarly, a receive succeeds only when a message is already available.
If either operation does not succeed, the caller is blocked.
An attempt to send a notify to a receiver that is not waiting results in a bit being set, which triggers notification when a receive is done later.
As an example of the message flow, consider a user doing a read.
The user process sends a message to the FS requesting it.
If the data are not in the FS’ cache, the FS asks the driver to read it from the disk.
When the disk interrupt happens, the system task is notified, allowing it to reply to the disk driver, which then replies to the FS.
At this point, the FS asks the system task to copy the data from its cache, where the newly requested block has been placed, to the user.
When a process is interrupted, a stack is created within the process table entry of the process, and all the information needed to restart it is put on the new stack.
Any process can be restarted by setting the stack pointer to point to its process table entry and initiating a sequence of instructions to restore the CPU registers, culminating with an iretd instruction.
The scheduler decides which process table entry to put into the stack pointer.
If an exception occurs when the kernel is running, the kernel stack, rather than a stack within the process table, is used.
When an interrupt has been serviced, a process is restarted.
System processes normally run in the highest priority queues and user processes in lower priority queues, but priorities are assigned on a process-by-process basis.
A process stuck in a loop may have its priority temporarily reduced; the priority can be restored when other processes have had a chance to run.
The nice command can be used to change the priority of a process within defined limits.
Processes are run round robin for a quantum that can vary per process.
However, after a process has blocked and becomes ready again it will be put on the head of its queue with just the unused part of its quantum.
This is intended to give faster response to processes doing I/O.
Device drivers and servers are allowed a large quantum, as they are expected to run until they block.
However, even system processes can be preempted if they run too long.
The kernel image includes a system task which facilitates communication of user-space processes with the kernel.
It supports the servers and device drivers by performing privileged operations on their behalf.
In MINIX 3, the clock task is also compiled with the kernel.
It is not a device driver in the ordinary sense.
Why is multiprogramming central to the operation of a modern operating system?
Suppose that you were to design an advanced computer architecture that did process switching in hardware, instead of having interrupts.
What information would the CPU need? Describe how the hardware process switching might work.
On all current computers, at least part of the interrupt handlers are written in assembly language.
When a process is created, it is initially in the New state.
In the text it was stated that the model of Fig.
In a system with threads, is there normally one stack per thread or one stack per process? Explain.
Give an example of a race condition that could possibly occur when buying airplane tickets for two people to go on a trip together.
Write a shell script that produces a file of sequential numbers by reading the last number in the file, adding 1 to it, and then appending to the file.
Run one instance of the script in the background and one in the foreground, each accessing the same file.
Give a sketch of how an operating system that can disable interrupts could implement semaphores.
Show how counting semaphores (i.e., semaphores that can hold an arbitrarily large value) can be implemented using only binary semaphores and ordinary machine instructions.
Does the same problem occur if round-robin scheduling is used instead of priority scheduling? Discuss.
Synchronization within monitors uses condition variables and two special operations, WAIT and SIGNAL.
A more general form of synchronization would be to have a single primitive, WAITUNTIL, that had an arbitrary Boolean predicate as parameter.
This scheme is clearly more general than that of Hoare or Brinch Hansen, but it is not used.
Each employee can be regarded as a communicating sequential process.
When sending to a full mailbox or trying to receive from an empty one, a process does not block.
The process responds to the error code by just trying again, over and over, until it succeeds.
The readers and writers problem can be formulated in several ways with regard to which category of processes can be started when.
Carefully describe three different variations of the problem, each one favoring (or not favoring) some category of processes.
For each variation, specify what happens when a reader or a writer becomes ready to access the data base, and what happens when a process is finished using the data base.
The process switching was done by special hardware, and the overhead was zero.
If a process needed T sec to complete in the absence of competition, how much time would it need if processor sharing was used with n processes?
Round- robin schedulers normally maintain a list of all runnable processes, with each process occurring exactly once in the list.
What would happen if a process occurred twice in the list? Can you think of any reason for allowing this?
Five batch jobs A through E, arrive at a computer center at almost the same time.
For each of the following scheduling algorithms, determine the mean process turnaround time.
For (a), assume that the system is multiprogrammed, and that each job gets its fair share of the CPU.
For (b) through (d) assume that only one job at a time runs, until it finishes.
A process running on CTSS needs 30 quanta to complete.
How many times must it be swapped in, including the very first time (before it has run at all)?
The aging algorithm with a = 1/2 is being used to predict run times.
Could this idea be applied to an interactive system without newly-arriving jobs? How?
How many possible thread sequences are there for the first four times scheduling is done?
What is the largest value of x for which the system is schedulable?
Explain how this design decision causes problems with clock and keyboard interrupts.
When a message is sent to a sleeping process in MINIX 3, the procedure ready is called to put that process on the proper scheduling queue.
The lowest class (user processes) has round-robin scheduling, but the tasks and servers always are allowed to run until they block.
Is it possible for processes in the lowest class to starve? Why (or why not)?
Is MINIX 3 suitable for real-time applications, such as data logging? If not, what could be done to make it so?
Assume that you have an operating system that provides semaphores.
A student majoring in anthropology and minoring in computer science has embarked on a research project to see if African baboons can be taught about deadlocks.
He locates a deep canyon and fastens a rope across it, so the baboons can cross handover-hand.
Several baboons can cross at the same time, provided that they are all going in the same direction.
If eastward moving and westward moving baboons ever get onto the rope at the same time, a deadlock will result (the baboons will get stuck in the middle) because it is impossible for one baboon to climb over another one while suspended over the canyon.
If a baboon wants to cross the canyon, he must check to see that no other baboon is currently crossing in the opposite direction.
Do not worry about a series of eastward moving baboons holding up the westward moving baboons indefinitely.
When a baboon that wants to cross to the east arrives at the rope and finds baboons crossing to the west, he waits until the rope is empty, but no more westward moving baboons are allowed to start until at least one baboon has crossed the other way.
Solve the dining philosophers problem using monitors instead of semaphores.
When no task or server wants to run, pick the user process that has had the smallest share of the CPU.
Modify MINIX 3 so that each process can explicitly set the scheduling priority of its children using a new system call setpriority with parameters pid and priority.
If you do not have access to a running MINIX 3 system, explain the items in Fig.
In the discussion of initialization of the process table we mentioned that some C compilers may generate slightly better code if you add a constant to the array instead of the index.
Write a pair of short C programs to test this hypothesis.
Modify MINIX 3 to collect statistics about messages sent by whom to whom and write a program to collect and print these statistics in a useful way.
One of the main functions of an operating system is to control all the computer’s I/O (Input/Output) devices.
It must issue commands to the devices, catch interrupts, and handle errors.
It should also provide an interface between the devices and the rest of the system that is simple and easy to use.
To the extent possible, the interface should be the same for all devices (device independence)
The I/O code represents a significant fraction of the total operating system.
Thus to really understand what an operating system does, you have to understand how I/O works.
How the operating system manages I/O is the main subject of this chapter.
First we will look at some of the principles of how I/O hardware is organized.
I/O software can be structured in layers, with each layer having a welldefined task to perform.
We will look at these layers to see what they do and how they fit together.
We will define deadlocks precisely, show how they are caused, give two models for analyzing them, and discuss some algorithms for preventing their occurrence.
Following that introduction, we will look at several I/O devices in detail: disks, keyboards, and displays.
For each device we will look at its hardware and software.
Electrical engineers look at it in terms of chips, wires, power supplies, motors, and all the other physical components that make up the hardware.
Programmers look at the interface presented to the software—the commands the hardware accepts, the functions it carries out, and the errors that can be reported back.
In this book we are concerned with programming I/O devices, not designing, building, or maintaining them, so our interest will be restricted to how the hardware is programmed, not how it works inside.
Nevertheless, the programming of many I/O devices is often intimately connected with their internal operation.
In the next three subsections we will provide a little general background on I/O hardware as it relates to programming.
I/O devices can be roughly divided into two categories: block devices and character devices.
A block device is one that stores information in fixed-size blocks, each one with its own address.
The essential property of a block device is that it is possible to read or write each block independently of all the other ones.
If you look closely, the boundary between devices that are block addressable and those that are not is not well defined.
Everyone agrees that a disk is a block addressable device because no matter where the arm currently is, it is always possible to seek to another cylinder and then wait for the required block to rotate under the head.
Now consider a tape drive used for making disk backups.
If the tape drive is given a command to read block N, it can always rewind the tape and go forward until it comes to block N.
This operation is analogous to a disk doing a seek, except that it takes much longer.
Also, it may or may not be possible to rewrite one block in the middle of a tape.
Even if it were possible to use tapes as random access block devices, that is stretching the point somewhat: they are not normally used that way.
The other type of I/O device is the character device.
A character device delivers or accepts a stream of characters, without regard to any block structure.
It is not addressable and does not have any seek operation.
Printers, network interfaces, mice (for pointing), rats (for psychology lab experiments), and most other devices that are not disk-like can be seen as character devices.
Still, the model of block and character devices is general enough that it can be used as a basis for making some of the operating system software dealing with I/O device.
The file system, for example, deals only with abstract block devices and leaves the device-dependent part to lower-level software called device drivers.
I/O devices cover a huge range in speeds, which puts considerable pressure on the software to perform well over many orders of magnitude in data rates.
Most of these devices tend to get faster as time goes on.
I/O units typically consist of a mechanical component and an electronic component.
It is often possible to separate the two portions to provide a more modular and general design.
The electronic component is called the device controller or adapter.
On personal computers, it often takes the form of a printed circuit card that can be inserted into an expansion slot.
The controller card usually has a connector on it, into which a cable leading to the device itself can be plugged.
Many controllers can handle two, four, or even eight identical devices.
If the interface between the controller and device is a standard interface, either an official ANSI, IEEE, or ISO standard or a de facto one, then companies can make controllers or devices that fit that interface.
Many companies, for example, make disk drives that match the IDE (Integrated Drive Electronics) and SCSI (Small Computer System Interface) interfaces.
We mention this distinction between controller and device because the operating system nearly always deals with the controller, not the device.
Most personal computers and servers use the bus model of Fig.
Large mainframes often use a different model, with specialized I/O computers called I/O channels taking some of the load off the main CPU.
A model for connecting the CPU, memory, controllers, and I/O devices.
The interface between the controller and the device is often low-level.
What actually comes off the drive, however, is a serial bit stream, starting with a preamble, then the 4096 bits in a sector, and finally a checksum, also called an Error-Correcting Code (ECC)
The preamble is written when the disk is formatted and contains the cylinder and sector number, the sector size, and similar data.
The controller’s job is to convert the serial bit stream into a block of bytes and perform any error correction necessary.
The block of bytes is typically first assembled, bit by bit, in a buffer inside the controller.
After its checksum has been verified and the block declared to be free of errors, it can then be copied to main memory.
The controller for a monitor also works as a bit serial device at an equally low level.
It reads bytes containing the characters to be displayed from memory and generates the signals used to modulate the CRT beam.
The controller also generates the signals for making a CRT beam do a horizontal retrace after it has finished a scan line, as well as the signals for making it do a vertical retrace after the entire screen has been scanned.
On an LCD screen these signals select individual pixels and control their brightness, simulating the effect of the electron beam in a CRT.
If it were not for the video controller, the operating system programmer.
With the controller, the operating system initializes the controller with a few parameters, such as the number of characters or pixels per line and number of lines per screen, and lets the controller take care of actually driving the display.
Controllers for some devices, especially disks, are becoming extremely sophisticated.
For example, modern disk controllers often have many megabytes of memory inside the controller.
As a result, when a read is being processed, as soon as the arm gets to the correct cylinder, the controller begins reading and storing data, even if it has not yet reached the sector it needs.
This cached data may come in handy for satisfying subsequent requests.
Furthermore, even after the requested data has been obtained, the controller may continue to cache data from subsequent sectors, since they are likely to be needed later.
In this manner, many disk reads can be handled without any disk activity at all.
Each controller has a few registers that are used for communicating with the CPU.
By writing into these registers, the operating system can command the device to deliver data, accept data, switch itself on or off, or otherwise perform some action.
By reading from these registers, the operating system can learn what the device’s state is, whether it is prepared to accept a new command, and so on.
In addition to the control registers, many devices have a data buffer that the operating system can read and write.
For example, a common way for computers to display pixels on the screen is to have a video RAM, which is basically just a data buffer, available for programs or the operating system to write into.
The issue thus arises of how the CPU communicates with the control registers and the device data buffers.
Most early computers, including nearly all mainframes, such as the IBM 360 and all of its successors, worked this way.
In this scheme, the address spaces for memory and I/O are different, as shown in Fig.
On other computers, I/O registers are part of the regular memory address space, as shown in Fig.
This scheme is called memory-mapped I/O, and was introduced with the PDP-11 minicomputer.
Usually, the assigned addresses are at the top of the address space.
A hybrid scheme, with memorymapped I/O data buffers and separate I/O ports for the control registers is shown in Fig.
How do these schemes work? In all cases, when the CPU wants to read a word, either from memory or from an I/O port, it puts the address it needs on the address lines of the bus and then asserts a READ signal on a bus control line.
A second signal line is used to tell whether I/O space or memory space is needed.
If it is memory space, the memory responds to the request.
If it is I/O space, the I/O device responds to the request.
If the address falls in its range, it responds to the request.
Since no address is ever assigned to both memory and an I/O device, there is no ambiguity and no conflict.
Usually, controller registers have one or more status bits that can be tested to determine if an output operation is complete or if new data is available from an input device.
A CPU can execute a loop, testing a status bit each time until a device is ready to accept or provide new data.
In the realm of I/O, where you might have to wait a very long time for the outside world to accept or produce data, polling is not acceptable except for very small dedicated systems not running multiple processes.
In addition to status bits, many controllers use interrupts to tell the CPU when they are ready to have their registers read or written.
We saw how interrupts are handled by the CPU in Sec.
In the context of I/O, all you need to know is that most interface devices provide an output which is logically the same as the ‘‘operation complete’’ or ‘‘data ready’’ status bit of a register, but which is meant to be used to drive one of the IRQ (Interrupt ReQuest) lines of the system bus.
Thus when an interrupt-enabled operation completes, it interrupts the CPU and starts the interrupt handler running.
This piece of code informs the operating system that I/O is complete.
The operating system may then check the status bits to verify that all went well, and either harvest the resulting data or initiate a retry.
The number of inputs to the interrupt controller may be limited; Pentium-class PCs have only 15 available for I/O devices.
Some controllers are hard-wired onto the system parentboard, for example, the disk and keyboard controllers of an IBM PC.
On older systems, the IRQ used by the device was set by a switch or jumper associated with the controller.
If a user bought a new plug-in board, he had to manually set the IRQ to avoid conflicts with existing IRQs.
Few users could do this correctly, which led the industry to develop Plug ’n Play, in which the BIOS can automatically assign IRQs to devices at boot time to avoid conflicts.
The CPU can request data from an I/O controller one byte at a time but doing so for a device like a disk that produces a large block of data wastes the CPU’s time, so a different scheme, called DMA (Direct Memory Access) is often used.
The operating system can only use DMA if the hardware has a DMA controller, which most systems do.
Sometimes this controller is integrated into disk controllers and other controllers, but such a design requires a separate DMA controller for each device.
More commonly, a single DMA controller is available (e.g., on the parentboard) for regulating transfers to multiple devices, often concurrently.
No matter where it is physically located, the DMA controller has access to the system bus independent of the CPU, as shown in Fig.
It contains several registers that can be written and read by the CPU.
These include a memory address register, a byte count register, and one or more control registers.
The control registers specify the I/O port to use, the direction of the transfer (reading from the I/O device or writing to the I/O device), the transfer unit (byte at a time or word at a time), and the number of bytes to transfer in one burst.
To explain how DMA works, let us first look at how disk reads occur when DMA is not used.
First the controller reads the block (one or more sectors) from the drive serially, bit by bit, until the entire block is in the controller’s internal buffer.
Next, it computes the checksum to verify that no read errors have occurred.
It also issues a command to the disk controller telling it to read data from the disk into its internal buffer and verify the checksum.
When valid data are in the disk controller’s buffer, DMA can begin.
The DMA controller initiates the transfer by issuing a read request over the bus to the disk controller (step 2)
This read request looks like any other read request, and the disk controller does not know or care whether it came from the CPU or from a DMA controller.
Typically, the memory address to write to is on the address lines of the bus so when the disk controller fetches the next word from its internal buffer, it knows where to write it.
The write to memory is another standard bus cycle (step 3)
When the write is complete, the disk controller sends an acknowledgement signal to the disk controller, also over the bus (step 4)
The DMA controller then increments the memory address to use and decrements the byte count.
When the operating system starts up, it does not have to copy the block to memory; it is already there.
You may be wondering why the controller does not just store the bytes in main memory as soon as it gets them from the disk.
In other words, why does it need an internal buffer? There are two reasons.
First, by doing internal buffering, the disk controller can verify the checksum before starting a transfer.
If the checksum is incorrect, an error is signaled and no transfer to memory is done.
The second reason is that once a disk transfer has started, the bits keep arriving from the disk at a constant rate, whether the controller is ready for them or not.
If the controller tried to write data directly to memory, it would have to go over the system bus for each word transferred.
If the bus were busy due to some other device using it, the controller would have to wait.
If the next disk word arrived before the previous one had been stored, the controller would have to store it somewhere.
If the bus were very busy, the controller might end up storing quite a few words and having a lot of administration to do as well.
When the block is buffered internally, the bus is not needed until the DMA begins, so the design of the controller is much simpler because the DMA transfer to memory is not time critical.
The argument against it is that the main CPU is often far faster than the DMA controller and can do the job much faster (when the limiting factor is not the speed of the I/O device)
If there is no other work for it to do, having the (fast) CPU wait for the (slow) DMA controller to finish is pointless.
Also, getting rid of the DMA controller and having the CPU do all the work in software saves money, important on low-end (embedded) computers.
Let us now turn away from the I/O hardware and look at the I/O software.
First we will look at the goals of the I/O software and then at the different ways I/O can be done from the point of view of the operating system.
A key concept in the design of I/O software is device independence.
What this means is that it should be possible to write programs that can access any I/O device without having to specify the device in advance.
For example, a program that reads a file as input should be able to read a file on a floppy disk, on a hard disk, or on a CD-ROM, without having to modify the program for each different device.
Similarly, one should be able to type a command such as.
It is up to the operating system to take care of the problems caused by the fact that these devices really are different and require very different command sequences to read or write.
Closely related to device independence is the goal of uniform naming.
The name of a file or a device should simply be a string or an integer and not depend on the device in any way.
In UNIX and MINIX 3, all disks can be integrated into.
For example, a floppy disk can be mounted on top of the directory /usr/ast/backup so that copying a file to that directory copies the file to the diskette.
In this way, all files and devices are addressed the same way: by a path name.
In general, errors should be handled as close to the hardware as possible.
If the controller discovers a read error, it should try to correct the error itself if it can.
If it cannot, then the device driver should handle it, perhaps by just trying to read the block again.
Many errors are transient, such as read errors caused by specks of dust on the read head, and will go away if the operation is repeated.
Only if the lower layers are not able to deal with the problem should the upper layers be told about it.
In many cases, error recovery can be done transparently at a low level without the upper levels even knowing about the error.
Still another key issue is synchronous (blocking) versus asynchronous (interrupt-driven) transfers.
Most physical I/O is asynchronous—the CPU starts the transfer and goes off to do something else until the interrupt arrives.
User programs are much easier to write if the I/O operations are blocking—after a receive system call the program is automatically suspended until the data are available in the buffer.
It is up to the operating system to make operations that are actually interrupt-driven look blocking to the user programs.
Often data that come off a device cannot be stored directly in its final destination.
For example, when a packet comes in off the network, the operating system does not know where to put it until it has stored the packet somewhere and examined it.
Also, some devices have severe real-time constraints (for example, digital audio devices), so the data must be put into an output buffer in advance to decouple the rate at which the buffer is filled from the rate at which it is emptied, in order to avoid buffer underruns.
Buffering involves considerable copying and often has a major impact on I/O performance.
The final concept that we will mention here is sharable versus dedicated devices.
Some I/O devices, such as disks, can be used by many users at the same time.
No problems are caused by multiple users having open files on the same disk at the same time.
Other devices, such as tape drives, have to be dedicated to a single user until that user is finished.
Having two or more users writing blocks intermixed at random to the same tape will definitely not work.
Introducing dedicated (unshared) devices also introduces a variety of problems, such as deadlocks.
Again, the operating system must be able to handle both shared and dedicated devices in a way that avoids problems.
I/O software is often organized in four layers, as shown in Fig.
In the following subsections we will look at each in turn, starting at the bottom.
The emphasis in this chapter is on the device drivers (layer 2), but we will summarize the rest of the I/O software to show how the pieces of the I/O system fit together.
Interrupts are an unpleasant fact of life; although they cannot be avoided, they should be hidden away, deep in the bowels of the operating system, so that as little of the operating system as possible knows about them.
The best way to hide them is to have the driver starting an I/O operation block until the I/O has completed and the interrupt occurs.
The driver can block itself by doing a down on a semaphore, a wait on a condition variable, a receive on a message, or something similar, for example.
When the interrupt happens, the interrupt procedure does whatever it has to in order to handle the interrupt.
In some cases it will just complete up on a semaphore.
In others it will do a signal on a condition variable in a monitor.
In still others, it will send a message to the blocked driver.
In all cases the net effect of the interrupt will be that a driver that was previously blocked will now be able to run.
This model works best if drivers are structured as independent processes, with their own states, stacks, and program counters.
Earlier in this chapter we saw that each device controller has registers used to give it commands or to read out its status or both.
The number of registers and the nature of the commands vary radically from device to device.
For example, a mouse driver has to accept information from the mouse telling how far it has moved and which buttons are currently depressed.
In contrast, a disk driver has to know about sectors, tracks, cylinders, heads, arm motion, motor drives, head settling times, and all the other mechanics of making the disk work properly.
Thus, each I/O device attached to a computer needs some device-specific code for controlling it.
This code, called the device driver, is generally written by the device’s manufacturer and delivered along with the device on a CD-ROM.
Since each operating system needs its own drivers, device manufacturers commonly supply drivers for several popular operating systems.
Each device driver normally handles one device type, or one class of closely related devices.
For example, it would probably be a good idea to have a single mouse driver, even if the system supports several different brands of mice.
As another example, a disk driver can usually handle multiple disks of different sizes and different speeds, and perhaps a CD-ROM as well.
On the other hand, a mouse and a disk are so different that different drivers are necessary.
In order to access the device’s hardware, meaning the controller’s registers, the device driver traditionally has been part of the system kernel.
This approach gives the best performance and the worst reliability since a bug in any device driver can crash the entire system.
As we shall see, in MINIX 3 each device driver is now a separate user-mode process.
As we mentioned earlier, operating systems usually classify drivers as block devices, such as disks, or character devices, such as keyboards and printers.
Most operating systems define a standard interface that all block drivers must support and a second standard interface that all character drivers must support.
These interfaces consist of a number of procedures that the rest of the operating system can call to get the driver to do work for it.
In general terms, the job of a device driver is to accept abstract requests from the device-independent software above it and see to it that the request is executed.
A typical request to a disk driver is to read block n.
If the driver is idle at the time a request comes in, it starts carrying out the request immediately.
If, however, it is already busy with a request, it will normally enter the new request into a queue of pending requests to be dealt with as soon as possible.
The first step in actually carrying out an I/O request is to check that the input parameters are valid and to return an error if they are not.
If the request is valid the next step is to translate it from abstract to concrete terms.
For a disk driver, this means figuring out where on the disk the requested block actually is, checking to see if the drive’s motor is running, determining if the arm is positioned on the proper cylinder, and so on.
In short, the driver must decide which controller operations are required and in what sequence.
Once the driver has determined which commands to issue to the controller, it starts issuing them by writing into the controller’s device registers.
Simple controllers can handle only one command at a time.
More sophisticated controllers are willing to accept a linked list of commands, which they then carry out by themselves without further help from the operating system.
After the command or commands have been issued, one of two situations will apply.
In many cases the device driver must wait until the controller does some work for it, so it blocks itself until the interrupt comes in to unblock it.
In other cases, however, the operation finishes without delay, so the driver need not block.
As an example of the latter situation, scrolling the screen on some graphics cards.
No mechanical motion is needed, so the entire operation can be completed in a few microseconds.
In the former case, the blocked driver will be awakened by the interrupt.
In the latter case, it will never go to sleep.
Either way, after the operation has been completed, it must check for errors.
If everything is all right, the driver may have data to pass to the device-independent software (e.g., a block just read)
Finally, it returns some status information for error reporting back to its caller.
If any other requests are queued, one of them can now be selected and started.
If nothing is queued, the driver blocks waiting for the next request.
Dealing with requests for reading and writing is the main function of a driver, but there may be other requirements.
For instance, the driver may need to initialize a device at system startup or the first time it is used.
Also, there may be a need to manage power requirements, handle Plug ’n Play, or log events.
Although some of the I/O software is device specific, a large fraction of it is device independent.
The exact boundary between the drivers and the deviceindependent software is system dependent, because some functions that could be done in a device-independent way may actually be done in the drivers, for efficiency or other reasons.
In MINIX 3, most of the device-independent software is part of the file system.
The basic function of the device-independent software is to perform the I/O functions that are common to all devices and to provide a uniform interface to the user-level software.
Below we will look at the above issues in more detail.
A major issue in an operating system is how to make all I/O devices and drivers look more-or-less the same.
With a standard interface it is much easier to plug in a new driver, providing it conforms to the driver interface.
It also means that driver writers know what is expected of them (e.g., what functions they must provide and what kernel functions they may call)
In practice, not all devices are absolutely identical, but usually there are only a small number of device types and even these are generally almost the same.
For example, even block and character devices have many functions in common.
Another aspect of having a uniform interface is how I/O devices are named.
The device-independent software takes care of mapping symbolic device names onto the proper driver.
The inode also contains the minor device number, which is passed as a parameter to the driver in order to specify the unit to be read or written.
All devices have major and minor numbers, and all drivers are accessed by using the major device number to select the driver.
How does the system prevent users from accessing devices that they are not entitled to access? In UNIX, MINIX 3, and also in later Windows versions such as Windows 2000 and Windows XP, devices appear in the file system as named objects, which means that the usual protection rules for files also apply to I/O devices.
The system administrator can then set the proper permissions (i.e., in UNIX the rwx bits) for each device.
Buffering is also an issue for both block and character devices.
For block devices, the hardware generally insists upon reading and writing entire blocks at once, but user processes are free to read and write in arbitrary units.
If a user process writes half a block, the operating system will normally keep the data around internally until the rest of the data are written, at which time the block can go out to the disk.
For character devices, users can write data to the system faster than it can be output, necessitating buffering.
Keyboard input that arrives before it is needed also requires buffering.
Errors are far more common in the context of I/O than in any other context.
When they occur, the operating system must handle them as best it can.
Many errors are device-specific, so only the driver knows what to do (e.g., retry, ignore, or panic)
A typical error is caused by a disk block that has been damaged and cannot be read any more.
After the driver has tried to read the block a certain number of times, it gives up and informs the device-independent software.
How the error is treated from here on is device independent.
If the error occurred while reading a user file, it may be sufficient to report the error back to the caller.
However, if it occurred while reading a critical system data structure, such as the block containing the bitmap showing which blocks are free, the operating system may have to display an error message and terminate.
Some devices, such as CD-ROM recorders, can be used only by a single process at any given moment.
It is up to the operating system to examine requests for device usage and accept or reject them, depending on whether the requested device is available or not.
A simple way to handle these requests is to require processes to perform opens on the special files for devices directly.
It is up to the device-independent software to hide this fact and provide a uniform block size to higher layers, for example, by treating several sectors as a single logical block.
In this way, the higher layers only deal with abstract devices that all use the same logical block size, independent of the physical sector size.
Similarly, some character devices deliver their data one byte at a time (e.g., modems), while others deliver theirs in larger units (e.g., network interfaces)
Although most of the I/O software is within the operating system, a small portion of it consists of libraries linked together with user programs, and even whole programs running outside the kernel.
System calls, including the I/O system calls, are normally made by library procedures.
While these procedures do little more than put their parameters in the appropriate place for the system call, there are other I/O procedures that actually do real work.
In particular, formatting of input and output is done by library procedures.
One example from C is printf, which takes a format string and possibly some variables as input, builds an ASCII string, and then calls write to output the string.
An example of a similar procedure for input is scanf which reads input and stores it into variables described in a format string using the same syntax as printf.
The standard I/O library contains a number of procedures that involve I/O and all run as part of user programs.
Spooling is a way of dealing with dedicated I/O devices in a multiprogramming system.
Although it would be technically simple to let any user process open the character special file for the printer, suppose a process opened it and then did nothing for hours? No other process could print anything.
Instead what is done is to create a special process, called a daemon, and a special directory, called a spooling directory.
To print a file, a process first generates the entire file to be printed and puts it in the spooling directory.
It is up to the daemon, which is the only process having permission to use the printer’s special file, to print the files in the directory.
By protecting the special file against direct use by users, the problem of having someone keeping it open unnecessarily long is eliminated.
Spooling is used not only for printers, but also in various other situations.
When a message is submitted it is put in a mail spool directory.
At any given instant of time a particular destination may be temporarily unreachable, so.
The daemon may also send a message back to the sender saying delivery is delayed, or, after a delay of hours or days, saying the message cannot be delivered.
Figure 3-8 summarizes the I/O system, showing the layers and principal functions of each layer.
Starting at the bottom, the layers are the hardware, interrupt handlers, device drivers, device-independent software, and the user processes.
Layers of the I/O system and the main functions of each layer.
When a user program tries to read a block from a file, for example, the operating system is invoked to carry out the call.
The device-independent software looks for it in the buffer cache, for example.
If the needed block is not there, it calls the device driver to issue the request to the hardware to go get it from the disk.
The process is then blocked until the disk operation has been completed.
When the disk is finished, the hardware generates an interrupt.
The interrupt handler is run to discover what has happened, that is, which device wants attention right now.
It then extracts the status from the device and wakes up the sleeping process to finish off the I/O request and let the user process continue.
Computer systems are full of resources that can only be used by one process at a time.
Common examples include printers, tape drives, and slots in the system’s internal tables.
Having two processes simultaneously writing to the printer leads to gibberish.
Having two processes using the same file system table slot will invariably lead to a corrupted file system.
Consequently, all operating systems have the ability to (temporarily) grant a process exclusive access to certain resources, both hardware and software.
For many applications, a process needs exclusive access to not one resource, but several.
Suppose, for example, two processes each want to record a scanned document on a CD.
Process A requests permission to use the scanner and is granted it.
Process B is programmed differently and requests the CD recorder first and is also granted it.
Now A asks for the CD recorder, but the request is denied until B releases it.
Unfortunately, instead of releasing the CD recorder B asks for the scanner.
At this point both processes are blocked and will remain so forever.
Deadlocks can occur in a variety of situations besides requesting dedicated I/O devices.
In a database system, for example, a program may have to lock several records it is using, to avoid race conditions.
Thus deadlocks can occur on hardware resources or on software resources.
In this section, we will look at deadlocks more closely, see how they arise, and study some ways of preventing or avoiding them.
Although this material is about deadlocks in the context of operating systems, they also occur in database systems and many other contexts in computer science, so this material is actually applicable to a wide variety of multiprocess systems.
Deadlocks can occur when processes have been granted exclusive access to devices, files, and so forth.
To make the discussion of deadlocks as general as possible, we will refer to the objects granted as resources.
A resource can be a hardware device (e.g., a tape drive) or a piece of information (e.g., a locked record in a database)
A computer will normally have many different resources that can be acquired.
For some resources, several identical instances may be available, such as three tape drives.
When interchangeable copies of a resource are available, called fungible resources†, any one of them can be used to satisfy any request for the resource.
In short, a resource is anything that can be used by only a single process at any instant of time.
A preemptable resource is one that can be taken away from the process owning it with no ill effects.
Process A requests and gets the printer, then starts to compute the values to print.
Before it has finished with the computation, it exceeds its time quantum and is swapped or paged out.
Gold is fungible: one gram of gold is as good as any other.
Fortunately, it is possible to preempt (take away) the memory from B by swapping it out and swapping A in.
Now A can run, do its printing, and then release the printer.
A nonpreemptable resource, in contrast, is one that cannot be taken away from its current owner without causing the computation to fail.
If a process has begun to burn a CD-ROM, suddenly taking the CD recorder away from it and giving it to another process will result in a garbled CD.
Potential deadlocks that involve preemptable resources can usually be resolved by reallocating resources from one process to another.
The sequence of events required to use a resource is given below in an abstract form.
If the resource is not available when it is requested, the requesting process is forced to wait.
In some operating systems, the process is automatically blocked when a resource request fails, and awakened when it becomes available.
In other systems, the request fails with an error code, and it is up to the calling process to wait a little while and try again.
Deadlock can be defined formally as follows: A set of processes is deadlocked if each process in the set is waiting for an event that only another process in the set can cause.
Because all the processes are waiting, none of them will ever cause any of the events that could wake up any of the other members of the set, and all the processes continue to wait forever.
For this model, we assume that processes have only a single thread and that there are no interrupts possible to wake up a blocked process.
The no-interrupts condition is needed to prevent an otherwise deadlocked process from being awakened by, say, an alarm, and then causing events that release other processes in the set.
In most cases, the event that each process is waiting for is the release of some resource currently possessed by another member of the set.
In other words, each member of the set of deadlocked processes is waiting for a resource that is owned by a deadlocked process.
None of the processes can run, none of them can release.
The number of processes and the number and kind of resources possessed and requested are unimportant.
This result holds for any kind of resource, including both hardware and software.
Each resource is either currently assigned to exactly one process or is available.
Processes currently holding resources that were granted earlier can request new resources.
Resources previously granted cannot be forcibly taken away from a process.
They must be explicitly released by the process holding them.
There must be a circular chain of two or more processes, each of which is waiting for a resource held by the next member of the chain.
All four of these conditions must be present for a deadlock to occur.
If one of them is absent, no deadlock is possible.
The literature contains examples of ‘‘deadlock’’ that do not really meet all of these conditions.
For instance, if four vehicles arrive simultaneously at a crossroad and try to obey the rule that each should yield to the vehicle on the right, none can proceed, but this is not a case where one process already has possession of a unique resource.
Rather, this problem is a ‘‘scheduling deadlock’’ which can be resolved by a decision about priorities imposed from outside by a policeman.
It is worth noting that each condition relates to a policy that a system can have or not have.
Can a given resource be assigned to more than one process at once? Can a process hold a resource and ask for another? Can resources be preempted? Can circular waits exist? Later on we will see how deadlocks can be attacked by trying to negate some of these conditions.
Holt (1972) showed how these four conditions can be modeled using directed graphs.
The graphs have two kinds of nodes: processes, shown as circles, and resources, shown as squares.
An arc from a resource node (square) to a process.
Now let us see how resource graphs can be used.
The requests and releases of the three processes are given in Fig.
The operating system is free to run any unblocked process at any instant, so it could decide to run A until A finished all its work, then run B to completion, and finally run C.
This ordering does not lead to any deadlocks (because there is no competition for resources) but it also has no parallelism at all.
In addition to requesting and releasing resources, processes compute and do I/O.
When the processes are run sequentially, there is no possibility that while one process is waiting for I/O, another can use the CPU.
Thus running the processes strictly sequentially may not be optimal.
On the other hand, if none of the processes do any I/O at all, shortest job first is better than round robin, so under some circumstances running all processes sequentially may be the best way.
Let us now suppose that the processes do both I/O and computing, so that round robin is a reasonable scheduling algorithm.
The resource requests might occur in the order of Fig.
If these six requests are carried out in that order, the six resulting resource graphs are shown in Fig.
After request 4 has been made, A blocks waiting for S, as shown in Fig.
In the next two steps B and C also block, ultimately leading to a cycle and the deadlock of Fig.
However, as we have already mentioned, the operating system is not required to run the processes in any special order.
In particular, if granting a particular request might lead to deadlock, the operating system can simply suspend the process without granting the request (i.e., just not schedule the process) until it is safe.
By running only A and C, we would get the requests and releases of Fig.
After step (q), process B can be granted S because A is finished and C has everything it needs.
Even if B should eventually block when requesting T, no deadlock can occur.
Later in this chapter we will study a detailed algorithm for making allocation decisions that do not lead to deadlock.
For the moment, the point to understand is that resource graphs are a tool that let us see if a given request/release sequence leads to deadlock.
We just carry out the requests and releases step by step, and after every step check the graph to see if it contains any cycles.
If so, we have a deadlock; if not, there is no deadlock.
Although our treatment of resource graphs has been for the case of a single resource of each type, resource graphs can also be generalized to handle multiple resources of the same type (Holt, 1972)
If even one branch of the graph is not part of a cycle, that is, if one process which is not deadlocked holds a copy of one of the resources, then deadlock may not occur.
In general, four strategies are used for dealing with deadlocks.
Prevention, by structurally negating one of the four conditions necessary to cause a deadlock.
We will examine each of these methods in turn in the next four sections.
Ostriches can run at 60 km/hour and their kick is powerful enough to kill any lion with visions of a big chicken dinner.
An example of how deadlock occurs and how it can be avoided.
Mathematicians find it completely unacceptable and say that deadlocks must be prevented at all costs.
Engineers ask how often the problem is expected, how often the system crashes for other reasons, and how serious a deadlock is.
If deadlocks occur on the average once every five years, but system crashes due to hardware failures, compiler errors, and operating system bugs occur once a week, most engineers would not be willing to pay a large penalty in performance or convenience to eliminate deadlocks.
To make this contrast more specific, UNIX (and MINIX 3) potentially suffer from deadlocks that are not even detected, let alone automatically broken.
The total number of processes in a system is determined by the number of entries in the process table.
If a fork fails because the table is full, a reasonable approach for the program doing the fork is to wait a random time and try again.
Ten programs are running, each of which needs to create 12 (sub)processes.
Each of the 10 original processes now sits in an endless loop forking and failing—a deadlock.
The probability of this happening is minuscule, but it could happen.
Should we abandon processes and the fork call to eliminate the problem?
The maximum number of open files is similarly restricted by the size of the inode table, so a similar problem occurs when it fills up.
In fact, almost every table in the operating system represents a finite resource.
Should we abolish all of these because it might happen that a collection of n processes might each claim 1/n of the total, and then each try to claim another one?
Most operating systems, including UNIX, MINIX 3, and Windows, just ignore the problem on the assumption that most users would prefer an occasional deadlock to a rule restricting all users to one process, one open file, and one of everything.
If deadlocks could be eliminated for free, there would not be much discussion.
The problem is that the price is high, mostly in terms of putting inconvenient restrictions on processes, as we will see shortly.
Thus we are faced with an unpleasant trade-off between convenience and correctness, and a great deal of discussion about which is more important, and to whom.
When this technique is used, the system does not do anything except monitor the requests and releases of resources.
Every time a resource is requested or released, the resource graph is updated, and a check is made to see if any cycles exist.
If this does not break the deadlock, another process is killed, and so on until the cycle is broken.
A somewhat cruder method is not even to maintain the resource graph but instead periodically to check to see if there are any processes that have been continuously blocked for more than say, 1 hour.
Detection and recovery is the strategy often used on large mainframe computers, especially batch systems in which killing a process and then restarting it is usually acceptable.
Care must be taken to restore any modified files to their original state, however, and undo any other side effects that may have occurred.
The third deadlock strategy is to impose suitable restrictions on processes so that deadlocks are structurally impossible.
If no resource were ever assigned exclusively to a single process, we would never have deadlocks.
However, it is equally clear that allowing two processes to write on the printer at the same time will lead to chaos.
By spooling printer output, several processes can generate output at the same time.
In this model, the only process that actually requests the physical printer is the printer daemon.
Since the daemon never requests any other resources, we can eliminate deadlock for the printer.
Unfortunately, not all devices can be spooled (the process table does not lend itself well to being spooled)
Furthermore, competition for disk space for spooling can itself lead to deadlock.
What would happen if two processes each filled up half of the available spooling space with output and neither was finished producing output? If the daemon was programmed to begin printing even before all the output was spooled, the printer might lie idle if an output process decided to wait several hours after the first burst of output.
For this reason, daemons are normally programmed to print only after the complete output file is available.
In this case we have two processes that have each finished part, but not all, of their output, and cannot continue.
Neither process will ever finish, so we have a deadlock on the disk.
The second of the conditions stated by Coffman et al.
If we can prevent processes that hold resources from waiting for more resources, we can eliminate deadlocks.
One way to achieve this goal is to require all processes to request all their resources before starting execution.
If everything is available, the process will be allocated whatever it needs and can run to completion.
If one or more resources are busy, nothing will be allocated and the process would just wait.
An immediate problem with this approach is that many processes do not know how many resources they will need until after they have started running.
Take, as an example, a process that reads data from an input tape, analyzes it for an hour, and then writes an output tape as well as plotting the results.
If all resources must be requested in advance, the process will tie up the output tape drive and the plotter for an hour.
A slightly different way to break the hold-and-wait condition is to require a process requesting a resource to first temporarily release all the resources it currently holds.
Then it tries to get everything it needs all at once.
Attacking the third condition (no preemption) is even less promising than attacking the second one.
If a process has been assigned the printer and is in the middle of printing its output, forcibly taking away the printer because a needed plotter is not available is tricky at best and impossible at worst.
One way is simply to have a rule saying that a process is entitled only to a single resource at any moment.
If it needs a second one, it must release the first one.
For a process that needs to copy a huge file from a tape to a printer, this restriction is unacceptable.
Another way to avoid the circular wait is to provide a global numbering of all the resources, as shown in Fig.
Now the rule is this: processes can request resources whenever they want to, but all requests must be made in numerical order.
A process may request first a scanner and then a tape drive, but it may not request first a plotter and then a scanner.
With this rule, the resource allocation graph can never have cycles.
Let us see why this is true for the case of two processes, in Fig.
We can get a deadlock only if A requests resource j and B requests resource i.
Assuming i and j are distinct resources, they will have different numbers.
If i > j, then A is not allowed to request j because that is lower than what it already has.
If i < j, then B is not allowed to request i because that is lower than what it already has.
At every instant, one of the assigned resources will be highest.
The process holding that resource will never ask for a resource already assigned.
It will either finish, or at worst, request even higher numbered resources, all of which are available.
At this point, some other process will hold the highest resource and can also finish.
In short, there exists a scenario in which all processes finish, so no deadlock is present.
A minor variation of this algorithm is to drop the requirement that resources be acquired in strictly increasing sequence and merely insist that no process request a resource lower than what it is already holding.
Although numerically ordering the resources eliminates the problem of deadlocks, it may be impossible to find an ordering that satisfies everyone.
When the resources include process table slots, disk spooler space, locked database records, and other abstract resources, the number of potential resources and different uses may be so large that no ordering could possibly work.
Also, as Levine (2005) points out, ordering resources negates fungibility—a perfectly good and available copy of a resource could be inaccessible with such a rule.
The various approaches to deadlock prevention are summarized in Fig.
The question arises: is there an algorithm that can always avoid deadlock by making the right choice all the time? The answer is a qualified yeswe can avoid deadlocks, but only if certain information is available in advance.
In this section we examine ways to avoid deadlock by careful resource allocation.
A scheduling algorithm that can avoid deadlocks is due to Dijkstra (1965) and is known as the banker’s algorithm.
It is modeled on the way a small-town banker might deal with a group of customers to whom he has granted lines of credit.
The banker does not necessarily have enough cash on hand to lend every customer the full amount of each one’s line of credit at the same time.
He also trusts every customer to be able to repay his loan soon after receiving his total line of credit (it is a small town), so he knows eventually he can service all the requests.
In this analogy, customers are processes, units are, say, tape drives, and the banker is the operating system.
Each part of the figure shows a state of the system with respect to resource allocation, that is, a list of customers showing the money already loaned (tape drives already assigned) and the maximum credit available (maximum number of tape drives needed at once later)
A state is safe if there exists a sequence of other states that leads to all customers getting loans up to their credit limits (all processes getting all their resources and terminating)
The customers go about their respective businesses, making loan requests from time to time (i.e., asking for resources)
At a certain moment, the situation is as shown in Fig.
This state is safe because with two units left, the banker can delay any requests except C’s, thus letting C finish and release all four of his resources.
With four units in hand, the banker can let either D or B have the necessary units, and so on.
Consider what would happen if a request from B for one more unit were granted in Fig.
If all the customers suddenly asked for their maximum loans, the banker could not satisfy any of them, and we would have a deadlock.
An unsafe state does not have to lead to deadlock, since a customer might not need the entire credit line available, but the banker cannot count on this behavior.
The banker’s algorithm considers each request as it occurs, and sees if granting it leads to a safe state.
If it does, the request is granted; otherwise, it is postponed until later.
To see if a state is safe, the banker checks to see if he has enough resources to satisfy some customer.
If so, those loans are assumed to be repaid, and the customer now closest to the limit is checked, and so on.
If all loans can eventually be repaid, the state is safe and the initial request can be granted.
The above algorithm was described in terms of a single resource class (e.g., only tape drives or only printers, but not some of each)
The horizontal axis represents the number of instructions executed by process A.
The vertical axis represents the number of instructions executed by process B.
Every point in the diagram represents a joint state of the two processes.
Initially, the state is at p, with neither process having executed any instructions.
If the scheduler chooses to run A first, we get to the point q, in which A has executed some number of instructions, but B has executed none.
At point q the trajectory becomes vertical, indicating that the scheduler has chosen to run B.
With a single processor, all paths must be horizontal or vertical, never diagonal.
Furthermore, motion is always to the north or east, never to the south or west (processes cannot run backward)
When A crosses the I 1 line on the path from r to s, it requests and is granted the printer.
The region with lines slanting from southwest to northeast represents both processes having the printer.
The mutual exclusion rule makes it impossible to enter this region.
Similarly, the region shaded the other way represents both processes having the plotter, and is equally impossible.
Under no conditions can the system enter the shaded regions.
At this point, A is requesting the plotter and B is requesting the printer, and both are already assigned.
The entire box is unsafe and must not be entered.
The important thing to see here is at point t B is requesting a resource.
The system must decide whether to grant it or not.
If the grant is made, the system will enter an unsafe region and eventually deadlock.
To avoid the deadlock, B should be suspended until A has requested and released the plotter.
This graphical model is difficult to apply to the general case of an arbitrary number of processes and an arbitrary number of resource classes, each with multiple instances (e.g., two plotters, three tape drives)
However, the banker’s algorithm can be generalized to do the job.
The matrix on the right shows how many resources each process still needs in order to complete.
As in the single resource case, processes must state their total resource needs before executing, so that the system can compute the right-hand matrix at each instant.
The three vectors at the right of the figure show the existing resources, E, the possessed resources, P, and the available resources, A, respectively.
From E we see that the system has six tape drives, three plotters, four printers, and two CDROM drives.
Of these, five tape drives, three plotters, two printers, and two CDROM drives are currently assigned.
This fact can be seen by adding up the four resource columns in the left-hand matrix.
The available resource vector is simply the difference between what the system has and what is currently in use.
The algorithm for checking to see if a state is safe can now be stated.
If no such row exists, the system will eventually deadlock since no process can run to completion.
Assume the process of the row chosen requests all the resources it needs (which is guaranteed to be possible) and finishes.
Mark that process as terminated and add all its resources to the A vector.
If several processes are eligible to be chosen in step 1, it does not matter which one is selected: the pool of available resources either gets larger or stays the same.
Now let us get back to the example of Fig.
This request can be granted because the resulting state is still safe (process D can finish, and then processes A or E, followed by the rest)
Now imagine that after giving B one of the two remaining printers, E wants the last printer.
Since that time, nearly every book on operating systems has described it in detail.
Innumerable papers have been written about various aspects of it.
Unfortunately, few authors have had the audacity to point out that although in theory the algorithm is wonderful, in practice it is essentially useless because processes rarely know in advance what their maximum resource needs will be.
In addition, the number of processes is not fixed, but dynamically varying as new users log in and out.
Furthermore, resources that were thought to be available can suddenly vanish (tape drives can break)
Thus in practice, few, if any, existing systems use the banker’s algorithm for avoiding deadlocks.
If you can think of a general-purpose algorithm that does the job in practice as well as in theory, write it up and send it to your local computer science journal.
Although both avoidance and prevention are not terribly promising in the general case, for specific applications, many excellent special-purpose algorithms are known.
As an example, in many database systems, an operation that occurs frequently is requesting locks on several records and then updating all the locked records.
When multiple processes are running at the same time, there is a real danger of deadlock.
In the first phase, the process tries to lock all the records it needs, one at a time.
If it succeeds, it begins the second phase, performing its updates and releasing the locks.
If during the first phase, some record is needed that is already locked, the process just releases all its locks and starts the first phase all over.
In a certain sense, this approach is similar to requesting all the resources needed in advance, or at least before anything irreversible is done.
In some versions of two-phase locking, there is no release and restart if a lock is encountered during the first phase.
In real-time systems and process control systems, for example, it is not acceptable to just terminate a process partway through because a resource is not available and start all over again.
Neither is it acceptable to start over if the process has read or written messages to the network, updated files, or anything else that cannot be safely repeated.
The algorithm works only in those situations where the programmer has very carefully arranged things so that the program can be stopped at any point during the first phase and restarted.
The top four layers of that figure correspond to the four-layered structure of MINIX 3 shown in Fig.
In the following sections we will look briefly at each of the layers, with an emphasis on the device drivers.
Many device drivers start some I/O device and then block, waiting for a message to arrive.
That message is usually generated by the interrupt handler for the device.
Other device drivers do not start any physical I/O (e.g., reading from RAM disk and writing to a memory-mapped display), do not use interrupts, and do not wait for a message from an I/O device.
In the previous chapter the mechanisms in the kernel by which interrupts generate messages and cause task switches has been presented in great detail, and we will say no more about it here.
Here we will discuss in a general way interrupts and I/O in device drivers.
We will return to the details when we look at the code for various devices.
For disk devices, input and output is generally a matter of commanding a device to perform its operation, and then waiting until the operation is complete.
The disk controller does most of the work, and very little is required of the interrupt handler.
Life would be simple if all interrupts could be handled so easily.
However, there is sometimes more for the low-level handler to do.
When an interrupt may occur frequently but the amount of I/O handled per interrupt is small, it may pay to make the handler itself do somewhat more work and to postpone sending a message to the driver until a subsequent interrupt, when there is more for the driver to do.
In MINIX 3 this is not possible for most I/O, because the low level handler in the kernel is a general purpose routine used for almost all devices.
The clock interrupt handler is unique in MINIX 3, because the clock is the only interrupt driven device that runs in kernel space.
The clock hardware is integral to the PC—in fact, the clock interrupt line does not connect to any pin on the sockets where add-on I/O controllers can be plugged in—so it is impossible to install a clock upgrade package with replacement clock hardware and a driver provided by the manufacturer.
It is reasonable, then, for the clock driver to be compiled into the kernel and have access to any variable in kernel space.
But a key design goal of MINIX 3 is to make it unnecessary for any other device driver to have that kind of access.
Device drivers that run in user space cannot directly access kernel memory or I/O ports.
Although possible, it would also violate the design principles of MINIX 3 to allow an interrupt service routine to make a far call to execute a service routine within the text segment of a user process.
This would be even more dangerous than letting a user space process call a function within kernel space.
In that case we would at least be sure the function was written by a competent, securityaware operating system designer, possibly one who had read this book.
But the kernel should not trust code provided by a user program.
There are several different levels of I/O access that might be needed by a user-space device driver.
A driver might need access to memory outside its normal data space.
The memory driver, which manages the RAM disk, is an example of a driver which needs only this kind of access.
A driver may need to read and write to I/O ports.
The machine-level instructions for these operations are available only in kernel mode.
As we will soon see, the hard disk driver needs this kind of access.
For example, the hard disk driver writes commands to the disk controller, which causes an interrupt to occur when the desired operation is complete.
This could be considered a subclass of the preceding item, but unpredictability complicates things.
All of these cases are supported by kernel calls handled by the system task.
The first case, access to extra memory segments, takes advantage of the hardware segmentation support provided by Intel processors.
Although a normal process has access only to its own text, data, and stack segments, the system task allows other segments to be defined and accessed by user-space processes.
Thus the memory driver can access a memory region reserved for use as a RAM disk, as well as other regions designated for special access.
The console driver accesses memory on a video display adapter in the same way.
For the second case, MINIX 3 provides kernel calls to use I/O instructions.
The system task does the actual I/O on behalf of a less-privileged process.
Later in this chapter we will see how the hard disk driver uses this service.
The disk driver may have to write to a single output port to select a disk, then read from another port in order to verify the device is ready.
If response is normally expected to be very quick, polling can be done.
There are kernel calls to specify a port and data to be written or a location for receipt of data read.
This requires that a call to read a port be nonblocking, and in fact, kernel calls do not block.
A polling loop could include a counter that terminates the loop if the device does not become ready after a certain number of iterations.
This is not a good idea in general because the loop execution time will depend upon the CPU speed.
One way around this is to start the counter with a value that is related to CPU time, possibly using a global variable initialized when the system starts.
A better way is provided by the MINIX 3 system library, which provides a getuptime function.
This uses a kernel call to retrieve a counter of clock ticks since system startup maintained by the clock task.
The cost of using this information to keep track of time spent in a loop is the overhead of an additional kernel call on each iteration.
Another possibility is to ask the system task to set a watchdog timer.
But to receive a notification from a timer a receive operation, which will block, is required.
This is not a good solution if a fast response is expected.
The hard disk also makes use of variants of the kernel calls for I/O that make it possible to send a list of ports and data to write or variables to be altered to the system task.
This is very useful—the hard disk driver we will examine requires writing a sequence of byte values to seven output ports to initiate an operation.
The last byte in the sequence is a command, and the disk controller generates an.
All this can be accomplished with a single kernel call, greatly reducing the number of messages needed.
Although in this case an interrupt is expected, it is prudent to hedge against the possibility that something might go wrong sometime.
To prepare for the possibility that the interrupt might fail to be triggered, a process can request the system task to set up a watchdog timer.
Watchdog timers also generate notification messages, and thus the receive operation could get a notification either because an interrupt occurred or because a timer expired.
This is not a problem because, although a notification does not convey much information, the notification message indicates its origin.
Although both notifications are generated by the system task, notification of an interrupt will appear to come from HARDWARE, and notification of a timer expiring will appear to come from CLOCK.
If an interrupt is received in a timely way and a watchdog timer has been set, expiration of the timer at some future time will be detected by another receive operation, possibly in the main loop of the driver.
One solution is to make a kernel call to disable the timer when the notification from HARDWARE is received.
Alternatively, if it is likely that the next receive operation will be one where a message from CLOCK is not expected, such a message could be ignored and receive called again.
Although less likely, it is conceivable that a disk operation could occur after an unexpectedly long delay, generating the interrupt only after the watchdog has timed out.
When a timeout occurs a kernel call can be made to disable an interrupt, or a receive operation that does not expect an interrupt could ignore any message from HARDWARE.
This is a good time to mention that when an interrupt is first enabled, a kernel call can be made to set a ‘‘policy’’ for the interrupt.
The policy is simply a flag that determines whether the interrupt should be automatically reenabled or whether it should remain disabled until the device driver it serves makes a kernel call to reenable it.
For the disk driver there may be a substantial amount of work to be done after an interrupt, and thus it may be best to leave the interrupt disabled until all data has been copied.
The fourth item in our list is the most problematic.
Keyboard support is part of the tty driver, which provides output as well as input.
So input may come from a local keyboard, but it can.
And several processes may be running, each producing output for a different local or remote terminal.
When you do not know when, if ever, an interrupt might occur, you cannot just make a blocking receive call to accept input from a single source if the same process may need to respond to other input and output sources.
The principal technique used by the terminal driver for dealing with keyboard input is to make the interrupt response as fast as possible, so characters will not be lost.
The minimum possible amount of work is done to get characters from the keyboard hardware to a buffer.
Additionally, when data has been fetched from the keyboard in response to an interrupt, as soon as the data is buffered the keyboard is read again before returning from the interrupt.
Interrupts generate notification messages, which do not block the sender; this helps to prevent loss of input.
A nonblocking receive operation is available, too, although it is only used to handle messages during a system crash.
Watchdog timers are also used to activate the routine that checks the keyboard.
For each class of I/O device present in a MINIX 3 system, a separate I/O device driver is present.
These drivers are full-fledged processes, each one with its own state, registers, stack, and so on.
Device drivers communicate with the file system using the standard message passing mechanism used by all MINIX 3 processes.
A simple device driver may be written as a single source file.
For the RAM disk, hard disk, and floppy disk there is a source file to support each type of device, as well as a set of common routines in driver.c and drvlib.c to support all blcok device types.
Although some common source code is used, the driver for each disk type runs as a separate process, in order to support rapid data transfers and isolate drivers from each other.
In this case, however, a single process supports all of the different device types.
For groups of devices such as disk devices and terminals, for which there are several source files, there are also header files.
The MINIX 3 design principle of running components of the operating system as completely separate processes in user space is highly modular and moderately efficient.
It is also one of the few places where MINIX 3 differs from UNIX in an essential way.
In MINIX 3 a process reads a file by sending a message to the file.
The file system, in turn, may send a message to the disk driver asking it to read the needed block.
The disk driver uses kernel calls to ask the system task to do the actual I/O and to copy data between processes.
This sequence (slightly simplified from reality) is shown in Fig.
By making these interactions via the message mechanism, we force various parts of the system to interface in standard ways with other parts.
The file system calls the device driver as a procedure.
In UNIX all processes have two parts: a user-space part and a kernel-space part, as shown in Fig.
When a system call is made, the operating system switches from the user-space part to the kernel-space part in a somewhat magical way.
This structure is a remnant of the MULTICS design, in which the switch was just an ordinary procedure call, rather than a trap followed by saving the state of the user-part, as it is in UNIX.
Device drivers in UNIX are simply kernel procedures that are called by the kernel-space part of the process.
When a driver needs to wait for an interrupt, it calls a kernel procedure that puts it to sleep until some interrupt handler wakes it up.
Note that it is the user process itself that is being put to sleep here, because the kernel and user parts are really different parts of the same process.
Among operating system designers, arguments about the merits of monolithic systems, as in UNIX, versus process-structured systems, as in MINIX 3, are endless.
The MINIX 3 approach is better structured (more modular), has cleaner interfaces between the pieces, and extends easily to distributed systems in which the.
The UNIX approach is more efficient, because procedure calls are much faster than sending messages.
The performance loss due to having most of the operating system run in user space is typically in the range of 5–10%
Be warned that some operating system designers do not share the belief that it is worth sacrificing a little speed for a more modular and more reliable system.
In this chapter, drivers for RAM disk, hard disk, clock, and terminal are discussed.
The standard MINIX 3 configuration also includes drivers for the floppy disk and the printer, which are not discussed in detail.
These may be compiled separately and started on the fly at any time.
All of these drivers interface with other parts of the MINIX 3 system in the same way: request messages are sent to the drivers.
The messages contain a variety of fields used to hold the operation code (e.g., READ or WRITE) and its parameters.
A driver attempts to fulfill a request and returns a reply message.
For block devices, the fields of the request and reply messages are shown in Fig.
The request message includes the address of a buffer area containing data to be transmitted or in which received data are expected.
The reply includes status information so the requesting process can verify that its request was properly carried out.
The fields for the character devices are basically similar but can vary slightly from driver to driver.
Messages to the terminal driver can contain the address of a data structure which specifies all of the many configurable aspects of a terminal, such as the characters to use for the intraline editing functions erase-character and kill-line.
The function of each driver is to accept requests from other processes, normally the file system, and carry them out.
All the block device drivers have been written to get a message, carry it out, and send a reply.
Among other things, this decision means that these drivers are strictly sequential and do not contain any internal multiprogramming, to keep them simple.
When a hardware request has been issued, the driver does a receive operation specifying that it is interested only in accepting interrupt messages, not new requests for work.
Any new request messages are just kept waiting until the current work has been done (rendezvous principle)
The terminal driver is slightly different, since a single driver services several devices.
Thus, it is possible to accept a new request for input from the keyboard while a request to read from a serial line is still being fulfilled.
Nevertheless, for each device a request must be completed before beginning a new one.
The main program for each block device driver is structurally the same and is outlined in Fig.
When the system first comes up, each one of the drivers is started up in turn to give each a chance to initialize internal tables and similar.
Fields of the messages sent by the file system to the block device drivers and fields of the replies sent back.
Then each device driver blocks by trying to get a message.
When a message comes in, the identity of the caller is saved, and a procedure is called to carry out the work, with a different procedure invoked for each operation available.
After the work has been finished, a reply is sent back to the caller, and the driver then goes back to the top of the loop to wait for the next request.
In MINIX 3 the file system process contains all the device-independent I/O code.
The I/O system is so closely related to the file system that they were merged into one process.
The functions performed by the file system are those shown in Fig.
They could, however, easily be added to the relevant device drivers should the need arise in the future.
In addition to handling the interface with the drivers, buffering, and block allocation, the file system also handles protection and the management of i-nodes, directories, and mounted file systems.
The general model outlined earlier in this chapter also applies here.
Library procedures are available for making system calls and for all the C functions required by the POSIX standard, such as the formatted input and output functions printf and scanf.
The standard MINIX 3 configuration contains one spooler daemon, lpd, which spools and prints files passed to it by the lp command.
The standard MINIX 3 software distribution also provides a number of daemons that support various network functions.
The MINIX 3 configuration described in this book supports most network operations, all that is needed is to enable the network server and drivers for ethernet adapters at startup time.
Recompiling the terminal driver with pseudo terminals and serial line support will add support for logins from remote terminals and networking over serial lines (including modems)
The network server runs at the same priority as the memory manager and the file system, and like them, it runs as a user process.
True to its heritage, MINIX 3 follows the same path as UNIX with respect to deadlocks of the types described earlier in this chapter: it just ignores the problem.
Normally, MINIX 3 does not contain any dedicated I/O devices, although if.
In short, the only place deadlocks can occur are with the implicit shared resources, such as process table slots, i-node table slots, and so on.
None of the known deadlock algorithms can deal with resources like these that are not requested explicitly.
Accepting the risk that user processes could deadlock is one thing, but within the operating system itself a few places do exist where considerable care has been taken to avoid problems.
For instance, user processes are only allowed to use the sendrec messaging method, so a user process should never lock up because it did a receive when there was no process with an interest in sending to it.
Servers only use send or sendrec to communicate with device drivers, and device drivers only use send or sendrec to communicate with the system task in the kernel layer.
In the rare case where servers must communicate between themselves, such as exchanges between the process manager and the file system as they initialize their parts of the process table, the order of communication is very carefully designed to avoid deadlock.
Also, at the very lowest level of the message passing system there is a check to make sure that when a process is about to do a send that the destination process is not trying to the same thing.
Notify is nonblocking, and notifications are stored when a recipient is not immediately available.
As we examine the implementation of MINIX 3 device drivers in this chapter we will see that notify is used extensively.
It is possible to lock devices and files even without operating system support.
A file name can serve as a truly global variable, whose presence or absence can be noted by all other processes.
A special directory, /usr/spool/locks/, is usually present on MINIX 3 systems, as on most UNIX-like systems, where processes can create lock files, to mark any resources they are using.
The MINIX 3 file system also supports POSIX-style advisory file locking.
They depend upon the good behavior of processes, and there is nothing to prevent a program from trying to use a resource that is locked by another process.
This is not exactly the same thing as preemption of the resource, because it does not prevent the first process from attempting to continue its use of the resource.
The result of such an action by an ill-behaved process is likely to be a mess, but no deadlock results.
Then we will discuss the RAM disk, the hard disk, and the floppy disk.
The RAM disk is a good example to study because it has all the properties of block devices in general except the actual I/O—because the ‘‘disk’’ is actually just a portion of memory.
The hard disk shows what a real disk driver looks like.
One might expect the floppy disk to be easier to support than the hard disk, but, in fact, it is not.
We will not discuss all the details of the floppy disk, but we will point out several of the complications to be found in the floppy disk driver.
Looking ahead, after the discussion of block drivers, we will discuss the terminal (keyboard + display) driver, which is important on all systems, and, furthermore is a good example of a character device driver.
Each of these sections describes the relevant hardware, the software principles behind the driver, an overview of the implementation, and the code itself.
This structure may make the sections useful reading even for readers who are not interested in the details of the code itself.
We mentioned earlier that the main procedures of all I/O drivers have a similar structure.
Usually, there are three block devices—both the floppy disk driver and an IDE (Integrated Drive Electronics) hard disk driver are present.
The driver for each block device driver is compiled independently, but a common library of source code is shared by all of them.
In older versions of MINIX a separate CD-ROM driver was sometimes present, and could be added if necessary.
They used to be necessary to support the proprietary interfaces of different drive manufacturers, but modern CD-ROM drives are usually connected to the IDE controller, although on notebook computers some CD-ROMs are USB.
The full version of the MINIX 3 hard disk device driver includes CD-ROM support, but we have taken the CD-ROM support out of the driver as described in this text and listed in Appendix B.
Each block device driver has to do some initialization, of course.
The RAM disk driver has to reserve some memory, the hard disk driver has to determine the parameters of the hard disk hardware, and so on.
All of the disk drivers are called individually for hardware-specific initialization.
After doing whatever may be necessary, each driver then calls the function containing its main loop.
This loop is executed forever; there is no return to the caller.
Within the main loop a message is received, a function to perform the operation needed by each message is called, and then a reply message is generated.
The technique used is to have each driver pass to the main loop a parameter consisting of a pointer to a table of the addresses of the functions that driver will use for each operation and then to call these functions indirectly.
If the drivers were compiled together in a single executable file only one copy of the main loop would be needed.
This code was, in fact, first written for an earlier version of MINIX in which all the drivers were compiled together.
The emphasis in MINIX 3 is on making individual operating system components as independent as possible, but using common source code for separate programs is still a good way to increase reliability.
Assuming you get it right once, it will be right for all the drivers.
Or, a bug found in one use might very well exist unnoticed in other uses.
Figure 3-19 shows an outline of the main loop, in a form similar to that of Fig.
Many of these operations are most likely familiar to readers with programming experience.
At the device driver level most operations are related to system calls with the same name.
For instance, the meanings of READ and WRITE should be fairly clear.
For each of these operations, a block of data is transferred from the device to the memory of the process that initiated the call, or vice versa.
That is fine as far as the caller is concerned; it is then free to reuse the buffer from which the operating system has copied the data to write.
Many I/O devices have operational parameters which occasionally must be examined and perhaps changed.
A familiar example is changing the speed of transmission or the parity of a communications line.
Examining or changing the way a disk device is partitioned is done using an IOCTL operation in MINIX 3 (although it could just as well have been done by reading and writing a block of data)
In such a request not all the transfers requested are necessarily honored by the device driver.
The request for each block may be modified by a flag bit that tells the device driver that the request is optional.
In effect the file system can say: ‘‘It would be nice to have all these data, but I do not really need them all right now.’’ The device can do what is best for it.
The floppy disk driver, for instance, will return all the data blocks it can read from a single track, effectively saying, ‘‘I will give you these, but it takes too long to move to another track; ask me again later for the rest.’’
This format was chosen so no conversions are necessary when working with memorybased devices, maximizing speed of response.
With real disks there are so many other factors delaying access that converting to sectors is not a significant inconvenience.
After doing whatever is requested in the message, some sort of cleanup may be necessary, depending upon the nature of the device.
For a floppy disk, for instance, this might involve starting a timer to turn off the disk drive motor if another request does not arrive soon.
It is possible for a routine that services one of the message types to return a EDONTREPLY value to suppress the reply message, but none of the current drivers use this option.
A bigger counter is needed because DMA uses absolute addresses, not addresses relative to a segment register.
Newer computers of the IBM PC family have better DMA controllers, and this code could be simplified, and a small amount of memory reclaimed, if one could be sure that one’s machine were immune to this problem.
If you are considering this, however, consider how the bug will manifest itself if you are wrong.
Most likely, when the failure occurs next month or next year, it will be attributed to the code that was last modified.
Unexpected hardware ‘‘features’’ like this can cause weeks of time spent looking for exceedingly obscure bugs (all the more so when, like this one, the technical reference manual says nary a word about them)
The files drvlib.h and drvlib.c contain system-dependent code that supports disk partitions on IBM PC compatible computers.
Partitioning allows a single storage device to be divided up into subdevices.
It is most commonly used with hard disks, but MINIX 3 provides support for partitioning floppy disks, as well.
If two or more operating systems with different file systems are used, it is more economical to partition a single large disk than to install multiple smaller disks for each operating system.
Operating systems may have limits to the device size they can handle.
Two or more different file systems may be used by an operating system.
For example, a standard file system may be used for ordinary files and a differently structured file system may be used for virtual memory swap space.
It may be convenient to put a portion of a system’s files on a separate logical device.
Putting the MINIX 3 root file system on a small device makes it easy to back up and facilitates copying it to a RAM disk at boot time.
But if more than one operating system is to run on a particular set of hardware, all must agree on a format for the partition table.
On IBM PCs the standard is set by the MS-DOS fdisk command, and other OSs, such as MINIX 3, Windows, and Linux, use this format so they can coexist with MS-DOS.
When MINIX 3 is ported to another machine type, it makes sense to use a partition table format compatible with other operating systems used on the new hardware.
Thus the MINIX 3 source code to support partitions on IBM computers is put in drvlib.c, rather than being included in driver.c, for two reasons.
As noted earlier, the memory driver links to driver.o but does not use the functions compiled into drvlib.o.
Second, this makes it easier to port MINIX 3 to different hardware.
It is easier to replace one small file than to edit a large one with many sections to be conditionally compiled for different environments.
Most of this information is not needed by MINIX 3 once the file system is verified.
Still within the loop, for all partitions on the device, if the partition is identified as a MINIX 3 partition, partition is called recursively to gather subpartition information.
If a partition is identified as an extended partition, the next function, extpartition, is called instead.
These use linked lists rather than fixed-size arrays to support subpartitions.
For simplicity MINIX 3 uses the same mechanism for subpartitions as for primary partitions.
However, minimal support for extended partitions is provided to support MINIX 3 commands to read and write files and directories of other operating systems.
These operations are easy; providing full support for mounting and otherwise using extended partitions in the same way as primary partitions would be much more complicated.
If a table is found, it copies it to the table address that was passed as an argument.
Finally, sort (line 11582) sorts the entries in a partition table by lowest sector.
Entries that are marked as having no partition are excluded from the sort, so they come at the end, even though they may have a zero value in their low sector field.
The sort is a simple bubble sort; there is no need to use a fancy algorithm to sort a list of four items.
Now we will get back to the individual block device drivers and study several of them in detail.
The first one we will look at is the memory driver.
It can be used to provide access to any part of memory.
Its primary use is to allow a part of memory to be reserved for use like an ordinary disk, and we will also refer to it as the RAM disk driver.
A RAM disk does not provide permanent storage, but once files have been copied to this area they can be accessed extremely quickly.
A RAM disk is also useful for initial installation of an operating system on a computer with only one removable storage device, whether a floppy disk, CDROM, or some other device.
By putting the root device on the RAM disk, removable storage devices can be mounted and unmounted as needed to transfer data to the hard disk.
Putting the root device on a floppy disk would make it impossible to save files on floppies, since the root device (the only floppy) cannot be unmounted.
Having the root device on the RAM disk makes the system highly flexible: any combination of floppy disks or hard disks can be mounted on it.
As we shall see, the memory driver supports several other functions in addition to a RAM disk.
It supports straightforward random access to any part of memory, byte by byte or in chunks of any size.
Used this way it acts as a character device rather than as a block device.
Other character devices supported by the memory driver are /dev/zero, and /dev/null, otherwise known as the great bit bucket in the sky.
A block device is a storage medium with two commands: write a block and read a block.
Normally, these blocks are stored on rotating memories, such as floppy disks or hard disks.
It just uses a preallocated portion of main memory for storing the blocks.
A RAM disk has the advantage of having instant access (no seek or rotational delay), making it suitable for storing programs or data that are frequently accessed.
As an aside, it is worth briefly pointing out a difference between systems that support mounted file systems and those that do not (e.g., MS-DOS and Windows)
With mounted file systems, the root device is always present and in a fixed location, and removable file systems (i.e., disks) can be mounted in the file tree to form an integrated file system.
Once everything has been mounted, the user need not worry at all about which device a file is on.
With only one or two floppy disks, this burden is manageable, but on a large computer system, with dozens of disks, having to keep track of devices all the time would be unbearable.
Remember that UNIX-like operating systems run on hardware ranging from small home and office machines to supercomputers such as the IBM Blue Gene/L supercomputer, the world’s fastest computer as of this writing; MS-DOS runs only on small systems.
Read and writes of RAM block 0 use this memory.
A RAM disk driver may support several areas of memory used as RAM disk, each distinguished by a different minor device number.
Usually, these areas are distinct, but in some fairly specific situations it may be convenient to have them overlap, as we shall see in the next section.
The MINIX 3 RAM disk driver is actually six closely related drivers in one.
Each message to it specifies a minor device as follows:
The first special file listed above, /dev/ram, is a true RAM disk.
Neither its size nor its origin is built into the driver.
They are determined by the file system when MINIX 3 is booted.
If the boot parameters specify that the root file system is to be on the RAM disk but the RAM disk size is not specified, a RAM disk of the same size as the root file system image device is created.
A boot parameter can be used to specify a RAM disk larger than the root file system, or if the root is not to be copied to the RAM, the specified size may be any value that fits in memory and leaves enough memory for system operation.
Once the size is known, a block of memory big enough is found and removed from the memory pool by the process manager during its initialization.
This strategy makes it possible to increase or reduce the amount of RAM disk present without having to recompile the operating system.
The next two minor devices are used to read and write physical memory and kernel memory, respectively.
When /dev/mem is opened and read, it yields the contents of physical memory locations starting at absolute address zero (the realmode interrupt vectors)
Ordinary user programs never do this, but a system program concerned with debugging the system might possibly need this facility.
Opening /dev/mem and writing on it will change the interrupt vectors.
Needless to say, this should only be done with the greatest of caution by an experienced user who knows exactly what he is doing.
It too is used mostly for debugging and very special programs.
Note that the RAM disk areas covered by these two minor devices overlap.
If you know exactly how the kernel is placed in memory, you can open /dev/mem, seek to the beginning of the kernel’s data area, and see exactly the same thing as reading from the beginning of /dev/kmem.
But, if you recompile the kernel, changing its size, or if in a subsequent version of MINIX 3 the kernel is moved somewhere else in memory, you will have to seek a different amount in /dev/mem to see the same thing you now see at the start of /dev/kmem.
Both of these special files should be protected to prevent everyone except the superuser from using them.
The next file in this group, /dev/null, is a special file that accepts data and throws them away.
It is commonly used in shell commands when the program being called generates output that is not needed.
The RAM disk driver effectively treats this minor device as having zero size, so no data are ever copied to or from it.
If you read from it you will get an immediate EOF (End of File)
If you have looked at the directory entries for these files in /dev/ you may have noticed that, of those mentioned so far, only /dev/ram is a block special file.
There is one more block device supported by the memory driver.
From the point of view of the device driver it is another block device implemented in RAM, just like /dev/ram.
However, it is meant to be initialized by copying a file appended to the boot image after init into memory, rather than starting with an empty block of memory, as is done for /dev/ram.
Support for this device is provided for future use and it is not used in MINIX 3 as described in this text.
Finally, the last device supported by the memory driver is another character special file, /dev/zero.
It is sometimes convenient to have a source of zeros.
Writing to /dev/zero is like writing to /dev/null; it throws data away.
But reading /dev/zero gives you zeros, in any quantity you want, whether a single character or a disk full.
As with other disk drivers, the main loop of the RAM disk driver is in the file driver.c.
The device-specific support for memory devices is in memory.c (line 10800)
The kinfo and machine structures that are defined next will hold data retrieved from the kernel during startup that is necessary for initializing the memory driver.
The main procedure main (line 11672) calls one function to do some local initialization.
After that, it calls the main loop, which gets messages, dispatches to the appropriate procedures, and sends the replies.
This is so the number of bytes transferred (although this number is zero for /dev/null) can be returned, as would be done for any write operation.
For that reason, we will now study them, starting with the hardware, then moving on to say some general things about disk software.
After that we will delve into the way MINIX 3 controls its disks.
All real disks are organized into cylinders, each one containing as many tracks as there are heads stacked vertically.
The simplest designs have the same number of sectors on each track.
All sectors contain the same number of bytes, although a little thought will make it clear that sectors close to the outer rim of the disk will be physically longer than those close to the hub.
The time to read or write each sector will be same, however.
The data density is obviously higher on the innermost cylinders, and some disk designs require a change in the drive current to the read-write heads for the inner tracks.
This is handled by the disk controller hardware and is not visible to the user (or the implementer of an operating system)
The difference in data density between inner and outer tracks means a sacrifice in capacity, and more sophisticated systems exist.
Floppy disk designs that rotate at higher speeds when the heads are over the outer tracks have been tried.
This allows more sectors on those tracks, increasing disk capacity.
Such disks are not supported by any system for which MINIX 3 is currently available, however.
Modern large hard drives also have more sectors per track on outer tracks than on inner tracks.
These are IDE (Integrated Drive Electronics) drives, and the sophisticated processing done by the drive’s built-in electronics masks the details.
To the operating system they appear to have a simple geometry with the same number of sectors on each track.
The drive and controller electronics are as important as the mechanical hardware.
The main element of the disk controller is a specialized integrated circuit, really a small microcomputer.
Once this would have been on a card plugged into the computer’s backplane, but on modern systems, the disk controller is on the parentboard.
For a modern hard disk this disk controller circuitry may be simpler than for a floppy disk, since a hard drive has a powerful electronic controller integrated into the drive itself.
A device feature that has important implications for the disk driver is the possibility of a controller doing seeks on two or more drives at the same time.
While the controller and software are waiting for a seek to complete on one drive, the controller can initiate a seek on another drive.
Many controllers can also read or write on one drive while seeking on one.
Reading or writing requires the controller to move bits on a microsecond time scale, so one transfer uses up most of its computing power.
The situation is different for hard disks with integrated controllers, and in a system with more than one of these hard drives they can operate simultaneously, at least to the extent of transferring between the disk and the controller’s buffer memory.
Only one transfer between the controller and the system memory is possible at once, however.
The ability to perform two or more operations at the same time can reduce the average access time considerably.
One thing to be aware of in looking at the specifications of modern hard disks is that the geometry specified, and used by the driver software, is almost always different from the physical format.
These numbers correspond to a disk size of 8 GB, but are used for all disks this size or larger.
So if you try to install a large hard drive into a very old computer you may find you can access only 8 GB, even though you have a much bigger disk.
The usual way around this limitation is to use logical block addressing in which disk sectors are just numbered consecutively starting at zero, without regard to the disk geometry.
The geometry of a modern disk is a fiction, anyway.
On a modern disk the surface is divided into 20 or more zones.
Zones closer to the center of the disk have fewer sectors per track than zones nearer the periphery.
Thus sectors have approximately the same physical length no matter where they are located on the disk, making more efficient use of the disk surface.
Internally, the integrated controller addresses the disk by calculating the zone, cylinder, head, and sector.
But this is never visible to the user, and the details are rarely found in published specifications.
The bottom line is, there is no point to using cylinder, head, sector addressing of a disk unless you are working with a very old computer that does not support logical block addressing.
This is a good place to mention a confusing point about disk capacity specifications.
However, in this text the authors, being set in their ways.
Although modern disks are much faster than older ones, improvements in CPU performance have far exceeded improvements in disk performance.
It has occurred to various people over the years that parallel disk I/O might be helpful.
Thus has come about a new class of I/O device called a RAID, an acronym for Redundant Array of Independent Disks.
Actually, the designers of RAID (at Berkeley) originally used the acronym RAID to stand for ‘‘Redundant Array of Inexpensive Disks’’ to contrast this design with a SLED (Single Large Expensive Disk)
However, when RAID became commercially popular, disk manufacturers changed the meaning of the acronym because it was tough to sell an expensive product whose name stood for ‘‘inexpensive.’’ The basic idea behind a RAID is to install a box full of disks next to the computer, typically a large server, replace the disk controller card with a RAID controller, copy the data over to the RAID, and then continue normal operation.
The independent disks can be used together in a variety of ways.
We do not have space for an exhaustive description of all of these, and MINIX 3 does not (yet) support RAID, but an introduction to operating systems should at least mention some of the possibilities.
For example, consider a very simple RAID of two drives.
The controller divides up the data and the two disks are written simultaneously, doubling the writing speed.
When reading, both drives are read simultaneously, but the controller reassembles the data in the proper order, and to the rest of the system it just looks like the reading speed is twice as fast.
This works best when data are usually read or written in large blocks.
Obviously, nothing is gained if a typical disk request is for a single sector at a time.
The previous example shows how multiple drives can increase speed.
Again, a very simple array of two drives could be used, and all of the data could be written to both of them.
If an error is detected during reading there is no need for a retry if the other drive reads the data correctly.
The controller just has to make sure the correct data is passed on to the system.
It probably would not be a good idea to skip retries if errors are detected while writing, however.
And if errors occur frequently enough that skipping retries actually makes reading noticeably faster it is.
Typically the drives used for RAIDs are hot-swappable, meaning they can be replaced without powering down the system.
More complex arrays of multiple disks can increase both speed and reliability.
Bytes could be split into 4-bit nybbles, with each bit being recorded on one of four drives and with the other three drives being used to record a three bit error-correcting code.
If a drive goes bad and needs to be hot-swapped for a new one, a missing drive is equivalent to one bad bit, so the system can keep running while maintenance is done.
For the cost of seven drives you get reliable performance that is four times as fast as one drive, and no downtime.
In this section we will look at some issues related to disk drivers in general.
First, consider how long it takes to read or write a disk block.
The seek time (the time to move the arm to the proper cylinder)
For most disks, the seek time dominates the other two times, so reducing the mean seek time can improve system performance substantially.
Some kind of error check, a checksum or a cyclic redundancy check, is always recorded along with the data in each sector on a disk.
Even the sector addresses recorded when the disk is formatted have check data.
Floppy disk controller hardware can usually report when an error is detected, but the software must then decide what to do about it.
Hard disk controllers often take on much of this burden.
Particularly with hard disks, the transfer time for consecutive sectors within a track can be very fast.
Thus reading more data than requested and caching it in memory can be very effective in speeding disk access.
If the disk driver accepts requests one at a time and carries them out in that order, that is, First-Come, First-Served (FCFS), little can be done to optimize seek time.
However, another strategy is possible when the disk is heavily loaded.
It is likely that while the arm is seeking on behalf of one request, other disk requests.
Many disk drivers maintain a table, indexed by cylinder number, with all pending requests for each cylinder chained together in a linked list headed by the table entries.
Given this kind of data structure, we can improve upon the first-come, firstserved scheduling algorithm.
They are entered into the table of pending requests, with a separate linked list for each cylinder.
When the current request (for cylinder 11) is finished, the disk driver has a.
Alternatively, it could always handle the closest request next, to minimize seek time.
This algorithm, Shortest Seek First (SSF), cuts the total arm motion almost in half compared to FCFS.
Suppose that more requests keep coming in while the requests of Fig.
With a heavily loaded disk, the arm will tend to stay in the middle of the disk most of the time, so requests at either extreme will have to wait until a statistical fluctuation in the load causes there to be no requests near the middle.
The goals of minimal response time and fairness are in conflict here.
The problem of scheduling an elevator in a tall building is similar to that of scheduling a disk arm.
Requests come in continuously calling the elevator to floors (cylinders) at random.
The microprocessor running the elevator could easily keep track of the sequence in which customers pushed the call button and service them using FCFS.
However, most elevators use a different algorithm to reconcile the conflicting goals of efficiency and fairness.
They keep moving in the same direction until there are no more outstanding requests in that direction, then they switch directions.
This algorithm, known both in the disk world and the elevator world as the elevator algorithm, requires the software to maintain 1 bit: the current direction bit, UP or DOWN.
When a request finishes, the disk or elevator driver checks the bit.
If it is UP, the arm or cabin is moved to the next highest pending request.
If no requests are pending at higher positions, the direction bit is reversed.
When the bit is set to DOWN, the move is to the next lowest requested position, if any.
Figure 3-22 shows the elevator algorithm using the same seven requests as Fig.
In this case the elevator algorithm is slightly better than SSF, although it is usually worse.
One nice property that the elevator algorithm has is that given any collection of requests, the upper bound on the total motion is fixed: it is just twice the number of cylinders.
A slight modification of this algorithm that has a smaller variance in response times is to always scan in the same direction (Teory, 1972)
When the highest numbered cylinder with a pending request has been serviced, the arm goes to the lowest-numbered cylinder with a pending request and then continues moving in an upward direction.
In effect, the lowest-numbered cylinder is thought of as being just above the highest-numbered cylinder.
Some disk controllers provide a way for the software to inspect the current sector number under the head.
If two or more requests for the same cylinder are pending, the driver can.
Note that when multiple tracks are present in a cylinder, consecutive requests can be for different tracks with no penalty.
The controller can select any of its heads instantaneously, because head selection involves neither arm motion nor rotational delay.
With a modern hard disk, the data transfer rate is so much faster than that of a floppy disk that some kind of automatic caching is necessary.
Typically any request to read a sector will cause that sector and up to the rest of the current track to be read, depending upon how much space is available in the controller’s cache memory.
When several drives are present, a pending request table should be kept for each drive separately.
Whenever any drive is idle, a seek should be issued to move its arm to the cylinder where it will be needed next (assuming the controller allows overlapped seeks)
When the current transfer finishes, a check can be made to see if any drives are positioned on the correct cylinder.
If one or more are, the next transfer can be started on a drive that is already on the right cylinder.
If none of the arms is in the right place, the driver should issue a new seek on the drive that just completed a transfer and wait until the next interrupt to see which arm gets to its destination first.
Another area in which RAM disks are simpler than real disks is error handling.
Transient checksum error (e.g., caused by dust on the head)
It is up to the disk driver to handle each of these as best it can.
Programming errors occur when the driver tells the controller to seek to a.
Most controllers check the parameters given to them and complain if they are invalid.
In theory, these errors should never occur, but what should the driver do if the controller indicates that one has happened? For a home-grown system, the best thing to do is stop and print a message like ‘‘Call the programmer’’ so the error can be tracked down and fixed.
Probably the only thing to do is terminate the current disk request with an error and hope it will not recur too often.
Transient checksum errors are caused by specks of dust in the air that get between the head and the disk surface.
Most of the time they can be eliminated by just repeating the operation a few times.
If the error persists, the block has to be marked as a bad block and avoided.
One way to avoid bad blocks is to write a very special program that takes a list of bad blocks as input and carefully hand crafts a file containing all the bad blocks.
Once this file has been made, the disk allocator will think these blocks are occupied and never allocate them.
As long as no one ever tries to read the bad block file, no problems will occur.
Not reading the bad block file is easier said than done.
Many disks are backed up by copying their contents a track at a time to a backup tape or disk drive.
If this procedure is followed, the bad blocks will cause trouble.
Backing up the disk one file at a time is slower but will solve the problem, provided that the backup program knows the name of the bad block file and refrains from copying it.
Another problem that cannot be solved with a bad block file is the problem of a bad block in a file system data structure that must be in a fixed location.
Almost every file system has at least one data structure whose location is fixed, so it can be found easily.
On a partitioned file system it may be possible to repartition and work around a bad track, but a permanent error in the first few sectors of either a floppy or hard disk generally means the disk is unusable.
Intelligent’’ controllers reserve a few tracks not normally available to user programs.
When a disk drive is formatted, the controller determines which blocks are bad and automatically substitutes one of the spare tracks for the bad one.
The table that maps bad tracks to spare tracks is kept in the controller’s internal memory and on the disk.
The technology of manufacturing disk recording surfaces is better than it used to be, but it is still not perfect.
However, the technology of hiding the imperfections from the user has also improved.
Many controllers also manage new errors that may develop with use, permanently assigning substitute blocks when they determine that an error is unrecoverable.
With such disks the driver software rarely sees any indication that there any bad blocks.
Seek errors are caused by mechanical problems in the arm.
To perform a seek, it issues a series of pulses to the arm motor, one pulse per cylinder, to move the arm to the new cylinder.
When the arm gets to its destination, the controller reads the actual cylinder number (written when the drive was formatted)
If the arm is in the wrong place, a seek error has occurred and some corrective action is required.
Most hard disk controllers correct seek errors automatically, but many floppy controllers (including the IBM PCs) just set an error bit and leave the rest to the driver.
As we have seen, the controller is really a specialized little computer, complete with software, variables, buffers, and occasionally, bugs.
Sometimes an unusual sequence of events such as an interrupt on one drive occurring simultaneously with a recalibrate command for another drive will trigger a bug and cause the controller to go into a loop or lose track of what it was doing.
Controller designers usually plan for the worst and provide a pin on the chip which, when asserted, forces the controller to forget whatever it was doing and reset itself.
If all else fails, the disk driver can set a bit to invoke this signal and reset the controller.
If that does not help, all the driver can do is print a message and give up.
The time required to seek to a new cylinder is usually much more than the rotational delay, and always vastly more than the transfer time to read or write one sector.
In other words, once the driver has gone to the trouble of moving the arm somewhere, it hardly matters whether it reads one sector or a whole track.
This effect is especially true if the controller provides rotational sensing, so the driver can see which sector is currently under the head and issue a request for the next sector, thereby making it possible to read an entire disk track in a single rotation time.
Normally it takes half a rotation plus one sector time just to read a single sector, on the average.
Some disk drivers take advantage of these timing properties by maintaining a secret track-at-a-time cache, unknown to the device-independent software.
If a sector that is in the cache is needed, no disk transfer is required.
A disadvantage of track-at-a-time caching (in addition to the software complexity and buffer space needed) is that transfers from the cache to the calling program will have to be done by the CPU using a programmed loop, rather than letting the DMA hardware do the job.
Some controllers take this process a step further, and do track-at-a-time caching in their own internal memory, transparent to the driver, so that transfer between the controller and memory can use DMA.
If the controller works this way, there is little point in having the disk driver do it as well.
Note that both the controller and the driver are in a good position to read and write entire tracks in one command, but that the device-independent software cannot, because it regards a disk as a linear sequence of blocks, without regard to how they are divided up into tracks and cylinders.
The hard disk driver is the first part of MINIX 3 we have looked at that has to deal with a range of different types of hardware.
Before we discuss the driver, we will briefly consider some of the problems hardware differences can cause.
Not only are different processors used in different members of the family, there are also some major differences in the basic hardware.
A feature of the AT bus is that it was cleverly designed so older 8-bit peripherals could still be used.
Later systems added a 32bit PCI bus for peripherals, while still providing AT bus slots.
The newest designs have dropped AT-bus support, providing only a PCI bus.
For every bus there is a different family of I/O adapters.
On older systems these are separate circuit boards which plug into the system parentboard.
On newer systems many standard adapters, especially disk controllers, are integrated parts of the parentboard chipset.
In itself this is not a problem for the programmer, as integrated adapters usually have a software interface identical to that of removable devices.
This allows use of a more advanced add-on device, such as a SCSI controller, in place of a built-in device.
To take advantage of this flexibility the operating system should not be restricted to using just one kind of adapter.
In the IBM PC family, as in most other computer systems, each bus design also comes with firmware in the Basic I/O System Read-Only Memory (the BIOS ROM) which is designed to bridge the gap between the operating system and the peculiarities of the hardware.
Some peripheral devices may even provide extensions to the BIOS in ROM chips on the peripheral cards themselves.
The difficulty faced by an operating system implementer is that the BIOS in IBM-type computers (certainly the early ones) was designed for an operating system, MSDOS, that does not support multiprogramming and that runs in 16-bit real mode, the lowest common denominator of the various modes of operation available from the 80x86 family of CPUs.
The implementer of a new operating system for the IBM PC is thus faced with several choices.
One is whether to use the driver support for peripherals in the BIOS or to write new drivers from scratch.
This was not a hard choice in the design of early versions of MINIX, since the BIOS was in many ways not suitable to its needs.
Of course, to start MINIX 3 the boot monitor uses the BIOS to do the initial loading of the system, whether from hard disk, CD-ROM, or floppy diskthere is no practical alternative to doing it this way.
Once we have loaded the system, including our own I/O drivers, we can do better than the BIOS.
The second choice then must be faced: without the BIOS support how are we going to make our drivers adapt to the varied kinds of hardware on different systems? To make the discussion concrete, consider that there are two fundamentally different types of hard disk controller usable on the modern 32-bit Pentium systems for which MINIX 3 has been designed: the integrated IDE controller and add-on SCSI controllers for the PCI bus.
There are several possible ways to deal with all these alternatives:
Recompile a unique version of the operating system for each type of hard disk controller we need to accommodate.
Compile several different hard disk drivers into the boot image and have the system automatically determine at startup time which one to use.
Compile several different hard disk drivers into the boot image and provide a way for the user to determine which one to use.
The first way is really the best way in the long run.
For use on a particular installation there is no need to use up disk and memory space with code for alternative drivers that will never be used.
However, it is a nightmare for the distributor of the software.
Supplying four different startup disks and advising users on how to use them is expensive and difficult.
Thus, another method is advisable, at least for the initial installation.
The second method is to have the operating system probe the peripherals, by reading the ROM on each card or writing and reading I/O ports to identify each card.
This is possible (and works better on newer IBM-type systems than on older ones), but it does not accommodate nonstandard I/O devices.
Also, probing I/O ports to identify one device sometimes can activate another device which seizes control and disables the system.
This method complicates the startup code for each device, and yet still does not work very well.
The third method, used in MINIX 3, is to allow inclusion of several drivers in the boot image.
The MINIX 3 boot monitor allows various boot parameters to be read at startup time.
These can be entered by hand, or stored permanently on the disk.
At startup time, if a boot parameter of the form.
There are two other things MINIX 3 does to try to minimize problems with multiple hard disk drivers.
One is that there is, after all, a driver that interfaces between MINIX 3 and the ROM BIOS hard disk support.
This driver is almost guaranteed to work on any system and can be selected by use of a.
Switching out of protected mode and back again whenever a routine in the BIOS is called is very slow.
The other strategy MINIX 3 uses in dealing with drivers is to postpone initialization until the last possible moment.
Thus, if on some hardware configuration none of the hard disk drivers work, we can still start MINIX 3 from a floppy disk and do some useful work.
This may not seem like a major breakthrough in user friendliness, but consider this: if all the drivers try to initialize immediately on system startup, the system can be totally paralyzed by improper configuration of some device we do not need anyway.
By postponing initialization of each driver until it is needed, the system can continue with whatever does work, while the user tries to resolve the problems.
We learned this lesson the hard way: earlier versions of MINIX tried to initialize the hard disk as soon as the system was booted.
This behavior was especially unfortunate because MINIX would run quite happily on a system without a hard disk, albeit with restricted storage capacity and reduced performance.
In the discussion in this section and the next, we will take as our model the AT-style hard disk driver, which is the default driver in the standard MINIX 3 distribution.
This is a versatile driver that handles hard disk controllers from the ones used in the earliest 80286 systems to modern EIDE (Extended Integrated Drive Electronics) controllers that handle gigabyte capacity hard disks.
However, in order to simplify our discussion the extensions that support CD-ROMs have been taken out of the code listed in Appendix B.
The general aspects of hard disk operation we discuss in this section apply to the other supported drivers as well.
Requests for reading and writing are not mixed in a vector of requests, nor can requests be marked as optional.
The elements of a request vector are for contiguous disk sectors, and the vector is sorted by the file system before being passed to the device driver, so it suffices to specify just the starting position on the disk for an entire array of requests.
The driver is expected to succeed in reading or writing at least the first request in a request vector, and to return when a request fails.
It is up to the file system to decide what to do; the file system will try to complete a write operation but will return to the calling process only as much data as it can get on a read.
The file system itself, by using scattered I/O, can implement something similar to Teory’s version of the elevator algorithm—recall that in a scattered I/O request the list of requests is sorted on the block number.
The second step in scheduling takes place in the controller of a modern hard disk.
Such controllers are ‘‘smart’’ and can buffer large quantities of data, using internally programmed algorithms to retrieve data in the most efficient order, irrespective of the order of receipt of the requests.
Small hard disks used on microcomputers are sometimes called ‘‘winchester’’ disks.
The term was IBM’s code name for the project that developed the disk technology in which the read/write heads fly on a thin cushion of air and land on the recording medium when the disk stops spinning.
Supposedly this reminded the developers of the Winchester 30-30 firearm which figures in many tales of the United States’ western frontier.
Whatever the origin of the name, the basic technology remains the same, although.
On older systems with IDE controllers, the disk functions as if it were an ATstyle peripheral card, even though it may be integrated on the parentboard.
On line 12824 a test is made for whether this is enabled.
The test will always fail with the version of MINIX 3 described here.
This is good, because no code is provided to be executed if the test succeeds.
You will have to make changes in many places to handle the 48-bit addresses.
The registers used by a standard IBM-AT class hard disk controller are shown in Fig.
The numbers in parentheses are the bits of the logical block address selected by each register in LBA mode.
We have mentioned several times reading and writing to I/O ports, but we tacitly treated them just like memory addresses.
In fact, I/O ports often behave differently from memory addresses.
Thus, the data written to a particular address cannot necessarily be retrieved by a subsequent read operation.
It is also common that the very act of reading or writing an I/O device register causes an action to occur, independently of the details of the data transferred.
This is true of the command register on the AT disk controller.
In use, data are written to the lower-numbered registers to select the disk address to be read from or written to, and then the command register is written last with an operation code.
The data written to the command register determines what the operation will be.
The act of writing the operation code into the command register starts the operation.
It is also the case that the use of some registers or fields in the registers may vary with different modes of operation.
The floppy disk driver is longer and more complicated than the hard disk driver.
This may seem paradoxical, since floppy disk mechanisms are simpler than those of hard disks, but the simpler mechanism has a more primitive controller that requires more attention from the operating system.
Also, the fact that the medium is removable adds complications.
In this section we will describe some of the things an implementer must consider in dealing with floppy disks.
However, we will not go into the details of the MINIX 3 floppy disk driver code.
In fact, we have not listed the floppy disk driver in Appendix B.
The most important parts are similar to those for the hard disk.
One of the things we do not have to worry about with the floppy driver is the multiple types of controller to support that we had to deal with in the case of the hard disk driver.
Although the high-density floppy disks currently used were not supported in the design of the original IBM PC, the floppy disk controllers of all computers in the IBM PC family are supported by a single software driver.
The contrast with the hard disk situation is probably due to lack of motivation to increase floppy disk performance.
Floppy disks are rarely used as working storage during operation of a computer system; their speed and data capacity are too limited compared to those of hard disks.
Floppy disks at one time were important for distribution of new software and for backup, but as networks and largercapacity removable storage devices have become common, PCs rarely come standard with a floppy disk drives any more.
The floppy disk driver does not use the SSF or the elevator algorithm.
It is strictly sequential, accepting a request and carrying it out before even accepting another request.
In the original design of MINIX it was felt that, since MINIX was intended for use on personal computers, most of the time there would be only one process active.
Thus the chance of a disk request arriving while another was being carried out was small.
There would be little to gain from the considerable increase in software complexity that would be required for queueing requests.
Complexity is even less worthwhile now, since floppy disks are rarely used for anything but transferring data into or out of a system with a hard disk.
That said, the floppy driver, like any other block driver, can handle a request for scattered I/O.
However, in the case of the floppy driver the array of requests.
The simplicity of the floppy disk hardware is responsible for some of the complications in floppy disk driver software.
Cheap, slow, low-capacity floppy drives do not justify the sophisticated integrated controllers that are part of modern hard drives, so the driver software has to deal explicitly with aspects of disk operation that are hidden in the operation of a hard drive.
As an example of a complication caused by the simplicity of floppy drives, consider positioning the read/write head to a particular track during a SEEK operation.
No hard disk has ever required the driver software to explicitly call for a SEEK.
For a hard disk the cylinder, head, and sector geometry visible to the programmer often do not correspond to the physical geometry.
Typically there are multiple zones (groups of cylinders) with more sectors per track on outer zones than on inner ones.
Modern hard disks accept Logical Block Addressing (LBA), addressing by the absolute sector number on the disk, as an alternative to cylinder, head, and sector addressing.
Even if addressing is done by cylinder, head, and sector, any geometry that does not address nonexistent sectors may be used, since the integrated controller on the disk calculates where to move the read/write heads and does a seek operation when required.
For a floppy disk, however, explicit programming of SEEK operations is needed.
This makes it possible for the controller to advance them to a desired track position by stepping the heads a known number of times.
Similar operations are necessary for the hard drive, of course, but the controller handles them without detailed guidance from the device driver software.
Some hard disk controllers provide for removable media, for instance, on a CD-ROM drive, but the drive controller is generally able to handle any complications without support in the device driver software.
With a floppy disk, however, the built-in support is not there, and yet it is needed more.
Some of the most common uses for floppy disks—installing new software or backing up files—are likely to require switching of disks in and out of the drives.
It will cause grief if data intended for one diskette are written onto another.
The device driver should do what it can to prevent this.
This is not always possible, as not all floppy drive hardware allows determination of whether the drive door has been opened since the last access.
Another problem that can be caused by removable media is that a.
This can be solved if an open door can be detected, but since this is not always possible some provision must be made for a timeout and an error return if an operation on a floppy disk does not terminate in a reasonable time.
Removable media can be replaced with other media, and in the case of floppy disks there are many different possible formats.
Two possible solutions are possible for the problem this causes.
One way is to refer to each possible format as a distinct drive and provide multiple minor devices.
Some formats have more cylinders, and others have more sectors per track than other formats.
Determination of the format of a diskette is done by attempting to read the higher numbered sectors and tracks.
By a process of elimination the format can be determined.
Another possible problem is that a disk with bad sectors could be misidentified.
A utility program is available for testing disks; doing so automatically in the operating system would be too slow.
The final complication of the floppy disk driver is motor control.
Diskettes cannot be read or written unless they are revolving.
Hard disks are designed to run for thousands of hours on end without wearing out, but leaving the motors on all the time causes a floppy drive and diskette to wear out quickly.
If the motor is not already on when a drive is accessed, it is necessary to issue a command to start the drive and then to wait about a half second before attempting to read or write data.
Turning the motors on or off is slow, so MINIX 3 leaves a drive motor on for a few seconds after a drive is used.
If the drive is used again within this interval, the timer is extended for another few seconds.
If the drive is not used in this interval, the motor is turned off.
For decades, users have communicated with computers using devices consisting of a keyboard for user input and a display for computer output.
For many years, these were combined into free-standing devices called terminals, which.
Large mainframes used in the financial and travel industries sometimes still use these terminals, typically connected to the mainframe via a modem, especially when they are far from the mainframe.
However, with the emergence of the personal computer, the keyboard and display have become separate peripherals rather than a single device, but they are so closely interrelated that we will discuss them together here under the combined name of ‘‘terminal.’’
It is up to the terminal driver to hide all these differences, so that the device-independent part of the operating system and the user programs do not have to be rewritten for each kind of terminal.
In the following sections we will follow our now-standard approach of first discussing terminal hardware and software in general, and then discussing the MINIX 3 software.
From the operating system’s point of view, terminals can be divided into three broad categories based on how the operating system communicates with them as well as their actual hardware characteristics.
The first category consists of memory-mapped terminals, which consist of a keyboard and a display, both of which are hardwired to the computer.
This model is used in all personal computers for the keyboard and the monitor.
The second category consists of terminals that interface via a serial communication line using the RS-232 standard, most frequently over a modem.
This model is still used on some mainframes, but PCs also have serial line interfaces.
The third category consists of terminals that are connected to the computer via a network.
These are an integral part of the computers themselves, especially personal computers.
Also on the video RAM card is a chip called a video controller.
This chip pulls bytes out of the video RAM and generates the video signal used to drive the display.
Displays are usually one of two types: CRT monitors or flat panel displays.
A CRT monitor generates a beam of electrons that scans horizontally across the screen, painting lines on it.
The video controller signal modulates the intensity of the electron beam, determining whether a given pixel will be light or dark.
Color monitors have three beams, for red, green, and blue, which are modulated independently.
A flat panel display works very differently internally, but a CRT-compatible flat-panel display accepts the same synchronization and video signals as a CRT and uses these to control a liquid crystal element at each pixel position.
In fact, most fetch each character once per scan line to eliminate the need for buffering in the controller.
The 9-by-14 bit patterns for the characters are kept in a ROM used by the video controller.
The original IBM PC had several modes for the screen.
In the simplest one, it used a character-mapped display for the console.
The low-order character was the ASCII code for the character to be displayed.
The high-order character was the attribute byte, which was used to specify the color, reverse video, blinking, and so on.
With a memory-mapped display, the keyboard is completely decoupled from the screen.
It may be interfaced via a serial or parallel port.
On every key action the CPU is interrupted, and the keyboard driver extracts the character typed by reading an I/O port.
On a PC, the keyboard contains an embedded microprocessor which communicates through a specialized serial port with a controller chip on the main board.
An interrupt is generated whenever a key is struck and also when one is released.
Furthermore, all that the keyboard hardware provides is the key number, not the ASCII code.
When the A key is struck, the key code (30) is put in an I/O register.
It is up to the driver to determine whether it is lower case, upper case, CTRL-A, ALT-A, CTRL-ALT-A, or some other combination.
Since the driver can tell which keys have been depressed but not yet released (e.g., shift), it has enough.
Although this keyboard interface puts the full burden on the software, it is extremely flexible.
For example, user programs may be interested in whether a digit just typed came from the top row of keys or the numeric key pad on the side.
The other pins are for various control functions, most of which are not used.
A parity bit which provides rudimentary error detection may also be inserted preceding the stop bits, although this is commonly required only for communication with mainframe systems.
RS-232 terminals are commonly used to communicate with a remote computer using a modem and a telephone line.
An RS-232 terminal communicates with a computer over a communication line, one bit at a time.
UARTs are attached to the computer by plugging RS-232 interface cards into the bus as illustrated in Fig.
On modern computers the UART and RS-232 interface is frequently part of the parentboard chipset.
It may be possible disable the on-board UART to allow use of a modem interface card plugged into the bus or two of them may be able to coexist.
However, to the computer the UART looks the same whether the medium is a dedicated serial cable or a telephone line.
RS-232 terminals are gradually dying off, being replaced by PCs, but they are still encountered on older mainframe systems, especially in banking, airline reservation, and similar applications.
Terminal programs that allow a remote computer to simulate a terminal are still widely used, however.
To print a character, the terminal driver writes the character to the interface card, where it is buffered and then shifted out over the serial line one bit at a time by the UART.
As a result of this slow transmission rate, the driver generally outputs a character to the RS-232 card and blocks, waiting for the interrupt generated by the interface when the character has been transmitted and the UART is able to accept another character.
The UART can simultaneously send and receive characters, as its name implies.
An interrupt is also generated when a character is received, and usually a small number of input characters can be buffered.
The terminal driver must check a register when an interrupt is received to determine the cause of the interrupt.
Some interface cards have a CPU and memory and can handle multiple lines, taking over much of the I/O load from the main CPU.
RS-232 terminals can be subdivided into categories, as mentioned above.
Characters typed on the keyboard were transmitted to the computer.
Characters sent by the computer were typed on the paper.
They have a CPU and memory and contain software, usually in ROM.
From the operating system’s viewpoint, the main difference between a glass tty and an intelligent terminal is that the latter understands certain escape sequences.
For example, by sending the ASCII ESC character (033), followed by various other characters, it may be possible to move the cursor to any position on the screen, insert text in the middle of the screen, and so forth.
The keyboard and display are almost independent devices, so we will treat them separately here.
They are not quite independent, since typed characters must be displayed on the screen.
In MINIX 3 the keyboard and screen drivers are part of the same process; in other systems they may be split into distinct drivers.
The basic job of the keyboard driver is to collect input from the keyboard and pass it to user programs when they read from the terminal.
In the first one, the driver’s job is just to accept input and pass it upward unmodified.
A program reading from the terminal gets a raw sequence of ASCII codes.
Giving user programs the key numbers is too primitive, as well as being highly machine dependent.
This philosophy is well suited to the needs of sophisticated screen editors such as emacs, which allow the user to bind an arbitrary action to any character or sequence of characters.
It does, however, mean that if the user types dste instead of date and then corrects the error by typing three backspaces and ate, followed by a carriage return, the user program will be given all 11 ASCII codes typed.
They just want the corrected input, not the exact sequence of how it was produced.
This observation leads to the second philosophy: the driver handles all the intraline editing, and just delivers corrected lines to the user programs.
Originally they were referred to as raw mode and cooked mode, respectively.
The POSIX standard uses the less-picturesque term canonical mode to describe line-oriented mode.
On most systems canonical mode refers to a well-defined configuration.
Noncanonical mode is equivalent to raw mode, although many details of terminal behavior can be changed.
POSIXcompatible systems provide several library functions that support selecting either mode and changing many aspects of terminal configuration.
In MINIX 3 the ioctl system call supports these functions.
The first task of the keyboard driver is to collect characters.
If every keystroke causes an interrupt, the driver can acquire the character during the interrupt.
If interrupts are turned into messages by the low-level software, it is possible to put the newly acquired character in the message.
Alternatively, it can be put in a small buffer in memory and the message used to tell the driver that something has arrived.
The latter approach is actually safer if a message can be sent only to a waiting process and there is some chance that the keyboard driver might still be busy with the previous character.
Once the driver has received the character, it must begin processing it.
If the keyboard delivers key numbers rather than the character codes used by application software, then the driver must convert between the codes by using a table.
Not all IBM ‘‘compatibles’’ use standard key numbering, so if the driver wants to support these machines, it must map different keyboards with different tables.
A simple approach is to compile a table that maps between the codes provided by the keyboard and ASCII (American Standard Code for Information Interchange) codes into the keyboard driver, but this is unsatisfactory for users of languages other than English.
Keyboards are arranged differently in different countries, and the ASCII character set is not adequate even for the majority of people in the Western.
Hemisphere, where speakers of Spanish, Portuguese, and French need accented characters and punctuation marks not used in English.
To respond to the need for flexibility of keyboard layouts to provide for different languages, many operating systems provide for loadable keymaps or code pages, which make it possible to choose the mapping between keyboard codes and codes delivered to the application, either when the system is booted or later.
If the terminal is in canonical (i.e., cooked) mode, characters must be stored until an entire line has been accumulated, because the user may subsequently decide to erase part of it.
Even if the terminal is in raw mode, the program may not yet have requested input, so the characters must be buffered to allow type ahead.
System designers who do not allow users to type far ahead ought to be tarred and feathered, or worse yet, be forced to use their own system.
In the first one, the driver contains a central pool of buffers, each buffer holding perhaps 10 characters.
Associated with each terminal is a data structure, which contains, among other items, a pointer to the chain of buffers for input collected from that terminal.
As more characters are typed, more buffers are acquired and hung on the chain.
When the characters are passed to a user program, the buffers are removed and put back in the central pool.
The other approach is to do the buffering directly in the terminal data structure itself, with no central pool of buffers.
Since it is common for users to type a command that will take a little while (say, a compilation) and then type a few lines ahead, to be safe the driver should allocate something like 200 characters per terminal.
On the other hand, a dedicated buffer per terminal makes the driver simpler (no linked list management) and is to be preferred on personal computers with only one or two terminals.
Although the keyboard and display are logically separate devices, many users have grown accustomed to seeing the characters they have just typed appear on the screen.
Some (older) terminals oblige by automatically displaying (in hardware) whatever has just been typed, which is not only a nuisance when passwords are being entered but greatly limits the flexibility of sophisticated editors and other programs.
It is therefore up to the software to display the input.
Echoing is complicated by the fact that a program may be writing to the screen while the user is typing.
At the very least, the keyboard driver has to figure out where to put the new input without it being overwritten by program output.
All keyboards have a tab key, but displays can handle tab on output.
It is up to the driver to compute where the cursor is currently located, taking into account both output from programs and output from echoing, and compute the proper number of spaces to be echoed.
Logically, at the end of a line of text, one wants a carriage return, to move the cursor back to column 1, and a linefeed, to advance to the next line.
Requiring users to type both at the end of each line would not sell well (although some old terminals had a key which generated both, with a 50 percent chance of doing so in the order that the software wanted them)
It was (and still is) up to the driver to convert whatever comes in to the standard internal format used by the operating system.
If the standard form is just to store a linefeed (the convention in UNIX and all its descendants), carriage returns should be turned into linefeeds.
If the internal format is to store both, then the driver should generate a linefeed when it gets a carriage return and a carriage return when it gets a linefeed.
No matter what the internal convention, the terminal may require both a linefeed and a carriage return to be echoed in order to get the screen updated properly.
Since a large computer may well have a wide variety of different terminals connected to it, it is up to the keyboard driver to get all the different carriage return/linefeed combinations converted to the internal system standard and arrange for all echoing to be done right.
A related problem is the timing of carriage return and linefeeds.
On some terminals, it may take longer to display a carriage return or linefeed than a letter or.
If the microprocessor inside the terminal actually has to copy a large block of text to achieve scrolling, then linefeeds may be slow.
If a mechanical print head has to be returned to the left margin of the paper, carriage returns may be slow.
In both cases it is up to the driver to insert filler characters (dummy null characters) into the output stream or just stop outputting long enough for the terminal to catch up.
Terminals with hardware tabs, especially hardcopy ones, may also require a delay after a tab.
When operating in canonical mode, a number of input characters have special meanings.
The defaults are all control characters that should not conflict with text input or codes used by programs, but all except the last two can be changed using the stty command, if desired.
Older versions of UNIX used different defaults for many of these.
The ERASE character allows the user to rub out the character just typed.
It is not added to the character queue but instead removes the previous character from the queue.
It should be echoed as a sequence of three characters, backspace, space, and backspace, in order to remove the previous character from the screen.
If the previous character was a tab, erasing it requires keeping track of where the cursor was prior to the tab.
In most systems, backspacing will only erase characters on the current line.
It will not erase a carriage return and back up into the previous line.
When the user notices an error at the start of the line being typed in, it is often convenient to erase the entire line and start again.
The KILL character (in MINIX 3 CTRL-U) erases the entire line.
Consequently, how to echo KILL is a matter of taste.
As with ERASE it is usually not possible to go further back than the current line.
When a block of characters is killed, it may or may not be worth the trouble for the driver to return buffers to the pool, if one is used.
Sometimes the ERASE or KILL characters must be entered as ordinary data.
The CTRL-V itself can be entered literally by typing CTRL-V CTRL-V.
After seeing a CTRL-V, the driver sets a flag saying that the next character is exempt from special processing.
The LNEXT character itself is not entered in the character queue.
To allow users to stop a screen image from scrolling out of view, control codes are provided to freeze the screen and restart it later.
In MINIX 3 these are STOP (CTRL-S) and START (CTRL-Q), respectively.
They are not stored but are used to set and clear a flag in the terminal data structure.
It is often necessary to kill a runaway program being debugged.
The INTR (CTRL-C) and QUIT (CTRL-\) characters can be used for this purpose.
In MINIX 3, CTRL-C sends the SIGINT signal to all the processes started up from the terminal.
The hard part is getting the information from the driver to the part of the system that handles signals, which, after all, has not asked for this information.
CTRL-\ is similar to CTRL-C, except that it sends the SIGQUIT signal, which forces a core dump if not caught or ignored.
When either of these keys is struck, the driver should echo a carriage return and linefeed and discard all accumulated input to allow for a fresh start.
Historically, DEL was commonly used as the default value for INTR on many UNIX systems.
Since many programs use DEL interchangeably with the backspace for editing, CTRL-C is now preferred.
Another special character is EOF (CTRL-D), which in MINIX 3 causes any pending read requests for the terminal to be satisfied with whatever is available in the buffer, even if the buffer is empty.
Typing CTRL-D at the start of a line causes the program to get a read of 0 bytes, which is conventionally interpreted as end-of-file and causes most programs to act the same way as they would upon seeing end-of-file on an input file.
Some terminal drivers allow much fancier intraline editing than we have sketched here.
They have special control characters to erase a word, skip backward or forward characters or words, go to the beginning or end of the line being typed, and so forth.
Adding all these functions to the terminal driver makes it much larger and, furthermore, is wasted when using fancy screen editors that work in raw mode anyway.
To allow programs to control terminal parameters, POSIX requires that several functions be available in the standard library, of which the most important are tcgetattr and tcsetattr.
Tcgetattr retrieves a copy of the structure shown in Fig.
A program can examine the current settings and modify them as desired.
Tcsetattr then writes the structure back to the terminal driver.
The POSIX standard does not specify whether its requirements should be implemented through library functions or system calls.
Output is simpler than input, but drivers for RS-232 terminals are radically different from drivers for memory-mapped terminals.
The method that is commonly used for RS-232 terminals is to have output buffers associated with each terminal.
The buffers can come from the same pool as the input buffers, or be dedicated, as with input.
When programs write to the terminal, the output is first copied to the buffers.
Similarly, output from echoing is also copied to the buffers.
After all the output has been copied to the buffers (or the buffers are full), the first character is output, and the driver goes to sleep.
When the interrupt comes in, the next character is output, and so on.
Characters to be printed are extracted one at a time from user space and put directly in the video RAM.
With RS-232 terminals, each character to be output is just put on the line to the terminal.
With memory mapping, some characters require special treatment, among them, backspace, carriage return, linefeed, and the audible bell (CTRL-G)
A driver for a memory-mapped terminal must keep track in software of the current position in the video RAM, so that printable characters can be put there and the current position advanced.
Backspace, carriage return, and linefeed all require this position to be updated appropriately.
Most video controllers contain a register that determines where in the video RAM to begin fetching bytes for the top line on the screen.
The only other thing the driver must do is copy whatever is needed to the new bottom line.
When the video controller gets to the top of the RAM, it just wraps around and continues merrily fetching bytes starting at the lowest address.
Another issue that the driver must deal with on a memory-mapped terminal is cursor positioning.
Again, the hardware usually provides some assistance in the form of a register that tells where the cursor is to go.
It is sounded by outputting a sine or square wave to the loudspeaker, a part of the computer quite separate from the video RAM.
Screen editors and many other sophisticated programs need to be able to update the screen in more complex ways than just scrolling text onto the bottom of the display.
To accommodate them, many terminal drivers support a variety of escape sequences.
Although some terminals support idiosyncratic escape sesequence sets, it is advantageous to have a standard to facilitate adapting software from one system to another.
The American National Standards Institute (ANSI) has defined a set of standard escape sequences, and MINIX 3 supports a subset of the ANSI sequences, shown in Fig.
When the driver sees the character that starts the escape sequences, it sets a flag and waits until the rest of the escape sequence comes in.
When everything has arrived, the driver must carry it out in software.
Inserting and deleting text require moving blocks of characters around the video RAM.
The hardware is of no help with anything except scrolling and displaying the cursor.
The ANSI escape sequences accepted by the terminal driver on output.
The size of the terminal driver is partly explained by the observation that the driver handles both the keyboard and the display, each of which is a complicated device in its own right, as well as two other optional types of terminals.
Still, it comes as a surprise to most people to learn that terminal I/O requires thirty times as much code as the scheduler.
This feeling is reinforced by looking at the numerous books on operating systems that devote thirty times as much space to scheduling as to all I/O combined.
The terminal driver accepts more than a dozen message types.
Read from the terminal (from FS on behalf of a user process)
Write to the terminal (from FS on behalf of a user process)
Set terminal parameters for ioctl (from FS on behalf of a user process)
Other message types are used for special purposes such as generating diagnostic displays when function keys are pressed or triggering panic dumps.
The messages used for reading and writing have the same format as shown in Fig.
With a disk, the program has to specify which block it wants to read.
With a keyboard, there is no choice: the program always gets the next character typed in.
The POSIX functions tcgetattr and tcgetattr, used to examine and modify terminal attributes (properties), are supported by the ioctl system call.
There are, however, some control operations needed by MINIX 3 that are not provided for in POSIX, for example, loading an alternate keymap, and for these the programmer must use ioctl explicitly.
To better understand how the driver works, let us first look at how characters typed in on the keyboard work their way through the system to the program that wants them.
Although this section is intended as an overview we will use line number references to help the reader find each function used.
When a user logs in on the system console, a shell is created for him with /dev/console as standard input, standard output, and standard error.
The shell starts up and tries to read from standard input by calling the library procedure read.
This procedure sends a message that contains the file descriptor, buffer address, and count to the file system.
After sending the message, the shell blocks, waiting for the reply.
User processes execute only the sendrec primitive, which combines a send with a receive from the process sent to.
The file system gets the message and locates the i-node corresponding to the specified file descriptor.
This i-node is for the character special file /dev/console and contains the major and minor device numbers for the terminal.
When a character is finally typed on the keyboard, it causes two interrupts, one when the key is depressed and one when it is released.
Read request from the keyboard when no characters are pending.
The TTY receives a message for every keypress and queues scan codes as they are entered.
Later these are interpreted and assembled into a buffer of ASCII codes which is copied to the user process.
This also applies to modifier keys such as CTRL and SHIFT.
Although ultimately these keys do not cause ASCII codes to be returned to the user process, they generate scan codes indicating which key was pressed (the driver can distinguish between the left and right shift keys if desired), and they still cause two interrupts per key.
When enough characters have come in, the terminal driver makes another kernel call (8) to ask the system task to copy the data to the address requested by the shell.
The copying of the data is not message passing and for that reason is shown by dashed lines (9) in Fig.
More than one such line is shown because there may be more than one such operation before the user’s request has been completely fulfilled.
Note that the system task copies the actual characters directly from the TTY’s address space to that of the shell.
With block I/O, data pass through the file system to allow it to maintain a buffer cache of the most recently used blocks.
If a requested block happens to be in the cache, the request can be satisfied directly by the file system, without doing any actual disk I/O.
Furthermore, a request from the file system to a disk driver can always be satisfied in at most a few hundred milliseconds, so there is no harm in having the file system wait.
Keyboard I/O may take hours to complete, or may never be complete (in canonical mode the terminal driver waits for a complete line, and it may also wait a long time in noncanonical mode, depending upon the settings of MIN and TIME)
Thus, it is unacceptable to have the file system block until a terminal input request is satisfied.
Readers who are familiar with earlier versions of MINIX may remember that in these versions the TTY driver (and all other drivers) were compiled together with the kernel.
Each driver had its own interrupt handler in kernel space.
In the case of the keyboard driver, the interrupt handler itself could buffer a certain number of scan codes, and also do some preliminary processing (scan codes for most key releases could be dropped, only for modifier keys like the shift key is it necessary to buffer the release codes)
The interrupt handler itself did not send messages to the TTY driver, because the probability was high that the TTY would not be blocked on a receive and able to receive a message at any given time.
Instead, the clock interrupt handler awakened the TTY driver periodically.
Earlier we made something of a point of the differences between handling expected interrupts, such as those generated by a disk controller, and handling unpredictable interrupts like those from a keyboard.
But in MINIX 3 nothing special seems to have been done to deal with the problems of unpredictable interrupts.
How is this possible? One thing to keep in mind is the enormous difference in performance between the computers for which the earliest versions of MINIX were written and current designs.
Another thing to keep in mind is that keyboard input is very slow by computer standards.
Even with a fast typist the terminal driver will probably be sent an interrupt message for each character typed at the keyboard.
At that speed approximately 120 characters may be received by the modem between clock ticks, but to allow for data compression on the modem link the serial port connected to the modem must be able to handle at least twice as many.
One thing to consider with a serial port, however, is that characters, not scan codes, are transmitted, so even with an old UART that does no buffering, there will be only one interrupt per keypress instead of two.
Ethernet-based networks can deliver characters at a rate much faster than a serial line, but ethernet adapters buffer entire packets, and only one interrupt is necessary per packet.
The left branch of the tree is taken to process a request to read characters.
The right branch is taken when a keyboard message is sent to the driver before a user has requested input.
In general, console output is simpler than terminal input, because the operating system is in control and does not need to be concerned with requests for output arriving at inconvenient times.
Also, because the MINIX 3 console is a memory-mapped display, output to the console is particularly simple.
No interrupts are needed; the basic operation is to copy data from one memory region to another.
On the other hand, all the details of managing the display, including handling escape sequences, must be handled by the driver software.
As we did with keyboard input in the previous section, we will trace through the steps involved in.
We will assume in this example that the active display is being written; minor complications caused by virtual consoles will be discussed later.
When a process wants to print something, it generally calls printf.
Printf calls write to send a message to the file system.
The message contains a pointer to the characters that are to be printed (not the characters themselves)
The file system then sends a message to the terminal driver, which fetches them and copies them to the video RAM.
Older display adapters generally have smaller memory but are able to wrap around and do hardware scrolling.
Newer adapters generally have much more memory than needed to display a single screen of text, but the controllers are not able to wrap.
Whichever method is used, a row of blanks is copied to the video RAM to ensure that the new line at the bottom of the screen is empty.
The limit is reached when the maximum possible number of consoles is configured.
Then every scroll operation will be a software scroll operation.
Fields of the console structure that relate to the current screen position.
The terminal driver supports escape sequences to allow screen editors and other interactive programs to update the screen in a flexible way.
There are two categories of escape sequences: those that never contain a variable parameter, and those that may contain parameters.
In the first category the only representative supported by MINIX 3 is ESC M, which reverse indexes the screen, moving the cursor up one line and scrolling the screen downward if the cursor is already on the first line.
The other category can have one or two numeric parameters.
A table of escape sequences defined by the ANSI standard and recognized by MINIX 3 was shown in Fig.
In a sequence that accepts a parameter, the parameter may be omitted, and in a sequence that accepts two parameters either or both of them may be omitted.
When a parameter is omitted or one that is outside the valid range is used, a default is substituted.
Consider the following ways a program could construct a sequence to move to the upper-left corner of the screen:
These examples are presented not to suggest one should deliberately use invalid parameters but to show that the code that parses such sequences is nontrivial.
The IBM PC keyboard does not generate ASCII codes directly.
When a key is pressed, MINIX 3 receives the key number as a scan code.
A scan code is also generated when a key is released, but the code generated upon release has the most significant bit set (equivalent to adding 128 to the key number)
By keeping track of which modifier keys have been pressed and not yet released, a large number of combinations are possible.
For ordinary purposes, of course, two-finger combinations, such as SHIFT-A or CTRL-D, are most manageable for two-handed typists, but for special occasions three-key (or more) combinations are possible, for instance, CTRL-SHIFT-A, or the wellknown CTRL-ALT-DEL combination that PC users recognize as the way to reset and reboot the system.
The source code for a keymap defines a large initialized array, and in the interest of saving space a keymap file is not printed in Appendix B.
The entry for code 1, the ESC key, shows that the value returned is unchanged when the SHIFT key or CTRL key are pressed, but that a different code is returned when an ALT key is pressed simultaneously with the ESC key.
The first three of these macros manipulate bits in the code for the quoted character to produce the necessary code to be returned to the application.
The last one sets the HASCAPS bit in the high byte of the 16-bit value.
This is a flag that indicates that the state of the capslock variable has to be checked and the code possibly modified before being returned.
Because the newline character in UNIX files is the LF (0x0A) code, and it is sometimes necessary to enter this directly, this keyboard map provides for a CTRL-ENTER combination, which produces this code, C(’J’)
Scan code 29 is one of the modifier codes and must be recognized no matter what other key is pressed, so the CTRL value is returned regardless of any other key that may be pressed.
The last entry shown in the figure, for scan code 127, is typical of many entries near the end of the array.
For many keyboards, certainly most of those used in Europe and the Americas, there are not enough keys to generate all the possible codes, and these entries in the table are filled with zeroes.
Early PCs had the patterns for generating characters on a video screen stored only in ROM, but the displays used on modern systems provide RAM on the video display adapters into which custom character generator patterns can be loaded.
In this section we will begin to look at the source code of the terminal driver in detail.
We saw when we studied the block devices that multiple drivers supporting several different devices could share a common base of software.
The case with the terminal devices is similar, but with the difference that there is one terminal driver that supports several different kinds of terminal device.
In later sections we will look at the device-dependent code for the keyboard and the memory-mapped console display.
The file tty.h contains definitions used by the C files which implement the terminal drivers.
These are used frequently in the terminal driver code, which does much copying of data into and out of buffers.
The first one is that since the last interrupt additional characters may have been read or characters to be written to an output device may be ready.
Before attempting to receive a message, the main loop always checks the.
Now, about the continue statements: in the C language, a continue statement short-circuits a loop, and returns control to the top of the loop.
So if any one of the message types mentioned so far is detected, as soon as it is serviced control returns to the top of the main loop, at line 13764, the check for events is repeated, and receive is called again to await a new message.
Particularly in the case of input it is important to be ready to respond again as quickly as possible.
Also, if any of the message-type tests in the first part of the loop succeeded there is no point in making any of the tests that come after the first switch.
In canonical mode a terminal device waits for input until either the number of characters asked for in the call has been received, or the end of a line or the end of the file is reached.
The ICANON bit in the termios structure is tested on line 13981 to see if canonical mode is in effect for the terminal.
If it is not set, the termios MIN and TIME values are checked to determine what action to take.
If virtual terminals are in use the same keymap and font apply to all consoles, the hardware does not permit any easy way of doing otherwise.
The window size operations copy a winsize structure between the user process and the terminal driver.
Note, however, the comment under the code for the TIOCSWINSZ operation.
When a process changes its window size, the kernel is expected to send a SIGWINCH signal to the process group under some versions of UNIX.
However, anyone thinking of using these structures should consider adding code here to initiate this signal.
The fields in a character code as it is placed into the input queue.
Characters which control echoing are acted upon immediately without being placed in the queue.
Here it is found that the IXON bit is set, by default, allowing use of the STOP (CTRL-S) and START (CTRL-Q) characters, but in the ensuing tests for these no match is found.
On line 14597 it is found that the ISIG bit, enabling the use of the INTR and QUIT characters, is set by default, but again no match is found.
The order of these two tests is important, making it possible to enter the LNEXT character itself twice in a row, in order to pass the second copy on to a process as actual data.
Select is a system call used when multiple I/O devices may require service at unpredictable times by a single process.
A classic example is a communications program which needs to pay attention to a local keyboard and a remote system, perhaps connected by a modem.
The select call allows opening several device files and monitoring all of them to see when they can be read from or written to without blocking.
Now we turn to the device-dependent code that supports the MINIX 3 console, which consists of an IBM PC keyboard and a memory-mapped display.
The physical devices that support these are entirely separate: on a standard desktop system the display uses an adapter card (of which there are at least a half-dozen basic types) plugged into the backplane, while the keyboard is supported by circuitry built into the parentboard which interfaces with an 8-bit single-chip computer inside the keyboard unit.
The two subdevices require entirely separate software support, which is found in the files keyboard.c and console.c.
Keyboard.c begins, like most source files we have seen, with several #include statements.
The keymap source file is not included in Appendix B because of its size, but some representative entries are illustrated in Fig.
Scan codes in the input buffer, with corresponding key actions below, for a line of text entered at the keyboard.
The code for a release is 128 more than the code for a press of the same key.
This is not a service generally provided in other operating systems, but MINIX was always intended to be a teaching tool.
Users are encouraged to tinker with it, which means users may need extra help for debugging.
In many cases the output produced by pressing one of the F-keys will be available even when the system has crashed.
But individual drivers can also register to respond to a function key.
Ethernet drivers typically do this, as a dump that shows packet statistics can be helpful in solving network problems.
Hit ESC to reboot, DEL to shutdown, F-keys for debug dumps.
The IBM PC display may be configured as several virtual terminals, if sufficient memory is available.
We will examine the console’s device-dependent code in this section.
We will also look at the debug dump routines that use low-level.
These provide support for limited interaction with the user at the console, even when other parts of the MINIX 3 system are not functioning and can provide useful information even following a near-total system crash.
The softscroll method is never the default; the operator is supposed to select it only if hardware scrolling does not work or is not desired for some reason.
One reason might be a desire to use the screendump command, either to save the screen memory in a file or to view the main console display when working from a remote terminal.
When hardware scrolling is in effect, screendump is likely to give unexpected results, because the start of the screen memory is likely not to coincide with the start of the visible display.
The first five sequences are generated, with no numeric arguments, by the four ‘‘arrow’’ keys and the Home key on the IBM PC keyboard.
In such cases, flush catches requests to move out of bounds and limits the move to the last row or the first row, as appropriate.
When generated by the ‘‘arrow’’ keys there is no numeric argument, and thus the default movement of one line or column occurs.
The beep function (line 16629) is called when a CTRL-G character must be output.
It takes advantage of the built-in support provided by the PC for making sounds by sending a square wave to the speaker.
Several routines display output on behalf of the terminal driver itself, the kernel, or another system component.
The first one, kputc (line 16775) just calls putk, a routine to output text a byte at a time, to be described below.
This routine is here because the library routine that provides the printf function used within system components is written to be linked to a character printing routine with this name, but other functions in the terminal driver expect one named putk.
This function is for displaying text on the console to report information, warnings, or errors to the user.
A substantial fraction of any operating system is concerned with I/O.
But I/O device drivers are often responsible for operating system problems.
Drivers are often written by programmers working for device manufacturers.
Conventional operating system designs usually require allowing drivers to have access to critical resources, such as interrupts, I/O ports, and memory belonging to other processes.
The design of MINIX 3 isolates drivers as independent processes with limited privileges, so a bug in a driver cannot crash the entire system.
We started out by looking at I/O hardware, and the relation of I/O devices to I/O controllers, which are what the software has to deal with.
Then we moved on to the four levels of I/O software: the interrupt routines, the device drivers, the device-independent I/O software, and the I/O libraries and spoolers that run in user space.
Then we examined the problem of deadlock and how it can be tackled.
Deadlock occurs when a group of processes each have been granted exclusive access to some resources, and each one wants yet another resource that belongs to another process in the group.
All of them are blocked and none will ever run again.
Deadlock can be prevented by structuring the system so it can never occur, for example, by allowing a process to hold only one resource at any instant.
It can also be avoided by examining each resource request to see if it leads to a situation in which deadlock is possible (an unsafe state) and denying or delaying those that lead to trouble.
Device drivers in MINIX 3 are implemented as independent processes running in user space.
We have looked at the RAM disk driver, hard disk driver, and terminal driver.
Each of these drivers has a main loop that gets requests and processes them, eventually sending back replies to report on what happened.
Source code for the main loops and common functions of the RAM disk, hard disk, and floppy disk drivers is provided in a common driver library, but each driver is compiled and linked with its own copy of the library routines.
Several different terminals, using the system console, the serial lines, and network connections, are all supported by a single terminal driver process.
Devices which can complete their work rapidly, such as the RAM disk and the memorymapped display, do not use interrupts at all.
The hard disk driver does most of its work in the driver code itself, and the interrupt handlers just return status information.
Interrupts are always expected, and a receive can be done to wait for one.
Messages generated by all interrupts for the terminal driver are received and processed in the main loop of the driver.
When a keyboard interrupt occurs the first stage of processing the input is done as quickly as possible in order to be ready for subsequent interrupts.
Interrupts are handled by the system task, which sends a message to notify a driver when an interrupt occurs.
Access to I/O ports is similarly mediated by the system task.
Many disks contain an ECC at the end of each sector.
If the ECC is wrong, what actions might be taken and by which piece of hardware or software?
Although DMA does not use the CPU, the maximum transfer rate is still limited.
Name three factors that might ultimately limit the rate of transfer.
CD-quality music requires sampling the sound signal 44,100 times per second.
What is the slowest clock rate that could be used and not lose any data? Assume that the number of instructions to be processed for an interrupt is constant, so halving the clock speed doubles the interrupt handling time.
Are there any circumstances you can think of in which polling is a better choice?
Disk controllers have internal buffers and they are getting larger with each new model.
Each device driver has two different interfaces with the operating system.
One interface is a set of function calls that the operating system makes on the driver.
The other is a set of calls that the driver makes on the operating system.
Why do operating system designers attempt to provide device-independent I/O wherever it is possible?
In which of the four I/O software layers is each of the following done? (a) Computing the track, sector, and head for a disk read.
Why are output files for the printer normally spooled on disk before being printed?
Give an example of a deadlock that could occur in the physical world.
Suppose that in step (o) C requested S instead of requesting R.
Would this lead to deadlock? Suppose that it requested both S and R?
If D asks for one more unit, does this lead to a safe state or an unsafe one? What if the request came from C instead of D?
Can you envision any circumstances in which diagonal trajectories were also possible?
A computer has six tape drives, with n processes competing for them.
For which values of n is the system deadlock free?
Can a system be in a state that is neither deadlocked nor safe? If so, give an example.
If not, prove that all states are either deadlocked or safe.
A distributed system using mailboxes has two IPC primitives, SEND and RECEIVE.
The latter primitive specifies a process to receive from, and blocks if no message from that process is available, even though messages may be waiting from other processes.
There are no shared resources, but processes need to communicate frequently about other matters.
In an electronic funds transfer system, there are hundreds of identical processes that work as follows.
Each process reads an input line specifying an amount of money, the account to be credited, and the account to be debited.
Then it locks both accounts and transfers the money, releasing the locks when done.
With many processes running in parallel, there is a very real danger that having locked account x it will be unable to lock y because y has been locked by a process now waiting for x.
Do not release an account record until you have completed the transactions.
In other words, solutions that lock one account and then release it immediately if the other is locked are not allowed.
The banker’s algorithm is being run in a system with m resource classes and n processes.
In the limit of large m and n, the number of operations that must be performed to check a state for safety is proportional to m an b.
Can these requests be met and the system still remain in a safe state?
To divide their property, they have agreed on the following algorithm.
Every morning, each one may send a letter to the other’s lawyer requesting one item of property.
Since it takes a day for letters to be delivered, they have agreed that if both discover that they have requested the same item on the same day, the next day they will send a letter canceling the request.
Among their property is their dog, Woofer, Woofer’s doghouse, their canary, Tweeter, and Tweeter’s cage.
The animals love their houses, so it has been agreed that any division of property separating an animal from its house is invalid, requiring the whole division to start over from scratch.
So they can go on (separate) vacations, each spouse has programmed a personal computer to handle the negotiation.
When they come back from vacation, the computers are still negotiating.
Why? Is deadlock possible? Is starvation (waiting forever) possible? Discuss.
What is the maximum sustainable burst rate? How long can such a burst last?
The user issues a system call to write data packets to the network.
The operating system then copies the data to a kernel buffer.
Then it copies the data to the network controller board.
When all the bytes are safely inside the controller, they are sent over the network at a rate of 10 megabits/sec.
The receiving network controller stores each bit a microsecond after it is sent.
When the last bit arrives, the destination CPU is interrupted, and the kernel copies the newly arrived packet to a kernel buffer to inspect it.
Once it has figured out which user the packet is for, the kernel copies the data to the user space.
For simplicity, assume the time to get the acknowledgement back is so small it can be ignored.
Could any fields be omitted for character devices? Which ones?
How much seek time is needed for (a) First-come, first served.
A personal computer salesman visiting a university in South-West Amsterdam remarked during his sales pitch that his company had devoted substantial effort to making their version of UNIX very fast.
As an example, he noted that their disk driver used the elevator algorithm and also queued multiple requests within a cylinder in sector order.
He took it home and wrote a program to randomly read 10,000 blocks spread across the disk.
To his amazement, the performance that he measured was identical to what would be expected from first-come, first-served.
A UNIX process has two parts—the user part and the kernel part.
Is the kernel part like a subroutine or a coroutine?
The clock interrupt handler on a certain computer requires 2 msec (including process switching overhead) per clock tick.
What fraction of the CPU is devoted to the clock?
Two examples of watchdog timers were given in the text: timing the startup of the floppy disk motor and allowing for carriage return on hardcopy terminals.
Why are RS232 terminals interrupt driven, but memory-mapped terminals not interrupt driven?
When the character has been printed, an interrupt occurs and a message is sent to the blocked driver, which outputs the next character and then blocks again.
To scroll a window, the CPU (or controller) must move all the lines of text upward by copying their bits from one part of the video RAM to another.
Now compute the baud rate for the same terminal in color, with 4 bits/pixel.
Putting a character on the screen now takes 200 microsec.
Why do operating systems provide escape characters, such as CTRL-V in MINIX?
Many RS232 terminals have escape sequences for deleting the current line and moving all the lines below it up one line.
How do you think this feature is implemented inside the terminal?
On the original IBM PC’s color display, writing to the video RAM at any time other than during the CRT beam’s vertical retrace caused ugly spots to appear all over the screen.
What fraction of the time is the video RAM available for writing in?
Write a graphics driver for the IBM color display, or some other suitable bitmap display.
The driver should accept commands to set and clear individual pixels, move rectangles around the screen, and any other features you think are interesting.
User programs interface to the driver by opening /dev/graphics and writing commands to it.
Modify the MINIX floppy disk driver to do track-at-a-time caching.
In this way, users can read large chunks of data from the disk, which are DMA’ed directly to user space, greatly improving performance.
This driver would primarily be of interest to programs that need to read the raw bits on the disk, without regard to the file system.
Implement the UNIX PROFIL system call, which is missing from MINIX.
A new hard disk device with removable media has been added to a MINIX 3 system.
Memory is an important resource that must be carefully managed.
To paraphrase Parkinson’s law, ‘‘Programs and their data expand to fill the memory available to hold them.’’ In this chapter we will study how operating systems manage memory.
Ideally, what every programmer would like is an infinitely large, infinitely fast memory that is also nonvolatile, that is, does not lose its contents when the electric power fails.
While we are at it, why not also ask for it to be inexpensive, too? Unfortunately technology cannot turn such dreams into memories.
Consequently, most computers have a memory hierarchy, with a small amount of very fast, expensive, volatile cache memory, hundreds of megabytes of medium-speed, medium-price, volatile main memory (RAM), and tens or hundreds of gigabytes of slow, cheap, nonvolatile disk storage.
It is the job of the operating system to coordinate how these memories are used.
The part of the operating system that manages the memory hierarchy is usually called the memory manager.
Its job is to keep track of which parts of memory are in use and which parts are not in use, to allocate memory to processes when they need it and deallocate it when they are done, and to manage swapping between main memory and disk when main memory is too small to hold all the processes.
In most systems (but not MINIX 3), it is in the kernel.
In this chapter we will investigate a number of different memory management schemes, ranging from very simple to highly sophisticated.
We will start at the beginning and look first at the simplest possible memory management system and then gradually progress to more and more elaborate ones.
The cycle is now repeating itself with palmtops, PDAs, and embedded systems.
In these systems, simple memory management schemes are still in use.
Memory management systems can be divided into two basic classes: those that move processes back and forth between main memory and disk during execution (swapping and paging), and those that do not.
The latter are simpler, so we will study them first.
Later in the chapter we will examine swapping and paging.
Throughout this chapter the reader should keep in mind that swapping and paging are largely artifacts caused by the lack of sufficient main memory to hold all programs and data at once.
If main memory ever gets so large that there is truly enough of it, the arguments in favor of one kind of memory management scheme or another may become obsolete.
On the other hand, as mentioned above, software seems to grow as fast as memory, so efficient memory management may always be needed.
Now Microsoft recommends having at least 128 MB for a single-user Windows XP system.
The trend toward multimedia puts even more demands on memory, so good memory management is probably going to be needed for the next decade at least.
The simplest possible memory management scheme is to run just one program at a time, sharing the memory between that program and the operating system.
The operating system may be at the bottom of memory in RAM (Random Access Memory), as shown in Fig.
The first model was formerly used on mainframes and minicomputers but is rarely used any more.
The second model is used on some palmtop computers and embedded.
The third model was used by early personal computers (e.g., running MS-DOS), where the portion of the system in the ROM is called the BIOS (Basic Input Output System)
Three simple ways of organizing memory with an operating system and one user process.
When the system is organized in this way, only one process at a time can be running.
As soon as the user types a command, the operating system copies the requested program from disk to memory and executes it.
When the process finishes, the operating system displays a prompt character and waits for a new command.
When it receives the command, it loads a new program into memory, overwriting the first one.
Except on very simple embedded systems, monoprogramming is hardly used any more.
Most modern systems allow multiple processes to run at the same time.
Having multiple processes running at once means that when one process is blocked waiting for I/O to finish, another one can use the CPU.
Network servers always have the ability to run multiple processes (for different clients) at the same time, but most client (i.e., desktop) machines also have this ability nowadays.
The easiest way to achieve multiprogramming is simply to divide memory up into n (possibly unequal) partitions.
This partitioning can, for example, be done manually when the system is started up.
When a job arrives, it can be put into the input queue for the smallest partition large enough to hold it.
Since the partitions are fixed in this scheme, any space in a partition not used by a job is wasted while that job runs.
The disadvantage of sorting the incoming jobs into separate queues becomes apparent when the queue for a large partition is empty but the queue for a small.
Here small jobs have to wait to get into memory, even though plenty of memory is free.
An alternative organization is to maintain a single queue as in Fig.
Whenever a partition becomes free, the job closest to the front of the queue that fits in it could be loaded into the empty partition and run.
Since it is undesirable to waste a large partition on a small job, a different strategy is to search the whole input queue whenever a partition becomes free and pick the largest job that fits.
Note that the latter algorithm discriminates against small jobs as being unworthy of having a whole partition, whereas usually it is desirable to give the smallest jobs (often interactive jobs) the best service, not the worst.
One way out is to have at least one small partition around.
Such a partition will allow small jobs to run without having to allocate a large partition for them.
Another approach is to have a rule stating that a job that is eligible to run may not be skipped over more than k times.
Each time it is skipped over, it gets one point.
When it has acquired k points, it may not be skipped again.
This system, with fixed partitions set up by the operator in the morning and not changed thereafter, was used by OS/360 on large IBM mainframes for many years.
It was called MFT (Multiprogramming with a Fixed number of Tasks or OS/MFT)
However, nowadays, few, if any, operating systems, support this model, even on mainframe batch systems.
Multiprogramming introduces two essential problems that must be solvedrelocation and protection.
From the figure it is clear that different jobs will be run at different addresses.
When a program is linked (i.e., the main program, user-written procedures, and library procedures are combined into a single address space), the linker must know at what address the program will begin in memory.
For example, suppose that the first instruction is a call to a procedure at absolute address 100 within the binary file produced by the linker.
One possible solution is to actually modify the instructions as the program is loaded into memory.
To perform relocation during loading like this, the linker must include in the binary program a list or bitmap telling which program words are addresses to be relocated and which are opcodes, constants, or other items that must not be relocated.
A malicious program can always construct a new instruction and jump to it.
Because programs in this system use absolute memory addresses rather than addresses relative to a register, there is no way to stop a program from building an instruction that reads or writes any word in memory.
In multiuser systems, it is highly undesirable to let processes read and write memory belonging to other users.
The 360 hardware trapped any attempt by a running process to access memory whose protection code differed from the PSW key.
Since only the operating system could change the protection codes and key, user processes were prevented from interfering with one another and with the operating system itself.
An alternative solution to both the relocation and protection problems is to equip the machine with two special hardware registers, called the base and limit registers.
When a process is scheduled, the base register is loaded with the address of the start of its partition, and the limit register is loaded with the length of the partition.
Every memory address generated automatically has the base register contents added to it before being sent to memory.
Addresses are also checked against the limit register to make sure that they do not attempt to.
The hardware protects the base and limit registers to prevent user programs from modifying them.
A disadvantage of this scheme is the need to perform an addition and a comparison on every memory reference.
Comparisons can be done fast, but additions are slow due to carry propagation time unless special addition circuits are used.
The Intel 8088 CPU used for the original IBM PC used a slightly weaker version of this scheme—base registers, but no limit registers.
With a batch system, organizing memory into fixed partitions is simple and effective.
Each job is loaded into a partition when it gets to the head of the queue.
As long as enough jobs can be kept in memory to keep the CPU busy all the time, there is no reason to use anything more complicated.
With timesharing systems or graphics-oriented personal computers, the situation is different.
Sometimes there is not enough main memory to hold all the currently active processes, so excess processes must be kept on disk and brought in to run dynamically.
Two general approaches to memory management can be used, depending (in part) on the available hardware.
The simplest strategy, called swapping, consists of bringing in each process in its entirety, running it for a while, then putting it back on the disk.
The other strategy, called virtual memory, allows programs to run even when they are only partially in main memory.
The operation of a swapping system is illustrated in Fig.
Then processes B and C are created or swapped in from disk.
Since A is now at a different location, addresses contained in it must be relocated, either by software when it is swapped in or (more likely) by hardware during program execution.
The flexibility of not being tied to a fixed number of partitions that may be too large or too small improves memory utilization, but it also complicates allocating and deallocating memory, as well as keeping track of it.
When swapping creates multiple holes in memory, it is possible to combine them all into one big one by moving all the processes downward as far as possible.
It is usually not done because it requires a lot of CPU time.
Memory allocation changes as processes come into memory and leave it.
That may not seem like much time, but it would be noticeably disruptive to a user watching a video stream.
A point that is worth making concerns how much memory should be allocated for a process when it is created or swapped in.
If processes are created with a fixed size that never changes, then the allocation is simple: the operating system allocates exactly what is needed, no more and no less.
If, however, processes’ data segments can grow, for example, by dynamically allocating memory from a heap, as in many programming languages, a problem occurs whenever a process tries to grow.
If a hole is adjacent to the process, it can be allocated and the process can be allowed to grow into the hole.
On the other hand, if the process is adjacent to another process, the growing process will either have to be moved to a hole in memory large enough for it, or one or more processes will have to be swapped out to create a large enough hole.
If a process cannot grow in memory and the swap area on the disk is full, the process will have to wait or be killed.
If it is expected that most processes will grow as they run, it is probably a good idea to allocate a little extra memory whenever a process is swapped in or moved, to reduce the overhead associated with moving or swapping processes that no longer fit in their allocated memory.
However, when swapping processes to disk, only the memory actually in use should be swapped; it is wasteful to swap the extra memory as well.
If processes can have two growing segments, for example, the data segment being used as a heap for variables that are dynamically allocated and released and.
In this figure we see that each process illustrated has a stack at the top of its allocated memory that is growing downward, and a data segment just beyond the program text that is growing upward.
The memory between them can be used for either segment.
If it runs out, either the process will have to be moved to a hole with sufficient space, swapped out of memory until a large enough hole can be created, or killed.
When memory is assigned dynamically, the operating system must manage it.
In general terms, there are two ways to keep track of memory usage: bitmaps and free lists.
In this section and the next one we will look at these two methods in turn.
With a bitmap, memory is divided up into allocation units, perhaps as small as a few words and perhaps as large as several kilobytes.
Figure 4-5 shows part of memory and the corresponding bitmap.
The size of the allocation unit is an important design issue.
If the allocation unit is chosen large, the bitmap will be smaller, but appreciable memory may be wasted in the last unit of the process if the process size is not an exact multiple of the allocation unit.
Another way of keeping track of memory is to maintain a linked list of allocated and free memory segments, where a segment is either a process or a hole between two processes.
Each entry in the list specifies a hole (H) or process (P), the address at which it starts, the length, and a pointer to the next entry.
In this example, the segment list is kept sorted by address.
Sorting this way has the advantage that when a process terminates or is swapped out, updating the list is straightforward.
A terminating process normally has two neighbors (except when it is at the very top or very bottom of memory)
These may be either processes or holes, leading to the four combinations shown in Fig.
Since the process table slot for the terminating process will normally point to the list entry for the process itself, it may be more convenient to have the list as a double-linked list, rather than the single-linked list of Fig.
This structure makes it easier to find the previous entry and to see if a merge is possible.
When the processes and holes are kept on a list sorted by address, several algorithms can be used to allocate memory for a newly created process (or an existing process being swapped in from disk)
We assume that the memory manager knows how much memory to allocate.
The process manager scans along the list of segments until it finds a hole that is big enough.
The hole is then broken up into two pieces, one for the process and one for the unused memory, except in the statistically unlikely case of an exact fit.
First fit is a fast algorithm because it searches as little as possible.
It works the same way as first fit, except that it keeps track of where it is whenever it finds a suitable hole.
The next time it is called to find a hole, it starts searching the list from the place where it left off last time, instead of always at the beginning, as first fit does.
Simulations by Bays (1977) show that next fit gives slightly worse performance than first fit.
Best fit searches the entire list and takes the smallest hole that is adequate.
Rather than breaking up a big hole that might be needed later, best fit tries to find a hole that is close to the actual size needed.
As an example of first fit and best fit, consider Fig.
Best fit is slower than first fit because it must search the entire list every time it is called.
Somewhat surprisingly, it also results in more wasted memory than first fit or next fit because it tends to fill up memory with tiny, useless holes.
To get around the problem of breaking up nearly exact matches into a process and a tiny hole, one could think about worst fit, that is, always take the largest.
Simulation has shown that worst fit is not a very good idea either.
All four algorithms can be speeded up by maintaining separate lists for processes and holes.
In this way, all of them devote their full energy to inspecting holes, not processes.
The inevitable price that is paid for this speedup on allocation is the additional complexity and slowdown when deallocating memory, since a freed segment has to be removed from the process list and inserted into the hole list.
If distinct lists are maintained for processes and holes, the hole list may be kept sorted on size, to make best fit faster.
When best fit searches a list of holes from smallest to largest, as soon as it finds a hole that fits, it knows that the hole is the smallest one that will do the job, hence the best fit.
No further searching is needed, as it is with the single list scheme.
With a hole list sorted by size, first fit and best fit are equally fast, and next fit is pointless.
When the holes are kept on separate lists from the processes, a small optimization is possible.
Instead of having a separate set of data structures for maintaining the hole list, as is done in Fig.
The first word of each hole could be the hole size, and the second word a pointer to the following entry.
Yet another allocation algorithm is quick fit, which maintains separate lists for some of the more common sizes requested.
With quick fit, finding a hole of the required size is extremely fast, but it has the same disadvantage as all schemes that sort by hole size, namely, when a process terminates or is swapped out, finding its neighbors to see if a merge is possible is expensive.
If merging is not done, memory will quickly fragment into a large number of small holes into which no processes fit.
Many years ago people were first confronted with programs that were too big.
The solution usually adopted was to split the program into pieces, called overlays.
Some overlay systems were highly complex, allowing multiple overlays in memory at once.
The overlays were kept on the disk and swapped in and out of memory by the operating system, dynamically, as needed.
Although the actual work of swapping overlays in and out was done by the system, the decision of how to split the program into pieces had to be done by the.
Splitting up large programs into small, modular pieces was time consuming and boring.
It did not take long before someone thought of a way to turn the whole job over to the computer.
The method that was devised has come to be known as virtual memory (Fotheringham, 1961)
The basic idea behind virtual memory is that the combined size of the program, data, and stack may exceed the amount of physical memory available for it.
The operating system keeps those parts of the program currently in use in main memory, and the rest on the disk.
Virtual memory can also work in a multiprogramming system, with bits and pieces of many programs in memory at once.
While a program is waiting for part of itself to be brought in, it is waiting for I/O and cannot run, so the CPU can be given to another process, the same way as in any other multiprogramming system.
Most virtual memory systems use a technique called paging, which we will now describe.
On any computer, there exists a set of memory addresses that programs can produce.
Addresses can be generated using indexing, base registers, segment registers, and other ways.
Here the MMU is shown as being a part of the CPU chip because it commonly is nowadays.
However, logically it could be a separate chip and was in years gone by.
These program-generated addresses are called virtual addresses and form the virtual address space.
On computers without virtual memory, the virtual address is put directly onto the memory bus and causes the physical memory word with the same address to be read or written.
When virtual memory is used, the virtual addresses do not go directly to the memory bus.
Instead, they go to an MMU (Memory Management Unit) that maps the virtual addresses onto the physical memory addresses as illustrated in Fig.
A very simple example of how this mapping works is shown in Fig.
A complete copy of a program’s memory image, up to 64 KB, must be present on the disk, however, so that pieces can be brought in as needed.
The virtual address space is divided up into units called pages.
The corresponding units in the physical memory are called page frames.
The pages and page frames are always the same size.
Transfers between RAM and disk are always in units of a page.
When the program tries to access address 0, for example, using the instruction.
The memory knows nothing at all about the MMU and just sees a request for reading or writing address 8192, which it honors.
By itself, this ability to map the 16 virtual pages onto any of the eight page frames by setting the MMU’s map appropriately does not solve the problem that.
The relation between virtual addresses and physical memory addresses is given by the page table.
Since we have only eight physical page frames, only eight of the virtual pages in Fig.
The others, shown as crosses in the figure, are not mapped.
In the actual hardware, a present/absent bit keeps track of which pages are physically present in memory.
What happens if the program tries to use an unmapped page, for example, by using the instruction.
The operating system picks a little-used page frame and writes its contents back to the disk.
It then fetches the page just referenced into the page frame just freed, changes the map, and restarts the trapped instruction.
Virtual page = 2 is used as an index into the page table Incoming.
The page number is used as an index into the page table, yielding the number of the page frame corresponding to that virtual page.
If the present/absent bit is 0, a trap to the operating system is caused.
The output register is then put onto the memory bus as the physical memory address.
In the simplest case, the mapping of virtual addresses onto physical addresses is as we have just described it.
The virtual address is split into a virtual page number (high-order bits) and an offset (low-order bits)
The virtual page number is used as an index into the page table to find the entry for that virtual page.
From the page table entry, the page frame number (if any) is found.
The page frame number is attached to the high-order end of the offset, replacing the virtual page number, to form a physical address that can be sent to the memory.
The purpose of the page table is to map virtual pages onto page frames.
Mathematically speaking, the page table is a function, with the virtual page number as argument and the physical frame number as result.
Using the result of this function, the virtual page field in a virtual address can be replaced by a page frame field, thus forming a physical memory address.
Despite this simple description, two major issues must be faced:
The first point follows from the fact that modern computers use virtual addresses of at least 32 bits.
And remember that each process needs its own page table (because it has its own virtual address space)
A typical instruction has an instruction word, and often a memory operand as well.
Consequently, it is necessary to make one, two, or sometimes more page table references per instruction.
The need for large, fast page mapping is a significant constraint on the way computers are built.
Although the problem is most serious with top-of-the-line machines that must be very fast, it is also an issue at the low end as well, where cost and the price/performance ratio are critical In this section and the following ones, we will look at page table design in detail and show a number of hardware solutions that have been used in actual computers.
The simplest design (at least conceptually) is to have a single page table consisting of an array of fast hardware registers, with one entry for each virtual page, indexed by virtual page number, as shown in Fig.
When a process is started up, the operating system loads the registers with the process’ page table, taken from a copy kept in main memory.
During process execution, no more memory references are needed for the page table.
The advantages of this method are that it is straightforward and requires no memory references during mapping.
A disadvantage is that it is potentially expensive (if the page table is large)
Also, having to load the full page table at every context switch hurts performance.
At the other extreme, the page table can be entirely in main memory.
All the hardware needs then is a single register that points to the start of the page table.
This design allows the memory map to be changed at a context switch by reloading one register.
Of course, it has the disadvantage of requiring one or more memory references to read page table entries during the execution of each instruction.
For this reason, this approach is rarely used in its most pure form, but below we will study some variations that have much better performance.
To get around the problem of having to store huge page tables in memory all the time, many computers use a multilevel page table.
The secret to the multilevel page table method is to avoid keeping all the page tables in memory all the time.
In particular, those that are not needed should not be kept around.
In between the top of the data and the bottom of the stack is a gigantic hole that is not used.
When a virtual address is presented to the MMU, it first extracts the PT1 field and uses this value as an index into the top-level page table.
The entry located by indexing into the top-level page table yields the address or the page frame number of a second-level page table.
The PT2 field is now used as an index into the selected second-level page table to find the page frame number for the page itself.
This entry contains the page frame number of the page containing virtual address 0x00403004
If the page is in memory, the page frame number taken from the second-level page table is combined with the offset (4) to construct a physical address.
This address is put on the bus and sent to memory.
Should this occur, the operating system will notice that the process is trying to reference memory that it is not supposed to and will take appropriate action, such as sending it a signal or killing it.
Additional levels give more flexibility, but it is doubtful that the additional complexity is worth it beyond two levels.
Let us now turn from the structure of the page tables in the large, to the details of a single page table entry.
The exact layout of an entry is highly machine dependent, but the kind of information present is roughly the same from machine to machine.
The size varies from computer to computer, but 32 bits is a common size.
After all, the goal of the page mapping is to locate this value.
If this bit is 1, the entry is valid and can be used.
If it is 0, the virtual page to which the entry belongs is not currently in memory.
Accessing a page table entry with this bit set to 0 causes a page fault.
The protection bits tell what kinds of access are permitted.
A more sophisticated arrangement is having 3 independent bits, one bit each for individually enabling reading, writing, and executing the page.
The modified and referenced bits keep track of page usage.
When a page is written to, the hardware automatically sets the modified bit.
This bit is used when the operating system decides to reclaim a page frame.
If the page in it has been modified (i.e., is ‘‘dirty’’), it must be written back to the disk.
If it has not been modified (i.e., is ‘‘clean’’), it can just be abandoned, since the disk copy is still valid.
The bit is sometimes called the dirty bit, since it reflects the page’s state.
The referenced bit is set whenever a page is referenced, either for reading or writing.
Its value is to help the operating system choose a page to evict when a page fault occurs.
Pages that are not being used are better candidates than pages that are, and this bit plays an important role in several of the page replacement algorithms that we will study later in this chapter.
Finally, the last bit allows caching to be disabled for the page.
This feature is important for pages that map onto device registers rather than memory.
If the operating system is sitting in a tight loop waiting for some I/O device to respond to a command it was just given, it is essential that the hardware keep fetching the word from the device, and not use an old cached copy.
Machines that have a separate I/O space and do not use memory mapped I/O do not need this bit.
Note that the disk address used to hold the page when it is not in memory is not part of the page table.
The page table holds only that information the hardware needs to translate a virtual address to a physical address.
Information the operating system needs to handle page faults is kept in software tables inside the operating system.
In most paging schemes, the page tables are kept in memory, due to their large size.
Consider, for example, an instruction that copies one register to another.
In the absence of paging, this instruction makes only one memory reference, to fetch the instruction.
With paging, additional memory references will be needed to access the page table.
Since execution speed is generally limited by the rate the CPU can get instructions and data out of the memory, having to make two page table references per memory reference reduces performance by 2/3
Computer designers have known about this problem for years and have come up with a solution.
Their solution is based on the observation that most programs tend to make a large number of references to a small number of pages, and not the other way around.
Thus only a small fraction of the page table entries are heavily read; the rest are barely used at all.
This is an example of locality of reference, a concept we will come back to in a later section.
The solution that has been devised is to equip computers with a small hardware device for rapidly mapping virtual addresses to physical addresses without.
The device, called a TLB (Translation Lookaside Buffer) or sometimes an associative memory, is illustrated in Fig.
These fields have a one-to-one correspondence with the fields in the page table.
Another bit indicates whether the entry is valid (i.e., in use) or not.
Page 140 contains the indices used in the array calculations.
When a virtual address is presented to the MMU for translation, the hardware first checks to see if its virtual page number is present in the TLB by comparing it to all the entries simultaneously (i.e., in parallel)
If a valid match is found and the access does not violate the protection bits, the page frame is taken directly from the TLB, without going to the page table.
If the virtual page number is present in the TLB but the instruction is trying to write on a read-only page, a protection fault is generated, the same way as it would be from the page table itself.
The interesting case is what happens when the virtual page number is not in the TLB.
The MMU detects the miss and does an ordinary page table lookup.
It then evicts one of the entries from the TLB and replaces it with the page table entry just looked up.
Thus if that page is used again soon, the second time around it will result in a hit rather than a miss.
When the TLB is loaded from the page table, all the fields are taken from memory.
Up until now, we have assumed that every machine with paged virtual memory has page tables recognized by the hardware, plus a TLB.
In this design, TLB management and handling TLB faults are done entirely by the MMU hardware.
Traps to the operating system occur only when a page is not in memory.
However, many modern RISC machines, including the SPARC, MIPS, HP PA, and PowerPC, do nearly all of this page management in software.
On these machines, the TLB entries are explicitly loaded by the operating system.
When a TLB miss occurs, instead of the MMU just going to the page tables to find and fetch the needed page reference, it just generates a TLB fault and tosses the problem into the lap of the operating system.
The system must find the page, remove an entry from the TLB, enter the new one, and restart the instruction that faulted.
And, of course, all of this must be done in a handful of instructions because TLB misses occur much more frequently than page faults.
Surprisingly enough, if the TLB is reasonably large (say, 64 entries) to reduce the miss rate, software management of the TLB turns out to be acceptably efficient.
The main gain here is a much simpler MMU, which frees up a considerable amount of area on the CPU chip for caches and other features that can improve performance.
Various strategies have been developed to improve performance on machines that do TLB management in software.
One approach attacks both reducing TLB misses and reducing the cost of a TLB miss when it does occur (Bala et al., 1994)
To reduce TLB misses, sometimes the operating system can use its intuition to figure out which pages are likely to be used next and to preload entries for them in the TLB.
For example, when a client process sends a message to a server process on the same machine, it is very likely that the server will have to run soon.
Knowing this, while processing the trap to do the send, the system can also check to see where the server’s code, data, and stack pages are and map them in before they can cause TLB faults.
The normal way to process a TLB miss, whether in hardware or in software, is to go to the page table and perform the indexing operations to locate the page referenced.
The problem with doing this search in software is that the pages holding the page table may not be in the TLB, which will cause additional TLB faults during the processing.
These faults can be reduced by maintaining a large (e.g., 4-KB or larger) software cache of TLB entries in a fixed location whose page is always kept in the TLB.
By first checking the software cache, the operating system can substantially reduce the number of TLB misses.
Traditional page tables of the type described so far require one entry per virtual page, since they are indexed by virtual page number.
As a bare minimum, the page table will have to be at least 4 megabytes.
However, as 64-bit computers become more common, the situation changes drastically.
Tying up 30 million gigabytes just for the page table is not doable, not now and not for years to come, if ever.
Consequently, a different solution is needed for 64-bit paged virtual address spaces.
In this design, there is one entry per page frame in real memory, rather than one entry per page of virtual address space.
The entry keeps track of which (process, virtual page) is located in the page frame.
When process n references virtual page p, the hardware can no longer find the physical page by using p as an index into the page table.
Instead, it must search the entire inverted page table for an entry (n, p)
Furthermore, this search must be done on every memory reference, not just on page faults.
Searching a 64K table on every memory reference is definitely not a good way to make your machine blindingly fast.
The way out of this dilemma is to use the TLB.
If the TLB can hold all of the heavily used pages, translation can happen just as fast as with regular page tables.
On a TLB miss, however, the inverted page table has to be searched in software.
One feasible way to accomplish this search is to have a hash table hashed on the virtual address.
All the virtual pages currently in memory that have the same hash value are chained together, as shown in Fig.
If the hash table has as many slots as the machine has physical pages, the average chain will be only one entry long, greatly speeding up the mapping.
Once the page frame number has been found, the new (virtual, physical) pair is entered into the TLB and the faulting instruction restarted.
Inverted page tables are currently used on IBM, Sun, and Hewlett-Packard workstations and will become more common as 64-bit machines become widespread.
Some hardware issues in implementation of virtual memory are discussed by Jacob and Mudge (1998)
Traditional page table with an entry for each of the 252 pages.
Comparison of a traditional page table with an inverted page table.
When a page fault occurs, the operating system has to choose a page to remove from memory to make room for the page that has to be brought in.
If the page to be removed has been modified while in memory, it must be rewritten to the disk to bring the disk copy up to date.
If, however, the page has not been changed (e.g., it contains program text), the disk copy is already up to date, so no rewrite is needed.
The page to be read in just overwrites the page being evicted.
While it would be possible to pick a random page to evict at each page fault, system performance is much better if a page that is not heavily used is chosen.
If a heavily used page is removed, it will probably have to be brought back in quickly, resulting in extra overhead.
Much work has been done on the subject of page replacement algorithms, both theoretical and experimental.
Below we will describe some of the most important algorithms.
It is worth noting that the problem of ‘‘page replacement’’ occurs in other areas of computer design as well.
When the cache is full, some block has to be chosen for removal.
This problem is precisely the same as page replacement except on a shorter time scale (it has to be done in a few nanoseconds, not milliseconds as with page replacement)
The reason for the shorter time scale is that cache block misses are satisfied from main memory, which has no seek time and no rotational latency.
The browser keeps copies of previously accessed web pages in its cache on the disk.
Usually, the maximum cache size is fixed in advance, so the cache is likely to be full if the browser is used a.
Whenever a web page is referenced, a check is made to see if a copy is in the cache and if so, if the page on the web is newer.
If the cached copy is up to date, it is used; otherwise, a fresh copy is fetched from the Web.
If the page is not in the cache at all or a newer version is available, it is downloaded.
If it is a newer copy of a cached page it replaces the one in the cache.
When the cache is full a decision has to be made to evict some other page in the case of a new page or a page that is larger than an older version.
The considerations are similar to pages of virtual memory, except for the fact that the Web pages are never modified in the cache and thus are never written back to the web server.
In a virtual memory system, pages in main memory may be either clean or dirty.
The best possible page replacement algorithm is easy to describe but impossible to implement.
At the moment that a page fault occurs, some set of pages is in memory.
One of these pages will be referenced on the very next instruction (the page containing that instruction)
Each page can be labeled with the number of instructions that will be executed before that page is first referenced.
The optimal page algorithm simply says that the page with the highest label should be removed.
Computers, like people, try to put off unpleasant events for as long as they can.
The only problem with this algorithm is that it is unrealizable.
At the time of the page fault, the operating system has no way of knowing when each of the pages will be referenced next.
In this way it is possible to compare the performance of realizable algorithms with the best possible one.
To avoid any possible confusion, it should be made clear that this log of page references refers only to the one program just measured and then with only one specific input.
The page replacement algorithm derived from it is thus specific to that one program and input data.
Although this method is useful for evaluating page replacement algorithms, it is of no use in practical systems.
Below we will study algorithms that are useful on real systems.
In order to allow the operating system to collect useful statistics about which pages are being used and which ones are not, most computers with virtual memory have two status bits associated with each page.
The bits are contained in each page table entry, as shown in Fig.
It is important to realize that these bits must be updated on every memory reference, so it is essential that they be set by the hardware.
If the hardware does not have these bits, they can be simulated as follows.
When a process is started up, all of its page table entries are marked as not in memory.
As soon as any page is referenced, a page fault will occur.
The operating system then sets the R bit (in its internal tables), changes the page table entry to point to the correct page, with mode READ ONLY, and restarts the instruction.
If the page is subsequently written on, another page fault will occur, allowing the operating system to set the M bit as well and change the page’s mode to READ/WRITE.
The R and M bits can be used to build a simple paging algorithm as follows.
When a process is started up, both page bits for all its pages are set to 0 by the operating system.
Periodically (e.g., on each clock interrupt), the R bit is cleared, to distinguish pages that have not been referenced recently from those that have been.
When a page fault occurs, the operating system inspects all the pages and divides them into four categories based on the current values of their R and M bits:
Although class 1 pages seem, at first glance, impossible, they occur when a class M bit because this information is needed to know whether the page has to be rewritten to disk or not.
Clearing R but not M leads to a class 1 page.
The NRU (Not Recently Used) algorithm removes a page at random from the lowest numbered nonempty class.
Implicit in this algorithm is that it is better to remove a modified page that has not been referenced in at least one clock tick (typically 20 msec) than a clean page that is in heavy use.
The main attraction of NRU is that it is easy to understand, moderately efficient to implement, and gives a performance that, while certainly not optimal, may be adequate.
To illustrate how this works, consider a supermarket that has enough shelves to display exactly k different products.
One day, some company introduces a new convenience food—instant, freeze-dried, organic yogurt that can be reconstituted in a microwave oven.
It is an immediate success, so our finite supermarket has to get rid of one old product in order to stock it.
One possibility is to find the product that the supermarket has been stocking the longest (i.e., something it began selling 120 years ago) and get rid of it on the grounds that no one is interested any more.
In effect, the supermarket maintains a linked list of all the products it currently sells in the order they were introduced.
The new one goes on the back of the list; the one at the front of the list is dropped.
As a page replacement algorithm, the same idea is applicable.
The operating system maintains a list of all pages currently in memory, with the page at the head of the list the oldest one and the page at the tail the most recent arrival.
On a page fault, the page at the head is removed and the new page added to the tail of the list.
When applied to stores, FIFO might remove mustache wax, but it might also remove flour, salt, or butter.
For this reason, FIFO in its pure form is rarely used.
A simple modification to FIFO that avoids the problem of throwing out a.
If it is 0, the page is both old and unused, so it is replaced immediately.
If the R bit is 1, the bit is cleared, the page is put onto the end of the list of pages, and its load time is updated as though it had just arrived in memory.
The operation of this algorithm, called second chance, is shown in Fig.
The oldest page is A, which arrived at time 0, when the process started.
If A has the R bit cleared, it is evicted from memory, either by being written to the disk (if it is dirty), or just abandoned (if it is clean)
What second chance is doing is looking for an old page that has not been referenced in the previous clock interval.
If all the pages have been referenced, second chance degenerates into pure FIFO.
One by one, the operating system moves the pages to the end of the list, clearing the R bit each time it appends a page to the end of the list.
Eventually, it comes back to page A, which now has its R bit cleared.
Although second chance is a reasonable algorithm, it is unnecessarily inefficient because it is constantly moving pages around on its list.
A better approach is to keep all the page frames on a circular list in the form of a clock, as shown in Fig.
When a page fault occurs, the page the hand is pointing to is inspected.
When a page fault occurs, the page being pointed to by the hand is inspected.
If its R bit is 0, the page is evicted, the new page is inserted into the clock in its place, and the hand is advanced one position.
If R is 1, it is cleared and the hand is advanced to the next page.
It differs from second chance only in the implementation, not in the page selected.
A good approximation to the optimal algorithm is based on the observation.
Conversely, pages that have not been used for ages will probably remain unused for a long time.
This idea suggests a realizable algorithm: when a page fault occurs, throw out the page that has been unused for the longest time.
To fully implement LRU, it is necessary to maintain a linked list of all pages in memory, with the most recently used page at the front and the least recently used page at the rear.
The difficulty is that the list must be updated on every memory reference.
Finding a page in the list, deleting it, and then moving it to the front is a very timeconsuming operation, even in hardware (assuming that such hardware could be built)
However, there are other ways to implement LRU with special hardware.
This method requires equipping the hardware with a 64-bit counter, C, that is automatically incremented after each instruction.
Furthermore, each page table entry must also have a field large enough to contain the counter.
After each memory reference, the current value of C is stored in the page table entry for the page just referenced.
When a page fault occurs, the operating system examines all the counters in the page table to find the lowest one.
After page 0 is referenced, we have the situation of Fig.
After page 1 is referenced, we have the situation of Fig.
Although both of the previous LRU algorithms are realizable in principle, few, if any, machines have this hardware, so they are of little use to the operating system designer who is making a system for a machine that does not have this hardware.
Instead, a solution that can be implemented in software is needed.
One possible software solution is called the NFU (Not Frequently Used) algorithm.
It requires a software counter associated with each page, initially zero.
At each clock interrupt, the operating system scans all the pages in memory.
In effect, the counters are an attempt to keep track of how often each page has been referenced.
When a page fault occurs, the page with the lowest counter is chosen for replacement.
The main problem with NFU is that it never forgets anything.
For example, in a multipass compiler, pages that were heavily used during pass 1 may still have a high count well into later passes.
Thus the operating system will remove useful pages instead of pages no longer in use.
Fortunately, a small modification to NFU makes it able to simulate LRU quite well.
First, the counters are each shifted right 1 bit before the R bit is added in.
Second, the R bit is added to the leftmost, rather than the rightmost bit.
Figure 4-17 illustrates how the modified algorithm, known as aging, works.
After the six corresponding counters have been shifted and the R bit inserted at the left, they have the values shown in Fig.
The four remaining columns show the values of the six counters after the next four clock ticks, respectively.
The five clock ticks are represented by (a) to (e)
When a page fault occurs, the page whose counter is the lowest is removed.
It is clear that a page that has not been referenced for, say, four clock ticks will have four leading zeros in its counter and thus will have a lower value than a counter that has not been referenced for three clock ticks.
Neither has been referenced for two clock ticks; both were referenced in the tick prior to that.
According to LRU, if a page must be replaced, we should choose one of these two.
By recording only one bit per time interval, we have lost the ability to distinguish references early in the clock interval from those occurring later.
The second difference between LRU and aging is that in aging the counters have a finite number of bits, 8 bits in this example.
All we can do is pick one of them at random.
If a page has not been referenced in 160 msec, it probably is not that important.
In the previous sections we have explained how paging works and have given.
To design a system, you have to know a lot more to make it work well.
It is like the difference between knowing how to move the rook, knight, and other pieces in chess, and being a good player.
In the following sections, we will look at other issues that operating system designers must consider in order to get good performance from a paging system.
In the purest form of paging, processes are started up with none of their pages in memory.
As soon as the CPU tries to fetch the first instruction, it gets a page fault, causing the operating system to bring in the page containing the first instruction.
Other page faults for global variables and the stack usually follow quickly.
After a while, the process has most of the pages it needs and settles down to run with relatively few page faults.
This strategy is called demand paging because pages are loaded only on demand, not in advance.
Of course, it is easy enough to write a test program that systematically reads all the pages in a large address space, causing so many page faults that there is not enough memory to hold them all.
They exhibit a locality of reference, meaning that during any phase of execution, the process references only a relatively small fraction of its pages.
Each pass of a multipass compiler, for example, references only a fraction of the pages, and a different fraction at that.
The concept of locality of reference is widely applicable in computer science, for a history see Denning (2005)
If the entire working set is in memory, the process will run without causing many faults until it moves into another execution phase (e.g., the next pass of the compiler)
If the available memory is too small to hold the entire working set, the process will cause numerous page faults and run slowly since executing an instruction takes a few nanoseconds and reading in a page from the disk typically takes 10 milliseconds.
At a rate of one or two instructions per 10 milliseconds, it will take ages to finish.
A program causing page faults every few instructions is said to be thrashing (Denning, 1968b)
In a multiprogramming system, processes are frequently moved to disk (i.e., all their pages are removed from memory) to let other processes have a turn at the CPU.
The question arises of what to do when a process is brought back in again.
The process will just cause page faults until its working set has been loaded.
Therefore, many paging systems try to keep track of each process’ working set and make sure that it is in memory before letting the process run.
This approach is called the working set model (Denning, 1970)
It is designed to greatly reduce the page fault rate.
Loading the pages before letting processes run is also called prepaging.
It has long been known that most programs do not reference their address space uniformly Instead the references tend to cluster on a small number of pages.
A memory reference may fetch an instruction, it may fetch data, or it may store data.
At any instant of time, t, there exists a set consisting of all the pages used by the k most recent memory references.
Because a larger value of k means looking further into the past, the number of pages counted as part of the working set cannot decrease as k is made larger.
So w(k, t) is a monotonically nondecreasing function of k.
The limit of w(k, t) as k becomes large is finite because a program cannot reference more pages than its address space contains, and few programs will use every single page.
Figure 4-18 depicts the size of the working set as a function of k.
The working set is the set of pages used by the k most recent memory references.
The function w(k, t) is the size of the working set at time t.
The fact that most programs randomly access a small number of pages, but that this set changes slowly in time explains the initial rapid rise of the curve and then the slow rise for large k.
For example, a program that is executing a loop occupying two pages using data on four pages, may reference all six pages every 1000 instructions, but the most recent reference to some other page may be a million instructions earlier, during the initialization phase.
Due to this asymptotic behavior, the contents of the working set is not sensitive to the value of k chosen.
To put it differently, there exists a wide range of k values for which the working set is unchanged.
Because the working set varies slowly with time, it is possible to make a reasonable guess as to which pages will be needed when the program is restarted on the basis of its working set when it was last stopped.
Prepaging consists of loading these pages before the process is allowed to run again.
To implement the working set model, it is necessary for the operating system to keep track of which pages are in the working set.
One way to monitor this information is to use the aging algorithm discussed above.
Any page containing a 1 bit among the high order n bits of the counter is considered to be a member of the working set.
If a page has not been referenced in n consecutive clock ticks, it is dropped from the working set.
The parameter n has to be determined experimentally for each system, but the system performance is usually not especially sensitive to the exact value.
Information about the working set can be used to improve the performance of the clock algorithm.
Normally, when the hand points to a page whose R bit is 0, the page is evicted.
The improvement is to check to see if that page is part of the working set of the current process.
In the preceding sections we have discussed several algorithms for choosing a page to replace when a fault occurs.
A major issue associated with this choice (which we have carefully swept under the rug until now) is how memory should be allocated among the competing runnable processes.
In this figure, three processes, A, B, and C, make up the set of runnable processes.
Should the page replacement algorithm try to find the least recently used page considering only the six pages currently allocated to A, or should it consider all the pages in memory? If it looks only at A’s pages, the page with the lowest age value is A5, so we get the situation of Fig.
On the other hand, if the page with the lowest age value is removed without regard to whose page it is, page B3 will be chosen and we will get the situation of Fig.
Local algorithms effectively correspond to allocating every process a fixed fraction of the memory.
Global algorithms dynamically allocate page frames among the runnable processes.
Thus the number of page frames assigned to each process varies in time.
In general, global algorithms work better, especially when the working set size can vary over the lifetime of a process.
If a local algorithm is used and the working set grows, thrashing will result, even if there are plenty of free page frames.
If a global algorithm is used, the system must continually decide how many page frames to assign to each process.
One way is to monitor the working set size as indicated by the aging bits, but this approach does not necessarily prevent thrashing.
The working set may change size in microseconds, whereas the aging bits are a crude measure spread over a number of clock ticks.
Another approach is to have an algorithm for allocating page frames to processes.
One way is to periodically determine the number of running processes and allocate each process an equal share.
The remaining 6 go into a pool to be used when page faults occur.
It is probably wise to give each process some minimum number, so it can run, no matter how small it is.
On some machines, for example, a single two-operand instruction may need as many as six pages because the instruction itself, the source operand, and the destination operand may all straddle page boundaries.
With an allocation of only five pages, programs containing such instructions cannot execute at all.
If a global algorithm is used, it may be possible to start each process up with some number of pages proportional to the process’ size, but the allocation has to be updated dynamically as the processes run.
One way to manage the allocation is to use the PFF (Page Fault Frequency) algorithm.
It tells when to increase or decrease a process’ page allocation but says nothing about which page to replace on a fault.
For a large class of page replacement algorithms, including LRU, it is known that the fault rate decreases as more pages are assigned, as we discussed above.
Page fault rate as a function of the number of page frames assigned.
Measuring the page fault rate is straightforward: just count the number of faults per second, possibly taking a running mean over past seconds as well.
One easy way to do this is to add the present second’s value to the current running mean and divide by two.
The dashed line marked A corresponds to a page fault rate that is unacceptably high, so the faulting process is given more page frames to reduce the fault rate.
The dashed line marked B corresponds to a page fault rate so low that it can be concluded that the process has too much memory.
In this case, page frames may be taken away from it.
Thus, PFF tries to keep the paging rate for each process within acceptable bounds.
If it discovers that there are so many processes in memory that it is not possible to keep all of them below A, then some process is removed from memory, and its page frames are divided up among the remaining processes or put into a pool of available pages that can be used on subsequent page faults.
The decision to remove a process from memory is a form of load control.
It shows that even with paging, swapping is still needed, only now swapping is used to reduce potential demand for memory, rather than to reclaim blocks of it for immediate use.
Swapping processes out to relieve the load on memory is reminiscent of two-level scheduling, in which some processes are put on disk and a short-term scheduler is used to schedule the remaining processes.
Clearly, the two ideas can be combined, with just enough processes swapped out to make the page-fault rate acceptable.
The page size is often a parameter that can be chosen by the operating system.
Determining the best page size requires balancing several competing factors.
A randomly chosen text, data, or stack segment will not fill an integral number of pages.
On the average, half of the final page will be empty.
With n segments in memory and a page size of p bytes, np /2 bytes will be wasted on internal fragmentation.
Another argument for a small page size becomes apparent if we think about a program consisting of eight sequential phases of 4 KB each.
In general, a large page size will cause more unused program to be in memory than a small page size.
On some machines, the page table must be loaded into hardware registers every time the CPU switches from one process to another.
On these machines having a small page size means that the time required to load the page registers gets longer as the page size gets smaller.
Furthermore, the space occupied by the page table increases as the page size decreases.
Let the average process size be s bytes and the page size be p bytes.
The approximate number of pages needed per process is then s /p, occupying se /p bytes of page table space.
The wasted memory in the last page of the process due to internal fragmentation is p /2
Thus, the total overhead due to the page table and the internal fragmentation loss is given by the sum of these two terms:
By taking the first derivative with respect to p and equating it to zero, we get the equation.
From this equation we can derive a formula that gives the optimum page size (considering only memory wasted in fragmentation and page table size)
As memories get larger, the page size tends to get larger as well (but not linearly)
Quadrupling the RAM size rarely even doubles the page size.
Up until now, our whole discussion has assumed that virtual memory is transparent to processes and programmers.
That is, all they see is a large virtual address space on a computer with a small(er) physical memory.
With many systems, that is true, but in some advanced systems, programmers have some control over the memory map and can use it in nontraditional ways to enhance program behavior.
In this section, we will briefly look at a few of these.
One reason for giving programmers control over their memory map is to allow two or more processes to share the same memory.
If programmers can name regions of their memory, it may be possible for one process to give another process the name of a memory region so that process can also map it in.
With two (or more) processes sharing the same pages, high bandwidth sharing becomes possible: one process writes into the shared memory and another one reads from it.
Sharing of pages can also be used to implement a high-performance message-passing system.
Normally, when messages are passed, the data are copied from one address space to another, at considerable cost.
If processes can control their page map, a message can be passed by having the sending process unmap the page(s) containing the message, and the receiving process mapping them in.
Here only the page names have to be copied, instead of all the data.
The idea here is to allow multiple processes over a network to share a set of pages, possibly, but not necessarily, as a single shared linear address space.
When a process references a page that is not currently mapped in, it gets a page fault.
The page fault handler, which may be in the kernel or in user space, then locates the machine holding the page and sends it a message asking it to unmap the page and send it over the network.
When the page arrives, it is mapped in and the faulting instruction is restarted.
The virtual memory discussed so far is one-dimensional because the virtual addresses go from 0 to some maximum address, one address after another.
For many problems, having two or more separate virtual address spaces may be much.
For example, a compiler has many tables that are built up as compilation proceeds, possibly including.
The source text being saved for the printed listing (on batch systems)
The symbol table, containing the names and attributes of variables.
The table containing all the integer and floating-point constants used.
The parse tree, containing the syntactic analysis of the program.
Each of the first four tables grows continuously as compilation proceeds.
The last one grows and shrinks in unpredictable ways during compilation.
In a onedimensional memory, these five tables would have to be allocated contiguous chunks of virtual address space, as in Fig.
Symbol table Symbol table has bumped into the source text table.
In a one-dimensional address space with growing tables, one table may bump into another.
Consider what happens if a program has an exceptionally large number of variables but a normal amount of everything else.
The chunk of address space allocated for the symbol table may fill up, but there may be lots of room in the other tables.
The compiler could, of course, simply issue a message saying that the compilation cannot continue due to too many variables, but doing so does not seem very sporting when unused space is left in the other tables.
Another possibility is to play Robin Hood, taking space from the tables with an excess of room and giving it to the tables with little room.
This shuffling can be done, but it is analogous to managing one’s own overlays—a nuisance at best and a great deal of tedious, unrewarding work at worst.
What is really needed is a way of freeing the programmer from having to manage the expanding and contracting tables, in the same way that virtual memory eliminates the worry of organizing the program into overlays.
A straightforward and extremely general solution is to provide the machine with many completely independent address spaces, called segments.
Each segment consists of a linear sequence of addresses, from 0 to some maximum.
The length of each segment may be anything from 0 to the maximum allowed.
The length of a stack segment may be increased whenever something is pushed onto the stack and decreased whenever something is popped off the stack.
Because each segment constitutes a separate address space, different segments can grow or shrink independently, without affecting each other.
If a stack in a certain segment needs more address space to grow, it can have it, because there is nothing else in its address space to bump into.
Of course, a segment can fill up but segments are usually very large, so this occurrence is rare.
To specify an address in this segmented or two-dimensional memory, the program must supply a two-part address, a segment number, and an address within the segment.
Figure 4-22 illustrates a segmented memory being used for the compiler tables discussed earlier.
A segmented memory allows each table to grow or shrink independently of the other tables.
We emphasize that in its purest form, a segment is a logical entity, which the programmer is aware of and uses as a logical entity.
A segment might contain one or more procedures, or an array, or a stack, or a collection of scalar variables, but usually it does not contain a mixture of different types.
A segmented memory has other advantages besides simplifying the handling of data structures that are growing or shrinking.
If each procedure occupies a separate segment, with address 0 as its starting address, the linking up of procedures compiled separately is greatly simplified.
If the procedure in segment n is subsequently modified and recompiled, no other procedures need be changed (because no starting addresses have been modified), even if the new version is larger than the old one.
With a one-dimensional memory, the procedures are packed tightly next to each other, with no address space between them.
Consequently, changing one procedure’s size can affect the starting address of other, unrelated procedures.
This, in turn, requires modifying all procedures that call any of the moved procedures, in order to incorporate their new starting addresses.
If a program contains hundreds of procedures, this process can be costly.
Segmentation also facilitates sharing procedures or data between several processes.
Modern workstations that run advanced window systems often have extremely large graphical libraries compiled into nearly every program.
In a segmented system, the graphical library can be put in a segment and shared by multiple processes, eliminating the need for having it in every process’ address space.
While it is also possible to have shared libraries in pure paging systems, it is much more complicated.
Because each segment forms a logical entity of which the programmer is aware, such as a procedure, or an array, or a stack, different segments can have different kinds of protection.
A procedure segment can be specified as execute only, prohibiting attempts to read from it or store into it.
A floating-point array can be specified as read/write but not execute, and attempts to jump to it will be caught.
You should try to understand why protection makes sense in a segmented memory but not in a one-dimensional paged memory.
In a segmented memory the user is aware of what is in each segment.
Normally, a segment would not contain a procedure and a stack, for example, but one or the other.
Since each segment contains only one type of object, the segment can have the protection appropriate for that particular type.
The contents of a page are, in a certain sense, accidental.
The programmer is unaware of the fact that paging is even occurring.
Although putting a few bits in each entry of the page table to specify the access allowed would be possible, to utilize this feature the programmer would have to keep track of where in his address space all the page boundaries were.
However, that is precisely the sort of complex administration that paging was invented to eliminate.
Need the programmer be aware that this technique is being used?
Can the total address space exceed the size of physical memory?
To get a large linear address space without having to buy more physical memory.
To allow programs and data to be broken up into logically independent address spaces and to aid sharing and protection.
The implementation of segmentation differs from paging in an essential way: pages are fixed size and segments are not.
Figure 4-24(a) shows an example of physical memory initially containing five segments.
After the system has been running for a while, memory will be divided up into a number of chunks, some containing segments and some containing holes.
This phenomenon, called checkerboarding or external fragmentation, wastes memory in the holes.
It can be dealt with by compaction, as shown in Fig.
The Pentium can be set up (by the operating system) to use only segmentation, only paging, or both.
Most operating systems, including Windows XP and all flavors of UNIX, use the pure paging model, in which each process has a single segment of 232 bytes.
Since the Pentium is capable of providing processes with a much larger address space, and one operating system (OS/2) did actually use the full power of the addressing, we will describe how Pentium virtual memory works in its full generality.
The heart of the Pentium virtual memory consists of two tables, the LDT (Local Descriptor Table) and the GDT (Global Descriptor Table)
Each program has its own LDT, but there is a single GDT, shared by all the programs on the computer.
The LDT describes segments local to each program, including its code, data, stack, and so on, whereas the GDT describes system segments, including the operating system itself.
To access a segment, a Pentium program first loads a selector for that segment into one of the machine’s six segment registers.
During execution, the CS register holds the selector for the code segment and the DS register holds the selector for the data segment.
Each selector is a 16-bit number, as shown in Fig.
One of the selector bits tells whether the segment is local or global (i.e., whether it is in the LDT or GDT)
Thirteen other bits specify the LDT or GDT entry number; thus tables are each restricted to holding 8K segment descriptors.
The other 2 bits relate to protection, and will be described later.
It may be safely loaded into a segment register to indicate that the segment register is not currently available.
At the time a selector is loaded into a segment register, the corresponding descriptor is fetched from the LDT or GDT and stored in microprogram registers, so it can be accessed quickly.
A descriptor consists of 8 bytes, including the segment’s base address, size, and other information, as depicted in Fig.
The format of the selector has been cleverly chosen to make locating the descriptor easy.
Finally, the address of either the LDT or GDT table is added to it, to give a direct pointer to the descriptor.
Let us trace the steps by which a (selector, offset) pair is converted to a physical address.
As soon as the microprogram knows which segment register is being used, it can find the complete descriptor corresponding to that selector in its internal registers.
If the segment does not exist (selector 0), or is currently paged out, a trap occurs.
It then checks to see if the offset is beyond the end of the segment, in which case a trap also occurs.
Logically, there should simply be a 32-bit field in the.
If it is 1, the limit field gives the segment size in pages instead of bytes.
Assuming that the segment is in memory and the offset is in range, the Pentium then adds the 32-bit base field in the descriptor to the offset to form what is called a linear address, as shown in Fig.
In effect, the base field allows each segment to start at an arbitrary place within the 32-bit linear address space.
Conversion of a (selector, offset) pair to a linear address.
If paging is disabled (by a bit in a global control register), the linear address is interpreted as the physical address and sent to the memory for the read or write.
Thus with paging disabled, we have a pure segmentation scheme, with each segment’s base address given in its descriptor.
Segments are permitted to overlap, incidentally, probably because it would be too much trouble and take too much time to verify that they were all disjoint.
On the other hand, if paging is enabled, the linear address is interpreted as a virtual address and mapped onto the physical address using page tables, pretty much as in our earlier examples.
It is located at an address pointed to by a global register.
The dir field is used to index into the page directory to locate a pointer to the proper page table.
Then the page field is used as an index into the page table.
Finally, offset is added to the address of the page frame to get the physical address of the byte or word needed.
The remaining bits contain access and dirty bits, set by the hardware for the benefit of the operating system, protection bits, and other utility bits.
A segment shorter than 4-MB will have a page directory with a single entry, a pointer to its one and only page table.
In this way, the overhead for short segments is only two pages, instead of the million pages that would be needed in a one-level page table.
A little thought will reveal the fact that when paging is used, there is really no point in having the base field in the descriptor be nonzero.
All that base does is cause a small offset to use an entry in the middle of the page directory, instead of at the beginning.
It is also worth noting that if some application does not need segmentation but is content with a single, paged, 32-bit address space, that model is possible.
All the segment registers can be set up with the same selector, whose descriptor has base = 0 and limit set to the maximum.
The instruction offset will then be the linear address, with only a single address space used—in effect, normal paging.
In fact, all current operating systems for the Pentium work this way.
OS/2 was the only one that used the full power of the Intel MMU architecture.
All in all, one has to give credit to the Pentium designers.
Given the conflicting goals of implementing pure paging, pure segmentation, and paged segments, while at the same time being compatible with the 286, and doing all of this efficiently, the resulting design is surprisingly simple and clean.
Although we have covered the complete architecture of the Pentium virtual memory, albeit briefly, it is worth saying a few words about protection, since this subject is intimately related to the virtual memory.
At each instant, a running program is at a certain level, indicated by a 2-bit field in its PSW.
As long as a program restricts itself to using segments at its own level, everything works fine.
Attempts to access data at a higher level are permitted.
Attempts to access data at a lower level are illegal and cause traps.
Attempts to call procedures at a different level (higher or lower) are allowed, but in a carefully controlled way.
To make an interlevel call, the CALL instruction must contain a selector instead of an address.
This selector designates a descriptor called a call gate, which gives the address of the procedure to be called.
A typical use for this mechanism is suggested in Fig.
At level 0, we find the kernel of the operating system, which handles I/O, memory management, and other critical matters.
User programs may call procedures here to have system calls carried out, but only a specific and protected list of procedures may be called.
Level 2 contains library procedures, possibly shared among many running programs.
User programs may call these procedures and read their data, but they may not modify them.
Finally, user programs run at level 3, which has the least protection.
Traps and interrupts use a mechanism similar to the call gates.
They, too, reference descriptors, rather than absolute addresses, and these descriptors point to specific procedures to be executed.
Memory management in MINIX 3 is simple: paging is not used at all.
Swapping code is available in the complete source and could be activated to make MINIX 3 work on a system with limited physical memory.
In practice, memories are so large now that swapping is rarely needed.
In this chapter we will study a user-space server designated the process manager, or PM for short.
The process manager handles system calls relating to process management.
The fork, exec, and brk calls are in this category.
Process management also includes processing system calls related to signals, setting and examining process properties such as user and group ownership, and reporting CPU usage times.
The MINIX 3 process manager also handles setting and querying the real time clock.
The PM maintains a list of holes sorted in numerical memory address order.
When memory is needed, either due to a fork or an exec system call, the hole list is searched using first fit for a hole that is big enough.
Without swapping, a process that has been placed in memory remains in exactly the same place during its entire execution.
It is never moved to another place in memory, nor does its allocated memory area ever grow or shrink.
This strategy for managing memory is somewhat unusual and deserves some explanation.
First, as a teaching system, avoiding complexity was highly desirable; a source code listing of nearly 250 pages was deemed long enough.
Second, the system was designed for the original IBM PC, which did not even have an MMU, so including paging was impossible to start with.
Third, since other computers of its era also lacked MMUs, this memory management strategy made porting to the Macintosh, Atari, Amiga, and other machines easier.
Of course, one can rightly ask if such a strategy still makes sense.
The first point is still valid, although the system has definitely grown over the years.
Modern PCs have more than 1000 times as much memory available as the original IBM PC.
While programs are bigger, most systems have so much memory that swapping and paging are hardly needed.
Finally, MINIX 3 is targeted to some extent at low-end systems such as embedded systems.
Nowadays, digital cameras, DVD players, stereos, cell phones, and other products have operating systems, but certainly do not support swapping or paging.
Nevertheless, some work is in progress to see what can be done in the area of virtual memory in the simplest possible way.
The Web site should be consulted to follow current developments.
It is also worth pointing out another way in which implementation of memory management in MINIX 3 differs from that of many other operating systems.
Instead, it is a process that runs in user space and communicates with the kernel by the standard message mechanism.
Moving the PM out of the kernel is an example of the separation of policy and mechanism.
The decisions about which process will be placed where in memory (policy) are made by the PM.
The actual setting of memory maps for processes (mechanism) is done by the system task within the kernel.
Most of the PM code is devoted to handling the MINIX 3 system calls that involve creating processes, primarily fork and exec, rather than just manipulating lists of processes and holes.
In the next section we will look at the memory layout, and in subsequent sections we will take a bird’s-eye view of how the process management system calls are handled by the PM.
This was the default for the original version of MINIX.
In MINIX 3, however, the default is to compile programs to use separate I and D space.
For clarity, allocation of memory for the simpler combined model will be discussed first.
Processes using separate I and D space can use memory more efficiently, but taking advantage of this feature complicates things.
We will discuss the complications after the simple case has been outlined.
In normal MINIX 3 operation memory is allocated on two occasions.
First, when a process forks, the amount of memory needed by the child is allocated.
Second, when a process changes its memory image via the exec system call, the space occupied by the old image is returned to the free list as a hole, and memory is allocated for the new image.
The new image may be in a part of memory different from the released memory.
Its location will depend upon where an adequate hole is found.
Memory is also released whenever a process terminates, either by exiting or by being killed by a signal.
There is a third case: a system process can request memory for its own use; for instance, the memory driver can request memory for the RAM disk.
Figure 4-30 shows memory allocation during a fork and an exec.
If the child now execs the file C, the memory looks like Fig.
Note that the old memory for the child is released before the new memory for C is allocated, so that C can use the child’s memory.
Holes would remain if the new memory had been allocated before the old memory had been released.
Consider the possible error condition that there is not enough memory to perform an exec.
A test for sufficient memory to complete the operation should be performed before the child’s memory is released, so the child can respond to the error somehow.
This means the child’s memory must be considered as if it were a hole while it is still in use.
When memory is allocated, either by the fork or exec system calls, a certain amount of it is taken for the new process.
In the former case, the amount taken is identical to what the parent process has.
In the latter case, the PM takes the amount specified in the header of the file executed.
Once this allocation has been made, under no conditions is the process ever allocated any more total memory.
What has been said so far applies to programs that have been compiled with combined I and D space.
Programs with separate I and D space take advantage of an enhanced mode of memory management called shared text.
When such a process does a fork, only the amount of memory needed for a copy of the new process’ data and stack is allocated.
Both the parent and the child share the executable code already in use by the parent.
When such a process does an exec, the process table is searched to see if another process is already using the executable code needed.
If one is found, new memory is allocated only for the data and stack, and the text already in memory is shared.
When a process terminates it always releases the memory occupied by its data and stack.
But it only releases the memory occupied by its text segment after a search of the process table reveals that no other current process is sharing that memory.
Thus a process may be allocated more memory when it starts than it releases when it terminates, if it loaded its own text when it started but that text is being shared by one or more other processes when the first process terminates.
The header on the disk file contains information about the sizes of the different parts of the image, as well as the total size.
In the header of a program with common I and D space, a field specifies the total size of the text and data parts; these parts are copied directly to the memory image.
The data part in the image is enlarged by the amount specified in the bss field in the header.
This area is cleared to contain all zeroes and is used for uninitialized static data.
The total amount of memory to be allocated is specified by the total field in the header.
A program file on the disk may also contain a symbol table.
This is for use in debugging and is not copied into memory.
Data segment grows upward (or downward) when BRK calls are made.
In both parts of the figure the lowest disk or memory address is at the bottom and the highest address is at the top.
If the programmer knows that the total memory needed for the combined growth of the data and stack segments for the file a.out is at most 10 KB, he can give the command.
For the above example, a total of 16 KB will be allocated on all subsequent execs of the file.
For a program using separate I and D space (indicated by a bit in the header that is set by the linker), the total field in the header applies to the combined data and stack space only.
The boundary of the data segment can be moved only by the brk system call.
All brk does is check to see if the new data segment bumps into the current stack pointer, and if not, notes the change in some internal tables.
This is entirely internal to the memory originally allocated to the process; no additional memory is allocated by the operating system.
If the new data segment bumps into the stack, the call fails.
This is a good place to mention a possible semantic difficulty.
When we use the word ‘‘segment,’’ we refer to an area of memory defined by the operating system.
Hardware designers often try to provide support for the operating systems that they expect to be used on their machines, and the terminology used to describe registers and other aspects of a processor’s architecture usually reflects an idea of how the features will be used.
Such features are often useful to the implementer of an operating system, but they may not be used in the same way the hardware designer foresaw.
This can lead to misunderstandings when the same word has different meanings when used to describe an aspect of an operating system or of the underlying hardware.
Like all the other components of MINIX 3, the process manager is message driven.
After the system has been initialized, PM enters its main loop, which consists of waiting for a message, carrying out the request contained in the message, and sending a reply.
Two message categories may be received by the process manager.
For high priority communication between the kernel and system servers such as PM, a system notification message is used.
These are special cases to be discussed in the implementation section of this chapter.
The majority of messages received by the process manager result from system calls originated by user processes.
For this category, Figure 4-32 gives the list of legal message types, input parameters, and values sent back in the reply message.
Fork, exit, wait, waitpid, brk, and exec are clearly closely related to memory allocation and deallocation.
The calls kill, alarm, and pause are all related to signals, as are sigaction, sigsuspend, sigpending, sigmask, and sigreturn.
These also can affect what is in memory, because when a signal kills a process the memory used by that process is deallocated.
The seven get/set calls have nothing to do with memory management at all, but they certainly relate to process management.
Other calls could go either in the file system or the PM, since every system call is handled by one or the other.
They were put here simply because the file system was large enough already.
The time, stime, and times calls were put here for this reason, as was ptrace, which is used in debugging.
Reboot has effects throughout the operating system, but its first job is to send signals to terminate all processes in a controlled way, so the PM is a good place for it.
The same is true of svrctl, whose most important use is to enable or disable swapping in the PM.
You may have noticed that the last two calls mentioned here, reboot and svrctl, were not listed in Fig.
None of these are intended for use by ordinary user processes, and they are not parts of the POSIX standard.
They are provided because they are needed in a system like.
In a system with a monolithic kernel the operations provided by these calls could be provided by calls to functions compiled into the kernel.
But in MINIX 3 components that are normally considered part of the operating system run in user space, and additional system calls are needed.
Some of these do little more than implement an interface to a kernel call, a term we use for calls that request kernel services via the system task.
The library routine computes the amount of memory needed by adding the increment or decrement specified as parameter to the current size and makes a brk call to set the size.
Similarly, there are no separate system calls for geteuid and getegid.
The calls getuid and getgid return both the effective and real identifiers.
In like manner, getpid returns the PID of both the calling process and its parent.
Two key data structures are used by the process manager: the process table and the hole table.
We will now look at each of these in turn.
In MINIX 3, each of these three pieces of the operating system has its own process table, containing just those fields that it needs.
With a few exceptions, entries correspond exactly, to keep things simple.
Thus, slot k of the PM’s table refers to the same process as slot k of the file system’s table.
When a process is created or destroyed, all three parts update their tables to reflect the new situation, in order to keep them synchronized.
The exceptions are processes that are not known outside of the kernel, either because they are compiled into the kernel, like the CLOCK and SYSTEM tasks, or because they are place holders like IDLE, and KERNEL.
In the kernel process table their slots are designated by negative numbers.
These slots do not exist in the process manager or file system process tables.
Thus, strictly speaking, what was said above about slot k in the tables is true for k equal to or greater than zero.
The message types, input parameters, and reply values used for communicating with the PM.
The method used for recording memory allocation is shown in Fig.
In this model, the text segment is always empty, and the data segment contains both text and data.
Note that the virtual address at which the stack begins depends initially on the total amount of memory allocated to the process.
Note that, in this example, growth of the stack by one click would reduce the gap to nothing if there were no increase of the total memory allocation.
Thus, this change will not be made until the next brk system call, at which point the operating system explicitly reads SP and recomputes the segment entries.
On a machine with a stack trap, the stack segment’s entry could be updated as soon as the stack outgrew its segment.
We mentioned previously that the efforts of hardware designers may not always produce exactly what the software designer needs.
Even in protected mode on a Pentium, MINIX 3 does not trap when the stack outgrows its segment.
The MINIX 3 data and stack segments each use part of this space, and thus either or both can expand into the gap between them.
The CPU has no way to detect errors involving the gap, since as far as the hardware is concerned the gap is a valid part of both the data area and the stack area.
Of course, the hardware can detect a very large error, such as an attempt to access memory outside the combined data-gapstack area.
This will protect one process from the mistakes of another process but is not enough to protect a process from itself.
We recognize an argument can be made for abandoning the shared hardware-defined segment that allows MINIX 3 to dynamically reallocate the gap area.
The alternative, using the hardware to define nonoverlapping stack and data segments, would offer somewhat more security from certain errors but would make MINIX 3 more memory-hungry.
The source code is available to anybody who wants to evaluate the other approach.
In addition to the segment information, mproc also holds additional information about the process.
This includes the process ID (PID) of the process itself and of its parent, the UIDs and GIDs (both real and effective), information about signals, and the exit status, if the process has already terminated but its parent has not yet done a wait for it.
Also in mproc there are fields for a timer for sigalarm and for accumulated user and system time use by child processes.
The gaps between the data and stack segments are not considered holes; they have already been allocated to processes.
Consequently, they are not contained in the free hole list.
Each hole list entry has three fields: the base address of the hole, in clicks; the length of the hole, in clicks; and a pointer to the next entry on the list.
The list is singly linked, so it is easy to find the next hole starting from any given hole, but to find the previous hole, you have to search the entire list from the beginning until you come to the given hole.
Because of space limitations alloc.c is not included in the printed listing although it is on the CD-ROM.
But the code defining the hole list is simple, and is shown in Fig.
The principal operations on the hole list are allocating a piece of memory of a given size and returning an existing allocation.
To allocate memory, the hole list is searched, starting at the hole with the lowest address, until a hole that is large enough is found (first fit)
The segment is then allocated by reducing the hole by the amount needed for the segment, or in the rare case of an exact fit, removing the hole from the list.
This scheme is fast and simple but suffers from both a small amount of internal fragmentation (up to 1023 bytes may be wasted in the final click, since an integral number of clicks is always taken) and external fragmentation.
When a process terminates and is cleaned up, its data and stack memory are returned to the free list.
If it uses combined I and D, this releases all its memory, since such programs never have a separate allocation of memory for text.
If the program uses separate I and D and a search of the process table reveals no other process is sharing the text, the text allocation will also be returned.
Since with shared text the text and data regions are not necessarily contiguous, two regions of memory may be returned.
For each region returned, if either or both of the region’s neighbors are holes, they are merged, so adjacent holes never occur.
In this way, the number, location, and sizes of the holes vary continuously during system operation.
Whenever all user processes have terminated, all of available memory is once again ready for allocation.
When processes are created or destroyed, memory must be allocated or deallocated.
Also, the process table must be updated, including the parts held by the kernel and FS.
Process creation is done by fork, and carried out in the series of steps shown in Fig.
It is difficult and inconvenient to stop a fork call part way through, so the PM maintains a count at all times of the number of processes currently in existence in order to see easily if a process table slot is available.
If the table is not full, an attempt is made to allocate memory for the child.
If the program is one with separate I and D space, only enough memory for new data and stack allocations is requested.
If this step also succeeds, the fork is guaranteed to work.
The newly allocated memory is then filled in, a process slot is located and filled in, a PID is chosen, and the other parts of the system are informed that a new process has been created.
Try to allocate memory for the child’s data and stack.
Copy the parent’s data and stack to the child’s memory.
Find a free process slot and copy parent’s slot to it.
The steps required to carry out the fork system call.
It is prevented from being scheduled and has its alarm timer turned off (if it was on), but it is not removed from the process table.
When the parent finally does the wait, the process table slot is freed, and the file system and kernel are informed.
A problem arises if the parent of an exiting process is itself already dead.
If no special action were taken, the exiting process would remain a zombie forever.
Instead, the tables are changed to make it a child of the init process.
When the system comes up, init reads the /etc/ttytab file to get a list of all terminals, and then forks off a login process to handle each one.
When a command is typed at the terminal, the shell forks off a new process, which then executes the command requested.
It would have been possible to have a single system call to do both fork and exec at once, but they were provided as two distinct calls for a very good reason: to make it easy to implement I/O redirection.
When the shell forks, if standard input is redirected, the child closes standard input and then opens the new standard input before executing the command.
In this way the newly started process inherits the redirected standard input.
It must replace the current memory image with a new one, including setting up a new stack.
The new image must be a binary executable file, of course.
An executable file may also be a script that must be interpreted by another program, such as the shell or perl.
In that case the file whose image must be placed in memory is the binary of the interpreter, with the name of the script as an argument.
Later, when we discuss implementation of exec, the additional processing required to execute a script will be described.
Exec carries out its job in a series of steps, as shown in Fig.
Read the header to get the segment and total sizes.
Copy data (and possibly text) segment to new memory image.
The steps required to carry out the exec system call.
Each step consists, in turn, of yet smaller steps, some of which can fail.
The order in which the tests are made has been carefully chosen to make sure the old memory image is not released until it is certain that the exec will succeed, to avoid the embarrassing situation of not being able to set up a new memory image, but not having the old one to go back to, either.
Normally exec does not return, but if it fails, the calling process must get control again, with an error indication.
First is the question of whether or not there is enough room.
After determining how much memory is needed, which requires determining if the text memory of another process can be shared, the hole list is searched to check whether there is sufficient physical memory before freeing the old memory.
If the old memory were freed first and there were insufficient memory, it would be hard to get the old image back again and we would be up a tree.
It sometimes rejects exec calls that, in fact, could succeed.
Suppose, for example, the process doing the exec call occupies 20 KB and its text is not shared by any other process.
By testing before releasing, we will discover that only 30 KB is available and reject the call.
A more sophisticated implementation could handle this situation a little better.
Another possible improvement would be to search for two holes, one for the text segment and one for the data segment, if the process to be execed uses separate I and D space.
A more subtle issue is whether the executable file fits in the virtual address space.
The problem is that memory is allocated not in bytes, but in 1024-byte clicks.
Each click must belong to a single segment, and may not be, for example, half data, half stack, because the entire memory administration is in clicks.
Together they exceed 64 clicks, and thus cannot co-exist, even though the number of bytes needed fits in the virtual address space (barely)
A system that does not check for rare, but possible, conditions is likely to crash in an unexpected way if one of them ever occurs.
Another important issue is how the initial stack is set up.
The library call normally used to invoke exec with arguments and an environment is.
It would be easy enough to implement exec by just putting the three pointers in the message to the PM and letting it fetch the file name and two arrays by itself.
Then it would have to fetch each argument and each string one at a time.
Doing it this way requires at least one message to the system task per argument or string and probably more, since the PM has no way of knowing in advance the size of each one.
To avoid the overhead of multiple messages to read all these pieces, a completely different strategy has been chosen.
The execve library procedure builds the entire initial stack inside itself and passes its base address and size to the PM.
Building the new stack within the user space is highly efficient, because references to the arguments and strings are just local memory references, not references to a different address space.
The contents of the two pointer arrays are shown in Fig.
The procedure execve, within the shell’s address space, now builds.
This stack is eventually copied intact to the PM during the processing of the exec call.
Instead, it will be put at the end of the memory allocation, as determined by the total memory size field in the executable file’s header.
It is up to the PM to relocate the pointers within the stack so that when deposited into the new address, the stack looks like Fig.
When the exec call completes and the program starts running, the stack will indeed look exactly like Fig.
The main program of the executed file is probably declared something like this:
With one integer and two pointers, the three parameters are expected to occupy the three words just before the return address.
The solution is that programs do not begin with main.
Instead, a small, assembly language routine called the C run-time, start-off procedure, or crtso, is always linked in at text address 0 so it gets control first.
Its job is to push three more words onto the stack and then to call main using the standard call instruction.
Thus, main is tricked into thinking it was called in the usual way (actually, it is not really a trick; it is called that way)
If the programmer neglects to call exit at the end of main, control will pass back to the C run-time, start-off routine when main is finished.
Again, the compiler just sees main as an ordinary procedure and generates the usual code to return from it after the last statement.
Thus main returns to its caller, the C runtime, start-off routine which then calls exit itself.
Most of the code of 32-bit crtso is shown in Fig.
Left out are initialization of the environment if not defined by the programmer, code to load the registers that are pushed and a few lines that set a flag that indicates if a floating point coprocessor is present or not.
The key part of crtso, the C run-time, start-off routine.
The library procedures brk and sbrk are used to adjust the upper bound of the data segment.
The former takes an absolute size (in bytes) and calls brk.
The latter takes a positive or negative increment to the current size, computes the new data segment size, and then calls brk.
An interesting question is: ‘‘How does sbrk keep track of the current size, so it can compute the new size?’’ The answer is that a variable, brksize, always holds the current size so sbrk can find it.
This variable is initialized to a compiler generated symbol giving the initial size of text plus data (combined I and D) or just data (separate I and D)
The name, and, in fact, very existence of such a symbol is compiler dependent, and thus it will not be found defined in any header file in.
It is defined in the library, in the file brksize.s.
Exactly where it will be found depends on the system, but it will be in the same directory as crtso.s.
All that must be done is to check to see that everything still fits in the address space, adjust the tables, and tell the kernel.
A defined set of signals exists, and each signal has a default action—either kill the process to which it is directed, or ignore the signal.
Signal processing would be easy to understand and to implement if these were the only alternatives.
However, processes can use system calls to alter these responses.
A process can request that any signal (except for the special sigkill signal) be ignored.
Furthermore, a user process can prepare to catch a signal by requesting that a signal handler procedure internal to the process be activated instead of the default action for any signal (except, again, for sigkill)
Thus to the programmer it appears that there are two distinct times when the operating system deals with signals: a preparation phase when a process may modify its response to a future signal, and a response phase when a signal is generated and acted upon.
The action can be execution of a custom-written signal handler.
When a user-written handler terminates, a special system call cleans up and restores normal operation of the signaled process.
The programmer does not need to know about this third phase.
He writes a signal handler just like any other function.
The operating system takes care of the details of invoking and terminating the handler and managing the stack.
In the preparation phase there are several system calls that a process can execute at any time to change its response to a signal.
The most general of these is sigaction, which can specify that the process ignore some signal, catch some signal (replacing the default action with execution of user-defined signal-handling code within the process), or restore the default response to some signal.
Another system call, sigprocmask, can block a signal, causing it to be queued and to be acted upon only when and if the process unblocks that particular signal at a later time.
These calls may be made at any time, even from within a signal catching.
When a signal is generated, multiple parts of the MINIX 3 system may become involved.
The response begins in the PM, which figures out which processes should get the signal using the data structures just mentioned.
If the signal is to be caught, it must be delivered to the target process.
This requires saving information about the state of the process, so normal execution can be resumed.
The information is stored on the stack of the signaled process, and a check must be made to determine that there is sufficient stack space.
The PM does this checking, since this is within its realm, and then calls the system task in the kernel to put the information on the stack.
The system task also manipulates the program counter of the process, so the process can execute the handler code.
When the handler terminates, a sigreturn system call is made.
Through this call, both the PM and the kernel participate in restoring the signal context and registers of the signaled process so it can resume normal execution.
If the signal is not caught, the default action is taken, which may involve calling the file system to produce a core dump (writing the memory image of the process to a file that may be examined with a debugger), as well as killing the process, which involves all of the PM, file system, and kernel.
The PM may direct one or more repetitions of these actions, since a single signal may need to be delivered to a group of processes.
All of the mandatory POSIX signals are defined in MINIX 3, but not all the optional ones are.
For instance, POSIX requires several signals related to job control, the ability to put a running program into the background and bring it back.
Job control has not been implemented because it was intended to provide a way to start a program running, then detach from it to allow the user to do something else.
Virtual terminals are a kind of poor man’s windowing system, but eliminate the need for job control and its signals, at least if you are working on the local console.
In a traditional UNIX system, signals can be generated in two ways: by the kill system call, and by the kernel.
Some user-space processes in MINIX 3 do things that would be done by the kernel in a traditional system.
Sigint, sigquit, and sigkill can be initiated by pressing special key combinations on the keyboard.
The kill program can be used to cause any signal to be sent to any process.
The implementer of the operating system must provide code to generate a signal in response to the trap.
Just because the hardware can trap on a certain condition does not mean the capability can be used fully by the operating system implementer.
Separate exceptions are generated for violations of the limits of the hardware-defined stack segment and for other segments, since these might need to be treated differently.
However, because of the way MINIX 3 uses memory, the hardware cannot detect all the errors that might occur.
The hardware defines a base and a limit for each segment.
The stack and data segments are combined in a single harware segment.
Signals marked (M) are not defined by POSIX, but are defined by MINIX 3 for compatibility with older programs.
Kernel signals are MINIX 3 specific signals generated by the kernel, and used to inform system processes about system events.
Similarly, the hardware defines the stack as the maximum amount of memory the MINIX 3 stack could use if the data area could shrink to nothing.
Although certain violations can be detected by the hardware, the hardware cannot detect the most probable stack violation, growth of the stack into the data area, since as far as the hardware registers and descriptor tables are concerned the data area and the stack area overlap.
Conceivably some code could be added to the kernel that would check the register contents of a process after each time the process gets a chance to run and generate a sigsegv signal upon detection of a violation of the integrity of the MINIX 3-defined data or stack areas.
Whether this would be worthwhile is unclear; hardware traps can catch a violation immediately.
A software check might not get a chance to do its work until many thousands of additional instructions had been executed, and at that point there might be very little a signal handler could do to try to recover.
Whatever their origin, the PM processes all signals the same way.
For each process to be signaled, a variety of checks are made to see if the signal is feasible.
One process can signal another if the signaler is the superuser or if the real or effective UID of the signaler is equal to either the real or effective UID of the signaled process.
But there are several conditions that can prevent a signal being sent.
A process cannot be signaled if it has explicitly called sigaction to ignore the signal or sigprocmask to block it.
Blocking a signal is distinct from ignoring it; receipt of a blocked signal is remembered, and it is delivered when and if the signaled process removes the block.
Finally, if its stack space is not adequate the signaled process is killed.
If all the conditions are met, the signal can be sent.
If the process has not arranged for the signal to be caught, no information needs to be passed to the process.
In this case the PM executes the default action for the signal, which is usually to kill the process, possibly also producing a core dump.
For a few signals the default action is to ignore the signal.
Catching a signal means executing custom signal-handling code, the address of which is stored in a sigaction structure in the process table.
By modifying the stackframe of a process to be signaled, it can be arranged that when the process next is allowed to execute the signal handler will run.
By modifying the stack of the process in user space, it can be arranged that when the signal handler terminates the sigreturn system call will be made.
It is executed after the kernel puts its address on the stack in such a way that its address becomes the return address popped from the stack when a signal handler terminates.
Sigreturn restores the original stackframe of the signaled process, so it can resume execution at the point where it was interrupted by the signal.
Although the final stage of sending a signal is done by the system task, this is a good place to summarize how it is done, since the data used are passed to the kernel from the PM.
Catching a signal requires something much like the context switch that occurs when one process is taken out of execution and another process is put into execution, since when the handler terminates the process ought to be able to continue as if nothing had happened.
Part (a) of the figure is a simplified view of the stack of a process and part of its process table entry just after it has been taken out of execution following an interrupt.
At the time of suspension the contents of all of the CPU registers are copied into the stackframe structure in the process table entry for the suspended process in the kernel part of the process table.
This will be the situation at the moment a signal is generated.
A signal is generated by a process or task different from the intended recipient, so the recipient cannot be running at that time.
The stack of a process (above) and its stackframe in the process table (below) corresponding to phases in handling a signal.
In preparation for handling the signal, the stackframe from the process table is copied onto the stack of the receiving process as a sigcontext structure, thus.
This structure contains information to be used by sigreturn after the handler finishes.
As will be seen, however, the latter address is not used during normal execution.
Although the handler is written as an ordinary procedure by the programmer, it is not called by a call instruction.
The instruction pointer (program counter) field in the stackframe in the process table is altered to cause the signal handler to begin executing when restart puts the signaled process back into execution.
Figure 4-43(b) shows the situation after this preparation has been completed and as the signal handler executes.
Recall that the signal handler is an ordinary procedure, so when it terminates, ret addr1 is popped and sigreturn executes.
The rest of the sigframe structure is now sigreturn’s local variables.
Part of sigreturn’s action is to adjust its own stack pointer so that if it were to terminate like an ordinary function, it would use ret addr2 as its return address.
It terminates like other system calls, allowing the scheduler in the kernel to decide which process to restart.
Eventually, the signaled process will be rescheduled and will restart at this address, because the address is also in the process’ original stackframe.
The reason this address is on the stack is that a user might want to trace a program using a debugger, and this fools the debugger into a reasonable interpretation of the stack while a signal handler is being traced.
In each phase the stack looks like that of an ordinary process, with local variables on top of a return address.
The real work of sigreturn is to restore things to the state they were in before the signal was received, and to clean up.
Most importantly, the stackframe in the process table is restored to its original state, using the copy that was saved on the signaled process’ stack.
When sigreturn terminates, the situation will be as in Fig.
For most signals the default action is to kill the signaled process.
The PM takes care of this for any signal that is not ignored by default, and which the recipient process has not been enabled to handle, block, or ignore.
If the parent is waiting for it, the killed process is cleaned up and removed from the process table.
If the parent is not waiting, it becomes a zombie.
For certain signals (e.g., SIGQUIT), the PM also writes a core dump of the process to the current directory.
It can easily happen that a signal is sent to a process that is currently blocked waiting for a read on a terminal for which no input is available.
If the process has not specified that the signal is to be caught, it is just killed in the usual way.
If, however, the signal is caught, the issue arises of what to do after the signal interrupt has been processed.
Should the process go back to waiting, or should it continue with the next statement? Decisions, decisions.
What MINIX 3 does is this: the system call is terminated in such a way as to return the error code EINTR, so the process can see that the call was broken off by a signal.
Determining that a signaled process was blocked on a system call is not entirely trivial.
The PM must ask the file system to check for it.
This behavior is suggested, but not required, by POSIX, which also allows a read to return the number of bytes read so far at the time of receipt of the signal.
Returning EINTR makes it possible to set an alarm and to catch sigalrm.
This is an easy way to implement a timeout, for instance to terminate login and hang up a modem line if a user does not respond within a certain period.
Generating an alarm to wake up a process after a preset period of time is one of the most common uses of signals.
In a conventional operating system, alarms would be managed entirely by the kernel, or a clock driver running in kernel space.
In MINIX 3 responsibility for alarms to user processes is delegated to the process manager.
The idea is to lighten the kernel’s load, and simplify the code that runs in kernel space.
If it is true that some number b of bugs are inevitable per some number l of lines of code, it is reasonable to expect that a smaller kernel will mean fewer bugs in the kernel.
Even if the total number of bugs remains the same, their effects should be less serious if they occur in user-space operating system components rather than in the kernel itself.
Can we handle alarms without depending upon kernel-space code at all? In MINIX 3, at least, the answer is no, of course not.
Alarms are managed in the first place by the kernel-space clock task, which maintains a linked list, or queue, of timers, as schematized in Fig.
On every interrupt from the clock chip the expiration time of the timer at the head of the queue is compared to the current time, and if it has expired the clock task main loop is activated.
The clock task then causes a notification to be sent to the process that requested the alarm.
The innovation in MINIX 3 is that timers in kernel space are maintained only for system processes.
The process manager maintains another queue of timers on behalf of user processes that have requested alarms.
The process manager requests an alarm from the clock only for the timer at the head of its queue.
If a new request is not added to the head of the queue no request to the clock is necessary at the time it is added.
Actually, of course, an alarm request is made through the system task, since the clock task does not communicate directly with any other process.
When expiration of an alarm is detected after a clock interrupt a notification comes to the process manager.
The PM then does all the work of checking its own timer queue, signaling user processes, and possibly requesting another alarm if there is still an active alarm request at the head of its list.
So far this does not sound as if it saves much effort at the kernel level, but there are several other considerations.
First there is the possibility that more than one timer may be found to have expired on a particular clock tick.
However, although the clock checks for timer expirations on every interrupt from the clock chip, interrupts are sometimes disabled, as we have seen.
A call to the PC BIOS can cause enough interrupts to be missed that special provision is made to catch up.
This means the time maintained by the clock task can jump by multiple ticks, making it possible that multiple timeouts may need to be handled at once.
If these are handled by the process manager the kernel-space code does not have to traverse its own linked list, cleaning it up and generating multiple notifications.
A user process may terminate before a timer set on its behalf expires.
Or a timer may have been set as a backup to prevent a process from waiting forever for an event that might never occur.
When the event does occur the alarm can be cancelled.
Clearly, it eases the load on the kernel-space code if cancellation of timers is done on a queue maintained by the process manager, and not in the kernel.
The kernel-space queue only needs attention when the timer at its head expires or when the process manager makes a change to the head of its queue.
The implementation of timers will be easier to understand if we take a quick tour of the functions used in handling an alarm now.
Many functions in the process manager and in the kernel are involved, and it is hard to see the whole picture when looking at details, one function at a time.
The PM also handles a few more simple system calls.
They are handled here largely because the PM is a convenient place to put them.
The library functions getuid and geteuid both invoke the getuid system call, which returns both values in its return message.
Similarly, the getgid system call also returns real and effective values for use by the getgid and getegid functions.
Two additional system calls exist in this group, getpgrp and setsid.
The former returns the process group ID, and the latter sets it to the current PID value.
These seven calls are the simplest MINIX 3 system calls.
The ptrace and reboot system calls are also handled by the PM.
It is appropriate to place it in the PM because its first action is to send signals to kill all processes except init.
After that, it calls upon the file system and the system task to complete its work.
Armed with a general overview of how the PM works, let us now turn to the code itself.
The PM is written entirely in C, is straightforward, and contains a substantial amount of commentary in the code itself, so our treatment of most parts need not be long or involved.
We will first look briefly at the header files, then the main program, and finally the files for the various system call groups discussed previously.
Several header files in the PM source directory have the same names as files in the kernel directory; these names will be seen again in the file system.
The parallel structure is designed to make it easier to understand the organization of the whole MINIX 3 system.
The PM also has a number of headers with unique names.
As in other parts of the system, storage for global variables is reserved when the PM’s version of table.c is compiled.
In this section we will look at all of the header files, as well as table.c.
It is included in every compilation, and it in turn includes all the system-wide header files from /usr/include and its subdirectories that are needed by every object module.
Most of the files that are included in kernel/kernel.h are also included here.
The PM also needs some definitions in include/fcntl.h and include/unistd.h.
The PM’s own versions of const.h, type.h, proto.h, and glo.h also are included.
Const.h (line 17100) defines some constants used by the PM.
Type.h is currently unused and exists in skeletal form just so the PM files will.
Proto.h (line 17300) collects in one place function prototypes needed throughout the PM.
Putting these macros here simplifies compiling a version without swapping; otherwise many other source files would have to be modified to remove calls to these functions.
The PM’s global variables are declared in glo.h (line 17500)
The same trick used in the kernel with EXTERN is used here, namely, EXTERN is normally a macro that expands to extern, except in the file table.c.
There it becomes the null string so storage can be reserved for the variables declared as EXTERN.
The next file is param.h (line 17700), which contains macros for many of the system call parameters contained in the request message.
It also contains twelve macros for fields in the reply message, and three macros used only in messages to the file system.
A final note on header files: because MINIX 3 is still being actively developed, there are still some rough edges.
One of these is that some source files in pm/ include header files from the kernel directory.
It may be hard to find some important definitions if you are not aware of this.
Arguably definitions used by more than one major component of MINIX 3 should be consolidated into header files in the include/ directory.
Finally, although the comment on line 18070 indicates this is where a reply is sent back, that is also a simplification.
The call to setreply constructs a reply in the space we mentioned earlier, in the process table entry for the current process.
It will be easier to understand the description that follows if we first show how memory is organized when the PM is activated.
Before the MINIX 3 boot image itself is loaded into memory, the boot monitor determines the layout of available memory.
From the boot menu, you can press the ESC key to see the boot parameters.
One line in the display shows blocks of unused memory, and looks like this:
The exact numbers you see may be different, of course.
Memory below 0x800 is used for BIOS data and by the master boot record and the bootblock.
It really does not matter how it is used, it is not available by the time the boot monitor starts up.
If you have been keeping track of these numbers you will have noticed that the amount of free base memory is less than the 638 KB you might have expected.
This is a good place to note that memory use could be more complicated than is in this example.
For instance, one method of running MINIX, not yet ported to MINIX 3 at the time this is being written, uses an MS-DOS file to simulate a MINIX disk.
The technique requires loading some components of MS-DOS before starting the MINIX 3 boot monitor.
If these are not loaded adjacent to memory regions already in use more than two regions of free memory will be reported by the boot monitor.
When the boot monitor loads the boot image into memory information about the image components is displayed on the console screen.
The PM, file system, reincarnation server, and other components not shown in the figure are installed in the block of free memory that starts at 1 MB.
This was an arbitrary design choice; enough memory remains below the 588 KB limit for some of these components.
However, when MINIX 3 is compiled with a large block cache, as is true in this.
It was easier, but by no means essential, just to load everything in the higher region of memory.
Nothing is lost by this, the memory manager can make use of the hole in memory below 588 KB once the system is running and user processes are started.
Boot monitor display of memory usage of the first few boot image components.
The message contains only the process number and the PID; this is sufficient to initialize the FS process table slot, as all the processes in the system boot image belong to the superuser and can be given the same default values.
Each message is dispatched with a send operation, so no reply is expected.
After sending the message the name of the process is displayed on the console (line 18237):
Building process table: pm fs rs tty memory log driver init In this display driver is a stand-in for the default disk driver; multiple disk drivers may be compiled into the boot image, with one selected as the default by a label= assignment in the boot parameters.
The PM’s own process table entry is a special case.
After the main loop is complete the PM makes some changes to its own entry and then sends a final message to the file system with a symbolic value of NONE as the process number.
This message is sent with a sendrec call, and the process manager blocks expecting a response.
Receiving the message with process number NONE tells the FS that all system processes have been initialized, so it can exit its loop and send a synchronization message to unblock the PM.
If any are found, they are disinherited and become children of init.
If init is waiting and a child is hanging, cleanup is then called for that child.
This deals with situations such as the one shown in Fig.
Now we have the situation that 53, which has already exited, is the child of a process doing a wait.
When a process has exited and its parent is waiting for it, in whichever order these events occur, the procedure cleanup (line 18660) is called to perform the last rites.
The parent is awakened from its wait or waitpid call and is given the PID of the terminated child, as well as its exit and signal status.
The file system has already released the child’s memory, and the kernel has already suspended scheduling and freed up the child’s slot in the process table.
Rather than just reading the file block by block and then copying the blocks to the user, a trick is used to allow the file system to load the entire segment directly to the user space.
In effect, the call is decoded by the file system in a slightly special way so that it appears to be a read of the entire segment by the user process itself.
Only a few lines at the beginning of the file system’s read routine know that some monkey business is going on here.
As we have just seen, the basic memory model used by MINIX 3 is quite simple: each process is given a single contiguous allocation for its data and stack when it is created.
It is never moved around in memory, it never grows, and it never shrinks.
All that can happen is that the data segment can eat away at the gap from the low end, and the stack can eat away at it from the high end.
It consists of verifying that the new sizes are feasible and then updating the tables to reflect them.
The base of the data segment is constant, so if adjust has to adjust the data segment, all it does is update the length field.
The stack grows downward from a fixed end point, so if adjust also notices that the stack pointer, which is given to it as a parameter, has grown beyond the stack segment (to a lower address), both the origin and length are updated.
These system calls, as well as the signals themselves, are processed in the file signal.c.
The sigaction system call supports the sigaction and signal functions, which allow a process to alter how it will respond to signals.
The library functions sigaddset and sigdelset are used, to modify the signal bitmaps, although the actions are straightforward bit manipulation operations that could have been implemented with simple macros.
However, these functions are required by the POSIX standard in order to make programs that use them easily portable, even to systems in which the number of signals exceeds the number of bits available in an integer.
Using the library functions helps to make MINIX 3 itself easily portable to different architectures.
The default action of the SIGALRM signal is to kill the process if it is not caught.
If the SIGALRM is to be caught, a handler must be installed by sigaction.
First, in message (1) the user does an alarm call via a message to the PM.
At this point the process manager sets up a timer in the queue of timers it maintains for user processes, and acknowledges with message (2)
But after this timer reaches the head of the kernel466 MEMORY MANAGEMENT CHAP.
All that is necessary is to set a bit and return the SUSPEND code, which causes the main loop of the PM to refrain from replying, thus keeping the caller blocked.
The kernel need not even be informed, since it knows that the caller is blocked.
A message is constructed to be sent to the kernel, some parts of which are copies of information in the PM’s part of the process table.
If the process to be signaled was previously suspended by sigsuspend, the signal mask that was saved at the time of suspension is included in the message; otherwise the current signal mask is included (line 19914)
Other items included in the message are several addresses in the space of the signaled process space: the signal handler, the address of the sigreturn library routine to be called on completion of the handler, and the current stack pointer.
Figure 4-49 shows the structure that is put on the stack.
The sigcontext portion is put on the stack to preserve it for later restoration, since the corresponding structure in the process table itself is altered in preparation for execution of the signal handler.
The sigframe part provides a return address for the signal handler and data needed by sigreturn to complete restoration of the process’ state when the handler is done.
They are there to fool a debugger if anyone should ever try to trace execution of a signal handler.
The structure to be put on the signaled process’ stack is fairly large.
The sigcontext and sigframe structures pushed on the stack to prepare for a signal handler.
The processor registers are a copy of the stackframe used during a context switch.
The MINIX 3 process manager handles requests for alarms from user processes, which are not allowed to contact the kernel or the system task directly themselves.
All details of scheduling an alarm at the clock task are hidden behind this interface.
Only system processes are allowed to set an alarm timer at the kernel.
Support for this is provided in the file timers.c (line 20200)
The process manager maintains a list of requests for alarms, and asks the system task to notify it when it is time for an alarm.
When an alarm comes from the kernel the process manager passes it on to the process that should receive it.
The process manager handles three system calls that involve time in time.c: time, stime, and times.
Setting the uid or gid is slightly more complex than just reading it.
A check has to be made to see if the caller is authorized to set the uid or gid.
If the caller passes the test, the file system must be informed of the new uid or gid, since file protection depends on it.
The setsid call creates a new session, and a process which is already a process group leader is not allowed to do this.
The file system completes the job of making a process into a session leader with no controlling terminal.
In contrast to the system calls considered so far in this chapter, the calls in misc.c are not required by POSIX.
These calls are necessary because the user472 MEMORY MANAGEMENT CHAP.
We will end this chapter by describing briefly two more files which provide support functions for the process manager.
Because internal details of these files are not discussed here, they are not printed in Appendix B (to keep this already fat book from becoming even fatter)
However, they are available on the CD-ROM and the MINIX 3 Web site.
Alloc.c is where the system keeps track of which parts of memory are in use and which are free.
Then the whole process table is searched to make sure that the PID to be assigned is not already in use.
If it is in use the procedure is repeated until a free PID is found.
Panic is called only when the PM has detected an error from which it cannot recover.
It reports the error to the system task, which then brings MINIX 3 to a screeching halt.
We saw that the simplest systems do not swap or page at all.
Once a program is loaded into memory, it remains there until it finishes.
Embedded systems usually work like this, possibly with the code even in ROM.
Some operating systems allow only one process at a time in memory, while others support multiprogramming.
When swapping is used, the system can handle more processes than it has room for in memory.
Processes for which there is no room are swapped out to the disk.
Free space in memory and on disk can be kept track of with a bitmap or a hole list.
More advanced computers often have some form of virtual memory.
In the simplest form, each process’ address space is divided up into uniformly sized blocks called pages, which can be placed into any available page frame in.
Two of the better known ones are second chance and aging.
To make paging systems work well, choosing an algorithm is not enough; attention to issues such as determining the working set, memory allocation policy, and page size are required.
Segmentation helps in handling data structures that change size during execution and simplifies linking and sharing.
Sometimes segmentation and paging are combined to provide a two-dimensional virtual memory.
Memory is allocated when a process executes a fork or exec system call.
The memory so allocated is never increased or decreased as long as the process lives.
Small programs can have instructions and data in the same memory segment.
Larger programs use separate instruction and data space (separate I and D)
Processes with separate I and D space can share the text portion of their memory, so only data and stack memory must be allocated during a fork.
This may also be true during an exec if another process already is using the text needed by the new program.
Most of the work of the PM is concerned not with keeping track of free memory, which it does using a hole list and the first fit algorithm, but rather with carrying out the system calls relating to process management.
A number of system calls support POSIX-style signals, and since the default action of most signals is to terminate the signaled process, it is appropriate to handle them in the PM, which initiates termination of all processes.
Several system calls not directly related to memory are also handled by the PM, mainly because it is smaller than the file system, and thus it was most convenient to put them here.
A computer system has enough room to hold four programs in its main memory.
These programs are each idle half the time waiting for I/O.
How many KB are needed if a bitmap is used to keep track of free memory?
How much memory is needed for the list in the best case and in the worst case? Assume the operating system occupies the bottom 512 KB of memory.
What is the difference between a physical address and a virtual address?
In general, is it permitted for the number of page bits of the virtual address to be smaller, equal to, or larger than the number of page bits of the physical address? Discuss your answer.
Nevertheless, some companies previously sold systems that contained an unmodified 8086 CPU and do paging.
Make an educated guess as to how they did it.
If an instruction takes 1 nsec and a page fault takes an additional n nsec, give a formula for the effective instruction time if page faults occur every k instructions.
The page table is entirely in hardware, with one 32-bit word per entry.
When a process starts, the page table is copied to the hardware from memory, at one word every 100 nsec.
If each process runs for 100 msec (including the time to load the page table), what fraction of the CPU time is devoted to loading the page tables?
A computer with a 32-bit address uses a two-level page table.
How large are the pages and how many are there in the address space?
Below is the listing of a short assembly language program for a computer with 512byte pages.
Suppose that a 32-bit virtual address is broken up into four fields, a, b, c, and d.
The first three are used for a three-level page table system.
Does the number of pages depend on the sizes of all four fields? If not, which ones matter and which ones do not?
A computer whose processes have 1024 pages in their address spaces keeps its page tables in memory.
The overhead required for reading a word from the page table is 500 nsec.
What hit rate is needed to reduce the mean overhead to 200 nsec?
The TLB on the VAX did not contain an R bit.
Was this omission just an artifact of its era (1980s) or is there some other reason for its absence?
The time of loading, time of last access, and the R and M bits for each page are as shown below (the times are in clock ticks):
If FIFO page replacement is used with four page frames and eight pages, how many page faults will occur with the reference string 0172327103 if the four frames are initially empty? Now repeat this problem for LRU.
A small computer has 8 page frames, each containing a page.
What is the order that second chance considers pages and which one is selected?
Are there any circumstances in which clock and second chance choose different pages to replace? If so, what are they?
Suppose that a computer uses the PFF page replacement algorithm but there is sufficient memory to hold all the processes without page faults.
If the aging algorithm is used with an 8-bit counter, give the values of the four counters after the last tick.
Will this program fit in the address space? If the page size were 512 bytes, would it fit? Remember that a page may not contain parts of two different segments.
It has been observed that the number of instructions executed between page faults is directly proportional to the number of page frames allocated to a program.
If the available memory is doubled, the mean interval between page faults is also doubled.
A group of operating system designers for the Frugal Computer Company are thinking about ways of reducing the amount of backing store needed in their new operating system.
The head guru has just suggested not bothering to save the program text in the swap area at all, but just page it in directly from the binary file whenever it is needed.
Which one occurs in paging systems? Which one occurs in systems using pure segmentation?
When segmentation and paging are both being used, as in the Pentium, first the segment descriptor must be looked up, then the page descriptor.
Does the TLB also work this way, with two levels of lookup?
Why does the MINIX 3 memory management scheme make it necessary to have a program like chmem?
What will be the cs value for the next component loaded after rs?
In the previous problem does it matter whether the boot monitor takes exactly as much memory as it needs or if it is rounded up to units of clicks?
Redesign adjust to avoid the problem of signaled processes being killed unnecessarily because of a too-strict test for stack space.
Modify chmem to make a new command showmem, which simply displays the current memory allocation of its argument.
While a process is running, it can store a limited amount of information within its own address space.
However, the storage capacity is restricted to the size of the virtual address space.
For some applications this size is adequate, but for others, such as airline reservations, banking, or corporate record keeping, it is far too small.
A second problem with keeping information within a process’ address space is that when the process terminates, the information is lost.
For many applications, (e.g., for databases), the information must be retained for weeks, months, or even forever.
Having it vanish when the process using it terminates is unacceptable.
Furthermore, it must not go away when a computer crash kills the process.
A third problem is that it is frequently necessary for multiple processes to access (parts of) the information at the same time.
If we have an online telephone directory stored inside the address space of a single process, only that process can access it.
The way to solve this problem is to make the information itself independent of any one process.
Thus we have three essential requirements for long-term information storage:
It must be possible to store a very large amount of information.
The information must survive the termination of the process using it.
Multiple processes must be able to access the information concurrently.
The usual solution to all these problems is to store information on disks and other external media in units called files.
Processes can then read them and write new ones if need be.
Information stored in files must be persistent, that is, not be affected by process creation and termination.
A file should only disappear when its owner explicitly removes it.
How they are structured, named, accessed, used, protected, and implemented are major topics in operating system design.
As a whole, that part of the operating system dealing with files is known as the file system and is the subject of this chapter.
From the users’ standpoint, the most important aspect of a file system is how it appears to them, that is, what constitutes a file, how files are named and protected, what operations are allowed on files, and so on.
The details of whether linked lists or bitmaps are used to keep track of free storage and how many sectors there are in a logical block are of less interest, although they are of great importance to the designers of the file system.
For this reason, we have structured the chapter as several sections.
The first two are concerned with the user interface to files and directories, respectively.
Then comes a discussion of alternative ways a file system can be implemented.
Following a discussion of security and protection mechanisms, we conclude with a description of the MINIX 3 file system.
In the following pages we will look at files from the user’s point of view, that is, how they are used and what properties they have.
They provide a way to store information on the disk and read it back later.
This must be done in such a way as to shield the user from the details of how and where the information is stored, and how the disks actually work.
Probably the most important characteristic of any abstraction mechanism is the way the objects being managed are named, so we will start our examination of file systems with the subject of file naming.
When a process creates a file, it gives the file a name.
When the process terminates, the file continues to exist and can be accessed by other processes using its name.
The exact rules for file naming vary somewhat from system to system, but all current operating systems allow strings of one to eight letters as legal file names.
Many file systems support names as long as 255 characters.
Some file systems distinguish between upper- and lower-case letters, whereas others do not.
Thus a UNIX system can have all of the following as three distinct files: maria, Maria, and MARIA.
In MS-DOS, all these names refer to the same file.
With each new version improvements were added but the features we will discuss are mostly common to MS-DOS and ‘‘classic’’ Windows versions.
However, the latter systems also have a native file system (NTFS) that has different properties (such as file names in Unicode)
This file system also has seen changes in successive versions.
In this chapter, we will refer to the older systems as the Windows 98 file system.
If a feature does not apply to the MS-DOS or Windows 95 versions we will say so.
Many operating systems support two-part file names, with the two parts separated by a period, as in prog.c.
The part following the period is called the file extension and usually indicates something about the file, in this example that it is a C programming language source file.
Some of the more common file extensions and their meanings are shown in Fig.
In some systems (e.g., UNIX), file extensions are just conventions and are not enforced by the operating system.
A file named file.txt might be some kind of text file, but that name is more to remind the owner than to convey any actual information to the computer.
On the other hand, a C compiler may actually insist that files it is to compile end in .c, and it may refuse to compile them if they do not.
Conventions like this are especially useful when the same program can handle several different kinds of files.
The C compiler, for example, can be given a list of files to compile and link together, some of them C files (e.g., foo.c), some of them assembly language files (e.g., bar.s), and some of them object files (e.g., other.o)
The extension then becomes essential for the compiler to tell which are C files, which are assembly files, and which are object files.
In contrast, Windows is very much aware of the extensions and assigns meaning to them.
Users (or processes) can register extensions with the operating system and specify which program ‘‘owns’’ which one.
When a user double clicks on a file name, the program assigned to its file extension is launched and given.
For example, double clicking on file.doc starts Microsoft Word with file.doc as the initial file to edit.
Some might think it odd that Microsoft chose to make common extensions invisible by default since they are so important.
Fortunately most of the ‘‘wrong by default’’ settings of Windows can be changed by a sophisticated user who knows where to look.
Files can be structured in any one of several ways.
In effect, the operating system does not know or care what is in the file.
Having the operating system regard files as nothing more than byte sequences provides the maximum flexibility.
User programs can put anything they want in their files and name them any way that is convenient.
The operating system does not help, but it also does not get in the way.
For users who want to do unusual things, the latter can be very important.
The first step up in structure is shown in Fig.
In this model, a file is a sequence of fixed-length records, each with some internal structure.
Central to the idea of a file being a sequence of records is the idea that the read operation returns one record and the write operation overwrites or appends one record.
The third kind of file structure is shown in Fig.
In this organization, a file consists of a tree of records, not necessarily all the same length, each containing a key field in a fixed position in the record.
The tree is sorted on the key field, to allow rapid searching for a particular key.
The basic operation here is not to get the ‘‘next’’ record, although that is also possible, but to get the record with a specific key.
Furthermore, new records can be added to the file, with the operating system, and not the user, deciding where to place them.
This type of file is clearly quite different from the unstructured byte streams used in UNIX and Windows 98 but is widely used on the large mainframe computers still used in some commercial data processing.
Windows XP also uses metadata files, which we will mention later.
Directories are system files for maintaining the structure of the file system.
Character special files are related to input/output and used to model serial I/O devices such as terminals, printers, and networks.
In this chapter we will be primarily interested in regular files.
Regular files are generally either ASCII files or binary files.
In some systems each line is terminated by a carriage return character.
The great advantage of ASCII files is that they can be displayed and printed as is, and they can be edited with any text editor.
Furthermore, if large numbers of programs use ASCII files for input and output, it is easy to connect the output of one program to the input of another, as in shell pipelines.
The interprocess plumbing is not any easier, but interpreting the information certainly is if a standard convention, such as ASCII, is used for expressing it.
Other files are binary files, which just means that they are not ASCII files.
Listing them on the printer gives an incomprehensible listing full of what is apparently random junk.
Usually, they have some internal structure known to programs that use them.
Although technically the file is just a sequence of bytes, the operating system will only execute a file if it has the proper format.
It has five sections: header, text, data, relocation bits, and symbol table.
The header starts with a so-called magic number, identifying the file as an executable file (to prevent the accidental execution of a file not in this format)
Then come the sizes of the various pieces of the file, the address at which execution starts, and some flag bits.
Following the header are the text and data of the program itself.
These are loaded into memory and relocated using the relocation bits.
Our second example of a binary file is an archive, also from UNIX.
It consists of a collection of library procedures (modules) compiled but not linked.
Each one is prefaced by a header telling its name, creation date, owner, protection code, and size.
Just as with the executable file, the module headers are full of binary numbers.
Every operating system must recognize at least one file type: its own executable file, but some operating systems recognize more.
Then it located the source file and saw if the source had been modified since the binary was made.
In UNIX terms, the make program had been built into the shell.
The file extensions were mandatory so the operating system could tell which binary program was derived from which source.
Having strongly typed files like this causes problems whenever the user does anything that the system designers did not expect.
Consider, as an example, a system in which program output files have extension .dat (data files)
If a user writes a program formatter that reads a .c file (C program), transforms it (e.g., by converting it to a standard indentation layout), and then writes the transformed file as output, the output file will be of type .dat.
If the user tries to offer this to the C compiler to compile it, the system will refuse because it has the wrong extension.
Attempts to copy file.dat to file.c will be rejected by the system as invalid (to protect the user against mistakes)
While this kind of ‘‘user friendliness’’ may help novices, it drives experienced users up the wall since they have to devote considerable effort to circumventing the operating system’s idea of what is reasonable and what is not.
Early operating systems provided only a single kind of file access: sequential access.
In these systems, a process could read all the bytes or records in a file in order, starting at the beginning, but could not skip around and read them out of order.
Sequential files could be rewound, however, so they could be read as often as needed.
Sequential files were convenient when the storage medium was magnetic tape, rather than disk.
When disks came into use for storing files, it became possible to read the bytes or records of a file out of order, or to access records by key, rather than by position.
Files whose bytes or records can be read in any order are called random access files.
Random access files are essential for many applications, for example, database systems.
If an airline customer calls up and wants to reserve a seat on a particular flight, the reservation program must be able to access the record for that flight without having to read the records for thousands of other flights first.
Two methods are used for specifying where to start reading.
In the first one, every read operation gives the position in the file to start reading at.
In the second one, a special operation, seek, is provided to set the current position.
After a seek, the file can be read sequentially from the now-current position.
In some older mainframe operating systems, files are classified as being either sequential or random access at the time they are created.
This allows the system to use different storage techniques for the two classes.
In addition, all operating systems associate other information with each file, for example, the date and time the file was created and the file’s size.
We will call these extra items the file’s attributes although some people called them metadata.
The list of attributes varies considerably from system to system.
No existing system has all of these, but each is present in some system.
The first four attributes relate to the file’s protection and tell who may access it and who may not.
All kinds of schemes are possible, some of which we will study later.
In some systems the user must present a password to access a file, in which case the password must be one of the attributes.
The flags are bits or short fields that control or enable some specific property.
Hidden files, for example, do not appear in listings of the files.
The archive flag is a bit that keeps track of whether the file has been backed up.
The backup program clears it, and the operating system sets it whenever a file is changed.
The temporary flag allows a file to be marked for automatic deletion when the process that created it terminates.
The record length, key position, and key length fields are only present in files whose records can be looked up using a key.
The various times keep track of when the file was created, most recently accessed and most recently modified.
For example, a source file that has been modified after the creation of the corresponding object file needs to be recompiled.
The current size tells how big the file is at present.Some old mainframe operating systems require the maximum size to be specified when the file is created, in order to let the operating system reserve the maximum amount of storage in advance.
Modern operating systems are clever enough to do without this feature.
Files exist to store information and allow it to be retrieved later.
Different systems provide different operations to allow storage and retrieval.
Below is a discussion of the most common system calls relating to files.
The purpose of the call is to announce that the file is coming and to set some of the attributes.
When the file is no longer needed, it has to be deleted to free up disk space.
The purpose of the open call is to allow the system to fetch the attributes and list of disk addresses into main memory for rapid access on later calls.
When all the accesses are finished, the attributes and disk addresses are no longer needed, so the file should be closed to free up some internal table space.
Many systems encourage this by imposing a maximum number of open files on processes.
A disk is written in blocks, and closing a file forces writing of the file’s last block, even though that block may not be entirely full yet.
The caller must specify how much data are needed and must also provide a buffer to put them in.
Data are written to the file, again, usually at the current position.
If the current position is the end of the file, the file’s size increases.
If the current position is in the middle of the file, existing data are overwritten and lost forever.
It can only add data to the end of the file.
Systems that provide a minimal set of system calls do not generally have append, but many systems provide multiple ways of doing the same thing, and these systems sometimes have append.
For random access files, a method is needed to specify from where to take the data.
One common approach is a system call, seek, that repositions the file pointer to a specific place in the file.
After this call has completed, data can be read from, or written to, that position.
Processes often need to read file attributes to do their work.
For example, the UNIX make program is commonly used to manage software development projects consisting of many source files.
When make is called, it examines the modification times of all.
To do its job, it must look at the attributes, namely, the modification times.
Some of the attributes are user settable and can be changed after the file has been created.
It frequently happens that a user needs to change the name of an existing file.
It is not always strictly necessary, because the file can usually be copied to a new file with the new name, and the old file then deleted.
Locking a file or a part of a file prevents multiple simultaneous access by different process.
For an airline reservation system, for instance, locking the database while making a reservation prevents reservation of a seat for two different travelers.
To keep track of files, file systems normally have directories or folders, which, in many systems, are themselves files.
In this section we will discuss directories, their organization, their properties, and the operations that can be performed on them.
A directory typically contains a number of entries, one per file.
Here a directory entry holds the file name and a pointer to another data structure where the attributes and disk addresses are found.
When a file is opened, the operating system searches its directory until it finds the name of the file to be opened.
It then extracts the attributes and disk addresses, either directly from the directory entry or from the data structure pointed to, and puts them in a table in main memory.
All subsequent references to the file use the information in main memory.
The simplest form of directory system is a single directory containing all files for all users, as illustrated in Fig.
On early personal computers, this single-directory system was common, in part because there was only one user.
The problem with having only one directory in a system with multiple users is that different users may accidentally use the same names for their files.
For example, if user A creates a file called mailbox, and then later user B also creates a file called mailbox, B’s file will overwrite A’s file.
Consequently, this scheme is not used on multiuser systems any more, but could be used on a small embedded system, for example, a handheld personal digital assistant or a cellular telephone.
To avoid conflicts caused by different users choosing the same file name for their own files, the next step up is giving each user a private directory.
In that way, names chosen by one user do not interfere with names chosen by a different user and there is no problem caused by the same name occurring in two or more directories.
This design could be used, for example, on a multiuser computer or on a simple network of personal computers that shared a common file server over a local area network.
Implicit in this design is that when a user tries to open a file, the operating system knows which user it is in order to know which directory to search.
As a consequence, some kind of login procedure is needed, in which the user specifies a login name or identification, something not required with a single-level directory system.
When this system is implemented in its most basic form, users can only access files in their own directories.
But another problem is that users with many files may want to group them in smaller subgroups, for instance a professor might want to separate handouts for a class from drafts of chapters of a new textbook.
What is needed is a general hierarchy (i.e., a tree of directories)
With this approach, each user can have as many directories as are needed so that files can be grouped together in natural ways.
The ability to create an arbitrary number of subdirectories provides a powerful structuring tool for users to organize their work.
For this reason nearly all modern PC and server file systems are organized this way.
However, as we have pointed out before, history often repeats itself with new technologies.
Digital cameras have to record their images somewhere, usually on a flash memory card.
However, it did not take very long for camera manufacturers to build file systems with multiple directories, as in Fig.
What difference does it make that none of the camera owners understand how to use multiple directories, and probably could not conceive of any use for this feature even if they did understand it? It is only (embedded) software, after all, and thus costs the camera manufacturer next to nothing to provide.
Can digital cameras with full-blown hierarchical file systems, multiple login names, and 255-character file names be far behind?
When the file system is organized as a directory tree, some way is needed for specifying file names.
In the first method, each file is given an absolute path name consisting of the path from the root directory to the file.
As an example, the path /usr/ast/mailbox means that the root directory contains a subdirectory usr/, which in turn contains a subdirectory.
Absolute path names always start at the root directory and are unique.
In UNIX the components of the path are separated by /
Thus the same path name would be written as follows in these two systems:
No matter which character is used, if the first character of the path name is the separator, then the path is absolute.
The other kind of name is the relative path name.
This is used in conjunction with the concept of the working directory (also called the current directory)
A user can designate one directory as the current working directory, in which case all path names not beginning at the root directory are taken relative to the working directory.
For example, if the current working directory is /usr/ast, then the file whose absolute path is /usr/ast/mailbox can be referenced simply as mailbox.
The relative form is often more convenient, but it does the same thing as the absolute form.
Some programs need to access a specific file without regard to what the working directory is.
In that case, they should always use absolute path names.
It should use the full, absolute path name in this case because it does not know what the working directory will be when it is called.
The absolute path name will always work, no matter what the working directory is.
Of course, if the spelling checker needs a large number of files from /usr/lib/, an alternative approach is for it to issue a system call to change its working directory to /usr/lib/, and then use just dictionary as the first parameter to open.
By explicitly changing the working directory, it knows for sure where it is in the directory tree, so it can then use relative paths.
Each process has its own working directory, so when a process changes its working directory and later exits, no other processes are affected and no traces of the change are left behind in the file system.
In this way it is always perfectly safe for a process to change its working directory whenever that is convenient.
On the other hand, if a library procedure changes the working directory and does not change back to where it was when it is finished, the rest of the program may not work since its assumption about where it is may now suddenly be invalid.
For this reason, library procedures rarely change the working directory, and when they must, they always change it back again before returning.
To see how these are used, consider the UNIX file tree of Fig.
The first path instructs the system to go upward (to the usr directory), then to go down to the directory lib/ to find the file dictionary.
When the cp command gets a directory name (including dot) as its last argument, it copies all the files there.
Of course, a more normal way to do the copy would be to type.
Here the use of dot saves the user the trouble of typing dictionary a second time.
The system calls for managing directories exhibit more variation from system to system than system calls for files.
To give an impression of what they are and how they work, we will give a sample (taken from UNIX)
It is empty except for dot and dotdot, which are put there automatically by the system (or in a few cases, by the mkdir program)
A directory containing only dot and dotdot is considered empty as these cannot usually be deleted.
For example, to list all the files in a directory, a listing program opens the directory to read out the names of all the files it contains.
Before a directory can be read, it must be opened, analogous to opening and reading a file.
When a directory has been read, it should be closed to free up internal table space.
This call returns the next entry in an open directory.
Formerly, it was possible to read directories using the usual read system call, but that approach has the disadvantage of forcing the programmer to know and deal with the internal structure of directories.
In contrast, readdir always returns one entry in a standard format, no matter which of the possible directory structures is being used.
In many respects, directories are just like files and can be renamed the same way files can be.
Linking is a technique that allows a file to appear in more than one directory.
This system call specifies an existing file and a path name, and creates a link from the existing file to the name specified by the path.
In this way, the same file may appear in multiple directories.
A link of this kind, which increments the counter in the file’s i-node (to keep track of the number of directory entries containing the file), is sometimes called a hard link.
If the file being unlinked is only present in one directory (the normal case), it is removed from the file system.
If it is present in multiple directories, only the path name specified is removed.
In UNIX, the system call for deleting files (discussed earlier) is, in fact, unlink.
The above list gives the most important calls, but there are a few others as well, for example, for managing the protection information associated with a directory.
Now it is time to turn from the user’s view of the file system to the implementer’s view.
Users are concerned with how files are named, what operations are allowed on them, what the directory tree looks like, and similar interface issues.
Implementers are interested in how files and directories are stored, how disk space is managed, and how to make everything work efficiently and reliably.
In the following sections we will examine a number of these areas to see what the issues and trade-offs are.
To review this material briefly, most disks can be divided up into partitions, with independent file systems on each partition.
Sector 0 of the disk is called the MBR (Master Boot Record) and is used to boot the computer.
This table gives the starting and ending addresses of each partition.
One of the partitions in the table may be marked as active.
When the computer is booted, the BIOS reads in and executes the code in the MBR.
The first thing the MBR program does is locate the active partition, read in its first block, called the boot block, and execute it.
The program in the boot block loads the operating system contained in that partition.
For uniformity, every partition starts with a boot block, even if it does not contain a bootable operating system.
Besides, it might contain one in the some time in the future, so reserving a boot block is a good idea anyway.
The above description must be true, regardless of the operating system in use, for any hardware platform on which the BIOS is to be able to start more than one operating system.
For instance the master boot record may sometimes be called the IPL (Initial Program Loader), Volume Boot Code, or simply masterboot.
Once the BIOS has loaded an MBR or boot sector the actions may vary.
For instance, more than one block of a partition may be used to contain the program that loads the operating system.
The BIOS can be counted on only to load the first block, but that block may then load additional blocks if the implementer of the operating system writes the boot block that way.
An implementer can also supply a custom MBR, but it must work with a standard partition table if multiple operating systems are to be supported.
On PC-compatible systems there can be no more than four primary partitions because there is only room for a four-element array of partition descriptors between the master boot record and the end of the first 512-byte sector.
Some operating systems allow one entry in the partition table to be an extended partition which points to a linked list of logical partitions.
This makes it possible to have any number of additional partitions.
The BIOS cannot start an operating system from a logical partition, so initial startup from a primary partition is required to load code that can manage logical partitions.
An alternative to extended partitions is used by MINIX 3, which allows a partition to contain a subpartition table.
An advantage of this is that the same code that manages a primary partition table can manage a subpartition table, which has the same structure.
Potential uses for subpartitions are to have different ones for the root device, swapping, the system binaries, and the users’ files.
In this way, problems in one subpartition cannot propagate to another one, and a new version of the operating system can be easily installed by replacing the contents of some of the subpartitions but not all.
Floppy disks usually start with a boot block in the first sector.
The BIOS reads the first sector of a disk and looks for a magic number which identifies it as valid executable code, to prevent an attempt to execute the first sector of an unformatted or corrupted disk.
A master boot record and a boot block use the same magic number, so the executable code may be either one.
Also, what we say here is not limited to electromechanical disk devices.
A device such as a camera or personal digital assistant that uses nonvolatile (e.g., flash) memory typically has part of the memory organized to simulate a disk.
Other than starting with a boot block, the layout of a disk partition varies considerably from file system to file system.
A UNIX-like file system will contain some of the items shown in Fig.
It contains all the key parameters about the file system and is read into memory when the computer is booted or the file system is first touched.
Next might come information about free blocks in the file system.
This might be followed by the i-nodes, an array of data structures, one per file, telling all about the file and where its blocks are located.
After that might come the root directory, which contains the top of the file system tree.
Finally, the remainder of the disk typically contains all the other directories and files.
Probably the most important issue in implementing file storage is keeping track of which disk blocks go with which file.
In this section, we will examine a few of them.
The simplest allocation scheme is to store each file as a contiguous run of disk blocks.
First, it is simple to implement because keeping track of where a file’s blocks are is reduced to remembering two numbers: the disk address of the first block and the number of blocks in the file.
Given the number of the first block, the number of any other block can be found by a simple addition.
Second, the read performance is excellent because the entire file can be read from the disk in a single operation.
After that, no more seeks or rotational delays are needed so data come in at the full bandwidth of the disk.
Thus contiguous allocation is simple to implement and has high performance.
Unfortunately, contiguous allocation also has a major drawback: in time, the disk becomes fragmented, consisting of files and holes.
Initially, this fragmentation is not a problem since each new file can be written at the end of disk, following the previous one.
However, eventually the disk will fill up and it will become necessary to either compact the disk, which is prohibitively expensive, or to reuse the free space in the holes.
Reusing the space requires maintaining a list of holes, which is doable.
However, when a new file is to be created, it is necessary to know its final size in order to choose a hole of the correct size to place it in.
Contiguous allocation was actually used on magnetic disk file systems years ago due to its simplicity and high performance (user friendliness did not count for much then)
Then the idea was dropped due to the nuisance of having to specify final file size at file creation time.
But with the advent of CD-ROMs, DVDs, and other write-once optical media, suddenly contiguous files are a good idea again.
For such media, contiguous allocation is feasible and, in fact, widely used.
Here all the file sizes are known in advance and will never change during subsequent use of the CD-ROM file system.
It is thus important to study old systems and ideas that were conceptually clean and simple because they may be applicable to future systems in surprising ways.
The second method for storing files is to keep each one as a linked list of disk blocks, as shown in Fig.
The first word of each block is used as a pointer to the next one.
Storing a file as a linked list of disk blocks.
Unlike contiguous allocation, every disk block can be used in this method.
No space is lost to disk fragmentation (except for internal fragmentation in the last block of each file)
Also, it is sufficient for the directory entry to merely store the disk address of the first block.
Also, the amount of data storage in a block is no longer a power of two because the pointer takes up a few bytes.
While not fatal, having a peculiar size is less efficient because many programs read and write in blocks whose size is a power of two.
With the first few bytes of each block occupied to a pointer to the next block, reads of the full block size require acquiring and concatenating information from two disk blocks, which generates extra overhead due to the copying.
Linked list allocation using a file allocation table in main memory.
Using this organization, the entire block is available for data.
Although the chain must still be followed to find a given offset within the file, the chain is entirely in memory, so it can be followed.
Like the previous method, it is sufficient for the directory entry to keep a single integer (the starting block number) and still be able to locate all the blocks, no matter how large the file is.
The primary disadvantage of this method is that the entire table must be in memory all the time.
Each entry has to be a minimum of 3 bytes.
Conceivably the table could be put in pageable memory, but it would still occupy a great deal of virtual memory and disk space as well as generating paging traffic.
MS-DOS and Windows 98 use only FAT file systems and later versions of Windows also support it.
Our last method for keeping track of which blocks belong to which file is to associate with each file a data structure called an i-node (index-node), which lists the attributes and disk addresses of the file’s blocks.
Given the i-node, it is then possible to find all the blocks of the file.
The big advantage of this scheme over linked files using an in-memory table is that the i-node need only be in memory when the corresponding file is open.
If each i-node occupies n bytes and a maximum of k files may be open at once, the total memory occupied by the array holding the i-nodes for the open files is only kn bytes.
This array is usually far smaller than the space occupied by the file table described in the previous section.
The table for holding the linked list of all disk blocks is proportional in size to the disk itself.
If the disk has n blocks, the table needs n entries.
As disks grow larger, this table grows linearly with them.
In contrast, the i-node scheme requires an array in memory whose size is proportional to the maximum number of files that may be open at once.
One problem with i-nodes is that if each one has room for a fixed number of disk addresses, what happens when a file grows beyond this limit? One solution is to reserve the last disk address not for a data block, but instead for the address of an indirect block containing more disk block addresses.
This idea can be extended to use double indirect blocks and triple indirect blocks, as shown in Fig.
Before a file can be read, it must be opened.
When a file is opened, the operating system uses the path name supplied by the user to locate the directory entry.
Finding a directory entry means, of course, that the root directory must be.
The root directory may be in a fixed location relative to the start of a partition.
Alternatively, its position may be determined from other information, for instance, in a classic UNIX file system the superblock contains information about the size of the file system data structures that precede the data area.
From the superblock the location of the i-nodes can be found.
The first i-node will point to the root directory, which is created when a UNIX file system is made.
In Windows XP, information in the boot sector (which is really much bigger than one sector) locates the MFT (Master File Table), which is used to locate other parts of the file system.
Once the root directory is located a search through the directory tree finds the desired directory entry.
The directory entry provides the information needed to find the disk blocks.
Depending on the system, this information may be the disk address of the entire file (contiguous allocation), the number of the first block (both linked list schemes), or the number of the i-node.
In all cases, the main function of the directory system is to map the ASCII name of the file onto the information needed to locate the data.
A closely related issue is where the attributes should be stored.
Every file system maintains file attributes, such as each file’s owner and creation time, and they must be stored somewhere.
One obvious possibility is to store them directly in the directory entry.
In its simplest form, a directory consists of a list of fixedsize entries, one per file, containing a (fixed-length) file name, a structure of the.
For systems that use i-nodes, another possibility for storing the attributes is in the i-nodes, rather than in the directory entries, as in Fig.
In this case, the directory entry can be shorter: just a file name and an i-node number.
In UNIX the use of i-nodes for storing file attributes makes sharing easy; any number of directory entries can point to a single i-node.
The i-node contains a field which is incremented when a new link is added, and which is decremented when a link is deleted.
Only when the link count reaches zero are the actual data and the i-node itself deleted.
This kind of link is sometimes called a hard link.
A major limitation is that directories and i-nodes are data structures of a single file system (partition), so a directory in one file system cannot point to an i-node on another file system.
Also, a file can have only one owner and one set of permissions.
If the owner of a shared file deletes his own directory entry for that file, another user could be stuck with a file in his directory that he cannot delete if the permissions do not allow it.
An alternative way to share files is to create a new kind of file whose data is the path to another file.
This kind of link will work across mounted file systems.
In fact, if a means is provided for path names to include network addresses, such a link can refer to a file on a different computer.
This second kind of link is called a symbolic link in UNIX-like systems, a shortcut in Windows, and an alias in Apple’s Mac OS.
Symbolic links can be used on systems where attributes are stored within directory entries.
A little thought should convince you that multiple directory entries containing file attributes would be difficult to synchronize.
Any change to a file would have to affect every directory entry for that file.
But the extra directory entries for symbolic links do not contain the attributes of the file to which they point.
A disadvantage of symbolic links is that when a file is deleted, or even just renamed, a link becomes an orphan.
The file system of the original release of Windows 95 was identical to the MS-DOS file system, but a second release added support for longer file names and bigger files.
The base directory entry has all the information that was in the directory entries of older Windows versions, and more.
How about long file names? The answer to the problem of providing long file names while retaining compatibility with the older systems was to use additional directory entries.
For files with long names a shortened form of the name is generated automatically.
A bit in the Sequence field tells the system which is the last entry.
Providing backward compatibility so an earlier simpler system can continue to function while providing additional features for a newer system is likely to be messy.
A purist might decide not to go to so much trouble.
However, a purist would probably not become rich selling new versions of operating systems.
The traditional UNIX directory structure is extremely simple, as shown in Fig.
Each entry contains just a file name and its i-node number.
All the information about the type, size, times, ownership, and disk blocks is contained in the i-node.
Some UNIX systems have a different layout, but in all cases, a directory entry ultimately contains only an ASCII string and an i-node number.
When a file is opened, the file system must take the file name supplied and locate its disk blocks.
Let us consider how the path name /usr/ast/mbox is looked up.
We will use UNIX as an example, but the algorithm is basically the same for all hierarchical directory systems.
The first entry in this array is the i-node of the root directory.
The file system looks up the first component of the path, usr, in the root directory to find the i-node number of the file /usr/
Locating an i-node from its number is straightforward, since each one has a fixed location relative to the first one.
From this i-node, the system locates the directory for /usr/ and looks up the next component, ast, in it.
When it has found the entry for ast, it has the i-node for the directory /usr/ast/
From this i-node it can find the directory itself and look up mbox.
The i-node for this file is then read into memory and kept there until the file is closed.
Relative path names are looked up the same way as absolute ones, only starting from the working directory instead of starting from the root directory.
As far as the directory system is concerned, they are just ordinary ASCII strings, just the same as any other names.
We do not have space for a detailed description of NTFS, but will just briefly look at some of the problems NTFS deals with and the solutions used.
But since older versions of Windows cannot read NTFS file systems, a complicated backwardcompatible directory structure is not needed, and filename fields are variable length.
Unicode uses 16 bits for each character, enough to represent multiple languages with very large symbol sets (e.g., Japanese)
But using multiple languages raises problems in addition to representation of different character sets.
For instance, in Spanish some combinations of two characters count as single characters when sorting.
If the default is to make filenames case sensitive, there may still be a need to do case-insensitive searches.
For Latin-based languages it is obvious how to do that, at least to native users of these languages.
In general, if only one language is in use, users will probably know the rules.
However, Unicode allows a mixture of languages: Greek, Russian, and Japanese filenames could all appear in a single directory at an international organization.
The NTFS solution is an attribute for each file that defines the case conventions for the language of the filename.
In NTFS a file is a collection of attributes, and each attribute is a stream of bytes.
If that is not enough, an attribute within the MFT can be a header that points to an additional file with an extension of the attribute values.
The MFT itself is a file, and it has an entry for every file and directory in the file system.
Since it can grow very large, when an NTFS file system is created about 12.5% of the space on the partition is reserved for growth of the MFT.
Thus it can grow without becoming fragmented, at least until the initial reserved space is used, after which another large chunk of space will be reserved.
So if the MFT becomes fragmented it will consists of a small number of very large fragments.
What about data in NTFS? Data is just another attribute.
In fact an NTFS file may have more than one data stream.
This feature was originally provided to allow Windows servers to serve files to Apple MacIntosh clients.
In the original MacIntosh operating system (through Mac OS 9) all files had two data streams, called the resource fork and the data fork.
Multiple data streams have other uses, for instance a large graphic image may have a smaller thumbnail image associated with it.
At the other extreme, NTFS can handle small files by putting a few hundred bytes in the attribute header.
This is called an immediate file (Mullender and Tanenbaum, 1984)
We have only touched upon a few ways that NTFS deals with issues not addressed by older and simpler file systems.
Describing all these features and their implementation would require much more space than we can spare here.
For a more throrough look at NTFS see Tanenbaum (2001) or look on the World Wide Web for more information.
Files are normally stored on disk, so management of disk space is a major concern to file system designers.
Two general strategies are possible for storing an n byte file: n consecutive bytes of disk space are allocated, or the file is split up into a number of (not necessarily) contiguous blocks.
The same trade-off is present in memory management systems between pure segmentation and paging.
As we have seen, storing a file as a contiguous sequence of bytes has the obvious problem that if a file grows, it will probably have to be moved on the disk.
The same problem holds for segments in memory, except that moving a segment in memory is a relatively fast operation compared to moving a file from one disk position to another.
For this reason, nearly all file systems chop files up into fixed-size blocks that need not be adjacent.
Once it has been decided to store files in fixed-size blocks, the question arises of how big the blocks should be.
Given the way disks are organized, the sector, the track and the cylinder are obvious candidates for the unit of allocation (although these are all device dependent, which is a minus)
In a paging system, the page size is also a major contender.
However, having a large allocation unit, such as a cylinder, means that every file, even a 1-byte file, ties up an entire cylinder.
On the other hand, using a small allocation unit means that each file will consist of many blocks.
Reading each block normally requires a seek and a rotational delay, so reading a file consisting of many small blocks will be slow.
The time in milliseconds to read a block of k bytes is then the sum of the seek, rotational delay, and transfer times:
To compute the space efficiency, we need to make an assumption about the mean file size.
As an aside, the median is a better metric than the mean because a very small number of files can influence the mean enormously, but not the median.
A few 100-MB hardware manuals or a promotional videos or to can greatly skew the mean but have little effect on the median.
In an experiment to see if Windows NT file usage was appreciably different from UNIX file usage, Vogels (1999) made measurements on files at Cornell University.
He observed that NT file usage is more complicated than on UNIX.
Given the fact that Cornell has considerable large-scale scientific computing and the difference in measurement technique (static versus dynamic), the results are reasonably consistent with a median file size of around 2 KB.
For simplicity, let us assume all files are 2 KB, which leads to the dashed curve in Fig.
The solid curve (left-hand scale) gives the data rate of a disk.
The dashed curve (right-hand scale) gives the disk space efficiency.
The access time for a block is completely dominated by the seek time and rotational delay, so given that it is going to cost 14 msec to access a block, the more data that are fetched, the better.
Hence the data rate goes up with block size (until the transfers take so long that the transfer time begins to dominate)
In reality, few files are a multiple of the disk block size, so some space is always wasted in the last block of a file.
What the curves show, however, is that performance and space utilization are inherently in conflict.
Small blocks are bad for performance but good for disk space utilization.
For this data, 4 KB might be a good choice, but some operating systems made their choices a long time ago, when the disk parameters and file sizes were different.
Once a block size has been chosen, the next issue is how to keep track of free blocks.
The first one consists of using a linked list of disk blocks, with each block holding as many free disk block numbers as will fit.
Often free blocks are used to hold the free list.
A disk with n blocks requires a bitmap with n bits.
Only if the disk is nearly full (i.e., has few free blocks) will the linked list scheme require fewer blocks than the bitmap.
On the other hand, if there are many blocks free, some of them can be borrowed to hold the free list without any loss of disk capacity.
When the free list method is used, only one block of pointers need be kept in main memory.
When a file is created, the needed blocks are taken from the block of pointers.
When it runs out, a new block of pointers is read in from the disk.
Similarly, when a file is deleted, its blocks are freed and added to the block of pointers in main memory.
When this block fills up, it is written to disk.
Destruction of a file system is often a far greater disaster than destruction of a computer.
If a computer is destroyed by fire, lightning surges, or a cup of coffee poured onto the keyboard, it is annoying and will cost money, but generally a replacement can be purchased with a minimum of fuss.
Inexpensive personal computers can even be replaced within an hour by just going to the dealer (except at universities, where issuing a purchase order takes three committees, five signatures, and 90 days)
If a computer’s file system is irrevocably lost, whether due to hardware, software, or rats gnawing on the backup tapes, restoring all the information will be difficult and time consuming at best, and in many cases will be impossible.
For the people whose programs, documents, customer files, tax records, databases, marketing plans, or other data are gone forever, the consequences can be catastrophic.
While the file system cannot offer any protection against physical destruction of the equipment and media, it can help protect the information.
In this section we will look at some of the issues involved in safeguarding the file system.
Floppy disks are generally perfect when they leave the factory, but they can develop bad blocks during use.
It is arguable that this is more likely now than it was in the days when floppy disks were more widely used.
Networks and large capacity removable devices such as writeable CDs have led to floppy disks being used infrequently.
Cooling fans draw air and airborne dust in through floppy disk drives, and a drive that has not been used for a long time may be so dirty that it ruins the next disk that is inserted.
A floppy drive that is used frequently is less likely to damage a disk.
Hard disks frequently have bad blocks right from the start: it is just too expensive to manufacture them completely free of all defects.
On these disks, tracks are at least one sector bigger than needed, so that at least one bad spot can be skipped by leaving it in a gap between two consecutive sectors.
A few spare sectors are provided on each cylinder so the controller can do automatic sector remapping if it notices that a sector needs more than a certain number of retries to be read or written.
Thus the user is usually unaware of bad blocks or their management.
Nevertheless, when a modern IDE or SCSI disk fails, it will usually fail horribly, because it has run out of spare sectors.
If the driver notes this and displays a message on the monitor the user will know it is time to buy a new disk when these messages begin to appear frequently.
A simple software solution to the bad block problem exists, suitable for use on older disks.
This approach requires the user or file system to carefully construct a file containing all the bad blocks.
This technique removes them from the free list, so they will never occur in data files.
As long as the bad block file is never read or written, no problems will arise.
Care has to be taken during disk backups to avoid reading this file and trying to back it up.
Most people do not think making backups of their files is worth the time and effort—until one fine day their disk abruptly dies, at which time most of them undergo a deathbed conversion.
Companies, however, (usually) well understand the value of their data and generally do a backup at least once a day, usually to tape.
Modern tapes hold tens or sometimes even hundreds of gigabytes and cost pennies per gigabyte.
Nevertheless, making backups is not quite as trivial as it sounds, so we will examine some of the related issues below.
Backups to tape are generally made to handle one of two potential problems:
The first one covers getting the computer running again after a disk crash, fire, flood, or other natural catastrophe.
In practice, these things do not happen very often, which is why many people do not bother with backups.
These people also tend not to have fire insurance on their houses for the same reason.
The second reason is that users often accidentally remove files that they later need again.
This problem occurs so often that when a file is ‘‘removed’’ in Windows, it is not deleted at all, but just moved to a special directory, the recycle bin, so it can be fished out and restored easily later.
Backups take this principle further and allow files that were removed days, even weeks ago, to be restored from old backup tapes.
Making a backup takes a long time and occupies a large amount of space, so doing it efficiently and conveniently is important.
First, should the entire file system be backed up or only part of it? At many installations, the executable (binary) programs are kept in a limited part of the file system tree.
It is not necessary to back up these files if they can all be reinstalled from the manufacturers’ CD-ROMs.
There is usually no reason to back it up either.
In UNIX, all the special files (I/O devices) are kept in a directory /dev/
Not only is backing up this directory not necessary, it is downright dangerous because the backup program would hang forever if it tried to read each of these to completion.
In short, it is usually desirable to back up only specific directories and everything in them rather than the entire file system.
Second, it is wasteful to back up files that have not changed since the last backup, which leads to the idea of incremental dumps.
The simplest form of incremental dumping is to make a complete dump (backup) periodically, say weekly or monthly, and to make a daily dump of only those files that have been modified since the last full dump.
Even better is to dump only those files that have changed since they were last dumped.
While this scheme minimizes dumping time, it makes recovery more complicated because first the most recent full dump has to be restored, followed by all the incremental dumps in reverse order, oldest one first.
To ease recovery, more sophisticated incremental dumping schemes are often used.
Third, since immense amounts of data are typically dumped, it may be desirable to compress the data before writing them to tape.
However, with many compression algorithms, a single bad spot on the backup tape can foil the decompression algorithm and make an entire file or even an entire tape unreadable.
Thus the decision to compress the backup stream must be carefully considered.
Fourth, it is difficult to perform a backup on an active file system.
If files and directories are being added, deleted, and modified during the dumping process, the resulting dump may be inconsistent.
However, since making a dump may take hours, it may be necessary to take the system offline for much of the night to make the backup, something that is not always acceptable.
For this reason, algorithms have been devised for making rapid snapshots of the file system state by copying critical data structures, and then requiring future changes to files and directories to copy the blocks instead of updating them in place (Hutchinson et al., 1999)
In this way, the file system is effectively frozen at the moment of the snapshot, so it can be backed up at leisure afterward.
Fifth and last, making backups introduces many nontechnical problems into an organization.
The best online security system in the world may be useless if the system administrator keeps all the backup tapes in his office and leaves it open and unguarded whenever he walks down the hall to get output from the printer.
All a spy has to do is pop in for a second, put one tiny tape in his pocket, and saunter off jauntily.
Also, making a daily backup has little use if the fire that burns down the computers also burns up all the backup tapes.
For this reason, backup tapes should be kept off-site, but that introduces more security.
For a thorough discussion of these and other practical administration issues, see Nemeth et al.
Below we will discuss only the technical issues involved in making file system backups.
Two strategies can be used for dumping a disk to tape: a physical dump or a logical dump.
A physical dump starts at block 0 of the disk, writes all the disk blocks onto the output tape in order, and stops when it has copied the last one.
Such a program is so simple that it can probably be made 100% bug free, something that can probably not be said about any other useful program.
Nevertheless, it is worth making several comments about physical dumping.
For one thing, there is no value in backing up unused disk blocks.
If the dumping program can get access to the free block data structure, it can avoid dumping unused blocks.
However, skipping unused blocks requires writing the number of each block in front of the block (or the equivalent), since it is no longer true that block k on the tape was block k on the disk.
If all bad blocks are remapped by the disk controller and hidden from the operating system as we described in Sec.
On the other hand, if they are visible to the operating system and maintained in one or more ‘‘bad block files’’ or bitmaps, it is absolutely essential that the physical dumping program get access to this information and avoid dumping them to prevent endless disk read errors during the dumping process.
The main advantages of physical dumping are simplicity and great speed (basically, it can run at the speed of the disk)
The main disadvantages are the inability to skip selected directories, make incremental dumps, and restore individual files upon request.
A logical dump starts at one or more specified directories and recursively dumps all files and directories found there that have changed since some given base date (e.g., the last backup for an incremental dump or system installation for a full dump)
Thus in a logical dump, the dump tape gets a series of carefully identified directories and files, which makes it easy to restore a specific file or directory upon request.
In order to be able to properly restore even a single file correctly, all information needed to recreate the path to that file must be saved to the backup medium.
Thus the first step in doing a logical dump is doing an analysis of the directory tree.
Obviously, we need to save any file or directory that has been modified.
But for proper restoration, all directories, even unmodified ones, that lie on the path to a modified file or directory must be saved.
This means saving not just the data (file names and pointers to i-nodes), all the attributes of the directories must be saved, so they can be restored with the original permissions.
The directories and their attributes are written to the tape first, and then modified files (with their attributes) are saved.
This makes it possible to restore the dumped files and directories to a fresh file system on a different computer.
In this way, the dump and restore programs can be used to transport entire file systems between computers.
A second reason for dumping unmodified directories above modified files is to make it possible to incrementally restore a single file (possibly to handle recovery from accidental deletion)
Suppose that a full file system dump is done Sunday evening and an incremental dump is done on Monday evening.
On Tuesday the directory /usr/jhs/proj/nr3/ is removed, along with all the directories and files under it.
To get their owners, modes, times, etc., correct, these directories must be present on the dump tape even though they themselves were not modified since the previous full dump.
Restoring a file system from the dump tapes is straightforward.
To start with, an empty file system is created on the disk.
Since the directories appear first on the tape, they are all restored first, giving a skeleton of the file system.
This process is then repeated with the first incremental dump made after the full dump, then the next one, and so on.
Although logical dumping is straightforward, there are a few tricky issues.
For one, since the free block list is not a file, it is not dumped and hence it must be reconstructed from scratch after all the dumps have been restored.
Doing so is always possible since the set of free blocks is just the complement of the set of blocks contained in all the files combined.
If a file is linked to two or more directories, it is important that the file is restored only one time and that all the directories that are supposed to point to it do so.
Still another issue is the fact that UNIX files may contain holes.
It is legal to open a file, write a few bytes, then seek to a distant file offset and write a few more bytes.
The blocks in between are not part of the file and should not be dumped and not be restored.
Core dump files often have a large hole between the data segment and the stack.
Finally, special files, named pipes, and the like should never be dumped, no matter in which directory they may occur (they need not be confined to /dev/)
For more information about file system backups, see Chervenak et al.
Another area where reliability is an issue is file system consistency.
Many file systems read blocks, modify them, and write them out later.
If the system crashes before all the modified blocks have been written out, the file system can.
This problem is especially critical if some of the blocks that have not been written out are i-node blocks, directory blocks, or blocks containing the free list.
To deal with the problem of inconsistent file systems, most computers have a utility program that checks file system consistency.
For example, UNIX has fsck and Windows has chkdsk (or scandisk in earlier versions)
This utility can be run whenever the system is booted, especially after a crash.
Chkdsk is somewhat different because it works on a different file system, but the general principle of using the file system’s inherent redundancy to repair it is still valid.
All file system checkers verify each file system (disk partition) independently of the other ones.
Two kinds of consistency checks can be made: blocks and files.
The counters in the first table keep track of how many times each block is present in a file; the counters in the second table record how often each block is present in the free list (or the bitmap of free blocks)
Starting from an i-node, it is possible to build a list of all the block numbers used in the corresponding file.
As each block number is read, its counter in the first table is incremented.
The program then examines the free list or bitmap, to find all the blocks that are not in use.
Each occurrence of a block in the free list results in its counter in the second table being incremented.
If the file system is consistent, each block will have a 1 either in the first table or in the second table, as illustrated in Fig.
However, as a result of a crash, the tables might look like Fig.
While missing blocks do no real harm, they do waste space and thus reduce the capacity of the disk.
The solution to missing blocks is straightforward: the file system checker just adds them to the free list.
Here we see a block, number 4, that occurs twice in the free list.
Duplicates can occur only if the free list is really a list; with a bitmap it is impossible.
The solution here is also simple: rebuild the free list.
The worst thing that can happen is that the same data block is present in two or more files, as shown in Fig.
If either of these files is removed, block 5 will be put on the free list, leading to a situation in which the same block is both in use and free at the same time.
If both files are removed, the block will be put onto the free list twice.
The appropriate action for the file system checker to take is to allocate a free block, copy the contents of block 5 into it, and insert the copy into one of the files.
In this way, the information content of the files is unchanged (although almost assuredly one is garbled), but the file system structure is at least made consistent.
The error should be reported, to allow the user to inspect the damage.
In addition to checking to see that each block is properly accounted for, the file system checker also checks the directory system.
It, too, uses a table of counters, but these are per file, rather than per block.
It starts at the root directory and recursively descends the tree, inspecting each directory in the file system.
For every file in every directory, it increments a counter for that file’s usage count.
Remember that due to hard links, a file may appear in two or more directories.
Symbolic links do not count and do not cause the counter for the target file to be incremented.
When it is all done, it has a list, indexed by i-node number, telling how many directories contain each file.
It then compares these numbers with the link counts stored in the i-nodes themselves.
These counts start at 1 when a file is created and are incremented each time a (hard) link is made to the file.
However, two kinds of errors can occur: the link count in the i-node can be too high or it can be too low.
If the link count is higher than the number of directory entries, then even if all the files are removed from the directories, the count will still be nonzero and the i-node will not be removed.
This error is not serious, but it wastes space on the disk with files that are not in any directory.
It should be fixed by setting the link count in the i-node to the correct value.
If two directory entries are linked to a file, but the i-node says that there is only one, when either directory entry is removed, the i-node count will go to zero.
When an i-node count goes to zero, the file system marks it as unused and releases all of its blocks.
This action will result in one of the directories now pointing to an unused i-node, whose blocks may soon be assigned to other files.
Again, the solution is just to force the link count in the i-node to the actual number of directory entries.
These two operations, checking blocks and checking directories, are often integrated for efficiency reasons (i.e., only one pass over the i-nodes is required)
For example, directories have a definite format, with i-node numbers and ASCII names.
If an i-node number is larger than the number of i-nodes on the disk, the directory has been damaged.
Furthermore, each i-node has a mode, some of which are legal but strange, such as 0007, which allows the owner and his group no access at all, but allows outsiders to read, write, and execute the file.
It might be useful to at least report files that give outsiders more rights than the owner.
Directories with more than, say, 1000 entries are also suspicious.
Files located in user directories, but which are owned by the superuser and have the SETUID bit on, are potential security problems because such files acquire the powers of the superuser when executed by any user.
With a little effort, one can put together a fairly long list of technically legal but still peculiar situations that might be worth reporting.
The previous paragraphs have discussed the problem of protecting the user against crashes.
Some file systems also worry about protecting the user against himself.
In some systems, when a file is removed, all that happens is that a bit is set in the directory or i-node marking the file as removed.
No disk blocks are returned to the free list until they are actually needed.
Thus, if the user discovers the error immediately, it is possible to run a special utility program that ‘‘unremoves’’ (i.e., restores) the removed files.
In Windows, files that are removed are placed in the recycle bin, from which they can later be retrieved if need be.
Of course, no storage is reclaimed until they are actually deleted from this directory.
A secure system would actually overwrite the data blocks with zeros or random bits when a disk is deleted, so another user could not retrieve it.
Confidential or sensitive data can often be recovered from disks that have been discarded (Garfinkel and Shelat, 2003)
Access to disk is much slower than access to memory.
If only a single word is needed, the memory access is on the order of a million times as fast as disk access.
As a result of this difference in access time, many file systems have been designed with various optimizations to improve performance.
The most common technique used to reduce disk accesses is the block cache or buffer cache.
Cache ispronounced ‘‘cash’’ and is derived from the French cacher, meaning to hide.
In this context, a cache is a collection of blocks that logically belong on the disk but are being kept in memory for performance reasons.
Various algorithms can be used to manage the cache, but a common one is to check all read requests to see if the needed block is in the cache.
If it is, the read request can be satisfied without a disk access.
If the block is not in the cache, it is first read into the cache, and then copied to wherever it is needed.
Subsequent requests for the same block can be satisfied from the cache.
Since there are many (often thousands of) blocks in the cache, some way is needed to determine quickly if a given block is present.
The usual way is to hash the device and disk address and look up the result in a hash table.
All the blocks with the same hash value are chained together on a linked list so the collision chain can be followed.
When a block has to be loaded into a full cache, some block has to be removed (and rewritten to the disk if it has been modified since being brought in)
This situation is very much like paging, and all the usual page replacement algorithms described in Chap.
One pleasant difference between paging and caching is that cache references are relatively infrequent, so that it is feasible to keep all the blocks in exact LRU order with linked lists.
When a block is referenced, it can be removed from its position on the bidirectional list and put at the end.
Now that we have a situation in which exact LRU is possible, it turns out that LRU is undesirable.
The problem has to do with the crashes and file system consistency discussed in the previous section.
If a critical block, such as an i-node block, is read into the cache and modified, but not rewritten to the disk, a crash will leave the file system in an inconsistent state.
If the i-node block is put at the end of the LRU chain, it may be quite a while before it reaches the front and is rewritten to the disk.
Furthermore, some blocks, such as i-node blocks, are rarely referenced twice within a short interval.
These considerations lead to a modified LRU scheme, taking two factors into account:
Is the block essential to the consistency of the file system?
For both questions, blocks can be divided into categories such as i-node blocks, indirect blocks, directory blocks, full data blocks, and partially full data blocks.
Blocks that will probably not be needed again soon go on the front, rather than the rear of the LRU list, so their buffers will be reused quickly.
Blocks that might be needed again soon, such as a partly full block that is being written, go on the end of the list, so they will stay around for a long time.
If the block is essential to the file system consistency (basically, everything except data blocks), and it has been modified, it should be written to disk immediately, regardless of which end of the LRU list it is put on.
By writing critical blocks quickly, we greatly reduce the probability that a crash will wreck the file system.
While a user may be unhappy if one of his files is ruined in a crash, he is likely to be far more unhappy if the whole file system is lost.
Even with this measure to keep the file system integrity intact, it is undesirable to keep data blocks in the cache too long before writing them out.
Consider the plight of someone who is using a personal computer to write a book.
Even if our writer periodically tells the editor to write the file being edited to the disk, there is a good chance that everything will still be in the cache and nothing on the disk.
If the system crashes, the file system structure will not be corrupted, but a whole day’s work will be lost.
This situation need not happen very often before we have a fairly unhappy user.
The UNIX way is to have a system call, sync, which forces all the modified blocks out onto the disk immediately.
As a result, no more than 30 seconds of work is lost due to a system crash, a comforting thought for many people.
The Windows way is to write every modified block to disk as soon as it has been written.
Caches in which all modified blocks are written back to the disk immediately are called write-through caches.
The difference between these two approaches can be seen when a program writes a 1-KB block full, one character at a time.
Windows will make a disk access for every character written.
Of course, most programs do internal buffering, so they normally write not a character, but a line or a larger unit on each write system call.
A consequence of this difference in caching strategy is that just removing a (floppy) disk from a UNIX system without doing a sync will almost always result in lost data, and frequently in a corrupted file system as well.
These differing strategies were chosen because UNIX was developed in an environment in which all disks were hard disks and not removable, whereas Windows started out in the floppy disk world.
As hard disks became the norm, the UNIX approach, with its better efficiency, became the norm, and is also used now on Windows for hard disks.
A second technique for improving perceived file system performance is to try to get blocks into the cache before they are needed to increase the hit rate.
When the file system is asked to produce block k in a file, it does that, but when it is finished, it makes a sneaky check in the cache to see if block k + 1 is already there.
If it is not, it schedules a read for block k + 1 in the hope that when it is needed, it will have already arrived in the cache.
At the very least, it will be on the way.
Of course, this read ahead strategy only works for files that are being read sequentially.
If a file is being randomly accessed, read ahead does not help.
In fact, it hurts by tying up disk bandwidth reading in useless blocks and removing potentially useful blocks from the cache (and possibly tying up more disk bandwidth writing them back to disk if they are dirty)
To see whether read ahead is worth doing, the file system can keep track of the access patterns to each open file.
However, whenever a seek is done, the bit is cleared.
If sequential reads start happening again, the bit is set once again.
In this way, the file system can make a reasonable guess about whether it should read ahead or not.
If it gets it wrong once it a while, it is not a disaster, just a little bit of wasted disk bandwidth.
Caching and read ahead are not the only ways to increase file system performance.
Another important technique is to reduce the amount of disk arm motion by putting blocks that are likely to be accessed in sequence close to each other, preferably in the same cylinder.
When an output file is written, the file system has to allocate the blocks one at a time, as they are needed.
If the free blocks are recorded in a bitmap, and the whole bitmap is in main memory, it is easy enough to choose a free block as close as possible to the previous block.
With a free list, part of which is on disk, it is much harder to allocate blocks close together.
However, even with a free list, some block clustering can be done.
The trick is to keep track of disk storage not in blocks, but in groups of consecutive blocks.
A variation on the same theme is to take account of rotational positioning.
When allocating blocks, the system attempts to place consecutive blocks in a file in the same cylinder.
Another performance bottleneck in systems that use i-nodes or anything equivalent to i-nodes is that reading even a short file requires two disk accesses: one for the i-node and one for the block.
Here all the i-nodes are near the beginning of the disk, so the average distance between an i-node and its blocks will be about half the number of cylinders, requiring long seeks.
Disk is divided into cylinder groups, each with its own i-nodes.
One easy performance improvement is to put the i-nodes in the middle of the disk, rather than at the start, thus reducing the average seek between the i-node.
When creating a new file, any i-node can be chosen, but an attempt is made to find a block in the same cylinder group as the i-node.
If none is available, then a block in a nearby cylinder group is used.
Changes in technology are putting pressure on current file systems.
In particular, CPUs keep getting faster, disks are becoming much bigger and cheaper (but not much faster), and memories are growing exponentially in size.
The one parameter that is not improving by leaps and bounds is disk seek time.
The combination of these factors means that a performance bottleneck is arising in many file systems.
Research done at Berkeley attempted to alleviate this problem by designing a completely new kind of file system, LFS (the Log-structured File System)
In this section we will briefly describe how LFS works.
For a more complete treatment, see Rosenblum and Ousterhout (1991)
The idea that drove the LFS design is that as CPUs get faster and RAM memories get larger, disk caches are also increasing rapidly.
Consequently, it is now possible to satisfy a very substantial fraction of all read requests directly from the file system cache, with no disk access needed.
It follows from this observation, that in the future, most disk accesses will be writes, so the read-ahead mechanism used in some file systems to fetch blocks before they are needed no longer gains much performance.
To see where all the small writes come from, consider creating a new file on a UNIX system.
To write this file, the i-node for the directory, the directory block, the i-node for the file, and the file itself must all be written.
While these writes can be delayed, doing so exposes the file system to serious consistency problems if a crash occurs before the writes are done.
For this reason, the i-node writes are generally done immediately.
From this reasoning, the LFS designers decided to re-implement the UNIX file system in such a way as to achieve the full bandwidth of the disk, even in the face of a workload consisting in large part of small random writes.
The basic idea is to structure the entire disk as a log.
Periodically, and also when there is a special need for it, all the pending writes being buffered in memory are collected into a single segment and written to the disk as a single contiguous segment at the end of the log.
A single segment may thus contain i-nodes, directory blocks, data blocks, and other kinds of blocks all mixed together.
If the average segment can be made to be about 1 MB, almost the full bandwidth of the disk can be utilized.
In this design, i-nodes still exist and have the same structure as in UNIX, but they are now scattered all over the log, instead of being at a fixed position on the disk.
Nevertheless, when an i-node is located, locating the blocks is done in the usual way.
Of course, finding an i-node is now much harder, since its address cannot simply be calculated from its i-node number, as in UNIX.
To make it possible to find i-nodes, an i-node map, indexed by i-node number, is maintained.
Entry i in this map points to i-node i on the disk.
The map is kept on disk, but it is also cached, so the most heavily used parts will be in memory most of the time in order to improve performance.
To summarize what we have said so far, all writes are initially buffered in memory, and periodically all the buffered writes are written to the disk in a single segment, at the end of the log.
Opening a file now consists of using the map to locate the i-node for the file.
Once the i-node has been located, the addresses of the blocks can be found from it.
All of the blocks will themselves be in segments, somewhere in the log.
If disks were infinitely large, the above description would be the entire story.
However, real disks are finite, so eventually the log will occupy the entire disk, at which time no new segments can be written to the log.
Fortunately, many existing segments may have blocks that are no longer needed, for example, if a file is overwritten, its i-node will now point to the new blocks, but the old ones will still be occupying space in previously written segments.
To deal with this problem, LFS has a cleaner thread that spends its time scanning the log circularly to compact it.
It starts out by reading the summary of the first segment in the log to see which i-nodes and files are there.
It then checks the current i-node map to see if the i-nodes are still current and file blocks are still in use.
The i-nodes and blocks that are still in use go into memory to be written out in the next segment.
The original segment is then marked as free, so the log can use it for new data.
In this manner, the cleaner moves along the log, removing old segments from the back and putting any live data into memory for rewriting in the next segment.
Consequently, the disk is a big circular buffer, with the writer thread adding new segments to the front and the cleaner thread removing old ones from the back.
The bookkeeping here is nontrivial, since when a file block is written back to a new segment, the i-node of the file (somewhere in the log) must be located, updated, and put into memory to be written out in the next segment.
The i-node map must then be updated to point to the new copy.
Nevertheless, it is possible to do the administration, and the performance results show that all this complexity is worthwhile.
Measurements given in the papers cited above show that LFS outperforms UNIX by an order of magnitude on small writes, while having a performance that is as good as or better than UNIX for reads and large writes.
File systems generally contain information that is highly valuable to their users.
Protecting this information against unauthorized usage is therefore a major concern of all file systems.
In the following sections we will look at a variety of issues concerned with security and protection.
These issues apply equally well to timesharing systems as to networks of personal computers connected to shared servers via local area networks.
Nevertheless, it is frequently useful to make a distinction between the general problems involved in making sure that files are not read or modified by unauthorized persons, which include technical, administrative, legal, and political issues on the one hand, and the specific operating system mechanisms used to provide security, on the other.
To avoid confusion, we will use the term security to refer to the overall problem, and the term protection mechanisms to refer to the specific operating system mechanisms used to safeguard information in the computer.
First we will look at security to see what the nature of the problem is.
Later on in the chapter we will look at the protection mechanisms and models available to help achieve security.
Three of the more important ones are the nature of the threats, the nature of intruders, and accidental data loss.
From a security perspective, computer systems have three general goals, with corresponding threats to them, as listed in Fig.
The first one, data confidentiality, is concerned with having secret data remain secret.
More specifically, if the owner of some data has decided that these data are only to be made available to certain people and no others, the system should guarantee that release of the data to unauthorized people does not occur.
As a bare minimum, the owner should be able to specify who can see what, and the system should enforce these specifications.
The second goal, data integrity, means that unauthorized users should not be able to modify any data without the owner’s permission.
Data modification in this context includes not only changing the data, but also removing data and adding false data as well.
If a system cannot guarantee that data deposited in it remain unchanged until the owner decides to change them, it is not worth much as an information system.
Another aspect of the security problem is privacy: protecting individuals from misuse of information about them.
Should the government compile dossiers on everyone in order to catch Xcheaters, where X is ‘‘welfare’’ or ‘‘tax,’’ depending on your politics? Should the police be able to look up anything on anyone in order to stop organized crime? Do employers and insurance companies have rights? What happens when these rights conflict with individual rights? All of these issues are extremely important but are beyond the scope of this book.
Most people are pretty nice and obey the law, so why worry about security? Because there are unfortunately a few people around who are not so nice and want to cause trouble (possibly for their own commercial gain)
In the security literature, people who are nosing around places where they have no business being are called intruders or sometimes adversaries.
Passive intruders just want to read files they are not authorized to read.
Active intruders are more malicious; they want to make unauthorized changes.
When designing a system to be secure against intruders, it is important to keep in mind the kind of intruder one is trying to protect against.
Many people have personal computers on their desks that are connected to a shared file server, and human nature being what it is, some of them will read other people’s electronic mail and other files if no barriers are placed in the way.
Most UNIX systems, for example, have the default that all newly created files are publicly readable.
Students, system programmers, operators, and other technical personnel often consider it to be a personal challenge to break the security of the local computer system.
They often are highly skilled and are willing to devote a substantial amount of time to the effort.
Some bank programmers have attempted to steal from the bank they were working for.
Schemes have varied from changing the software to truncate rather than round interest, keeping the fraction of a cent for themselves, to siphoning off accounts not used in years, to blackmail (‘‘Pay me or I will destroy all the bank’s records.’’)
Espionage refers to a serious and well-funded attempt by a competitor or a foreign country to steal programs, trade secrets, patentable ideas, technology, circuit designs, business plans, and so forth.
Often this attempt will involve wiretapping or even erecting antennas directed at the computer to pick up its electromagnetic radiation.
It should be clear that trying to keep a hostile foreign government from stealing military secrets is quite a different matter from trying to keep students from inserting a funny message-of-the-day into the system.
The amount of effort needed for security and protection clearly depends on who the enemy is thought to be.
Another category of security pest is malicious programs, sometimes called malware.
In a sense, a writer of malware is also an intruder, often with high technical skills.
The difference between a conventional intruder and malware is that the former refers to a person who is personally trying to break into a system to cause damage whereas the latter is a program written by such a person and then released into the world.
Some malware seems to have been written just to cause damage, but some is targeted more specifically.
The most well known kind of malware is the virus.
Basically a virus is a piece of code that can reproduce itself by attaching a copy of itself to another program, analogous to how biological viruses reproduce.
The virus can do other things in addition to reproducing itself.
For example, it can type a message, display an image on the screen, play music, or something else harmless.
Unfortunately, it can also modify, destroy, or steal files (by e-mailing them somewhere)
Another thing a virus can do is to render the computer unusable as long as the virus is running.
Viruses (and the other forms of malware to be described) can also be used to cause a DDOS (Distributed Denial Of Service) attack.
In this case the virus does not do anything immediately upon infecting a computer.
At a predetermined date and time thousands of copies of the virus on computers all over the world start requesting web pages or other network services from their target, for instance the Web site of a political party or a corporation.
This can overload the targeted server and the networks that service it.
Much (if not most) unwanted junk e-mail (‘‘spam’’) is relayed to its final destinations by networks of computers that have been infected by viruses or other forms of malware.
A computer infected by such a rogue program becomes a slave, and reports its status to its master, somewhere on the Internet.
The master then sends spam to be relayed to all the e-mail addresses that can be gleaned from e-mail address books and other files on the slave.
Another kind of malware for profit scheme installs a key logger on an infected computer.
It is not too difficult to filter this data and extract information such as usernamepassword combinations or credit card numbers and expiration dates.
This information is then sent back to a master where it can be used or sold for criminal use.
Whereas a virus is spread by attaching itself to another program, and is executed when its host program is executed, a worm is a free-standing program.
Worms spread by using networks to transmit copies of themselves to other computers.
Windows systems always have a Startup directory for each user; any program in that folder will be executed when the user logs in.
So all the worm has to do is arrange to put itself (or a shortcut to itself) in the Startup directory on a remote system.
Other ways exist, some much more difficult to detect, to cause a remote computer to execute a program file that has been copied to its file system.
The effects of a worm can be the same as those of a virus.
Indeed, the distinction between a virus and a worm is not always clear; some malware uses both methods to spread.
This is a program that apparently performs a valid function—perhaps it is a game or a supposedly ‘‘improved’’ version of a useful utility.
But when the Trojan horse is executed some other function is performed, perhaps launching a worm or virus or performing one of the nasty things that malware does.
The effects of a Trojan horse are likely to be subtle and stealthy.
Unlike worms and viruses, Trojan horses are voluntarily downloaded by users, and as soon as they are recognized for what they are and the word gets out, a Trojan horse will be deleted from reputable download sites.
This device is a piece of code written by one of a company’s (currently employed) programmers and secretly inserted into the production operating system.
As long as the programmer feeds it its daily password, it does nothing.
Going off might involve clearing the disk, erasing files at random, carefully making hard-to-detect changes to key programs, or encrypting essential files.
In the latter case, the company has a tough choice about whether to call the police (which may or may not result in a conviction many months later) or to give in to this blackmail and to rehire the ex-programmer as a ‘‘consultant’’ for an astronomical sum to fix the problem (and hope that he does not plant new logic bombs while doing so)
In its simplest form spyware may be nothing more than a cookie.
Cookies are small files exchanged between web browsers and web servers.
A cookie contains some information that will allow the Web site to identify you.
It is like the ticket you get when you leave a bicycle to be repaired.
When you return to the shop, your half of the ticket gets matched with your bicycle (and its repair bill)
Web connections are not persistent, so, for example, if you indicate an interest in buying this book when visiting an online bookstore, the bookstore asks your browser to accept a cookie.
When you have finished browsing and perhaps have selected other books to buy, you click on the page where your order is finalized.
At that point the web server asks your browser to return the cookies it has stored from the current session, It can use the information in these to generate the list of items you have said you want to buy.
Normally, cookies used for a purpose like this expire quickly.
But some Web sites use cookies for purposes that are not so benign.
For instance, advertisements on Web sites are often furnished by companies other than the information provider.
If a cookie is placed when you visit a page with information about, say, bicycle equipment, and you then go to another Web site that sells clothing, the same advertising company may provide ads on this page, and may collect cookies you obtained elsewhere.
Thus you may suddenly find yourself viewing ads for special gloves or jackets especially made for cyclists.
Advertisers can collect a lot of information about your interests this way; you may not want to share so much information about yourself.
What is worse, there are various ways a Web site may be able to download executable program code to your computer.
Most browsers accept plug-ins to add additional function, such as displaying new kinds of files.
Users often accept offers for new plugins without knowing much about what the plugin does.
Or a user may willingly accept an offer to be provided with a new cursor for the desktop that looks like a dancing kitten.
And a bug in a web browser may allow a remote site to install an unwanted program, perhaps after luring the user to a page that has been carefully constructed to take advantage of the vulnerability.
Any time a program is accepted from another source, voluntarily or not, there is a risk it could contain code that does you harm.
In addition to threats caused by malicious intruders, valuable data can be lost by accident.
Some of the common causes of accidental data loss are.
Acts of God: fires, floods, earthquakes, wars, riots, or rats gnawing tapes or floppy disks.
Hardware or software errors: CPU malfunctions, unreadable disks or tapes, telecommunication errors, program bugs.
Human errors: incorrect data entry, wrong tape or disk mounted, wrong program run, lost disk or tape, or some other mistake.
Most of these can be dealt with by maintaining adequate backups, preferably far away from the original data.
While protecting data against accidental loss may seem mundane compared to protecting against clever intruders, in practice, probably more damage is caused by the former than the latter.
The usual way to test a system’s security is to hire a group of experts, known as tiger teams or penetration teams, to see if they can break in.
In the course of the years, these penetration teams have discovered a number of areas in which systems are likely to be weak.
Below we have listed some of the more common attacks that are often successful.
When designing a system, be sure it can withstand attacks like these.
Request memory pages, disk space, or tapes and just read them.
Many systems do not erase them before allocating them, and they may be full of interesting information written by the previous owner.
Try illegal system calls, or legal system calls with illegal parameters, or even legal system calls with legal but unreasonable parameters.
Start logging in and then hit DEL, RUBOUT or BREAK halfway through the login sequence.
In some systems, the password checking program will be killed and the login considered successful.
Try modifying complex operating system structures kept in user space (if any)
In some systems (especially on mainframes), to open a file, the program builds a large data structure containing the file name and many other parameters and passes it to the system.
As the file is read and written, the system sometimes updates the structure itself.
Spoof the user by writing a program that types ‘‘login:’’ on the screen and go away.
Many users will walk up to the terminal and willingly tell it their login name and password, which the program carefully records for its evil master.
Convince a system programmer to change the system to skip certain vital security checks for any user with your login name.
All else failing, the penetrator might find the computer center director’s secretary and offer a large bribe.
The secretary probably has easy access to all kinds of wonderful information, and is usually poorly paid.
Many other sources of information on security and testing security can be found, especially on the Web.
Saltzer and Schroeder (1975) have identified several general principles that can be used as a guide to designing secure systems.
A brief summary of their ideas (based on experience with MULTICS) is given below.
Assuming that the intruder will not know how the system works serves only to delude the designers.
Errors in which legitimate access is refused will be reported much faster than errors in which unauthorized access is allowed.
The system should not check for permission, determine that access is permitted, and then squirrel away this information for subsequent use.
Many systems check for permission when a file is opened, and not afterward.
This means that a user who opens a file, and keeps it open for weeks, will continue to have access, even if the owner has long since changed the file protection.
If an editor has only the authority to access the file to be edited (specified when the editor is invoked), editors with Trojan horses will not be able to do much damage.
Fifth, the protection mechanism should be simple, uniform, and built into the lowest layers of the system.
Trying to retrofit security to an existing insecure system is nearly impossible.
If users feel that protecting their files is too much work, they just will not do it.
Replies of the form ‘‘It is your own fault’’ will generally not be well received.
Many protection schemes are based on the assumption that the system knows the identity of each user.
The problem of identifying users when they log in is called user authentication.
Most authentication methods are based on identifying something the user knows, something the user has, or something the user is.
The most widely used form of authentication is to require the user to type a password.
Password protection is easy to understand and easy to implement.
In UNIX it works like this: The login program asks the user to type his name and password.
The login program then reads the password file, which is a series of ASCII lines, one per user, until it finds the line containing the user’s login name.
If the (encrypted) password contained in this line matches the encrypted password just computed, the login is permitted, otherwise it is refused.
One frequently reads about groups of high school, or even junior high school students who, with the aid of their trusty home computers, have broken into some top secret system owned by a large corporation or government agency.
Virtually all the time the break-in consists of guessing a user name and password combination.
They compiled a list of likely passwords: first and last names, street names, city names, words from a moderate-sized dictionary (also words spelled backward), license plate numbers, and short strings of random characters.
They then encrypted each of these using the known password encryption algorithm and checked to see if any of the encrypted passwords matched entries in their list.
Over 86 percent of all passwords turned up in their list.
Even if it is considered politically impossible to require users to pick reasonable passwords, Morris and Thompson have described a technique that renders their own attack (encrypting a large number of passwords in advance) almost useless.
Their idea is to associate an n-bit random number with each password.
The random number is changed whenever the password is changed.
The random number is stored in the password file in unencrypted form, so that everyone can read it.
Instead of just storing the encrypted password in the password file, the password and the random number are first concatenated and then encrypted together.
Now consider the implications for an intruder who wants to build up a list of likely passwords, encrypt them, and save the results in a sorted file, f, so that any encrypted password can be looked up easily.
If an intruder suspects that Marilyn might be a password, it is no longer sufficient just to encrypt Marilyn and put the result in f.
Some versions of UNIX make the password file itself unreadable but provide a program to look up entries upon request, adding just enough delay to greatly slow down any attacker.
Although this method offers protection against intruders who try to precompute a large list of encrypted passwords, it does little to protect a user David whose password is also David.
One way to encourage people to pick better passwords is to have the computer offer advice.
Some computers have a program that generates random easy-to-pronounce nonsense words, such as fotally, garbungy, or bipitty that can be used as passwords (preferably with some upper case and special characters thrown in)
Other computers require users to change their passwords regularly, to limit the damage done if a password leaks out.
The most extreme form of this approach is the one-time password.
When one-time passwords are used, the user gets a book containing a list of passwords.
If an intruder ever discovers a password, it will not do him any good, since next time a different password must be used.
It is suggested that the user try to avoid losing the password book.
It goes almost without saying that while a password is being typed in, the computer should not display the typed characters, to keep them from prying eyes near the terminal.
What is less obvious is that passwords should never be stored in the computer in unencrypted form.
Furthermore, not even the computer center management should have unencrypted copies.
A variation on the password idea is to have each new user provide a long list of questions and answers that are then stored in the computer in encrypted form.
The questions should be chosen so that the user does not need to write them down.
In other words, they should be things no one forgets.
At login, the computer asks one of them at random and checks the answer.
The algorithm can be different in the morning and afternoon, on different days of the week, from different terminals, and so on.
A completely different approach to authorization is to check to see if the user has some item, normally a plastic card with a magnetic stripe on it.
The card is inserted into the terminal, which then checks to see whose card it is.
Yet another approach is to measure physical characteristics that are hard to forge.
For example, a fingerprint or a voiceprint reader in the terminal could verify the user’s identity.
It makes the search go faster if the user tells the computer who he is, rather than making the computer compare the given fingerprint to the entire data base.
Direct visual recognition is not yet feasible but may be one day.
The user signs his name with a special pen connected to the terminal, and the computer compares it to a known specimen stored on line.
Even better is not to compare the signature, but compare the pen motions made while writing it.
A good forger may be able to copy the signature, but will not have a clue as to the exact order in which the strokes were made.
When this is used, each terminal has a device like the one of Fig.
The user inserts his hand into it, and the length of each of his fingers is measured and checked against the data base.
We could go on and on with more examples, but two more will help make an important point.
Cats and other animals mark off their territory by urinating around its perimeter.
Suppose that someone comes up with a tiny device capable of doing an instant urinalysis, thereby providing a foolproof identification.
Each terminal could be equipped with one of these devices, along with a discreet sign reading: ‘‘For login, please deposit sample here.’’ This might be an absolutely unbreakable system, but it would probably have a fairly serious user acceptance problem.
The same could be said of a system consisting of a thumbtack and a small spectrograph.
The user would be requested to jab his thumb against the thumbtack, thus extracting a drop of blood for spectrographic analysis.
The point is that any authentication scheme must be psychologically acceptable to the user community.
Finger-length measurements probably will not cause any problem, but even something as nonintrusive as storing fingerprints on line may be unacceptable to many people.
Computer installations that are really serious about security—and few are until the day after an intruder has broken in and done major damage—often take steps to make unauthorized entry much harder.
For example, each user could be allowed to log in only from a specific terminal, and only during certain days of the week and hours of the day.
Dial-up telephone lines could be made to work as follows.
Anyone can dial up and log in, but after a successful login, the system immediately breaks the connection and calls the user back at an agreed upon number.
This measure means than an intruder cannot just try breaking in from any phone line; only the user’s (home) phone will do.
In any event, with or without call back, the system should take at least 10 seconds to check any password typed in on a dial-up line, and should increase this time after several consecutive unsuccessful login attempts, in.
After three failed login attempts, the line should be disconnected for 10 minutes and security personnel notified.
When a user logs in, the system should report the time and terminal of the previous login, so he can detect possible break ins.
The next step up is laying baited traps to catch intruders.
A simple scheme is to have one special login name with an easy password (e.g., login name: guest, password: guest)
Whenever anyone logs in using this name, the system security specialists are immediately notified.
Other traps can be easy-to-find bugs in the operating system and similar things, designed for the purpose of catching intruders in the act.
Stoll (1989) has written an entertaining account of the traps he set to track down a spy who broke into a university computer in search of military secrets.
In the previous sections we have looked at many potential problems, some of them technical, some of them not.
In the following sections we will concentrate on some of the detailed technical ways that are used in operating systems to protect files and other things.
All of these techniques make a clear distinction between policy (whose data are to be protected from whom) and mechanism (how the system enforces the policy)
The separation of policy and mechanism is discussed by Sandhu (1993)
In some systems, protection is enforced by a program called a reference monitor.
Every time an access to a potentially protected resource is attempted, the system first asks the reference monitor to check its legality.
The reference monitor then looks at its policy tables and makes a decision.
Below we will describe the environment in which a reference monitor operates.
A computer system contains many ‘‘objects’’ that need to be protected.
These objects can be hardware (e.g., CPUs, memory segments, disk drives, or printers), or they can be software (e.g., processes, files, databases, or semaphores)
Each object has a unique name by which it is referenced, and a finite set of operations that processes are allowed to carry out on it.
The read and write operations are appropriate to a file; up and down make sense on a semaphore.
It is obvious that a way is needed to prohibit processes from accessing objects that they are not authorized to access.
Furthermore, this mechanism must also make it possible to restrict processes to a subset of the legal operations when that is needed.
For example, process A may be entitled to read, but not write, file F.
In order to discuss different protection mechanisms, it is useful to introduce the concept of a domain.
Each pair specifies an object and some subset of the operations that can be performed on it.
A right in this context means permission to perform one of the operations.
Often a domain corresponds to a single user, telling what the user can do and not do, but a domain can also be more general than just one user.
Figure 5-24 shows three domains, showing the objects in each domain and the rights [Read, Write, eXecute] available on each object.
Note that Printer1 is in two domains at the same time.
Although not shown in this example, it is possible for the same object to be in multiple domains, with different rights in each one.
At every instant of time, each process runs in some protection domain.
In other words, there is some collection of objects it can access, and for each object it has some set of rights.
Processes can also switch from domain to domain during execution.
To make the idea of a protection domain more concrete, let us look at UNIX.
In UNIX, the domain of a process is defined by its UID and GID.
Two processes with the same (UID, GID) combination will have access to exactly the same set of objects.
Processes with different (UID, GID) values will have access to a different set of files, although there may be considerable overlap in most cases.
Furthermore, each process in UNIX has two halves: the user part and the kernel part.
When the process does a system call, it switches from the user part to the kernel part.
The kernel part has access to a different set of objects from the user part.
For example, the kernel can access all the pages in physical memory, the entire disk, and all the other protected resources.
When a process does an exec on a file with the SETUID or SETGID bit on, it acquires a new effective UID or GID.
With a different (UID, GID) combination, it has a different set of files and operations available.
Running a program with SETUID or SETGID is also a domain switch, since the rights available change.
An important question is how the system keeps track of which object belongs to which domain.
Conceptually, at least, one can envision a large matrix, with the.
Each box lists the rights, if any, that the domain contains for the object.
Given this matrix and the current domain number, the system can tell if an access to a given object in a particular way from a specified domain is allowed.
Domain switching itself can be easily included in the matrix model by realizing that a domain is itself an object, with the operation enter.
Most domains have no access at all to most objects, so storing a very large, mostly empty, matrix is a waste of disk space.
Two methods that are practical, however, are storing the matrix by rows or by columns, and then storing.
In this section we will look at storing it by column; in the next one we will study storing it by row.
The first technique consists of associating with each object an (ordered) list containing all the domains that may access the object, and how.
This list is called the Access Control List or ACL and is illustrated in Fig.
Here we see three processes, each belonging to a different domain.
For simplicity, we will assume that each domain corresponds to exactly one user, in this case, users A, B, and C.
Often in the security literature, the users are called subjects or principals, to contrast them with the things owned, the objects, such as files.
File F1 has two entries in its ACL (separated by a semicolon)
The first entry says that any process owned by user A may read and write the file.
The second entry says that any process owned by user B may read the file.
All other accesses by these users and all accesses by other users are forbidden.
Note that the rights are granted by user, not by process.
As far as the protection system goes, any process owned by user A can read and write file F1
It does not matter if there is one such process or 100 of them.
It is the owner, not the process ID, that matters.
File F2 has three entries in its ACL: A, B, and C can all read the file, and in addition B can also write it.
File F3 is apparently an executable program, since B and C can both read and execute it.
This example illustrates the most basic form of protection with ACLs.
To start with, we have only shown three rights so far: read, write, and execute.
Some of these may be generic, that is, apply to all objects, and some may be object specific.
Examples of generic rights are destroy object and copy object.
These could hold for any object, no matter what type it is.
Object-specific rights might include append message for a mailbox object and sort alphabetically for a directory object.
So far, our ACL entries have been for individual users.
Many systems support the concept of a group of users.
In some systems, each process has a user ID (UID) and group ID (GID)
In such systems, an ACL entry contains entries of the form.
Under these conditions, when a request is made to access an object, a check is made using the caller’s UID and GID.
If they are present in the ACL, the rights listed are available.
If the (UID, GID) combination is not in the list, the access is not permitted.
Using groups this way effectively introduces the concept of a role.
Consider an installation in which Tana is system administrator, and thus in the group sysadm.
However, suppose that the company also has some clubs for employees and Tana is a member of the pigeon fanciers club.
Club members belong to the group pigfan and have access to the company’s computers for managing their pigeon database.
A portion of the ACL might be as shown in Fig.
If Tana tries to access one of these files, the result depends on which group she is currently logged in as.
When she logs in, the system may ask her to choose which of her groups she is currently using, or there might even be different login names and/or passwords to keep them separate.
The point of this scheme is to prevent Tana from accessing the password file when she currently has her pigeon fancier’s hat on.
She can only do that when logged in as the system administrator.
In some cases, a user may have access to certain files independent of which group she is currently logged in as.
That case can be handled by introducing wildcards, which mean everyone.
Yet another possibility is that if a user belongs to any of the groups that have certain access rights, the access is permitted.
In this case, a user belonging to multiple groups does not have to specify which group to use at login time.
A disadvantage of this approach is that it provides less encapsulation: Tana can edit the password file during a pigeon club meeting.
The use of groups and wildcards introduces the possibility of selectively blocking a specific user from accessing a file.
It sometimes occurs that a user or a group has certain permissions with respect to a file that the file owner later wishes to revoke.
With access control lists, it is relatively straightforward to revoke a previously granted access.
All that has to be done is edit the ACL to make the change.
However, if the ACL is checked only when a file is opened, most likely the change will only take effect on future calls to open.
Any file that is already open will continue to have the rights it had when it was opened, even if the user is no longer authorized to access the file at all.
The other way of slicing up the matrix of Fig.
When this method is used, associated with each process is a list of objects that may be accessed, along with an indication of which operations are permitted on each, in other words, its domain.
A set of three processes and their capability lists is shown in Fig.
Each capability grants the owner certain rights on a certain object.
Usually, a capability consists of a file (or more generally, an object) identifier and a bitmap for the various rights.
In a UNIX-like system, the file identifier would probably be the i-node number.
Capability lists are themselves objects and may be pointed to from other capability lists, thus facilitating sharing of subdomains.
It is fairly obvious that capability lists must be protected from user tampering.
When capabilities are used, each process has a capability list.
The tag bit is not used by arithmetic, comparison, or similar ordinary instructions, and it can be modified only by programs running in kernel mode (i.e., the operating system)
Taggedarchitecture machines have been built and can be made to work well (Feustal, 1972)
The second way is to keep the C-list inside the operating system.
Capabilities are then referred to by their position in the capability list.
The third way is to keep the C-list in user space, but manage the capabilities cryptographically so that users cannot tamper with them.
This approach is particularly suited to distributed systems and works as follows.
When a client process sends a message to a remote server, for example, a file server, to create an object for it, the server creates the object and generates a long random number, the check field, to go with it.
A slot in the server’s file table is reserved for the object and the check field is stored there along with the addresses of the disk blocks, etc.
In UNIX terms, the check field is stored on the server in the i-node.
It is not sent back to the user and never put on the network.
The server then generates and returns a capability to the user of the form shown in Fig.
The capability returned to the user contains the server’s identifier, the object number (the index into the server’s tables, essentially, the i-node number), and the.
For a newly created object, all the rights bits are turned on.
When the user wishes to access the object, it sends the capability to the server as part of the request.
The server then extracts the object number to index into its tables to find the object.
It then computes f(Object, Rights, Check) taking the first two parameters from the capability itself and the third one from its own tables.
If the result agrees with the fourth field in the capability, the request is honored; otherwise, it is rejected.
If a user tries to access someone else’s object, he will not be able to fabricate the fourth field correctly since he does not know the check field, and the request will be rejected.
This new capability is sent back to the requesting process.
The user can now give this to a friend by just sending it in a message.
If the friend turns on rights bits that should be off, the server will detect this when the capability is used since the f value will not correspond to the false rights field.
Since the friend does not know the true check field, he cannot fabricate a capability that corresponds to the false rights bits.
This scheme was developed for the Amoeba system and used extensively there (Tanenbaum et al., 1990)
Copy capability: create a new capability for the same object.
Copy object: create a duplicate object with a new capability.
Remove capability: delete an entry from the C-list; object unaffected.
A last remark worth making about capability systems is that revoking access to an object is quite difficult in the kernel-managed version.
It is hard for the system to find all the outstanding capabilities for any object to take them back, since they may be stored in C-lists all over the disk.
One approach is to have each capability point to an indirect object, rather than to the object itself.
By having the indirect object point to the real object, the system can always break that connection, thus invalidating the capabilities.
When a capability to the indirect object is later presented to the system, the user will discover that the indirect object is now pointing to a null object.
All that needs to be done is change the check field stored with the object.
However, neither scheme allows selective revocation, that is, taking back, say, John’s permission, but nobody else’s.
This defect is generally recognized to be a problem with all capability systems.
Another general problem is making sure the owner of a valid capability does not give a copy to 1000 of his best friends.
Having the kernel manage capabilities, as in Hydra, solves this problem, but this solution does not work well in a distributed system such as Amoeba.
On the other hand, capabilities solve the problem of sandboxing mobile code very elegantly.
When a foreign program is started, it is given a capability list containing only those capabilities that the machine owner wants to give it, such as the ability to write on the screen and the ability to read and write files in one scratch directory just created for it.
If the mobile code is put into its own process with only these limited capabilities, it will not be able to access any other system resources and thus be effectively confined to a sandbox without the need to modify its code or run it interpretively.
Running code with as few access rights as possible is known as the principle of least privilege and is a powerful guideline for producing secure systems.
With ACLs, a (potentially long) search of the ACL may be needed.
If groups are not supported, then granting everyone read access to a file requires enumerating all users in the ACL.
Capabilities also allow a process to be encapsulated easily, whereas ACLs do not.
On the other hand, ACLs allow selective revocation of rights, which capabilities do not.
Finally, if an object is removed and the capabilities are not or the capabilities are removed and an object is not, problems arise.
Even with access control lists and capabilities, security leaks can still occur.
In this section we discuss how information can still leak out even when it has been rigorously proven that such leakage is mathematically impossible.
Lampson’s model was originally formulated in terms of a single timesharing system, but the same ideas can be adapted to LANs and other multiuser environments.
In the purest form, it involves three processes on some protected machine.
The first process is the client, which wants some work performed by the second one, the server.
The client and the server do not entirely trust each other.
For example, the server’s job is to help clients with filling out their tax forms.
The server is worried that the clients will try to steal the valuable tax program.
The third process is the collaborator, which is conspiring with the server to indeed steal the client’s confidential data.
The collaborator and server are typically owned by the same person.
The object of this exercise is to design a system in which it is impossible for the server process to leak to the collaborator process the information that it has legitimately received from the client process.
From the system designer’s point of view, the goal is to encapsulate or confine the server in such a way that it cannot pass information to the collaborator.
Using a protection matrix scheme we can easily guarantee that the server cannot communicate with the collaborator by writing a file to which the collaborator has read access.
We can probably also ensure that the server cannot communicate with the collaborator using the system’s normal interprocess communication mechanism.
For example, the server can try to communicate a binary bit stream as follows: To send a 1 bit, it computes as hard as it can for a fixed interval of time.
To send a 0 bit, it goes to sleep for the same length of time.
The collaborator can try to detect the bit stream by carefully monitoring its response time.
This communication channel is known as a covert channel, and is illustrated in Fig.
Of course, the covert channel is a noisy channel, containing a lot of extraneous information, but information can be reliably sent over a noisy channel by using an error-correcting code (e.g., a Hamming code, or even something more sophisticated)
The use of an error-correcting code reduces the already low bandSEC.
It is fairly obvious that no protection model based on a matrix of objects and domains is going to prevent this kind of leakage.
Modulating the CPU usage is not the only covert channel.
In fact, almost any way of degrading system performance in a clocked way is a candidate.
On some systems, it may be possible for a process to detect the status of a lock even on a file that it cannot access.
In this example, the secret bit stream 11010100 is being transmitted.
Locking and unlocking a prearranged file, S is not an especially noisy channel, but it does require fairly accurate timing unless the bit rate is very low.
The reliability and performance can be increased even more using an acknowledgement protocol.
After the server locks or unlocks S, it flips the lock status of F1 to indicate that a bit has been sent.
Since timing is no longer involved, this protocol is fully reliable, even in a busy system and can proceed as fast as the two processes can get scheduled.
This call works even though the collaborator has no permission to use the file.
Lampson also mentioned a way of leaking information to the (human) owner of the server process.
Presumably the server process will be entitled to tell its owner how much work it did on behalf of the client, so the client can be billed.
Just finding all the covert channels, let alone blocking them, is extremely difficult.
Introducing a process that causes page faults at random, or otherwise spends its time degrading system performance in order to reduce the bandwidth of the covert channels is not an attractive proposition.
Like any file system, the MINIX 3 file system must deal with all the issues we have just studied.
It must allocate and deallocate space for files, keep track of disk blocks and free space, provide some way to protect files against unauthorized usage, and so on.
In the remainder of this chapter we will look closely at MINIX 3 to see how it accomplishes these goals.
In the first part of this chapter, we have repeatedly referred to UNIX rather than to MINIX 3 for the sake of generality, although the external interfaces of the two is virtually identical.
To read and write files, user processes send messages to the file system telling what they want done.
The file system does the work and then sends back a reply.
The file system is, in fact, a network file server that happens to be running on the same machine as the caller.
For another, it is very easy to move the file system to any computer that has a C compiler, compile it there, and use it as a free-standing UNIX-like remote file server.
The only changes that need to be made are in the area of how messages are sent and received, which differs from system to system.
In the following sections, we will present an overview of many of the key areas of the file system design.
Specifically, we will look at messages, the file system layout, the bitmaps, i-nodes, the block cache, directories and paths, file descriptors, file locking, and special files (plus pipes)
After studying these topics, we will show a simple example of how the pieces fit together by tracing what happens when a user process executes the read system call.
The code status as reply value means OK or ERROR.
The file system accepts 39 types of messages requesting work.
Of the system calls, 31 are accepted from user processes.
Six system call messages are for system calls which are handled first by the process manager, which then calls the file system to do a part of the work.
Two other messages are also handled by the file system.
The structure of the file system is basically the same as that of the process manager and all the I/O device drivers.
It has a main loop that waits for a message to arrive.
When a message arrives, its type is extracted and used as an index into a table containing pointers to the procedures within the file system that handle all the types.
Then the appropriate procedure is called, it does its work and returns a status value.
The file system then sends a reply back to the caller and goes back to the top of the loop to wait for the next message.
A MINIX 3 file system is a logical, self-contained entity with i-nodes, directories, and data blocks.
It can be stored on any block device, such as a floppy disk or a hard disk partition.
In all cases, the layout of the file system has the same structure.
Even for a floppy disk, only 64 inodes puts a severe limit on the number of files, so rather than the four blocks reserved for i-nodes in the figure, more would probably be used.
Reserving eight blocks for i-nodes would be more practical but our diagram would not look as nice.
For a modern hard disk, both the i-node and zone bitmaps will be much larger than 1 block, of course.
But all the components are always present and in the same order.
When the computer is turned on, the hardware reads the boot block from the boot device into memory, jumps to it, and begins executing its code.
The boot block code begins the process of loading the operating system itself.
Once the system has been booted, the boot block is not used any more.
Not every disk drive can be used as a boot device, but to keep the structure uniform, every block device has a block reserved for boot block code.
At worst this strategy wastes one block.To prevent the hardware from trying to boot an unbootable device, a magic number.
When booting from a device, the hardware (actually, the BIOS code) will refuse to attempt to load from a device lacking the magic number.
Doing this prevents inadvertently using garbage as a boot program.
The superblock contains information describing the layout of the file system.
Like the boot block, the superblock is always 1024 bytes, regardless of the block size used for the rest of the file system.
The main function of the superblock is to tell the file system how big the various pieces of the file system are.
Given the block size and the number of i-nodes, it is easy to calculate the size of the i-node bitmap and the number of blocks of inodes.
With 64 i-nodes, four disk blocks are needed to contain them all.
The zone bitmap keeps track of free storage in zones, not blocks.
Note that the number of blocks per zone is not stored in the superblock, as it is never needed.
All that is needed is the base 2 logarithm of the zone to block ratio, which is used as the shift count to convert zones to blocks and vice versa.
Magic number padding Block size (bytes) FS sub-version Pointer to i-node for root of mounted file system.
Pointer to i-node mounted upon i-nodes/block Device number Read-only flag Native or byte-swapped flag FS version Direct zones/i-node Indirect zones/indirect block First free bit in i-node bitmap First free bit in zone bitmap.
The zone bitmap includes only the data zones (i.e., the blocks used for the bitmaps and i-nodes are not in the map), with the first data zone designated zone 1 in the bitmap.
One is for the nonexistent 0th i-node or zone; the other is for the i-node and zone used by the root directory on the device, which is placed there when the file system is created.
The information in the superblock is redundant because sometimes it is needed in one form and sometimes in another.
With 1 KB devoted to the superblock, it makes sense to compute this information in all the forms it is needed, rather than having to recompute it frequently during execution.
The rest of the superblock is wasted anyhow, so using up another word of it costs nothing.
When MINIX 3 is booted, the superblock for the root device is read into a table in memory.
Similarly, as other file systems are mounted, their superblocks are also brought into memory.
The superblock table holds a number of fields not present on the disk.
These include flags that allow a device to be specified as read-only or as following a byte-order convention opposite to the standard, and fields to speed access by indicating points in the bitmaps below which all bits are marked used.
In addition, there is a field describing the device from which the superblock came.
Before a disk can be used as a MINIX 3 file system, it must be given the structure of Fig.
The utility program mkfs has been provided to build file systems.
This program can be called either by a command like.
This command also puts a magic number in the superblock to identify the file system as a valid MINIX file system.
The MINIX file system has evolved, and some aspects of the file system (for instance, the size of i-nodes) were different previously.
The magic number identifies the version of mkfs that created the file system, so differences can be accommodated.
Attempts to mount a file system not in MINIX 3 format, such as an MS-DOS diskette, will be rejected by the mount system call, which checks the superblock for a valid magic number and other things.
When a file is removed, it is then a simple matter to calculate which block of the bitmap contains the bit for the i-node being freed and to find it using the normal cache mechanism.
Zones are released from the zone bitmap in the same way.
Logically, when a file is to be created, the file system must search through the bit-map blocks one at a time for the first free i-node.
In fact, the in-memory copy of the superblock has a field which points to the first free i-node, so no search is necessary until after a node is used, when the pointer must be updated to point to the new next free i-node, which will often turn out to be the next one, or a close one.
Everything that has been said here about the inode bitmaps also applies to the zone bitmap; logically it is searched for the first free zone when space is needed, but a pointer to the first free zone is maintained to eliminate most of the need for sequential searches through the bitmap.
With this background, we can now explain the difference between zones and blocks.
The idea behind zones is to help ensure that disk blocks that belong to the same file are located on the same cylinder, to improve performance when the file is read sequentially.
The approach chosen is to make it possible to allocate several blocks at a time.
Disk transfers are always a block at a time, and the buffer cache also works with individual blocks.
Only a few parts of the system that keep track of physical disk addresses (e.g., the zone bitmap and the i-nodes) know about zones.
Some design decisions had to be made in developing the MINIX 3 file system.
In 1985, when MINIX was conceived, disk capacities were small, and it was expected that many users would have only floppy disks.
As MINIX developed, and larger disks became much more common, it was obvious that changes were desirable.
Many files are smaller than 1 KB, so increasing the block size would mean wasting disk bandwidth, reading and writing mostly empty blocks and wasting precious main memory storing them in the buffer cache.
The zone size could have been increased, but a larger zone size means more wasted disk space, and it was still desirable to retain efficient operation on small disks.
Another reasonable alternative would have been to have different zone sizes on large and small devices.
In the end it was decided to increase the size of disk pointers to 32 bits.
Increasing the size of disk pointers required an increase in the size of i-nodes.
Suppose that a file is of length 1-KB, meaning that one zone has been allocated for it.
Reads beyond the end of a file always return a count of 0 and no data.
Subsequent seeks to byte 1024 followed by attempts to read the data will now be able to read the previous contents of the block, a major security breach.
The solution is to check for this situation when a write is done beyond the end of a file, and explicitly zero all the not-yet-allocated blocks in the zone that was previously the last one.
Although this situation rarely occurs, the code has to deal with it, making the system slightly more complex.
The layout of the MINIX 3 i-node is given in Fig.
It is almost the same as a standard UNIX i-node.
The MINIX 3 i-node access, modification time and i-node change times are standard, as in UNIX.
The last of these is updated for almost every file operation except a read of the file.
When a file is opened, its i-node is located and brought into the inode table in memory, where it remains until the file is closed.
The inode table has a few additional fields not present on the disk, such as the i-node’s device and number, so the file system knows where to rewrite the i-node if it is modified while in memory.
If the same file is opened more than once, only one copy of the i-node is kept in memory, but the counter is incremented each time the file is opened and decremented each time the file is closed.
Only when the counter finally reaches zero is the i-node removed from the table.
If it has been modified since being loaded into memory, it is also rewritten to the disk.
The main function of a file’s i-node is to tell where the data blocks are.
The first seven zone numbers are given right in the i-node itself.
Beyond 7 KB, indirect zones are needed, using the scheme of Fig.
File type and rwx bits Directory entries for this file Identifies user who owns file Owner’s group.
Zone numbers for the first seven data zones in the file.
The i-node also holds the mode information, which tells what kind of a file it is (regular, directory, block special, character special, or pipe), and gives the protection and SETUID and SETGID bits.
The link field in the i-node records how many directory entries point to the i-node, so the file system knows when to release the file’s storage.
This field should not be confused with the counter (present only in the inode table in memory, not on the disk) that tells how many times the file is currently open, typically by different processes.
As a final note on i-nodes, we mention that the structure of Fig.
An example used in MINIX 3 is the i-nodes for block and character device special files.
These do not need zone pointers, because they don’t have to reference data areas on the disk.
The major and minor device numbers are stored in the Zone-0 space in Fig.
Another way an inode could be used, although not implemented in MINIX 3, is as an immediate file with a small amount of data stored in the i-node itself.
The cache is implemented as a fixed array of buffers, each consisting of a header containing pointers, counters, and flags, and a body with room for one disk block.
All the buffers that are not in use are chained together in a double-linked list, from most recently used (MRU) to least recently used (LRU) as illustrated in Fig.
In addition, to be able to quickly determine if a given block is in the cache or not, a hash table is used.
All the buffers containing a block that has hash code k are linked together on a single-linked list pointed to by entry k in the hash table.
The hash function just extracts the low-order n bits from the block number, so.
At that time all the other hash table entries contain a null pointer, but once the system starts, buffers will be removed from the 0th chain and other chains will be built.
Once a block has been chosen for eviction from the block cache, another flag in its header is checked to see if the block has been modified since being read in.
At this point the block needed is read in by sending a message to the disk driver.
The file system is suspended until the block arrives, at which time it continues and a pointer to the block is returned to the caller.
Whether to put the block on the front or rear of the LRU list.
Whether to write the block (if modified) to disk immediately or not.
Almost all blocks go on the rear of the list in true LRU fashion.
The exception is blocks from the RAM disk; since they are already in memory there is little advantage to keeping them in the block cache.
It reaches the front of the LRU chain and is evicted.
Sync does not traverse the LRU chain but instead indexes through the array of.
Even if a buffer has not been released yet, if it has been modified, sync will find it and ensure that the copy on disk is updated.
In an older version of MINIX a superblock was modified when a file system was mounted, and was always rewritten immediately to reduce the chance of corrupting the file system in the event of a crash.
Superblocks are modified only if the size of a RAM disk must be adjusted at startup time because the RAM disk was created bigger than the RAM image device.
However, the superblock is not read or written as a normal block, because it is always 1024 bytes in size, like the boot block, regardless of the block size used for blocks handled by the cache.
This was intended to make the file system more robust; the price paid was slower operation.
A power failure occurring when all blocks have not been yet been written is going to cause a headache whether it is an i-node or a data block that is lost.
Another important subsystem within the file system manages directories and path names.
Many system calls, such as open, have a file name as a parameter.
What is really needed is the i-node for that file, so it is up to the file system to look up the file in the directory tree and locate its i-node.
The use of fixed-length directory entries, in this case, 64 bytes, is an example of a tradeoff involving simplicity, speed, and storage.
It is also very fast for both looking up names and storing new ones, since no heap management is ever required.
The price paid is wasted disk storage, because most files are much shorter than 60 characters.
It is our firm belief that optimizing to save disk storage (and some RAM storage since directories are occasionally in memory) is the wrong choice.
Code simplicity and correctness should come first and speed should come second.
With modern disks usually exceeding 100 GB, saving a small amount of disk space at the price of more complicated and slower code is generally not a good idea.
Unfortunately, many programmers grew up in an era of tiny disks and even tinier RAMs, and were trained from day 1 to resolve all trade-offs between code complexity, speed, and space in favor of minimizing space requirements.
This implicit assumption really has to be reexamined in light of current realities.
Now let us see how the path /usr/ast/mbox/ is looked up.
The system first looks up usr in the root directory, then it looks up ast in /usr/, and finally it looks up mbox in /usr/ast/
The actual lookup proceeds one path component at a time, as illustrated in Fig.
The only complication is what happens when a mounted file system is encountered.
The usual configuration for MINIX 3 and many other UNIX-like systems is to have a small root file system containing the files needed to start the system and to do basic system maintenance, and to have the majority of the files, including users’ directories, on a separate device mounted on /usr.
This is a good time to look at how mounting is done.
The file systems before and after mounting are shown in Fig.
When a path such as /usr/ast/f2 is being looked up, the file system will see a flag in the i-node for /usr/ and realize that it must continue searching at the root inode of the file system mounted on /usr/
The question is: ‘‘How does it find this root i-node?’’
The system searches all the superblocks in memory until it finds the one whose i-node mounted on field points to /usr/
This must be the superblock for the file system mounted on /usr/
Once it has the superblock, it is easy to follow the other pointer to find the root i-node for the mounted file system.
Once a file has been opened, a file descriptor is returned to the user process for use in subsequent read and write calls.
In this section we will look at how file descriptors are managed within the file system.
Like the kernel and the process manager, the file system maintains part of the process table within its address space.
The first two are pointers to the i-nodes for the root directory and the working directory.
The third interesting field in the process table is an array indexed by file descripttor number.
It is used to locate the proper file when a file descriptor is presented.
At first glance, it might seem sufficient to have the k-th entry in this array just point to the i-node for the file belonging to file descriptor k.
After all, the i-node is fetched into memory when the file is opened and kept there until it is closed, so it is sure to be available.
Unfortunately, this simple plan fails because files can be shared in subtle ways in MINIX 3 (as well as in UNIX)
The trouble arises because associated with each file is a 32-bit number that indicates the next byte to be read or written.
It is this number, called the file position, that is changed by the lseek system call.
The problem can be stated easily: ‘‘Where should the file pointer be stored?’’
The first possibility is to put it in the i-node.
Unfortunately, if two or more processes have the same file open at the same time, they must all have their own file pointers, since it would hardly do to have an lseek by one process affect the next read of a different process.
What about putting it in the process table? Why not have a second array, paralleling the file descriptor array, giving the current position of each file? This idea does not work either, but the reasoning is more subtle.
Basically, the trouble comes from the semantics of the fork system call.
When a process forks, both the parent and the child are required to share a single pointer giving the current position of each open file.
To better understand the problem, consider the case of a shell script whose output has been redirected to a file.
This position is then inherited by the child, which writes, say, 1 KB of output.
Now the shell reads some more of the shell script and forks off another child.
It is essential that the second child inherit a file position of 1024 from the shell, so it will begin writing at the place where the first program left off.
If the shell did not share the file position with its children, the second program would overwrite the output from the first one, instead of appending to it.
As a result, it is not possible to put the file position in the process table.
The solution used in UNIX and MINIX 3 is to introduce a new, shared table, filp, which contains all the file positions.
By having the file position truly shared, the semantics of fork can be implemented correctly, and shell scripts work properly.
Although the only thing that the filp table really must contain is the shared file position, it is convenient to put the i-node pointer there, too.
In this way, all that the file descriptor array in the process table contains is a pointer to a filp entry.
The filp entry also contains the file mode (permission bits), some flags indicating.
How file positions are shared between a parent and a child.
Yet another aspect of file system management requires a special table.
This permits any part, or multiple parts, of a file to be marked as locked.
The operating system does not enforce locking, but processes are expected to be well behaved and to look for locks on a file before doing anything that would conflict with another process.
The reasons for providing a separate table for locks are similar to the justifications for the filp table discussed in the previous section.
A single process can have more than one lock active, and different parts of a file may be locked by more than one process (although, of course, the locks cannot overlap), so neither the process table nor the filp table is a good place to record locks.
Since a file may have more than one lock placed upon it, the i-node is not a good place either.
Pipes and special files differ from ordinary files in an important way.
When a process tries to read or write a block of data from a disk file, it is almost certain that the operation will complete within a few hundred milliseconds at most.
In the worst case, two or three disk accesses might be needed, not more.
Similarly, when reading from a terminal, a process will have to wait until somebody types something.
As a consequence, the file system’s normal rule of handling a request until it is finished does not work.
It is necessary to suspend these requests and restart them later.
When a process tries to read or write from a pipe, the file system can check the state of the pipe immediately to see if the operation can be completed.
If it can be, it is, but if it cannot be, the file system records the parameters of the system call in the process table, so it can restart the process when the time comes.
Note that the file system need not take any action to have the caller suspended.
All it has to do is refrain from sending a reply, leaving the caller blocked waiting for the reply.
Thus, after suspending a process, the file system goes back to its main loop to wait for the next system call.
As soon as another process modifies the pipe’s state so that the suspended process can complete, the file system sets a flag so that next time through the main loop it extracts the suspended process’ parameters from the process table and executes the call.
The situation with terminals and other character special files is slightly different.
The i-node for each special file contains two numbers, the major device and the minor device.
The major device number indicates the device class (e.g., RAM disk, floppy disk, hard disk, terminal)
It is used as an index into a file system table that maps it onto the number of the corresponding I/O device driver.
In effect, the major device determines which I/O driver to call.
The minor device number is passed to the driver as a parameter.
In some cases, most notably terminal devices, the minor device number encodes some information about a category of devices handled by a driver.
Virtual consoles are handled by the same part of the driver software.
Similarly, network terminals use pseudo-terminal drivers, and these also need different low-level software.
These numbers are chosen to make it easy for the device driver to call the low-level functions required for each group of devices.
When a process reads from a special file, the file system extracts the major and minor device numbers from the file’s i-node, and uses the major device number as an index into a file system table to map it onto the process number of the corresponding device driver.
If the driver is able to carry out the work immediately (e.g., a line of input has already been typed on the terminal), it copies the data from its own internal buffers to the user and sends the file system a reply message saying that the work is done.
The file system then sends a reply message to the user, and the call is finished.
Note that the driver does not copy the data to the file system.
Data from block devices go through the block cache, but data from character special files do not.
On the other hand, if the driver is not able to carry out the work, it records the message parameters in its internal tables, and immediately sends a reply to the file system saying that the call could not be completed.
At this point, the file system is in the same situation as having discovered that someone is trying to read from an empty pipe.
It records the fact that the process is suspended and waits for the next message.
When the driver has acquired enough data to complete the call, it transfers them to the buffer of the still-blocked user and then sends the file system a message reporting what it has done.
All the file system has to do is send a reply message to the user to unblock it and report the number of bytes transferred.
As we shall see shortly, most of the code of the file system is devoted to carrying out system calls.
Therefore, it is appropriate that we conclude this overview with a brief sketch of how the most important call, read, works.
The request is then broken up into pieces such that each piece fits within a block.
For each of these pieces in turn, a check is made to see if the relevant block is in the cache.
If the block is not present, the file system picks the least recently.
Then the disk driver is asked to fetch the block to be read.
After the copy has been done, the file system sends a reply message to the user specifying how many bytes have been copied.
When the reply comes back to the user, the library function read extracts the reply code and returns it as the function value to the caller.
One extra step is not really part of the read call itself.
After the file system completes a read and sends a reply, it initiates reading additional blocks, provided that the read is from a block device and certain other conditions are met.
Since sequential file reads are common, it is reasonable to expect that the next blocks in a file will be requested in the next read request, and this makes it likely that the desired block will already be in the cache when it is needed.
The number of blocks requested depends upon the size of the block cache; as many as 32 additional blocks may be requested.
The device driver does not necessarily return this many blocks, and if at least one block is returned a request is considered successful.
Requests to carry out system calls come in, are carried out, and replies are sent.
In the following sections we will go through it a file at a time, pointing out the highlights.
The code itself contains many comments to aid the reader.
In looking at the code for other parts of MINIX 3 we have generally looked at the main loop of a process first and then looked at the routines that handle the different message types.
We will organize our approach to the file system differently.
Then we will look at the main loop and the system calls that operate upon files.
Next we will look at systems call that operate upon directories, and then, we will discuss the remaining system calls that fall into neither category.
Finally we will see how device special files are handled.
Like the kernel and process manager, various data structures and tables used in the file system are defined in header files.
Some of these data structures are placed in system-wide header files in include/ and its subdirectories.
The file system’s own header files are in the file system source directory src/fs/
Many file names will be familiar from studying other parts of the MINIX 3 system.
It includes other header files needed by all the C source files in the file system.
As in the other parts of MINIX 3, the file system master header includes the file system’s own const.h, type.h, proto.h, and glo.h.
Const.h (line 21000) defines some constants, such as table sizes and flags, that are used throughout the file system.
Support for older versions is useful not only for accessing files on older MINIX file systems, it may also be useful for exchanging files.
The new version provides space for the three time fields which UNIX systems provide.
The message buffers for the incoming and reply messages are also here.
The now-familiar trick with the EXTERN macro is used, so these variables can be accessed by all parts of the file system.
As in the other parts of MINIX 3, the storage space will be reserved when table.c is compiled.
The file system’s part of the process table is contained in fproc.h (line 21500)
It holds the mode mask, pointers to the i-nodes for the current root directory and working directory, the file descriptor array, uid, gid, and terminal number for each process.
The process id and the process group id are also found here.
The process id is duplicated in the part of the process table located in the process manager.
Now we come to files that define other tables maintained by the file system.
The array buf holds all the buffers, each of which contains a data part, b, and a header full of pointers, flags, and counters.
It also tells whether the file was opened for reading, writing, or both, and how many file descriptors are currently pointing to the entry.
In inode.h (line 21900) the i-node table inode is declared (using EXTERN)
As we said earlier, when a file is opened its i-node is read into memory and kept there until the file is closed.
The inode structure definition provides for information that is kept in memory, but is not written to the disk i-node.
Notice that there is only one version, and nothing is version-specific here.
The rest of the file system does not need to know about the file system format on the disk, at least until the time comes to write back modified information.
Associated with each of the main tables—blocks, i-nodes, superblocks, and so forth—is a file that contains procedures that manage the table.
These procedures are heavily used by the rest of the file system and form the principal interface between tables and the file system.
For this reason, it is appropriate to begin our study of the file system code with them.
The first step is usually to search a hash chain for a block, although there is a special case, when a hole in a sparse file is being read, where this search is skipped.
The next procedure in the file is invalidate (line 22680)
It is called when a disk is unmounted, for example, to remove from the cache all the blocks belonging to the file system just unmounted.
If this were not done, then when the device were reused (with a different floppy disk), the file system might find the old blocks instead of the new ones.
The block cache is not the only file system table that needs support procedures.
Many of the procedures are similar in function to the block management procedures.
The file super.c contains procedures that manage the superblock and the bitmaps.
Six procedures are defined in this file, listed in Fig.
The outer one loops on all the blocks of a bitmap.
The middle one loops on all the words of a block.
The inner one loops on all the bits of a word.
The next function, mounted (line 23489), is called only when a block device is closed.
Normally, all cached data for a device are discarded when it is closed.
But, if the device happens to be mounted, this is not desirable.
Mounted is called with a pointer to the i-node for a device.
It just returns TRUE if the device is the root device, or if it is a mounted device.
Even though it is not currently used in MINIX 3, the method of determining whether a disk was written on a system with a different byte order is clever and worth noting.
These operations are requested by using an FCNTL system call.
This strategy is a compromise; it would take extra code to figure out exactly which processes were waiting for a particular lock to be released.
Those processes that are still waiting for a locked file will block again when they start.
This strategy is based on an assumption that locking will be used infrequently.
If a major multiuser data base were to be built upon a MINIX 3 system, it might be desirable to reimplement this.
Then several other functions are invoked to initialize the block cache and the device table, to load the RAM disk if necessary, and to load the root device superblock.
Normally, blocks are released and returned to the LRU chain immediately.
Figure 5-44(c) shows the situation after the block has been returned to the LRU chain.
Although it is no longer in use, it can be accessed again to provide the.
After the system has been in operation for awhile, almost all of the blocks can be expected to have been used and to be distributed among the different hash chains at random.
If you have used the sysenv command to to look at the boot parameters on a working MINIX 3 system, you have seen that sysenv reports devices numerically, displaying strings like.
The file system uses numbers like this to identify devices.
The number is simply 256 x major + minor, where major and minor are the major and minor device numbers.
The cdprobe function, not discussed in this text, is used.
Interested readers are referred to the code in fs/cdprobe.c, which can be found on the CD-ROM or the Web site.
Anything else would be complicated, since the block size cannot be known until the superblock has been loaded.
In this section we will look at the system calls that operate on individual files one at a time (as opposed to, say, operations on directories)
We will start with how files are created, opened, and closed.
After that we will examine in some detail the mechanism by which files are read and written.
Then that we will look at pipes and how operations on them differ from those on files.
The file open.c contains the code for six system calls: creat, open, mknod, mkdir, close, and lseek.
We will examine creat and open together, and then look at each of the others.
Finding the i-node (allocating and initializing if the file is new)
Setting up and returning a file descriptor for the file.
Again we see that the file system must constantly check for errors, and upon encountering one, carefully release all the resources, such as i-nodes and blocks that it is holding.
Note that returning an i-node means that its counter in the inode table is decremented, so it can be removed from the table eventually.
This operation has nothing to do with freeing the i-node (i.e., setting a bit in the bitmap saying that it is available)
The i-node is only freed when the file has been removed from all directories.
As we discussed in the MINIX 3 overview, the presence of multiple blocks per zone causes problems that must be dealt with explicitly.
This loop breaks the request up into chunks, each of which fits in a single disk block.
A chunk begins at the current position and extends until one of the following conditions is met:
These rules mean that a chunk never requires two disk blocks to satisfy it.
Finally, if read ahead is called for, the i-node to read from and the position to read from are stored in global variables, so that after the reply message is sent to the user, the file system can start getting the next block.
Three examples of how the first chunk size is determined for a 10-byte file.
This arrangement overlaps processing and I/O and can improve performance substantially.
For blocks further into the file, one or more indirect blocks may have to be read.
Figure 5-46 shows the relations between some of the major procedures involved in reading a file–in particular, who calls whom.
Again, if we could just toss in the sponge and panic at this point, the code would be much simpler.
However, from the user’s point of view it is much nicer that running out of disk space just returns an error from write, rather than crashing the computer with a corrupted file system.
Reading and writing a pipe is slightly different from reading and writing a file, because a pipe has a finite capacity.
An attempt to write to a pipe that is already full will cause the writer to be suspended.
Similarly, reading from an empty pipe will suspend the reader.
In effect, a pipe has two pointers, the current position (used by readers) and the size (used by writers), to determine where data come from or go to.
The procedure release (line 26099) is called to check if a process that was suspended on a pipe can now be allowed to continue.
If it finds one, it calls revive to set a flag so that the main loop will notice it later.
This function is not a system call, but is listed in Fig.
We have now finished looking at how files are read and written.
Our next task is to see how path names and directories are handled.
Many system calls (e.g., open, unlink, and mount) have path names (i.e., file names) as a parameter.
Most of these calls must fetch the i-node for the named file before they can start working on the call itself.
Two system calls that affect the file system as a whole are mount and umount.
They allow independent file systems on different minor devices to be ‘‘glued’’ together to form a single, seamless naming tree.
One of them points to the i-node mounted on, and the other points to the root i-node of the mounted file system.
Some of the procedures used in looking up path names.
The special file is a block device but is already mounted.
The file system to be mounted has a rotten magic number.
The file system to be mounted is invalid (e.g., no i-nodes)
The file to be mounted on does not exist or is a special file.
There is no room for the mounted file system’s bitmaps.
There is no room for the mounted file system’s superblock.
There is no room for the mounted file system’s root i-node.
Perhaps it seems inappropriate to keep harping on this point, but the reality of any practical operating system is that a substantial fraction of the code is devoted to doing minor chores that are not intellectually very exciting but are crucial to making a system usable.
If a user attempts to mount the wrong floppy disk by accident, say, once a month, and this leads to a crash and a corrupted file system, the user will perceive the system as being unreliable and blame the designer, not himself.
The famous inventor Thomas Edison once made a remark that is relevant here.
The difference between a good system and a mediocre one is not the brilliance of the former’s scheduling algorithm, but its attention to getting all the details right.
Both parent directories must be on the same device (line 27221)
Neither the old nor the new name may be a directory with a file system mounted upon it.
Some other conditions must be checked if the new name already exists.
Most importantly it must be possible to remove an existing file with the new name.
The last group of system calls is a mixed bag of things involving status, directories, protection, time, and other services.
The umask system call allows the user to set a mask (stored in the process table), which then masks out bits in subsequent creat system calls.
The complete implementation would be only one statement, line 27907, except that the call must return the old mask value as its result.
As we have mentioned more than once, a design goal was to make MINIX 3 a more robust operating system by having all device drivers run as user-space processes without direct access to kernel data structures or kernel code.
The primary advantage of this approach is that a faulty device driver will not cause the entire system to crash, but there are some other implications of this approach.
One is that device drivers not needed immediately upon startup can be started at any time after startup is complete.
This also implies that a device driver can be stopped, restarted, or replaced by a different driver for the same device at any time while the system is running.
However, if the hard disk driver crashes, it can be restarted from a copy on the RAM disk.
In response to user requests for I/O the file system sends messages to the user-space device drivers.
The dmap table has an entry for every possible major device type.
It provides the mapping between the major device number and the corresponding device driver.
The next two files we will consider deal with the dmap table.
This file also supports initialization of the table and a new system call, devctl, which is intended to support starting, stopping, and restarting of device drivers.
After that we will look at device.c which supports normal runtime operations on devices, such as open, close, read, write, and ioctl.
When a device is opened, closed, read, or written, dmap provides the name of the procedure to call to handle the operation.
All of these procedures are located in the file system’s address space.
Many of these procedures do nothing, but some call a device driver to request actual I/O.
The process number corresponding to each major device is also provided by the table.
Whenever a new major device is added to MINIX 3, a line must be added to this table telling what action, if any, is to be taken when the device is opened, closed, read, or written.
As a simple example, if a tape drive is added to MINIX 3, when its special file is opened, the procedure in the table could check to see if the tape drive is already in use.
It might be helpful to describe how the devctl call is used, and plans for its use in the future.
A server process, the reincarnation server (RS) is used in MINIX 3 to support starting user-space servers and drivers after the operating system is up and running.
The interface to the reincarnation server is the service utility, and examples of its use can be seen in /etc/rc.
The reincarnation server is new, and in the release of MINIX 3 described here it is still rudimentary.
Plans for future releases of MINIX 3 include a more powerful reincarnation server that will be able to stop and restart drivers in addition to starting them.
It will also be able to monitor drivers and restart them automatically if problems develop.
Now we come to the file device.c, which contains the procedures needed for device I/O at run time.
The C library will translate such functions into ioctl calls.
For devices other than terminals ioctl is used for many operations, many of which were described in Chap.
There are a number of files that are not listed in Appendix B, but which are required to compile a working system.
In this section we will review some files that support additional system calls.
In the next section we will mention files and functions that provide more general support for the file system.
The file misc.c contains procedures for a few system and kernel calls that do not fit in anywhere else.
The system calls fork, exec, exit, and set are really process manager calls, but the results have to be posted here as well.
When a process forks, it is essential that the kernel, process manager, and file system all know about it.
These ‘‘system calls’’ do not come from user processes, but from the process manager.
One system call merits a header file as well as a C source file to support it.
Select.h and select.c provide support for the select system call.
Select is used when a single process has to do deal with multiple I/O streams, as, for instance, a communications or network program.
Describing it in detail is beyond the scope of this book.
The file system contains a few general purpose utility procedures that are used in various places.
These routines are called when reading from or writing to a disk data structure, such as an i-node or bitmap.
The byte order in the system that created the disk is recorded in the superblock.
If it is different from the order used by the local processor the order will be swapped.
The rest of the file system does not need to know anything about the byte order on the disk.
Finally, there are two other files that provide specialized utility services to the file manager.
The file system can ask the system task to set an alarm for it, but if it needs more than one timer it can maintain its own linked list of timers, similar to what we saw for the process manager in the previous chapter.
The file timers.c provides this support for the file system.
The MINIX 3 files are not visible to operating systems that support only standard CD-ROM file formats.
The process manager discussed in the previous chapter and the file system discussed in this chapter are user-space servers which provide support that would be integrated into a monolithic kernel in an operating system of conventional design.
These are not the only server processes in a MINIX 3 system, however.
There are other user-space processes that have system privileges and should be considered part of the operating system.
We do not have enough space in this book to discuss their internals, but we should at least mention them here.
This is the reincarnation server, RS, which can start an ordinary process and turn it into a system process.
It is used in the current version of MINIX 3 to launch device drivers that are not part of the system boot image.
In future releases it will also be able to stop and restart drivers, and, indeed, to monitor drivers and stop and restart them automatically if they seem to be malfunctioning.
The source code for the reincarnation server is in the src/servers/rs/ directory.
Another server that has been mentioned in passing is the information server, IS.
It is used to generate the debugging dumps that can be triggered by pressing the function keys on a PC-style keyboard.
The source code for the information server is in the src/servers/is/ directory.
The information server and the reincarnation servers are relatively small programs.
There is a third, optional, server, the network server, or INET.
The INET program image on disk is comparable in size to the MINIX 3 boot image.
It is started by the reincarnation server in much the same way that device drivers are started.
Finally, we will mention one other system component which is considered a device driver, not a server.
With so many different components of the operating system running as independent processes, it is desirable to provide a standardized way of handling diagnostic, warning, and error messages.
The MINIX 3 solution is to have a device driver for a pseudo-device known as /dev/klog which can receive messages and handle writing them to a file.
The source code for the log driver is in the src/drivers/log/ directory.
When seen from the outside, a file system is a collection of files and directories, plus operations on them.
Files can be read and written, directories can be created and destroyed, and files can be moved from directory to directory.
When seen from the inside, a file system looks quite different.
The file system designers have to be concerned with how storage is allocated, and how the system keeps track of which block goes with which file.
We have also seen how different systems have different directory structures.
Security and protection are of vital concern to both the system users and system designers.
We discussed some security flaws in older systems, and generic problems that many systems have.
We also looked at authentication, with and without passwords, access control lists, and capabilities, as well as a matrix model for thinking about protection.
Finally, we studied the MINIX 3 file system in detail.
It accepts requests for work from user processes, indexes into a table of procedure pointers, and calls that procedure to carry out the requested system call.
Due to its modular structure and position outside the kernel, it can be removed from MINIX 3 and used as a free-standing network file server with only minor modifications.
Internally, MINIX 3 buffers data in a block cache and attempts to read ahead when making sequential access to file.
If the cache is made large enough, most program text will be found to be already in memory during operations that repeatedly access a particular set of programs, such as a compilation.
Give an advantage of Unicode file naming over ASCII file naming.
Would that be a useful file attribute? If so, how might it be used?
If /usr/jim/ is the working directory, what is the absolute path name for the file whose relative path name is ../ast/x?
Instead of having a single root for the file system, give each user a personal root.
Does that make the system more flexible? Why or why not?
The UNIX file system has a call chroot that changes the root to a given directory.
Does this have any security implications? If so, what are they?
The UNIX system has a call to read a directory entry.
Since directories are just files, why is it necessary to have a special call? Can users not just read the raw directories themselves?
A standard PC can hold only four operating systems at once.
Is there any way to increase this limit? What consequences would your proposal have?
Contiguous allocation of files leads to disk fragmentation, as mentioned in the text.
Is this internal fragmentation or external fragmentation? Make an analogy with something discussed in the previous chapter.
Figure 5-10 shows the structure of the original FAT file system used on MS-DOS.
If that scheme were to be directly extended to file systems with 232 blocks, how much space would the FAT occupy?
An operating system only supports a single directory but allows that directory to have arbitrarily many files with arbitrarily long file names.
Can something approximating a hierarchical file system be simulated? How?
Free disk space can be kept track of using a free list or a bitmap.
For a disk with B blocks, F of which are free, state the condition under which the free list uses less space than the bitmap.
For D having the value 16 bits, express your answer as a percentage of the disk space that must be free.
It has been suggested that the first part of each UNIX file be kept in the same disk block as its i-node.
The performance of a file system depends upon the cache hit rate (fraction of blocks found in the cache)
What is the difference between a hard link and a symbolic link? Give an advantage of each one.
Name three pitfalls to watch out for when backing up a file system.
If no attempt is made to put the blocks of a file close to each other, two blocks that are logically consecutive (i.e., follow one another in the file) will require an average seek, which takes 5 msec.
How long does it take to read a 20 microsec per block?
Would compacting disk storage periodically be of any conceivable value? Explain.
After getting your degree, you apply for a job as director of a large university computer center that has just put its ancient operating system out to pasture and switched over to UNIX.
Fifteen minutes after starting work, your assistant bursts into your office screaming: ‘‘Some students discovered the algorithm we use for encrypting passwords and posted it on the Internet.’’ What should you do?
Two computer science students, Carolyn and Elinor, are having a discussion about inodes.
Carolyn maintains that memories have gotten so large and so cheap that when a file is opened, it is simpler and faster just to fetch a new copy of the i-node into the i-node table, rather than search the entire table to see if it is already there.
The Morris-Thompson protection scheme with the n-bit random numbers was designed to make it difficult for an intruder to discover a large number of passwords by encrypting common strings in advance.
Does the scheme also offer protection against a student user who is trying to guess the superuser password on his machine?
A computer science department has a large collection of UNIX machines on its local network.
Users on any machine can issue a command of the form.
This feature is implemented by having the user’s kernel send the command and his uid to the remote machine.
Is this scheme secure if the kernels are all trustworthy (e.g., large timeshared minicomputers with protection hardware)? What if some of the machines are students’ personal computers, with no protection hardware?
When a file is removed, its blocks are generally put back on the free list, but they are not erased.
Do you think it would be a good idea to have the operating system erase each block before releasing it? Consider both security and performance factors in your answer, and explain the effect of each.
Three different protection mechanisms that we have discussed are capabilities, access control lists, and the UNIX rwx bits.
For each of the following protection problems, tell which of these mechanisms can be used.
For UNIX, assume that groups are categories such as faculty, students, secretaries, etc.
What aspects of file system design would be affected by this development?
Symbolic links are files that point to other files or directories indirectly.
Unlike ordinary links such as those currently implemented in MINIX 3, a symbolic link has its own i-node, which points to a data block.
The data block contains the path to the file being linked to, and the i-node makes it possible for the link to have different ownership and permissions from the file linked to.
A symbolic link and the file or directory to which it points can be located on different devices.
Show if setting the (now-unused) ROBUST flag might make the file system more or less robust in the face of a crash.
Whether this is the case in the current version of MINIX 3 has not been researched, so it may be either way.
Take a good look at what happens when a modified block is evicted from the cache.
Take into account that a modified data block may be accompanied by a modified i-node and bitmap.
Write a pair of programs, in C or as shell scripts, to send and receive a message by a covert channel on a MINIX 3 system.
Hint: A permission bit can be seen even when a file is otherwise inaccessible, and the sleep command or system call is guaranteed to delay for a fixed time, set by its argument.
Then create an artificially heavy load by starting up numerous different background processes and measure the data rate again.
Implement immediate files in MINIX 3, that is small files actually stored in the i-node itself, thus saving a disk access to retrieve them.
In the previous five chapters we have touched upon a variety of topics.
This chapter is intended as an aid to readers interested in pursuing their study of operating systems further.
Section 6.2 is an alphabetical bibliography of all books and articles cited in this book.
In addition to the references given below, the Proceedings of the n-th ACM Symposium on Operating Systems Principles (ACM) held every other year and the Proceedings of the n-th International Conference on Distributed Computing Systems (IEEE) held every year are good places to look for recent papers on operating systems.
Below is a list of suggested readings keyed by chapter.
For anyone wishing to understand how the Linux kernel works internally, this.
Brinch Hansen, Classic Operating Systems Operating system have been around long enough now that some of them can.
This book is a collection of 24 papers about seminal operating systems, categorized as open shop, batch, multiprogramming, timesharing, personal computer, and distributed operating systems.
Anyone interested in the history of operating systems should read this book.
Brooks, The Mythical Man-Month: Essays on Software Engineering A witty, amusing, and informative book on how not to write an operating system by someone who learned the hard way.
His conclusion is that all complex systems will ultimately fail, and that to have any chance for success at all, it is absolutely essential to avoid complexity and strive for simplicity and elegance in design.
Dijkstra, ‘‘My Recollections of Operating System Design’’ Reminiscences by one of the pioneers of operating system design, starting.
Some parts are actually quite readable, especially Annex B, ‘‘Rationale and Notes,’’ which sheds light on why things are done as they are.
One advantage of referring to the standard document is that, by definition, there are no errors.
If a typographical error in a macro name makes it through the editing process it is no longer an error, it is official.
Lampson, ‘‘Hints for Computer System Design’’ Butler Lampson, one of the world’s leading designers of innovative operating.
Like Brooks’ book, this is required reading for every aspiring operating system designer.
Lewine, POSIX Programmer’s Guide This book describes the POSIX standard in a much more readable way than.
There are numerous examples of code, including several complete programs.
For a thorough explanation of how a modern version of UNIX, in this case FreeBSD, works inside, this is the place to look.
It covers processes, I/O, memory management, networking, and just about everything else.
Milojicic, ‘‘Operating Systems: Now and in the Future,’’ Suppose you were to ask six of the world’s leading experts in operating systems a series of questions about the field and where it was going.
It will help you understand examples in this book if you are comfortable as a.
This is just one of a number of available beginners’ guides to working with the UNIX operating system.
Although implemented differently, MINIX looks like UNIX to a user, and this or a similar book will also be helpful in your work with MINIX.
This book tells how to write C programs that use the UNIX system call interface and the standard C library.
The relationship of these implementations to POSIX is described in detail.
Andrews and Schneider, ‘‘Concepts and Notations for Concurrent Programming’’ A tutorial and survey of processes and interprocess communication, including.
The article also shows how these concepts are embedded in various programming languages.
The second part discusses distributed programming and languages useful for distributed programming.
The third part is on principles of implementation of concurrency.
Bic and Shaw, Operating System Principles This operating systems textbook has four chapters on processes, including not.
Milo et al., ‘‘Process Migration’’ As clusters of PCs gradually replace supercomputers, the issue of moving.
In this survey, the authors discuss how process migration works, along with its benefits and pitfalls.
Chen et al., ‘‘RAID: High Performance Reliable Secondary Storage’’ The use of multiple disk drives in parallel for fast I/O is a trend in high end.
The authors discuss this idea and examine different organizations in terms of performance, cost, and reliability.
Coffman et al., ‘‘System Deadlocks’’ A short introduction to deadlocks, what causes them, and how they can be.
If you really really really want to know how I/O works, try writing a device.
This book tells you how to do it for Linux.
Levine, ‘‘Defining Deadlocks’’ In this short article, Levine raises interesting questions about conventional.
Swift et al., ‘‘Recovering Device Drivers’’ Device drivers have an error rate an order of magnitude higher than other.
Is there anything that can be done to improve reliability then? This paper describes how shadow drivers can be used to achieve this goal.
Linux and Windows have quite different architectures for their device drivers.
This papers discusses both of them and shows how they are similar and how they are different.
In this paper, the authors describe in some detail the system they built at HP Labs.
Bic and Shaw, Operating System Principles Three chapters of this book are devoted to memory management, physical.
Denning, ‘‘Virtual Memory’’ A classic paper on many aspects of virtual memory.
Denning, ‘‘Working Sets Past and Present’’ A good overview of numerous memory management and paging algorithms.
Denning, ‘‘The Locality Principle’’ A recent look back at the history of the locality principle and a discussion of.
Halpern, ‘‘VIM: Taming Software with Hardware’’ In this provocative article, Halpern argues that a tremendous amount of.
First fit, best fit, and other memory management algorithms are discussed and.
Craig Neidorf’’ When a young hacker discovered and published information about how the.
This article describes the case, which involved many fundamental issues, including freedom of speech.
The article is followed by some dissenting views and a rebuttal by Denning.
Ghemawat et al., ‘‘The Google File System’’ Suppose you decided you wanted to store the entire Internet at home so you.
Step 2 would be to read this paper to find out how Google does it.
Hafner and Markoff, Cyberpunk: Outlaws and Hackers on the Computer Frontier Three compelling tales of young hackers breaking into computers around the.
Harbron, File Systems: Structures and Algorithms A book on file system design, applications, and performance.
Satyanarayanan, ‘‘The Evolution of Coda’’ As mobile computing becomes more common, the need to integrate and synchronize mobile and fixed file systems becomes more urgent.
Chapter 16 contains a fair amount of material about the security environment.
Uppuluri et al., ‘‘Preventing Race Condition Attacks on File Systems’’ Situations exist in which a process assumes that two operations will be performed atomically, with no intervening operations.
If another process manages to sneak in and perform an operation between them, security may be breached.
Yang et al., ‘‘Using Model Checking to Find Serious File System Errors’’ File system errors can lead to lost data, so getting them debugged is very.
Serial ATA, USB, and SCSI disks are not supported at present.
If you want to run MINIX 3 on a simulator instead of native, see Part V first.
If you do not have an IDE CD-ROM, either get the special USB CD-ROM boot image or use a simulator.
Burn it to a CD-ROM to make a bootable CD-ROM.
During setup you will be asked which Ethernet chip you have, if any.
Expand the + next to ‘‘Network adapters’’ to see what you have.
If you do not have a supported chip, you can still run MINIX 3, but without Ethernet.
You can boot the computer from your CD-ROM if you like and MINIX 3 will.
But before partitioning, be sure to back up your data to an external medium like CD-ROM or DVD as a safety precaution, just in case something goes wrong.
If you do not know how to manage partitions but have a partitioning program like Partition Magic, use it to create a region of free disk space.
Also make sure there is at least one primary partition (i.e., Master Boot Record slot) free.
The MINIX 3 setup script will guide you through creating a MINIX partition in the free space, which can be on either the first or second IDE disk.
In all other cases, please read the online tutorial cited above.
WARNING: If you make a mistake during disk partitioning, you can lose all the data on the disk, so be sure to back it up to CD-ROM or DVD before starting.
By now you should have allocated some free space on your disk.
Insert the CD-ROM into your CD-ROM drive and boot the computer from it.
To start the installation of MINIX 3 on the hard disk, type.
After this and all other commands, be sure to type ENTER (RETURN)
When the installation script ends a screen with a colon, hit ENTER to continue.
If the screen suddenly goes blank, press CTRL-F3 to select software scrolling (should only be needed on very old computers)
Note that CTRL-key means depress the CTRL key and while holding it down, press ‘‘key.’’
When you are asked to select your national keyboard, do so.
In most steps, the default is generally a good choice for beginners.
The us-swap keyboard interchanges the CAPS LOCK and CTRL keys, as is conventional on UNIX systems.
You will now be asked which of the available Ethernet drivers you want installed (or none)
Basic minimal or full distribution? If you are tight on disk space, select M for a minimal installation which.
The minimal option does not install the sources of the commands.
If you have 1 GB or more, choose F for a full installation.
This region will be overwritten and its previous contents lost forever.
Substep 4.3: Confirm your choices You have now reached the point of no return.
If you do, the data in the selected region will be lost forever.
To exit the setup script without changing the partition table, hit CTRL-C.
If you chose an existing MINIX 3 partition, in this step you will be offered a.
This design means that you can put your personal files on /home and reinstall a newer version of MINIX 3 when it is available without losing your personal files.
The selected partition will be divided into three subpartitions: root, /usr, and.
Specify how much of the partition should be set aside for your files.
The setup script will now scan each partition for bad disk blocks.
If you are absolutely certain there are no bad blocks, you can kill each scan by hitting CTRL-C.
When the scan finishes, files will be automatically copied from the CD-ROM.
This section tells you how to test your installation, rebuild the system after modifying it, and boot it later.
This is a good time to create a root password.
To test the system, log in as bin (required) and type.
They should all run correctly but they can take 20 min on a fast machine and over an hour on a slow one.
Note: It is necessary to compile the test suite when running as root but execute it as bin in order to see if the setuid bit works correctly.
If all the tests work correctly, you can now rebuild the system.
Besides, rebuilding the system is a good test to see if it works.
Now make a new bootable image by typing su make clean time make image.
You just rebuilt the operating system, including all the kernel and user-mode parts.
When you are asked to complete the path, type: fd0
This approach does not currently work with USB floppies since there is no MINIX 3 USB floppy disk driver yet.
To update the boot image currently installed on the hard disk, type.
To boot the new system, first shut down by typing: shutdown.
This command saves certain files and returns you to the MINIX 3 boot monitor.
To get a summary of what the boot monitor can do, while in it, type:
You can now remove any CD-ROM or floppy disk and turn off the computer.
If you have a legacy floppy disk drive, the simplest way to boot MINIX 3 is by.
Alternatively, boot from the MINIX 3 CD-ROM, login as bin and type:
Then you can boot any of your operating systems easily.
A completely different approach to running MINIX 3 is to run it on top of another operating system instead of native on the bare metal.
Various virtual machines, simulators, and emulators are available for this purpose.
Running a program on a simulator is similar to running it on the actual machine, so you should go back to Part I and acquire the latest CD-ROM and continue from there.
Some of these old names had better be defined when not POSIX.
All of these are generated by the kernel and relate to message passing.
The <signal.h> header defines all the ANSI and POSIX signals.
A driver that uses ioctls claims a character for its series of commands.
User processes share the same properties and count for one.
Extra system library definitions to support device drivers and servers.
This id must be fixed because it is used to check send mask entries.
Call C startup code to set up a proper environment to run main()
This file contains essentially all of the process and message handling.
Together with "mpx.s" it forms the lowest layer of the MINIX kernel.
The following names are synonyms for the variables in the input message.
The following names are synonyms for the variables in the input message.
This file takes care of those system calls that deal with time.
Below is a list of Minimum System Requirements to install the software supplied on this CD.
If you wish to retain your existing operating system and data (recommended) and create a dual-boot machine, you will need to partition your hard drive.
Complete installation instructions are supplied on the CD in Adobe Acrobat PDF format.
