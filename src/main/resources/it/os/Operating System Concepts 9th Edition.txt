Operating systems are an essential part of any computer system.
Similarly, a course on operating systems is an essential part of any computer science education.
This ﬁeld is undergoing rapid change, as computers are now prevalent in virtually every arena of day-to-day life—from embedded devices in automobiles through the most sophisticated planning tools for governments and multinational ﬁrms.
Yet the fundamental concepts remain fairly clear, and it is on these that we base this book.
We wrote this book as a text for an introductory course in operating systems at the junior or senior undergraduate level or at the ﬁrst-year graduate level.
It provides a clear description of the concepts that underlie operating systems.
As prerequisites, we assume that the reader is familiar with basic data structures, computer organization, and a high-level language, such as C or Java.
In that chapter, we also include an overview of the fundamental data structures that are prevalent in most operating systems.
For code examples, we use predominantly C, with some Java, but the reader can still understand the algorithms without a thorough knowledge of these languages.
Important theoretical results are covered, but formal proofs are largely omitted.
The bibliographical notes at the end of each chapter contain pointers to research papers in which results were ﬁrst presented and proved, as well as references to recent material for further reading.
In place of proofs, ﬁgures and examples are used to suggest why we should expect the result in question to be true.
The fundamental concepts and algorithms covered in the book are often based on those used in both commercial and open-source operating systems.
Our aim is to present these concepts and algorithms in a general setting that is not tied to one particular operating system.
However, we present a large number of examples that pertain to the most popular and the most innovative operating systems, including Linux, Microsoft Windows, Apple Mac OS X, and Solaris.
We also include examples of both Android and iOS, currently the two dominant mobile operating systems.
The organization of the text reﬂects our many years of teaching courses on operating systems, as well as curriculum guidelines published by the IEEE.
Consideration was also given to the feedback provided by the reviewers of the text, along with the many comments and suggestions we received from readers of our previous editions and from our current and former students.
These chapters discuss what the common features of an operating system are and what an operating system does for the user.
We include coverage of both traditional PC and server operating systems, as well as operating systems for mobile devices.
We have avoided a discussion of how things are done internally in these chapters.
Therefore, they are suitable for individual readers or for students in lower-level classes who want to learn what an operating system is without getting into the details of the internal algorithms.
A process is the unit of work in a system.
Such a system consists of a collection of concurrently executing processes, some of which are operating-system processes (those that execute system code) and the rest of which are user processes (those that execute user code)
These chapters cover methods for process scheduling, interprocess communication, process synchronization, and deadlock handling.
Also included is a discussion of threads, as well as an examination of issues related to multicore systems and parallel programming.
To improve both the utilization of the CPU and the speed of its response to its users, the computer must keep several processes in memory.
There are many different memory-management schemes, reﬂecting various approaches to memory management, and the effectiveness of a particular algorithm depends on the situation.
The ﬁle system provides the mechanism for on-line storage of and access to both data and programs.
We describe the classic internal algorithms and structures of storage management and provide a ﬁrm practical understanding of the algorithms used—their properties, advantages, and disadvantages.
Since the I/O devices that attach to a computer vary widely, the operating system needs to provide a wide range of functionality to applications to allow them to control all aspects of these devices.
We discuss system I/O in depth, including I/O system design, interfaces, and internal system structures and functions.
In many ways, I/O devices are the slowest major components of the computer.
The processes in an operating system must be protected from one another’s activities, and to provide such protection, we must ensure that only processes that have gained proper authorization from the operating system can operate on the ﬁles, memory, CPU, and other resources of the system.
Protection is a mechanism for controlling the access of programs, processes, or users to computer-system resources.
This mechanism must provide a means of specifying the controls to be imposed, as well as a means of enforcement.
Security protects the integrity of the information stored in the system (both data and code), as well as the physical resources of the system, from unauthorized access, malicious destruction or alteration, and accidental introduction of inconsistency.
Chapter 16 is a new chapter that provides an overview of virtual machines and their relationship to contemporary operating systems.
Included is an overview of the hardware and software techniques that make virtualization possible.
Chapter 17 condenses and updates the three chapters on distributed computing from the previous edition.
This change is meant to make it easier for instructors to cover the material in the limited time available during a semester and for students to gain an understanding of the core ideas of distributed computing more quickly.
Coverage of both Linux and Windows 7 are presented throughout this text; however, the case studies provide much more detail.
It is especially interesting to compare and contrast the design of these two very different systems.
Chapter 20 brieﬂy describes a few other inﬂuential operating systems.
As we wrote this Ninth Edition of Operating System Concepts, we were guided by the recent growth in three fundamental areas that affect operating systems:
To emphasize these topics, we have integrated relevant coverage throughout this new edition—and, in the case of virtualization, have written an entirely new chapter.
Additionally, we have rewritten material in almost every chapter by bringing older material up to date and removing material that is no longer interesting or relevant.
For example, we have eliminated the chapter on real-time systems and instead have integrated appropriate coverage of these systems throughout the text.
We have reordered the chapters on storage management and have moved up the presentation of process synchronization so that it appears before process scheduling.
Most of these organizational changes are based on our experiences while teaching courses on operating systems.
Below, we provide a brief outline of the major changes to the various chapters:
Additionally, the coverage of computing environments now includes mobile systems and cloud computing.
Chapter 2, Operating-System Structures, provides new coverage of user interfaces for mobile devices, including discussions of iOS and Android, and expanded coverage of Mac OS X as a type of hybrid system.
Chapter 3, Processes, now includes coverage of multitasking in mobile operating systems, support for the multiprocess model in Google’s Chrome web browser, and zombie and orphan processes in UNIX.
Chapter 4, Threads, supplies expanded coverage of parallelism and Amdahl’s law.
It also provides a new section on implicit threading, including OpenMP and Apple’s Grand Central Dispatch.
Coverage of real-time scheduling algorithms has also been integrated into this chapter.
Chapter 8, Main Memory, includes new coverage of swapping on mobile.
Chapter 9, Virtual Memory, updates kernel memory management to include the Linux SLUB and SLOB memory allocators.
Chapter 15, Security, has a revised cryptography section with modern.
The chapter also includes new coverage of Windows 7 security.
Chapter 16, Virtual Machines, is a new chapter that provides an overview of virtualization and how it relates to contemporary operating systems.
This book uses examples of many real-world operating systems to illustrate fundamental operating-system concepts.
Particular attention is paid to Linux and Microsoft Windows, but we also refer to various versions of UNIX (including Solaris, BSD, and Mac OS X)
The text also provides several example programs written in C and Java.
These programs are intended to run in the following programming environments:
Java is a widely used programming language with a rich API and built-in language support for thread creation and management.
Java programs run on any operating system supporting a Java virtual machine (or JVM)
We illustrate various operating-system and networking concepts with Java programs tested using the Java 1.6 JVM.
The primary programming environment for Windows systems is the Windows API, which provides a comprehensive set of functions for managing processes, threads, memory, and peripheral devices.
We supply several C programs illustrating the use of this API.
We have chosen these three programming environments because we believe that they best represent the two most popular operating-system models —Windows and UNIX/Linux—along with the widely used Java environment.
Most programming examples are written in C, and we expect readers to be comfortable with this language.
Readers familiar with both the C and Java languages should easily understand most programs provided in this text.
In some instances—such as thread creation—we illustrate a speciﬁc concept using all three programming environments, allowing the reader to contrast the three different libraries as they address the same task.
In other situations, we may use just one of the APIs to demonstrate a concept.
For example, we illustrate shared memory using just the POSIX API; socket programming in TCP/IP is highlighted using the Java API.
To help students gain a better understanding of the Linux system, we provide a Linux virtual machine, including the Linux source code, that is available for download from the the website supporting this text (http://www.os-book.com)
This virtual machine also includes a gcc development environment with compilers and editors.
Most of the programming assignments in the book can be completed on this virtual machine, with the exception of assignments that require Java or the Windows API.
We also provide three programming assignments that modify the Linux kernel through kernel modules:
Adding a kernel module that uses various kernel data structures.
Adding a kernel module that iterates over tasks in a running Linux system.
Over time it is our intention to add additional kernel module assignments on the supporting website.
When you visit the website supporting this text at http://www.os-book.com, you can download the following resources:
On the website for this text, we provide several sample syllabi that suggest various approaches for using the text in both introductory and advanced courses.
As a general rule, we encourage instructors to progress sequentially through the chapters, as this strategy provides the most thorough study of operating systems.
However, by using the sample syllabi, an instructor can select a different ordering of chapters (or subsections of chapters)
In this edition, we have added over sixty new written exercises and over twenty new programming problems and projects.
Most of the new programming assignments involve processes, threads, process synchronization, and memory management.
Some involve adding kernel modules to the Linux system which requires using either the Linux virtual machine that accompanies this text or another suitable Linux distribution.
Solutions to written exercises and programming assignments are available to instructors who have adopted this text for their operating-system class.
To obtain these restricted supplements, contact your local John Wiley & Sons sales representative.
You can ﬁnd your Wiley representative by going to http://www.wiley.com/college and clicking “Who’s my rep?”
We encourage you to take advantage of the practice exercises that appear at the end of each chapter.
Solutions to the practice exercises are available for download from the supporting website http://www.os-book.com.
We also encourage you to read through the study guide, which was prepared by one of our students.
Finally, for students who are unfamiliar with UNIX and Linux systems, we recommend that you download and install the Linux virtual machine that we include on the supporting website.
Not only will this provide you with a new computing experience, but the open-source nature of Linux will allow you to easily examine the inner details of this popular operating system.
We wish you the very best of luck in your study of operating systems.
We have endeavored to eliminate typos, bugs, and the like from the text.
But, as in new releases of software, bugs almost surely remain.
An up-to-date errata list is accessible from the book’s website.
We would be grateful if you would notify us of any errors or omissions in the book that are not on the current list of errata.
We would be glad to receive suggestions on improvements to the book.
We also welcome any contributions to the book website that could be of.
Robert Love updated both Chapter 18 and the Linux coverage throughout the text, as well as answering many of our Android-related questions.
Chapter 18 was derived from an unpublished manuscript by Stephen Tweedie.
Cliff Martin helped with updating the UNIX appendix to cover FreeBSD.
Some of the exercises and accompanying solutions were supplied by Arvind Krishnamurthy.
Andrew DeNicola prepared the student study guide that is available on our website.
Some of the the slides were prepeared by Marilyn Turnamian.
Judi Paige helped with generating ﬁgures and presentation of slides.
Mark Wogahn has made sure that the software to produce this book (LATEX and fonts) works properly.
Ranjan Kumar Meher rewrote some of the LATEX software used in the production of this new text.
She was assisted by Katherine Willis, who managed many details of the project smoothly.
The cover illustrator was Susan Cyr, and the cover designer was Madelyn Lesure.
Overview An operating system acts as an intermediary between the user of a computer and the computer hardware.
The purpose of an operating system is to provide an environment in which a user can execute programs in a convenient and efﬁcient manner.
An operating system is software that manages the computer hardware.
Internally, operating systems vary greatly in their makeup, since they are organized along many different lines.
The design of a new operating system is a major task.
It is important that the goals of the system be well deﬁned before the design begins.
These goals form the basis for choices among various algorithms and strategies.
Each of these pieces should be a well-delineated portion of the system, with carefully deﬁned inputs, outputs, and functions.
An operating system is a program that manages a computer’s hardware.
It also provides a basis for application programs and acts as an intermediary between the computer user and the computer hardware.
An amazing aspect of operating systems is how they vary in accomplishing these tasks.
Mainframe operating systems are designed primarily to optimize utilization of hardware.
Personal computer (PC) operating systems support complex games, business applications, and everything in between.
Operating systems for mobile computers provide an environment in which a user can easily interface with the computer to execute programs.
Thus, some operating systems are designed to be convenient, others to be efﬁcient, and others to be some combination of the two.
Before we can explore the details of computer system operation, we need to know something about system structure.
We thus discuss the basic functions of system startup, I/O, and storage early in this chapter.
We also describe the basic computer architecture that makes it possible to write a functional operating system.
Because an operating system is large and complex, it must be created piece by piece.
Each of these pieces should be a well-delineated portion of the system, with carefully deﬁned inputs, outputs, and functions.
In this chapter, we provide a general overview of the major components of a contemporary computer system as well as the functions provided by the operating system.
Additionally, we cover several other topics to help set the stage for the remainder of this text: data structures used in operating systems, computing environments, and open-source operating systems.
To provide a grand tour of the major components of operating systems.
To give an overview of the many types of computing environments.
Figure 1.1 Abstract view of the components of a computer system.
We begin our discussion by looking at the operating system’s role in the overall computer system.
A computer system can be divided roughly into four components: the hardware, the operating system, the application programs, and the users (Figure 1.1)
The hardware—the central processing unit (CPU), the memory, and the input/output (I/O) devices—provides the basic computing resources for the system.
The application programs—such as word processors, spreadsheets, compilers, and Web browsers—deﬁne the ways in which these resources are used to solve users’ computing problems.
The operating system controls the hardware and coordinates its use among the various application programs for the various users.
We can also view a computer system as consisting of hardware, software, and data.
The operating system provides the means for proper use of these resources in the operation of the computer system.
Like a government, it performs no useful function by itself.
It simply provides an environment within which other programs can do useful work.
To understand more fully the operating system’s role, we next explore operating systems from two viewpoints: that of the user and that of the system.
The user’s view of the computer varies according to the interface being used.
Most computer users sit in front of a PC, consisting of a monitor, keyboard, mouse, and system unit.
The goal is to maximize the work (or play) that the user is performing.
In this case, the operating system is designed mostly for ease of use, with some attention paid to performance and none paid to resource utilization—how various hardware and software resources are shared.
Performance is, of course, important to the user; but such systems are optimized for the single-user experience rather than the requirements of multiple users.
In other cases, a user sits at a terminal connected to a mainframe or a minicomputer.
Other users are accessing the same computer through other terminals.
The operating system in such cases is designed to maximize resource utilizationto assure that all available CPU time, memory, and I/O are used efﬁciently and that no individual user takes more than her fair share.
In still other cases, users sit at workstations connected to networks of other workstations and servers.
These users have dedicated resources at their disposal, but they also share resources such as networking and servers, including ﬁle, compute, and print servers.
Therefore, their operating system is designed to compromise between individual usability and resource utilization.
Recently, many varieties of mobile computers, such as smartphones and tablets, have come into fashion.
Quite often, they are connected to networks through cellular or other wireless technologies.
Increasingly, these mobile devices are replacing desktop and laptop computers for people who are primarily interested in using computers for e-mail and web browsing.
The user interface for mobile computers generally features a touch screen, where the user interacts with the system by pressing and swiping ﬁngers across the screen rather than using a physical keyboard and mouse.
For example, embedded computers in home devices and automobiles may have numeric keypads and may turn indicator lights on or off to show status, but they and their operating systems are designed primarily to run without user intervention.
From the computer’s point of view, the operating system is the program most intimately involved with the hardware.
In this context, we can view an operating system as a resource allocator.
A computer system has many resources that may be required to solve a problem: CPU time, memory space, ﬁle-storage space, I/O devices, and so on.
The operating system acts as the manager of these resources.
Facing numerous and possibly conﬂicting requests for resources, the operating system must decide how to allocate them to speciﬁc programs and users so that it can operate the computer system efﬁciently and fairly.
As we have seen, resource allocation is especially important where many users access the same mainframe or minicomputer.
A slightly different view of an operating system emphasizes the need to control the various I/O devices and user programs.
A control program manages the execution of user programs to prevent errors and improper use of the computer.
It is especially concerned with the operation and control of I/O devices.
By now, you can probably see that the term operating system covers many roles and functions.
That is the case, at least in part, because of the myriad designs and uses of computers.
Computers are present within toasters, cars, ships, spacecraft, homes, and businesses.
They are the basis for game machines, music players, cable TV tuners, and industrial control systems.
Although computers have a relatively short history, they have evolved rapidly.
Computing started as an experiment to determine what could be done and quickly moved to ﬁxed-purpose systems for military uses, such as code breaking and trajectory plotting, and governmental uses, such as census calculation.
Those early computers evolved into general-purpose, multifunction mainframes, and that’s when operating systems were born.
In the 1960s, Moore’s Law predicted that the number of transistors on an integrated circuit would double every eighteen months, and that prediction has held true.
Computers gained in functionality and shrunk in size, leading to a vast number of uses and a vast number and variety of operating systems.
See Chapter 20 for more details on the history of operating systems.
How, then, can we deﬁne what an operating system is? In general, we have no completely adequate deﬁnition of an operating system.
Operating systems exist because they offer a reasonable way to solve the problem of creating a usable computing system.
The fundamental goal of computer systems is to execute user programs and to make solving user problems easier.
Since bare hardware alone is not particularly easy to use, application programs are developed.
These programs require certain common operations, such as those controlling the I/O devices.
The common functions of controlling and allocating resources are then brought together into one piece of software: the operating system.
In addition, we have no universally accepted deﬁnition of what is part of the operating system.
A simple viewpoint is that it includes everything a vendor ships when you order “the operating system.” The features included, however, vary greatly across systems.
Some systems take up less than a megabyte of space and lack even a full-screen editor, whereas others require gigabytes of space and are based entirely on graphical windowing systems.
A more common deﬁnition, and the one that we usually follow, is that the operating system is the one program running at all times on the computer—usually called the kernel.
Along with the kernel, there are two other types of programs: system programs, which are associated with the operating system but are not necessarily part of the kernel, and application programs, which include all programs not associated with the operation of the system.
The matter of what constitutes an operating system became increasingly important as personal computers became more widespread and operating systems grew increasingly sophisticated.
In 1998, the United States Department of Justice ﬁled suit against Microsoft, in essence claiming that Microsoft included too much functionality in its operating systems and thus prevented application vendors from competing.
For example, a Web browser was an integral part of the operating systems.
As a result, Microsoft was found guilty of using its operating-system monopoly to limit competition.
Today, however, if we look at operating systems for mobile devices, we see that once again the number of features constituting the operating system.
Mobile operating systems often include not only a core kernel but also middleware—a set of software frameworks that provide additional services to application developers.
For example, each of the two most prominent mobile operating systems—Apple’s iOS and Google’s Android—features a core kernel along with middleware that supports databases, multimedia, and graphics (to name a only few)
Before we can explore the details of how computer systems operate, we need general knowledge of the structure of a computer system.
In this section, we look at several parts of this structure.
The section is mostly concerned with computer-system organization, so you can skim or skip it if you already understand the concepts.
A modern general-purpose computer system consists of one or more CPUs and a number of device controllers connected through a common bus that provides access to shared memory (Figure 1.2)
Each device controller is in charge of a speciﬁc type of device (for example, disk drives, audio devices, or video displays)
The CPU and the device controllers can execute in parallel, competing for memory cycles.
To ensure orderly access to the shared memory, a memory controller synchronizes access to the memory.
For a computer to start running—for instance, when it is powered up or rebooted—it needs to have an initial program to run.
This initial program, or bootstrap program, tends to be simple.
Typically, it is stored within the computer hardware in read-only memory (ROM) or electrically erasable programmable read-only memory (EEPROM), known by the general term ﬁrmware.
It initializes all aspects of the system, from CPU registers to device controllers to memory contents.
The bootstrap program must know how to load the operating system and how to start executing that system.
Figure 1.3 Interrupt timeline for a single process doing output.
Once the kernel is loaded and executing, it can start providing services to the system and its users.
Some services are provided outside of the kernel, by system programs that are loaded into memory at boot time to become system processes, or system daemons that run the entire time the kernel is running.
On UNIX, the ﬁrst system process is “init,” and it starts many other daemons.
Once this phase is complete, the system is fully booted, and the system waits for some event to occur.
The occurrence of an event is usually signaled by an interrupt from either the hardware or the software.
Hardware may trigger an interrupt at any time by sending a signal to the CPU, usually by way of the system bus.
Software may trigger an interrupt by executing a special operation called a system call (also called a monitor call)
When the CPU is interrupted, it stops what it is doing and immediately transfers execution to a ﬁxed location.
The ﬁxed location usually contains the starting address where the service routine for the interrupt is located.
The interrupt service routine executes; on completion, the CPU resumes the interrupted computation.
A timeline of this operation is shown in Figure 1.3
Each computer design has its own interrupt mechanism, but several functions are common.
The interrupt must transfer control to the appropriate interrupt service routine.
The straightforward method for handling this transfer would be to invoke a generic routine to examine the interrupt information.
Since only a predeﬁned number of interrupts is possible, a table of pointers to interrupt routines can be used instead to provide the necessary speed.
The interrupt routine is called indirectly through the table, with no intermediate routine needed.
Generally, the table of pointers is stored in low memory (the ﬁrst hundred or so locations)
These locations hold the addresses of the interrupt service routines for the various devices.
This array, or interrupt vector, of addresses is then indexed by a unique device number, given with the interrupt request, to provide the address of the interrupt service routine for.
All other storage in a computer is based on collections of bits.
Given enough bits, it is amazing how many things a computer can represent: numbers, letters, images, movies, sounds, documents, and programs, to name a few.
A byte is 8 bits, and on most computers it is the smallest convenient chunk of storage.
For example, most computers don’t have an instruction to move a bit but do have one to move a byte.
A less common term is word, which is a given computer architecture’s native unit of data.
A word is made up of one or more bytes.
A computer executes many operations in its native word size rather than a byte at a time.
Computer storage, along with most computer throughput, is generally measured and manipulated in bytes and collections of bytes.
Networking measurements are an exception to this general rule; they are given in bits (because networks move data a bit at a time)
Operating systems as different as Windows and UNIX dispatch interrupts in this manner.
The interrupt architecture must also save the address of the interrupted instruction.
Many old designs simply stored the interrupt address in a ﬁxed location or in a location indexed by the device number.
More recent architectures store the return address on the system stack.
If the interrupt routine needs to modify the processor state—for instance, by modifying register values—it must explicitly save the current state and then restore that state before returning.
After the interrupt is serviced, the saved return address is loaded into the program counter, and the interrupted computation resumes as though the interrupt had not occurred.
The CPU can load instructions only from memory, so any programs to run must be stored there.
General-purpose computers run most of their programs from rewritable memory, called main memory (also called random-access memory, or RAM)
Main memory commonly is implemented in a semiconductor technology called dynamic random-access memory (DRAM)
Because ROM cannot be changed, only static programs, such as the bootstrap program described earlier, are stored there.
The immutability of ROM is of use in game cartridges.
For example, smartphones have EEPROM to store their factory-installed programs.
Interaction is achieved through a sequence of load or store instructions to speciﬁc memory addresses.
The load instruction moves a byte or word from main memory to an internal register within the CPU, whereas the store instruction moves the content of a register to main memory.
Aside from explicit loads and stores, the CPU automatically loads instructions from main memory for execution.
The instruction is then decoded and may cause operands to be fetched from memory and stored in some internal register.
After the instruction on the operands has been executed, the result may be stored back in memory.
Notice that the memory unit sees only a stream of memory addresses.
It does not know how they are generated (by the instruction counter, indexing, indirection, literal addresses, or some other means) or what they are for (instructions or data)
Accordingly, we can ignore how a memory address is generated by a program.
We are interested only in the sequence of memory addresses generated by the running program.
Ideally, we want the programs and data to reside in main memory permanently.
This arrangement usually is not possible for the following two reasons:
Main memory is usually too small to store all needed programs and data permanently.
Main memory is a volatile storage device that loses its contents when power is turned off or otherwise lost.
Thus, most computer systems provide secondary storage as an extension of main memory.
The main requirement for secondary storage is that it be able to hold large quantities of data permanently.
The most common secondary-storage device is a magnetic disk, which provides storage for both programs and data.
Most programs (system and application) are stored on a disk until they are loaded into memory.
Many programs then use the disk as both the source and the destination of their processing.
Others include cache memory, CD-ROM, magnetic tapes, and so on.
Each storage system provides the basic functions of storing a datum and holding that datum until it is retrieved at a later time.
The main differences among the various storage systems lie in speed, cost, size, and volatility.
The wide variety of storage systems can be organized in a hierarchy (Figure 1.4) according to speed and cost.
As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases.
This trade-off is reasonable; if a given storage system were both faster and less expensive than another—other properties being the same—then there would be no reason to use the slower, more expensive memory.
The top four levels of memory in Figure 1.4 may be constructed using semiconductor memory.
In addition to differing in speed and cost, the various storage systems are either volatile or nonvolatile.
As mentioned earlier, volatile storage loses its contents when the power to the device is removed.
In the absence of expensive battery and generator backup systems, data must be written to nonvolatile storage for safekeeping.
In the hierarchy shown in Figure 1.4, the storage systems above the solid-state disk are volatile, whereas those including the solid-state disk and below are nonvolatile.
Solid-state disks have several variants but in general are faster than magnetic disks and are nonvolatile.
One type of solid-state disk stores data in a large DRAM array during normal operation but also contains a hidden magnetic hard disk and a battery for backup power.
If external power is interrupted, this solid-state disk’s controller copies the data from RAM to the magnetic disk.
When external power is restored, the controller copies the data back into RAM.
Another form of solid-state disk is ﬂash memory, which is popular in cameras and personal digital assistants (PDAs), in robots, and increasingly for storage on general-purpose computers.
Flash memory is slower than DRAM but needs no power to retain its contents.
Another form of nonvolatile storage is NVRAM, which is DRAM with battery backup power.
This memory can be as fast as DRAM and (as long as the battery lasts) is nonvolatile.
The design of a complete memory system must balance all the factors just discussed: it must use only as much expensive memory as necessary while providing as much inexpensive, nonvolatile memory as possible.
Storage is only one of many types of I/O devices within a computer.
A large portion of operating system code is dedicated to managing I/O, both because of its importance to the reliability and performance of a system and because of the varying nature of the devices.
A general-purpose computer system consists of CPUs and multiple device controllers that are connected through a common bus.
Each device controller is in charge of a speciﬁc type of device.
Depending on the controller, more than one device may be attached.
For instance, seven or more devices can be attached to the small computer-systems interface (SCSI) controller.
A device controller maintains some local buffer storage and a set of special-purpose registers.
The device controller is responsible for moving the data between the peripheral devices that it controls and its local buffer storage.
Typically, operating systems have a device driver for each device controller.
This device driver understands the device controller and provides the rest of the operating system with a uniform interface to the device.
To start an I/O operation, the device driver loads the appropriate registers within the device controller.
The device controller, in turn, examines the contents of these registers to determine what action to take (such as “read a character from the keyboard”)
The controller starts the transfer of data from the device to its local buffer.
Once the transfer of data is complete, the device controller informs the device driver via an interrupt that it has ﬁnished its operation.
The device driver then returns control to the operating system, possibly returning the data or a pointer to the data if the operation was a read.
This form of interrupt-driven I/O is ﬁne for moving small amounts of data but can produce high overhead when used for bulk data movement such as disk I/O.
To solve this problem, direct memory access (DMA) is used.
After setting up buffers, pointers, and counters for the I/O device, the device controller transfers an entire block of data directly to or from its own buffer storage to memory, with no intervention by the CPU.
Only one interrupt is generated per block, to tell the device driver that the operation has completed, rather than the one interrupt per byte generated for low-speed devices.
While the device controller is performing these operations, the CPU is available to accomplish other work.
On these systems, multiple components can talk to other components concurrently, rather than competing for cycles on a shared bus.
Figure 1.5 shows the interplay of all components of a computer system.
In Section 1.2, we introduced the general structure of a typical computer system.
A computer system can be organized in a number of different ways, which we.
On a singleprocessor system, there is one main CPU capable of executing a general-purpose instruction set, including instructions from user processes.
Almost all singleprocessor systems have other special-purpose processors as well.
They may come in the form of device-speciﬁc processors, such as disk, keyboard, and graphics controllers; or, on mainframes, they may come in the form of more general-purpose processors, such as I/O processors that move data rapidly among the components of the system.
All of these special-purpose processors run a limited instruction set and do not run user processes.
Sometimes, they are managed by the operating system, in that the operating system sends them information about their next task and monitors their status.
For example, a disk-controller microprocessor receives a sequence of requests from the main CPU and implements its own disk queue and scheduling algorithm.
This arrangement relieves the main CPU of the overhead of disk scheduling.
PCs contain a microprocessor in the keyboard to convert the keystrokes into codes to be sent to the CPU.
In other systems or circumstances, special-purpose processors are low-level components built into the hardware.
The operating system cannot communicate with these processors; they do their jobs autonomously.
The use of special-purpose microprocessors is common and does not turn a single-processor system into.
If there is only one general-purpose CPU, then the system is a single-processor system.
Within the past several years, multiprocessor systems (also known as parallel systems or multicore systems) have begun to dominate the landscape of computing.
Such systems have two or more processors in close communication, sharing the computer bus and sometimes the clock, memory, and peripheral devices.
Multiprocessor systems ﬁrst appeared prominently appeared in servers and have since migrated to desktop and laptop systems.
Recently, multiple processors have appeared on mobile devices such as smartphones and tablet computers.
By increasing the number of processors, we expect to get more work done in less time.
The speed-up ratio with N processors is not N, however; rather, it is less than N.
When multiple processors cooperate on a task, a certain amount of overhead is incurred in keeping all the parts working correctly.
This overhead, plus contention for shared resources, lowers the expected gain from additional processors.
Similarly, N programmers working closely together do not produce N times the amount of work a single programmer would produce.
Multiprocessor systems can cost less than equivalent multiple single-processor systems, because they can share peripherals, mass storage, and power supplies.
If several programs operate on the same set of data, it is cheaper to store those data on one disk and to have all the processors share them than to have many computers with local disks and many copies of the data.
If functions can be distributed properly among several processors, then the failure of one processor will not halt the system, only slow it down.
If we have ten processors and one fails, then each of the remaining nine processors can pick up a share of the work of the failed processor.
Thus, the entire system runs only 10 percent slower, rather than failing altogether.
Increased reliability of a computer system is crucial in many applications.
The ability to continue providing service proportional to the level of surviving hardware is called graceful degradation.
Some systems go beyond graceful degradation and are called fault tolerant, because they can suffer a failure of any single component and still continue operation.
Fault tolerance requires a mechanism to allow the failure to be detected, diagnosed, and, if possible, corrected.
The HP NonStop (formerly Tandem) system uses both hardware and software duplication to ensure continued operation despite faults.
The system consists of multiple pairs of CPUs, working in lockstep.
Both processors in the pair execute each instruction and compare the results.
If the results differ, then one CPU of the pair is at fault, and both are halted.
The process that was being executed is then moved to another pair of CPUs, and the instruction that failed.
This solution is expensive, since it involves special hardware and considerable hardware duplication.
The multiple-processor systems in use today are of two types.
Some systems use asymmetric multiprocessing, in which each processor is assigned a speciﬁc task.
A boss processor controls the system; the other processors either look to the boss for instruction or have predeﬁned tasks.
The boss processor schedules and allocates work to the worker processors.
The most common systems use symmetric multiprocessing (SMP), in which each processor performs all tasks within the operating system.
Notice that each processor has its own set of registers, as well as a private—or local —cache.
An example of an SMP system is AIX, a commercial version of UNIX designed by IBM.
An AIX system can be conﬁgured to employ dozens of processors.
The beneﬁt of this model is that many processes can run simultaneously—N processes can run if there are N CPUs—without causing performance to deteriorate signiﬁcantly.
However, we must carefully control I/O to ensure that the data reach the appropriate processor.
Also, since the CPUs are separate, one may be sitting idle while another is overloaded, resulting in inefﬁciencies.
These inefﬁciencies can be avoided if the processors share certain data structures.
A multiprocessor system of this form will allow processes and resources—such as memoryto be shared dynamically among the various processors and can lower the variance among the processors.
The difference between symmetric and asymmetric multiprocessing may result from either hardware or software.
Special hardware can differentiate the multiple processors, or the software can be written to allow only one boss and multiple workers.
If the CPU has an integrated memory controller, then adding CPUs can also increase the amount.
Either way, multiprocessing can cause a system to change its memory access model from uniform memory access (UMA) to non-uniform memory access (NUMA)
With NUMA, some parts of memory may take longer to access than other parts, creating a performance penalty.
Operating systems can minimize the NUMA penalty through resource management, as discussed in Section 9.5.4
A recent trend in CPU design is to include multiple computing cores on a single chip.
They can be more efﬁcient than multiple chips with single cores because on-chip communication is faster than between-chip communication.
In addition, one chip with multiple cores uses signiﬁcantly less power than multiple single-core chips.
It is important to note that while multicore systems are multiprocessor systems, not all multiprocessor systems are multicore, as we shall see in Section 1.3.3
In our coverage of multiprocessor systems throughout this text, unless we state otherwise, we generally use the more contemporary term multicore, which excludes some multiprocessor systems.
In Figure 1.7, we show a dual-core design with two cores on the same chip.
In this design, each core has its own register set as well as its own local cache.
Other designs might use a shared cache or a combination of local and shared caches.
Aside from architectural considerations, such as cache, memory, and bus contention, these multicore CPUs appear to the operating system as N standard processors.
This characteristic puts pressure on operating system designers—and application programmers—to make use of those processing cores.
Finally, blade servers are a relatively recent development in which multiple processor boards, I/O boards, and networking boards are placed in the same chassis.
The difference between these and traditional multiprocessor systems is that each blade-processor board boots independently and runs its own operating system.
Some blade-server boards are multiprocessor as well, which blurs the lines between types of computers.
In essence, these servers consist of multiple independent multiprocessor systems.
Figure 1.7 A dual-core design with two cores placed on the same chip.
Another type of multiprocessor system is a clustered system, which gathers together multiple CPUs.
Clustered systems differ from the multiprocessor systems described in Section 1.3.2 in that they are composed of two or more individual systems—or nodes—joined together.
Each node may be a single processor system or a multicore system.
We should note that the deﬁnition of clustered is not concrete; many commercial packages wrestle to deﬁne a clustered system and why one form is better than another.
The generally accepted deﬁnition is that clustered computers share storage and are closely linked via a local-area network LAN (as described in Chapter 17) or a faster interconnect, such as InﬁniBand.
Clustering is usually used to provide high-availability service—that is, service will continue even if one or more systems in the cluster fail.
Generally, we obtain high availability by adding a level of redundancy in the system.
A layer of cluster software runs on the cluster nodes.
Each node can monitor one or more of the others (over the LAN)
If the monitored machine fails, the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine.
The users and clients of the applications see only a brief interruption of service.
In asymmetric clustering, one machine is in hot-standby mode while the other is running the applications.
The hot-standby host machine does nothing but monitor the active server.
If that server fails, the hot-standby host becomes the active server.
In symmetric clustering, two or more hosts are running applications and are monitoring each other.
This structure is obviously more efﬁcient, as it uses all of the available hardware.
However it does require that more than one application be available to run.
Since a cluster consists of several computer systems connected via a network, clusters can also be used to provide high-performance computing environments.
Such systems can supply signiﬁcantly greater computational power than single-processor or even SMP systems because they can run an application concurrently on all computers in the cluster.
The application must have been written speciﬁcally to take advantage of the cluster, however.
This involves a technique known as parallelization, which divides a program into separate components that run in parallel on individual computers in the cluster.
Typically, these applications are designed so that once each computing node in the cluster has solved its portion of the problem, the results from all the nodes are combined into a ﬁnal solution.
Other forms of clusters include parallel clusters and clustering over a wide-area network (WAN) (as described in Chapter 17)
Parallel clusters allow multiple hosts to access the same data on shared storage.
Because most operating systems lack support for simultaneous data access by multiple hosts, parallel clusters usually require the use of special versions of software and special releases of applications.
For example, Oracle Real Application Cluster is a version of Oracle’s database that has been designed to run on a parallel cluster.
Each machine runs Oracle, and a layer of software tracks access to the shared disk.
Each machine has full access to all data in the database.
To provide this shared access, the system must also supply access control and locking to.
No single speciﬁc software package is required to construct a cluster.
Rather, the nodes use a set of open-source software libraries to communicate with one another.
Thus, there are a variety of approaches to constructing a Beowulf cluster.
Typically, though, Beowulf computing nodes run the Linux operating system.
Since Beowulf clusters require no special hardware and operate using open-source software that is available free, they offer a low-cost strategy for building a high-performance computing cluster.
In fact, some Beowulf clusters built from discarded personal computers are using hundreds of nodes to solve computationally expensive scientiﬁc computing problems.
This function, commonly known as a distributed lock manager (DLM), is included in some cluster technology.
Some cluster products support dozens of systems in a cluster, as well as clustered nodes that are separated by miles.
Many of these improvements are made possible by storage-area networks (SANs), as described in Section 10.3.3, which allow many systems to attach to a pool of storage.
If the applications and their data are stored on the SAN, then the cluster software can assign the application to run on any host that is attached to the SAN.
If the host fails, then any other host can take over.
In a database cluster, dozens of hosts can share the same database, greatly increasing performance and reliability.
Figure 1.8 depicts the general structure of a clustered system.
Now that we have discussed basic computer-system organization and architecture, we are ready to talk about operating systems.
An operating system provides the environment within which programs are executed.
Internally, operating systems vary greatly in their makeup, since they are organized along many different lines.
There are, however, many commonalities, which we consider in this section.
One of the most important aspects of operating systems is the ability to multiprogram.
A single program cannot, in general, keep either the CPU or the I/O devices busy at all times.
Multiprogramming increases CPU utilization by organizing jobs (code and data) so that the CPU always has one to execute.
The idea is as follows: The operating system keeps several jobs in memory simultaneously (Figure 1.9)
Since, in general, main memory is too small to accommodate all jobs, the jobs are kept initially on the disk in the job pool.
This pool consists of all processes residing on disk awaiting allocation of main memory.
The set of jobs in memory can be a subset of the jobs kept in the job pool.
The operating system picks and begins to execute one of the jobs in memory.
Eventually, the job may have to wait for some task, such as an I/O operation, to complete.
In a multiprogrammed system, the operating system simply switches to, and executes, another job.
When that job needs to wait, the CPU switches to another job, and so on.
Eventually, the ﬁrst job ﬁnishes waiting and gets the CPU back.
As long as at least one job needs to execute, the CPU is never idle.
A lawyer does not work for only one client at a time, for example.
While one case is waiting to go to trial or have papers typed, the lawyer can work on another case.
If he has enough clients, the lawyer will never be idle for lack of work.
Idle lawyers tend to become politicians, so there is a certain social value in keeping lawyers busy.
Multiprogrammed systems provide an environment in which the various system resources (for example, CPU, memory, and peripheral devices) are utilized effectively, but they do not provide for user interaction with the computer system.
Time sharing (or multitasking) is a logical extension of multiprogramming.
In time-sharing systems, the CPU executes multiple jobs by switching among them, but the switches occur so frequently that the users can interact with each program while it is running.
Time sharing requires an interactive computer system, which provides direct communication between the user and the system.
The user gives instructions to the operating system or to a program directly, using a input device such as a keyboard, mouse, touch pad, or touch screen, and waits for immediate results on an output device.
Accordingly, the response time should be short—typically less than one second.
A time-shared operating system allows many users to share the computer simultaneously.
Since each action or command in a time-shared system tends to be short, only a little CPU time is needed for each user.
As the system switches rapidly from one user to the next, each user is given the impression that the entire computer system is dedicated to his use, even though it is being shared among many users.
A time-shared operating system uses CPU scheduling and multiprogramming to provide each user with a small portion of a time-shared computer.
Each user has at least one separate program in memory.
A program loaded into memory and executing is called a process.
When a process executes, it typically executes for only a short time before it either ﬁnishes or needs to perform I/O.
I/O may be interactive; that is, output goes to a display for the user, and input comes from a user keyboard, mouse, or other device.
Since interactive I/O typically runs at “people speeds,” it may take a long time to complete.
Input, for example, may be bounded by the user’s typing speed; seven characters per second is fast for people but incredibly slow for computers.
Rather than let the CPU sit idle as this interactive input takes place, the operating system will rapidly switch the CPU to the program of some other user.
Time sharing and multiprogramming require that several jobs be kept simultaneously in memory.
If several jobs are ready to be brought into memory, and if there is not enough room for all of them, then the system must choose among them.
When the operating system selects a job from the job pool, it loads that job into memory for execution.
In addition, if several jobs are ready to run at the same time, the system must choose which job will run ﬁrst.
Finally, running multiple jobs concurrently requires that their ability to affect one another be limited in all phases of the operating system, including process scheduling, disk storage, and memory management.
In a time-sharing system, the operating system must ensure reasonable response time.
This goal is sometimes accomplished through swapping, whereby processes are swapped in and out of main memory to the disk.
A more common method for ensuring reasonable response time is virtual memory, a technique that allows the execution of a process that is not completely in.
The main advantage of the virtual-memory scheme is that it enables users to run programs that are larger than actual physical memory.
Further, it abstracts main memory into a large, uniform array of storage, separating logical memory as viewed by the user from physical memory.
The ﬁle system resides on a collection of disks; hence, disk management must be provided (Chapter 10)
In addition, a time-sharing system provides a mechanism for protecting resources from inappropriate use (Chapter 14)
If there are no processes to execute, no I/O devices to service, and no users to whom to respond, an operating system will sit quietly, waiting for something to happen.
Events are almost always signaled by the occurrence of an interrupt or a trap.
A trap (or an exception) is a software-generated interrupt caused either by an error (for example, division by zero or invalid memory access) or by a speciﬁc request from a user program that an operating-system service be performed.
The interrupt-driven nature of an operating system deﬁnes that system’s general structure.
For each type of interrupt, separate segments of code in the operating system determine what action should be taken.
An interrupt service routine is provided to deal with the interrupt.
Since the operating system and the users share the hardware and software resources of the computer system, we need to make sure that an error in a user program could cause problems only for the one program running.
With sharing, many processes could be adversely affected by a bug in one program.
For example, if a process gets stuck in an inﬁnite loop, this loop could prevent the correct operation of many other processes.
More subtle errors can occur in a multiprogramming system, where one erroneous program might modify another program, the data of another program, or even the operating system itself.
Without protection against these sorts of errors, either the computer must execute only one process at a time or all output must be suspect.
A properly designed operating system must ensure that an incorrect (or malicious) program cannot cause other programs to execute incorrectly.
In order to ensure the proper execution of the operating system, we must be able to distinguish between the execution of operating-system code and userdeﬁned code.
The approach taken by most computer systems is to provide hardware support that allows us to differentiate among various modes of execution.
At the very least, we need two separate modes of operation: user mode and kernel mode (also called supervisor mode, system mode, or privileged mode)
With the mode bit, we can distinguish between a task that is executed on behalf of the operating system and one that is executed on behalf of the user.
When the computer system is executing on behalf of a user application, the system is in user mode.
However, when a user application requests a service from the operating system (via a system call), the system must transition from user to kernel mode to fulﬁll the request.
As we shall see, this architectural enhancement is useful for many other aspects of system operation as well.
At system boot time, the hardware starts in kernel mode.
The operating system is then loaded and starts user applications in user mode.
Whenever a trap or interrupt occurs, the hardware switches from user mode to kernel mode (that is, changes the state of the mode bit to 0)
Thus, whenever the operating system gains control of the computer, it is in kernel mode.
The system always switches to user mode (by setting the mode bit to 1) before passing control to a user program.
The dual mode of operation provides us with the means for protecting the operating system from errant users—and errant users from one another.
We accomplish this protection by designating some of the machine instructions that may cause harm as privileged instructions.
The hardware allows privileged instructions to be executed only in kernel mode.
If an attempt is made to execute a privileged instruction in user mode, the hardware does not execute the instruction but rather treats it as illegal and traps it to the operating system.
The instruction to switch to kernel mode is an example of a privileged instruction.
Some other examples include I/O control, timer management, and interrupt management.
As we shall see throughout the text, there are many additional privileged instructions.
The concept of modes can be extended beyond two modes (in which case the CPU uses more than one bit to set and test the mode)
CPUs that support virtualization (Section 16.1) frequently have a separate mode to indicate when the virtual machine manager (VMM)—and the virtualization management software—is in control of the system.
In this mode, the VMM has more privileges than user processes but fewer than the kernel.
It needs that level of privilege so it can create and manage virtual machines, changing the CPU state to do so.
We should note that, as an alternative to modes, the CPU designer may use other methods to differentiate operational privileges.
The Intel 64 family of CPUs supports four privilege levels, for example, and supports virtualization but does not have a separate mode for virtualization.
We can now see the life cycle of instruction execution in a computer system.
Initial control resides in the operating system, where instructions are executed in kernel mode.
When control is given to a user application, the mode is set to user mode.
Eventually, control is switched back to the operating system via an interrupt, a trap, or a system call.
System calls provide the means for a user program to ask the operating system to perform tasks reserved for the operating system on the user program’s behalf.
A system call is invoked in a variety of ways, depending on the functionality provided by the underlying processor.
In all forms, it is the method used by a process to request action by the operating system.
A system call usually takes the form of a trap to a speciﬁc location in the interrupt vector.
This trap can be executed by a generic trap instruction, although some systems (such as MIPS) have a speciﬁc syscall instruction to invoke a system call.
When a system call is executed, it is typically treated by the hardware as a software interrupt.
Control passes through the interrupt vector to a service routine in the operating system, and the mode bit is set to kernel mode.
The system-call service routine is a part of the operating system.
The kernel examines the interrupting instruction to determine what system call has occurred; a parameter indicates what type of service the user program is requesting.
Additional information needed for the request may be passed in registers, on the stack, or in memory (with pointers to the memory locations passed in registers)
The kernel veriﬁes that the parameters are correct and legal, executes the request, and returns control to the instruction following the system call.
The lack of a hardware-supported dual mode can cause serious shortcomings in an operating system.
For instance, MS-DOS was written for the Intel 8088 architecture, which has no mode bit and therefore no dual mode.
A user program running awry can wipe out the operating system by writing over it with data; and multiple programs are able to write to a device at the same time, with potentially disastrous results.
Modern versions of the Intel CPU do provide dual-mode operation.
Accordingly, most contemporary operating systems—such as Microsoft Windows 7, as well as Unix and Linux—take advantage of this dual-mode feature and provide greater protection for the operating system.
Once hardware protection is in place, it detects errors that violate modes.
If a user program fails in some way—such as by making an attempt either to execute an illegal instruction or to access memory that is not in the user’s address space—then the hardware traps to the operating system.
The trap transfers control through the interrupt vector to the operating system, just as an interrupt does.
When a program error occurs, the operating system must terminate the program abnormally.
This situation is handled by the same code as a user-requested abnormal termination.
An appropriate error message is given, and the memory of the program may be dumped.
The memory dump is usually written to a ﬁle so that the user or programmer can examine it and perhaps correct it and restart the program.
We must ensure that the operating system maintains control over the CPU.
We cannot allow a user program to get stuck in an inﬁnite loop or to fail to call system services and never return control to the operating system.
A timer can be set to interrupt the computer after a speciﬁed period.
A variable timer is generally implemented by a ﬁxed-rate clock and a counter.
Before turning over control to the user, the operating system ensures that the timer is set to interrupt.
If the timer interrupts, control transfers automatically to the operating system, which may treat the interrupt as a fatal error or may give the program more time.
Clearly, instructions that modify the content of the timer are privileged.
We can use the timer to prevent a user program from running too long.
A simple technique is to initialize a counter with the amount of time that a program is allowed to run.
As long as the counter is positive, control is returned to the user program.
When the counter becomes negative, the operating system terminates the program for exceeding the assigned time limit.
A program does nothing unless its instructions are executed by a CPU.
A time-shared user program such as a compiler is a process.
A word-processing program being run by an individual user on a PC is a process.
A system task, such as sending output to a printer, can also be a process (or at least part of one)
For now, you can consider a process to be a job or a time-shared program, but later you will learn that the concept is more general.
As we shall see in Chapter 3, it is possible to provide system calls that allow processes to create subprocesses to execute concurrently.
These resources are either given to the process when it is created or allocated to it while it is running.
In addition to the various physical and logical resources that a process obtains when it is created, various initialization data (input) may be passed along.
For example, consider a process whose function is to display the status of a ﬁle on the screen of a terminal.
The process will be given the name of the ﬁle as an input and will execute the appropriate instructions and system calls to obtain and display the desired information on the terminal.
When the process terminates, the operating system will reclaim any reusable resources.
We emphasize that a program by itself is not a process.
A program is a passive entity, like the contents of a ﬁle stored on disk, whereas a process.
A single-threaded process has one program counter specifying the next instruction to execute.
The CPU executes one instruction of the process after another, until the process completes.
Further, at any time, one instruction at most is executed on behalf of the process.
Thus, although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences.
A multithreaded process has multiple program counters, each pointing to the next instruction to execute for a given thread.
A process is the unit of work in a system.
A system consists of a collection of processes, some of which are operating-system processes (those that execute system code) and the rest of which are user processes (those that execute user code)
All these processes can potentially execute concurrently—by multiplexing on a single CPU, for example.
The operating system is responsible for the following activities in connection with process management:
As we discussed in Section 1.2.2, the main memory is central to the operation of a modern computer system.
Main memory is a large array of bytes, ranging in size from hundreds of thousands to billions.
Main memory is a repository of quickly accessible data shared by the CPU and I/O devices.
The central processor reads instructions from main memory during the instruction-fetch cycle and both reads and writes data from main memory during the data-fetch cycle (on a von Neumann architecture)
As noted earlier, the main memory is generally the only large storage device that the CPU is able to address and access directly.
For example, for the CPU to process data from disk, those data must ﬁrst be transferred to main memory by CPU-generated I/O calls.
In the same way, instructions must be in memory for the CPU to execute them.
For a program to be executed, it must be mapped to absolute addresses and loaded into memory.
As the program executes, it accesses program instructions and data from memory by generating these absolute addresses.
Eventually, the program terminates, its memory space is declared available, and the next program can be loaded and executed.
To improve both the utilization of the CPU and the speed of the computer’s response to its users, general-purpose computers must keep several programs in memory, creating a need for memory management.
These schemes reﬂect various approaches, and the effectiveness of any given algorithm depends on the situation.
In selecting a memory-management scheme for a speciﬁc system, we must take into account many factors—especially the hardware design of the system.
The operating system is responsible for the following activities in connection with memory management:
Keeping track of which parts of memory are currently being used and who is using them.
Deciding which processes (or parts of processes) and data to move into and out of memory.
To make the computer system convenient for users, the operating system provides a uniform, logical view of information storage.
The operating system abstracts from the physical properties of its storage devices to deﬁne a logical storage unit, the ﬁle.
The operating system maps ﬁles onto physical media and accesses these ﬁles via the storage devices.
File management is one of the most visible components of an operating system.
Computers can store information on several different types of physical media.
Magnetic disk, optical disk, and magnetic tape are the most common.
Each of these media has its own characteristics and physical organization.
Each medium is controlled by a device, such as a disk drive or tape drive, that also has its own unique characteristics.
These properties include access speed, capacity, data-transfer rate, and access method (sequential or random)
A ﬁle is a collection of related information deﬁned by its creator.
Commonly, ﬁles represent programs (both source and object forms) and data.
Files may be free-form (for example, text ﬁles), or they may be formatted rigidly (for example, ﬁxed ﬁelds)
Clearly, the concept of a ﬁle is an extremely general one.
The operating system implements the abstract concept of a ﬁle by managing mass-storage media, such as tapes and disks, and the devices that control them.
In addition, ﬁles are normally organized into directories to make them easier to use.
Finally, when multiple users have access to ﬁles, it may be desirable to control which user may access a ﬁle and how that user may access it (for example, read, write, append)
The operating system is responsible for the following activities in connection with ﬁle management:
As we have already seen, because main memory is too small to accommodate all data and programs, and because the data that it holds are lost when power is lost, the computer system must provide secondary storage to back up main memory.
Most modern computer systems use disks as the principal on-line storage medium for both programs and data.
Most programs—including compilers, assemblers, word processors, editors, and formatters—are stored on a disk until loaded into memory.
They then use the disk as both the source and destination of their processing.
Hence, the proper management of disk storage is of central importance to a computer system.
The operating system is responsible for the following activities in connection with disk management:
Because secondary storage is used frequently, it must be used efﬁciently.
The entire speed of operation of a computer may hinge on the speeds of the disk subsystem and the algorithms that manipulate that subsystem.
There are, however, many uses for storage that is slower and lower in cost (and sometimes of higher capacity) than secondary storage.
Backups of disk data, storage of seldom-used data, and long-term archival storage are some examples.
Magnetic tape drives and their tapes and CD and DVD drives and platters are typical tertiary storage devices.
The media (tapes and optical platters) vary between WORM (write-once, read-many-times) and RW (readwrite) formats.
Tertiary storage is not crucial to system performance, but it still must be managed.
Some operating systems take on this task, while others leave tertiary-storage management to application programs.
Some of the functions that operating systems can provide include mounting and unmounting media in devices, allocating and freeing the devices for exclusive use by processes, and migrating data from secondary to tertiary storage.
Information is normally kept in some storage system (such as main memory)
As it is used, it is copied into a faster storage system—the cache—on a.
When we need a particular piece of information, we ﬁrst check whether it is in the cache.
If it is, we use the information directly from the cache.
If it is not, we use the information from the source, putting a copy in the cache under the assumption that we will need it again soon.
In addition, internal programmable registers, such as index registers, provide a high-speed cache for main memory.
For instance, most systems have an instruction cache to hold the instructions expected to be executed next.
Without this cache, the CPU would have to wait several cycles while an instruction was fetched from main memory.
For similar reasons, most systems have one or more high-speed data caches in the memory hierarchy.
We are not concerned with these hardware-only caches in this text, since they are outside the control of the operating system.
Because caches have limited size, cache management is an important design problem.
Careful selection of the cache size and of a replacement policy can result in greatly increased performance.
Figure 1.11 compares storage performance in large workstations and small servers.
Main memory can be viewed as a fast cache for secondary storage, since data in secondary storage must be copied into main memory for use and data must be in main memory before being moved to secondary storage for safekeeping.
The ﬁle-system data, which resides permanently on secondary storage, may appear on several levels in the storage hierarchy.
At the highest level, the operating system may maintain a cache of ﬁle-system data in main memory.
In addition, solid-state disks may be used for high-speed storage that is accessed through the ﬁle-system interface.
The magnetic-disk storage, in turn, is often backed up onto magnetic tapes or removable disks to protect against data loss in case of a hard-disk failure.
Some systems automatically archive old ﬁle data from secondary storage to tertiary storage, such as tape jukeboxes, to lower the storage cost (see Chapter 10)
Figure 1.12 Migration of integer A from disk to register.
The movement of information between levels of a storage hierarchy may be either explicit or implicit, depending on the hardware design and the controlling operating-system software.
For instance, data transfer from cache to CPU and registers is usually a hardware function, with no operating-system intervention.
In contrast, transfer of data from disk to memory is usually controlled by the operating system.
In a hierarchical storage structure, the same data may appear in different levels of the storage system.
For example, suppose that an integer A that is to be incremented by 1 is located in ﬁle B, and ﬁle B resides on magnetic disk.
The increment operation proceeds by ﬁrst issuing an I/O operation to copy the disk block on which A resides to main memory.
This operation is followed by copying A to the cache and to an internal register.
Thus, the copy of A appears in several places: on the magnetic disk, in main memory, in the cache, and in an internal register (see Figure 1.12)
Once the increment takes place in the internal register, the value of A differs in the various storage systems.
The value of A becomes the same only after the new value of A is written from the internal register back to the magnetic disk.
In a computing environment where only one process executes at a time, this arrangement poses no difﬁculties, since an access to integer A will always be to the copy at the highest level of the hierarchy.
However, in a multitasking environment, where the CPU is switched back and forth among various processes, extreme care must be taken to ensure that, if several processes wish to access A, then each of these processes will obtain the most recently updated value of A.
The situation becomes more complicated in a multiprocessor environment where, in addition to maintaining internal registers, each of the CPUs also contains a local cache (Figure 1.6)
In such an environment, a copy of A may exist simultaneously in several caches.
Since the various CPUs can all execute in parallel, we must make sure that an update to the value of A in one cache is immediately reﬂected in all other caches where A resides.
This situation is called cache coherency, and it is usually a hardware issue (handled below the operating-system level)
In a distributed environment, the situation becomes even more complex.
In this environment, several copies (or replicas) of the same ﬁle can be kept on different computers.
Since the various replicas may be accessed and updated concurrently, some distributed systems ensure that, when a replica is updated in one place, all other replicas are brought up to date as soon as possible.
One of the purposes of an operating system is to hide the peculiarities of speciﬁc hardware devices from the user.
Only the device driver knows the peculiarities of the speciﬁc device to which it is assigned.
We discussed in Section 1.2.3 how interrupt handlers and device drivers are used in the construction of efﬁcient I/O subsystems.
In Chapter 13, we discuss how the I/O subsystem interfaces to the other system components, manages devices, transfers data, and detects I/O completion.
If a computer system has multiple users and allows the concurrent execution of multiple processes, then access to data must be regulated.
For that purpose, mechanisms ensure that ﬁles, memory segments, CPU, and other resources can be operated on by only those processes that have gained proper authorization from the operating system.
For example, memory-addressing hardware ensures that a process can execute only within its own address space.
The timer ensures that no process can gain control of the CPU without eventually relinquishing control.
Device-control registers are not accessible to users, so the integrity of the various peripheral devices is protected.
Protection, then, is any mechanism for controlling the access of processes or users to the resources deﬁned by a computer system.
This mechanism must provide means to specify the controls to be imposed and to enforce the controls.
Protection can improve reliability by detecting latent errors at the interfaces between component subsystems.
Early detection of interface errors can often prevent contamination of a healthy subsystem by another subsystem that is malfunctioning.
Furthermore, an unprotected resource cannot defend against use (or misuse) by an unauthorized or incompetent user.
A system can have adequate protection but still be prone to failure and allow inappropriate access.
Consider a user whose authentication information (her means of identifying herself to the system) is stolen.
Her data could be copied or deleted, even though ﬁle and memory protection are working.
It is the job of security to defend a system from external and internal attacks.
Such attacks spread across a huge range and include viruses and worms, denial-ofservice attacks (which use all of a system’s resources and so keep legitimate users out of the system), identity theft, and theft of service (unauthorized use of a system)
Prevention of some of these attacks is considered an operating-system function on some systems, while other systems leave it to policy or additional software.
Protection and security require the system to be able to distinguish among all its users.
Most operating systems maintain a list of user names and associated user identiﬁers (user IDs)
When a user logs in to the system, the authentication stage determines the appropriate user ID for the user.
That user ID is associated with all of the user’s processes and threads.
When an ID needs to be readable by a user, it is translated back to the user name via the user name list.
In some circumstances, we wish to distinguish among sets of users rather than individual users.
For example, the owner of a ﬁle on a UNIX system may be allowed to issue all operations on that ﬁle, whereas a selected set of users may be allowed only to read the ﬁle.
To accomplish this, we need to deﬁne a group name and the set of users belonging to that group.
Group functionality can be implemented as a system-wide list of group names and group identiﬁers.
A user can be in one or more groups, depending on operating-system design decisions.
The user’s group IDs are also included in every associated process and thread.
In the course of normal system use, the user ID and group ID for a user are sufﬁcient.
However, a user sometimes needs to escalate privileges to gain extra permissions for an activity.
The user may need access to a device that is restricted, for example.
On UNIX, for instance, the setuid attribute on a program causes that program to run with the user ID of the owner of the ﬁle, rather than the current user’s ID.
The process runs with this effective UID until it turns off the extra privileges or terminates.
We turn next to a topic central to operating-system implementation: the way data are structured in the system.
In this section, we brieﬂy describe several fundamental data structures used extensively in operating systems.
Readers who require further details on these structures, as well as others, should consult the bibliography at the end of the chapter.
After arrays, lists are perhaps the most fundamental data structures in computer science.
Whereas each item in an array can be accessed directly, the items in a list must be accessed in a particular order.
That is, a list represents a collection of data values as a sequence.
In a singly linked list, each item points to its successor, as illustrated in Figure 1.13
In a doubly linked list, a given item can refer either to its predecessor or to its successor, as illustrated in Figure 1.14
In a circularly linked list, the last element in the list refers to the ﬁrst element, rather than to null, as illustrated in Figure 1.15
Linked lists accommodate items of varying sizes and allow easy insertion and deletion of items.
One potential disadvantage of using a list is that performance for retrieving a speciﬁed item in a list of size n is linear — O(n), as it requires potentially traversing all n elements in the worst case.
Frequently, though, they are used for constructing more powerful data structures, such as stacks and queues.
A stack is a sequentially ordered data structure that uses the last in, ﬁrst out (LIFO) principle for adding and removing items, meaning that the last item placed onto a stack is the ﬁrst item removed.
The operations for inserting and removing items from a stack are known as push and pop, respectively.
An operating system often uses a stack when invoking function calls.
Parameters, local variables, and the return address are pushed onto the stack when a function is called; returning from the function call pops those items off the stack.
A queue, in contrast, is a sequentially ordered data structure that uses the ﬁrst in, ﬁrst out (FIFO) principle: items are removed from a queue in the order in which they were inserted.
There are many everyday examples of queues, including shoppers waiting in a checkout line at a store and cars waiting in line at a trafﬁc signal.
Queues are also quite common in operating systems—jobs that are sent to a printer are typically printed in the order in which they were submitted, for example.
As we shall see in Chapter 6, tasks that are waiting to be run on an available CPU are often organized in queues.
A tree is a data structure that can be used to represent data hierarchically.
Data values in a tree structure are linked through parent–child relationships.
In a general tree, a parent may have an unlimited number of children.
In a binary tree, a parent may have at most two children, which we term the left child and the right child.
A binary search tree additionally requires an ordering between the parent’s two children in which le f t child <= right child.
Figure 1.16 provides an example of a binary search tree.
When we search for an item in a binary search tree, the worst-case performance is O(n) (consider how this can occur)
To remedy this situation, we can use an algorithm to create a balanced binary search tree.
Here, a tree containing n items has at most lg n levels, thus ensuring worst-case performance of O(lg n)
We shall see in Section 6.7.1 that Linux uses a balanced binary search tree as part its CPU-scheduling algorithm.
A hash function takes data as its input, performs a numeric operation on this data, and returns a numeric value.
This numeric value can then be used as an index into a table (typically an array) to quickly retrieve the data.
Whereas searching for a data item through a list of size n can require up to O(n) comparisons in the worst case, using a hash function for retrieving data from table can be as good as O(1) in the worst case, depending on implementation details.
Because of this performance, hash functions are used extensively in operating systems.
One potential difﬁculty with hash functions is that two inputs can result in the same output value—that is, they can link to the same table location.
We can accommodate this hash collision by having a linked list at that table location that contains all of the items with the same hash value.
Of course, the more collisions there are, the less efﬁcient the hash function is.
One use of a hash function is to implement a hash map, which associates (or maps) [key:value] pairs using a hash function.
For example, we can map the key operating to the value system.
Once the mapping is established, we can apply the hash function to the key to obtain the value from the hash map (Figure 1.17)
For example, suppose that a user name is mapped to a password.
Password authentication then proceeds as follows: a user enters his user name and password.
The hash function is applied to the user name, which is then used to retrieve the password.
The retrieved password is then compared with the password entered by the user for authentication.
A bitmap is a string of n binary digits that can be used to represent the status of n items.
The value of the i th position in the bitmap is associated with the i th resource.
The power of bitmaps becomes apparent when we consider their space.
If we were to use an eight-bit Boolean value instead of a single bit, the resulting data structure would be eight times larger.
Thus, bitmaps are commonly used when there is a need to represent the availability of a large number of resources.
A medium-sized disk drive might be divided into several thousand individual units, called disk blocks.
A bitmap can be used to indicate the availability of each disk block.
Thus, we will see the structures discussed here, along with others, throughout this text as we explore kernel algorithms and their implementations.
The data structures used in the Linux kernel are available in the kernel source code.
The include ﬁle <linux/list.h> provides details of the linked-list data structure used throughout the kernel.
A queue in Linux is known as a kfifo, and its implementation can be found in the kfifo.c ﬁle in the kernel directory of the source code.
Linux also provides a balanced binary search tree implementation using red-black trees.
So far, we have brieﬂy described several aspects of computer systems and the operating systems that manage them.
We turn now to a discussion of how operating systems are used in a variety of computing environments.
As computing has matured, the lines separating many of the traditional computing environments have blurred.
Consider the “typical ofﬁce environment.” Just a few years ago, this environment consisted of PCs connected to a network, with servers providing ﬁle and print services.
Remote access was awkward, and portability was achieved by use of laptop computers.
Terminals attached to mainframes were prevalent at many companies as well, with even fewer remote access and portability options.
The current trend is toward providing more ways to access these computing environments.
Web technologies and increasing WAN bandwidth are stretching the boundaries of traditional computing.
Companies establish portals, which provide Web accessibility to their internal servers.
Network computers (or thin clients)—which are essentially terminals that understand web-based computing—are used in place of traditional workstations where more security or easier maintenance is desired.
Mobile computers can synchronize with PCs to allow very portable use of company information.
Mobile computers can also connect to wireless networks and cellular data networks to use the company’s Web portal (as well as the myriad other Web resources)
At home, most users once had a single computer with a slow modem connection to the ofﬁce, the Internet, or both.
Today, network-connection speeds once available only at great cost are relatively inexpensive in many places, giving home users more access to more data.
These fast data connections are allowing home computers to serve up Web pages and to run networks that include printers, client PCs, and servers.
Many homes use ﬁrewalls to protect their networks from security breaches.
In the latter half of the 20th century, computing resources were relatively scarce.
For a period of time, systems were either batch or interactive.
Batch systems processed jobs in bulk, with predetermined input from ﬁles or other data sources.
To optimize the use of the computing resources, multiple users shared time on these systems.
The same scheduling technique is still in use on desktop computers, laptops, servers, and even mobile computers, but frequently all the processes are owned by the same user (or a single user and the operating system)
User processes, and system processes that provide services to the user, are managed so that each frequently gets a slice of computer time.
Consider the windows created while a user is working on a PC, for example, and the fact that they may be performing different tasks at the same time.
Even a web browser can be composed of multiple processes, one for each website currently being visited, with time sharing applied to each web browser process.
Mobile computing refers to computing on handheld smartphones and tablet computers.
These devices share the distinguishing physical features of being portable and lightweight.
Historically, compared with desktop and laptop computers, mobile systems gave up screen size, memory capacity, and overall functionality in return for handheld mobile access to services such as e-mail and web browsing.
Over the past few years, however, features on mobile devices have become so rich that the distinction in functionality between, say, a consumer laptop and a tablet computer may be difﬁcult to discern.
In fact, we might argue that the features of a contemporary mobile device allow it to provide functionality that is either unavailable or impractical on a desktop or laptop computer.
Today, mobile systems are used not only for e-mail and web browsing but also for playing music and video, reading digital books, taking photos, and recording high-deﬁnition video.
Accordingly, tremendous growth continues in the wide range of applications that run on such devices.
Many developers are now designing applications that take advantage of the unique features of mobile devices, such as global positioning system (GPS) chips, accelerometers, and gyroscopes.
An embedded GPS chip allows a mobile device to use satellites to determine its precise location on earth.
That functionality is especially useful in designing applications that provide navigation—for example, telling users which way to walk or drive or perhaps directing them to nearby services, such as restaurants.
An accelerometer allows a mobile device to detect its orientation with respect to the ground and to detect certain other forces, such as tilting and shaking.
In several computer games that employ accelerometers, players interface with the system not by using a mouse or a keyboard but rather by tilting, rotating, and shaking the mobile device! Perhaps more a practical use of these features is found in augmented-reality applications, which overlay information on a display of the current environment.
It is difﬁcult to imagine how equivalent applications could be developed on traditional laptop or desktop computer systems.
To provide access to on-line services, mobile devices typically use either IEEE standard 802.11 wireless or cellular data networks.
The memory capacity and processing speed of mobile devices, however, are more limited than those of PCs.
Two operating systems currently dominate mobile computing: Apple iOS and Google Android.
Android powers smartphones and tablet computers available from many manufacturers.
A distributed system is a collection of physically separate, possibly heterogeneous, computer systems that are networked to provide users with access to the various resources that the system maintains.
Access to a shared resource increases computation speed, functionality, data availability, and reliability.
Some operating systems generalize network access as a form of ﬁle access, with the details of networking contained in the network interface’s device driver.
Generally, systems contain a mix of the two modes—for example FTP and NFS.
The protocols that create a distributed system can greatly affect that system’s utility and popularity.
A network, in the simplest terms, is a communication path between two or more systems.
Networks vary by the protocols used, the distances between nodes, and the transport media.
TCP/IP is the most common network protocol, and it provides the fundamental architecture of the Internet.
To an operating system, a network protocol simply needs an interface device—a network adapter, for example—with a device driver to manage it, as well as software to handle data.
Networks are characterized based on the distances between their nodes.
A local-area network (LAN) connects computers within a room, a building, or a campus.
A wide-area network (WAN) usually links buildings, cities, or countries.
A global company may have a WAN to connect its ofﬁces worldwide, for example.
The continuing advent of new technologies brings about new forms of networks.
For example, a metropolitan-area network (MAN) could link buildings within a city.
BlueTooth and 802.11 devices use wireless technology to communicate over a distance of several feet, in essence creating a personal-area network (PAN) between a phone and a headset or a smartphone and a desktop computer.
They include copper wires, ﬁber strands, and wireless transmissions between satellites, microwave dishes, and radios.
When computing devices are connected to cellular phones, they create a network.
Even very short-range infrared communication can be used for networking.
At a rudimentary level, whenever computers communicate, they use or create a network.
Some operating systems have taken the concept of networks and distributed systems further than the notion of providing network connectivity.
A network operating system is an operating system that provides features such as ﬁle sharing across the network, along with a communication scheme that allows different processes on different computers to exchange messages.
A computer running a network operating system acts autonomously from all other computers on the network, although it is aware of the network and is able to communicate with other networked computers.
The different computers communicate closely enough to provide the illusion that only a single operating system controls the network.
As PCs have become faster, more powerful, and cheaper, designers have shifted away from centralized system architecture.
Terminals connected to centralized systems are now being supplanted by PCs and mobile devices.
Correspondingly, user-interface functionality once handled directly by centralized systems is increasingly being handled by PCs, quite often through a web interface.
As a result, many of today’s systems act as server systems to satisfy requests generated by client systems.
This form of specialized distributed system, called a client–server system, has the general structure depicted in Figure 1.18
Server systems can be broadly categorized as compute servers and ﬁle servers:
The compute-server system provides an interface to which a client can send a request to perform an action (for example, read data)
In response, the server executes the action and sends the results to the client.
A server running a database that responds to client requests for data is an example of such a system.
The ﬁle-server system provides a ﬁle-system interface where clients can create, update, read, and delete ﬁles.
An example of such a system is a web server that delivers ﬁles to clients running web browsers.
Another structure for a distributed system is the peer-to-peer (P2P) system model.
In this model, clients and servers are not distinguished from one another.
Instead, all nodes within the system are considered peers, and each may act as either a client or a server, depending on whether it is requesting or providing a service.
In a client-server system, the server is a bottleneck; but in a peer-to-peer system, services can be provided by several nodes distributed throughout the network.
To participate in a peer-to-peer system, a node must ﬁrst join the network of peers.
Once a node has joined the network, it can begin providing services to—and requesting services from—other nodes in the network.
Determining what services are available is accomplished in one of two general ways:
When a node joins a network, it registers its service with a centralized lookup service on the network.
Any node desiring a speciﬁc service ﬁrst contacts this centralized lookup service to determine which node provides the service.
The remainder of the communication takes place between the client and the service provider.
Instead, a peer acting as a client must discover what node provides a desired service by broadcasting a request for the service to all other nodes in the network.
The node (or nodes) providing that service responds to the peer making the request.
To support this approach, a discovery protocol must be provided that allows peers to discover services provided by other peers in the network.
Peer-to-peer networks gained widespread popularity in the late 1990s with several ﬁle-sharing services, such as Napster and Gnutella, that enabled peers to exchange ﬁles with one another.
The Napster system used an approach similar to the ﬁrst type described above: a centralized server maintained an index of all ﬁles stored on peer nodes in the Napster network, and the actual.
The Gnutella system used a technique similar to the second type: a client broadcasted ﬁle requests to other nodes in the system, and nodes that could service the request responded directly to the client.
The future of exchanging ﬁles remains uncertain because peer-to-peer networks can be used to exchange copyrighted materials (music, for example) anonymously, and there are laws governing the distribution of copyrighted material.
It allows clients to make voice calls and video calls and to send text messages over the Internet using a technology known as voice over IP (VoIP)
It includes a centralized login server, but it also incorporates decentralized peers and allows two peers to communicate.
Virtualization is a technology that allows operating systems to run as applications within other operating systems.
At ﬁrst blush, there seems to be little reason for such functionality.
But the virtualization industry is vast and growing, which is a testament to its utility and importance.
Broadly speaking, virtualization is one member of a class of software that also includes emulation.
Emulation is used when the source CPU type is different from the target CPU type.
For example, when Apple switched from the IBM Power CPU to the Intel x86 CPU for its desktop and laptop computers, it included an emulation facility called “Rosetta,” which allowed applications compiled for the IBM CPU to run on the Intel CPU.
That same concept can be extended to allow an entire operating system written for one platform to run on another.
Every machine-level instruction that runs natively on the source system must be translated to the equivalent function on the target system, frequently resulting in several target instructions.
If the source and target CPUs have similar performance levels, the emulated code can run much slower than the native code.
A common example of emulation occurs when a computer language is not compiled to native code but instead is either executed in its high-level form or translated to an intermediate form.
Some languages, such as BASIC, can be either compiled or interpreted.
Interpretation is a form of emulation in that the high-level language code is translated to native CPU instructions, emulating not another CPU but a theoretical virtual machine on which that language could run natively.
Thus, we can run Java programs on “Java virtual machines,” but technically those virtual machines are Java emulators.
With virtualization, in contrast, an operating system that is natively compiled for a particular CPU architecture runs within another operating system also native to that CPU.
Virtualization ﬁrst came about on IBM mainframes as a method for multiple users to run tasks concurrently.
Running multiple virtual machines allowed (and still allows) many users to run tasks on a system designed for a single user.
Later, in response to problems with running multiple Microsoft Windows XP applications on the Intel x86 CPU, VMware created a new virtualization technology in the form of an application that ran on XP.
That application ran one or more guest copies of Windows or other native.
Windows was the host operating system, and the VMware application was the virtual machine manager VMM.
The VMM runs the guest operating systems, manages their resource use, and protects each guest from the others.
Even though modern operating systems are fully capable of running multiple applications reliably, the use of virtualization continues to grow.
On laptops and desktops, a VMM allows the user to install multiple operating systems for exploration or to run applications written for operating systems other than the native host.
For example, an Apple laptop running Mac OS X on the x86 CPU can run a Windows guest to allow execution of Windows applications.
Companies writing software for multiple operating systems can use virtualization to run all of those operating systems on a single physical server for development, testing, and debugging.
Within data centers, virtualization has become a common method of executing and managing computing environments.
VMMs like VMware, ESX, and Citrix XenServer no longer run on host operating systems but rather are the hosts.
Cloud computing is a type of computing that delivers computing, storage, and even applications as a service across a network.
In some ways, it’s a logical extension of virtualization, because it uses virtualization as a base for its functionality.
For example, the Amazon Elastic Compute Cloud (EC2) facility has thousands of servers, millions of virtual machines, and petabytes of storage available for use by anyone on the Internet.
Users pay per month based on how much of those resources they use.
There are actually many types of cloud computing, including the following:
Public cloud—a cloud available via the Internet to anyone willing to pay for the services.
Private cloud—a cloud run by a company for that company’s own use.
Hybrid cloud—a cloud that includes both public and private cloud.
Software as a service (SaaS)—one or more applications (such as word processors or spreadsheets) available via the Internet.
Platform as a service (PaaS)—a software stack ready for application use via the Internet (for example, a database server)
Infrastructure as a service (IaaS)—servers or storage available over the Internet (for example, storage available for making backup copies of production data)
These cloud-computing types are not discrete, as a cloud computing environment may provide a combination of several types.
For example, an organization may provide both SaaS and IaaS as a publicly available service.
Certainly, there are traditional operating systems within many of the types of cloud infrastructure.
Beyond those are the VMMs that manage the virtual machines in which the user processes run.
At a higher level, the VMMs themselves are managed by cloud management tools, such as Vware vCloud Director and the open-source Eucalyptus toolset.
These tools manage the resources within a given cloud and provide interfaces to the cloud components, making a good argument for considering them a new type of operating system.
Notice that both the cloud services and the cloud user interface are protected by a ﬁrewall.
Embedded computers are the most prevalent form of computers in existence.
These devices are found everywhere, from car engines and manufacturing robots to DVDs and microwave ovens.
The systems they run on are usually primitive, and so the operating systems provide limited features.
Usually, they have little or no user interface, preferring to spend their time monitoring and managing hardware devices, such as automobile engines and robotic arms.
Some are general-purpose computers, running standard operating systems—such as Linux—with special-purpose applications to implement the functionality.
Others are hardware devices with a special-purpose embedded operating system providing just the functionality desired.
Yet others are hardware devices with applicationspeciﬁc integrated circuits (ASICs) that perform their tasks without an operating system.
The power of these devices, both as standalone units and as elements of networks and the web, is sure to increase as well.
Even now, entire houses can be computerized, so that a central computer—either a general-purpose computer or an embedded system—can control heating and lighting, alarm systems, and even coffee makers.
Web access can enable a home owner to tell the house to heat up before she arrives home.
Someday, the refrigerator can notify the grocery store when it notices the milk is gone.
A real-time system is used when rigid time requirements have been placed on the operation of a processor or the ﬂow of data; thus, it is often used as a control device in a dedicated application.
The computer must analyze the data and possibly adjust controls to modify the sensor inputs.
Systems that control scientiﬁc experiments, medical imaging systems, industrial control systems, and certain display systems are realtime systems.
Some automobile-engine fuel-injection systems, home-appliance controllers, and weapon systems are also real-time systems.
Processing must be done within the deﬁned constraints, or the system will fail.
For instance, it would not do for a robot arm to be instructed to halt after it had smashed into the car it was building.
A real-time system functions correctly only if it returns the correct result within its time constraints.
Contrast this system with a time-sharing system, where it is desirable (but not mandatory) to respond quickly, or a batch system, which may have no time constraints at all.
In Chapter 6, we consider the scheduling facility needed to implement real-time functionality in an operating system.
In Chapter 9, we describe the design of memory management for real-time computing.
We noted at the beginning of this chapter that the study of operating systems has been made easier by the availability of a vast number of open-source.
Open-source operating systems are those available in source-code format rather than as compiled binary code.
Linux is the most famous opensource operating system, while Microsoft Windows is a well-known example of the opposite closed-source approach.
Apple’s Mac OS X and iOS operating systems comprise a hybrid approach.
They contain an open-source kernel named Darwin yet include proprietary, closed-source components as well.
Starting with the source code allows the programmer to produce binary code that can be executed on a system.
Doing the opposite—reverse engineering the source code from the binaries—is quite a lot of work, and useful items such as comments are never recovered.
Learning operating systems by examining the source code has other beneﬁts as well.
With the source code in hand, a student can modify the operating system and then compile and run the code to try out those changes, which is an excellent learning tool.
This text includes projects that involve modifying operating-system source code, while also describing algorithms at a high level to be sure all important operating-system topics are covered.
Throughout the text, we provide pointers to examples of open-source code for deeper study.
There are many beneﬁts to open-source operating systems, including a community of interested (and usually unpaid) programmers who contribute to the code by helping to debug it, analyze it, provide support, and suggest changes.
Arguably, open-source code is more secure than closed-source code because many more eyes are viewing the code.
Certainly, open-source code has bugs, but open-source advocates argue that bugs tend to be found and ﬁxed faster owing to the number of people using and viewing the code.
Companies that earn revenue from selling their programs often hesitate to open-source their code, but Red Hat and a myriad of other companies are doing just that and showing that commercial companies beneﬁt, rather than suffer, when they open-source their code.
Revenue can be generated through support contracts and the sale of hardware on which the software runs, for example.
In the early days of modern computing (that is, the 1950s), a great deal of software was available in open-source format.
The original hackers (computer enthusiasts) at MIT’s Tech Model Railroad Club left their programs in drawers for others to work on.
Later, company-speciﬁc user groups, such as Digital Equipment Corporation’s DEC, accepted contributions of source-code programs, collected them onto tapes, and distributed the tapes to interested members.
Computer and software companies eventually sought to limit the use of their software to authorized computers and paying customers.
Releasing only the binary ﬁles compiled from the source code, rather than the source code itself, helped them to achieve this goal, as well as protecting their code and their ideas from their competitors.
Operating systems and other programs can limit the ability to play back movies and music or display electronic books to authorized computers.
Such copy protection or digital rights management (DRM) would not be effective if the source code that implemented these limits were published.
Digital Millennium Copyright Act (DMCA), make it illegal to reverse-engineer DRM code or otherwise try to circumvent copy protection.
To counter the move to limit software use and redistribution, Richard Stallman in 1983 started the GNU project to create a free, open-source, UNIXcompatible operating system.
In 1985, he published the GNU Manifesto, which argues that all software should be free and open-sourced.
He also formed the Free Software Foundation (FSF) with the goal of encouraging the free exchange of software source code and the free use of that software.
Rather than copyright its software, the FSF “copylefts” the software to encourage sharing and improvement.
The GNU General Public License (GPL) codiﬁes copylefting and is a common license under which free software is released.
Fundamentally, GPL requires that the source code be distributed with any binaries and that any changes made to the source code be released under the same GPL license.
As an example of an open-source operating system, consider GNU/Linux.
The GNU project produced many UNIX-compatible tools, including compilers, editors, and utilities, but never released a kernel.
In 1991, a student in Finland, Linus Torvalds, released a rudimentary UNIX-like kernel using the GNU compilers and tools and invited contributions worldwide.
The advent of the Internet meant that anyone interested could download the source code, modify it, and submit changes to Torvalds.
Releasing updates once a week allowed this so-called Linux operating system to grow rapidly, enhanced by several thousand programmers.
The resulting GNU/Linux operating system has spawned hundreds of unique distributions, or custom builds, of the system.
Distributions vary in function, utility, installed applications, hardware support, user interface, and purpose.
For example, RedHat Enterprise Linux is geared to large commercial use.
PCLinuxOS is a LiveCD—an operating system that can be booted and run from a CD-ROM without being installed on a system’s hard disk.
One variant of PCLinuxOS—called “PCLinuxOS Supergamer DVD”—is a LiveDVD that includes graphics drivers and games.
A gamer can run it on any compatible system simply by booting from the DVD.
When the gamer is ﬁnished, a reboot of the system resets it to its installed operating system.
You can run Linux on a Windows system using the following simple, free approach:
Choose a Linux version from among the hundreds of “appliances,” or virtual machine images, available from VMware at.
These images are preinstalled with operating systems and applications and include many ﬂavors of Linux.
With this text, we provide a virtual machine image of Linux running the Debian release.
This image contains the Linux source code as well as tools for software development.
Releases from the University of California at Berkeley (UCB) came in source and binary form, but they were not opensource because a license from AT&T was required.
To explore the source code of FreeBSD, simply download the virtual machine image of the version of interest and boot it within VMware, as described above for Linux.
The source code comes with the distribution and is stored in /usr/src/
For example, to examine the virtual memory implementation code in the FreeBSD kernel, see the ﬁles in /usr/src/sys/vm.
Darwin, the core kernel component of Mac OS X, is based on BSD UNIX and is open-sourced as well.
Every Mac OS X release has its opensource components posted at that site.
The name of the package that contains the kernel begins with “xnu.” Apple also provides extensive developer tools, documentation, and support at http://connect.apple.com.
Solaris is the commercial UNIX-based operating system of Sun Microsystems.
In 2005, Sun open-sourced most of the Solaris code as the OpenSolaris project.
The purchase of Sun by Oracle in 2009, however, left the state of this project unclear.
The source code as it was in 2005 is still available via a source code browser and for download at http://src.opensolaris.org/source.
Several groups interested in using OpenSolaris have started from that base and expanded its features.
Their working set is Project Illumos, which has expanded from the OpenSolaris base to include more features and to be the basis for several products.
The free software movement is driving legions of programmers to create thousands of open-source projects, including operating systems.
Sites like http://freshmeat.net/ and http://distrowatch.com/ provide portals to many of these projects.
As we stated earlier, open-source projects enable students to use source code as a learning tool.
The availability of source code for historic projects, such as Multics, can help students to understand those projects and to build knowledge that will help in the implementation of new projects.
GNU/Linux and BSD UNIX are all open-source operating systems, but each has its own goals, utility, licensing, and purpose.
Sometimes, licenses are not mutually exclusive and cross-pollination occurs, allowing rapid improvements in operating-system projects.
For example, several major components of OpenSolaris have been ported to BSD UNIX.
The advantages of free software and open sourcing are likely to increase the number and quality of open-source projects, leading to an increase in the number of individuals and companies that use these projects.
An operating system is software that manages the computer hardware, as well as providing an environment for application programs to run.
Perhaps the most visible aspect of an operating system is the interface to the computer system it provides to the human user.
For a computer to do its job of executing programs, the programs must be in main memory.
Main memory is the only large storage area that the processor can access directly.
It is an array of bytes, ranging in size from millions to billions.
The main memory is usually a volatile storage device that loses its contents when power is turned off or lost.
Most computer systems provide secondary storage as an extension of main memory.
Secondary storage provides a form of nonvolatile storage that is capable of holding large quantities of data permanently.
The most common secondary-storage device is a magnetic disk, which provides storage of both programs and data.
The wide variety of storage systems in a computer system can be organized in a hierarchy according to speed and cost.
As we move down the hierarchy, the cost per bit generally decreases, whereas the access time generally increases.
There are several different strategies for designing a computer system.
Single-processor systems have only one processor, while multiprocessor systems contain two or more processors that share physical memory and peripheral devices.
The most common multiprocessor design is symmetric multiprocessing (or SMP), where all processors are considered peers and run independently of one another.
Clustered systems are a specialized form of multiprocessor systems and consist of multiple computer systems connected by a local-area network.
To best utilize the CPU, modern operating systems employ multiprogramming, which allows several jobs to be in memory at the same time, thus ensuring that the CPU always has a job to execute.
Time-sharing systems are an extension of multiprogramming wherein CPU scheduling algorithms rapidly switch between jobs, thus providing the illusion that each job is running concurrently.
The operating system must ensure correct operation of the computer system.
To prevent user programs from interfering with the proper operation of.
There has never been a more interesting time to study operating systems, and it has never been easier.
The open-source movement has overtaken operating systems, causing many of them to be made available in both source and binary (executable) format.
The list of operating systems available in both formats includes Linux, BSD UNIX, Solaris, and part of Mac OS X.
The availability of source code allows us to study operating systems from the inside out.
Questions that we could once answer only by looking at documentation or the behavior of an operating system we can now answer by examining the code itself.
Operating systems that are no longer commercially viable have been open-sourced as well, enabling us to study how systems operated in a time of fewer CPU, memory, and storage resources.
An extensive but incomplete list of open-source operating-system projects is available from http://dmoz.org/Computers/Software/Operating Systems/Open Source/
In addition, the rise of virtualization as a mainstream (and frequently free) computer function makes it possible to run many operating systems on top of one core system.
For example, VMware (http://www.vmware.com) provides a free “player” for Windows on which hundreds of free “virtual appliances” can run.
Virtualbox ( http://www.virtualbox.com) provides a free, opensource virtual machine manager on many operating systems.
Using such tools, students can try out hundreds of operating systems without dedicated hardware.
In some cases, simulators of speciﬁc hardware are also available, allowing the operating system to run on “native” hardware, all within the conﬁnes of a modern computer and modern operating system.
An interested student can search the Internet to ﬁnd the original papers that describe the operating system, as well as the original manuals.
The advent of open-source operating systems has also made it easier to make the move from student to operating-system developer.
With some knowledge, some effort, and an Internet connection, a student can even create a new operating-system distribution.
Just a few years ago, it was difﬁcult or impossible to get access to source code.
Now, such access is limited only by how much interest, time, and disk space a student has.
Various instructions (such as I/O instructions and halt instructions) are privileged and can be executed only in kernel mode.
The memory in which the operating system resides must also be protected from modiﬁcation by the user.
These facilities (dual mode, privileged instructions, memory protection, and timer interrupt) are basic building blocks used by operating systems to achieve correct operation.
A process (or job) is the fundamental unit of work in an operating system.
Process management includes creating and deleting processes and providing mechanisms for processes to communicate and synchronize with each other.
An operating system manages memory by keeping track of what parts of memory are being used and by whom.
The operating system is also responsible for dynamically allocating and freeing memory space.
Storage space is also managed by the operating system; this includes providing ﬁle systems for representing ﬁles and directories and managing space on mass-storage devices.
Operating systems must also be concerned with protecting and securing the operating system and users.
Protection measures control the access of processes or users to the resources made available by the computer system.
Security measures are responsible for defending a computer system from external or internal attacks.
Several data structures that are fundamental to computer science are widely used in operating systems, including lists, stacks, queues, trees, hash functions, maps, and bitmaps.
Traditional computing involves desktop and laptop PCs, usually connected to a computer network.
Mobile computing refers to computing on handheld smartphones and tablet computers, which offer several unique features.
Distributed systems allow users to share resources on geographically dispersed hosts connected via a computer network.
Services may be provided through either the clientserver model or the peer-to-peer model.
Virtualization involves abstracting a computer’s hardware into several different execution environments.
Cloud computing uses a distributed system to abstract services into a “cloud,” where users may access the services from remote locations.
Real-time operating systems are designed for embedded environments, such as consumer devices, automobiles, and robotics.
The free software movement has created thousands of open-source projects, including operating systems.
Because of these projects, students are able to use source code as a learning tool.
They can modify programs and test them, help ﬁnd and ﬁx bugs, and otherwise explore mature, full-featured operating systems, compilers, tools, user interfaces, and other types of programs.
The advantages of free software and open sourcing are likely to increase the number and quality of open-source projects, leading to an increase in the number of individuals and companies that use these projects.
When is it appropriate for the operating system to forsake this principle and to “waste” resources? Why is such a system not really wasteful?
Argue both that it should and that it should not, and support your answers.
Describe two difﬁculties that you think could arise with such a scheme.
Provide a short description of how this could be accomplished.
What problems do they solve? What problems do they cause? If a cache can be made as large as the device for which it is caching (for instance, a cache as large as a disk), why not make it that large and eliminate the device?
Can we ensure the same degree of security in a time-shared machine as in a dedicated machine? Explain your answer.
List what resources must be managed carefully in the following settings:
What are three advantages and one disadvantage of multiprocessor systems?
Describe two ways in which the cluster software can manage access to the data on the disk.
How does the CPU interface with the device to coordinate the transfer?
How does the CPU know when the memory operations are complete?
The CPU is allowed to execute other programs while the DMA controller is transferring data.
Does this process interfere with the execution of the user programs? If so, describe what forms of interference are caused.
Is it possible to construct a secure operating system for these computer systems? Give arguments both that it is and that it is not possible.
Illustrate with an example how data residing in memory could in fact have a different value in each of the local caches.
Include the types of people who would ﬁnd each aspect to be an advantage or a disadvantage.
The history of open sourcing and its beneﬁts and challenges appears in [Raymond (1999)]
The Free Software Foundation has published its philosophy in http://www.gnu.org/philosophy/free-software-for-freedom.html.
The open source of Mac OS X are available from http://www.apple.com/opensource/
Wikipedia has an informative entry about the contributions of Richard Stallman at http://en.wikipedia.org/wiki/Richard Stallman.
The source code of Multics is available athttp://web.mit.edu/multics-history /source/Multics Internet Server/Multics sources.html.
An operating system provides the environment within which programs are executed.
Internally, operating systems vary greatly in their makeup, since they are organized along many different lines.
The design of a new operating system is a major task.
It is important that the goals of the system be well deﬁned before the design begins.
These goals form the basis for choices among various algorithms and strategies.
We can view an operating system from several vantage points.
One view focuses on the services that the system provides; another, on the interface that it makes available to users and programmers; a third, on its components and their interconnections.
In this chapter, we explore all three aspects of operating systems, showing the viewpoints of users, programmers, and operating system designers.
We consider what services an operating system provides, how they are provided, how they are debugged, and what the various methodologies are for designing such systems.
Finally, we describe how operating systems are created and how a computer starts its operating system.
To describe the services an operating system provides to users, processes, and other systems.
To discuss the various ways of structuring an operating system.
To explain how operating systems are installed and customized and how they boot.
An operating system provides an environment for the execution of programs.
It provides certain services to programs and to the users of those programs.
The speciﬁc services provided, of course, differ from one operating system to another, but we can identify common classes.
These operating system services are provided for the convenience of the programmer, to make the programming.
Figure 2.1 shows one view of the various operating-system services and how they interrelate.
One set of operating system services provides functions that are helpful to the user.
One is a command-line interface (CLI), which uses text commands and a method for entering them (say, a keyboard for typing in commands in a speciﬁc format with speciﬁc options)
Another is a batch interface, in which commands and directives to control those commands are entered into ﬁles, and those ﬁles are executed.
Here, the interface is a window system with a pointing device to direct I/O, choose from menus, and make selections and a keyboard to enter text.
Some systems provide two or all three of these variations.
The system must be able to load a program into memory and to run that program.
The program must be able to end its execution, either normally or abnormally (indicating error)
A running program may require I/O, which may involve a ﬁle or an I/O device.
For speciﬁc devices, special functions may be desired (such as recording to a CD or DVD drive or blanking a display screen)
For efﬁciency and protection, users usually cannot control I/O devices directly.
Therefore, the operating system must provide a means to do I/O.
Obviously, programs need to read and write ﬁles and directories.
They also need to create and delete them by name, search for a given ﬁle, and list ﬁle information.
Finally, some operating systems include permissions management to allow or deny access to ﬁles or directories based on ﬁle ownership.
Many operating systems provide a variety of ﬁle systems, sometimes to allow personal choice and sometimes to provide speciﬁc features or performance characteristics.
There are many circumstances in which one process needs to exchange information with another process.
Such communication may occur between processes that are executing on the same computer or between processes that are executing on different computer systems tied together by a computer network.
Communications may be implemented via shared memory, in which two or more processes read and write to a shared section of memory, or message passing, in which packets of information in predeﬁned formats are moved between processes by the operating system.
The operating system needs to be detecting and correcting errors constantly.
Errors may occur in the CPU and memory hardware (such as a memory error or a power failure), in I/O devices (such as a parity error on disk, a connection failure on a network, or lack of paper in the printer), and in the user program (such as an arithmetic overﬂow, an attempt to access an illegal memory location, or a too-great use of CPU time)
For each type of error, the operating system should take the appropriate action to ensure correct and consistent computing.
Sometimes, it has no choice but to halt the system.
At other times, it might terminate an error-causing process or return an error code to a process for the process to detect and possibly correct.
Another set of operating system functions exists not for helping the user but rather for ensuring the efﬁcient operation of the system itself.
Systems with multiple users can gain efﬁciency by sharing the computer resources among the users.
When there are multiple users or multiple jobs running at the same time, resources must be allocated to each of them.
Some (such as CPU cycles, main memory, and ﬁle storage) may have special allocation code, whereas others (such as I/O devices) may have much more general request and release code.
For instance, in determining how best to use the CPU, operating systems have CPU-scheduling routines that take into account the speed of the CPU, the jobs that must be executed, the number of registers available, and other factors.
There may also be routines to allocate printers, USB storage drives, and other peripheral devices.
We want to keep track of which users use how much and what kinds of computer resources.
This record keeping may be used for accounting (so that users can be billed) or simply for accumulating usage statistics.
Usage statistics may be a valuable tool for researchers who wish to reconﬁgure the system to improve computing services.
The owners of information stored in a multiuser or networked computer system may want to control use of that information.
When several separate processes execute concurrently, it should not be possible for one process to interfere with the others or with the operating system itself.
Protection involves ensuring that all access to system resources is controlled.
It extends to defending external I/O devices, including network adapters, from invalid access attempts and to recording all such connections for detection of break-ins.
If a system is to be protected and secure, precautions must be instituted throughout it.
A chain is only as strong as its weakest link.
We mentioned earlier that there are several ways for users to interface with the operating system.
One provides a command-line interface, or command interpreter, that allows users to directly enter commands to be performed by the operating system.
The other allows users to interface with the operating system via a graphical user interface, or GUI.
Some operating systems include the command interpreter in the kernel.
Others, such as Windows and UNIX, treat the command interpreter as a special program that is running when a job is initiated or when a user ﬁrst logs on (on interactive systems)
On systems with multiple command interpreters to choose from, the interpreters are known as shells.
For example, on UNIX and Linux systems, a user may choose among several different shells, including the Bourne shell, C shell, Bourne-Again shell, Korn shell, and others.
Most shells provide similar functionality, and a user’s choice of which shell to use is generally based on personal preference.
The main function of the command interpreter is to get and execute the next user-speciﬁed command.
Many of the commands given at this level manipulate ﬁles: create, delete, list, print, copy, execute, and so on.
In one approach, the command interpreter itself contains the code to execute the command.
For example, a command to delete a ﬁle may cause the command interpreter to jump to a section of its code that sets up the parameters and makes the appropriate system call.
In this case, the number of commands that can be given determines the size of the command interpreter, since each command requires its own implementing code.
An alternative approach—used by UNIX, among other operating systems —implements most commands through system programs.
In this case, the command interpreter does not understand the command in any way; it merely uses the command to identify a ﬁle to be loaded into memory and executed.
In this way, programmers can add new commands to the system easily by creating new ﬁles with the proper names.
A second strategy for interfacing with the operating system is through a userfriendly graphical user interface, or GUI.
Here, rather than entering commands directly via a command-line interface, users employ a mouse-based windowand-menu system characterized by a desktop metaphor.
The user moves the mouse to position its pointer on images, or icons, on the screen (the desktop) that represent programs, ﬁles, directories, and system functions.
Depending on the mouse pointer’s location, clicking a button on the mouse can invoke a program, select a ﬁle or directory—known as a folder—or pull down a menu that contains commands.
Graphical user interfaces ﬁrst appeared due in part to research taking place in the early 1970s at Xerox PARC research facility.
However, graphical interfaces became more widespread with the advent of Apple Macintosh computers in the 1980s.
The user interface for the Macintosh operating system (Mac OS) has undergone various changes over the years, the most signiﬁcant being the adoption of the Aqua interface that appeared with Mac OS X.
Microsoft’s ﬁrst version of Windows—Version 1.0—was based on the addition of a GUI interface to the MS-DOS operating system.
Because a mouse is impractical for most mobile systems, smartphones and handheld tablet computers typically use a touchscreen interface.
Here, users interact by making gestures on the touchscreen—for example, pressing and swiping ﬁngers across the screen.
Whereas earlier smartphones included a physical keyboard, most smartphones now simulate a keyboard on the touchscreen.
These include the Common Desktop Environment (CDE) and X-Windows systems, which are common on commercial versions of UNIX, such as Solaris and IBM’s AIX system.
In addition, there has been signiﬁcant development in GUI designs from various open-source projects, such as K Desktop Environment (or KDE) and the GNOME desktop by the GNU project.
Both the KDE and GNOME desktops run on Linux and various UNIX systems and are available under open-source licenses, which means their source code is readily available for reading and for modiﬁcation under speciﬁc license terms.
The choice of whether to use a command-line or GUI interface is mostly one of personal preference.
System administrators who manage computers and power users who have deep knowledge of a system frequently use the command-line interface.
For them, it is more efﬁcient, giving them faster access to the activities they need to perform.
Indeed, on some systems, only a subset of system functions is available via the GUI, leaving the less common tasks to those who are command-line knowledgeable.
Further, commandline interfaces usually make repetitive tasks easier, in part because they have their own programmability.
For example, if a frequent task requires a set of command-line steps, those steps can be recorded into a ﬁle, and that ﬁle can be run just like a program.
The program is not compiled into executable code but rather is interpreted by the command-line interface.
These shell scripts are very common on systems that are command-line oriented, such as UNIX and Linux.
In contrast, most Windows users are happy to use the Windows GUI environment and almost never use the MS-DOS shell interface.
The various changes undergone by the Macintosh operating systems provide a nice study in contrast.
Historically, Mac OS has not provided a command-line interface, always requiring its users to interface with the operating system using its GUI.
However, with the release of Mac OS X (which is in part implemented using a UNIX kernel), the operating system now provides both a Aqua interface and a command-line interface.
The user interface can vary from system to system and even from user to user within a system.
It typically is substantially removed from the actual system structure.
The design of a useful and friendly user interface is therefore not a direct function of the operating system.
In this book, we concentrate on the fundamental problems of providing adequate service to user programs.
From the point of view of the operating system, we do not distinguish between user programs and system programs.
System calls provide an interface to the services made available by an operating system.
These calls are generally available as routines written in C and C++, although certain low-level tasks (for example, tasks where hardware must be accessed directly) may have to be written using assembly-language instructions.
Before we discuss how an operating system makes system calls available, let’s ﬁrst use an example to illustrate how system calls are used: writing a simple program to read data from one ﬁle and copy them to another ﬁle.
The ﬁrst input that the program will need is the names of the two ﬁles: the input ﬁle and the output ﬁle.
These names can be speciﬁed in many ways, depending on the operating-system design.
One approach is for the program to ask the user for the names.
In an interactive system, this approach will require a sequence of system calls, ﬁrst to write a prompting message on the screen and then to read from the keyboard the characters that deﬁne the two ﬁles.
On mouse-based and icon-based systems, a menu of ﬁle names is usually displayed in a window.
The user can then use the mouse to select the source name, and a window can be opened for the destination name to be speciﬁed.
Once the two ﬁle names have been obtained, the program must open the input ﬁle and create the output ﬁle.
Possible error conditions for each operation can require additional system calls.
When the program tries to open the input ﬁle, for example, it may ﬁnd that there is no ﬁle of that name or that the ﬁle is protected against access.
In these cases, the program should print a message on the console (another sequence of system calls) and then terminate abnormally (another system call)
If the input ﬁle exists, then we must create a new output ﬁle.
We may ﬁnd that there is already an output ﬁle with the same name.
This situation may cause the program to abort (a system call), or we may delete the existing ﬁle (another system call) and create a new one (yet another system call)
Another option, in an interactive system, is to ask the user (via a sequence of system calls to output the prompting message and to read the response from the terminal) whether to replace the existing ﬁle or to abort the program.
When both ﬁles are set up, we enter a loop that reads from the input ﬁle (a system call) and writes to the output ﬁle (another system call)
Each read and write must return status information regarding various possible error conditions.
On input, the program may ﬁnd that the end of the ﬁle has been reached or that there was a hardware failure in the read (such as a parity error)
The write operation may encounter various errors, depending on the output device (for example, no more disk space)
Finally, after the entire ﬁle is copied, the program may close both ﬁles (another system call), write a message to the console or window (more system calls), and ﬁnally terminate normally (the ﬁnal system call)
As you can see, even simple programs may make heavy use of the operating system.
Typically, application developers design programs according to an application programming interface (API)
The API speciﬁes a set of functions that are available to an application programmer, including the parameters that are passed to each function and the return values the programmer can expect.
Three of the most common APIs available to application programmers are the Windows API for Windows systems, the POSIX API for POSIX-based systems (which include virtually all versions of UNIX, Linux, and Mac OS X), and the Java API for programs that run on the Java virtual machine.
A programmer accesses an API via a library of code provided by the operating system.
In the case of UNIX and Linux for programs written in the C language, the library is called libc.
Note that—unless speciﬁed—the system-call names used throughout this text are generic examples.
Each operating system has its own name for each system call.
Behind the scenes, the functions that make up an API typically invoke the actual system calls on behalf of the application programmer.
For example, the Windows function CreateProcess() (which unsurprisingly is used to create a new process) actually invokes the NTCreateProcess() system call in the Windows kernel.
Why would an application programmer prefer programming according to an API rather than invoking actual system calls? There are several reasons for doing so.
Example System Call Sequence Acquire input file name Write prompt to screen Accept input Acquire output file name Write prompt to screen Accept input Open the input file if file doesn't exist, abort Create output file if file exists, abort Loop Read from input file Write to output file Until read fails Close output file Write completion message to screen Terminate normally.
As an example of a standard API, consider the read() function that is available in UNIX and Linux systems.
The API for this function is obtained from the man page by invoking the command.
A program that uses the read() function must include the unistd.h header ﬁle, as this ﬁle deﬁnes the ssize t and size t data types (among other things)
Furthermore, actual system calls can often be more detailed and difﬁcult to work with than the API available to an application programmer.
Nevertheless, there often exists a strong correlation between a function in the API and its associated system call within the kernel.
In fact, many of the POSIX and Windows APIs are similar to the native system calls provided by the UNIX, Linux, and Windows operating systems.
For most programming languages, the run-time support system (a set of functions built into libraries included with a compiler) provides a systemcall interface that serves as the link to system calls made available by the operating system.
The system-call interface intercepts function calls in the API and invokes the necessary system calls within the operating system.
Typically, a number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers.
Figure 2.6 The handling of a user application invoking the open() system call.
The caller need know nothing about how the system call is implemented or what it does during execution.
Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call.
Thus, most of the details of the operating-system interface are hidden from the programmer by the API and are managed by the run-time support library.
The relationship between an API, the system-call interface, and the operating system is shown in Figure 2.6, which illustrates how the operating system handles a user application invoking the open() system call.
System calls occur in different ways, depending on the computer in use.
Often, more information is required than simply the identity of the desired system call.
The exact type and amount of information vary according to the particular operating system and call.
For example, to get input, we may need to specify the ﬁle or device to use as the source, as well as the address and length of the memory buffer into which the input should be read.
Of course, the device or ﬁle and length may be implicit in the call.
Three general methods are used to pass parameters to the operating system.
The simplest approach is to pass the parameters in registers.
In some cases, however, there may be more parameters than registers.
In these cases, the parameters are generally stored in a block, or table, in memory, and the address of the block is passed as a parameter in a register (Figure 2.7)
Parameters also can be placed, or pushed, onto the stack by the program and popped off the stack by the operating system.
Some operating systems prefer the block or stack method because those approaches do not limit the number or length of parameters being passed.
System calls can be grouped roughly into six major categories: process control, ﬁle manipulation, device manipulation, information maintenance, communications, and protection.
Most of these system calls support, or are supported by, concepts and functions that are discussed in later chapters.
Figure 2.8 summarizes the types of system calls normally provided by an operating system.
As mentioned, in this text, we normally refer to the system calls by generic names.
Throughout the text, however, we provide examples of the actual counterparts to the system calls for Windows, UNIX, and Linux systems.
A running program needs to be able to halt its execution either normally (end()) or abnormally (abort())
If a system call is made to terminate the currently running program abnormally, or if the program runs into a problem and causes an error trap, a dump of memory is sometimes taken and an error message generated.
The dump is written to disk and may be examined by a debugger—a system program designed to aid the programmer in ﬁnding and correcting errors, or bugs—to determine the cause of the problem.
Under either normal or abnormal circumstances, the operating system must transfer control to the invoking command interpreter.
In an interactive system, the command interpreter simply continues with the next command; it is assumed that the user will issue an appropriate command to respond to any error.
In a GUI system, a pop-up window might alert the user to the error and ask for guidance.
In a batch system, the command interpreter usually terminates the entire job and continues with the next job.
Some systems may allow for special recovery actions in case an error occurs.
If the program discovers an error in its input and wants to terminate abnormally, it may also want to deﬁne an error level.
More severe errors can be indicated by a higher-level error parameter.
The command interpreter or a following program can use this error level to determine the next action automatically.
A process or job executing one program may want to load() and execute() another program.
This feature allows the command interpreter to execute a program as directed by, for example, a user command, the click of a.
An interesting question is where to return control when the loaded program terminates.
This question is related to whether the existing program is lost, saved, or allowed to continue execution concurrently with the new program.
If control returns to the existing program when the new program terminates, we must save the memory image of the existing program; thus, we have effectively created a mechanism for one program to call another program.
If both programs continue concurrently, we have created a new job or process to be multiprogrammed.
Often, there is a system call speciﬁcally for this purpose (create process() or submit job())
If we create a new job or process, or perhaps even a set of jobs or processes, we should be able to control its execution.
This control requires the ability to determine and reset the attributes of a job or process, including the job’s priority, its maximum allowable execution time, and so on (get process attributes() andset process attributes())
We may also want to terminate a job or process that we created (terminate process()) if we ﬁnd that it is incorrect or is no longer needed.
The standard C library provides a portion of the system-call interface for many versions of UNIX and Linux.
As an example, let’s assume a C program invokes the printf() statement.
The C library intercepts this call and invokes the necessary system call (or calls) in the operating system—in this instance, the write() system call.
The C library takes the value returned by write() and passes it back to the user program.
Having created new jobs or processes, we may need to wait for them to ﬁnish their execution.
We may want to wait for a certain amount of time to pass (wait time())
More probably, we will want to wait for a speciﬁc event to occur (wait event())
The jobs or processes should then signal when that event has occurred (signal event())
To ensure the integrity of the data being shared, operating systems often provide system calls allowing a process to lock shared data.
Then, no other process can access the data until the lock is released.
Typically, such system calls include acquire lock() and release lock()
There are so many facets of and variations in process and job control that we next use two examples—one involving a single-tasking system and the other a multitasking system—to clarify these concepts.
The MS-DOS operating system is an example of a single-tasking system.
It has a command interpreter that is invoked when the computer is started (Figure 2.9(a))
Because MS-DOS is single-tasking, it uses a simple method to run a program and does not create a new process.
It loads the program into memory, writing over most of itself to.
Next, it sets the instruction pointer to the ﬁrst instruction of the program.
The program then runs, and either an error causes a trap, or the program executes a system call to terminate.
In either case, the error code is saved in the system memory for later use.
Following this action, the small portion of the command interpreter that was not overwritten resumes execution.
Its ﬁrst task is to reload the rest of the command interpreter from disk.
Then the command interpreter makes the previous error code available to the user or to the next program.
FreeBSD (derived from Berkeley UNIX) is an example of a multitasking system.
When a user logs on to the system, the shell of the user’s choice is run.
This shell is similar to the MS-DOS shell in that it accepts commands and executes programs that the user requests.
However, since FreeBSD is a multitasking system, the command interpreter may continue running while another program is executed (Figure 2.10)
Then, the selected program is loaded into memory via an exec() system call, and the program is executed.
Depending on the way the command was issued, the shell then either waits for the process to ﬁnish or runs the process “in the background.” In the latter case, the shell immediately requests another command.
When a process is running in the background, it cannot receive input directly from the keyboard, because the shell is using this resource.
I/O is therefore done through ﬁles or through a GUI interface.
Meanwhile, the user is free to ask the shell to run other programs, to monitor the progress of the running process, to change that program’s priority, and so on.
When the process is done, it executes an exit() system call to terminate, returning to the invoking process a status code of 0 or a nonzero error code.
This status or error code is then available to the shell or other programs.
Processes are discussed in Chapter 3 with a program example using the fork() and exec() system calls.
We can, however, identify several common system calls dealing with ﬁles.
We ﬁrst need to be able to create() and delete() ﬁles.
Either system call requires the name of the ﬁle and perhaps some of the ﬁle’s attributes.
Once the ﬁle is created, we need to open() it and to use it.
We may also read(), write(), or reposition() (rewind or skip to the end of the ﬁle, for example)
Finally, we need to close() the ﬁle, indicating that we are no longer using it.
We may need these same sets of operations for directories if we have a directory structure for organizing ﬁles in the ﬁle system.
In addition, for either ﬁles or directories, we need to be able to determine the values of various attributes and perhaps to reset them if necessary.
File attributes include the ﬁle name, ﬁle type, protection codes, accounting information, and so on.
At least two system calls, get file attributes() and set file attributes(), are required for this function.
Some operating systems provide many more calls, such as calls for ﬁle move() and copy()
Others might provide an API that performs those operations using code and other system calls, and others might provide system programs to perform those tasks.
If the system programs are callable by other programs, then each can be considered an API by other system programs.
A process may need several resources to execute—main memory, disk drives, access to ﬁles, and so on.
If the resources are available, they can be granted, and control can be returned to the user process.
Otherwise, the process will have to wait until sufﬁcient resources are available.
The various resources controlled by the operating system can be thought of as devices.
Some of these devices are physical devices (for example, disk drives), while others can be thought of as abstract or virtual devices (for example, ﬁles)
A system with multiple users may require us to ﬁrst request() a device, to ensure exclusive use of it.
After we are ﬁnished with the device, we release() it.
These functions are similar to the open() and close() system calls for ﬁles.
Once the device has been requested (and allocated to us), we can read(), write(), and (possibly) reposition() the device, just as we can with ﬁles.
In fact, the similarity between I/O devices and ﬁles is so great that many operating systems, including UNIX, merge the two into a combined ﬁle–device structure.
In this case, a set of system calls is used on both ﬁles and devices.
Sometimes, I/O devices are identiﬁed by special ﬁle names, directory placement, or ﬁle attributes.
The user interface can also make ﬁles and devices appear to be similar, even though the underlying system calls are dissimilar.
This is another example of the many design decisions that go into building an operating system and user interface.
Many system calls exist simply for the purpose of transferring information between the user program and the operating system.
For example, most systems have a system call to return the current time() and date()
Other system calls may return information about the system, such as the number of current users, the version number of the operating system, the amount of free memory or disk space, and so on.
Another set of system calls is helpful in debugging a program.
A program trace lists each system call as it is executed.
Even microprocessors provide a CPU mode known as single step, in which a trap is executed by the CPU after every instruction.
Many operating systems provide a time proﬁle of a program to indicate the amount of time that the program executes at a particular location or set of locations.
A time proﬁle requires either a tracing facility or regular timer interrupts.
At every occurrence of the timer interrupt, the value of the program counter is recorded.
With sufﬁciently frequent timer interrupts, a statistical picture of the time spent on various parts of the program can be obtained.
In addition, the operating system keeps information about all its processes, and system calls are used to access this information.
Generally, calls are also used to reset the process information (get process attributes() and set process attributes())
In Section 3.1.3, we discuss what information is normally kept.
There are two common models of interprocess communication: the messagepassing model and the shared-memory model.
In the message-passing model, the communicating processes exchange messages with one another to transfer information.
Messages can be exchanged between the processes either directly or indirectly through a common mailbox.
Before communication can take place, a connection must be opened.
The name of the other communicator must be known, be it another process on the same system or a process on another computer connected by a communications network.
Each computer in a network has a host name by which it is commonly known.
Similarly, each process has a process name, and this name is translated into an identiﬁer by which the operating system can refer to the process.
The get hostid() and get processid() system calls do this translation.
The recipient process usually must give its permission for communication to take place with an accept connection() call.
Most processes that will be receiving connections are special-purpose daemons, which are system programs provided for that purpose.
They execute a wait for connection() call and are awakened when a connection is made.
The source of the communication, known as the client, and the receiving daemon, known as a server, then exchange messages by usingread message() and write message() system calls.
In the shared-memory model, processes use shared memory create() and shared memory attach() system calls to create and gain access to regions of memory owned by other processes.
Recall that, normally, the operating system tries to prevent one process from accessing another process’s memory.
Shared memory requires that two or more processes agree to remove this restriction.
They can then exchange information by reading and writing data in the shared areas.
The form of the data is determined by the processes and is not under the operating system’s control.
The processes are also responsible for ensuring that they are not writing to the same location simultaneously.
In Chapter 4, we look at a variation of the process scheme—threads—in which memory is shared by default.
Both of the models just discussed are common in operating systems, and most systems implement both.
Message passing is useful for exchanging smaller amounts of data, because no conﬂicts need be avoided.
It is also easier to implement than is shared memory for intercomputer communication.
Shared memory allows maximum speed and convenience of communication, since it can be done at memory transfer speeds when it takes place within a computer.
Problems exist, however, in the areas of protection and synchronization between the processes sharing memory.
Protection provides a mechanism for controlling access to the resources provided by a computer system.
Historically, protection was a concern only on multiprogrammed computer systems with several users.
However, with the advent of networking and the Internet, all computer systems, from servers to mobile handheld devices, must be concerned with protection.
Typically, system calls providing protection include set permission() and get permission(), which manipulate the permission settings of resources such as ﬁles and disks.
The allow user() and deny user() system calls specify whether particular users can—or cannot—be allowed access to certain resources.
Another aspect of a modern system is its collection of system programs.
Next is the operating system, then the system programs, and ﬁnally the application programs.
System programs, also known as system utilities, provide a convenient environment for program development and execution.
Some of them are simply user interfaces to system calls.
These programs create, delete, copy, rename, print, dump, list, and generally manipulate ﬁles and directories.
Some programs simply ask the system for the date, time, amount of available memory or disk space, number of users, or similar status information.
Others are more complex, providing detailed performance, logging, and debugging information.
Typically, these programs format and print the output to the terminal or other output devices or ﬁles or display it in a window of the GUI.
Some systems also support a registry, which is used to store and retrieve conﬁguration information.
Several text editors may be available to create and modify the content of ﬁles stored on disk or other storage devices.
There may also be special commands to search contents of ﬁles or perform transformations of the text.
Compilers, assemblers, debuggers, and interpreters for common programming languages (such as C, C++, Java, and PERL) are often provided with the operating system or available as a separate download.
Once a program is assembled or compiled, it must be loaded into memory to be executed.
The system may provide absolute loaders, relocatable loaders, linkage editors, and overlay loaders.
Debugging systems for either higher-level languages or machine language are needed as well.
These programs provide the mechanism for creating virtual connections among processes, users, and computer systems.
They allow users to send messages to one another’s screens, to browse Web pages, to send e-mail messages, to log in remotely, or to transfer ﬁles from one machine to another.
All general-purpose systems have methods for launching certain system-program processes at boot time.
Some of these processes terminate after completing their tasks, while others continue to run until the system is halted.
Constantly running system-program processes are known as services, subsystems, or daemons.
One example is the network daemon discussed in Section 2.4.5
In that example, a system needed a service to listen for network connections in order to connect those requests to the correct processes.
Other examples include process schedulers that start processes according to a speciﬁed schedule, system error monitoring services, and print servers.
In addition, operating systems that run important activities in user context rather than in kernel context may use daemons to run these activities.
Along with system programs, most operating systems are supplied with programs that are useful in solving common problems or performing common operations.
The view of the operating system seen by most users is deﬁned by the application and system programs, rather than by the actual system calls.
When a user’s computer is running the Mac OS X operating system, the user might see the GUI, featuring a mouse-and-windows interface.
Alternatively, or even in one of the windows, the user might have a command-line UNIX shell.
Both use the same set of system calls, but the system calls look different and act in different ways.
Further confusing the user view, consider the user dual-booting from Mac OS X into Windows.
Now the same user on the same hardware has two entirely different interfaces and two sets of applications using the same physical resources.
On the same hardware, then, a user can be exposed to multiple user interfaces sequentially or concurrently.
In this section, we discuss problems we face in designing and implementing an operating system.
There are, of course, no complete solutions to such problems, but there are approaches that have proved successful.
The ﬁrst problem in designing a system is to deﬁne goals and speciﬁcations.
At the highest level, the design of the system will be affected by the choice of hardware and the type of system: batch, time sharing, single user, multiuser, distributed, real time, or general purpose.
Beyond this highest design level, the requirements may be much harder to specify.
The requirements can, however, be divided into two basic groups: user goals and system goals.
The system should be convenient to use, easy to learn and to use, reliable, safe, and fast.
Of course, these speciﬁcations are not particularly useful in the system design, since there is no general agreement on how to achieve them.
A similar set of requirements can be deﬁned by those people who must design, create, maintain, and operate the system.
The system should be easy to design, implement, and maintain; and it should be ﬂexible, reliable, error free, and efﬁcient.
Again, these requirements are vague and may be interpreted in various ways.
There is, in short, no unique solution to the problem of deﬁning the requirements for an operating system.
The wide range of systems in existence shows that different requirements can result in a large variety of solutions for different environments.
Specifying and designing an operating system is a highly creative task.
Although no textbook can tell you how to do it, general principles have been developed in the ﬁeld of software engineering, and we turn now to a discussion of some of these principles.
One important principle is the separation of policy from mechanism.
Mechanisms determine how to do something; policies determine what will be done.
For example, the timer construct (see Section 1.5.2) is a mechanism for ensuring CPU protection, but deciding how long the timer is to be set for a particular user is a policy decision.
The separation of policy and mechanism is important for ﬂexibility.
Policies are likely to change across places or over time.
In the worst case, each change in policy would require a change in the underlying mechanism.
A general mechanism insensitive to changes in policy would be more desirable.
A change in policy would then require redeﬁnition of only certain parameters of the system.
For instance, consider a mechanism for giving priority to certain types of programs over others.
If the mechanism is properly separated from policy, it can be used either to support a policy decision that I/O-intensive programs should have priority over CPU-intensive ones or to support the opposite policy.
Microkernel-based operating systems (Section 2.7.3) take the separation of mechanism and policy to one extreme by implementing a basic set of primitive building blocks.
These blocks are almost policy free, allowing more advanced mechanisms and policies to be added via user-created kernel modules or user programs themselves.
In the latest version of Solaris, scheduling is controlled by loadable tables.
Depending on the table currently loaded, the system can be time sharing, batch processing, real time, fair share, or any combination.
Making the scheduling mechanism general purpose allows vast policy changes to be made with a single load-new-table command.
At the other extreme is a system such as Windows, in which both mechanism and policy are encoded in the system to enforce a global look and feel.
All applications have similar interfaces, because the interface itself is built into the kernel and system libraries.
Whenever it is necessary to decide whether or not to allocate a resource, a policy decision must be made.
Whenever the question is how rather than what, it is a mechanism that must be determined.
Once an operating system is designed, it must be implemented.
Because operating systems are collections of many programs, written by many people over a long period of time, it is difﬁcult to make general statements about how they are implemented.
Now, although some operating systems are still written in assembly language, most are written in a higher-level language such as C or an even higher-level language such as C++
Actually, an operating system can be written in more than one language.
The lowest levels of the kernel might be assembly language.
Higher-level routines might be in C, and system programs might be in C or C++, in interpreted scripting languages like PERL or Python, or in shell scripts.
In fact, a given Linux distribution probably includes programs written in all of those languages.
The ﬁrst system that was not written in assembly language was probably the Master Control Program (MCP) for Burroughs computers.
MULTICS, developed at MIT, was written mainly in the system programming language PL/1
The Linux and Windows operating system kernels are written mostly in C, although there are some small sections of assembly code for device drivers and for saving and restoring the state of registers.
In addition, improvements in compiler technology will improve the generated code for the entire operating system by simple recompilation.
Finally, an operating system is far easier to port—to move to some other hardwareif it is written in a higher-level language.
For example, MS-DOS was written in Intel 8088 assembly language.
Consequently, it runs natively only on the Intel X86 family of CPUs.
As we mentioned in Chapter 1, emulators are programs that duplicate the functionality of one system on another system.
The Linux operating system, in contrast, is written mostly in C and is available natively on a number of different CPUs, including Intel X86, Oracle SPARC, and IBMPowerPC.
The only possible disadvantages of implementing an operating system in a higher-level language are reduced speed and increased storage requirements.
This, however, is no longer a major issue in today’s systems.
Although an expert assembly-language programmer can produce efﬁcient small routines, for large programs a modern compiler can perform complex analysis and apply sophisticated optimizations that produce excellent code.
Modern processors have deep pipelining and multiple functional units that can handle the details of complex dependencies much more easily than can the human mind.
As is true in other systems, major performance improvements in operating systems are more likely to be the result of better data structures and algorithms than of excellent assembly-language code.
In addition, although operating systems are large, only a small amount of the code is critical to high performance; the interrupt handler, I/O manager, memory manager, and CPU scheduler are probably the most critical routines.
After the system is written and is working correctly, bottleneck routines can be identiﬁed and can be replaced with assembly-language equivalents.
A system as large and complex as a modern operating system must be engineered carefully if it is to function properly and be modiﬁed easily.
A common approach is to partition the task into small components, or modules, rather than have one monolithic system.
Each of these modules should be a well-deﬁned portion of the system, with carefully deﬁned inputs, outputs, and functions.
We have already discussed brieﬂy in Chapter 1 the common components of operating systems.
In this section, we discuss how these components are interconnected and melded into a kernel.
Frequently, such systems started as small, simple, and limited systems and then grew beyond their original scope.
It was originally designed and implemented by a few people who had no idea that it would become so popular.
It was written to provide the most functionality in the least space, so it was not carefully divided into modules.
In MS-DOS, the interfaces and levels of functionality are not well separated.
For instance, application programs are able to access the basic I/O routines to write directly to the display and disk drives.
Such freedom leaves MS-DOS vulnerable to errant (or malicious) programs, causing entire system crashes when user programs fail.
Of course, MS-DOS was also limited by the hardware of its era.
Because the Intel 8088 for which it was written provides no dual mode and no hardware protection, the designers of MS-DOS had no choice but to leave the base hardware accessible.
Another example of limited structuring is the original UNIX operating system.
It consists of two separable parts: the kernel and the system programs.
We can view the traditional UNIX operating system as being layered to some extent, as shown in Figure 2.12
Everything below the system-call interface and above the physical hardware is the kernel.
The kernel provides the ﬁle system, CPU scheduling, memory management, and other operating-system functions through system calls.
Taken in sum, that is an enormous amount of functionality to be combined into one level.
It had a distinct performance advantage, however: there is very little overhead in the system call interface or in communication within the kernel.
We still see evidence of this simple, monolithic structure in the UNIX, Linux, and Windows operating systems.
With proper hardware support, operating systems can be broken into pieces that are smaller and more appropriate than those allowed by the original MS-DOS and UNIX systems.
The operating system can then retain much greater control over the computer and over the applications that make use of that computer.
Implementers have more freedom in changing the inner workings of the system and in creating modular operating systems.
Under a topdown approach, the overall functionality and features are determined and are separated into components.
Information hiding is also important, because it leaves programmers free to implement the low-level routines as they see ﬁt, provided that the external interface of the routine stays unchanged and that the routine itself performs the advertised task.
One method is the layered approach, in which the operating system is broken into a number of layers (levels)
The bottom layer (layer 0) is the hardware; the highest (layer N) is the user interface.
An operating-system layer is an implementation of an abstract object made up of data and the operations that can manipulate those data.
A typical operating-system layer—say, layer M—consists of data structures and a set of routines that can be invoked by higher-level layers.
Layer M, in turn, can invoke operations on lower-level layers.
The main advantage of the layered approach is simplicity of construction and debugging.
The layers are selected so that each uses functions (operations) and services of only lower-level layers.
The ﬁrst layer can be debugged without any concern for the rest of the system, because, by deﬁnition, it uses only the basic hardware (which is assumed correct) to implement its functions.
Once the ﬁrst layer is debugged, its correct functioning can be assumed while the second layer is debugged, and so on.
If an error is found during the debugging of a particular layer, the error must be on that layer, because the layers below it are already debugged.
Thus, the design and implementation of the system are simpliﬁed.
Each layer is implemented only with operations provided by lower-level layers.
A layer does not need to know how these operations are implemented; it needs to know only what these operations do.
Hence, each layer hides the existence of certain data structures, operations, and hardware from higher-level layers.
The major difﬁculty with the layered approach involves appropriately deﬁning the various layers.
Because a layer can use only lower-level layers, careful planning is necessary.
For example, the device driver for the backing store (disk space used by virtual-memory algorithms) must be at a lower level than the memory-management routines, because memory management requires the ability to use the backing store.
The backing-store driver would normally be above the CPU scheduler, because the driver may need to wait for I/O and the CPU can be rescheduled during this time.
Therefore, this information may need to be swapped in and out of memory, requiring the backing-store driver routine to be below the CPU scheduler.
A ﬁnal problem with layered implementations is that they tend to be less efﬁcient than other types.
For instance, when a user program executes an I/O operation, it executes a system call that is trapped to the I/O layer, which calls the memory-management layer, which in turn calls the CPU-scheduling layer, which is then passed to the hardware.
At each layer, the parameters may be modiﬁed, data may need to be passed, and so on.
The net result is a system call that takes longer than does one on a nonlayered system.
These limitations have caused a small backlash against layering in recent years.
Fewer layers with more functionality are being designed, providing most of the advantages of modularized code while avoiding the problems of layer deﬁnition and interaction.
We have already seen that as UNIX expanded, the kernel became large and difﬁcult to manage.
In the mid-1980s, researchers at Carnegie Mellon University developed an operating system called Mach that modularized the kernel using the microkernel approach.
This method structures the operating system by removing all nonessential components from the kernel and implementing them as system and user-level programs.
There is little consensus regarding which services should remain in the kernel and which should be implemented in user space.
Typically, however, microkernels provide minimal process and memory management, in addition to a communication facility.
The main function of the microkernel is to provide communication between the client program and the various services that are also running in user space.
Communication is provided through message passing, which was described in Section 2.4.5
For example, if the client program wishes to access a ﬁle, it.
Rather, they communicate indirectly by exchanging messages with the microkernel.
One beneﬁt of the microkernel approach is that it makes extending the operating system easier.
All new services are added to user space and consequently do not require modiﬁcation of the kernel.
When the kernel does have to be modiﬁed, the changes tend to be fewer, because the microkernel is a smaller kernel.
The resulting operating system is easier to port from one hardware design to another.
The microkernel also provides more security and reliability, since most services are running as user—rather than kernelprocesses.
If a service fails, the rest of the operating system remains untouched.
Tru64 UNIX (formerly Digital UNIX) provides a UNIX interface to the user, but it is implemented with a Mach kernel.
The Mach kernel maps UNIX system calls into messages to the appropriate user-level services.
The Mac OS X kernel (also known as Darwin) is also partly based on the Mach microkernel.
Another example is QNX, a real-time operating system for embedded systems.
The QNX Neutrino microkernel provides services for message passing and process scheduling.
All other services in QNX are provided by standard processes that run outside the kernel in user mode.
Unfortunately, the performance of microkernels can suffer due to increased system-function overhead.
Windows NT 4.0 partially corrected the performance problem by moving layers from user space to kernel space and integrating them more closely.
By the time Windows XP was designed, Windows architecture had become more monolithic than microkernel.
Perhaps the best current methodology for operating-system design involves using loadable kernel modules.
Here, the kernel has a set of core components and links in additional services via modules, either at boot time or during run time.
This type of design is common in modern implementations of UNIX, such as Solaris, Linux, and Mac OS X, as well as Windows.
The idea of the design is for the kernel to provide core services while other services are implemented dynamically, as the kernel is running.
Linking services dynamically is preferable to adding new features directly to the kernel, which would require recompiling the kernel every time a change was made.
Thus, for example, we might build CPU scheduling and memory management algorithms directly into the kernel and then add support for different ﬁle systems by way of loadable modules.
The overall result resembles a layered system in that each kernel section has deﬁned, protected interfaces; but it is more ﬂexible than a layered system, because any module can call any other module.
The approach is also similar to the microkernel approach in that the primary module has only core functions and knowledge of how to load and communicate with other modules; but it.
The Solaris operating system structure, shown in Figure 2.15, is organized around a core kernel with seven types of loadable kernel modules:
Linux also uses loadable kernel modules, primarily for supporting device drivers and ﬁle systems.
We cover creating loadable kernel modules in Linux as a programming exercise at the end of this chapter.
In practice, very few operating systems adopt a single, strictly deﬁned structure.
Instead, they combine different structures, resulting in hybrid systems that address performance, security, and usability issues.
For example, both Linux and Solaris are monolithic, because having the operating system in a single address space provides very efﬁcient performance.
However, they are also modular, so that new functionality can be dynamically added to the kernel.
Windows is largely monolithic as well (again primarily for performance reasons), but it retains some behavior typical of microkernel systems, including providing support for separate subsystems (known as operating-system personalities) that run as user-mode processes.
Windows systems also provide support for dynamically loadable kernel modules.
In the remainder of this section, we explore the structure of.
As shown in Figure 2.16, it is a layered system.
The top layers include the Aqua user interface (Figure 2.4) and a set of application environments and services.
Notably, the Cocoa environment speciﬁes an API for the Objective-C programming language, which is used for writing Mac OS X applications.
Below these layers is the kernel environment, which consists primarily of the Mach microkernel and the BSD UNIX kernel.
Mach provides memory management; support for remote procedure calls (RPCs) and interprocess communication (IPC) facilities, including message passing; and thread scheduling.
The BSD component provides a BSD command-line interface, support for networking and ﬁle systems, and an implementation of POSIX APIs, including Pthreads.
In addition to Mach and BSD, the kernel environment provides an I/O kit for development of device drivers and dynamically loadable modules (which Mac OS X refers to as kernel extensions)
As shown in Figure 2.16, the BSD application environment can make use of BSD facilities directly.
Cocoa Touch is an API for Objective-C that provides several frameworks for developing applications that run on iOS devices.
The fundamental difference between Cocoa, mentioned earlier, and Cocoa Touch is that the latter provides support for hardware features unique to mobile devices, such as touch screens.
The media services layer provides services for graphics, audio, and video.
The core services layer provides a variety of features, including support for cloud computing and databases.
The bottom layer represents the core operating system, which is based on the kernel environment shown in Figure 2.16
The Android operating system was designed by the Open Handset Alliance (led primarily by Google) and was developed for Android smartphones and tablet computers.
Whereas iOS is designed to run on Apple mobile devices and is close-sourced, Android runs on a variety of mobile platforms and is open-sourced, partly explaining its rapid rise in popularity.
Android is similar to iOS in that it is a layered stack of software that provides a rich set of frameworks for developing mobile applications.
At the bottom of this software stack is the Linux kernel, although it has been modiﬁed by Google and is currently outside the normal distribution of Linux releases.
Linux is used primarily for process, memory, and device-driver support for hardware and has been expanded to include power management.
The Android runtime environment includes a core set of libraries as well as the Dalvik virtual machine.
Software designers for Android devices develop applications in the Java language.
However, rather than using the standard Java API, Google has designed a separate Android API for Java development.
The Java class ﬁles are ﬁrst compiled to Java bytecode and then translated into an executable ﬁle that runs on the Dalvik virtual machine.
The Dalvik virtual machine was designed for Android and is optimized for mobile devices with limited memory and CPU processing capabilities.
The set of libraries available for Android applications includes frameworks for developing web browsers (webkit), database support (SQLite), and multimedia.
The libc library is similar to the standard C library but is much smaller and has been designed for the slower CPUs that characterize mobile devices.
Broadly, debugging is the activity of ﬁnding and ﬁxing errors in a system, both in hardware and in software.
Performance problems are considered bugs, so debugging can also include performance tuning, which seeks to improve performance by removing processing bottlenecks.
In this section, we explore debugging process and kernel errors and performance problems.
If a process fails, most operating systems write the error information to a log ﬁle to alert system operators or users that the problem occurred.
The operating system can also take a core dump—a capture of the memory of the processand store it in a ﬁle for later analysis.
Memory was referred to as the “core” in the early days of computing.
Running programs and core dumps can be probed by a debugger, which allows a programmer to explore the code and memory of a process.
Operating-system kernel debugging is even more complex because of the size and complexity of the kernel, its control of the hardware, and the lack of user-level debugging tools.
When a crash occurs, error information is saved to a log ﬁle, and the memory state is saved to a crash dump.
Operating-system debugging and process debugging frequently use different tools and techniques due to the very different nature of these two tasks.
Consider that a kernel failure in the ﬁle-system code would make it risky for the kernel to try to save its state to a ﬁle on the ﬁle system before rebooting.
A common technique is to save the kernel’s memory state to a section of disk set aside for this purpose that contains no ﬁle system.
If the kernel detects an unrecoverable error, it writes the entire contents of memory, or at least the kernel-owned parts of the system memory, to the disk area.
When the system reboots, a process runs to gather the data from that area and write it to a crash.
Debugging is twice as hard as writing the code in the ﬁrst place.
Obviously, such strategies would be unnecessary for debugging ordinary user-level processes.
We mentioned earlier that performance tuning seeks to improve performance by removing processing bottlenecks.
To identify bottlenecks, we must be able to monitor system performance.
Thus, the operating system must have some means of computing and displaying measures of system behavior.
In a number of systems, the operating system does this by producing trace listings of system behavior.
All interesting events are logged with their time and important parameters and are written to a ﬁle.
Later, an analysis program can process the log ﬁle to determine system performance and to identify bottlenecks and inefﬁciencies.
These same traces can be run as input for a simulation of a suggested improved system.
Traces also can help people to ﬁnd errors in operating-system behavior.
Another approach to performance tuning uses single-purpose, interactive tools that allow users and administrators to question the state of various system components to look for bottlenecks.
One such tool employs the UNIX command top to display the resources used on the system, as well as a sorted list of the “top” resource-using processes.
Other tools display the state of disk I/O, memory allocation, and network trafﬁc.
The task manager includes information for current applications as well as processes, CPU and memory usage, and networking statistics.
A screen shot of the task manager appears in Figure 2.19
Making operating systems easier to understand, debug, and tune as they run is an active area of research and implementation.
A new generation of kernel-enabled performance analysis tools has made signiﬁcant improvements in how this goal can be achieved.
Next, we discuss a leading example of such a tool: the Solaris 10 DTrace dynamic tracing facility.
DTrace is a facility that dynamically adds probes to a running system, both in user processes and in the kernel.
These probes can be queried via the D programming language to determine an astonishing amount about the kernel, the system state, and process activities.
For example, Figure 2.20 follows an application as it executes a system call (ioctl()) and shows the functional calls within the kernel as they execute to perform the system call.
Lines ending with “U” are executed in user mode, and lines ending in “K” in kernel mode.
Debugging the interactions between user-level and kernel code is nearly impossible without a toolset that understands both sets of code and can instrument the interactions.
For that toolset to be truly useful, it must be able to debug any area of a system, including areas that were not written with debugging in mind, and do so without affecting system reliability.
This tool must also have a minimum performance impact—ideally it should have no impact when not in use and a proportional impact during use.
The DTrace tool meets these requirements and provides a dynamic, safe, low-impact debugging environment.
Until the DTrace framework and tools became available with Solaris 10, kernel debugging was usually shrouded in mystery and accomplished via happenstance and archaic code and tools.
For example, CPUs have a breakpoint feature that will halt execution and allow a debugger to examine the state of the system.
Then execution can continue until the next breakpoint or termination.
This method cannot be used in a multiuser operating-system kernel without negatively affecting all of the users on the system.
Proﬁling, which periodically samples the instruction pointer to determine which code is being executed, can show statistical trends but not individual activities.
Code can be included in the kernel to emit speciﬁc data under speciﬁc circumstances, but that code slows down the kernel and tends not to be included in the part of the kernel where the speciﬁc problem being debugged is occurring.
In contrast, DTrace runs on production systems—systems that are running important or critical applications—and causes no harm to the system.
It slows activities while enabled, but after execution it resets the system to its pre-debugging state.
It can broadly debug everything happening in the system (both at the user and kernel levels and between the user and kernel layers)
It can also delve deep into code, showing individual CPU instructions or kernel subroutine activities.
DTrace is composed of a compiler, a framework, providers of probes written within that framework, and consumers of those probes.
Kernel structures exist to keep track of all probes that the providers have created.
The probes are stored in a hash-table data structure that is hashed by name and indexed according to unique probe identiﬁers.
When a probe is enabled, a bit of code in the area to be probed is rewritten to call dtrace probe(probe identifier) and then continue with the code’s original operation.
For example, a kernel system-call probe works differently from a user-process probe, and that is different from an I/O probe.
DTrace features a compiler that generates a byte code that is run in the kernel.
This code is assured to be “safe” by the compiler.
For example, no loops are allowed, and only speciﬁc kernel state modiﬁcations are allowed when speciﬁcally requested.
The generated code runs in the kernel and enables probes.
It also enables consumers in user mode and enables communications between the two.
A DTrace consumer is code that is interested in a probe and its results.
A consumer requests that the provider create one or more probes.
When a probe ﬁres, it emits data that are managed by the kernel.
Within the kernel, actions called enabling control blocks, or ECBs, are performed when probes ﬁre.
One probe can cause multiple ECBs to execute if more than one consumer is interested in that probe.
Each ECB contains a predicate (“if statement”) that can ﬁlter out that ECB.
Otherwise, the list of actions in the ECB is executed.
The most common action is to capture some bit of data, such as a variable’s value at that point of the probe execution.
By gathering such data, a complete picture of a user or kernel action can be built.
Further, probes ﬁring from both user space and the kernel can show how a user-level action caused kernel-level reactions.
Such data are invaluable for performance monitoring and code optimization.
If there are no ECBs consuming a probe, the probe is removed.
That involves rewriting the code to remove the dtrace probe() call and put back the original code.
Thus, before a probe is created and after it is destroyed, the system is exactly the same, as if no probing occurred.
DTrace takes care to assure that probes do not use too much memory or CPU capacity, which could harm the running system.
The buffers used to hold the probe results are monitored for exceeding default and maximum limits.
If limits are exceeded, the consumer is terminated, along with the offending probes.
Buffers are allocated per CPU to avoid contention and data loss.
An example of D code and its output shows some of its utility.
The following program shows the DTrace code to enable scheduler probes and record the amount of CPU time of each process running with user ID 101 while those probes are enabled (that is, while the program runs):
The output of the program, showing the processes and how much time (in nanoseconds) they spend running on the CPUs, is shown in Figure 2.21
Because DTrace is part of the open-source OpenSolaris version of the Solaris.
For example, DTrace has been added to Mac OS X and FreeBSD and will likely spread further due to its unique capabilities.
Other operating systems, especially the Linux derivatives, are adding kernel-tracing functionality as well.
Still other operating systems are beginning to include performance and tracing tools fostered by research at various institutions, including the Paradyn project.
It is possible to design, code, and implement an operating system speciﬁcally for one machine at one site.
More commonly, however, operating systems are designed to run on any of a class of machines at a variety of sites with a variety of peripheral conﬁgurations.
The system must then be conﬁgured or generated for each speciﬁc computer site, a process sometimes known as system generation SYSGEN.
The operating system is normally distributed on disk, on CD-ROM or DVD-ROM, or as an “ISO” image, which is a ﬁle in the format of a CD-ROM or DVD-ROM.
This SYSGEN program reads from a given ﬁle, or asks the operator of the system for information concerning the speciﬁc conﬁguration of the hardware system, or probes the hardware directly to determine what components are there.
What CPU is to be used? What options (extended instruction sets, ﬂoatingpoint arithmetic, and so on) are installed? For multiple CPU systems, each CPU may be described.
How will the boot disk be formatted? How many sections, or “partitions,” will it be separated into, and what will go into each partition?
How much memory is available? Some systems will determine this value themselves by referencing memory location after memory location until an “illegal address” fault is generated.
This procedure deﬁnes the ﬁnal legal address and hence the amount of available memory.
What devices are available? The system will need to know how to address each device (the device number), the device interrupt number, the device’s type and model, and any special device characteristics.
What operating-system options are desired, or what parameter values are to be used? These options or values might include how many buffers of which sizes should be used, what type of CPU-scheduling algorithm is desired, what the maximum number of processes to be supported is, and so on.
Once this information is determined, it can be used in several ways.
At one extreme, a system administrator can use it to modify a copy of the source code of the operating system.
Data declarations, initializations, and constants, along with conditional compilation, produce an output-object version of the operating system that is tailored to the system described.
At a slightly less tailored level, the system description can lead to the creation of tables and the selection of modules from a precompiled library.
These modules are linked together to form the generated operating system.
Selection allows the library to contain the device drivers for all supported I/O devices, but only those needed are linked into the operating system.
Because the system is not recompiled, system generation is faster, but the resulting system may be overly general.
At the other extreme, it is possible to construct a system that is completely table driven.
All the code is always part of the system, and selection occurs at execution time, rather than at compile or link time.
System generation involves simply creating the appropriate tables to describe the system.
The major differences among these approaches are the size and generality of the generated system and the ease of modifying it as the hardware conﬁguration changes.
Consider the cost of modifying the system to support a newly acquired graphics terminal or another disk drive.
Balanced against that cost, of course, is the frequency (or infrequency) of such changes.
After an operating system is generated, it must be made available for use by the hardware.
But how does the hardware know where the kernel is or how to load that kernel? The procedure of starting a computer by loading the kernel is known as booting the system.
On most computer systems, a small piece of code known as the bootstrap program or bootstrap loader locates the kernel, loads it into main memory, and starts its execution.
Some computer systems, such as PCs, use a two-step process in which a simple bootstrap loader fetches a more complex boot program from disk, which in turn loads the kernel.
When a CPU receives a reset event—for instance, when it is powered up or rebooted—the instruction register is loaded with a predeﬁned memory.
This program is in the form of read-only memory (ROM), because the RAM is in an unknown state at system startup.
Usually, one task is to run diagnostics to determine the state of the machine.
If the diagnostics pass, the program can continue with the booting steps.
It can also initialize all aspects of the system, from CPU registers to device controllers and the contents of main memory.
Some systems—such as cellular phones, tablets, and game consoles—store the entire operating system in ROM.
Storing the operating system in ROM is suitable for small operating systems, simple supporting hardware, and rugged operation.
A problem with this approach is that changing the bootstrap code requires changing the ROM hardware chips.
Some systems resolve this problem by using erasable programmable read-only memory (EPROM), which is readonly except when explicitly given a command to become writable.
All forms of ROM are also known as ﬁrmware, since their characteristics fall somewhere between those of hardware and those of software.
A problem with ﬁrmware in general is that executing code there is slower than executing code in RAM.
Some systems store the operating system in ﬁrmware and copy it to RAM for fast execution.
A ﬁnal issue with ﬁrmware is that it is relatively expensive, so usually only small amounts are available.
For large operating systems (including most general-purpose operating systems like Windows, Mac OS X, and UNIX) or for systems that change frequently, the bootstrap loader is stored in ﬁrmware, and the operating system is on disk.
In this case, the bootstrap runs diagnostics and has a bit of code that can read a single block at a ﬁxed location (say block zero) from disk into memory and execute the code from that boot block.
The program stored in the boot block may be sophisticated enough to load the entire operating system into memory and begin its execution.
More typically, it is simple code (as it ﬁts in a single disk block) and knows only the address on disk and length of the remainder of the bootstrap program.
All of the disk-bound bootstrap, and the operating system itself, can be easily changed by writing new versions to disk.
A disk that has a boot partition (more on that in Section 10.5.1) is called a boot disk or system disk.
Now that the full bootstrap program has been loaded, it can traverse the ﬁle system to ﬁnd the operating system kernel, load it into memory, and start its execution.
It is only at this point that the system is said to be running.
At the lowest level, system calls allow a running program to make requests from the operating system directly.
At a higher level, the command interpreter or shell provides a mechanism for a user to issue a request without writing a program.
Commands may come from ﬁles during batch-mode execution or directly from a terminal or desktop GUI when in an interactive or time-shared mode.
System programs are provided to satisfy many common user requests.
The system-call level must provide the basic functions, such as process control and ﬁle and device manipulation.
Higher-level requests, satisﬁed by the command interpreter or system programs, are translated into a sequence of system calls.
System services can be classiﬁed into several categories: program control, status requests, and I/O requests.
The design of a new operating system is a major task.
It is important that the goals of the system be well deﬁned before the design begins.
The type of system desired is the foundation for choices among various algorithms and strategies that will be needed.
Throughout the entire design cycle, we must be careful to separate policy decisions from implementation details (mechanisms)
This separation allows maximum ﬂexibility if policy decisions are to be changed later.
Once an operating system is designed, it must be implemented.
A system as large and complex as a modern operating system must be engineered carefully.
Designing a system as a sequence of layers or using a microkernel is considered a good technique.
Many operating systems now support dynamically loaded modules, which allow adding functionality to an operating system while it is executing.
Generally, operating systems adopt a hybrid approach that combines several different types of structures.
Debugging process and kernel failures can be accomplished through the use of debuggers and other tools that analyze core dumps.
Tools such as DTrace analyze production systems to ﬁnd bottlenecks and understand other system behavior.
To create an operating system for a particular machine conﬁguration, we must perform system generation.
For the computer system to begin running, the CPU must initialize and start executing the bootstrap program in ﬁrmware.
The bootstrap can execute the operating system directly if the operating system is also in the ﬁrmware, or it can complete a sequence in which it loads progressively smarter programs from ﬁrmware and disk until the operating system itself is loaded into memory and executed.
In which cases would it be impossible for user-level programs to provide these services? Explain your answer.
Brieﬂy describe the two categories, and discuss how they differ.
Identify a scenario in which it is unclear how to layer two system components that require tight coupling of their functionalities.
To optimize system-call performance, the kernel assembles routines within kernel space to minimize the path that the system call must take through the kernel.
This approach is the antithesis of the layered approach, in which the path through the kernel is extended to make building the operating system easier.
Discuss the pros and cons of the Synthesis approach to kernel design and system-performance optimization.
This program works by ﬁrst prompting the user for the name of the source and destination ﬁles.
Write this program using either the Windows or POSIX API.
Be sure to include all necessary error checking, including ensuring that the source ﬁle exists.
Once you have correctly designed and tested the program, if you used a system that supports it, run the program using a utility that traces system calls.
Linux systems provide the strace utility, and Solaris and Mac OS X systems use the dtrace command.
As Windows systems do not provide such features, you will have to trace through the Windows version of this program using a debugger.
In this project, you will learn how to create a kernel module and load it into the Linux kernel.
The project can be completed using the Linux virtual machine that is available with this text.
Although you may use an editor to write these C programs, you will have to use the terminal application to compile the programs, and you will have to enter commands on the command line to manage the modules in the kernel.
As you’ll discover, the advantage of developing kernel modules is that it is a relatively easy method of interacting with the kernel, thus allowing you to write programs that directly invoke kernel functions.
It is important for you to keep in mind that you are indeed writing kernel code that directly interacts with the kernel.
That normally means that any errors in the code could crash the system! However, since you will be using a virtual machine, any failures will at worst only require rebooting the system.
The ﬁrst part of this project involves following a series of steps for creating and inserting a module into the Linux kernel.
You can list all kernel modules that are currently loaded by entering the command.
This command will list the current kernel modules in three columns: name, size, and where the module is being used.
The following program (named simple.c and available with the source code for this text) illustrates a very basic kernel module that prints appropriate messages when the kernel module is loaded and unloaded.
The function simple init() is the module entry point, which represents the function that is invoked when the module is loaded into the kernel.
Similarly, the simple exit() function is the module exit point—the function that is called when the module is removed from the kernel.
The module entry point function must return an integer value, with 0 representing success and any other value representing failure.
Neither the module entry point nor the module exit point is passed any parameters.
The two following macros are used for registering the module entry and exit points with the kernel:
Notice how both the module entry and exit point functions make calls to the printk() function.
One difference between printf() and printk() is that printk() allows us to specify a priority ﬂag whose values are given in the <linux/printk.h> include ﬁle.
In this instance, the priority is KERN INFO, which is deﬁned as an informational message.
The ﬁnal lines—MODULE LICENSE(), MODULE DESCRIPTION(), and MODULE AUTHOR()—represent details regarding the software license, description of the module, and author.
For our purposes, we do not depend on this information, but we include it because it is standard practice in developing kernel modules.
This kernel module simple.c is compiled using the Makefile accompanying the source code with this project.
To compile the module, enter the following on the command line:
The following step illustrates inserting this module into the Linux kernel.
Kernel modules are loaded using theinsmod command, which is run as follows:
To check whether the module has loaded, enter the lsmod command and search for the module simple.
Recall that the module entry point is invoked when the module is inserted into the kernel.
To check the contents of this message in the kernel log buffer, enter the command.
You should see the message "Loading Module." Removing the kernel module involves invoking the rmmod command.
Be sure to check with the dmesg command to ensure the module has been removed.
Because the kernel log buffer can ﬁll up quickly, it often makes sense to clear the buffer periodically.
Proceed through the steps described above to create the kernel module and to load and unload the module.
Be sure to check the contents of the kernel log buffer using dmesg to ensure you have properly followed the steps.
The second part of this project involves modifying the kernel module so that it uses the kernel linked-list data structure.
In Section 1.10, we covered various data structures that are common in operating systems.
Here, we explore using the circular, doubly linked list that is available to kernel developers.
Initially, you must deﬁne a struct containing the elements that are to be inserted in the linked list.
The list head structure is deﬁned in the include ﬁle <linux/types.h>
Its intention is to embed the linked list within the nodes that comprise the list.
This list head structure is quite simple—it merely holds two members, next and prev, that point to the next and previous entries in the list.
By embedding the linked list within the structure, Linux makes it possible to manage the data structure with a series of macro functions.
We can declare a list head object, which we use as a reference to the head of the list by using the LIST HEAD() macro.
This macro deﬁnes and initializes the variable birthday list, which is of type struct list head.
We create and initialize instances of struct birthday as follows:
The kmalloc() function is the kernel equivalent of the user-level malloc() function for allocating memory, except that kernel memory is being allocated.
The macro INIT LIST HEAD() initializes the list member in struct birthday.
We can then add this instance to the end of the linked list using the list add tail() macro:
Traversing the list involves using the list for each entry() Macro, which accepts three parameters:
A pointer to the head of the list being iterated over.
The name of the variable containing the list head structure.
Removing elements from the list involves using the list del() macro, which is passed a pointer to struct list head.
This removes element from the list while maintaining the structure of the remainder of the list.
Perhaps the simplest approach for removing all elements from a linked list is to remove each element as you traverse the list.
The macro list for each entry safe() behaves much like list for each entry()
This is necessary for preserving the structure of the list.
Notice that after deleting each element, we return memory that was previously allocated with kmalloc() back to the kernel with the call to kfree()
Careful memory management—which includes releasing memory to prevent memory leaks—is crucial when developing kernel-level code.
In the module entry point, create a linked list containing ﬁvestruct birthday elements.
Traverse the linked list and output its contents to the kernel log buffer.
Invoke the dmesg command to ensure the list is properly constructed once the kernel module has been loaded.
In the module exit point, delete the elements from the linked list and return the free memory back to the kernel.
Again, invoke the dmesg command to check that the list has been removed once the kernel module has been unloaded.
Windows XP internals are described in [Russinovich and Solomon (2009)]
In particular, [Love (2010)] covers Linux kernel modules as well as kernel data structures.
Several UNIX systems—including Mach—are treated in detail in [Vahalia (1996)]
Mac OS X is presented at http://www.apple.com/macosx and in [Singh (2007)]
A process can be thought of as a program in execution.
These resources are allocated to the process either when it is created or while it is executing.
A process is the unit of work in most systems.
Systems consist of a collection of processes: operating-system processes execute system code, and user processes execute user code.
Although traditionally a process contained only a single thread of control as it ran,mostmodern operating systems nowsupport processes that have multiple threads.
The operating system is responsible for several important aspects of process and thread management: the creation and deletion of both user and system processes; the scheduling of processes; and the provision of mechanisms for synchronization, communication, and deadlock handling for processes.
Early computers allowed only one program to be executed at a time.
This program had complete control of the system and had access to all the system’s resources.
In contrast, contemporary computer systems allow multiple programs to be loaded into memory and executed concurrently.
A process is the unit of work in a modern time-sharing system.
The more complex the operating system is, the more it is expected to do on behalf of its users.
Although its main concern is the execution of user programs, it also needs to take care of various system tasks that are better left outside the kernel itself.
A system therefore consists of a collection of processes: operatingsystem processes executing system code and user processes executing user code.
Potentially, all these processes can execute concurrently, with the CPU (or CPUs) multiplexed among them.
By switching the CPU between processes, the operating system can make the computer more productive.
In this chapter, you will read about what processes are and how they work.
To introduce the notion of a process—a program in execution, which forms the basis of all computation.
To describe the various features of processes, including scheduling, creation, and termination.
To explore interprocess communication using shared memory and message passing.
A question that arises in discussing operating systems involves what to call all the CPU activities.
Even on a single-user system, a user may be able to run several programs at one time: a word processor, a Web browser, and an e-mail package.
And even if a user can execute only one program at a time, such as on an embedded device that does not support multitasking, the operating system may need to support its own internal programmed activities, such as memory management.
In many respects, all these activities are similar, so we call all of them processes.
The terms job and process are used almost interchangeably in this text.
Although we personally prefer the term process, much of operating-system theory and terminology was developed during a time when the major activity of operating systems was job processing.
It would be misleading to avoid the use of commonly accepted terms that include the word job (such as job scheduling) simply because process has superseded job.
Informally, as mentioned earlier, a process is a program in execution.
A process is more than the program code, which is sometimes known as the text section.
It also includes the current activity, as represented by the value of the program counter and the contents of the processor’s registers.
A process generally also includes the process stack, which contains temporary data (such as function parameters, return addresses, and local variables), and a data section, which contains global variables.
A process may also include a heap, which is memory that is dynamically allocated during process run time.
The structure of a process in memory is shown in Figure 3.1
We emphasize that a program by itself is not a process.
A program is a passive entity, such as a ﬁle containing a list of instructions stored on disk (often called an executable ﬁle)
In contrast, a process is an active entity, with a program counter specifying the next instruction to execute and a set of associated resources.
A program becomes a process when an executable ﬁle is loaded into memory.
Although two processes may be associated with the same program, they are nevertheless considered two separate execution sequences.
For instance, several users may be running different copies of the mail program, or the same user may invoke many copies of the web browser program.
Each of these is a separate process; and although the text sections are equivalent, the data, heap, and stack sections vary.
It is also common to have a process that spawns many processes as it runs.
Note that a process itself can be an execution environment for other code.
In most circumstances, an executable Java program is executed within the Java virtual machine (JVM)
The JVM executes as a process that interprets the loaded Java code and takes actions (via native machine instructions) on behalf of that code.
For example, to run the compiled Java program Program.class, we would enter.
The concept is the same as simulation, except that the code, instead of being written for a different instruction set, is written in the Java language.
The state of a process is deﬁned in part by the current activity of that process.
A process may be in one of the following states:
The process is waiting to be assigned to a processor.
These names are arbitrary, and they vary across operating systems.
The states that they represent are found on all systems, however.
It is important to realize that only one process can be running on any processor at any instant.
The state diagram corresponding to these states is presented in Figure 3.2
Each process is represented in the operating system by a process control block (PCB)—also called a task control block.
It contains many pieces of information associated with a speciﬁc process, including these:
The state may be new, ready, running, waiting, halted, and so on.
The counter indicates the address of the next instruction to be executed for this process.
The registers vary in number and type, depending on the computer architecture.
They include accumulators, index registers, stack pointers, and general-purpose registers, plus any condition-code information.
Along with the program counter, this state information must be saved when an interrupt occurs, to allow the process to be continued correctly afterward (Figure 3.4)
This information includes a process priority, pointers to scheduling queues, and any other scheduling parameters.
This information may include such items as the value of the base and limit registers and the page tables, or the segment tables, depending on the memory system used by the operating system (Chapter 8)
Figure 3.4 Diagram showing CPU switch from process to process.
This information includes the amount of CPU and real time used, time limits, account numbers, job or process numbers, and so on.
This information includes the list of I/O devices allocated to the process, a list of open ﬁles, and so on.
In brief, the PCB simply serves as the repository for any information that may vary from process to process.
The process model discussed so far has implied that a process is a program that performs a single thread of execution.
For example, when a process is running a word-processor program, a single thread of instructions is being executed.
This single thread of control allows the process to perform only one task at a time.
The user cannot simultaneously type in characters and run the spell checker within the same process, for example.
Most modern operating systems have extended the process concept to allow a process to have multiple threads of execution and thus to perform more than one task at a time.
This feature is especially beneﬁcial on multicore systems, where multiple threads can run in parallel.
On a system that supports threads, the PCB is expanded to include information for each thread.
Other changes throughout the system are also needed to support threads.
The process control block in the Linux operating system is represented by the C structure task struct, which is found in the <linux/sched.h> include ﬁle in the kernel source-code directory.
This structure contains all the necessary information for representing a process, including the state of the process, scheduling and memory-management information, list of open ﬁles, and pointers to the process’s parent and a list of its children and siblings.
A process’s parent is the process that created it; its children are any processes that it creates.
For example, the state of a process is represented by the ﬁeld long state in this structure.
Within the Linux kernel, all active processes are represented using a doubly linked list of task struct.
The kernel maintains a pointercurrent—to the process currently executing on the system, as shown below:
As an illustration of how the kernel might manipulate one of the ﬁelds in the task struct for a speciﬁed process, let’s assume the system would like to change the state of the process currently running to the value new state.
If current is a pointer to the process currently executing, its state is changed with the following:
The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization.
The objective of time sharing is to switch the CPU among processes so frequently that users can interact with each program.
Figure 3.5 The ready queue and various I/O device queues.
To meet these objectives, the process scheduler selects an available process (possibly from a set of several available processes) for program execution on the CPU.
For a single-processor system, there will never be more than one running process.
If there are more processes, the rest will have to wait until the CPU is free and can be rescheduled.
As processes enter the system, they are put into a job queue, which consists of all processes in the system.
The processes that are residing in main memory and are ready and waiting to execute are kept on a list called the ready queue.
A ready-queue header contains pointers to the ﬁrst and ﬁnal PCBs in the list.
Each PCB includes a pointer ﬁeld that points to the next PCB in the ready queue.
When a process is allocated the CPU, it executes for a while and eventually quits, is interrupted, or waits for the occurrence of a particular event, such as the completion of an I/O request.
Suppose the process makes an I/O request to a shared device, such as a disk.
Since there are many processes in the system, the disk may be busy with the I/O request of some other process.
The process therefore may have to wait for the disk.
The list of processes waiting for a particular I/O device is called a device queue.
A common representation of process scheduling is a queueing diagram, such as that in Figure 3.6
Two types of queues are present: the ready queue and a set of device queues.
The circles represent the resources that serve the queues, and the arrows indicate the ﬂow of processes in the system.
A new process is initially put in the ready queue.
It waits there until it is selected for execution, or dispatched.
Once the process is allocated the CPU and is executing, one of several events could occur:
The process could issue an I/O request and then be placed in an I/O queue.
The process could create a new child process and wait for the child’s.
The process could be removed forcibly from the CPU, as a result of an interrupt, and be put back in the ready queue.
In the ﬁrst two cases, the process eventually switches from the waiting state to the ready state and is then put back in the ready queue.
A process continues this cycle until it terminates, at which time it is removed from all queues and has its PCB and resources deallocated.
A process migrates among the various scheduling queues throughout its lifetime.
The operating system must select, for scheduling purposes, processes from these queues in some fashion.
The selection process is carried out by the appropriate scheduler.
Often, in a batch system, more processes are submitted than can be executed immediately.
These processes are spooled to a mass-storage device (typically a disk), where they are kept for later execution.
The long-term scheduler, or job scheduler, selects processes from this pool and loads them into memory for.
The short-term scheduler, or CPU scheduler, selects from among the processes that are ready to execute and allocates the CPU to one of them.
The primary distinction between these two schedulers lies in frequency of execution.
The short-term scheduler must select a new process for the CPU frequently.
A process may execute for only a few milliseconds before waiting for an I/O request.
Often, the short-term scheduler executes at least once every 100 milliseconds.
Because of the short time between executions, the short-term scheduler must be fast.
The long-term scheduler executes much less frequently; minutes may separate the creation of one new process and the next.
The long-term scheduler controls the degree of multiprogramming (the number of processes in memory)
If the degree of multiprogramming is stable, then the average rate of process creation must be equal to the average departure rate of processes leaving the system.
Thus, the long-term scheduler may need to be invoked only when a process leaves the system.
Because of the longer interval between executions, the long-term scheduler can afford to take more time to decide which process should be selected for execution.
It is important that the long-term scheduler make a careful selection.
In general, most processes can be described as either I/O bound or CPU bound.
An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations.
A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations.
It is important that the long-term scheduler select a good process mix of I/O-bound and CPU-bound processes.
If all processes are I/O bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do.
If all processes are CPU bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced.
The system with the best performance will thus have a combination of CPU-bound and I/O-bound processes.
On some systems, the long-term scheduler may be absent or minimal.
For example, time-sharing systems such as UNIX and Microsoft Windows systems often have no long-term scheduler but simply put every new process in memory for the short-term scheduler.
The stability of these systems depends either on a physical limitation (such as the number of available terminals) or on the self-adjusting nature of human users.
If performance declines to unacceptable levels on a multiuser system, some users will simply quit.
Some operating systems, such as time-sharing systems, may introduce an additional, intermediate level of scheduling.
The key idea behind a medium-term scheduler is that sometimes it can be advantageous to remove a process from memory (and from active contention for the CPU) and thus reduce the degree of multiprogramming.
Later, the process can be reintroduced into memory, and its execution can be continued where it left off.
The process is swapped out, and is later swapped in, by the medium-term scheduler.
Swapping may be necessary to improve the process mix or because a change in memory requirements has overcommitted available memory, requiring memory to be freed up.
Figure 3.7 Addition of medium-term scheduling to the queueing diagram.
As mentioned in Section 1.2.1, interrupts cause the operating system to change a CPU from its current task and to run a kernel routine.
When an interrupt occurs, the system needs to save the current context of the process running on the CPU so that it can restore that context when its processing is done, essentially suspending the process and then resuming it.
The context is represented in the PCB of the process.
It includes the value of the CPU registers, the process state (see Figure 3.2), and memory-management information.
Generically, we perform a state save of the current state of the CPU, be it in kernel or user mode, and then a state restore to resume operations.
Switching the CPU to another process requires performing a state save of the current process and a state restore of a different process.
When a context switch occurs, the kernel saves the context of the old process in its PCB and loads the saved context of the new process scheduled to run.
Context-switch time is pure overhead, because the system does no useful work while switching.
Switching speed varies from machine to machine, depending on the memory speed, the number of registers that must be copied, and the existence of special instructions (such as a single instruction to load or store all registers)
For instance, some processors (such as the Sun UltraSPARC) provide multiple sets of registers.
A context switch here simply requires changing the pointer to the current register set.
Of course, if there are more active processes than there are register sets, the system resorts to copying register data to and from memory, as before.
Also, the more complex the operating system, the greater the amount of work that must be done during a context switch.
As we will see in Chapter 8, advanced memory-management techniques may require that extra data be switched with each context.
For instance, the address space of the current process must be preserved as the space of the next task is prepared for use.
How the address space is preserved, and what amount of work is needed to preserve it, depend on the memory-management method of the operating system.
Because of the constraints imposed on mobile devices, early versions of iOS did not provide user-application multitasking; only one application runs in the foreground and all other user applications are suspended.
Operatingsystem tasks were multitasked because they were written by Apple and well behaved.
However, beginning with iOS 4, Apple now provides a limited form of multitasking for user applications, thus allowing a single foreground application to run concurrently with multiple background applications.
On a mobile device, the foreground application is the application currently open and appearing on the display.
The background application remains in memory, but does not occupy the display screen.
The iOS 4 programming API provides support for multitasking, thus allowing a process to run in the background without being suspended.
However, it is limited and only available for a limited number of application types, including applications.
Apple probably limits multitasking due to battery life and memory use concerns.
The CPU certainly has the features to support multitasking, but Apple chooses to not take advantage of some of them in order to better manage resource use.
Android does not place such constraints on the types of applications that can run in the background.
If an application requires processing while in the background, the application must use a service, a separate application component that runs on behalf of the background process.
Consider a streaming audio application: if the application moves to the background, the service continues to send audio ﬁles to the audio device driver on behalf of the background application.
In fact, the service will continue to run even if the background application is suspended.
Services do not have a user interface and have a small memory footprint, thus providing an efﬁcient technique for multitasking in a mobile environment.
The processes in most systems can execute concurrently, and they may be created and deleted dynamically.
Thus, these systems must provide a mechanism for process creation and termination.
In this section, we explore the mechanisms involved in creating processes and illustrate process creation on UNIX and Windows systems.
During the course of execution, a process may create several new processes.
As mentioned earlier, the creating process is called a parent process, and the new processes are called the children of that process.
Each of these new processes may in turn create other processes, forming a tree of processes.
Most operating systems (including UNIX, Linux, and Windows) identify processes according to a unique process identiﬁer (or pid), which is typically an integer number.
The pid provides a unique value for each process in the system, and it can be used as an index to access various attributes of a process within the kernel.
Figure 3.8 illustrates a typical process tree for the Linux operating system, showing the name of each process and its pid.
We use the term process rather loosely, as Linux prefers the term task instead.
Theinitprocess (which always has a pid of 1) serves as the root parent process for all user processes.
Once the system has booted, theinitprocess can also create various user processes, such as a web or print server, an ssh server, and the like.
In Figure 3.8, we see two children of init—kthreadd and sshd.
The kthreadd process is responsible for creating additional processes that perform tasks on behalf of the kernel (in this situation, khelper and pdflush)
The sshd process is responsible for managing clients that connect to the system by using ssh (which is short for secure shell)
Theloginprocess is responsible for managing clients that directly log onto the system.
Using the bash command-line interface, this user has created the process ps as well as the emacs editor.
On UNIX and Linux systems, we can obtain a listing of processes by using the ps command.
It is easy to construct a process tree similar to the one shown in Figure 3.8 by recursively tracing parent processes all the way to the init process.
Figure 3.8 A tree of processes on a typical Linux system.
In general, when a process creates a child process, that child process will need certain resources (CPU time, memory, ﬁles, I/O devices) to accomplish its task.
A child process may be able to obtain its resources directly from the operating system, or it may be constrained to a subset of the resources of the parent process.
The parent may have to partition its resources among its children, or it may be able to share some resources (such as memory or ﬁles) among several of its children.
Restricting a child process to a subset of the parent’s resources prevents any process from overloading the system by creating too many child processes.
In addition to supplying various physical and logical resources, the parent process may pass along initialization data (input) to the child process.
For example, consider a process whose function is to display the contents of a ﬁle —say, image.jpg—on the screen of a terminal.
When the process is created, it will get, as an input from its parent process, the name of the ﬁle image.jpg.
Using that ﬁle name, it will open the ﬁle and write the contents out.
It may also get the name of the output device.
On such a system, the new process may get two open ﬁles, image.jpg and the terminal device, and may simply transfer the datum between the two.
When a process creates a new process, two possibilities for execution exist:
The parent waits until some or all of its children have terminated.
There are also two address-space possibilities for the new process:
The child process is a duplicate of the parent process (it has the same program and data as the parent)
The child process has a new program loaded into it.
To illustrate these differences, let’s ﬁrst consider the UNIX operating system.
In UNIX, as we’ve seen, each process is identiﬁed by its process identiﬁer, which is a unique integer.
A new process is created by the fork() system call.
The new process consists of a copy of the address space of the original process.
This mechanism allows the parent process to communicate easily with its child process.
Both processes (the parent and the child) continue execution at the instruction after the fork(), with one difference: the return code for the fork() is zero for the new (child) process, whereas the (nonzero) process identiﬁer of the child is returned to the parent.
After a fork() system call, one of the two processes typically uses the exec() system call to replace the process’s memory space with a new program.
The exec() system call loads a binary ﬁle into memory (destroying the memory image of the program containing the exec() system call) and starts its execution.
In this manner, the two processes are able to communicate and then go their separate ways.
The parent can then create more children; or, if it has nothing else to do while the child runs, it can issue a wait() system call to move itself off the ready queue until the termination of the child.
Figure 3.9 Creating a separate process using the UNIX fork() system call.
The C program shown in Figure 3.9 illustrates the UNIX system calls previously described.
We now have two different processes running copies of the same program.
The only difference is that the value of pid (the process identiﬁer) for the child process is zero, while that for the parent is an integer value greater than zero (in fact, it is the actual pid of the child process)
The child process inherits privileges and scheduling attributes from the parent, as well certain resources, such as open ﬁles.
The child process then overlays its address space with the UNIX command /bin/ls (used to get a directory listing) using the execlp() system call (execlp() is a version of the exec() system call)
The parent waits for the child process to complete with the wait() system call.
When the child process completes (by either implicitly or explicitly invoking exit()), the parent process resumes from the call to wait(), where it completes using the exit() system call.
Of course, there is nothing to prevent the child from not invoking exec() and instead continuing to execute as a copy of the parent process.
In this scenario, the parent and child are concurrent processes running the same code.
Because the child is a copy of the parent, each process has its own copy of any data.
As an alternative example, we next consider process creation in Windows.
Processes are created in the Windows API using the CreateProcess() function, which is similar to fork() in that a parent creates a new child process.
However, whereas fork() has the child process inheriting the address space of its parent, CreateProcess() requires loading a speciﬁed program into the address space of the child process at process creation.
Furthermore, whereas fork() is passed no parameters, CreateProcess() expects no fewer than ten parameters.
The C program shown in Figure 3.11 illustrates the CreateProcess() function, which creates a child process that loads the application mspaint.exe.
We opt for many of the default values of the ten parameters passed to CreateProcess()
Readers interested in pursuing the details of process creation and management in the Windows API are encouraged to consult the bibliographical notes at the end of this chapter.
The two parameters passed to the CreateProcess() function are instances of the STARTUPINFO and PROCESS INFORMATION structures.
The PROCESS INFORMATION structure contains a handle and the identiﬁers to the newly created process and its thread.
We invoke the ZeroMemory() function to allocate memory for each of these structures before proceeding with CreateProcess()
The ﬁrst two parameters passed to CreateProcess() are the application name and command-line parameters.
If the application name is NULL (as it is in this case), the command-line parameter speciﬁes the application to load.
In this instance, we are loading the Microsoft Windows mspaint.exe application.
Beyond these two initial parameters, we use the default parameters for inheriting process and thread handles as well as specifying that there will be no creation ﬂags.
We also use the parent’s existing environment block and starting directory.
Last, we provide two pointers to the STARTUPINFO and PROCESS INFORMATION structures created at the beginning of the program.
In Figure 3.9, the parent process waits for the child to complete by invoking the wait() system call.
A process terminates when it ﬁnishes executing its ﬁnal statement and asks the operating system to delete it by using the exit() system call.
At that point, the process may return a status value (typically an integer) to its parent process (via the wait() system call)
All the resources of the process—including physical and virtual memory, open ﬁles, and I/O buffers—are deallocated by the operating system.
A process can cause the termination of another process via an appropriate system call (for example, TerminateProcess() in Windows)
Note that a parent needs to know the identities of its children if it is to terminate them.
Thus, when one process creates a new process, the identity of the newly created process is passed to the parent.
A parent may terminate the execution of one of its children for a variety of reasons, such as these:
The child has exceeded its usage of some of the resources that it has been allocated.
To determine whether this has occurred, the parent must have a mechanism to inspect the state of its children.
The task assigned to the child is no longer required.
The parent is exiting, and the operating system does not allow a child to.
Some systems do not allow a child to exist if its parent has terminated.
In such systems, if a process terminates (either normally or abnormally), then all its children must also be terminated.
This phenomenon, referred to as cascading termination, is normally initiated by the operating system.
To illustrate process execution and termination, consider that, in Linux and UNIX systems, we can terminate a process by using the exit() system call, providing an exit status as a parameter:
In fact, under normal termination, exit() may be called either directly (as shown above) or indirectly (by a return statement in main())
A parent process may wait for the termination of a child process by using the wait() system call.
The wait() system call is passed a parameter that allows the parent to obtain the exit status of the child.
This system call also returns the process identiﬁer of the terminated child so that the parent can tell which of its children has terminated:
When a process terminates, its resources are deallocated by the operating system.
However, its entry in the process table must remain there until the parent calls wait(), because the process table contains the process’s exit status.
A process that has terminated, but whose parent has not yet called wait(), is known as a zombie process.
All processes transition to this state when they terminate, but generally they exist as zombies only brieﬂy.
Once the parent calls wait(), the process identiﬁer of the zombie process and its entry in the process table are released.
Now consider what would happen if a parent did not invoke wait() and instead terminated, thereby leaving its child processes as orphans.
Linux and UNIX address this scenario by assigning the init process as the new parent to.
Recall from Figure 3.8 that the init process is the root of the process hierarchy in UNIX and Linux systems.
The init process periodically invokes wait(), thereby allowing the exit status of any orphaned process to be collected and releasing the orphan’s process identiﬁer and process-table entry.
Processes executing concurrently in the operating system may be either independent processes or cooperating processes.
A process is independent if it cannot affect or be affected by the other processes executing in the system.
Any process that does not share data with any other process is independent.
A process is cooperating if it can affect or be affected by the other processes executing in the system.
Clearly, any process that shares data with other processes is a cooperating process.
There are several reasons for providing an environment that allows process cooperation:
Since several users may be interested in the same piece of information (for instance, a shared ﬁle), we must provide an environment to allow concurrent access to such information.
If we want a particular task to run faster, we must break it into subtasks, each of which will be executing in parallel with the others.
Notice that such a speedup can be achieved only if the computer has multiple processing cores.
Even an individual user may work on many tasks at the same time.
For instance, a user may be editing, listening to music, and compiling in parallel.
Cooperating processes require an interprocess communication (IPC) mechanism that will allow them to exchange data and information.
There are two fundamental models of interprocess communication: shared memory and message passing.
In the shared-memory model, a region of memory that is shared by cooperating processes is established.
Processes can then exchange information by reading and writing data to the shared region.
In the message-passing model, communication takes place by means of messages exchanged between the cooperating processes.
Both of the models just mentioned are common in operating systems, and many systems implement both.
Message passing is useful for exchanging smaller amounts of data, because no conﬂicts need be avoided.
Message passing is also easier to implement in a distributed system than shared memory.
Although there are systems that provide distributed shared memory, we do not consider them in this text.
Shared memory can be faster than message passing, since message-passing systems are typically implemented using system calls.
Many websites contain active content such as JavaScript, Flash, and HTML5 to provide a rich and dynamic web-browsing experience.
Unfortunately, these web applications may also contain software bugs, which can result in sluggish response times and can even cause the web browser to crash.
This isn’t a big problem in a web browser that displays content from only one website.
But most contemporary web browsers provide tabbed browsing, which allows a single instance of a web browser application to open several websites at the same time, with each site in a separate tab.
To switch between the different sites , a user need only click on the appropriate tab.
A problem with this approach is that if a web application in any tab crashes, the entire process—including all other tabs displaying additional websites —crashes as well.
Google’s Chrome web browser was designed to address this issue by using a multiprocess architecture.
Chrome identiﬁes three different types of processes: browser, renderers, and plug-ins.
The browser process is responsible for managing the user interface as well as disk and network I/O.
A new browser process is created when Chrome is started.
Thus, they contain the logic for handling HTML, Javascript, images, and so forth.
As a general rule, a new renderer process is created for each website opened in a new tab, and so several renderer processes may be active at the same time.
A plug-in process is created for each type of plug-in (such as Flash or QuickTime) in use.
Plug-in processes contain the code for the plug-in as well as additional code that enables the plug-in to communicate with associated renderer processes and the browser process.
The advantage of the multiprocess approach is that websites run in isolation from one another.
If one website crashes, only its renderer process is affected; all other processes remain unharmed.
Furthermore, renderer processes run in a sandbox, which means that access to disk and network I/O is restricted, minimizing the effects of any security exploits.
Once shared memory is established, all accesses are treated as routine memory accesses, and no assistance from the kernel is required.
Recent research on systems with several processing cores indicates that message passing provides better performance than shared memory on such systems.
Shared memory suffers from cache coherency issues, which arise because shared data migrate among the several caches.
As the number of processing cores on systems increases, it is possible that we will see message passing as the preferred mechanism for IPC.
In the remainder of this section, we explore shared-memory and messagepassing systems in more detail.
Interprocess communication using shared memory requires communicating processes to establish a region of shared memory.
Typically, a shared-memory region resides in the address space of the process creating the shared-memory segment.
Other processes that wish to communicate using this shared-memory segment must attach it to their address space.
Recall that, normally, the operating system tries to prevent one process from accessing another process’s memory.
Shared memory requires that two or more processes agree to remove this restriction.
They can then exchange information by reading and writing data in the shared areas.
The form of the data and the location are determined by these processes and are not under the operating system’s control.
The processes are also responsible for ensuring that they are not writing to the same location simultaneously.
To illustrate the concept of cooperating processes, let’s consider the producer–consumer problem, which is a common paradigm for cooperating processes.
A producer process produces information that is consumed by a consumer process.
For example, a compiler may produce assembly code that is consumed by an assembler.
The assembler, in turn, may produce object modules that are consumed by the loader.
We generally think of a server as a producer and a client as a consumer.
For example, a web server produces (that is, provides) HTML ﬁles and images, which are consumed (that is, read) by the client web browser requesting the resource.
To allow producer and consumer processes to run concurrently, we must have available a buffer of items that can be ﬁlled by the producer and emptied by the consumer.
This buffer will reside in a region of memory that is shared by the producer and consumer processes.
A producer can produce one item while the consumer is consuming another item.
The producer and consumer must be synchronized, so that the consumer does not try to consume an item that has not yet been produced.
The unbounded buffer places no practical limit on the size of the buffer.
The consumer may have to wait for new items, but the producer can always produce new items.
In this case, the consumer must wait if the buffer is empty, and the producer must wait if the buffer is full.
Let’s look more closely at how the bounded buffer illustrates interprocess communication using shared memory.
The following variables reside in a region of memory shared by the producer and consumer processes:
The shared buffer is implemented as a circular array with two logical pointers: in and out.
The variable in points to the next free position in the buffer; out points to the ﬁrst full position in the buffer.
The buffer is empty when in == out; the buffer is full when ((in + 1) % BUFFER SIZE) == out.
The consumer process has a local variable next consumed in which the item to be consumed is stored.
One issue this illustration does not address concerns the situation in which both the producer process and the consumer process attempt to access the shared buffer concurrently.
In Chapter 5, we discuss how synchronization among cooperating processes can be implemented effectively in a sharedmemory environment.
In Section 3.4.1, we showed how cooperating processes can communicate in a shared-memory environment.
The scheme requires that these processes share a region of memory and that the code for accessing and manipulating the shared memory be written explicitly by the application programmer.
Another way to achieve the same effect is for the operating system to provide the means for cooperating processes to communicate with each other via a message-passing facility.
Message passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space.
It is particularly useful in a distributed environment, where the communicating processes may reside on different computers connected by a network.
For example, an Internet chat program could be designed so that chat participants communicate with one another by exchanging messages.
Messages sent by a process can be either ﬁxed or variable in size.
If only ﬁxed-sized messages can be sent, the system-level implementation is straightforward.
This restriction, however, makes the task of programming more difﬁcult.
This is a common kind of tradeoff seen throughout operating-system design.
If processes P and Q want to communicate, they must send messages to and receive messages from each other: a communication link must exist between them.
This link can be implemented in a variety of ways.
We are concerned here not with the link’s physical implementation (such as shared memory, hardware bus, or network, which are covered in Chapter 17) but rather with its logical implementation.
Here are several methods for logically implementing a link and the send()/receive() operations:
We look at issues related to each of these features next.
Processes that want to communicate must have a way to refer to each other.
Under direct communication, each process that wants to communicate must explicitly name the recipient or sender of the communication.
In this scheme, the send() and receive() primitives are deﬁned as:
A communication link in this scheme has the following properties:
A link is established automatically between every pair of processes that want to communicate.
The processes need to know only each other’s identity to communicate.
Between each pair of processes, there exists exactly one link.
This scheme exhibits symmetry in addressing; that is, both the sender process and the receiver process must name the other to communicate.
Here, only the sender names the recipient; the recipient is not required to name the sender.
In this scheme, the send() and receive() primitives are deﬁned as follows:
The disadvantage in both of these schemes (symmetric and asymmetric) is the limited modularity of the resulting process deﬁnitions.
Changing the identiﬁer of a process may necessitate examining all other process deﬁnitions.
All references to the old identiﬁer must be found, so that they can be modiﬁed to the new identiﬁer.
In general, any such hard-coding techniques, where identiﬁers must be explicitly stated, are less desirable than techniques involving indirection, as described next.
With indirect communication, the messages are sent to and received from mailboxes, or ports.
A mailbox can be viewed abstractly as an object into which messages can be placed by processes and from which messages can be removed.
For example, POSIX message queues use an integer value to identify a mailbox.
A process can communicate with another process via a number of different mailboxes, but two processes can communicate only if they have a shared mailbox.
In this scheme, a communication link has the following properties:
A link is established between a pair of processes only if both members of the pair have a shared mailbox.
A link may be associated with more than two processes.
Between each pair of communicating processes, a number of different links.
Which process will receive the message sent by P1? The answer depends on which of the following methods we choose:
Allow a link to be associated with two processes at most.
Allow at most one process at a time to execute a receive() operation.
Allow the system to select arbitrarily which process will receive the.
The system may deﬁne an algorithm for selecting which process will receive the message (for example, round robin, where processes take turns receiving messages)
A mailbox may be owned either by a process or by the operating system.
If the mailbox is owned by a process (that is, the mailbox is part of the address space of the process), then we distinguish between the owner (which can only receive messages through this mailbox) and the user (which can only send messages to the mailbox)
Since each mailbox has a unique owner, there can be no confusion about which process should receive a message sent to this mailbox.
When a process that owns a mailbox terminates, the mailbox.
Any process that subsequently sends a message to this mailbox must be notiﬁed that the mailbox no longer exists.
In contrast, a mailbox that is owned by the operating system has an existence of its own.
It is independent and is not attached to any particular process.
The operating system then must provide a mechanism that allows a process to do the following:
The process that creates a new mailbox is that mailbox’s owner by default.
Initially, the owner is the only process that can receive messages through this mailbox.
However, the ownership and receiving privilege may be passed to other processes through appropriate system calls.
Of course, this provision could result in multiple receivers for each mailbox.
Communication between processes takes place through calls to send() and receive() primitives.
Message passing may be either blocking or nonblockingalso known as synchronous and asynchronous.
Throughout this text, you will encounter the concepts of synchronous and asynchronous behavior in relation to various operating-system algorithms.
The sending process is blocked until the message is received by the receiving process or by the mailbox.
When both send() and receive() are blocking, we have a rendezvous between the sender and the receiver.
The solution to the producer–consumer problem becomes trivial when we use blocking send() and receive() statements.
The producer merely invokes the blocking send() call and waits until the message is delivered to either the receiver or the mailbox.
Likewise, when the consumer invokes receive(), it blocks until a message is available.
Whether communication is direct or indirect, messages exchanged by communicating processes reside in a temporary queue.
The queue has a maximum length of zero; thus, the link cannot have any messages waiting in it.
In this case, the sender must block until the recipient receives the message.
The queue has ﬁnite length n; thus, at most n messages can reside in it.
If the queue is not full when a new message is sent, the message is placed in the queue (either the message is copied or a pointer to the message is kept), and the sender can continue execution without waiting.
If the link is full, the sender must block until space is available in the queue.
The queue’s length is potentially inﬁnite; thus, any number of messages can wait in it.
The zero-capacity case is sometimes referred to as a message system with no buffering.
The other cases are referred to as systems with automatic buffering.
We ﬁrst cover the POSIX API for shared memory and then discuss message passing in the Mach operating system.
We conclude with Windows, which interestingly uses shared memory as a mechanism for providing certain types of message passing.
Several IPC mechanisms are available for POSIX systems, including shared memory and message passing.
The ﬁrst parameter speciﬁes the name of the shared-memory object.
Processes that wish to access this shared memory must refer to the object by this name.
The subsequent parameters specify that the shared-memory object is to be created if it does not yet exist (O CREAT) and that the object is open for reading and writing (O RDRW)
The last parameter establishes the directory permissions of the shared-memory object.
A successful call toshm open() returns an integer ﬁle descriptor for the shared-memory object.
Once the object is established, the ftruncate() function is used to conﬁgure the size of the object in bytes.
It also returns a pointer to the memory-mapped ﬁle that is used for accessing the shared-memory object.
The producer establishes a sharedmemory object and writes to shared memory, and the consumer reads from shared memory.
The producer, shown in Figure 3.17, creates a shared-memory object named OS and writes the infamous string "Hello World!" to shared memory.
The program memory-maps a shared-memory object of the speciﬁed size and allows writing to the object.
The ﬂag MAP SHARED speciﬁes that changes to the shared-memory object will be visible to all processes sharing the object.
Notice that we write to the shared-memory object by calling the sprintf() function and writing the formatted string to the pointer ptr.
After each write, we must increment the pointer by the number of bytes written.
The consumer process, shown in Figure 3.18, reads and outputs the contents of the shared memory.
The consumer also invokes the shm unlink() function, which removes the shared-memory segment after the consumer has accessed it.
We provide further exercises using the POSIX shared-memory API in the programming exercises at the end of this chapter.
Additionally, we provide more detailed coverage of memory mapping in Section 9.7
As an example of message passing, we next consider the Mach operating system.
You may recall that we introduced Mach in Chapter 2 as part of the Mac OS X operating system.
The Mach kernel supports the creation and destruction of multiple tasks, which are similar to processes but have multiple threads of control and fewer associated resources.
Most communication in Machincluding all intertask information—is carried out by messages.
Messages are sent to and received from mailboxes, called ports in Mach.
When a task is created, two special mailboxes—the Kernel mailbox and the Notify mailbox—are also created.
The kernel uses the Kernel mailbox to communicate with the task and sends notiﬁcation of event occurrences to the Notify port.
The msg send() call sends a message to a mailbox.
Remote procedure calls (RPCs) are executed via msg rpc(), which sends a message and waits for exactly one return message from the sender.
Remote procedure calls are covered in detail in Section 3.6.2
The port allocate() system call creates a new mailbox and allocates space for its queue of messages.
The maximum size of the message queue defaults to eight messages.
The task that creates the mailbox is that mailbox’s owner.
The owner is also allowed to receive from the mailbox.
Only one task at a time can either own or receive from a mailbox, but these rights can be sent to other tasks.
As messages are sent to the mailbox, the messages are copied into the mailbox.
Mach guarantees that multiple messages from the same sender are queued in ﬁrst-in, ﬁrst-out (FIFO) order but does not guarantee an absolute ordering.
For instance, messages from two senders may be queued in any order.
The messages themselves consist of a ﬁxed-length header followed by a variable-length data portion.
The header indicates the length of the message and includes two mailbox names.
Commonly, the sending thread expects a reply, so the mailbox name of the sender is passed on to the receiving task, which can use it as a “return address.”
The variable part of a message is a list of typed data items.
Each entry in the list has a type, size, and value.
The type of the objects speciﬁed in the message is important, since objects deﬁned by the operating system—such as ownership or receive access rights, task states, and memory segments—may be sent in messages.
For instance, when a message is sent to a mailbox, the mailbox may be full.
If the mailbox is not full, the message is copied to the mailbox, and the sending thread continues.
If the mailbox is full, the sending thread has four options:
Here, a message is given to the operating system to keep, even though the mailbox to which that message is being sent is full.
When the message can be put in the mailbox, a message is sent back to the sender.
Only one message to a full mailbox can be pending at any time for a given sending thread.
The ﬁnal option is meant for server tasks, such as a line-printer driver.
After ﬁnishing a request, such tasks may need to send a one-time reply to the task that requested service, but they must also continue with other service requests, even if the reply mailbox for a client is full.
The receive operation must specify the mailbox or mailbox set from which a message is to be received.
A mailbox set is a collection of mailboxes, as declared by the task, which can be grouped together and treated as one mailbox for the purposes of the task.
Threads in a task can receive only from a mailbox or mailbox set for which the task has receive access.
A port status() system call returns the number of messages in a given mailbox.
If no message is waiting to be received, the receiving thread can either wait at most n milliseconds or not wait at all.
The Mach system was especially designed for distributed systems, which we discuss in Chapter 17, but Mach was shown to be suitable for systems with fewer processing cores, as evidenced by its inclusion in the Mac OS X system.
The major problem with message systems has generally been poor performance caused by double copying of messages: the message is copied ﬁrst from the sender to the mailbox and then from the mailbox to the receiver.
Essentially, Mach maps the address space containing the sender’s message into the receiver’s address space.
This message-management technique provides a large performance boost but works for only intrasystem messages.
The Mach operating system is discussed in more detail in the online Appendix B.
The Windows operating system is an example of modern design that employs modularity to increase functionality and decrease the time needed to implement new features.
Application programs communicate with these subsystems via a message-passing mechanism.
Thus, application programs can be considered clients of a subsystem server.
The message-passing facility in Windows is called the advanced local procedure call (ALPC) facility.
It is used for communication between two processes on the same machine.
It is similar to the standard remote procedure call (RPC) mechanism that is widely used, but it is optimized for and speciﬁc to Windows.
Remote procedure calls are covered in detail in Section 3.6.2
Like Mach, Windows uses a port object to establish and maintain a connection between two processes.
Windows uses two types of ports: connection ports and communication ports.
Server processes publish connection-port objects that are visible to all processes.
When a client wants services from a subsystem, it opens a handle to the server’s connection-port object and sends a connection request to that port.
The server then creates a channel and returns a handle to the client.
The channel consists of a pair of private communication ports: one for client—server messages, the other for server—client messages.
Additionally, communication channels support a callback mechanism that allows the client and server to accept requests when they would normally be expecting a reply.
When an ALPC channel is created, one of three message-passing techniques is chosen:
For small messages (up to 256 bytes), the port’s message queue is used as intermediate storage, and the messages are copied from one process to the other.
Larger messages must be passed through a section object, which is a region of shared memory associated with the channel.
When the amount of data is too large to ﬁt into a section object, an API is available that allows server processes to read and write directly into the address space of a client.
The client has to decide when it sets up the channel whether it will need to send a large message.
If the client determines that it does want to send large messages, it asks for a section object to be created.
Similarly, if the server decides that replies will be large, it creates a section object.
So that the section object can be used, a small message is sent that contains a pointer and size information about the section object.
This method is more complicated than the ﬁrst method listed above, but it avoids data copying.
The structure of advanced local procedure calls in Windows is shown in Figure 3.19
It is important to note that the ALPC facility in Windows is not part of the Windows API and hence is not visible to the application programmer.
Rather, applications using the Windows API invoke standard remote procedure calls.
When the RPC is being invoked on a process on the same system, the RPC is handled indirectly through an ALPC.
Additionally, many kernel services use ALPC to communicate with client processes.
In Section 3.4, we described how processes can communicate using shared memory and message passing.
These techniques can be used for communication in client–server systems (Section 1.11.4) as well.
In this section, we explore three other strategies for communication in client–server systems: sockets, remote procedure calls (RPCs), and pipes.
A pair of processes communicating over a network employs a pair of sockets—one for each process.
A socket is identiﬁed by an IP address concatenated with a port number.
The server waits for incoming client requests by listening to a speciﬁed port.
Once a request is received, the server accepts a connection from the client socket to complete the connection.
All ports below 1024 are considered well known; we can use them to implement standard services.
When a client process initiates a request for a connection, it is assigned a port by its host computer.
The packets traveling between the hosts are delivered to the appropriate process based on the destination port number.
This ensures that all connections consist of a unique pair of sockets.
Although most program examples in this text use C, we will illustrate sockets using Java, as it provides a much easier interface to sockets and has a rich library for networking utilities.
Those interested in socket programming in C or C++ should consult the bibliographical notes at the end of the chapter.
Finally, theMulticastSocket class is a subclass of the DatagramSocket class.
A multicast socket allows data to be sent to multiple recipients.
The operation allows clients to request the current date and time from the server.
When a connection is received, the server returns the date and time to the client.
The server then begins listening to the port with the accept() method.
The server blocks on the accept() method waiting for a client to request a connection.
When a connection request is received, accept() returns a socket that the server can use to communicate with the client.
The details of how the server communicates with the socket are as follows.
The server ﬁrst establishes aPrintWriterobject that it will use to communicate with the client.
A PrintWriter object allows the server to write to the socket using the routine print() and println() methods for output.
The server process sends the date to the client, calling the method println()
Once it has written the date to the socket, the server closes the socket to the client and resumes listening for more requests.
A client communicates with the server by creating a socket and connecting to the port on which the server is listening.
We implement such a client in the Java program shown in Figure 3.22
Once the connection is made, the client can read from the socket using normal stream I/O statements.
After it has received the date from the server, the client closes.
The IP address 127.0.0.1 is a special IP address known as the loopback.
When a computer refers to IP address 127.0.0.1, it is referring to itself.
This mechanism allows a client and server on the same host to communicate using the TCP/IP protocol.
The IP address 127.0.0.1 could be replaced with the IP address of another host running the date server.
Communication using sockets—although common and efﬁcient—is considered a low-level form of communication between distributed processes.
One reason is that sockets allow only an unstructured stream of bytes to be exchanged between the communicating threads.
It is the responsibility of the client or server application to impose a structure on the data.
In the next two subsections, we look at two higher-level methods of communication: remote procedure calls (RPCs) and pipes.
One of the most common forms of remote service is the RPC paradigm, which we discussed brieﬂy in Section 3.5.2
It is similar in many respects to the IPC mechanism described in Section 3.4, and it is usually built on top of such a system.
Here, however, because we are dealing with an environment in which the processes are executing on separate systems, we must use a message-based communication scheme to provide remote service.
In contrast to IPC messages, the messages exchanged in RPC communication are well structured and are thus no longer just packets of data.
Each message is addressed to an RPC daemon listening to a port on the remote system, and each contains an identiﬁer specifying the function to execute and the parameters to pass to that function.
The function is then executed as requested, and any output is sent back to the requester in a separate message.
A port is simply a number included at the start of a message packet.
Whereas a system normally has one network address, it can have many ports within that address to differentiate the many network services it supports.
If a remote process needs a service, it addresses a message to the proper port.
The semantics of RPCs allows a client to invoke a procedure on a remote host as it would invoke a procedure locally.
The RPC system hides the details that allow communication to take place by providing a stub on the client side.
Typically, a separate stub exists for each separate remote procedure.
When the client invokes a remote procedure, the RPC system calls the appropriate stub, passing it the parameters provided to the remote procedure.
This stub locates the port on the server and marshals the parameters.
Parameter marshalling involves packaging the parameters into a form that can be transmitted over a network.
The stub then transmits a message to the server using message passing.
A similar stub on the server side receives this message and invokes the procedure on the server.
If necessary, return values are passed back to the client using the same technique.
On Windows systems, stub code is compiled from a speciﬁcation written in the Microsoft Interface Deﬁnition Language (MIDL), which is used for deﬁning the interfaces between client and server programs.
One issue that must be dealt with concerns differences in data representation on the client and server machines.
Some systems (known as big-endian) store the most signiﬁcant byte ﬁrst, while other systems (known as little-endian) store the least signiﬁcant byte ﬁrst.
Neither order is “better” per se; rather, the choice is arbitrary within a computer architecture.
One such representation is known as external data representation (XDR)
On the client side, parameter marshalling involves converting the machine-dependent data into XDR before they are sent to the server.
On the server side, the XDR data are unmarshalled and converted to the machine-dependent representation for the server.
Whereas local procedure calls fail only under extreme circumstances, RPCs can fail, or be duplicated and executed more than once, as a result of common network errors.
One way to address this problem is for the operating system to ensure that messages are acted on exactly once, rather than at most once.
Most local procedure calls have the “exactly once” functionality, but it is more difﬁcult to implement.
First, consider “at most once.” This semantic can be implemented by attaching a timestamp to each message.
The server must keep a history of all the timestamps of messages it has already processed or a history large enough to ensure that repeated messages are detected.
Incoming messages that have a timestamp already in the history are ignored.
The client can then send a message one or more times and be assured that it only executes once.
For “exactly once,” we need to remove the risk that the server will never receive the request.
To accomplish this, the server must implement the “at most once” protocol described above but must also acknowledge to the client that the RPC call was received and executed.
The client must resend each RPC call periodically until it receives the ACK for that call.
Yet another important issue concerns the communication between a server and a client.
With standard procedure calls, some form of binding takes place during link, load, or execution time (Chapter 8) so that a procedure call’s name.
The RPC scheme requires a similar binding of the client and the server port, but how does a client know the port numbers on the server? Neither system has full information about the other, because they do not share memory.
First, the binding information may be predetermined, in the form of ﬁxed port addresses.
At compile time, an RPC call has a ﬁxed port number associated with it.
Once a program is compiled, the server cannot change the port number of the requested service.
Second, binding can be done dynamically by a rendezvous mechanism.
Typically, an operating system provides a rendezvous (also called a matchmaker) daemon on a ﬁxed RPC port.
A client then sends a message containing the name of the RPC to the rendezvous daemon requesting the port address of the RPC it needs to execute.
The port number is returned, and the RPC calls can be sent to that port until the process terminates (or the server crashes)
This method requires the extra overhead of the initial request but is more ﬂexible than the ﬁrst approach.
The RPC scheme is useful in implementing a distributed ﬁle system (Chapter 17)
Such a system can be implemented as a set of RPC daemons.
The messages are addressed to the distributed ﬁle system port on a server on which a ﬁle operation is to take place.
The disk operation might be read, write, rename, delete, or status, corresponding to the usual ﬁle-related system calls.
The return message contains any data resulting from that call, which is executed by the DFS daemon on behalf of the client.
For instance, a message might contain a request to transfer a whole ﬁle to a client or be limited to a simple block request.
In the latter case, several requests may be needed if a whole ﬁle is to be transferred.
A pipe acts as a conduit allowing two processes to communicate.
Pipes were one of the ﬁrst IPC mechanisms in early UNIX systems.
They typically provide one of the simpler ways for processes to communicate with one another, although they also have some limitations.
Does the pipe allow bidirectional communication, or is communication unidirectional?
If two-way communication is allowed, is it half duplex (data can travel only one way at a time) or full duplex (data can travel in both directions at the same time)?
Must a relationship (such as parent–child) exist between the communicating processes?
Can the pipes communicate over a network, or must the communicating processes reside on the same machine?
In the following sections, we explore two common types of pipes used on both UNIX and Windows systems: ordinary pipes and named pipes.
Ordinary pipes allow two processes to communicate in standard producerconsumer fashion: the producer writes to one end of the pipe (the write-end) and the consumer reads from the other end (the read-end)
As a result, ordinary pipes are unidirectional, allowing only one-way communication.
If two-way communication is required, two pipes must be used, with each pipe sending data in a different direction.
We next illustrate constructing ordinary pipes on both UNIX and Windows systems.
In both program examples, one process writes the message Greetings to the pipe, while the other process reads this message from the pipe.
On UNIX systems, ordinary pipes are constructed using the function.
Thus, pipes can be accessed using ordinary read() and write() system calls.
An ordinary pipe cannot be accessed from outside the process that created it.
Typically, a parent process creates a pipe and uses it to communicate with a child process that it creates via fork()
Recall from Section 3.3.1 that a child process inherits open ﬁles from its parent.
Since a pipe is a special type of ﬁle, the child inherits the pipe from its parent process.
Figure 3.24 illustrates the relationship of the ﬁle descriptor fd to the parent and child processes.
In the UNIX program shown in Figure 3.25, the parent process creates a pipe and then sends a fork() call creating the child process.
What occurs after the fork() call depends on how the data are to ﬂow through the pipe.
In this instance, the parent writes to the pipe, and the child reads from it.
It is important to notice that both the parent process and the child process initially close their unused ends of the pipe.
Ordinary pipes on Windows systems are termed anonymous pipes, and they behave similarly to their UNIX counterparts: they are unidirectional and.
In addition, reading and writing to the pipe can be accomplished with the ordinary ReadFile() and WriteFile() functions.
The Windows API for creating pipes is the CreatePipe() function, which is passed four parameters.
Furthermore, (4) the size of the pipe (in bytes) may be speciﬁed.
Figure 3.27 illustrates a parent process creating an anonymous pipe for communicating with its child.
This is accomplished by ﬁrst initializing the SECURITY ATTRIBUTES structure to allow handles to be inherited and then redirecting the child process’s handles for standard input or standard output to the read or write handle of the pipe.
Since the child will be reading from the pipe, the parent must redirect the child’s standard input to the read handle of the pipe.
Furthermore, as the pipes are half duplex, it is necessary to prohibit the child from inheriting the write-end of the pipe.
The program to create the child process is similar to the program in Figure 3.11, except that the ﬁfth parameter is set to TRUE, indicating that the child process is to inherit designated handles from its parent.
Before writing to the pipe, the parent ﬁrst closes its unused read end of the pipe.
The child process that reads from the pipe is shown in Figure 3.29
Before reading from the pipe, this program obtains the read handle to the pipe by invoking GetStdHandle()
Note that ordinary pipes require a parent–child relationship between the communicating processes on both UNIX and Windows systems.
This means that these pipes can be used only for communication between processes on the same machine.
Ordinary pipes provide a simple mechanism for allowing a pair of processes to communicate.
However, ordinary pipes exist only while the processes are communicating with one another.
On both UNIX and Windows systems, once the processes have ﬁnished communicating and have terminated, the ordinary pipe ceases to exist.
Communication can be bidirectional, and no parent–child relationship is required.
Once a named pipe is established, several processes can use it for communication.
In fact, in a typical scenario, a named pipe has several writers.
Additionally, named pipes continue to exist after communicating processes have.
Both UNIX and Windows systems support named pipes, although the details of implementation differ greatly.
Next, we explore named pipes in each of these systems.
Named pipes are referred to as FIFOs in UNIX systems.
Once created, they appear as typical ﬁles in the ﬁle system.
A FIFO is created with the mkfifo() system call and manipulated with the ordinary open(), read(), write(), and close() system calls.
It will continue to exist until it is explicitly deleted from the ﬁle system.
Although FIFOs allow bidirectional communication, only half-duplex transmission is permitted.
If data must travel in both directions, two FIFOs are typically used.
Additionally, the communicating processes must reside on the same machine.
If intermachine communication is required, sockets (Section 3.6.1) must be used.
Named pipes on Windows systems provide a richer communication mechanism than their UNIX counterparts.
Full-duplex communication is allowed, and the communicating processes may reside on either the same or different machines.
Additionally, only byte-oriented data may be transmitted across a UNIX FIFO, whereas Windows systems allow either byte- or message-oriented data.
Named pipes are created with the CreateNamedPipe() function, and a client can connect to a named pipe using ConnectNamedPipe()
Communication over the named pipe can be accomplished using the ReadFile() and WriteFile() functions.
The state of a process is deﬁned by that process’s current activity.
Each process may be in one of the following states: new, ready, running, waiting, or terminated.
Pipes are used quite often in the UNIX command-line environment for situations in which the output of one command serves as input to another.
For example, the UNIX ls command produces a directory listing.
For especially long directory listings, the output may scroll through several screens.
The command more manages output by displaying only one screen of output at a time; the user must press the space bar to move from one screen to the next.
Setting up a pipe between the ls and more commands (which are running as individual processes) allows the output of ls to be delivered as the input to more, enabling the user to display a large directory listing a screen at a time.
A pipe can be constructed on the command line using the | character.
In this scenario, the ls command serves as the producer, and its output is consumed by the more command.
Windows systems provide a more command for the DOS shell with functionality similar to that of its UNIX counterpart.
The DOS shell also uses the | character for establishing a pipe.
The only difference is that to get a directory listing, DOS uses the dir command rather than ls, as shown below:
Each process is represented in the operating system by its own process control block (PCB)
A process, when it is not executing, is placed in some waiting queue.
There are two major classes of queues in an operating system: I/O request queues and the ready queue.
The ready queue contains all the processes that are ready to execute and are waiting for the CPU.
The operating system must select processes from various scheduling queues.
Long-term (job) scheduling is the selection of processes that will be allowed to contend for the CPU.
Short-term (CPU) scheduling is the selection of one process from the ready queue.
Operating systems must provide a mechanism for parent processes to create new child processes.
The parent may wait for its children to terminate before proceeding, or the parent and children may execute concurrently.
There are several reasons for allowing concurrent execution: information sharing, computation speedup, modularity, and convenience.
The processes executing in the operating system may be either independent processes or cooperating processes.
Cooperating processes require an interprocess communication mechanism to communicate with each other.
Principally, communication is achieved through two schemes: shared memory and message passing.
The processes are expected to exchange information through the use of these shared variables.
In a shared-memory system, the responsibility for providing communication rests with the application programmers; the operating system needs to provide only the shared memory.
The responsibility for providing communication may rest with the operating system itself.
These two schemes are not mutually exclusive and can be used simultaneously within a single operating system.
A connection between a pair of applications consists of a pair of sockets, one at each end of the communication channel.
An RPC occurs when a process (or thread) calls a procedure on a remote application.
Pipes provide a relatively simple ways for processes to communicate with one another.
Ordinary pipes allow communication between parent and child processes, while named pipes permit unrelated processes to communicate.
Discuss three major complications that concurrent processing adds to an operating system.
Describe what happens when a context switch occurs if the new context is already loaded into one of the register sets.
What happens if the new context is in memory rather than in a register set and all the register sets are in use?
Does the algorithm for implementing this semantic execute correctly even if the ACK message sent back to the client is lost due to a network problem? Describe the sequence of messages, and discuss whether “exactly once” is still preserved.
What mechanisms would be required to guarantee the “exactly once” semantic for execution of RPCs?
To obtain process information for the UNIX or Linux system, use the command ps -ael.
Use the command man ps to get more information about the ps command.
Describe the undesirable consequences that could arise from not enforcing either the “at most once” or “exactly once” semantic.
Describe possible uses for a mechanism that has neither of these guarantees.
This zombie process must remain in the system for at least 10 seconds.
The process states are shown below the S column; processes with a state of Z are zombies.
The process identiﬁer (pid) of the child process is listed in the PID column, and that of the parent is listed in the PPID column.
Perhaps the easiest way to determine that the child process is indeed a zombie is to run the program that you have written in the background (using the &) and then run the command ps -l to determine whether the child is a zombie process.
Because you do not want too many zombie processes existing in the system, you will need to remove the one that you have created.
The easiest way to do that is to terminate the parent process using the kill command.
For example, if the process id of the parent is 4884, you would enter.
When a process is ﬁrst created, it is assigned a unique pid by the pid manager.
The pid is returned to the pid manager when the process completes execution, and the manager may later reassign this pid.
What is most important here is to recognize that process identiﬁers must be unique; no two active processes can have the same pid.
Use the following constants to identify the range of possible pid values:
You may use any data structure of your choice to represent the availability of process identiﬁers.
Implement the following API for obtaining and releasing a pid:
Write a C program using the fork() system call that generates this sequence in the child process.
The starting number will be provided from the command line.
Because the parent and child processes have their own copies of the data, it will be necessary for the child to output the sequence.
Have the parent invoke the wait() call to wait for the child process to complete before exiting the program.
Perform necessary error checking to ensure that a positive integer is passed on the command line.
Another approach to designing this program is to establish a shared-memory object between the parent and child processes.
This technique allows the child to write the contents of the sequence to the shared-memory object.
The parent can then output the sequence when the child completes.
Because the memory is shared, any changes the child makes will be reﬂected in the parent process as well.
This program will be structured using POSIX shared memory as described in Section 3.5.1
Create the child process and wait for it to terminate.
One area of concern with cooperating processes involves synchronization issues.
In this exercise, the parent and child processes must be coordinated so that the parent does not output the sequence until the child ﬁnishes execution.
These two processes will be synchronized using the wait() system call: the parent process will invoke wait(), which will suspend it until the child process exits.
When a client connects to port 17 on a server, the server responds with a quote for that day.
Modify the date server shown in Figure 3.21 so that it delivers a quote of the day rather than the current date.
The quotes should be printable ASCII characters and should contain fewer than 512 characters, although multiple lines are allowed.
The date client shown in Figure 3.22 can be used to read the quotes returned by your server.
When a client connects to this port, the server responds with a haiku.
The date client shown in Figure 3.22 can be used to read the quotes returned by your haiku server.
For example, if a client sends the server the string Hello there!, the server will respond with Hello there!
Write an echo server using the Java networking API described in Section 3.6.1
This server will wait for a client connection using the accept() method.
When a client connection is received, the server will loop, performing the following steps:
Write the contents of the buffer back to the client.
The server will break out of the loop only when it has determined that the client has closed the connection.
For example, if the ﬁrst process sends the messageHi There, the second process will return hI tHERE.
This will require using two pipes, one for sending the original message from the ﬁrst to the second process and the other for sending the modiﬁed message from the second to the ﬁrst process.
You can write this program using either UNIX or Windows pipes.
This program will be passed two parameters: the name of the ﬁle to be.
The program will then create an ordinary pipe and write the contents of the ﬁle to be copied to the pipe.
The child process will read this ﬁle from the pipe and write it to the destination ﬁle.
The child process will read the contents of this ﬁle and write it to the destination ﬁle copy.txt.
You may write this program using either UNIX or Windows pipes.
This project consists of designing a C program to serve as a shell interface that accepts user commands and then executes each command in a separate process.
A shell interface gives the user a prompt, after which the next command is entered.
The example below illustrates the prompt osh> and the user’s next command: cat prog.c.
This command displays the ﬁle prog.c on the terminal using the UNIX cat command.
One technique for implementing a shell interface is to have the parent process ﬁrst read what the user enters on the command line (in this case, cat prog.c), and then create a separate child process that performs the command.
Unless otherwise speciﬁed, the parent process waits for the child to exit before continuing.
This is similar in functionality to the new process creation illustrated in Figure 3.10
However, UNIX shells typically also allow the child process to run in the background, or concurrently.
To accomplish this, we add an ampersand (&) at the end of the command.
The separate child process is created using the fork() system call, and the.
A C program that provides the general operations of a command-line shell is supplied in Figure 3.36
The main() function presents the prompt osh-> and outlines the steps to be taken after input from the user has been read.
The ﬁrst task is to modify the main() function in Figure 3.36 so that a child process is forked and executes the command speciﬁed by the user.
This will require parsing what the user has entered into separate tokens and storing the tokens in an array of character strings (args in Figure 3.36)
For example, if the user enters the command ps -ael at the osh> prompt, the values stored in the args array are:
This args array will be passed to the execvp() function, which has the following prototype:
Here, command represents the command to be performed and params stores the parameters to this command.
For this project, the execvp() function should be invoked as execvp(args[0], args)
Be sure to check whether the user included an & to determine whether or not the parent process is to wait for the child to exit.
The next task is to modify the shell interface program so that it provides a history feature that allows the user to access the most recently entered commands.
The user will be able to access up to 10 commands by using the feature.
The user will be able to list the command history by entering the command.
As an example, assume that the history consists of the commands (from most to least recent):
Your program should support two techniques for retrieving commands from the command history:
When the user enters !!, the most recent command in the history is executed.
When the user enters a single ! followed by an integer N, the Nth command in the history is executed.
Continuing our example from above, if the user enters !!, the ps command will be performed; if the user enters !3, the command cal will be executed.
Any command executed in this fashion should be echoed on the user’s screen.
The command should also be placed in the history buffer as the next command.
If there are no commands in the history, entering !! should result in a message “No commands in history.” If there is no command corresponding to the number entered with the single !, the program should output "No such command in history."
In this project, you will write a kernel module that lists all current tasks in a Linux system.
Be sure to review the programming project in Chapter 2, which deals with creating Linux kernel modules, before you begin this project.
The project can be completed using the Linux virtual machine provided with this text.
In Linux, the for each process() macro easily allows iteration over all current tasks in the system:
The various ﬁelds in task struct can then be displayed as the program loops through the for each process() macro.
Design a kernel module that iterates through all tasks in the system using the for each process() macro.
In particular, output the task name (known as executable name), state, and process id of each task.
You will probably have to read through the task struct structure in <linux/sched.h> to obtain the names of these ﬁelds.
Write this code in the module entry point so that its contents will appear in the kernel log buffer, which can be viewed using the dmesg command.
To verify that your code is working correctly, compare the contents of the kernel log buffer with the output of the following command, which lists all tasks in the system:
Because tasks are dynamic, however, it is possible that a few tasks may appear in one listing but not the other.
The second portion of this project involves iterating over all tasks in the system using a depth-ﬁrst search (DFS) tree.
Linux maintains its process tree as a series of lists.
Examining the task struct in <linux/sched.h>, we see two struct list head objects:
These objects are pointers to a list of the task’s children, as well as its siblings.
Linux also maintains references to the init task (struct task struct init task)
Using this information as well as macro operations on lists, we can iterate over the children of init as follows:
The list for each() macro is passed two parameters, both of type struct list head:
A pointer to the head of the list to be traversed.
A pointer to the head node of the list to be traversed.
At each iteration of list for each(), the ﬁrst parameter is set to the list structure of the next child.
We then use this value to obtain each structure in the list using the list entry() macro.
Beginning from the init task, design a kernel module that iterates over all tasks in the system using a DFS tree.
Just as in the ﬁrst part of this project, output the name, state, and pid of each task.
Perform this iteration in the kernel entry module so that its output appears in the kernel log buffer.
If you output all tasks in the system, you may see many more tasks than appear with the ps -ael command.
This is because some threads appear as children but do not show up as ordinary processes.
Therefore, to check the output of the DFS tree, use the command.
To verify that you have indeed performed an appropriate DFS iteration, you will have to examine the relationships among the various tasks output by the ps command.
Coverage of the multiprocess model used in Google’s Chrome can be found at http://blog.chromium.org/2008/09/multi-process-architecture.html.
Message passing for multicore systems is discussed in [Holland and Seltzer (2011)]
The implementation of RPCs is discussed by [Birrell and Nelson (1984)]
The process model introduced in Chapter 3 assumed that a process was an executing program with a single thread of control.
Virtually all modern operating systems, however, provide features enabling a process to contain multiple threads of control.
In this chapter, we introduce many concepts associated with multithreaded computer systems, including a discussion of the APIs for the Pthreads, Windows, and Java thread libraries.
We look at a number of issues related to multithreaded programming and its effect on the design of operating systems.
Finally, we explore how the Windows and Linux operating systems support threads at the kernel level.
To introduce the notion of a thread—a fundamental unit of CPU utilization that forms the basis of multithreaded computer systems.
To cover operating system support for threads in Windows and Linux.
A thread is a basic unit of CPU utilization; it comprises a thread ID, a program counter, a register set, and a stack.
It shares with other threads belonging to the same process its code section, data section, and other operating-system resources, such as open ﬁles and signals.
A traditional (or heavyweight) process has a single thread of control.
If a process has multiple threads of control, it can perform more than one task at a time.
Figure 4.1 illustrates the difference between a traditional single-threaded process and a multithreaded process.
Most software applications that run on modern computers are multithreaded.
An application typically is implemented as a separate process with several.
A web browser might have one thread display images or text while another thread retrieves data from the network, for example.
A word processor may have a thread for displaying graphics, another thread for responding to keystrokes from the user, and a third thread for performing spelling and grammar checking in the background.
Applications can also be designed to leverage processing capabilities on multicore systems.
Such applications can perform several CPU-intensive tasks in parallel across the multiple computing cores.
In certain situations, a single application may be required to perform several similar tasks.
For example, a web server accepts client requests for web pages, images, sound, and so forth.
A busy web server may have several (perhaps thousands of) clients concurrently accessing it.
If the web server ran as a traditional single-threaded process, it would be able to service only one client at a time, and a client might have to wait a very long time for its request to be serviced.
One solution is to have the server run as a single process that accepts requests.
When the server receives a request, it creates a separate process to service that request.
In fact, this process-creation method was in common use before threads became popular.
If the new process will perform the same tasks as the existing process, why incur all that overhead? It is generally more efﬁcient to use one process that contains multiple threads.
If the web-server process is multithreaded, the server will create a separate thread that listens for client requests.
When a request is made, rather than creating another process, the server creates a new thread to service the request and resume listening for additional requests.
Threads also play a vital role in remote procedure call (RPC) systems.
Recall from Chapter 3 that RPCs allow interprocess communication by providing a communication mechanism similar to ordinary function or procedure calls.
Several threads operate in the kernel, and each thread performs a speciﬁc task, such as managing devices, managing memory, or interrupt handling.
For example, Solaris has a set of threads in the kernel speciﬁcally for interrupt handling; Linux uses a kernel thread for managing the amount of free memory in the system.
The beneﬁts of multithreaded programming can be broken down into four major categories:
Multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user.
For instance, consider what happens when a user clicks a button that results in the performance of a time-consuming operation.
A single-threaded application would be unresponsive to the user until the operation had completed.
In contrast, if the time-consuming operation is performed in a separate thread, the application remains responsive to the user.
Processes can only share resources through techniques such as shared memory and message passing.
However, threads share the memory and the resources of the process to which they belong by default.
The beneﬁt of sharing code and data is that it allows an application to have several different threads of activity within the same address space.
Because threads share the resources of the process to which they belong, it is more economical to create and context-switch threads.
Empirically gauging the difference in overhead can be difﬁcult, but in general it is signiﬁcantly more time consuming to create and manage processes than threads.
In Solaris, for example, creating a process is about thirty times.
The beneﬁts of multithreading can be even greater in a multiprocessor architecture, where threads may be running in parallel on different processing cores.
A single-threaded process can run on only one processor, regardless how many are available.
Earlier in the history of computer design, in response to the need for more computing performance, single-CPU systems evolved into multi-CPU systems.
A more recent, similar trend in system design is to place multiple computing cores on a single chip.
Each core appears as a separate processor to the operating system (Section 1.3.2)
Whether the cores appear across CPU chips or within CPU chips, we call these systems multicore or multiprocessor systems.
Multithreaded programming provides a mechanism for more efﬁcient use of these multiple computing cores and improved concurrency.
On a system with a single computing core, concurrency merely means that the execution of the threads will be interleaved over time (Figure 4.3), because the processing core is capable of executing only one thread at a time.
On a system with multiple cores, however, concurrency means that the threads can run in parallel, because the system can assign a separate thread to each core (Figure 4.4)
Notice the distinction between parallelism and concurrency in this discussion.
A system is parallel if it can perform more than one task simultaneously.
In contrast, a concurrent system supports more than one task by allowing all the tasks to make progress.
Before the advent of SMP and multicore architectures, most computer systems had only a single processor.
Amdahl’s Law is a formula that identiﬁes potential performance gains from adding additional computing cores to an application that has both serial (nonparallel) and parallel components.
If S is the portion of the application that must be performed serially on a system with N processing cores, the formula appears as follows:
If we run this application on a system with two processing cores, we can get a speedup of 1.6 times.
If we add two additional cores (for a total of four), the speedup is 2.28 times.
One interesting fact about Amdahl’s Law is that as N approaches inﬁnity, the speedup converges to 1/S.
This is the fundamental principle behind Amdahl’s Law: the serial portion of an application can have a disproportionate effect on the performance we gain by adding additional computing cores.
Some argue that Amdahl’s Law does not take into account the hardware performance enhancements used in the design of contemporary multicore systems.
Such arguments suggest Amdahl’s Law may cease to be applicable as the number of processing cores continues to increase on modern computer systems.
As systems have grown from tens of threads to thousands of threads, CPU designers have improved system performance by adding hardware to improve thread performance.
Modern Intel CPUs frequently support two threads per core, while the Oracle T4 CPU supports eight threads per core.
This support means that multiple threads can be loaded into the core for fast switching.
Multicore computers will no doubt continue to increase in core counts and hardware thread support.
The trend towards multicore systems continues to place pressure on system designers and application programmers to make better use of the multiple computing cores.
Designers of operating systems must write scheduling algorithms that use multiple processing cores to allow the parallel execution shown in Figure 4.4
For application programmers, the challenge is to modify existing programs as well as design new programs that are multithreaded.
In general, ﬁve areas present challenges in programming for multicore systems:
This involves examining applications to ﬁnd areas that can be divided into separate, concurrent tasks.
Ideally, tasks are independent of one another and thus can run in parallel on individual cores.
While identifying tasks that can run in parallel, programmers must also ensure that the tasks perform equal work of equal value.
In some instances, a certain task may not contribute as much value to the overall process as other tasks.
Using a separate execution core to run that task may not be worth the cost.
Just as applications are divided into separate tasks, the data accessed and manipulated by the tasks must be divided to run on separate cores.
The data accessed by the tasks must be examined for dependencies between two or more tasks.
When one task depends on data from another, programmers must ensure that the execution of the tasks is synchronized to accommodate the data dependency.
When a program is running in parallel on multiple cores, many different execution paths are possible.
Testing and debugging such concurrent programs is inherently more difﬁcult than testing and debugging single-threaded applications.
Because of these challenges, many software developers argue that the advent of multicore systems will require an entirely new approach to designing software systems in the future.
Similarly, many computer science educators believe that software development must be taught with increased emphasis on parallel programming.
Task parallelism involves distributing not data but tasks (threads) across multiple computing cores.
Different threads may be operating on the same data, or they may be operating on different data.
In contrast to that situation, an example of task parallelism might involve two threads, each performing a unique statistical operation on the array of elements.
The threads again are operating in parallel on separate computing cores, but each is performing a unique operation.
Fundamentally, then, data parallelism involves the distribution of data across multiple cores and task parallelism on the distribution of tasks across multiple cores.
In practice, however, few applications strictly follow either data or task parallelism.
In most instances, applications use a hybrid of these two strategies.
Our discussion so far has treated threads in a generic sense.
However, support for threads may be provided either at the user level, for user threads, or by the kernel, for kernel threads.
User threads are supported above the kernel and are managed without kernel support, whereas kernel threads are supported and managed directly by the operating system.
Ultimately, a relationship must exist between user threads and kernel threads.
In this section, we look at three common ways of establishing such a relationship: the many-to-one model, the one-to-one model, and the many-tomany model.
The many-to-one model (Figure 4.5) maps many user-level threads to one kernel thread.
Thread management is done by the thread library in user space, so it is efﬁcient (we discuss thread libraries in Section 4.4)
However, the entire process will block if a thread makes a blocking system call.
Also, because only one thread can access the kernel at a time, multiple threads are unable to run in parallel on multicore systems.
Green threads—a thread library available for Solaris systems and adopted in early versions of Java—used the many-to-one model.
However, very few systems continue to use the model because of its inability to take advantage of multiple processing cores.
The one-to-one model (Figure 4.6) maps each user thread to a kernel thread.
It provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call.
It also allows multiple threads to run in parallel on multiprocessors.
The only drawback to this model is that creating a user thread requires creating the corresponding kernel thread.
Because the overhead of creating kernel threads can burden the performance of an application, most implementations of this model restrict the number of threads supported by the system.
Linux, along with the family of Windows operating systems, implement the one-to-one model.
The many-to-many model (Figure 4.7) multiplexes many user-level threads to a smaller or equal number of kernel threads.
The number of kernel threads may be speciﬁc to either a particular application or a particular machine (an application may be allocated more kernel threads on a multiprocessor than on a single processor)
Whereas the manyto-one model allows the developer to create as many user threads as she wishes, it does not result in true concurrency, because the kernel can schedule only one thread at a time.
The many-to-many model suffers from neither of these shortcomings: developers can create as many user threads as necessary, and the corresponding kernel threads can run in parallel on a multiprocessor.
Also, when a thread performs a blocking system call, the kernel can schedule another thread for execution.
One variation on the many-to-many model still multiplexes many userlevel threads to a smaller or equal number of kernel threads but also allows a user-level thread to be bound to a kernel thread.
This variation is sometimes referred to as the two-level model (Figure 4.8)
However, beginning with Solaris 9, this system uses the one-to-one model.
A thread library provides the programmer with an API for creating and managing threads.
There are two primary ways of implementing a thread library.
The ﬁrst approach is to provide a library entirely in user space with no kernel support.
All code and data structures for the library exist in user space.
This means that invoking a function in the library results in a local function call in user space and not a system call.
The second approach is to implement a kernel-level library supported directly by the operating system.
In this case, code and data structures for the library exist in kernel space.
Invoking a function in the API for the library typically results in a system call to the kernel.
Three main thread libraries are in use today: POSIX Pthreads, Windows, and Java.
Pthreads, the threads extension of the POSIX standard, may be provided as either a user-level or a kernel-level library.
The Windows thread library is a kernel-level library available on Windows systems.
The Java thread API allows threads to be created and managed directly in Java programs.
However, because in most instances the JVM is running on top of a host operating system, the Java thread API is generally implemented using a thread library available on the host system.
This means that on Windows systems, Java threads are typically implemented using the Windows API; UNIX and Linux systems often use Pthreads.
For POSIX and Windows threading, any data declared globally—that is, declared outside of any function—are shared among all threads belonging to the same process.
Because Java has no notion of global data, access to shared data must be explicitly arranged between threads.
Data declared local to a function are typically stored on the stack.
Since each thread has its own stack, each thread has its own copy of local data.
In the remainder of this section, we describe basic thread creation using these three thread libraries.
As an illustrative example, we design a multithreaded program that performs the summation of a non-negative integer in a separate thread using the well-known summation function:
Each of the three programs will be run with the upper bounds of the summation entered on the command line.
Before we proceed with our examples of thread creation, we introduce two general strategies for creating multiple threads: asynchronous threading and synchronous threading.
With asynchronous threading, once the parent creates a child thread, the parent resumes its execution, so that the parent and child execute concurrently.
Each thread runs independently of every other thread, and the parent thread need not know when its child terminates.
Because the threads are independent, there is typically little data sharing between threads.
Asynchronous threading is the strategy used in the multithreaded server illustrated in Figure 4.2
Synchronous threading occurs when the parent thread creates one or more children and then must wait for all of its children to terminate before it resumes —the so-called fork-join strategy.
Here, the threads created by the parent perform work concurrently, but the parent cannot continue until this work has been completed.
Once each thread has ﬁnished its work, it terminates and joins with its parent.
Only after all of the children have joined can the parent resume execution.
For example, the parent thread may combine the results calculated by its various children.
Pthreads refers to the POSIX standard (IEEE 1003.1c) deﬁning an API for thread creation and synchronization.
This is a speciﬁcation for thread behavior, not an implementation.
Operating-system designers may implement the speciﬁcation in any way they wish.
Although Windows doesn’t support Pthreads natively, some thirdparty implementations for Windows are available.
The C program shown in Figure 4.9 demonstrates the basic Pthreads API for constructing a multithreaded program that calculates the summation of a nonnegative integer in a separate thread.
When this program begins, a single thread of control begins in main()
After some initialization, main() creates a second thread that begins control in the runner() function.
Each thread has a set of attributes, including stack size and scheduling information.
The pthread attr t attr declaration represents the attributes for the thread.
We set the attributes in the function call pthread attr init(&attr)
Because we did not explicitly set any attributes, we use the default attributes provided.
In Chapter 6, we discuss some of the scheduling attributes provided by the Pthreads API.
A separate thread is created with the pthread create() function call.
In addition to passing the thread identiﬁer and the attributes for the thread, we also pass the name of the function where the new thread will begin execution—in this case, the runner() function.
Last, we pass the integer parameter that was provided on the command line, argv[1]
At this point, the program has two threads: the initial (or parent) thread in main() and the summation (or child) thread performing the summation operation in the runner() function.
This program follows the fork-join strategy described earlier: after creating the summation thread, the parent thread will wait for it to terminate by calling the pthread join() function.
The summation thread will terminate when it calls the function pthread exit()
Once the summation thread has returned, the parent thread will output the value of the shared data sum.
With the growing dominance of multicore systems, writing programs containing several threads has become increasingly common.
A simple method for waiting on several threads using the pthread join() function is to enclose the operation within a simple for loop.
For example, you can join on ten threads using the Pthread code shown in Figure 4.10
The technique for creating threads using the Windows thread library is similar to the Pthreads technique in several ways.
We illustrate the Windows thread API in the C program shown in Figure 4.11
Notice that we must include the windows.h header ﬁle when using the Windows API.
We also deﬁne the Summation() function that is to be performed in a separate thread.
This function is passed a pointer to a void, which Windows deﬁnes as LPVOID.
The thread performing this function sets the global data Sum to the value of the summation from 0 to the parameter passed to Summation()
Threads are created in the Windows API using the CreateThread() function, and—just as in Pthreads—a set of attributes for the thread is passed to this function.
These attributes include security information, the size of the stack, and a ﬂag that can be set to indicate if the thread is to start in a suspended state.
In this program, we use the default values for these attributes.
The default values do not initially set the thread to a suspended state and instead make it eligible to be run by the CPU scheduler.
Once the summation thread is created, the parent must wait for it to complete before outputting the value of Sum, as the value is set by the summation thread.
Recall that the Pthread program (Figure 4.9) had the parent thread wait for the summation thread using the pthread join() statement.
For example, if THandles is an array of thread HANDLE objects of size N, the parent thread can wait for all its child threads to complete with this statement:
Threads are the fundamental model of program execution in a Java program, and the Java language and its API provide a rich set of features for the creation and management of threads.
All Java programs comprise at least a single thread of control—even a simple Java program consisting of only a main() method runs as a single thread in the JVM.
Java threads are available on any system that provides a JVM including Windows, Linux, and Mac OS X.
The Java thread API is available for Android applications as well.
There are two techniques for creating threads in a Java program.
One approach is to create a new class that is derived from the Thread class and to override its run() method.
An alternative—and more commonly usedtechnique is to deﬁne a class that implements the Runnable interface.
When a class implements Runnable, it must deﬁne a run() method.
The code implementing the run() method is what runs as a separate thread.
Figure 4.12 shows the Java version of a multithreaded program that determines the summation of a non-negative integer.
Thread creation is performed by creating an object instance of the Thread class and passing the constructor a Runnable object.
Creating a Thread object does not speciﬁcally create the new thread; rather, the start() method creates the new thread.
Calling the start() method for the new object does two things:
It allocates memory and initializes a new thread in the JVM.
It calls the run()method, making the thread eligible to be run by the JVM.
Note again that we never call the run() method directly.
Rather, we call the start() method, and it calls the run() method on our behalf.
When the summation program runs, the JVM creates two threads.
The ﬁrst is the parent thread, which starts execution in the main() method.
The second thread is created when the start() method on the Thread object is invoked.
This child thread begins execution in the run()method of the Summation class.
After outputting the value of the summation, this thread terminates when it exits from its run() method.
Data sharing between threads occurs easily in Windows and Pthreads, since shared data are simply declared globally.
As a pure object-oriented language, Java has no such notion of global data.
If two or more threads are to share data in a Java program, the sharing occurs by passing references to the shared object to the appropriate threads.
In the Java program shown in Figure 4.12, the main thread and the summation thread share the object instance of the Sum class.
This shared object is referenced through the appropriate getSum() and setSum() methods.
You might wonder why we don’t use an Integer object rather than designing a new sum class.
The reason is that the Integer class is immutable—that is, once its value is set, it cannot change.
If the parent must wait for several threads to ﬁnish, the join() method can be enclosed in a for loop similar to that shown for Pthreads in Figure 4.10
With the continued growth of multicore processing, applications containing hundreds—or even thousands—of threads are looming on the horizon.
Designing such applications is not a trivial undertaking: programmers must address not only the challenges outlined in Section 4.2 but additional difﬁculties as well.
One way to address these difﬁculties and better support the design of multithreaded applications is to transfer the creation and management of.
Figure 4.12 Java program for the summation of a non-negative integer.
The JVM is typically implemented on top of a host operating system (see Figure 16.10)
This setup allows the JVM to hide the implementation details of the underlying operating system and to provide a consistent, abstract environment that allows Java programs to operate on any platform that supports a JVM.
The speciﬁcation for the JVM does not indicate how Java threads are to be mapped to the underlying operating system, instead leaving that decision to the particular implementation of the JVM.
For example, the Windows XP operating system uses the one-to-one model; therefore, each Java thread for a JVM running on such a system maps to a kernel thread.
On operating systems that use the many-to-many model (such as Tru64 UNIX), a Java thread is mapped according to the many-to-many model.
Solaris initially implemented the JVM using the many-to-one model (the green threads library, mentioned earlier)
Later releases of the JVM were implemented using the many-to-many model.
Beginning with Solaris 9, Java threads were mapped using the one-to-one model.
In addition, there may be a relationship between the Java thread library and the thread library on the host operating system.
For example, implementations of a JVM for the Windows family of operating systems might use the Windows API when creating Java threads; Linux, Solaris, and Mac OS X systems might use the Pthreads API.
This strategy, termed implicit threading, is a popular trend today.
In this section, we explore three alternative approaches for designing multithreaded programs that can take advantage of multicore processors through implicit threading.
In this situation, whenever the server receives a request, it creates a separate thread to service the request.
Whereas creating a separate thread is certainly superior to creating a separate process, a multithreaded server nonetheless has potential problems.
The ﬁrst issue concerns the amount of time required to create the thread, together with the fact that the thread will be discarded once it has completed its work.
If we allow all concurrent requests to be serviced in a new thread, we have not placed a bound on the number of threads concurrently active in the system.
Unlimited threads could exhaust system resources, such as CPU time or memory.
One solution to this problem is to use a thread pool.
The general idea behind a thread pool is to create a number of threads at process startup and place them into a pool, where they sit and wait for work.
When a server receives a request, it awakens a thread from this pool—if one is available—and passes it the request for service.
Once the thread completes its service, it returns to the pool and awaits more work.
If the pool contains no available thread, the server waits until one becomes free.
Servicing a request with an existing thread is faster than waiting to create a thread.
A thread pool limits the number of threads that exist at any one point.
This is particularly important on systems that cannot support a large number of concurrent threads.
Separating the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task.
For example, the task could be scheduled to execute after a time delay or to execute periodically.
The number of threads in the pool can be set heuristically based on factors such as the number of CPUs in the system, the amount of physical memory, and the expected number of concurrent client requests.
More sophisticated thread-pool architectures can dynamically adjust the number of threads in the pool according to usage patterns.
Such architectures provide the further beneﬁt of having a smaller pool—thereby consuming less memory—when the load on the system is low.
We discuss one such architecture, Apple’s Grand Central Dispatch, later in this section.
The Windows API provides several functions related to thread pools.
Using the thread pool API is similar to creating a thread with the Thread Create() function, as described in Section 4.4.2
Here, a function that is to run as a separate thread is deﬁned.
A pointer to PoolFunction() is passed to one of the functions in the thread pool API, and a thread from the pool executes this function.
This causes a thread from the thread pool to invoke PoolFunction() on behalf of the programmer.
Because we specify 0 as a ﬂag, we provide the thread pool with no special instructions for thread creation.
Other members in the Windows thread pool API include utilities that invoke functions at periodic intervals or when an asynchronous I/O request completes.
OpenMP is a set of compiler directives as well as an API for programs written in C, C++, or FORTRAN that provides support for parallel programming in shared-memory environments.
OpenMP identiﬁes parallel regions as blocks of code that may run in parallel.
Application developers insert compiler directives into their code at parallel regions, and these directives instruct the OpenMP run-time library to execute the region in parallel.
The following C program illustrates a compiler directive above the parallel region containing the printf() statement:
Thus, for a dual-core system, two threads are created, for a quad-core system, four are created; and so forth.
As each thread exits the parallel region, it is terminated.
OpenMP provides several additional directives for running code regions in parallel, including parallelizing loops.
For example, assume we have two arrays a and b of size N.
We wish to sum their contents and place the results in array c.
We can have this task run in parallel by using the following code segment, which contains the compiler directive for parallelizing for loops:
OpenMP divides the work contained in the for loop among the threads it has created in response to the directive.
In addition to providing directives for parallelization, OpenMP allows developers to choose among several levels of parallelism.
For example, they can set the number of threads manually.
It also allows developers to identify whether data are shared between threads or are private to a thread.
OpenMP is available on several open-source and commercial compilers for Linux, Windows, and Mac OS X systems.
We encourage readers interested in learning more about OpenMP to consult the bibliography at the end of the chapter.
Grand Central Dispatch (GCD)—a technology for Apple’s Mac OS X and iOS operating systems—is a combination of extensions to the C language, an API, and a run-time library that allows application developers to identify sections of code to run in parallel.
Like OpenMP, GCD manages most of the details of threading.
When it removes a block from a queue, it assigns the block to an available thread from the thread pool it manages.
Blocks placed on a serial queue are removed in FIFO order.
Once a block has been removed from the queue, it must complete execution before another block is removed.
Each process has its own serial queue (known as its main queue)
Developers can create additional serial queues that are local to particular processes.
Serial queues are useful for ensuring the sequential execution of several tasks.
Blocks placed on a concurrent queue are also removed in FIFO order, but several blocks may be removed at a time, thus allowing multiple blocks to execute in parallel.
There are three system-wide concurrent dispatch queues, and they are distinguished according to priority: low, default, and high.
Priorities represent an approximation of the relative importance of blocks.
Quite simply, blocks with a higher priority should be placed on the highpriority dispatch queue.
The following code segment illustrates obtaining the default-priority concurrent queue and submitting a block to the queue using the dispatch async() function:
Thread pools, OpenMP, and Grand Central Dispatch are just a few of many emerging technologies for managing multithreaded applications.
Other commercial approaches include parallel and concurrent libraries, such as Intel’s Threading Building Blocks (TBB) and several products from Microsoft.
The Java language and API have seen signiﬁcant movement toward supporting concurrent programming as well.
In this section, we discuss some of the issues to consider in designing multithreaded programs.
In Chapter 3, we described how the fork() system call is used to create a separate, duplicate process.
The semantics of the fork() and exec() system calls change in a multithreaded program.
If one thread in a program calls fork(), does the new process duplicate all threads, or is the new process single-threaded? Some UNIX systems have chosen to have two versions of fork(), one that duplicates all threads and another that duplicates only the thread that invoked the fork() system call.
That is, if a thread invokes the exec() system call, the program speciﬁed in the parameter to exec()will replace the entire process—including all threads.
Which of the two versions of fork() to use depends on the application.
If exec() is called immediately after forking, then duplicating all threads is unnecessary, as the program speciﬁed in the parameters to exec() will replace the process.
In this instance, duplicating only the calling thread is appropriate.
If, however, the separate process does not callexec() after forking, the separate process should duplicate all threads.
A signal is used in UNIX systems to notify a process that a particular event has occurred.
All signals, whether synchronous or asynchronous, follow the same pattern:
A signal is generated by the occurrence of a particular event.
If a running program performs either of these actions, a signal is generated.
Synchronous signals are delivered to the same process that performed the operation that caused the signal (that is the reason they are considered synchronous)
When a signal is generated by an event external to a running process, that process receives the signal asynchronously.
Examples of such signals include terminating a process with speciﬁc keystrokes (such as <control><C>) and having a timer expire.
A signal may be handled by one of two possible handlers:
Every signal has a default signal handler that the kernel runs when handling that signal.
This default action can be overridden by a user-deﬁned signal handler that is called to handle the signal.
Some signals (such as changing the size of a window) are simply ignored; others (such as an illegal memory access) are handled by terminating the program.
Handling signals in single-threaded programs is straightforward: signals are always delivered to a process.
However, delivering signals is more complicated in multithreaded programs, where a process may have several threads.
Deliver the signal to the thread to which the signal applies.
Assign a speciﬁc thread to receive all signals for the process.
The method for delivering a signal depends on the type of signal generated.
For example, synchronous signals need to be delivered to the thread causing the signal and not to other threads in the process.
However, the situation with asynchronous signals is not as clear.
Some asynchronous signals—such as a signal that terminates a process (<control><C>, for example)—should be sent to all threads.
This function speciﬁes the process (pid) to which a particular signal (signal) is to be delivered.
Most multithreaded versions of UNIX allow a thread to specify which signals it will accept and which it will block.
Therefore, in some cases, an asynchronous signal may be delivered only to those threads that are not blocking it.
However, because signals need to be handled only once, a signal is typically delivered only to the ﬁrst thread found that is not blocking it.
Although Windows does not explicitly provide support for signals, it allows us to emulate them using asynchronous procedure calls (APCs)
The APC facility enables a user thread to specify a function that is to be called when the user thread receives notiﬁcation of a particular event.
As indicated by its name, an APC is roughly equivalent to an asynchronous signal in UNIX.
However, whereas UNIX must contend with how to deal with signals in a multithreaded environment, the APC facility is more straightforward, since an APC is delivered to a particular thread rather than a process.
Thread cancellation involves terminating a thread before it has completed.
For example, if multiple threads are concurrently searching through a database and one thread returns the result, the remaining threads might be canceled.
Another situation might occur when a user presses a button on a web browser that stops a web page from loading any further.
Often, a web page loads using several threads—each image is loaded in a separate thread.
When a user presses the stop button on the browser, all threads loading the page are canceled.
A thread that is to be canceled is often referred to as the target thread.
Cancellation of a target thread may occur in two different scenarios:
The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion.
The difﬁculty with cancellation occurs in situations where resources have been allocated to a canceled thread or where a thread is canceled while in the midst of updating data it is sharing with other threads.
Often, the operating system will reclaim system resources from a canceled thread but will not reclaim all resources.
Therefore, canceling a thread asynchronously may not free a necessary system-wide resource.
With deferred cancellation, in contrast, one thread indicates that a target thread is to be canceled, but cancellation occurs only after the target thread has checked a ﬂag to determine whether or not it should be canceled.
The thread can perform this check at a point at which it can be canceled safely.
In Pthreads, thread cancellation is initiated using the pthread cancel() function.
The identiﬁer of the target thread is passed as a parameter to the function.
Invoking pthread cancel()indicates only a request to cancel the target thread, however; actual cancellation depends on how the target thread is set up to handle the request.
Each mode is deﬁned as a state and a type, as illustrated in the table below.
A thread may set its cancellation state and type using an API.
As the table illustrates, Pthreads allows threads to disable or enable cancellation.
Obviously, a thread cannot be canceled if cancellation is disabled.
However, cancellation requests remain pending, so the thread can later enable cancellation and respond to the request.
Here, cancellation occurs only when a thread reaches a cancellation point.
One technique for establishing a cancellation point is to invoke the pthread testcancel() function.
If a cancellation request is found to be pending, a function known as a cleanup handler is invoked.
This function allows any resources a thread may have acquired to be released before the thread is terminated.
The following code illustrates how a thread may respond to a cancellation request using deferred cancellation:
Because of the issues described earlier, asynchronous cancellation is not recommended in Pthreads documentation.
An interesting note is that on Linux systems, thread cancellation using the Pthreads API is handled through signals (Section 4.6.2)
Threads belonging to a process share the data of the process.
Indeed, this data sharing provides one of the beneﬁts of multithreaded programming.
However, in some circumstances, each thread might need its own copy of certain data.
To associate each thread with its unique identiﬁer, we could use thread-local storage.
However, local variables are visible only during a single function invocation, whereas TLS data are visible across function invocations.
The difference is that TLS data are unique to each thread.
A ﬁnal issue to be considered with multithreaded programs concerns communication between the kernel and the thread library, which may be required by the many-to-many and two-level models discussed in Section 4.3.3
Such coordination allows the number of kernel threads to be dynamically adjusted to help ensure the best performance.
Many systems implementing either the many-to-many or the two-level model place an intermediate data structure between the user and kernel threads.
To the user-thread library, the LWP appears to be a virtual processor on which the application can schedule a user thread to run.
Each LWP is attached to a kernel thread, and it is kernel threads that the.
If a kernel thread blocks (such as while waiting for an I/O operation to complete), the LWP blocks as well.
Up the chain, the user-level thread attached to the LWP also blocks.
An application may require any number of LWPs to run efﬁciently.
In this scenario, only one thread can run at at a time, so one LWP is sufﬁcient.
An application that is I/O-intensive may require multiple LWPs to execute, however.
Typically, an LWP is required for each concurrent blocking system call.
Suppose, for example, that ﬁve different ﬁle-read requests occur simultaneously.
Five LWPs are needed, because all could be waiting for I/O completion in the kernel.
If a process has only four LWPs, then the ﬁfth request must wait for one of the LWPs to return from the kernel.
One scheme for communication between the user-thread library and the kernel is known as scheduler activation.
It works as follows: The kernel provides an application with a set of virtual processors (LWPs), and the application can schedule user threads onto an available virtual processor.
Furthermore, the kernel must inform an application about certain events.
Upcalls are handled by the thread library with an upcall handler, and upcall handlers must run on a virtual processor.
One event that triggers an upcall occurs when an application thread is about to block.
In this scenario, the kernel makes an upcall to the application informing it that a thread is about to block and identifying the speciﬁc thread.
The kernel then allocates a new virtual processor to the application.
The application runs an upcall handler on this new virtual processor, which saves the state of the blocking thread and relinquishes the virtual processor on which the blocking thread is running.
The upcall handler then schedules another thread that is eligible to run on the new virtual processor.
When the event that the blocking thread was waiting for occurs, the kernel makes another upcall to the thread library informing it that the previously blocked thread is now eligible to run.
The upcall handler for this event also requires a virtual processor, and the kernel may allocate a new virtual processor or preempt one of the user threads and run the upcall handler on its virtual processor.
After marking the unblocked thread as eligible to run, the application schedules an eligible thread to run on an available virtual processor.
At this point, we have examined a number of concepts and issues related to threads.
We conclude the chapter by exploring how threads are implemented in Windows and Linux systems.
Indeed, much of what is mentioned in this section applies to this entire family of operating systems.
A Windows application runs as a separate process, and each process may contain one or more threads.
Additionally, Windows uses the one-to-one mapping described in Section 4.3.2, where each user-level thread maps to an associated kernel thread.
A user stack, employed when the thread is running in user mode, and a.
A private storage area used by various run-time libraries and dynamic link libraries (DLLs)
The register set, stacks, and private storage area are known as the context of the thread.
The key components of the ETHREAD include a pointer to the process to which the thread belongs and the address of the routine in which the thread starts control.
The ETHREAD also contains a pointer to the corresponding KTHREAD.
The KTHREAD includes scheduling and synchronization information for the thread.
In addition, the KTHREAD includes the kernel stack (used when the thread is running in kernel mode) and a pointer to the TEB.
The ETHREAD and the KTHREAD exist entirely in kernel space; this means that only the kernel can access them.
The TEB is a user-space data structure that is accessed when the thread is running in user mode.
Among other ﬁelds, the TEB contains the thread identiﬁer, a user-mode stack, and an array for thread-local storage.
The structure of a Windows thread is illustrated in Figure 4.14
Linux also provides the ability to create threads using the clone() system call.
In fact, Linux uses the term task —rather than process or thread— when referring to a ﬂow of control within a program.
When clone() is invoked, it is passed a set of ﬂags that determine how much sharing is to take place between the parent and child tasks.
Using clone() in this fashion is equivalent to creating a thread as described in this chapter, since the parent task shares most of its resources with its child task.
However, if none of these ﬂags is set when clone() is invoked, no sharing takes place, resulting in functionality similar to that provided by the fork() system call.
The varying level of sharing is possible because of the way a task is represented in the Linux kernel.
A unique kernel data structure (speciﬁcally, struct task struct) exists for each task in the system.
This data structure, instead of storing data for the task, contains pointers to other data structures where these data are stored—for example, data structures that represent the list of open ﬁles, signal-handling information, and virtual memory.
When fork() is invoked, a new task is created, along with a copy of all the associated data.
Figure 4.15 Some of the ﬂags passed when clone() is invoked.
A new task is also created when the clone() system call is made.
However, rather than copying all data structures, the new task points to the data structures of the parent task, depending on the set of ﬂags passed to clone()
A thread is a ﬂow of control within a process.
A multithreaded process contains several different ﬂows of control within the same address space.
The beneﬁts of multithreading include increased responsiveness to the user, resource sharing within the process, economy, and scalability factors, such as more efﬁcient use of multiple processing cores.
User-level threads are threads that are visible to the programmer and are unknown to the kernel.
In general, user-level threads are faster to create and manage than are kernel threads, because no intervention from the kernel is required.
Three different types of models relate user and kernel threads.
The manyto-one model maps many user threads to a single kernel thread.
The one-to-one model maps each user thread to a corresponding kernel thread.
The many-tomany model multiplexes many user threads to a smaller or equal number of kernel threads.
Thread libraries provide the application programmer with an API for creating and managing threads.
Three primary thread libraries are in common use: POSIX Pthreads, Windows threads, and Java threads.
In addition to explicitly creating threads using the API provided by a library, we can use implicit threading, in which the creation and management of threading is transferred to compilers and run-time libraries.
Multithreaded programs introduce many challenges for programmers, including the semantics of the fork() and exec() system calls.
Other issues include signal handling, thread cancellation, thread-local storage, and scheduler activations.
Furthermore, the system allows developers to create real-time threads for use in real-time systems.
Is it necessary to bind a real-time thread to an LWP? Explain.
Would the same beneﬁts have been achieved if instead Chrome had been designed to open each new website in a separate thread? Explain.
The multithreaded Sudoku validator described in Project 1 in this.
The multithreaded sorting program described in Project 2 in this chapter.
All input is performed at program start-up, when a single ﬁle must be opened.
Your task is to improve the performance of this application by multithreading it.
The application runs on a system that uses the one-to-one threading model (each user thread maps to a kernel thread)
How many threads will you create to perform the input and output? Explain.
How many threads will you create for the CPU-intensive portion of the application? Explain.
Instead, Linux treats both in the same way, allowing a task to be more akin to a process or a thread depending on the set of ﬂags passed to the clone() system call.
However, other operating systems, such as Windows, treat processes and threads differently.
Typically, such systems use a notation in which the data structure for a process contains pointers to the separate threads belonging to the process.
Contrast these two approaches for modeling processes and threads within the kernel.
What would be the output from the program at LINE C and LINE P?
Let the number of user-level threads in the program be greater than the number of processing cores in the system.
The number of kernel threads allocated to the program is less than the number of processing cores.
The number of kernel threads allocated to the program is equal to the number of processing cores.
The number of kernel threads allocated to the program is greater than the number of processing cores but less than the number of user-level threads.
The pthread setcancelstate() function is used to set the cancellation state.
Using the code segment shown in Figure 4.17, provide examples of two operations that would be suitable to perform between the calls to disable and enable thread cancellation.
This modiﬁcation will consist of writing a multithreaded program that tests your solution to Exercise 3.20
You will create a number of threads—for example, 100—and each thread will request a pid, sleep for a random period of time, and then release the pid.
Sleeping for a random period of time approximates the typical pid usage in which a pid is assigned to a new process, the process executes and then terminates, and the pid is released on the process’s termination.
On UNIX and Linux systems, sleeping is accomplished through the sleep() function, which is passed an integer value representing the number of seconds to sleep.
This program will be passed a series of numbers on the command line and will then create three separate worker threads.
One thread will determine the average of the numbers, the second will determine the maximum value, and the third will determine the minimum value.
The variables representing the average, minimum, and maximum values will be stored globally.
The worker threads will set these values, and the parent thread will output the values once the workers have exited.
We could obviously expand this program by creating additional threads that determine other statistical values, such as median and standard deviation.
This technique works as follows: Suppose you have a circle inscribed within a square, as shown in Figure.
This program should work as follows: The user will run the program and will enter a number on the command line.
The program will then create a separate thread that outputs all the prime numbers less than or equal to the number entered by the user.
This program should work as follows: On the command line, the user will enter the number of Fibonacci numbers that the program is to generate.
The program will then create a separate thread that will generate the Fibonacci numbers, placing the sequence in data that can be shared by the threads (an array is probably the most convenient data structure)
When the thread ﬁnishes execution, the parent thread will output the sequence generated by the child thread.
Because the parent thread cannot begin outputting the Fibonacci sequence until the child thread ﬁnishes, the parent thread will have to wait for the child thread to ﬁnish.
Use the techniques described in Section 4.4 to meet this requirement.
This server is single-threaded, meaning that the server cannot respond to concurrent echo clients until the current client exits.
Modify the solution to Exercise 3.25 so that the echo server services each client in a separate request.
One suggested strategy is to create threads that check the following criteria:
This would result in a total of eleven separate threads for validating a Sudoku puzzle.
However, you are welcome to create even more threads for this project.
For example, rather than creating one thread that checks all nine.
The parent thread will create the worker threads, passing each worker the location that it must check in the Sudoku grid.
This step will require passing several parameters to each thread.
The easiest approach is to create a data structure using a struct.
For example, a structure to pass the row and column where a thread must begin validating would appear as follows:
Both Pthreads and Windows programs will create worker threads using a strategy similar to that shown below:
The data pointer will be passed to either the pthread create() (Pthreads) function or the CreateThread() (Windows) function, which in turn will pass it as a parameter to the function that is to run as a separate thread.
Each worker thread is assigned the task of determining the validity of a particular region of the Sudoku puzzle.
One good way to handle this is to create an array of integer values that is visible to each thread.
The i th index in this array corresponds to the i th worker thread.
If a worker sets its corresponding value to 1, it is indicating that its region of the Sudoku puzzle is valid.
When all worker threads have completed, the parent thread checks each entry in the result array to determine if the Sudoku puzzle is valid.
Write a multithreaded sorting program that works as follows: A list of integers is divided into two smaller lists of equal size.
Two separate threads (which we will term sorting threads) sort each sublist using a sorting algorithm of your choice.
The two sublists are then merged by a third thread—a merging thread —which merges the two sublists into a single sorted list.
Because global data are shared cross all threads, perhaps the easiest way to set up the data is to create a global array.
Each sorting thread will work on one half of this array.
A second global array of the same size as the unsorted integer array will also be established.
The merging thread will then merge the two sublists into this second array.
This programming project will require passing parameters to each of the sorting threads.
In particular, it will be necessary to identify the starting index from which each thread is to begin sorting.
Refer to the instructions in Project 1 for details on passing parameters to a thread.
The parent thread will output the sorted array once all sorting threads have exited.
A cooperating process is one that can affect or be affected by other processes executing in the system.
Cooperating processes can either directly share a logical address space (that is, both code and data) or be allowed to share data only through ﬁles or messages.
Concurrent access to shared data may result in data inconsistency, however.
In this chapter, we discuss various mechanisms to ensure the orderly execution of cooperating processes that share a logical address space, so that data consistency is maintained.
To introduce the critical-section problem, whose solutions can be used to ensure the consistency of shared data.
To present both software and hardware solutions of the critical-section problem.
To explore several tools that are used to solve process synchronization problems.
We’ve already seen that processes can execute concurrently or in parallel.
Section 3.2.2 introduced the role of process scheduling and described how the CPU scheduler switches rapidly between processes to provide concurrent execution.
This means that one process may only partially complete execution before another process is scheduled.
In fact, a process may be interrupted at any point in its instruction stream, and the processing core may be assigned to execute instructions of another process.
Additionally, Section 4.2 introduced parallel execution, in which two instruction streams (representing different processes) execute simultaneously on separate processing cores.
In Chapter 3, we developed a model of a system consisting of cooperating sequential processes or threads, all running asynchronously and possibly sharing data.
We illustrated this model with the producer–consumer problem, which is representative of operating systems.
Speciﬁcally, in Section 3.4.1, we described how a bounded buffer could be used to enable processes to share memory.
The code for the consumer process can be modiﬁed as follows:
Although the producer and consumer routines shown above are correct separately, they may not function correctly when executed concurrently.
As an illustration, suppose that the value of the variable counter is currently 5 and that the producer and consumer processes concurrently execute the statements “counter++” and “counter--”
We can show that the value of counter may be incorrect as follows.
Note that the statement “counter++” may be implemented in machine language (on a typical machine) as follows:
The concurrent execution of “counter++” and “counter--” is equivalent to a sequential execution in which the lower-level statements presented previously are interleaved in some arbitrary order (but the order within each high-level statement is preserved)
Notice that we have arrived at the incorrect state “counter == 4”, indicating that four buffers are full, when, in fact, ﬁve buffers are full.
We would arrive at this incorrect state because we allowed both processes to manipulate the variable counter concurrently.
A situation like this, where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place, is called a race condition.
To guard against the race condition above, we need to ensure that only one process at a time can be manipulating the variable counter.
To make such a guarantee, we require that the processes be synchronized in some way.
Situations such as the one just described occur frequently in operating systems as different parts of the system manipulate resources.
Furthermore, as we have emphasized in earlier chapters, the growing importance of multicore systems has brought an increased emphasis on developing multithreaded applications.
In such applications, several threads—which are quite possibly sharing data—are running in parallel on different processing cores.
Because of the importance of this issue, we devote a major portion of this chapter to process synchronization and coordination among cooperating processes.
A solution to the critical-section problem must satisfy the following three requirements:
If process Pi is executing in its critical section, then no other processes can be executing in their critical sections.
If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indeﬁnitely.
There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after a.
We assume that each process is executing at a nonzero speed.
However, we can make no assumption concerning the relative speed of the n processes.
At a given point in time, many kernel-mode processes may be active in the operating system.
As a result, the code implementing an operating system (kernel code) is subject to several possible race conditions.
Consider as an example a kernel data structure that maintains a list of all open ﬁles in the system.
This list must be modiﬁed when a new ﬁle is opened or closed (adding the ﬁle to the list or removing it from the list)
If two processes were to open ﬁles simultaneously, the separate updates to this list could result in a race condition.
Other kernel data structures that are prone to possible race conditions include structures for maintaining memory allocation, for maintaining process lists, and for interrupt handling.
It is up to kernel developers to ensure that the operating system is free from such race conditions.
Two general approaches are used to handle critical sections in operating systems: preemptive kernels and nonpreemptive kernels.
A preemptive kernel allows a process to be preempted while it is running in kernel mode.
A nonpreemptive kernel does not allow a process running in kernel mode to be preempted; a kernel-mode process will run until it exits kernel mode, blocks, or voluntarily yields control of the CPU.
Obviously, a nonpreemptive kernel is essentially free from race conditions on kernel data structures, as only one process is active in the kernel at a time.
We cannot say the same about preemptive kernels, so they must be carefully designed to ensure that shared kernel data are free from race conditions.
Preemptive kernels are especially difﬁcult to design for SMP architectures, since in these environments it is possible for two kernel-mode processes to run simultaneously on different processors.
Why, then, would anyone favor a preemptive kernel over a nonpreemptive one? A preemptive kernel may be more responsive, since there is less risk that a kernel-mode process will run for an arbitrarily long period before relinquishing the processor to waiting processes.
Of course, this risk can also be minimized by designing kernel code that does not behave in this way.
Furthermore, a preemptive kernel is more suitable for real-time programming, as it will allow a real-time process to preempt a process currently running in the kernel.
Later in this chapter, we explore how various operating systems manage preemption within the kernel.
Next, we illustrate a classic software-based solution to the critical-section problem known as Peterson’s solution.
Because of the way modern computer architectures perform basic machine-language instructions, such as load and store, there are no guarantees that Peterson’s solution will work correctly on such architectures.
However, we present the solution because it provides a good algorithmic description of solving the critical-section problem and illustrates some of the complexities involved in designing software that addresses the requirements of mutual exclusion, progress, and bounded waiting.
Peterson’s solution requires the two processes to share two data items:
The variable turn indicates whose turn it is to enter its critical section.
That is, if turn == i, then process Pi is allowed to execute in its critical section.
The flag array is used to indicate if a process is ready to enter its critical section.
For example, if flag[i] is true, this value indicates that Pi is ready to enter its critical section.
With an explanation of these data structures complete, we are now ready to describe the algorithm shown in Figure 5.2
To enter the critical section, process Pi ﬁrst sets flag[i] to be true and then sets turn to the value j, thereby asserting that if the other process wishes to enter the critical section, it can do so.
If both processes try to enter at the same time, turnwill be set to bothi and j at roughly the same time.
Only one of these assignments will last; the other will occur but will be overwritten immediately.
The eventual value of turn determines which of the two processes is allowed to enter its critical section ﬁrst.
To prove property 1, we note that each Pi enters its critical section only if either flag[j] == false or turn == i.
Hence, one of the processes —say, Pj —must have successfully executed the while statement, whereas Pi had to execute at least one additional statement (“turn == j”)
However, at that time, flag[j] == true and turn == j, and this condition will persist as long as Pj is in its critical section; as a result, mutual exclusion is preserved.
If Pj is not ready to enter the critical section, then flag[j] == false, and Pi can enter its critical section.
If Pj has set flag[j] to true and is also executing in its while statement, then either turn == i or turn == j.
If turn == i, then Pi will enter the critical section.
If turn == j, then Pj will enter the critical section.
However, once Pj exits its critical section, it will reset flag[j] to false, allowing Pi to enter its critical section.
If Pj resets flag[j] to true, it must also set turn to i.
Thus, since Pi does not change the value of the variable turn while executing the while statement, Pi will enter the critical section (progress) after at most one entry by Pj (bounded waiting)
We have just described one software-based solution to the critical-section problem.
However, as mentioned, software-based solutions such as Peterson’s are not guaranteed to work on modern computer architectures.
In the following discussions, we explore several more solutions to the critical-section problem using techniques ranging from hardware to software-based APIs available to both kernel developers and application programmers.
All these solutions are based on the premise of locking —that is, protecting critical regions through the use of locks.
As we shall see, the designs of such locks can be quite sophisticated.
We start by presenting some simple hardware instructions that are available on many systems and showing how they can be used effectively in solving the critical-section problem.
Hardware features can make any programming task easier and improve system efﬁciency.
The critical-section problem could be solved simply in a single-processor environment if we could prevent interrupts from occurring while a shared variable was being modiﬁed.
In this way, we could be sure that the current sequence of instructions would be allowed to execute in order without preemption.
No other instructions would be run, so no unexpected modiﬁcations could be made to the shared variable.
Figure 5.3 The deﬁnition of the test and set() instruction.
Unfortunately, this solution is not as feasible in a multiprocessor environment.
Disabling interrupts on a multiprocessor can be time consuming, since the message is passed to all the processors.
This message passing delays entry into each critical section, and system efﬁciency decreases.
Also consider the effect on a system’s clock if the clock is kept updated by interrupts.
Many modern computer systems therefore provide special hardware instructions that allow us either to test and modify the content of a word or to swap the contents of two words atomically—that is, as one uninterruptible unit.
We can use these special instructions to solve the critical-section problem in a relatively simple manner.
Rather than discussing one speciﬁc instruction for one speciﬁc machine, we abstract the main concepts behind these types of instructions by describing the test and set() and compare and swap() instructions.
The test and set() instruction can be deﬁned as shown in Figure 5.3
The important characteristic of this instruction is that it is executed atomically.
Thus, if two test and set() instructions are executed simultaneously (each on a different CPU), they will be executed sequentially in some arbitrary order.
If the machine supports the test and set() instruction, then we can implement mutual exclusion by declaring a boolean variable lock, initialized to false.
The structure of process Pi is shown in Figure 5.4
The compare and swap() instruction, in contrast to the test and set() instruction, operates on three operands; it is deﬁned in Figure 5.5
The operand value is set to new value only if the expression (*value == exected) is true.
Regardless, compare and swap() always returns the original value of the variable value.
Like the test and set() instruction, compare and swap() is.
Figure 5.5 The deﬁnition of the compare and swap() instruction.
Figure 5.6 Mutual-exclusion implementation with the compare and swap() instruction.
When a process exits its critical section, it sets lock back to 0, which allows another process to enter its critical section.
The structure of process Pi is shown in Figure 5.6
Although these algorithms satisfy the mutual-exclusion requirement, they do not satisfy the bounded-waiting requirement.
In Figure 5.7, we present another algorithm using the test and set() instruction that satisﬁes all the critical-section requirements.
To prove that the mutualexclusion requirement is met, we note that process Pi can enter its critical section only if either waiting[i] == false or key == false.
The value of key can become false only if the test and set() is executed.
The ﬁrst process to execute the test and set() will ﬁnd key == false; all others must wait.
The variable waiting[i] can become false only if another process leaves its critical section; only one waiting[i] is set to false, maintaining the mutual-exclusion requirement.
To prove that the progress requirement is met, we note that the arguments presented for mutual exclusion also apply here, since a process exiting the critical section either sets lock to false or sets waiting[j] to false.
Both allow a process that is waiting to enter its critical section to proceed.
Details describing the implementation of the atomic test and set() and compare and swap() instructions are discussed more fully in books on computer architecture.
The hardware-based solutions to the critical-section problem presented in Section 5.4 are complicated as well as generally inaccessible to application programmers.
Instead, operating-systems designers build software tools to solve the critical-section problem.
In fact, the term mutex is short for mutual exclusion.
We use the mutex lock to protect critical regions and thus prevent race conditions.
That is, a process must acquire the lock before entering a critical section; it releases the lock when it exits the critical section.
The acquire()function acquires the lock, and the release() function releases the lock, as illustrated in Figure 5.8
A mutex lock has a boolean variable available whose value indicates if the lock is available or not.
If the lock is available, a call to acquire() succeeds, and the lock is then considered unavailable.
A process that attempts to acquire an unavailable lock is blocked until the lock is released.
Calls to either acquire() or release() must be performed atomically.
Thus, mutex locks are often implemented using one of the hardware mechanisms described in Section 5.4, and we leave the description of this technique as an exercise.
The main disadvantage of the implementation given here is that it requires busy waiting.
While a process is in its critical section, any other process that tries to enter its critical section must loop continuously in the call to acquire()
In fact, this type of mutex lock is also called a spinlock because the process “spins” while waiting for the lock to become available.
We see the same issue with the code examples illustrating the test and set() instruction and the compare and swap() instruction.
This continual looping is clearly a problem in a real multiprogramming system, where a single CPU is shared among many processes.
Busy waiting wastes CPU cycles that some other process might be able to use productively.
Spinlocks do have an advantage, however, in that no context switch is required when a process must wait on a lock, and a context switch may take considerable time.
Thus, when locks are expected to be held for short times, spinlocks are useful.
They are often employed on multiprocessor systems where one thread can “spin” on one processor while another thread performs its critical section on another processor.
Later in this chapter (Section 5.7), we examine how mutex locks can be used to solve classical synchronization problems.
We also discuss how these locks are used in several operating systems, as well as in Pthreads.
Mutex locks, as we mentioned earlier, are generally considered the simplest of synchronization tools.
In this section, we examine a more robust tool that can.
The value of a counting semaphore can range over an unrestricted domain.
In fact, on systems that do not provide mutex locks, binary semaphores can be used instead for providing mutual exclusion.
Counting semaphores can be used to control access to a given resource consisting of a ﬁnite number of instances.
The semaphore is initialized to the number of resources available.
Each process that wishes to use a resource performs a wait() operation on the semaphore (thereby decrementing the count)
When a process releases a resource, it performs a signal() operation (incrementing the count)
When the count for the semaphore goes to 0, all resources are being used.
We can also use semaphores to solve various synchronization problems.
Recall that the implementation of mutex locks discussed in Section 5.5 suffers from busy waiting.
The deﬁnitions of the wait() and signal() semaphore operations just described present the same problem.
To overcome the need for busy waiting, we can modify the deﬁnition of the wait() and signal() operations as follows: When a process executes the wait() operation and ﬁnds that the semaphore value is not positive, it must wait.
However, rather than engaging in busy waiting, the process can block itself.
The block operation places a process into a waiting queue associated with the semaphore, and the state of the process is switched to the waiting state.
Then control is transferred to the CPU scheduler, which selects another process to execute.
A process that is blocked, waiting on a semaphore S, should be restarted when some other process executes a signal() operation.
The process is restarted by a wakeup() operation, which changes the process from the waiting state to the ready state.
The CPU may or may not be switched from the running process to the newly ready process, depending on the CPU-scheduling algorithm.
To implement semaphores under this deﬁnition, we deﬁne a semaphore as follows:
Each semaphore has an integer value and a list of processes list.
When a process must wait on a semaphore, it is added to the list of processes.
A signal() operation removes one process from the list of waiting processes and awakens that process.
The wakeup(P) operation resumes the execution of a blocked process P.
These two operations are provided by the operating system as basic system calls.
Note that in this implementation, semaphore values may be negative, whereas semaphore values are never negative under the classical deﬁnition of semaphores with busy waiting.
If a semaphore value is negative, its magnitude is the number of processes waiting on that semaphore.
This fact results from switching the order of the decrement and the test in the implementation of the wait() operation.
The list of waiting processes can be easily implemented by a link ﬁeld in each process control block (PCB)
Each semaphore contains an integer value and a pointer to a list of PCBs.
One way to add and remove processes from the list so as to ensure bounded waiting is to use a FIFO queue, where the semaphore contains both head and tail pointers to the queue.
In general, however, the list can use any queueing strategy.
Correct usage of semaphores does not depend on a particular queueing strategy for the semaphore lists.
We must guarantee that no two processes can execute wait() and signal() operations on the same semaphore at the same time.
This is a critical-section problem; and in a single-processor environment, we can solve it by simply inhibiting interrupts during the time the wait() and signal() operations are executing.
This scheme works in a single-processor environment because, once interrupts are inhibited, instructions from different processes cannot be interleaved.
Only the currently running process executes until interrupts are reenabled and the scheduler can regain control.
In a multiprocessor environment, interrupts must be disabled on every processor.
Otherwise, instructions from different processes (running on different processors) may be interleaved in some arbitrary way.
Disabling interrupts on every processor can be a difﬁcult task and furthermore can seriously diminish performance.
Therefore, SMP systems must provide alternative locking techniques—such as compare and swap() or spinlocks—to ensure that wait() and signal() are performed atomically.
It is important to admit that we have not completely eliminated busy waiting with this deﬁnition of the wait() and signal() operations.
Rather, we have moved busy waiting from the entry section to the critical sections of application programs.
Furthermore, we have limited busy waiting to the critical sections of the wait() and signal() operations, and these sections are short (if properly coded, they should be no more than about ten instructions)
Thus, the critical section is almost never occupied, and busy waiting occurs.
An entirely different situation exists with application programs whose critical sections may be long (minutes or even hours) or may almost always be occupied.
The implementation of a semaphore with a waiting queue may result in a situation where two or more processes are waiting indeﬁnitely for an event that can be caused only by one of the waiting processes.
The event in question is the execution of a signal() operation.
When such a state is reached, these processes are said to be deadlocked.
We say that a set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set.
The events with which we are mainly concerned here are resource acquisition and release.
In that chapter, we describe various mechanisms for dealing with the deadlock problem.
Another problem related to deadlocks is indeﬁnite blocking or starvation, a situation in which processes wait indeﬁnitely within the semaphore.
Indeﬁnite blocking may occur if we remove processes from the list associated with a semaphore in LIFO (last-in, ﬁrst-out) order.
A scheduling challenge arises when a higher-priority process needs to read or modify kernel data that are currently being accessed by a lower-priority process—or a chain of lower-priority processes.
Since kernel data are typically protected with a lock, the higher-priority process will have to wait for a lower-priority one to ﬁnish with the resource.
The situation becomes more complicated if the lower-priority process is preempted in favor of another process with a higher priority.
On systems with tight time constraints—such as real-time systems—priority inversion can cause a process to take longer than it should to accomplish a task.
When that happens, other failures can cascade, resulting in system failure.
Consider the Mars Pathﬁnder, a NASA space probe that landed a robot, the Sojourner rover, on Mars in 1997 to conduct experiments.
Shortly after the Sojourner began operating, it started to experience frequent computer resets.
If the problem had not been solved, the Sojourner would have failed in its mission.
The problem was caused by the fact that one high-priority task, “bc dist,” was taking longer than expected to complete its work.
This task was being forced to wait for a shared resource that was held by the lower-priority “ASI/MET” task, which in turn was preempted by multiple medium-priority tasks.
The “bc dist” task would stall waiting for the shared resource, and ultimately the “bc sched” task would discover the problem and perform the reset.
The Sojourner was suffering from a typical case of priority inversion.
The operating system on the Sojourner was the VxWorks real-time operating system, which had a global variable to enable priority inheritance on all semaphores.
A full description of the problem, its detection, and its solution was written by the software team lead and is available at http://research.microsoft.com/en-us/um/people/mbj/mars pathﬁnder/ authoritative account.html.
Ordinarily, process H would wait for L to ﬁnish using resource R.
However, now suppose that process M becomes runnable, thereby preempting process L.
Indirectly, a process with a lower priority—process M—has affected how long process H must wait for L to relinquish resource R.
It occurs only in systems with more than two priorities, so one solution is to have only two priorities.
According to this protocol, all processes that are accessing resources needed by a higher-priority process inherit the higher priority until they are ﬁnished with the resources in question.
When they are ﬁnished, their priorities revert to their original values.
When process L had ﬁnished using resource R, it would relinquish its inherited priority from H and assume its original priority.
Because resource R would now be available, process H—not M—would run next.
These problems are used for testing nearly every newly proposed synchronization scheme.
In our solutions to the problems, we use semaphores for synchronization, since that is the traditional way to present such solutions.
However, actual implementations of these solutions could use mutex locks in place of binary semaphores.
The bounded-buffer problem was introduced in Section 5.1; it is commonly used to illustrate the power of synchronization primitives.
Here, we present a general structure of this scheme without committing ourselves to any particular implementation.
We provide a related programming project in the exercises at the end of the chapter.
In our problem, the producer and consumer processes share the following data structures:
We assume that the pool consists of n buffers, each capable of holding one item.
The empty and full semaphores count the number of empty and full buffers.
We can interpret this code as the producer producing full buffers for the consumer or as the consumer producing empty buffers for the producer.
Suppose that a database is to be shared among several concurrent processes.
Some of these processes may want only to read the database, whereas others may want to update (that is, to read and write) the database.
We distinguish between these two types of processes by referring to the former as readers and to the latter as writers.
Obviously, if two readers access the shared data simultaneously, no adverse effects will result.
However, if a writer and some other process (either a reader or a writer) access the database simultaneously, chaos may ensue.
To ensure that these difﬁculties do not arise, we require that the writers have exclusive access to the shared database while writing to the database.
This synchronization problem is referred to as the readers–writers problem.
Since it was originally stated, it has been used to test nearly every new synchronization primitive.
The simplest one, referred to as the ﬁrst readers–writers problem, requires that no reader be kept waiting unless a writer has already obtained permission to use the shared object.
In other words, no reader should wait for other readers to ﬁnish simply because a writer is waiting.
The second readers –writers problem requires that, once a writer is ready, that writer perform its write as soon as possible.
In other words, if a writer is waiting to access the object, no new readers may start reading.
In the ﬁrst case, writers may starve; in the second case, readers may starve.
For this reason, other variants of the problem have been proposed.
Next, we present a solution to the ﬁrst readers–writers problem.
See the bibliographical notes at the end of the chapter for references describing starvation-free solutions to the second readers–writers problem.
In the solution to the ﬁrst readers–writers problem, the reader processes share the following data structures:
The semaphore rw mutex is common to both reader and writer.
The mutex semaphore is used to ensure mutual exclusion when the variable read count is updated.
The read count variable keeps track of how many processes are currently reading the object.
The semaphore rw mutex functions as a mutual exclusion semaphore for the writers.
It is also used by the ﬁrst or last reader that enters or exits the critical section.
It is not used by readers who enter or exit while other readers are in their critical sections.
The readers–writers problem and its solutions have been generalized to provide reader–writer locks on some systems.
Acquiring a reader–writer lock requires specifying the mode of the lock: either read or write access.
When a process wishes only to read shared data, it requests the reader–writer lock in read mode.
A process wishing to modify the shared data must request the lock in write mode.
Multiple processes are permitted to concurrently acquire a reader–writer lock in read mode, but only one process may acquire the lock for writing, as exclusive access is required for writers.
In applications where it is easy to identify which processes only read shared data and which processes only write shared data.
This is because readerwriter locks generally require more overhead to establish than semaphores or mutual-exclusion locks.
The increased concurrency of allowing multiple readers compensates for the overhead involved in setting up the readerwriter lock.
Consider ﬁve philosophers who spend their lives thinking and eating.
The philosophers share a circular table surrounded by ﬁve chairs, each belonging to one philosopher.
In the center of the table is a bowl of rice, and the table is laid with ﬁve single chopsticks (Figure 5.13)
When a philosopher thinks, she does not interact with her colleagues.
From time to time, a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her (the chopsticks that are between her and her left and right neighbors)
A philosopher may pick up only one chopstick at a time.
Obviously, she cannot pick up a chopstick that is already in the hand of a neighbor.
When a hungry philosopher has both her chopsticks at the same time, she eats without releasing the chopsticks.
When she is ﬁnished eating, she puts down both chopsticks and starts thinking again.
It is a simple representation of the need to allocate several resources among several processes in a deadlock-free and starvation-free manner.
One simple solution is to represent each chopstick with a semaphore.
A philosopher tries to grab a chopstick by executing a wait() operation on that semaphore.
She releases her chopsticks by executing the signal() operation on the appropriate semaphores.
The structure of philosopher i is shown in Figure 5.14
Although this solution guarantees that no two neighbors are eating simultaneously, it nevertheless must be rejected because it could create a deadlock.
Suppose that all ﬁve philosophers become hungry at the same time and each grabs her left chopstick.
When each philosopher tries to grab her right chopstick, she will be delayed forever.
Several possible remedies to the deadlock problem are replaced by:
Allow at most four philosophers to be sitting simultaneously at the table.
Allow a philosopher to pick up her chopsticks only if both chopsticks are.
Use an asymmetric solution—that is, an odd-numbered philosopher picks up ﬁrst her left chopstick and then her right chopstick, whereas an evennumbered philosopher picks up her right chopstick and then her left chopstick.
A deadlock-free solution does not necessarily eliminate the possibility of starvation.
Although semaphores provide a convenient and effective mechanism for process synchronization, using them incorrectly can result in timing errors that are difﬁcult to detect, since these errors happen only if particular execution sequences take place and these sequences do not always occur.
We have seen an example of such errors in the use of counters in our solution to the producer–consumer problem (Section 5.1)
In that example, the timing problem happened only rarely, and even then the counter value.
It is for this reason that semaphores were introduced in the ﬁrst place.
Unfortunately, such timing errors can still occur when semaphores are used.
To illustrate how, we review the semaphore solution to the critical-section problem.
If this sequence is not observed, two processes may be in their critical sections simultaneously.
Note that these difﬁculties will arise even if a single process is not well behaved.
This situation may be caused by an honest programming error or an uncooperative programmer.
Suppose that a process interchanges the order in which the wait() and signal() operations on the semaphore mutex are executed, resulting in the following execution:
In this situation, several processes may be executing in their critical sections simultaneously, violating the mutual-exclusion requirement.
This error may be discovered only if several processes are simultaneously active in their critical sections.
Suppose that a process omits the wait(mutex), or the signal(mutex), or both.
In this case, either mutual exclusion is violated or a deadlock will occur.
These examples illustrate that various types of errors can be generated easily when programmers use semaphores incorrectly to solve the critical-section problem.
Similar problems may arise in the other synchronization models discussed in Section 5.7
To deal with such errors, researchers have developed high-level language constructs.
In this section, we describe one fundamental high-level synchronization construct—the monitor type.
An abstract data type—or ADT—encapsulates data with a set of functions to operate on that data that are independent of any speciﬁc implementation of the ADT.
A monitor type is an ADT that includes a set of programmerdeﬁned operations that are provided with mutual exclusion within the monitor.
The monitor type also declares the variables whose values deﬁne the state of an instance of that type, along with the bodies of functions that operate on those variables.
The syntax of a monitor type is shown in Figure 5.15
The representation of a monitor type cannot be used directly by the various processes.
Thus, a function deﬁned within a monitor can access only those variables declared locally within the monitor and its formal parameters.
Similarly, the local variables of a monitor can be accessed by only the local functions.
The monitor construct ensures that only one process at a time is active within the monitor.
Consequently, the programmer does not need to code this synchronization constraint explicitly (Figure 5.16)
However, the monitor construct, as deﬁned so far, is not sufﬁciently powerful for modeling some synchronization schemes.
For this purpose, we need to deﬁne additional synchronization mechanisms.
A programmer who needs to write a tailor-made synchronization scheme can deﬁne one or more variables of type condition:
The only operations that can be invoked on a condition variable are wait() and signal()
If no process is suspended, then the signal() operation has no effect; that is, the state of x is the same as if the operation had never been executed (Figure 5.17)
Contrast this operation with the signal() operation associated with semaphores, which always affects the state of the semaphore.
Now suppose that, when the x.signal() operation is invoked by a process P, there exists a suspended process Q associated with condition x.
Clearly, if the suspended process Q is allowed to resume its execution, the signaling process P must wait.
Otherwise, both P and Q would be active simultaneously within the monitor.
Note, however, that conceptually both processes can continue with their execution.
There are reasonable arguments in favor of adopting either option.
On the one hand, since P was already executing in the monitor, the signal-andcontinue method seems more reasonable.
On the other, if we allow thread P to continue, then by the time Q is resumed, the logical condition for which Q was waiting may no longer hold.
A compromise between these two choices was adopted in the language Concurrent Pascal.
When thread P executes the signal operation, it immediately leaves the monitor.
Many programming languages have incorporated the idea of the monitor as described in this section, including Java and C# (pronounced “C-sharp”)
Other languages—such as Erlang—provide some type of concurrency support using a similar mechanism.
This solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available.
To code this solution, we need to distinguish among three states in which we may ﬁnd a philosopher.
This allows philosopher i to delay herself when she is hungry but is unable to obtain the chopsticks she needs.
Each philosopher, before starting to eat, must invoke the operation pickup()
This act may result in the suspension of the philosopher process.
After the successful completion of the operation, the philosopher may eat.
Thus, philosopher i must invoke the operations pickup() and putdown() in the following sequence:
It is easy to show that this solution ensures that no two neighbors are eating simultaneously and that no deadlocks will occur.
We note, however, that it is possible for a philosopher to starve to death.
We do not present a solution to this problem but rather leave it as an exercise for you.
We now consider a possible implementation of the monitor mechanism using semaphores.
For each monitor, a semaphore mutex (initialized to 1) is provided.
A process must execute wait(mutex) before entering the monitor and must execute signal(mutex) after leaving the monitor.
An integer variable next count is also provided to count the number of processes suspended on next.
We can now describe how condition variables are implemented as well.
In some cases, however, the generality of the implementation is unnecessary, and a signiﬁcant improvement in efﬁciency is possible.
We turn now to the subject of process-resumption order within a monitor.
If several processes are suspended on condition x, and an x.signal() operation is executed by some process, then how do we determine which of the suspended processes should be resumed next? One simple solution is to use a ﬁrst-come, ﬁrst-served (FCFS) ordering, so that the process that has been waiting the longest is resumed ﬁrst.
In many circumstances, however, such a simple scheduling scheme is not adequate.
The value of c, which is called a priority number, is then stored with the name of the process that is suspended.
When x.signal() is executed, the process with the smallest priority number is resumed next.
To illustrate this new mechanism, consider the ResourceAllocator monitor shown in Figure 5.19, which controls the allocation of a single resource among competing processes.
Each process, when requesting an allocation of this resource, speciﬁes the maximum time it plans to use the resource.
The monitor allocates the resource to the process that has the shortest time-allocation request.
A process that needs to access the resource in question must observe the following sequence:
A process might access a resource without ﬁrst gaining access permission to the resource.
A process might never release a resource once it has been granted access to the resource.
A process might attempt to release a resource that it never requested.
The same difﬁculties are encountered with the use of semaphores, and these difﬁculties are similar in nature to those that encouraged us to develop the monitor constructs in the ﬁrst place.
Previously, we had to worry about the correct use of semaphores.
Now, we have to worry about the correct use of higher-level programmer-deﬁned operations, with which the compiler can no longer assist us.
One possible solution to the current problem is to include the resourceaccess operations within the ResourceAllocator monitor.
However, using this solution will mean that scheduling is done according to the built-in monitor-scheduling algorithm rather than the one we have coded.
To ensure that the processes observe the appropriate sequences, we must inspect all the programs that make use of the ResourceAllocator monitor and its managed resource.
We must check two conditions to establish the correctness of this system.
First, user processes must always make their calls on the monitor in a correct sequence.
Second, we must be sure that an uncooperative process does not simply ignore the mutual-exclusion gateway provided by the monitor and try to access the shared resource directly, without using the access protocols.
Only if these two conditions can be ensured can we guarantee that no time-dependent errors will occur and that the scheduling algorithm will not be defeated.
Every object in Java has associated with it a single lock.
When a method is declared to be synchronized, calling the method requires owning the lock for the object.
We declare a synchronized method by placing the synchronized keyword in the method deﬁnition.
Next, we create an object instance of SimpleClass, such as the following:
Invoking sc.safeMethod() method requires owning the lock on the object instance sc.
If the lock is already owned by another thread, the thread calling the synchronizedmethod blocks and is placed in the entry set for the object’s lock.
The entry set represents the set of threads waiting for the lock to become available.
If the lock is available when a synchronized method is called, the calling thread becomes the owner of the object’s lock and can enter the method.
The lock is released when the thread exits the method.
A thread from the entry set is then selected as the new owner of the lock.
Java also provides wait() and notify() methods, which are similar in function to the wait() and signal() statements for a monitor.
Although this inspection may be possible for a small, static system, it is not reasonable for a large system or a dynamic system.
We next describe the synchronization mechanisms provided by the Windows, Linux, and Solaris operating systems, as well as the Pthreads API.
We have chosen these three operating systems because they provide good examples of different approaches to synchronizing the kernel, and we have included the.
Pthreads API because it is widely used for thread creation and synchronization by developers on UNIX and Linux systems.
As you will see in this section, the synchronization methods available in these differing systems vary in subtle and signiﬁcant ways.
The Windows operating system is a multithreaded kernel that provides support for real-time applications and multiple processors.
When the Windows kernel accesses a global resource on a single-processor system, it temporarily masks interrupts for all interrupt handlers that may also access the global resource.
On a multiprocessor system, Windows protects access to global resources using spinlocks, although the kernel uses spinlocks only to protect short code segments.
Furthermore, for reasons of efﬁciency, the kernel ensures that a thread will never be preempted while holding a spinlock.
For thread synchronization outside the kernel, Windows provides dispatcher objects.
Using a dispatcher object, threads synchronize according to several different mechanisms, including mutex locks, semaphores, events, and timers.
The system protects shared data by requiring a thread to gain ownership of a mutex to access the data and to release ownership when it is ﬁnished.
Events are similar to condition variables; that is, they may notify a waiting thread when a desired condition occurs.
Finally, timers are used to notify one (or more than one) thread that a speciﬁed amount of time has expired.
Dispatcher objects may be in either a signaled state or a nonsignaled state.
An object in a signaled state is available, and a thread will not block when acquiring the object.
An object in a nonsignaled state is not available, and a thread will block when attempting to acquire the object.
We illustrate the state transitions of a mutex lock dispatcher object in Figure 5.20
A relationship exists between the state of a dispatcher object and the state of a thread.
When a thread blocks on a nonsignaled dispatcher object, its state changes from ready to waiting, and the thread is placed in a waiting queue for that object.
When the state for the dispatcher object moves to signaled, the kernel checks whether any threads are waiting on the object.
If so, the kernel moves one thread—or possibly more—from the waiting state to the ready state, where they can resume executing.
The number of threads the kernel selects from the waiting queue depends on the type of dispatcher object for which it is waiting.
The kernel will select only one thread from the waiting queue for a mutex, since a mutex object may be “owned” by only a single.
For an event object, the kernel will select all threads that are waiting for the event.
We can use a mutex lock as an illustration of dispatcher objects and thread states.
If a thread tries to acquire a mutex dispatcher object that is in a nonsignaled state, that thread will be suspended and placed in a waiting queue for the mutex object.
When the mutex moves to the signaled state (because another thread has released the lock on the mutex), the thread waiting at the front of the queue will be moved from the waiting state to the ready state and will acquire the mutex lock.
A critical-section object is a user-mode mutex that can often be acquired and released without kernel intervention.
On a multiprocessor system, a critical-section object ﬁrst uses a spinlock while waiting for the other thread to release the object.
If it spins too long, the acquiring thread will then allocate a kernel mutex and yield its CPU.
Critical-section objects are particularly efﬁcient because the kernel mutex is allocated only when there is contention for the object.
In practice, there is very little contention, so the savings are signiﬁcant.
We provide a programming project at the end of this chapter that uses mutex locks and semaphores in the Windows API.
Prior to Version 2.6, Linux was a nonpreemptive kernel, meaning that a process running in kernel mode could not be preempted—even if a higher-priority process became available to run.
Now, however, the Linux kernel is fully preemptive, so a task can be preempted when it is running in the kernel.
Linux provides several different mechanisms for synchronization in the kernel.
As most computer architectures provide instructions for atomic versions of simple math operations, the simplest synchronization technique within the Linux kernel is an atomic integer, which is represented using the opaque data type atomic t.
As the name implies, all math operations using atomic integers are performed without interruption.
The following code illustrates declaring an atomic integer counter and then performing various atomic operations:
Atomic integers are particularly efﬁcient in situations where an integer variable —such as a counter—needs to be updated, since atomic operations do not require the overhead of locking mechanisms.
However, their usage is limited to these sorts of scenarios.
In situations where there are several variables contributing to a possible race condition, more sophisticated locking tools must be used.
Mutex locks are available in Linux for protecting critical sections within the kernel.
Here, a task must invoke the mutex lock() function prior to entering.
If the mutex lock is unavailable, a task calling mutex lock() is put into a sleep state and is awakened when the lock’s owner invokes mutex unlock()
Linux also provides spinlocks and semaphores (as well as reader–writer versions of these two locks) for locking in the kernel.
On SMP machines, the fundamental locking mechanism is a spinlock, and the kernel is designed so that the spinlock is held only for short durations.
On single-processor machines, such as embedded systems with only a single processing core, spinlocks are inappropriate for use and are replaced by enabling and disabling kernel preemption.
That is, on single-processor systems, rather than holding a spinlock, the kernel disables kernel preemption; and rather than releasing the spinlock, it enables kernel preemption.
Linux uses an interesting approach to disable and enable kernel preemption.
It provides two simple system calls—preempt disable() and preempt enable()—for disabling and enabling kernel preemption.
The kernel is not preemptible, however, if a task running in the kernel is holding a lock.
To enforce this rule, each task in the system has a thread-info structure containing a counter, preempt count, to indicate the number of locks being held by the task.
If the value of preempt count for the task currently running in the kernel is greater than 0, it is not safe to preempt the kernel, as this task currently holds a lock.
If the count is 0, the kernel can safely be interrupted (assuming there are no outstanding calls to preempt disable())
Spinlocks—along with enabling and disabling kernel preemption—are used in the kernel only when a lock (or disabling kernel preemption) is held for a short duration.
When a lock must be held for a longer period, semaphores or mutex locks are appropriate for use.
To control access to critical sections, Solaris provides adaptive mutex locks, condition variables, semaphores, reader–writer locks, and turnstiles.
An adaptive mutex protects access to every critical data item.
On a multiprocessor system, an adaptive mutex starts as a standard semaphore implemented as a spinlock.
If the data are locked and therefore already in use, the adaptive mutex does one of two things.
If the lock is held by a thread that is currently running on another CPU, the thread spins while waiting for the lock to become available, because the thread holding the lock is likely to ﬁnish soon.
If the thread holding the lock is not currently in run state, the thread.
It is put to sleep so that it will not spin while waiting, since the lock will not be freed very soon.
A lock held by a sleeping thread is likely to be in this category.
On a single-processor system, the thread holding the lock is never running if the lock is being tested by another thread, because only one thread can run at a time.
Therefore, on this type of system, threads always sleep rather than spin if they encounter a lock.
Solaris uses the adaptive-mutex method to protect only data that are accessed by short code segments.
That is, a mutex is used if a lock will be held for less than a few hundred instructions.
If the code segment is longer than that, the spin-waiting method is exceedingly inefﬁcient.
For these longer code segments, condition variables and semaphores are used.
If the desired lock is already held, the thread issues a wait and sleeps.
When a thread frees the lock, it issues a signal to the next sleeping thread in the queue.
The extra cost of putting a thread to sleep and waking it, and of the associated context switches, is less than the cost of wasting several hundred instructions waiting in a spinlock.
Reader–writer locks are used to protect data that are accessed frequently but are usually accessed in a read-only manner.
In these circumstances, reader–writer locks are more efﬁcient than semaphores, because multiple threads can read data concurrently, whereas semaphores always serialize access to the data.
Reader–writer locks are relatively expensive to implement, so again they are used only on long sections of code.
Solaris uses turnstiles to order the list of threads waiting to acquire either an adaptive mutex or a reader–writer lock.
A turnstile is a queue structure containing threads blocked on a lock.
For example, if one thread currently owns the lock for a synchronized object, all other threads trying to acquire the lock will block and enter the turnstile for that lock.
When the lock is released, the kernel selects a thread from the turnstile as the next owner of the lock.
Each synchronized object with at least one thread blocked on the object’s lock requires a separate turnstile.
However, rather than associating a turnstile with each synchronized object, Solaris gives each kernel thread its own turnstile.
Because a thread can be blocked only on one object at a time, this is more efﬁcient than having a turnstile for each object.
The turnstile for the ﬁrst thread to block on a synchronized object becomes the turnstile for the object itself.
Threads subsequently blocking on the lock will be added to this turnstile.
When the initial thread ultimately releases the lock, it gains a new turnstile from a list of free turnstiles maintained by the kernel.
This means that if a lower-priority thread currently holds a lock on which a higher-priority thread is blocked, the thread with the lower priority will temporarily inherit the priority of the higher-priority thread.
Upon releasing the lock, the thread will revert to its original priority.
Note that the locking mechanisms used by the kernel are implemented for user-level threads as well, so the same types of locks are available inside and outside the kernel.
To optimize Solaris performance, developers have reﬁned and ﬁne-tuned the locking methods.
Because locks are used frequently and typically are used for crucial kernel functions, tuning their implementation and use can produce great performance gains.
Although the locking mechanisms used in Solaris are available to user-level threads as well as kernel threads, basically the synchronization methods discussed thus far pertain to synchronization within the kernel.
In contrast, the Pthreads API is available for programmers at the user level and is not part of any particular kernel.
This API provides mutex locks, condition variables, and read–write locks for thread synchronization.
Mutex locks represent the fundamental synchronization technique used with Pthreads.
A mutex lock is used to protect critical sections of code—that is, a thread acquires the lock before entering a critical section and releases it upon exiting the critical section.
Pthreads uses the pthread mutex t data type for mutex locks.
A mutex is created with the pthread mutex init() function.
By passing NULL as a second parameter, we initialize the mutex to its default attributes.
The mutex is acquired and released with the pthread mutex lock() and pthread mutex unlock() functions.
If the mutex lock is unavailable when pthread mutex lock() is invoked, the calling thread is blocked until the owner invokes pthread mutex unlock()
The following code illustrates protecting a critical section with mutex locks:
All mutex functions return a value of 0 with correct operation; if an error occurs, these functions return a nonzero error code.
Many systems that implement Pthreads also provide semaphores, although semaphores are not part of the Pthreads standard and instead belong to the POSIX SEM extension.
The fundamental distinction between the two is that a named semaphore has an actual name in the ﬁle system and can be shared by multiple unrelated processes.
Unnamed semaphores can be used only by threads belonging to the same process.
The code below illustrates the sem init() function for creating and initializing an unnamed semaphore:
In this example, by passing the ﬂag 0, we are indicating that this semaphore can be shared only by threads belonging to the process that created the semaphore.
A nonzero value would allow other processes to access the semaphore as well.
In Section 5.6, we described the classical wait() and signal() semaphore operations.
Pthreads names these operations sem wait() and sem post(), respectively.
The following code sample illustrates protecting a critical section using the semaphore created above:
Just like mutex locks, all semaphore functions return 0 when successful, and nonzero when an error condition occurs.
There are other extensions to the Pthreads API — including spinlocks but it is important to note that not all extensions are considered portable from one implementation to another.
We provide several programming problems and projects at the end of this chapter that use Pthreads mutex locks and condition variables as well as POSIX semaphores.
With the emergence of multicore systems has come increased pressure to develop multithreaded applications that take advantage of multiple processing.
However, multithreaded applications present an increased risk of race conditions and deadlocks.
Traditionally, techniques such as mutex locks, semaphores, and monitors have been used to address these issues, but as the number of processing cores increases, it becomes increasingly difﬁcult to design multithreaded applications that are free from race conditions and deadlocks.
In this section, we explore various features provided in both programming languages and hardware that support designing thread-safe concurrent applications.
Quite often in computer science, ideas from one area of study can be used to solve problems in other areas.
The concept of transactional memory originated in database theory, for example, yet it provides a strategy for process synchronization.
A memory transaction is a sequence of memory read–write operations that are atomic.
If all operations in a transaction are completed, the memory transaction is committed.
The beneﬁts of transactional memory can be obtained through features added to a programming language.
Traditionally, this function would be written using mutex locks (or semaphores) such as the following:
However, using synchronization mechanisms such as mutex locks and semaphores involves many potential problems, including deadlock.
Additionally, as the number of threads increases, traditional locking scales less well, because the level of contention among threads for lock ownership becomes very high.
The advantage of using such a mechanism rather than locks is that the transactional memory system—not the developer—is responsible for.
Additionally, because no locks are involved, deadlock is not possible.
Furthermore, a transactional memory system can identify which statements in atomic blocks can be executed concurrently, such as concurrent read access to a shared variable.
It is, of course, possible for a programmer to identify these situations and use reader–writer locks, but the task becomes increasingly difﬁcult as the number of threads within an application grows.
Transactional memory can be implemented in either software or hardware.
Software transactional memory (STM), as the name suggests, implements transactional memory exclusively in software—no special hardware is needed.
The code is inserted by a compiler and manages each transaction by examining where statements may run concurrently and where speciﬁc low-level locking is required.
Hardware transactional memory (HTM) uses hardware cache hierarchies and cache coherency protocols to manage and resolve conﬂicts involving shared data residing in separate processors’ caches.
However, HTM does require that existing cache hierarchies and cache coherency protocols be modiﬁed to support transactional memory.
Transactional memory has existed for several years without widespread implementation.
However, the growth of multicore systems and the associated emphasis on concurrent and parallel programming have prompted a signiﬁcant amount of research in this area on the part of both academics and commercial software and hardware vendors.
In Section 4.5.2, we provided an overview of OpenMP and its support of parallel programming in a shared-memory environment.
Recall that OpenMP includes a set of compiler directives and an API.
Any code following the compiler directive #pragma omp parallel is identiﬁed as a parallel region and is performed by a number of threads equal to the number of processing cores in the system.
The advantage of OpenMP (and similar tools) is that thread creation and management are handled by the OpenMP library and are not the responsibility of application developers.
In this way, OpenMP provides support for ensuring that threads do not generate race conditions.
As an example of the use of the critical-section compiler directive, ﬁrst assume that the shared variable counter can be modiﬁed in the update() function as follows:
If the update() function can be part of—or invoked from—a parallel region, a race condition is possible on the variable counter.
The critical-section compiler directive can be used to remedy this race condition and is coded as follows:
An advantage of using the critical-section compiler directive in OpenMP is that it is generally considered easier to use than standard mutex locks.
However, a disadvantage is that application developers must still identify possible race conditions and adequately protect shared data using the compiler directive.
Additionally, because the critical-section compiler directive behaves much like a mutex lock, deadlock is still possible when two or more critical sections are identiﬁed.
Most well-known programming languages—such as C, C++, Java, and C#are known as imperative (or procedural) languages.
Imperative languages are used for implementing algorithms that are state-based.
In these languages, the ﬂow of the algorithm is crucial to its correct operation, and state is represented with variables and other data structures.
Of course, program state is mutable, as variables may be assigned different values over time.
With the current emphasis on concurrent and parallel programming for multicore systems, there has been greater focus on functional programming languages, which follow a programming paradigm much different from that offered by imperative languages.
The fundamental difference between imperative and functional languages is that functional languages do not maintain state.
That is, once a variable has been deﬁned and assigned a value, its value is immutable—it cannot change.
Because functional languages disallow mutable state, they need not be concerned with issues such as race conditions and deadlocks.
Essentially, most of the problems addressed in this chapter are nonexistent in functional languages.
Several functional languages are presently in use, and we brieﬂy mention two of them here: Erlang and Scala.
The Erlang language has gained signiﬁcant attention because of its support for concurrency and the ease with which it can be used to develop applications that run on parallel systems.
In fact, much of the syntax of Scala is similar to the popular object-oriented languages Java and C#
Given a collection of cooperating sequential processes that share data, mutual exclusion must be provided to ensure that a critical section of code is used by only one process or thread at a time.
Typically, computer hardware provides several operations that ensure mutual exclusion.
However, such hardwarebased solutions are too complicated for most developers to use.
Both tools can be used to solve various synchronization problems and can be implemented efﬁciently, especially if hardware support for atomic operations is available.
These problems are used to test nearly every newly proposed synchronization scheme.
The operating system must provide the means to guard against timing errors, and several language constructs have been proposed to deal with these problems.
Monitors provide a synchronization mechanism for sharing abstract data types.
A condition variable provides a method by which a monitor function can block its execution until it is signaled to continue.
For example, Windows, Linux, and Solaris provide mechanisms such as semaphores, mutex locks, spinlocks, and condition variables to control access to shared data.
The Pthreads API provides support for mutex locks and semaphores, as well as condition variables.
One approach uses transactional memory, which may address synchronization issues using either software or hardware techniques.
Finally, functional programming languages address synchronization issues by disallowing mutability.
Explain why this can occur and how such effects can be minimized.
Describe the circumstances under which they use spinlocks, mutex locks, semaphores, adaptive mutex locks, and condition variables.
Consider a banking system that maintains an account balance with two functions: deposit(amount) and withdraw(amount)
These two functions are passed the amount that is to be deposited or withdrawn from the bank account balance.
Assume that a husband and wife share a bank account.
Concurrently, the husband calls the withdraw() function and the wife calls deposit()
Describe how a race condition is possible and what might be done to prevent the race condition from occurring.
Prove that the algorithm satisﬁes all three requirements for the critical-section problem.
The structure of process Pi is shown in Figure 5.22
Prove that the algorithm satisﬁes all three requirements for the critical-section problem.
Figure 5.21 The structure of process Pi in Dekker’s algorithm.
Be sure to include a description of how a race condition can occur.
Assume that the following structure deﬁning the mutex lock is available:
Using this struct, illustrate how the following functions can be implemented using the test and set() and compare and swap() instructions:
Be sure to include any initialization that may be necessary.
Describe what changes would be necessary so that a process waiting to acquire a mutex lock would be blocked and placed into a waiting queue until the lock became available.
For each of the following scenarios, describe which is a better locking mechanism—a spinlock or a mutex lock where waiting processes sleep while waiting for the lock to become available:
The lock is to be held for a short duration.
The lock is to be held for a long duration.
A thread may be put to sleep while holding the lock.
Suggest an upper bound (in terms of T) for holding a spinlock.
If the spinlock is held for any longer, a mutex lock (where waiting threads are put to sleep) is a better alternative.
Consider the two following strategies to prevent a race condition on the variable hits.
The ﬁrst strategy is to use a basic mutex lock when updating hits:
Assume you have a mutex lock named mutex with the operations acquire() and release()
Indicate where the locking needs to be placed to prevent the race condition(s)
For example, a server may wish to have only N socket connections at any point in time.
As soon as N connections are made, the server will not accept another incoming connection until an existing connection is released.
Explain how semaphores can be used by a server to limit the number of concurrent connections.
Whereas most implementations of reader–writer locks favor either readers or writers, or perhaps order waiting threads using a FIFO policy, slim reader–writer locks favor neither readers nor writers, nor are waiting threads ordered in a FIFO queue.
If we let the parent thread access the Fibonacci numbers as soon as they have been computed by the child thread—rather than waiting for the child thread to terminate—what changes would be necessary to the solution for this exercise? Implement your modiﬁed solution.
Design a new scheme that is suitable for larger portions.
Propose a method for solving the readers–writers problem without causing starvation.
Suggest how the implementation described in Section 5.8 can be simpliﬁed in this situation.
Write a monitor that allocates three identical printers to these processes, using the priority numbers for deciding the order of allocation.
The ﬁle can be accessed simultaneously by several processes, subject to the following constraint: the sum of all unique numbers associated with all the processes currently accessing the ﬁle must be less than n.
How would the solution to the preceding exercise differ with these two different ways in which signaling can be performed?
Write a monitor using this scheme to implement the readerswriters problem.
Explain why, in general, this construct cannot be implemented efﬁciently.
You may assume the existence of a real hardware clock that invokes a function tick() in your monitor at regular intervals.
Now modify your solution to Exercise 4.20 by ensuring that the data structure used to represent the availability of process identiﬁers is safe from race conditions.
Processes may ask for a number of these resources and will return them once ﬁnished.
As an example, many commercial software packages provide a given number of licenses, indicating the number of applications that may run concurrently.
When the application is started, the license count is decremented.
When the application is terminated, the license count is incremented.
If all licenses are in use, requests to start the application are denied.
Such requests will only be granted when an existing license holder terminates the application and a license is returned.
The following program segment is used to manage a ﬁnite number of instances of an available resource.
The maximum number of resources and the number of available resources are declared as follows:
When a process wishes to obtain a number of resources, it invokes the decrease count() function:
When a process wants to return a number of resources, it calls the increase count() function:
Identify the location (or locations) in the code where the race condition occurs.
Using a semaphore or mutex lock, ﬁx the race condition.
It is permissible to modify the decrease count() function so that the calling process is blocked until sufﬁcient resources are available.
Rewrite the resource-manager code segment using a monitor and condition variables so that the decrease count() function suspends the process until sufﬁcient resources are available.
This will allow a process to invoke decrease count() by simply calling.
The process will return from this function call only when sufﬁcient resources are available.
When a thread reaches a barrier point, it cannot proceed until all other threads have reached this point as well.
When the last thread reaches the barrier point, all threads are released and can resume concurrent execution.
Assume that the barrier is initialized to N—the number of threads that must wait at the barrier point:
Each thread then performs some work until it reaches the barrier point:
Using synchronization tools described in this chapter, construct a barrier that implements the following API:
A university computer science department has a teaching assistant (TA) who helps undergraduate students with their programming assignments during regular ofﬁce hours.
The TA’s ofﬁce is rather small and has room for only one desk with a chair and computer.
There are three chairs in the hallway outside the ofﬁce where students can sit and wait if the TA is currently helping another student.
When there are no students who need help during ofﬁce hours, the TA sits at the desk and takes a nap.
If a student arrives during ofﬁce hours and ﬁnds the TA sleeping, the student must awaken the TA to ask for help.
If a student arrives and ﬁnds the TA currently helping another student, the student sits on one of the chairs in the hallway and waits.
If no chairs are available, the student will come back at a later time.
Using POSIX threads, mutex locks, and semaphores, implement a solution that coordinates the activities of the TA and the students.
The TA will run as a separate thread as well.
Student threads will alternate between programming for a period of time and seeking help from the TA.
Otherwise, they will either sit in a chair in the hallway or, if no chairs are available, will resume programming and will seek help at a later time.
If a student arrives and notices that the TA is sleeping, the student must notify the TA using a semaphore.
When the TA ﬁnishes helping a student, the TA must check to see if there are students waiting for help in the hallway.
If so, the TA must help each of these students in turn.
If no students are present, the TA may return to napping.
Perhaps the best option for simulating students programming—as well as the TA providing help to a student—is to have the appropriate threads sleep for a random period of time.
Coverage of POSIX mutex locks and semaphores is provided in Section 5.9.4
This problem will require implementing a solution using Pthreads mutex locks and condition variables.
To simulate both activities, have the thread sleep for a random period between one and three seconds.
When a philosopher wishes to eat, she invokes the function.
Condition variables in Pthreads behave similarly to those described in Section 5.8
However, in that section, condition variables are used within the context of a monitor, which provides a locking mechanism to ensure data integrity.
Since Pthreads is typically used in C programs—and since C does not have a monitor— we accomplish locking by associating a condition variable with a mutex lock.
Condition variables in Pthreads use the pthread cond t data type and are initialized using the pthread cond init() function.
The following code creates and initializes a condition variable as well as its associated mutex lock:
The pthread cond wait() function is used for waiting on a condition variable.
The following code illustrates how a thread can wait for the condition a == b to become true using a Pthread condition variable:
The mutex lock associated with the condition variable must be locked before the pthread cond wait() function is called, since it is used to protect the data in the conditional clause from a possible race condition.
Once this lock is acquired, the thread can check the condition.
If the condition is not true, the thread then invokes pthread cond wait(), passing the mutex lock and the condition variable as parameters.
Calling pthread cond wait() releases the mutex lock, thereby allowing another thread to access the shared data and possibly update its value so that the condition clause evaluates to true.
To protect against program errors, it is important to place the conditional clause within a loop so that the condition is rechecked after being signaled.
A thread that modiﬁes the shared data can invoke the pthread cond signal() function, thereby signaling one thread waiting on the condition variable.
It is important to note that the call to pthread cond signal() does not release the mutex lock.
It is the subsequent call to pthread mutex unlock() that releases the mutex.
Once the mutex lock is released, the signaled thread becomes the owner of the mutex lock and returns control from the call to pthread cond wait()
In Section 5.7.1, we presented a semaphore-based solution to the producerconsumer problem using a bounded buffer.
The solution presented in Section 5.7.1 uses three semaphores: empty and full, which count the number of empty and full slots in the buffer, and mutex, which is a binary (or mutualexclusion) semaphore that protects the actual insertion or removal of items in the buffer.
For this project, you will use standard counting semaphores for empty and full and a mutex lock, rather than a binary semaphore, to represent mutex.
The producer and consumer—running as separate threads—will move items to and from a buffer that is synchronized with the empty,full, andmutex structures.
You can solve this problem using either Pthreads or the Windows API.
Internally, the buffer will consist of a ﬁxed-size array of type buffer item (which will be deﬁned using a typedef)
The array of buffer item objects will be manipulated as a circular queue.
The deﬁnition of buffer item, along with the size of the buffer, can be stored in a header ﬁle such as the following:
The buffer will be manipulated with two functions, insert item() and remove item(), which are called by the producer and consumer threads, respectively.
The buffer will also require an initialization function that initializes the mutual-exclusion object mutex along with the empty and full semaphores.
The main() function will initialize the buffer and create the separate producer and consumer threads.
Once it has created the producer and consumer threads, the main() function will sleep for a period of time and, upon awakening, will terminate the application.
The main() function will be passed three parameters on the command line:
The producer thread will alternate between sleeping for a random period of time and inserting a random integer into the buffer.
Random numbers will be produced using the rand() function, which produces random integers between 0 and RAND MAX.
The consumer will also sleep for a random period of time and, upon awakening, will attempt to remove an item from the buffer.
An outline of the producer and consumer threads appears in Figure 5.26
As noted earlier, you can solve this problem using either Pthreads or the Windows API.
In the following sections, we supply more information on each of these choices.
Creating threads using the Pthreads API is discussed in Section 4.4.1
Coverage of mutex locks and semaphores using Pthreads is provided in Section 5.9.4
Refer to those sections for speciﬁc instructions on Pthreads thread creation and synchronization.
Refer to that section for speciﬁc instructions on creating threads.
Mutex locks are a type of dispatcher object, as described in Section 5.9.1
The following illustrates how to create a mutex lock using the CreateMutex() function:
Figure 5.26 An outline of the producer and consumer threads.
The ﬁrst parameter refers to a security attribute for the mutex lock.
By setting this attribute to NULL, we disallow any children of the process creating this mutex lock to inherit the handle of the lock.
The second parameter indicates whether the creator of the mutex lock is the lock’s initial owner.
Passing a value of FALSE indicates that the thread creating the mutex is not the initial owner.
However, because we provide a value of NULL, we do not name the mutex.
If successful, CreateMutex() returns a HANDLE to the mutex lock; otherwise, it returns NULL.
In Section 5.9.1, we identiﬁed dispatcher objects as being either signaled or nonsignaled.
A signaled dispatcher object (such as a mutex lock) is available for ownership.
Once it is acquired, it moves to the nonsignaled state.
The function is passed the HANDLE to the lock along with a ﬂag indicating how long to wait.
The following code demonstrates how the mutex lock created above can be acquired:
The parameter value INFINITE indicates that we will wait an inﬁnite amount of time for the lock to become available.
Other values could be used that would allow the calling thread to time out if the lock did not become available within a speciﬁed time.
A lock is released (moves to the signaled state) by invoking ReleaseMutex()—for example, as follows:
Semaphores in the Windows API are dispatcher objects and thus use the same signaling mechanism as mutex locks.
The ﬁrst and last parameters identify a security attribute and a name for the semaphore, similar to what we described for mutex locks.
The second and third parameters indicate the initial value and maximum value of the semaphore.
If successful, CreateSemaphore() returns a HANDLE to the mutex lock; otherwise, it returns NULL.
We acquire the semaphore Sem created in this example by using the following statement:
Otherwise, the calling thread blocks indeﬁnitely—as we are specifying INFINITE—until the semaphore returns to the signaled state.
The equivalent of the signal() operation for Windows semaphores is the ReleaseSemaphore() function.
We can use the following statement to increase Sem by 1:
Both ReleaseSemaphore() and ReleaseMutex() return a nonzero value if successful and 0 otherwise.
The mutual-exclusion problem was ﬁrst discussed in a classic paper by [Dijkstra (1965)]
Dekker’s algorithm (Exercise 5.8)—the ﬁrst correct software solution to the two-process mutual-exclusion problem—was developed by the Dutch mathematician T.
Some details of the locking mechanisms used in Solaris were presented in [Mauro and McDougall (2007)]
As noted earlier, the locking mechanisms used by the kernel are implemented for user-level threads as well, so the same types of locks are available inside and outside the kernel.
By switching the CPU among processes, the operating system can make the computer more productive.
In this chapter, we introduce basic CPU-scheduling concepts and present several CPU-scheduling algorithms.
We also consider the problem of selecting an algorithm for a particular system.
In Chapter 4, we introduced threads to the process model.
On operating systems that support them, it is kernel-level threads—not processes—that are in fact being scheduled by the operating system.
However, the terms "process scheduling" and "thread scheduling" are often used interchangeably.
In this chapter, we use process scheduling when discussing general scheduling concepts and thread scheduling to refer to thread-speciﬁc ideas.
To introduce CPU scheduling, which is the basis for multiprogrammed operating systems.
To discuss evaluation criteria for selecting a CPU-scheduling algorithm for a particular system.
In a single-processor system, only one process can run at a time.
Others must wait until the CPU is free and can be rescheduled.
The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization.
A process is executed until it must wait, typically for the completion of some I/O request.
In a simple computer system, the CPU then just sits idle.
All this waiting time is wasted; no useful work is accomplished.
Every time one process has to wait, another process can take over use of the CPU.
The CPU is, of course, one of the primary computer resources.
The success of CPU scheduling depends on an observed property of processes: process execution consists of a cycle of CPU execution and I/O wait.
That is followed by an I/O burst, which is followed by another CPU burst, then another I/O burst, and so on.
Eventually, the ﬁnal CPU burst ends with a system request to terminate execution (Figure 6.1)
Although they vary greatly from process to process and from computer to computer, they tend to have a frequency curve similar to that shown in Figure 6.2
The curve is generally characterized as exponential or hyperexponential, with a large number of short CPU bursts and a small number of long CPU bursts.
A CPU-bound program might have a few long CPU bursts.
This distribution can be important in the selection of an appropriate CPU-scheduling algorithm.
Whenever the CPU becomes idle, the operating system must select one of the processes in the ready queue to be executed.
The selection process is carried out by the short-term scheduler, or CPU scheduler.
The scheduler selects a process from the processes in memory that are ready to execute and allocates the CPU to that process.
Note that the ready queue is not necessarily a ﬁrst-in, ﬁrst-out (FIFO) queue.
As we shall see when we consider the various scheduling algorithms, a ready queue can be implemented as a FIFO queue, a priority queue, a tree, or simply an unordered linked list.
Conceptually, however, all the processes in the ready queue are lined up waiting for a chance to run on the CPU.
The records in the queues are generally process control blocks (PCBs) of the processes.
CPU-scheduling decisions may take place under the following four circumstances:
When a process switches from the running state to the waiting state (for example, as the result of an I/O request or an invocation of wait() for the termination of a child process)
When a process switches from the running state to the ready state (for example, when an interrupt occurs)
When a process switches from the waiting state to the ready state (for example, at completion of I/O)
A new process (if one exists in the ready queue) must be selected for execution.
Under nonpreemptive scheduling, once the CPU has been allocated to a process, the process keeps the CPU until it releases the CPU either by terminating or by switching to the waiting state.
Windows 95 introduced preemptive scheduling, and all subsequent versions of Windows operating systems have used preemptive scheduling.
The Mac OS X operating system for the Macintosh also uses preemptive scheduling; previous versions of the Macintosh operating system relied on cooperative scheduling.
Cooperative scheduling is the only method that can be used on certain hardware platforms, because it does not require the special hardware (for example, a timer) needed for preemptive scheduling.
Unfortunately, preemptive scheduling can result in race conditions when data are shared among several processes.
While one process is updating the data, it is preempted so that the second process can run.
The second process then tries to read the data, which are in an inconsistent state.
During the processing of a system call, the kernel may be busy with an activity on behalf of a process.
Such activities may involve changing important kernel data (for instance, I/O queues)
What happens if the process is preempted in the middle of these changes and the kernel (or the device driver) needs to read or modify the same structure? Chaos ensues.
Certain operating systems, including most versions of UNIX, deal with this problem by waiting either for a system call to complete or for an I/O block to take place before doing a context switch.
This scheme ensures that the kernel structure is simple, since the kernel will not preempt a process while the kernel data structures are in an inconsistent state.
Unfortunately, this kernel-execution model is a poor one for supporting real-time computing where tasks must complete execution within a given time frame.
In Section 6.6, we explore scheduling demands of real-time systems.
Because interrupts can, by deﬁnition, occur at any time, and because they cannot always be ignored by the kernel, the sections of code affected by interrupts must be guarded from simultaneous use.
The operating system needs to accept interrupts at almost all times.
So that these sections of code are not accessed concurrently by several processes, they disable interrupts at entry and reenable interrupts at exit.
It is important to note that sections of code that disable interrupts do not occur very often and typically contain few instructions.
Another component involved in the CPU-scheduling function is the dispatcher.
The dispatcher is the module that gives control of the CPU to the process selected by the short-term scheduler.
Jumping to the proper location in the user program to restart that program.
The dispatcher should be as fast as possible, since it is invoked during every process switch.
The time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency.
Different CPU-scheduling algorithms have different properties, and the choice of a particular algorithm may favor one class of processes over another.
In choosing which algorithm to use in a particular situation, we must consider the properties of the various algorithms.
Which characteristics are used for comparison can make a substantial difference in which algorithm is judged to be best.
We want to keep the CPU as busy as possible.
If the CPU is busy executing processes, then work is being done.
One measure of work is the number of processes that are completed per time unit, called throughput.
For long processes, this rate may be one process per hour; for short transactions, it may be ten processes per second.
From the point of view of a particular process, the important criterion is how long it takes to execute that process.
The interval from the time of submission of a process to the time of completion is the turnaround time.
Turnaround time is the sum of the periods spent waiting to get into memory, waiting in the ready queue, executing on the CPU, and doing I/O.
The CPU-scheduling algorithm does not affect the amount of time during which a process executes or does I/O.
It affects only the amount of time that a process spends waiting in the ready queue.
Waiting time is the sum of the periods spent waiting in the ready queue.
In an interactive system, turnaround time may not be the best criterion.
Often, a process can produce some output fairly early and can continue computing new results while previous results are being.
Thus, another measure is the time from the submission of a request until the ﬁrst response is produced.
This measure, called response time, is the time it takes to start responding, not the time it takes to output the response.
The turnaround time is generally limited by the speed of the output device.
It is desirable to maximize CPU utilization and throughput and to minimize turnaround time, waiting time, and response time.
However, under some circumstances, we prefer to optimize the minimum or maximum values rather than the average.
For example, to guarantee that all users get good service, we may want to minimize the maximum response time.
Investigators have suggested that, for interactive systems (such as desktop systems), it is more important to minimize the variance in the response time than to minimize the average response time.
A system with reasonable and predictable response time may be considered more desirable than a system that is faster on the average but is highly variable.
However, little work has been done on CPU-scheduling algorithms that minimize variance.
As we discuss various CPU-scheduling algorithms in the following section, we illustrate their operation.
An accurate illustration should involve many processes, each a sequence of several hundred CPU bursts and I/O bursts.
For simplicity, though, we consider only one CPU burst (in milliseconds) per process in our examples.
By far the simplest CPU-scheduling algorithm is the ﬁrst-come, ﬁrst-served (FCFS) scheduling algorithm.
With this scheme, the process that requests the CPU ﬁrst is allocated the CPU ﬁrst.
The implementation of the FCFS policy is easily managed with a FIFO queue.
When a process enters the ready queue, its PCB is linked onto the tail of the queue.
When the CPU is free, it is allocated to the process at the head of the queue.
The code for FCFS scheduling is simple to write and understand.
On the negative side, the average waiting time under the FCFS policy is often quite long.
Consider the following set of processes that arrive at time 0, with the length of the CPU burst given in milliseconds:
Thus, the average waiting time under an FCFS policy is generally not minimal and may vary substantially if the processes’ CPU burst times vary greatly.
In addition, consider the performance of FCFS scheduling in a dynamic situation.
Assume we have one CPU-bound process and many I/O-bound processes.
As the processes ﬂow around the system, the following scenario may result.
During this time, all the other processes will ﬁnish their I/O and will move into the ready queue, waiting for the CPU.
While the processes wait in the ready queue, the I/O devices are idle.
Eventually, the CPU-bound process ﬁnishes its CPU burst and moves to an I/O device.
All the I/O-bound processes, which have short CPU bursts, execute quickly and move back to the I/O queues.
The CPU-bound process will then move back to the ready queue and be allocated the CPU.
Again, all the I/O processes end up waiting in the ready queue until the CPU-bound process is done.
There is a convoy effect as all the other processes wait for the one big process to get off the CPU.
This effect results in lower CPU and device utilization than might be possible if the shorter processes were allowed to go ﬁrst.
Once the CPU has been allocated to a process, that process keeps the CPU until it releases the CPU, either by terminating or by requesting I/O.
The FCFS algorithm is thus particularly troublesome for time-sharing systems, where it is important that each user get a share of the CPU at regular intervals.
It would be disastrous to allow one process to keep the CPU for an extended period.
A different approach to CPU scheduling is the shortest-job-ﬁrst (SJF) scheduling algorithm.
This algorithm associates with each process the length of the process’s next CPU burst.
When the CPU is available, it is assigned to the.
If the next CPU bursts of two processes are the same, FCFS scheduling is used to break the tie.
We use the term SJF because most people and textbooks use this term to refer to this type of scheduling.
As an example of SJF scheduling, consider the following set of processes, with the length of the CPU burst given in milliseconds:
Using SJF scheduling, we would schedule these processes according to the following Gantt chart:
By comparison, if we were using the FCFS scheduling scheme, the average waiting time would be 10.25 milliseconds.
The SJF scheduling algorithm is provably optimal, in that it gives the minimum average waiting time for a given set of processes.
Moving a short process before a long one decreases the waiting time of the short process more than it increases the waiting time of the long process.
The real difﬁculty with the SJF algorithm is knowing the length of the next CPU request.
For long-term (job) scheduling in a batch system, we can use the process time limit that a user speciﬁes when he submits the job.
Although the SJF algorithm is optimal, it cannot be implemented at the level of short-term CPU scheduling.
With short-term scheduling, there is no way to know the length of the next CPU burst.
One approach to this problem is to try to approximate SJF scheduling.
We may not know the length of the next CPU burst, but we may be able to predict its value.
We expect that the next CPU burst will be similar in length to the previous ones.
By computing an approximation of the length of the next CPU burst, we can pick the process with the shortest predicted CPU burst.
The next CPU burst is generally predicted as an exponential average of the measured lengths of previous CPU bursts.
Figure 6.3 Prediction of the length of the next CPU burst.
The choice arises when a new process arrives at the ready queue while a previous process is still executing.
The next CPU burst of the newly arrived process may be shorter than what is left of the currently executing process.
A preemptive SJF algorithm will preempt the currently executing process, whereas a nonpreemptive SJF algorithm will allow the currently running process to ﬁnish its CPU burst.
As an example, consider the following four processes, with the length of the CPU burst given in milliseconds:
If the processes arrive at the ready queue at the times shown and need the indicated burst times, then the resulting preemptive SJF schedule is as depicted in the following Gantt chart:
A priority is associated with each process, and the CPU is allocated to the process with the highest priority.
An SJF algorithm is simply a priority algorithm where the priority (p) is the inverse of the (predicted) next CPU burst.
The larger the CPU burst, the lower the priority, and vice versa.
Note that we discuss scheduling in terms of high priority and low priority.
However, there is no general agreement on whether 0 is the highest or lowest priority.
Some systems use low numbers to represent low priority; others use low numbers for high priority.
In this text, we assume that low numbers represent high priority.
Using priority scheduling, we would schedule these processes according to the following Gantt chart:
For example, time limits, memory requirements, the number of open ﬁles, and the ratio of average I/O burst to average CPU burst have been used in computing priorities.
External priorities are set by criteria outside the operating system, such as the importance of the process, the type and amount of funds being paid for computer use, the department sponsoring the work, and other, often political, factors.
When a process arrives at the ready queue, its priority is compared with the priority of the currently running process.
A preemptive priority scheduling algorithm will preempt the CPU if the priority of the newly arrived process is higher than the priority of the currently running process.
A nonpreemptive priority scheduling algorithm will simply put the new process at the head of the ready queue.
A major problem with priority scheduling algorithms is indeﬁnite blocking, or starvation.
A process that is ready to run but waiting for the CPU can be considered blocked.
A priority scheduling algorithm can leave some lowpriority processes waiting indeﬁnitely.
In a heavily loaded computer system, a steady stream of higher-priority processes can prevent a low-priority process from ever getting the CPU.
A solution to the problem of indeﬁnite blockage of low-priority processes is aging.
Aging involves gradually increasing the priority of processes that wait in the system for a long time.
Eventually, even a process with an initial priority of 127 would have the highest priority in the system and would be executed.
The round-robin (RR) scheduling algorithm is designed especially for timesharing systems.
It is similar to FCFS scheduling, but preemption is added to enable the system to switch between processes.
A small unit of time, called a time quantum or time slice, is deﬁned.
The CPU scheduler goes around the ready queue, allocating the CPU to each process for a time interval of up to 1 time quantum.
To implement RR scheduling, we again treat the ready queue as a FIFO queue of processes.
New processes are added to the tail of the ready queue.
The CPU scheduler picks the ﬁrst process from the ready queue, sets a timer to interrupt after 1 time quantum, and dispatches the process.
The process may have a CPU burst of less than 1 time quantum.
In this case, the process itself will release the CPU voluntarily.
The scheduler will then proceed to the next process in the ready queue.
If the CPU burst of the currently running process is longer than 1 time quantum, the timer will go off and will cause an interrupt to the operating system.
A context switch will be executed, and the process will be put at the tail of the ready queue.
The CPU scheduler will then select the next process in the ready queue.
The average waiting time under the RR policy is often long.
Consider the following set of processes that arrive at time 0, with the length of the CPU burst given in milliseconds:
The CPU is then given to the next process, process P3
In the RR scheduling algorithm, no process is allocated the CPU for more than 1 time quantum in a row (unless it is the only runnable process)
If a process’s CPU burst exceeds 1 time quantum, that process is preempted and is put back in the ready queue.
The performance of the RR algorithm depends heavily on the size of the time quantum.
At one extreme, if the time quantum is extremely large, the RR policy.
Figure 6.4 How a smaller time quantum increases context switches.
In contrast, if the time quantum is extremely small (say, 1 millisecond), the RR approach can result in a large number of context switches.
Assume, for example, that we have only one process of 10 time units.
Thus, we want the time quantum to be large with respect to the contextswitch time.
The time required for a context switch is typically less than 10 microseconds; thus, the context-switch time is a small fraction of the time quantum.
Turnaround time also depends on the size of the time quantum.
As we can see from Figure 6.5, the average turnaround time of a set of processes does not necessarily improve as the time-quantum size increases.
In general, the average turnaround time can be improved if most processes ﬁnish their next CPU burst in a single time quantum.
If context-switch time is added in, the average turnaround time increases even more for a smaller time quantum, since more context switches are required.
Although the time quantum should be large compared with the contextswitch time, it should not be too large.
As we pointed out earlier, if the time quantum is too large, RR scheduling degenerates to an FCFS policy.
A rule of thumb is that 80 percent of the CPU bursts should be shorter than the time quantum.
Another class of scheduling algorithms has been created for situations in which processes are easily classiﬁed into different groups.
Figure 6.5 How turnaround time varies with the time quantum.
These two types of processes have different response-time requirements and so may have different scheduling needs.
In addition, foreground processes may have priority (externally deﬁned) over background processes.
A multilevel queue scheduling algorithm partitions the ready queue into several separate queues (Figure 6.6)
The processes are permanently assigned to one queue, generally based on some property of the process, such as memory size, process priority, or process type.
For example, separate queues might be used for foreground and background processes.
The foreground queue might be scheduled by an RR algorithm, while the background queue is scheduled by an FCFS algorithm.
In addition, there must be scheduling among the queues, which is commonly implemented as ﬁxed-priority preemptive scheduling.
For example, the foreground queue may have absolute priority over the background queue.
Let’s look at an example of a multilevel queue scheduling algorithm with ﬁve queues, listed below in order of priority:
No process in the batch queue, for example, could run unless the queues for system processes, interactive processes, and interactive editing processes were all empty.
If an interactive editing process entered the ready queue while a batch process was running, the batch process would be preempted.
Here, each queue gets a certain portion of the CPU time, which it can then schedule among its various processes.
Normally, when the multilevel queue scheduling algorithm is used, processes are permanently assigned to a queue when they enter the system.
If there are separate queues for foreground and background processes, for example, processes do not move from one queue to the other, since processes do not change their foreground or background nature.
This setup has the advantage of low scheduling overhead, but it is inﬂexible.
The multilevel feedback queue scheduling algorithm, in contrast, allows a process to move between queues.
The idea is to separate processes according to the characteristics of their CPU bursts.
If a process uses too much CPU time, it will be moved to a lower-priority queue.
This scheme leaves I/O-bound and interactive processes in the higher-priority queues.
In addition, a process that waits too long in a lower-priority queue may be moved to a higher-priority queue.
This scheduling algorithm gives highest priority to any process with a CPU burst of 8 milliseconds or less.
Such a process will quickly get the CPU, ﬁnish its CPU burst, and go off to its next I/O burst.
In general, a multilevel feedback queue scheduler is deﬁned by the following parameters:
The method used to determine when to upgrade a process to a higherpriority queue.
The method used to determine when to demote a process to a lowerpriority queue.
The method used to determine which queue a process will enter when that process needs service.
The deﬁnition of a multilevel feedback queue scheduler makes it the most general CPU-scheduling algorithm.
It can be conﬁgured to match a speciﬁc system under design.
In Chapter 4, we introduced threads to the process model, distinguishing between user-level and kernel-level threads.
On operating systems that support them, it is kernel-level threads—not processes—that are being scheduled by the operating system.
User-level threads are managed by a thread library, and the kernel is unaware of them.
To run on a CPU, user-level threads must ultimately be mapped to an associated kernel-level thread, although this mapping may be indirect and may use a lightweight process (LWP)
In this section, we explore scheduling issues involving user-level and kernel-level threads and offer speciﬁc examples of scheduling for Pthreads.
One distinction between user-level and kernel-level threads lies in how they are scheduled.
This scheme is known as processcontention scope (PCS), since competition for the CPU takes place among threads belonging to the same process.
When we say the thread library schedules user threads onto available LWPs, we do not mean that the threads are actually running on a CPU.
That would require the operating system to schedule the kernel thread onto a physical CPU.
To decide which kernel-level thread to schedule onto a CPU, the kernel uses system-contention scope (SCS)
Competition for the CPU with SCS scheduling takes place among all threads in the system.
Systems using the one-to-one model (Section 4.3.2), such as Windows, Linux, and Solaris, schedule threads using only SCS.
Typically, PCS is done according to priority—the scheduler selects the runnable thread with the highest priority to run.
User-level thread priorities are set by the programmer and are not adjusted by the thread library, although some thread libraries may allow the programmer to change the priority of a thread.
It is important to note that PCS will typically preempt the thread currently running in favor of a higher-priority thread; however, there is no guarantee of time slicing (Section 6.3.4) among threads of equal priority.
We provided a sample POSIX Pthread program in Section 4.4.1, along with an introduction to thread creation with Pthreads.
Now, we highlight the POSIX Pthread API that allows specifying PCS or SCS during thread creation.
On systems implementing the many-to-many model, the PTHREAD SCOPE PROCESS policy schedules user-level threads onto available LWPs.
The number of LWPs is maintained by the thread library, perhaps using scheduler activations (Section 4.6.5)
The PTHREAD SCOPE SYSTEM scheduling policy will create and bind an LWP for each user-level thread on many-to-many systems, effectively mapping threads using the one-to-one policy.
The Pthread IPC provides two functions for getting—and setting—the contention scope policy:
The ﬁrst parameter for both functions contains a pointer to the attribute set for the thread.
The second parameter for the pthread attr setscope() function is passed either the PTHREAD SCOPE SYSTEM or the PTHREAD SCOPE PROCESS value, indicating how the contention scope is to be set.
In the case of pthread attr getscope(), this second parameter contains a pointer to an int value that is set to the current value of the contention scope.
If an error occurs, each of these functions returns a nonzero value.
The program ﬁrst determines the existing contention scope and sets it to PTHREAD SCOPE SYSTEM.
It then creates ﬁve separate threads that will run using the SCS scheduling policy.
Note that on some systems, only certain contention scope values are allowed.
Our discussion thus far has focused on the problems of scheduling the CPU in a system with a single processor.
If multiple CPUs are available, load sharing becomes possible—but scheduling problems become correspondingly more complex.
Many possibilities have been tried; and as we saw with singleprocessor CPU scheduling, there is no one best solution.
We can then use any available processor to run any process in the queue.
Note, however, that even with homogeneous multiprocessors, there are sometimes limitations on scheduling.
Consider a system with an I/O device attached to a private bus of one processor.
Processes that wish to use that device must be scheduled to run on that processor.
One approach to CPU scheduling in a multiprocessor system has all scheduling decisions, I/O processing, and other system activities handled by a single processor—the master server.
This asymmetric multiprocessing is simple because only one processor accesses the system data structures, reducing the need for data sharing.
A second approach uses symmetric multiprocessing (SMP), where each processor is self-scheduling.
All processes may be in a common ready queue, or each processor may have its own private queue of ready processes.
As we saw in Chapter 5, if we have multiple processors trying to access and update a common data structure, the scheduler must be programmed carefully.
We must ensure that two separate processors do not choose to schedule the same process and that processes are not lost from the queue.
In the remainder of this section, we discuss issues concerning SMP systems.
Consider what happens to cache memory when a process has been running on a speciﬁc processor.
The data most recently accessed by the process populate the cache for the processor.
As a result, successive memory accesses by the process are often satisﬁed in cache memory.
Now consider what happens if the process migrates to another processor.
The contents of cache memory must be invalidated for the ﬁrst processor, and the cache for the second processor must be repopulated.
Because of the high cost of invalidating and repopulating caches, most SMP systems try to avoid migration of processes from one processor to another and instead attempt to keep a process running on the same processor.
This is known as processor afﬁnity—that is, a process has an afﬁnity for the processor on which it is currently running.
When an operating system has a policy of attempting to keep a process running on the same processor—but not guaranteeing that it will do so—we have a situation known as soft afﬁnity.
Here, the operating system will attempt to keep a process on a single processor, but it is possible for a process to migrate between processors.
In contrast, some systems provide system calls that support hard afﬁnity, thereby allowing a process to specify a subset of processors on which it may run.
For example, Linux implements soft afﬁnity, but it also provides the sched setaffinity() system call, which supports hard afﬁnity.
The main-memory architecture of a system can affect processor afﬁnity issues.
Figure 6.9 illustrates an architecture featuring non-uniform memory access (NUMA), in which a CPU has faster access to some parts of main memory than to other parts.
Typically, this occurs in systems containing combined CPU and memory boards.
The CPUs on a board can access the memory on that board faster than they can access memory on other boards in the system.
If the operating system’s CPU scheduler and memory-placement algorithms work together, then a process that is assigned afﬁnity to a particular CPU can be allocated memory on the board where that CPU resides.
This example also shows that operating systems are frequently not as cleanly deﬁned and implemented as described in operating-system textbooks.
Rather, the “solid lines” between sections of an operating system are frequently only “dotted lines,” with algorithms creating connections in ways aimed at optimizing performance and reliability.
On SMP systems, it is important to keep the workload balanced among all processors to fully utilize the beneﬁts of having more than one processor.
Otherwise, one or more processors may sit idle while other processors have high workloads, along with lists of processes awaiting the CPU.
Load balancing attempts to keep the workload evenly distributed across all processors in an SMP system.
It is important to note that load balancing is typically necessary only on systems where each processor has its own private queue of eligible processes to execute.
On systems with a common run queue, load balancing is often unnecessary, because once a processor becomes idle, it immediately extracts a runnable process from the common run queue.
It is also important to note, however, that in most contemporary operating systems supporting SMP, each processor does have a private queue of eligible processes.
There are two general approaches to load balancing: push migration and pull migration.
With push migration, a speciﬁc task periodically checks the load on each processor and—if it ﬁnds an imbalance—evenly distributes the load by moving (or pushing) processes from overloaded to idle or less-busy processors.
Pull migration occurs when an idle processor pulls a waiting task from a busy processor.
Push and pull migration need not be mutually exclusive and are in fact often implemented in parallel on load-balancing systems.
For example, the Linux scheduler (described in Section 6.7.1) and the ULE scheduler available for FreeBSD systems implement both techniques.
Interestingly, load balancing often counteracts the beneﬁts of processor afﬁnity, discussed in Section 6.5.2
That is, the beneﬁt of keeping a process running on the same processor is that the process can take advantage of its data being in that processor’s cache memory.
Either pulling or pushing a process from one processor to another removes this beneﬁt.
As is often the case in systems engineering, there is no absolute rule concerning what policy is best.
Thus, in some systems, an idle processor always pulls a process from a non-idle processor.
In other systems, processes are moved only if the imbalance exceeds a certain threshold.
Traditionally, SMP systems have allowed several threads to run concurrently by providing multiple physical processors.
Each core maintains its architectural state and thus appears to the operating system to be a separate physical processor.
Researchers have discovered that when a processor accesses memory, it spends a signiﬁcant amount of time waiting for the data to become available.
This situation, known as a memory stall, may occur for various reasons, such as a cache miss (accessing data that are not in cache memory)
In this scenario, the processor can spend up to 50 percent of its time waiting for data to become available from memory.
To remedy this situation, many recent hardware designs have implemented multithreaded processor cores in which two (or more) hardware threads are assigned to each core.
That way, if one thread stalls while waiting for memory, the core can switch to another thread.
From an operating-system perspective, each hardware thread appears as a logical processor that is available to run a software thread.
Thus, on a dual-threaded, dual-core system, four logical processors are presented to the operating system.
The UltraSPARC T3 CPU has sixteen cores per chip and eight hardware threads per core.
From the perspective of the operating system, there appear to be 128 logical processors.
In general, there are two ways to multithread a processing core: coarsegrained and ﬁne-grained multithreading.
With coarse-grained multithreading, a thread executes on a processor until a long-latency event such as a memory stall occurs.
Because of the delay caused by the long-latency event, the processor must switch to another thread to begin execution.
However, the cost of switching between threads is high, since the instruction pipeline must.
Once this new thread begins execution, it begins ﬁlling the pipeline with its instructions.
However, the architectural design of ﬁne-grained systems includes logic for thread switching.
As a result, the cost of switching between threads is small.
Notice that a multithreaded multicore processor actually requires two different levels of scheduling.
On one level are the scheduling decisions that must be made by the operating system as it chooses which software thread to run on each hardware thread (logical processor)
For this level of scheduling, the operating system may choose any scheduling algorithm, such as those described in Section 6.3
A second level of scheduling speciﬁes how each core decides which hardware thread to run.
The UltraSPARC T3, mentioned earlier, uses a simple roundrobin algorithm to schedule the eight hardware threads to each core.
Another example, the Intel Itanium, is a dual-core processor with two hardwaremanaged threads per core.
The Itanium identiﬁes ﬁve different events that may trigger a thread switch.
When one of these events occurs, the thread-switching logic compares the urgency of the two threads and selects the thread with the highest urgency value to execute on the processor core.
In general, we can distinguish between soft real-time systems and hard real-time systems.
Soft real-time systems provide no guarantee as to when a critical real-time process will be scheduled.
They guarantee only that the process will be given preference over noncritical processes.
A task must be serviced by its deadline; service after the deadline has expired is the same as no service at all.
In this section, we explore several issues related to process scheduling in both soft and hard real-time operating systems.
The system is typically waiting for an event in real time to occur.
Events may arise either in software —as when a timer expires—or in hardware—as when a remote-controlled vehicle detects that it is approaching an obstruction.
When an event occurs, the system must respond to and service it as quickly as possible.
We refer to event latency as the amount of time that elapses from when an event occurs to when it is serviced (Figure 6.12)
Any response that takes longer might result in the automobile’s veering out of control.
In contrast, an embedded system controlling radar in an airliner might tolerate a latency period of several seconds.
Two types of latencies affect the performance of real-time systems:
Interrupt latency refers to the period of time from the arrival of an interrupt at the CPU to the start of the routine that services the interrupt.
When an interrupt occurs, the operating system must ﬁrst complete the instruction it is executing and determine the type of interrupt that occurred.
It must then save the state of the current process before servicing the interrupt using the speciﬁc interrupt service routine (ISR)
The total time required to perform these tasks is the interrupt latency (Figure 6.13)
Indeed, for hard real-time systems, interrupt latency must not simply be minimized, it must be bounded to meet the strict requirements of these systems.
One important factor contributing to interrupt latency is the amount of time interrupts may be disabled while kernel data structures are being updated.
Real-time operating systems require that interrupts be disabled for only very short periods of time.
The amount of time required for the scheduling dispatcher to stop one process and start another is known as dispatch latency.
Providing real-time tasks with immediate access to the CPU mandates that real-time operating systems minimize this latency as well.
The most effective technique for keeping dispatch latency low is to provide preemptive kernels.
In Figure 6.14, we diagram the makeup of dispatch latency.
Release by low-priority processes of resources needed by a high-priority process.
As an example, in Solaris, the dispatch latency with preemption disabled is over a hundred milliseconds.
With preemption enabled, it is reduced to less than a millisecond.
The most important feature of a real-time operating system is to respond immediately to a real-time process as soon as that process requires the CPU.
As a result, the scheduler for a real-time operating system must support a priority-based algorithm with preemption.
Recall that priority-based scheduling algorithms assign each process a priority based on its importance; more important tasks are assigned higher priorities than those deemed less important.
If the scheduler also supports preemption, a process currently running on the CPU will be preempted if a higher-priority process becomes available to run.
Each of these systems assigns real-time processes the highest scheduling priority.
Note that providing a preemptive, priority-based scheduler only guarantees soft real-time functionality.
Hard real-time systems must further guarantee that real-time tasks will be serviced in accord with their deadline requirements, and making such guarantees requires additional scheduling features.
In the remainder of this section, we cover scheduling algorithms appropriate for hard real-time systems.
What is unusual about this form of scheduling is that a process may have to announce its deadline requirements to the scheduler.
Then, using a technique known as an admission-control algorithm, the scheduler does one of two things.
It either admits the process, guaranteeing that the process will complete on time, or rejects the request as impossible if it cannot guarantee that the task will be serviced by its deadline.
The rate-monotonic scheduling algorithm schedules periodic tasks using a static priority policy with preemption.
If a lower-priority process is running and a higher-priority process becomes available to run, it will preempt the lower-priority process.
Upon entering the system, each periodic task is assigned a priority inversely based on its period.
The shorter the period, the higher the priority; the longer the period, the lower the priority.
The rationale behind this policy is to assign a higher priority to tasks that require the CPU more often.
Furthermore, rate-monotonic scheduling assumes that the processing time of a periodic process is the same for each CPU burst.
That is, every time a process acquires the CPU, the duration of its CPU burst is the same.
The deadline for each process requires that it complete its CPU burst by the start of its next period.
We must ﬁrst ask ourselves whether it is possible to schedule these tasks so that each meets its deadlines.
Therefore, it seems we can schedule these tasks in such a way that both meet their deadlines and still leave the CPU with available cycles.
The execution of these processes in this situation is shown in Figure 6.17
Rate-monotonic scheduling is considered optimal in that if a set of processes cannot be scheduled by this algorithm, it cannot be scheduled by any other algorithm that assigns static priorities.
Let’s next examine a set of processes that cannot be scheduled using the rate-monotonic algorithm.
Rate-monotonic scheduling would assign process P1 a higher priority, as it has the shorter period.
Despite being optimal, then, rate-monotonic scheduling has a limitation: CPU utilization is bounded, and it is not always possible fully to maximize CPU resources.
With two processes, CPU utilization is bounded at about 83 percent.
The earlier the deadline, the higher the priority; the later the deadline, the lower the priority.
Under the EDF policy, when a process becomes runnable, it must announce its deadline requirements to the system.
Priorities may have to be adjusted to reﬂect the deadline of the newly runnable process.
Note how this differs from rate-monotonic scheduling, where priorities are ﬁxed.
To illustrate EDF scheduling, we again schedule the processes shown in Figure 6.18, which failed to meet deadline requirements under rate-monotonic scheduling.
The EDF scheduling of these processes is shown in Figure 6.19
Unlike the rate-monotonic algorithm, EDF scheduling does not require that processes be periodic, nor must a process require a constant amount of CPU time per burst.
The only requirement is that a process announce its deadline to the scheduler when it becomes runnable.
In practice, however, it is impossible to achieve this level of CPU utilization due to the cost of context switching between processes and interrupt handling.
Proportional share schedulers operate by allocating T shares among all applications.
An application can receive N shares of time, thus ensuring that the application will have N/T of the total processor time.
As an example, assume that a total of T = 100 shares is to be divided among three processes, A, B, and C.
Proportional share schedulers must work in conjunction with an admission-control policy to guarantee that an application receives its allocated shares of time.
An admission-control policy will admit a client requesting a particular number of shares only if sufﬁcient shares are available.
If a new process D requested 30 shares, the admission controller would deny D entry into the system.
Here, we cover some of the POSIX API related to scheduling real-time threads.
However, there is no time slicing among threads of equal priority.
Therefore, the highest-priority real-time thread at the front of the FIFO queue will be granted the CPU until it terminates or blocks.
It is similar to SCHED FIFO except that it provides time slicing among threads of equal priority.
The POSIX API speciﬁes the following two functions for getting and setting the scheduling policy:
The ﬁrst parameter to both functions is a pointer to the set of attributes for the thread.
This program ﬁrst determines the current scheduling policy and then sets the scheduling algorithm to SCHED FIFO.
We turn next to a description of the scheduling policies of the Linux, Windows, and Solaris operating systems.
It is important to note that we use the term process scheduling in a general sense here.
In fact, we are describing the scheduling of kernel threads with Solaris and Windows systems and of tasks with the Linux scheduler.
Prior to Version 2.5, the Linux kernel ran a variation of the traditional UNIX scheduling algorithm.
However, as this algorithm was not designed with SMP systems in mind, it did not adequately support systems with multiple processors.
In addition, it resulted in poor performance for systems with a large number of runnable processes.
The O(1) scheduler also provided increased support for SMP systems, including processor afﬁnity and load balancing between processors.
However, in practice, although the O(1) scheduler delivered excellent performance on SMP systems, it led to poor response times for the interactive processes that are common on many desktop computer systems.
Scheduling in the Linux system is based on scheduling classes.
By using different scheduling classes, the kernel can accommodate different scheduling algorithms based on the needs of the system and its processes.
The scheduling criteria for a Linux server, for example, may be different from those for a mobile device running Linux.
To decide which task to run next, the scheduler selects the highest-priority task belonging to the highest-priority scheduling class.
Rather, it records how long each task has run by maintaining the virtual run time of each task using the per-task variable vruntime.
The virtual run time is associated with a decay factor based on the priority of a task: lower-priority tasks have higher rates of decay than higher-priority tasks.
For tasks at normal priority (nice values of 0), virtual run time is identical to actual physical run time.
To decide which task to run next, the scheduler simply selects the task that has the smallest vruntime value.
In addition, a higher-priority task that becomes available to run can preempt a lower-priority task.
The Linux CFS scheduler provides an efﬁcient algorithm for selecting which task to run next.
Each runnable task is placed in a red-black tree—a balanced binary search tree whose key is based on the value of vruntime.
When a task becomes runnable, it is added to the tree.
If a task on the tree is not runnable (for example, if it is blocked while waiting for I/O), it is removed.
Generally speaking, tasks that have been given less processing time (smaller values of vruntime) are toward the left side of the tree, and tasks that have been given more processing time are on the right side.
According to the properties of a binary search tree, the leftmost node has the smallest key value, which for the sake of the CFS scheduler means that it is the task with the highest priority.
Because the red-black tree is balanced, navigating it to discover the leftmost node will require O(lgN) operations (where N is the number of nodes in the tree)
However, for efﬁciency reasons, the Linux scheduler caches this value in the variable rb leftmost, and thus determining which task to run next requires only retrieving the cached value.
Let’s examine the CFS scheduler in action: Assume that two tasks have the same nice values.
Typically, the I/O-bound task will run only for short periods before blocking for additional I/O, and the CPU-bound task will exhaust its time period whenever it has an opportunity to run on a processor.
Therefore, the value of vruntime will eventually be lower for the I/O-bound task than for the CPU-bound task, giving the I/O-bound task higher priority than the CPU-bound task.
At that point, if the CPU-bound task is executing when the I/O-bound task becomes eligible to run (for example, when I/O the task is waiting for becomes available), the I/O-bound task will preempt the CPU-bound task.
Linux also implements real-time scheduling using the POSIX standard as described in Section 6.6.6
Linux uses two separate priority ranges, one for real-time tasks and a second for normal tasks.
These two ranges map into a global priority scheme wherein numerically lower values indicate higher relative priorities.
The Windows scheduler ensures that the highest-priority thread will always run.
The portion of the Windows kernel that handles scheduling is called the dispatcher.
A thread selected to run by the dispatcher will run until it is preempted by a higher-priority thread, until it terminates, until its time quantum ends, or until it calls a blocking system call, such as for I/O.
If a higher-priority real-time thread becomes ready while a lower-priority thread is running, the lower-priority thread will be preempted.
This preemption gives a real-time thread preferential access to the CPU when the thread needs such access.
The dispatcher uses a 32-level priority scheme to determine the order of thread execution.
There is also a thread running at priority 0 that is used for memory management.
The dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it ﬁnds a thread that is ready to run.
If no ready thread is found, the dispatcher will execute a special thread called the idle thread.
There is a relationship between the numeric priorities of the Windows kernel and the Windows API.
The Windows API identiﬁes the following six priority classes to which a process can belong:
A process belongs to this class unless the parent of the process was a member of the IDLE PRIORITY CLASS or unless another class was speciﬁed when the process was created.
Additionally, the priority class of a process can be altered with the SetPriorityClass() function in the Windows API.
Priorities in all classes except the REALTIME PRIORITY CLASS are variable, meaning that the priority of a thread belonging to one of these classes can change.
A thread within a given priority classes also has a relative priority.
The priority of each thread is based on both the priority class it belongs to and its relative priority within that class.
The values of the priority classes appear in the top row.
The left column contains the values for the relative priorities.
Furthermore, each thread has a base priority representing a value in the priority range for the class to which the thread belongs.
The base priorities for each priority class are as follows:
When a thread’s time quantum runs out, that thread is interrupted.
If the thread is in the variable-priority class, its priority is lowered.
The priority is never lowered below the base priority, however.
Lowering the priority tends to limit the CPU consumption of compute-bound threads.
When a variablepriority thread is released from a wait operation, the dispatcher boosts the priority.
The amount of the boost depends on what the thread was waiting for.
For example, a thread waiting for keyboard I/O would get a large increase, whereas a thread waiting for a disk operation would get a moderate one.
This strategy tends to give good response times to interactive threads that are using the mouse and windows.
It also enables I/O-bound threads to keep the I/O devices busy while permitting compute-bound threads to use spare CPU cycles in the background.
This strategy is used by several time-sharing operating systems, including UNIX.
In addition, the window with which the user is currently interacting receives a priority boost to enhance its response time.
When a user is running an interactive program, the system needs to provide especially good performance.
For this reason, Windows has a special scheduling rule for processes in the NORMAL PRIORITY CLASS.
Windows distinguishes between the foreground process that is currently selected on the screen and the background processes that are not currently selected.
This increase gives the foreground process three times longer to run before a time-sharing preemption occurs.
Windows 7 introduced user-mode scheduling (UMS), which allows applications to create and manage threads independently of the kernel.
Thus, an application can create and schedule multiple threads without involving the Windows kernel scheduler.
For applications that create a large number of threads, scheduling threads in user mode is much more efﬁcient than kernel-mode thread scheduling, as no kernel intervention is necessary.
Earlier versions of Windows provided a similar feature known as ﬁbers, which allowed several user-mode threads (ﬁbers) to be mapped to a single kernel thread.
A ﬁber was unable to make calls to the Windows API because all ﬁbers had to share the thread environment block (TEB) of the thread on which they were running.
In addition, unlike ﬁbers, UMS is not intended to be used directly by the programmer.
The details of writing user-mode schedulers can be very challenging, and UMS does not include such a scheduler.
Rather, the schedulers come from programming language libraries that build on top of UMS.
For example, Microsoft provides Concurrency Runtime (ConcRT), a concurrent programming framework for C++ that is designed for task-based parallelism (Section 4.2) on multicore processors.
ConcRT provides a user-mode scheduler together with facilities for decomposing programs into tasks, which can then be scheduled on the available processing cores.
Further details on UMS can be found in Section 19.7.3.7
Within each class there are different priorities and different scheduling algorithms.
The default scheduling class for a process is time sharing.
The scheduling policy for the time-sharing class dynamically alters priorities and assigns time slices of different lengths using a multilevel feedback queue.
By default, there is an inverse relationship between priorities and time slices.
The higher the priority, the smaller the time slice; and the lower the priority, the larger the time slice.
Interactive processes typically have a higher priority; CPU-bound processes, a lower priority.
This scheduling policy gives good response time for interactive processes and good throughput for CPU-bound processes.
The interactive class uses the same scheduling policy as the time-sharing class, but it gives windowing applications—such as those created by the KDE or GNOME window managers—a higher priority for better performance.
Figure 6.23 shows the dispatch table for scheduling time-sharing and interactive threads.
These two scheduling classes include 60 priority levels, but for brevity, we display only a handful.
The dispatch table shown in Figure 6.23 contains the following ﬁelds:
Figure 6.23 Solaris dispatch table for time-sharing and interactive threads.
The new priority of a thread that has used its entire time quantum without blocking.
As shown in the table, these threads have their priorities lowered.
The priority of a thread that is returning from sleeping (such as from waiting for I/O)
Threads in the real-time class are given the highest priority.
A real-time process will run before a process in any other class.
This assignment allows a real-time process to have a guaranteed response from the system within a bounded period of time.
In general, however, few processes belong to the real-time class.
Solaris uses the system class to run kernel threads, such as the scheduler and paging daemon.
Once the priority of a system thread is established, it does not change.
The system class is reserved for kernel use (user processes running in kernel mode are not in the system class)
Threads in the ﬁxed-priority class have the same priority range as those in the time-sharing class; however, their priorities are not dynamically adjusted.
The fair-share scheduling class uses CPU shares instead of priorities to make scheduling decisions.
However, the scheduler converts the class-speciﬁc priorities into global priorities and selects the thread with the highest global priority to run.
If there are multiple threads with the same priority, the scheduler uses a round-robin queue.
Figure 6.24 illustrates how the six scheduling classes relate to one another and how they map to global priorities.
Notice that the kernel maintains ten threads for servicing interrupts.
These threads do not belong to any scheduling class and execute at the highest priority (160–169)
How do we select a CPU-scheduling algorithm for a particular system? As we saw in Section 6.3, there are many scheduling algorithms, each with its own parameters.
The ﬁrst problem is deﬁning the criteria to be used in selecting an algorithm.
As we saw in Section 6.2, criteria are often deﬁned in terms of CPU utilization, response time, or throughput.
To select an algorithm, we must ﬁrst deﬁne the relative importance of these elements.
Maximizing CPU utilization under the constraint that the maximum response time is 1 second.
Maximizing throughput such that turnaround time is (on average) linearly proportional to total execution time.
Once the selection criteria have been deﬁned, we want to evaluate the algorithms under consideration.
We next describe the various evaluation methods we can use.
Analytic evaluation uses the given algorithm and the system workload to produce a formula or number to evaluate the performance of the algorithm for that workload.
This method takes a particular predetermined workload and deﬁnes the performance of each algorithm for that workload.
For example, assume that we have the workload shown below.
All ﬁve processes arrive at time 0, in the order given, with the length of the CPU burst given in milliseconds:
Consider the FCFS, SJF, and RR (quantum = 10 milliseconds) scheduling algorithms for this set of processes.
For the FCFS algorithm, we would execute the processes as.
We can see that, in this case, the average waiting time obtained with the SJF policy is less than half that obtained with FCFS scheduling; the RR algorithm gives us an intermediate value.
It gives us exact numbers, allowing us to compare the algorithms.
However, it requires exact numbers for input, and its answers apply only to those cases.
The main uses of deterministic modeling are in describing scheduling algorithms and providing examples.
In cases where we are running the same program over and over again and can measure the program’s processing requirements exactly, we may be able to use deterministic modeling to select a scheduling algorithm.
Furthermore, over a set of examples, deterministic modeling may indicate trends that can then be analyzed and proved separately.
For example, it can be shown that, for the environment described (all processes and their times available at time 0), the SJF policy will always result in the minimum waiting time.
On many systems, the processes that are run vary from day to day, so there is no static set of processes (or times) to use for deterministic modeling.
What can be determined, however, is the distribution of CPU and I/O bursts.
These distributions can be measured and then approximated or simply estimated.
The result is a mathematical formula describing the probability of a particular CPU burst.
Commonly, this distribution is exponential and is described by its mean.
Similarly, we can describe the distribution of times when processes arrive in the system (the arrival-time distribution)
The computer system is described as a network of servers.
The CPU is a server with its ready queue, as is the I/O system with its device queues.
Knowing arrival rates and service rates, we can compute utilization, average queue length, average wait time, and so on.
We can use Little’s formula to compute one of the three variables if we know the other two.
Queueing analysis can be useful in comparing scheduling algorithms, but it also has limitations.
At the moment, the classes of algorithms and distributions that can be handled are fairly limited.
The mathematics of complicated algorithms and distributions can be difﬁcult to work with.
Thus, arrival and service distributions are often deﬁned in mathematically tractable —but unrealistic—ways.
It is also generally necessary to make a number of independent assumptions, which may not be accurate.
As a result of these difﬁculties, queueing models are often only approximations of real systems, and the accuracy of the computed results may be questionable.
To get a more accurate evaluation of scheduling algorithms, we can use simulations.
Running simulations involves programming a model of the computer system.
Software data structures represent the major components of the system.
As this variable’s value is increased, the simulator modiﬁes the system state to reﬂect the activities of the devices, the processes, and the scheduler.
As the simulation executes, statistics that indicate algorithm performance are gathered and printed.
The data to drive the simulation can be generated in several ways.
The most common method uses a random-number generator that is programmed to generate processes, CPU burst times, arrivals, departures, and so on, according to probability distributions.
The distributions can be deﬁned mathematically (uniform, exponential, Poisson) or empirically.
If a distribution is to be deﬁned empirically, measurements of the actual system under study are taken.
The results deﬁne the distribution of events in the real system; this distribution can then be used to drive the simulation.
The frequency distribution indicates only how many instances of each event occur; it does not indicate anything about the order of their occurrence.
We create a trace tape by monitoring the real system and recording the sequence of actual events (Figure 6.25)
Trace tapes provide an excellent way to compare two algorithms on exactly the same set of real inputs.
Simulations can be expensive, often requiring hours of computer time.
A more detailed simulation provides more accurate results, but it also takes more computer time.
In addition, trace tapes can require large amounts of storage space.
Finally, the design, coding, and debugging of the simulator can be a major task.
The only completely accurate way to evaluate a scheduling algorithm is to code it up, put it in the operating system, and see how it works.
This approach puts the actual algorithm in the real system for evaluation under real operating conditions.
The major difﬁculty with this approach is the high cost.
The expense is incurred not only in coding the algorithm and modifying the operating system to support it (along with its required data structures) but also in the reaction of the users to a constantly changing operating system.
Most users are not interested in building a better operating system; they merely want to get their processes executed and use their results.
A constantly changing operating system does not help the users to get their work done.
Another difﬁculty is that the environment in which the algorithm is used will change.
The environment will change not only in the usual way, as new.
If short processes are given priority, then users may break larger processes into sets of smaller processes.
If interactive processes are given priority over noninteractive processes, then users may switch to interactive use.
For example, researchers designed one system that classiﬁed interactive and noninteractive processes automatically by looking at the amount of terminal I/O.
If a process did not input or output to the terminal in a 1-second interval, the process was classiﬁed as noninteractive and was moved to a lower-priority queue.
In response to this policy, one programmer modiﬁed his programs to write an arbitrary character to the terminal at regular intervals of less than 1 second.
The system gave his programs a high priority, even though the terminal output was completely meaningless.
The most ﬂexible scheduling algorithms are those that can be altered by the system managers or by the users so that they can be tuned for a speciﬁc application or set of applications.
A workstation that performs high-end graphical applications, for instance, may have scheduling needs different from those of a Web server or ﬁle server.
For example, Solaris provides the dispadmin command to allow the system administrator to modify the parameters of the scheduling classes described in Section 6.7.3
Another approach is to use APIs that can modify the priority of a process or thread.
The downfall of this approach is that performance-tuning a system or application most often does not result in improved performance in more general situations.
The CPU is allocated to the selected process by the dispatcher.
First-come, ﬁrst-served (FCFS) scheduling is the simplest scheduling algorithm, but it can cause short processes to wait for very long processes.
Shortestjob-ﬁrst (SJF) scheduling is provably optimal, providing the shortest average waiting time.
Implementing SJF scheduling is difﬁcult, however, because predicting the length of the next CPU burst is difﬁcult.
The SJF algorithm is a special case of the general priority scheduling algorithm, which simply allocates the CPU to the highest-priority process.
Round-robin (RR) scheduling is more appropriate for a time-shared (interactive) system.
After q time units, if the process has not relinquished the CPU, it is preempted, and the process is put at the tail of the ready queue.
The major problem is the selection of the time quantum.
If the quantum is too large, RR scheduling degenerates to FCFS scheduling.
If the quantum is too small, scheduling overhead in the form of context-switch time becomes excessive.
The SJF and priority algorithms may be either preemptive or nonpreemptive.
Multilevel queue algorithms allow different algorithms to be used for different classes of processes.
The most common model includes a foreground interactive queue that uses RR scheduling and a background batch queue that uses FCFS scheduling.
Multilevel feedback queues allow processes to move from one queue to another.
Many contemporary computer systems support multiple processors and allow each processor to schedule itself independently.
Typically, each processor maintains its own private queue of processes (or threads), all of which are available to run.
Additional issues related to multiprocessor scheduling include processor afﬁnity, load balancing, and multicore processing.
A real-time computer system requires that results arrive within a deadline period; results arriving after the deadline has passed are useless.
Hard real-time systems must guarantee that real-time tasks are serviced within their deadline periods.
Soft real-time systems are less restrictive, assigning real-time tasks higher scheduling priority than other tasks.
Rate-monotonic scheduling assigns tasks that require the CPU more often a higher priority than tasks that require the CPU less often.
Proportional share scheduling divides up processor time into shares and assigning each process a number of shares, thus guaranteeing each process a proportional share of CPU time.
The POSIX Pthread API provides various features for scheduling real-time threads as well.
Operating systems supporting threads at the kernel level must schedule threads—not processes—for execution.
Both of these systems schedule threads using preemptive, prioritybased scheduling algorithms, including support for real-time threads.
The Linux process scheduler uses a priority-based algorithm with real-time support as well.
The scheduling algorithms for these three operating systems typically favor interactive over CPU-bound processes.
The wide variety of scheduling algorithms demands that we have methods to select among algorithms.
Analytic methods use mathematical analysis to determine the performance of an algorithm.
Simulation methods determine performance by imitating the scheduling algorithm on a “representative” sample of processes and computing the resulting performance.
However, simulation can at best provide an approximation of actual system performance.
The only reliable technique for evaluating a scheduling algorithm is to implement the algorithm on an actual system and monitor its performance in a “real-world” environment.
Given n processes to be scheduled on one processor, how many different schedules are possible? Give a formula in terms of n.
Each process will run for the amount of time listed.
In answering the questions, use nonpreemptive scheduling, and base all decisions on the information you have at the time the decision must be made.
What is the average turnaround time for these processes with the FCFS scheduling algorithm?
What is the average turnaround time for these processes with the SJF scheduling algorithm?
Multilevel feedback queues require parameters to deﬁne the number of queues, the scheduling algorithm for each queue, the criteria used to move processes between queues, and so on.
These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on)
One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an inﬁnite time quantum)
What (if any) relation holds between the following pairs of algorithm sets?
Why will this algorithm favor I/O-bound programs and yet not permanently starve CPU-bound programs?
Furthermore, the system allows program developers to create real-time threads.
Is it necessary to bind a real-time thread to an LWP?
The scheduler recalculates process priorities once per second using the following function:
What will be the new priorities for these three processes when priorities are recalculated? Based on this information, does the traditional UNIX scheduler raise or lower the relative priority of a CPU-bound process?
Most scheduling algorithms maintain a run queue, which lists processes eligible to run on a processor.
On multicore systems, there are two general options: (1) each processing core has its own run.
What are the advantages and disadvantages of each of these approaches?
What are the implications of assigning the following values to the parameters used by the algorithm?
This scheduler assigns each process a time quantum and a priority.
The initial value of a time quantum is 50 milliseconds.
However, every time a process has been allocated the CPU and uses its entire time quantum (does not block for I/O), 10 milliseconds is added to its time quantum, and its priority level is boosted.
The time quantum for a process can be increased to a maximum of 100 milliseconds.
When a process blocks before using its entire time quantum, its time quantum is reduced by 5 milliseconds, but its priority remains the same.
What type of process (CPU-bound or I/O-bound) does the regressive round-robin scheduler favor? Explain.
Draw four Gantt charts that illustrate the execution of these processes using the following scheduling algorithms: FCFS, SJF, nonpreemptive priority (a larger priority number implies a higher priority), and RR (quantum = 2)
What is the turnaround time of each process for each of the scheduling algorithms in part a?
What is the waiting time of each process for each of these scheduling algorithms?
Which of the algorithms results in the minimum average waiting time (over all processes)?
Each process is assigned a numerical priority, with a higher number indicating a higher relative priority.
In addition to the processes listed below, the system also has an idle.
This task has priority 0 and is scheduled whenever the system has no other available processes to run.
If a process is preempted by a higher-priority process, the preempted process is placed at the end of the queue.
Show the scheduling order of the processes using a Gantt chart.
What would be the effect of putting two pointers to the same process in the ready queue?
What would be two major advantages and two disadvantages of this scheme?
How would you modify the basic RR algorithm to achieve the same effect without the duplicate pointers?
Assume that the I/O-bound tasks issue an I/O operation once for every millisecond of CPU computing and that each I/O operation takes 10 milliseconds to complete.
Also assume that the context-switching overhead is 0.1 millisecond and that all processes are long-running tasks.
What strategy can a computer user employ to maximize the amount of CPU time allocated to the user’s process?
A thread in the REALTIME PRIORITY CLASS with a relative priority of NORMAL.
Assume that a thread with priority 50 has used its entire time quantum without blocking.
Assume that a thread with priority 20 blocks for I/O before its time quantum has expired.
Also discuss whether the solutions could be implemented within the context of a proportional share scheduler.
This feedback queue scheduling system was analyzed by [Schrage (1967)]
Details of the ULE scheduler can be found in [Roberson (2003)]
In a multiprogramming environment, several processes may compete for a ﬁnite number of resources.
A process requests resources; if the resources are not available at that time, the process enters a waiting state.
Sometimes, a waiting process is never again able to change state, because the resources it has requested are held by other waiting processes.
We discussed this issue brieﬂy in Chapter 5 in connection with semaphores.
Perhaps the best illustration of a deadlock can be drawn from a law passed by the Kansas legislature early in the 20th century.
It said, in part: “When two trains approach each other at a crossing, both shall come to a full stop and neither shall start up again until the other has gone.”
In this chapter, we describe methods that an operating system can use to prevent or deal with deadlocks.
Deadlock problems can only become more common, given current trends, including larger numbers of processes, multithreaded programs, many more resources within a system, and an emphasis on long-lived ﬁle and database servers rather than batch systems.
To develop a description of deadlocks, which prevent sets of concurrent processes from completing their tasks.
To present a number of different methods for preventing or avoiding deadlocks in a computer system.
A system consists of a ﬁnite number of resources to be distributed among a number of competing processes.
If a system has two CPUs, then the resource type CPU has two instances.
If a process requests an instance of a resource type, the allocation of any instance of the type should satisfy the request.
If it does not, then the instances are not identical, and the resource type classes have not been deﬁned properly.
These two printers may be deﬁned to be in the same resource class if no one cares which printer prints which output.
However, if one printer is on the ninth ﬂoor and the other is in the basement, then people on the ninth ﬂoor may not see both printers as equivalent, and separate resource classes may need to be deﬁned for each printer.
Chapter 5 discussed various synchronization tools, such as mutex locks and semaphores.
These tools are also considered system resources, and they are a common source of deadlock.
However, a lock is typically associated with protecting a speciﬁc data structure—that is, one lock may be used to protect access to a queue, another to protect access to a linked list, and so forth.
For that reason, each lock is typically assigned its own resource class, and deﬁnition is not a problem.
A process must request a resource before using it and must release the resource after using it.
A process may request as many resources as it requires to carry out its designated task.
Obviously, the number of resources requested may not exceed the total number of resources available in the system.
In other words, a process cannot request three printers if the system has only two.
Under the normal mode of operation, a process may utilize a resource in only the following sequence:
If the request cannot be granted immediately (for example, if the resource is being used by another process), then the requesting process must wait until it can acquire the resource.
The process can operate on the resource (for example, if the resource is a printer, the process can print on the printer)
Examples are the request() and release() device, open() and close() ﬁle, and allocate() and free() memory system calls.
Similarly, as we saw in Chapter 5, the request and release of semaphores can be accomplished through the wait() and signal() operations on semaphores or through acquire() and release() of a mutex lock.
For each use of a kernel-managed resource by a process or thread, the operating system checks to make sure that the process has requested and has been allocated the resource.
A system table records whether each resource is free or allocated.
For each resource that is allocated, the table also records the process to which it is allocated.
If a process requests a resource that is currently allocated to another process, it can be added to a queue of processes waiting for this resource.
A set of processes is in a deadlocked state when every process in the set is waiting for an event that can be caused only by another process in the set.
The resources may be either physical resources (for example, printers, tape drives, memory space, and CPU cycles) or logical resources (for example, semaphores, mutex locks, and ﬁles)
However, other types of events may result in deadlocks (for example, the IPC facilities discussed in Chapter 3)
To illustrate a deadlocked state, consider a system with three CD RW drives.
Suppose each of three processes holds one of these CD RW drives.
If each process now requests another drive, the three processes will be in a deadlocked state.
Each is waiting for the event “CD RW is released,” which can be caused only by one of the other waiting processes.
This example illustrates a deadlock involving the same resource type.
For example, consider a system with one printer and one DVD drive.
Suppose that process Pi is holding the DVD and process Pj is holding the printer.
If Pi requests the printer and Pj requests the DVD drive, a deadlock occurs.
Developers of multithreaded applications must remain aware of the possibility of deadlocks.
The locking tools presented in Chapter 5 are designed to avoid race conditions.
However, in using these tools, developers must pay careful attention to how locks are acquired and released.
In a deadlock, processes never ﬁnish executing, and system resources are tied up, preventing other jobs from starting.
Before we discuss the various methods for dealing with the deadlock problem, we look more closely at features that characterize deadlocks.
Let’s see how deadlock can occur in a multithreaded Pthread program using mutex locks.
Mutex locks are acquired and released using pthread mutex lock() and pthread mutex unlock(), respectively.
If a thread attempts to acquire a locked mutex, the call to pthread mutex lock() blocks the thread until the owner of the mutex lock invokes pthread mutex unlock()
Two mutex locks are created in the following code example:
Create and initialize the mutex locks */ pthread mutex t first mutex; pthread mutex t second mutex;
Next, two threads—thread one and thread two—are created, and both these threads have access to both mutex locks.
Deadlock is possible if thread one acquires first mutex while thread two acquires second mutex.
Note that, even though deadlock is possible, it will not occur if thread one can acquire and release the mutex locks for first mutex and second mutex before thread two attempts to acquire the locks.
And, of course, the order in which the threads run depends on how they are scheduled by the CPU scheduler.
This example illustrates a problem with handling deadlocks: it is difﬁcult to identify and test for deadlocks that may occur only under certain scheduling circumstances.
A deadlock situation can arise if the following four conditions hold simultaneously in a system:
At least one resource must be held in a nonsharable mode; that is, only one process at a time can use the resource.
If another process requests that resource, the requesting process must be delayed until the resource has been released.
A process must be holding at least one resource and waiting to acquire additional resources that are currently being held by other processes.
Resources cannot be preempted; that is, a resource can be released only voluntarily by the process holding it, after that process has completed its task.
We emphasize that all four conditions must hold for a deadlock to occur.
The circular-wait condition implies the hold-and-wait condition, so the four conditions are not completely independent.
We shall see in Section 7.4, however, that it is useful to consider each condition separately.
Pictorially, we represent each process Pi as a circle and each resource type Rj as a rectangle.
Since resource type Rj may have more than one instance, we represent each such instance as a dot within the rectangle.
Note that a request edge points to only the rectangle Rj , whereas an assignment edge must also designate one of the dots in the rectangle.
When this request can be fulﬁlled, the request edge is instantaneously transformed to an assignment edge.
When the process no longer needs access to the resource, it releases the resource.
If the graph does contain a cycle, then a deadlock may exist.
If each resource type has exactly one instance, then a cycle implies that a deadlock has occurred.
If the cycle involves only a set of resource types, each of which has only a single instance, then a deadlock has occurred.
In this case, a cycle in the graph is both a necessary and a sufﬁcient condition for the existence of deadlock.
If each resource type has several instances, then a cycle does not necessarily imply that a deadlock has occurred.
In this case, a cycle in the graph is a necessary but not a sufﬁcient condition for the existence of deadlock.
That resource can then be allocated to P3, breaking the cycle.
If there is a cycle, then the system may or may not be in a deadlocked state.
This observation is important when we deal with the deadlock problem.
Generally speaking, we can deal with the deadlock problem in one of three ways:
We can use a protocol to prevent or avoid deadlocks, ensuring that the system will never enter a deadlocked state.
We can allow the system to enter a deadlocked state, detect it, and recover.
We can ignore the problem altogether and pretend that deadlocks never.
The third solution is the one used by most operating systems, including Linux and Windows.
It is then up to the application developer to write programs that handle deadlocks.
Next, we elaborate brieﬂy on each of the three methods for handling deadlocks.
The basic approaches can be combined, however, allowing us to select an optimal approach for each class of resources in a system.
To ensure that deadlocks never occur, the system can use either a deadlockprevention or a deadlock-avoidance scheme.
Deadlock prevention provides a set of methods to ensure that at least one of the necessary conditions (Section 7.2.1) cannot hold.
These methods prevent deadlocks by constraining how requests for resources can be made.
Deadlock avoidance requires that the operating system be given additional information in advance concerning which resources a process will request and use during its lifetime.
With this additional knowledge, the operating system can decide for each request whether or not the process should wait.
To decide whether the current request can be satisﬁed or must be delayed, the system must consider the resources currently available, the resources currently allocated to each process, and the future requests and releases of each process.
In this environment, the system can provide an algorithm that examines the state of the system to determine whether a deadlock has occurred and an algorithm to recover from the deadlock (if a deadlock has indeed occurred)
In the absence of algorithms to detect and recover from deadlocks, we may arrive at a situation in which the system is in a deadlocked state yet has no way of recognizing what has happened.
In this case, the undetected deadlock will cause the system’s performance to deteriorate, because resources are being held by processes that cannot run and because more and more processes, as they make requests for resources, will enter a deadlocked state.
Eventually, the system will stop functioning and will need to be restarted manually.
Although this method may not seem to be a viable approach to the deadlock problem, it is nevertheless used in most operating systems, as mentioned earlier.
Ignoring the possibility of deadlocks is cheaper than the other approaches.
Since in many systems, deadlocks occur infrequently (say, once per year), the extra expense of the other methods may not seem worthwhile.
In addition, methods used to recover from other conditions may be put to use to recover from deadlock.
In some circumstances, a system is in a frozen state but not in a deadlocked state.
We see this situation, for example, with a real-time process running at the highest priority (or any process running on a nonpreemptive scheduler) and never returning control to the operating system.
The system must have manual recovery methods for such conditions and may simply use those techniques for deadlock recovery.
As we noted in Section 7.2.1, for a deadlock to occur, each of the four necessary conditions must hold.
By ensuring that at least one of these conditions cannot hold, we can prevent the occurrence of a deadlock.
We elaborate on this approach by examining each of the four necessary conditions separately.
Sharable resources, in contrast, do not require mutually exclusive access and thus cannot be involved in a deadlock.
Read-only ﬁles are a good example of a sharable resource.
If several processes attempt to open a read-only ﬁle at the same time, they can be granted simultaneous access to the ﬁle.
A process never needs to wait for a sharable resource.
In general, however, we cannot prevent deadlocks by denying the mutual-exclusion condition, because some resources are intrinsically nonsharable.
For example, a mutex lock cannot be simultaneously shared by several processes.
To ensure that the hold-and-wait condition never occurs in the system, we must guarantee that, whenever a process requests a resource, it does not hold any other resources.
One protocol that we can use requires each process to request and be allocated all its resources before it begins execution.
We can implement this provision by requiring that system calls requesting resources for a process precede all other system calls.
An alternative protocol allows a process to request resources only when it has none.
Before it can request any additional resources, it must release all the resources that it is currently allocated.
To illustrate the difference between these two protocols, we consider a process that copies data from a DVD drive to a ﬁle on disk, sorts the ﬁle, and then prints the results to a printer.
If all resources must be requested at the beginning of the process, then the process must initially request the DVD drive, disk ﬁle, and printer.
It will hold the printer for its entire execution, even though it needs the printer only at the end.
The second method allows the process to request initially only the DVD drive and disk ﬁle.
It copies from the DVD drive to the disk and then releases both the DVD drive and the disk ﬁle.
The process must then request the disk ﬁle and the printer.
After copying the disk ﬁle to the printer, it releases these two resources and terminates.
First, resource utilization may be low, since resources may be allocated but unused for a long period.
In the example given, for instance, we can release the DVD drive and disk ﬁle, and then request the disk ﬁle and printer, only if we can be sure that our data will remain on the disk ﬁle.
Otherwise, we must request all resources at the beginning for both protocols.
A process that needs several popular resources may have to wait indeﬁnitely, because at least one of the resources that it needs is always allocated to some other process.
The third necessary condition for deadlocks is that there be no preemption of resources that have already been allocated.
To ensure that this condition does not hold, we can use the following protocol.
If a process is holding some resources and requests another resource that cannot be immediately allocated to it (that is, the process must wait), then all resources the process is currently holding are preempted.
The preempted resources are added to the list of resources for which the process is waiting.
The process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
Alternatively, if a process requests some resources, we ﬁrst check whether they are available.
If they are not, we check whether they are allocated to some other process that is waiting for additional resources.
If so, we preempt the desired resources from the waiting process and allocate them to the requesting process.
If the resources are neither available nor held by a waiting process, the requesting process must wait.
While it is waiting, some of its resources may be preempted, but only if another process requests them.
A process can be restarted only when it is allocated the new resources it is requesting and recovers any resources that were preempted while it was waiting.
This protocol is often applied to resources whose state can be easily saved and restored later, such as CPU registers and memory space.
It cannot generally be applied to such resources as mutex locks and semaphores.
The fourth and ﬁnal condition for deadlocks is the circular-wait condition.
One way to ensure that this condition never holds is to impose a total ordering of all resource types and to require that each process requests resources in an increasing order of enumeration.
We can accomplish this scheme in an application program by developing an ordering among all synchronization objects in the system.
All requests for synchronization objects must be made in increasing order.
For example, if the lock ordering in the Pthread program shown in Figure 7.4 was.
Keep in mind that developing an ordering, or hierarchy, does not in itself.
It is up to application developers to write programs that follow the ordering.
Also note that the function F should be deﬁned according to the normal order of usage of the resources in a system.
Although ensuring that resources are acquired in the proper order is the responsibility of application developers, certain software can be used to verify that locks are acquired in the proper order and to give appropriate warnings when locks are acquired out of order and deadlock is possible.
One lock-order veriﬁer, which works on BSD versions of UNIX such as FreeBSD, is known as witness.
It works by dynamically maintaining the relationship of lock orders in a system.
Let’s use the program shown in Figure 7.4 as an example.
Witness records the relationship that first mutex must be acquired before second mutex.
If thread two later acquires the locks out of order, witness generates a warning message on the system console.
It is also important to note that imposing a lock ordering does not guarantee deadlock prevention if locks can be acquired dynamically.
For example, assume we have a function that transfers funds between two accounts.
To prevent a race condition, each account has an associated mutex lock that is obtained from a get lock() function such as shown in Figure 7.5:
Deadlock is possible if two threads simultaneously invoke the transaction() function, transposing different accounts.
We leave it as an exercise for students to ﬁx this situation.
The limits ensure that at least one of the necessary conditions for deadlock cannot occur.
Possible side effects of preventing deadlocks by this method, however, are low device utilization and reduced system throughput.
An alternative method for avoiding deadlocks is to require additional information about how resources are to be requested.
For example, in a system with one tape drive and one printer, the system might need to know that process P will request ﬁrst the tape drive and then the printer before releasing both resources, whereas process Q will request ﬁrst the printer and then the tape drive.
With this knowledge of the complete sequence of requests and releases for each process, the system can decide for each request whether or not the process should wait in order to avoid a possible future deadlock.
Each request requires that in making this decision the system consider the resources currently available, the resources currently allocated to each process, and the future requests and releases of each process.
The various algorithms that use this approach differ in the amount and type of information required.
The simplest and most useful model requires that each process declare the maximum number of resources of each type that it may need.
Given this a priori information, it is possible to construct an.
The resourceallocation state is deﬁned by the number of available and allocated resources and the maximum demands of the processes.
A state is safe if the system can allocate resources to each process (up to its maximum) in some order and still avoid a deadlock.
More formally, a system is in a safe state only if there exists a safe sequence.
In this situation, if the resources that Pi needs are not immediately available, then Pi can wait until all Pj have ﬁnished.
When they have ﬁnished, Pi can obtain all of its needed resources, complete its designated task, return its allocated resources, and terminate.
When Pi terminates, Pi+1 can obtain its needed resources, and so on.
If no such sequence exists, then the system state is said to be unsafe.
As long as the state is safe, the operating system can avoid unsafe (and deadlocked) states.
In an unsafe state, the operating system cannot prevent processes from requesting resources in such a way that a deadlock occurs.
A system can go from a safe state to an unsafe state.
At this point, only process P1 can be allocated all its tape drives.
When it returns them, the system will have only four available tape drives.
Since process P0 is allocated ﬁve tape drives but has a maximum of ten, it may request ﬁve more tape drives.
If it does so, it will have to wait, because they are unavailable.
Similarly, process P2 may request six additional tape drives and have to wait, resulting in a deadlock.
Our mistake was in granting the request from process P2 for one more tape drive.
If we had made P2 wait until either of the other processes had ﬁnished and released its resources, then we could have avoided the deadlock.
Given the concept of a safe state, we can deﬁne avoidance algorithms that ensure that the system will never deadlock.
The idea is simply to ensure that the system will always remain in a safe state.
Whenever a process requests a resource that is currently available, the system must decide whether the resource can be allocated immediately or whether the process must wait.
The request is granted only if the allocation leaves the system in a safe state.
In this scheme, if a process requests a resource that is currently available, it may still have to wait.
Thus, resource utilization may be lower than it would otherwise be.
If no cycle exists, then the allocation of the resource will leave the system in a safe state.
If a cycle is found, then the allocation will put the system in an unsafe state.
In that case, process Pi will have to wait for its requests to be satisﬁed.
A cycle, as mentioned, indicates that the system is in an unsafe state.
The name was chosen because the algorithm could be used in a banking system to ensure that the bank never.
When a new process enters the system, it must declare the maximum number of instances of each resource type that it may need.
This number may not exceed the total number of resources in the system.
When a user requests a set of resources, the system must determine whether the allocation of these resources will leave the system in a safe state.
If it will, the resources are allocated; otherwise, the process must wait until some other process releases enough resources.
Several data structures must be maintained to implement the banker’s algorithm.
We need the following data structures, where n is the number of processes in the system and m is the number of resource types:
A vector of length m indicates the number of available resources of each type.
If Available[j] equals k, then k instances of resource type Rj are available.
These data structures vary over time in both size and value.
To simplify the presentation of the banker’s algorithm, we next establish.
We can treat each row in the matrices Allocation and Need as vectors and refer to them as Allocationi and Needi.
The vector Allocationi speciﬁes the resources currently allocated to process Pi ; the vector Needi speciﬁes the additional resources that process Pi may still request to complete its task.
We can now present the algorithm for ﬁnding out whether or not a system is in a safe state.
If Finish[i] == true for all i, then the system is in a safe state.
Next, we describe the algorithm for determining whether requests can be safely granted.
If Requesti [ j] == k, then process Pi wants k instances of resource type Rj.
When a request for resources is made by process Pi , the following actions are taken:
Have the system pretend to have allocated the requested resources to process Pi by modifying the state as follows:
Resource type A has ten instances, resource type B has ﬁve instances, and resource type C has seven instances.
Suppose that, at time T0, the following snapshot of the system has been taken:
We must determine whether this new system state is safe.
Hence, we can immediately grant the request of process P1
We leave it as a programming exercise for students to implement the banker’s algorithm.
An algorithm that examines the state of the system to determine whether a deadlock has occurred.
In the following discussion, we elaborate on these two requirements as they pertain to systems with only a single instance of each resource type, as well as to systems with several instances of each resource type.
As before, a deadlock exists in the system if and only if the wait-for graph contains a cycle.
To detect deadlocks, the system needs to maintain the waitfor graph and periodically invoke an algorithm that searches for a cycle in the graph.
An algorithm to detect a cycle in a graph requires an order of n2 operations, where n is the number of vertices in the graph.
The algorithm employs several time-varying data structures that are similar to those used in the banker’s algorithm (Section 7.5.3):
A vector of length m indicates the number of available resources of each type.
Resource type A has seven instances, resource type B has two instances, and resource type C has six.
We claim that the system is not in a deadlocked state.
Suppose now that process P2 makes one additional request for an instance of type C.
Although we can reclaim the resources held by process P0, the number of available resources is not sufﬁcient to fulﬁll the requests of the other processes.
When should we invoke the detection algorithm? The answer depends on two factors:
How many processes will be affected by deadlock when it happens?
If deadlocks occur frequently, then the detection algorithm should be invoked frequently.
Resources allocated to deadlocked processes will be idle until the deadlock can be broken.
In addition, the number of processes involved in the deadlock cycle may grow.
Deadlocks occur only when some process makes a request that cannot be granted immediately.
This request may be the ﬁnal request that completes a chain of waiting processes.
In the extreme, then, we can invoke the deadlockdetection algorithm every time a request for allocation cannot be granted immediately.
In this case, we can identify not only the deadlocked set of.
In reality, each of the deadlocked processes is a link in the cycle in the resource graph, so all of them, jointly, caused the deadlock.
If there are many different resource types, one request may create many cycles in the resource graph, each cycle completed by the most recent request and “caused” by the one identiﬁable process.
Of course, invoking the deadlock-detection algorithm for every resource request will incur considerable overhead in computation time.
A less expensive alternative is simply to invoke the algorithm at deﬁned intervals—for example, once per hour or whenever CPU utilization drops below 40 percent.
A deadlock eventually cripples system throughput and causes CPU utilization to drop.
If the detection algorithm is invoked at arbitrary points in time, the resource graph may contain many cycles.
In this case, we generally cannot tell which of the many deadlocked processes “caused” the deadlock.
When a detection algorithm determines that a deadlock exists, several alternatives are available.
One possibility is to inform the operator that a deadlock has occurred and to let the operator deal with the deadlock manually.
Another possibility is to let the system recover from the deadlock automatically.
One is simply to abort one or more processes to break the circular wait.
The other is to preempt some resources from one or more of the deadlocked processes.
To eliminate deadlocks by aborting a process, we use one of two methods.
In both methods, the system reclaims all resources allocated to the terminated processes.
This method clearly will break the deadlock cycle, but at great expense.
The deadlocked processes may have computed for a long time, and the results of these partial computations must be discarded and probably will have to be recomputed later.
Abort one process at a time until the deadlock cycle is eliminated.
This method incurs considerable overhead, since after each process is aborted, a deadlock-detection algorithm must be invoked to determine whether any processes are still deadlocked.
If the process was in the midst of updating a ﬁle, terminating it will leave that ﬁle in an incorrect state.
Similarly, if the process was in the midst of printing data on a printer, the system must reset the printer to a correct state before printing the next job.
If the partial termination method is used, then we must determine which deadlocked process (or processes) should be terminated.
This determination is a policy decision, similar to CPU-scheduling decisions.
The question is basically an economic one; we should abort those processes whose termination will incur.
Unfortunately, the term minimum cost is not a precise one.
How long the process has computed and how much longer the process will compute before completing its designated task.
How many and what types of resources the process has used (for example, whether the resources are simple to preempt)
To eliminate deadlocks using resource preemption, we successively preempt some resources from processes and give these resources to other processes until the deadlock cycle is broken.
If preemption is required to deal with deadlocks, then three issues need to be addressed:
Which resources and which processes are to be preempted? As in process termination, we must determine the order of preemption to minimize cost.
Cost factors may include such parameters as the number of resources a deadlocked process is holding and the amount of time the process has thus far consumed.
If we preempt a resource from a process, what should be done with that process? Clearly, it cannot continue with its normal execution; it is missing some needed resource.
We must roll back the process to some safe state and restart it from that state.
Since, in general, it is difﬁcult to determine what a safe state is, the simplest solution is a total rollback: abort the process and then restart it.
Although it is more effective to roll back the process only as far as necessary to break the deadlock, this method requires the system to keep more information about the state of all running processes.
How do we ensure that starvation will not occur? That is, how can we guarantee that resources will not always be preempted from the same process?
In a system where victim selection is based primarily on cost factors, it may happen that the same process is always picked as a victim.
As a result, this process never completes its designated task, a starvation situation any practical system must address.
Clearly, we must ensure that a process can be picked as a victim only a (small) ﬁnite number of times.
The most common solution is to include the number of rollbacks in the cost factor.
A deadlocked state occurs when two or more processes are waiting indeﬁnitely for an event that can be caused only by one of the waiting processes.
Use some protocol to prevent or avoid deadlocks, ensuring that the system will never enter a deadlocked state.
Allow the system to enter a deadlocked state, detect it, and then recover.
Ignore the problem altogether and pretend that deadlocks never occur in.
The third solution is the one used by most operating systems, including Linux and Windows.
A deadlock can occur only if four necessary conditions hold simultaneously in the system: mutual exclusion, hold and wait, no preemption, and circular wait.
To prevent deadlocks, we can ensure that at least one of the necessary conditions never holds.
A method for avoiding deadlocks, rather than preventing them, requires that the operating system have a priori information about how each process will utilize system resources.
The banker’s algorithm, for example, requires a priori information about the maximum number of each resource class that each process may request.
A deadlockdetection algorithm must be invoked to determine whether a deadlock has occurred.
If a deadlock is detected, the system must recover either by terminating some of the deadlocked processes or by preempting resources from some of the deadlocked processes.
Where preemption is used to deal with deadlocks, three issues must be addressed: selecting a victim, rollback, and starvation.
In a system that selects victims for rollback primarily on the basis of cost factors, starvation may occur, and the selected process can never complete its designated task.
The basic approaches can be combined, however, allowing us to select an optimal approach for each class of resources in a system.
Show that it is possible for the processes to complete their execution without entering a deadlocked state.
Deadlocks occur about twice per month, and the operator must terminate and rerun about ten jobs per deadlock.
Each job is worth about two dollars (in CPU time), and the jobs terminated tend to be about half done when they are aborted.
A systems programmer has estimated that a deadlock-avoidance algorithm (like the banker’s algorithm) could be installed in the system with an increase of about 10 percent in the average execution time per job.
If you answer “no,” explain how the system can deal with the starvation problem.
Requests for and releases of resources are allowed at any time.
If a request for resources cannot be satisﬁed because the resources are not available, then we check any processes that are blocked waiting for resources.
If a blocked process has the desired resources, then these resources are taken away from it and are given to the requesting process.
The vector of resources for which the blocked process is waiting is increased to include the resources that were taken away.
For example, a system has three resource types, and the vector Available is initialized to (4,2,2)
Can deadlock occur? If you answer “yes,” give an example.
If you answer “no,” specify which necessary condition cannot occur.
Can you do so by simply using the safety algorithm code and redeﬁning Maxi = Waitingi + Allocationi , where Waitingi is a vector specifying the resources for which process i is waiting and Allocationi is as deﬁned in Section 7.5? Explain your answer.
Show that the four necessary conditions for deadlock hold in this example.
State a simple rule for avoiding deadlocks in this system.
Applying the four necessary conditions for deadlock, is deadlock still possible if multiple reader–writer locks are used?
Describe what role the CPU scheduler plays and how it can contribute to deadlock in this program.
However, we also point out that deadlock is possible in this situation if two threads simultaneously invoke the transaction() function.
Resources break or are replaced, new processes come and go, and new resources are bought and added to the system.
If deadlock is controlled by the banker’s algorithm, which of the following changes can be made safely (without introducing the possibility of deadlock), and under what circumstances?
Increase Max for one process (the process needs or wants more resources than allowed)
Decrease Max for one process (the process decides it does not need that many resources)
A process can request or release only one resource at a time.
Show that the system is deadlock free if the following two conditions hold:
The maximum need of each process is between one resource and m resources.
The sum of all maximum needs is less than m + n.
Assume that requests for chopsticks are made one at a time.
Describe a simple rule for determining whether a particular request can be satisﬁed without causing deadlock given the current allocation of chopsticks to philosophers.
Assume now that each philosopher requires three chopsticks to eat.
Describe some simple rules for determining whether a particular request can be satisﬁed without causing deadlock given the current allocation of chopsticks to philosophers.
Using the banker’s algorithm, determine whether or not each of the following states is unsafe.
If the state is safe, illustrate the order in which the processes may complete.
Illustrate that the system is in a safe state by demonstrating an order in which the processes may complete.
Farmers in the two villages use this bridge to deliver their produce to the neighboring town.
The bridge can become deadlocked if a northbound and a southbound farmer get on the bridge at the same time.
Vermont farmers are stubborn and are unable to back up.
Using semaphores and/or mutex locks, design an algorithm in pseudocode that prevents deadlock.
Initially, do not be concerned about starvation (the situation in which northbound farmers prevent southbound farmers from using the bridge, or vice versa)
In particular, represent northbound and southbound farmers as separate threads.
Once a farmer is on the bridge, the associated thread will sleep for a random period of time, representing traveling across the bridge.
Design your program so that you can create several threads representing the northbound and southbound farmers.
For this project, you will write a multithreaded program that implements the banker’s algorithm discussed in Section 7.5.3
The banker will grant a request only if it leaves the system in a safe state.
A request that leaves the system in an unsafe state will be denied.
The banker will consider requests from n customers for m resources types.
The banker will keep track of the resources using the following data structures:
Create n customer threads that request and release resources from the bank.
The customers will continually loop, requesting and then releasing random numbers of resources.
The customers’ requests for resources will be bounded by their respective values in the need array.
The banker will grant a request if it satisﬁes the safety algorithm outlined in Section 7.5.3.1
If a request does not leave the system in a safe state, the banker will deny it.
Function prototypes for requesting and releasing resources are as follows:
Therefore, access must be controlled through mutex locks to prevent race conditions.
You should invoke your program by passing the number of resources of each type on the command line.
For example, if there were three resource types, with ten instances of the ﬁrst type, ﬁve of the second type, and seven of the third type, you would invoke your program follows:
You may initialize themaximum array (which holds the maximum demand of each customer) using any method you ﬁnd convenient.
A study of deadlock handling is provided in [Levine (2003)]
The main purpose of a computer system is to execute programs.
These programs, together with the data they access, must be at least partially in main memory during execution.
To improve both the utilization of the CPU and the speed of its response to users, a general-purpose computer must keep several processes in memory.
Many memory-management schemes exist, reﬂecting various approaches, and the effectiveness of each algorithmdepends on the situation.
Selection of a memory-management scheme for a system depends onmany factors, especially on the hardware design of the system.
In Chapter 6, we showed how the CPU can be shared by a set of processes.
As a result of CPU scheduling, we can improve both the utilization of the CPU and the speed of the computer’s response to its users.
To realize this increase in performance, however, we must keep several processes in memory—that is, we must share memory.
In this chapter, we discuss various ways to manage memory.
The memorymanagement algorithms vary from a primitive bare-machine approach to paging and segmentation strategies.
Selection of a memory-management method for a speciﬁc system depends on many factors, especially on the hardware design of the system.
As we shall see, many algorithms require hardware support, leading many systems to have closely integrated hardware and operating-system memory management.
To provide a detailed description of various ways of organizing memory hardware.
To discuss in detail how paging works in contemporary computer systems.
As we saw in Chapter 1, memory is central to the operation of a modern computer system.
Memory consists of a large array of bytes, each with its own address.
The CPU fetches instructions from memory according to the value of the program counter.
These instructions may cause additional loading from and storing to speciﬁc memory addresses.
The instruction is then decoded and may cause operands to be fetched from memory.
After the instruction has been executed on the operands, results may be stored back in memory.
Accordingly, we can ignore how a program generates a memory address.
We are interested only in the sequence of memory addresses generated by the running program.
We begin our discussion by covering several issues that are pertinent to managing memory: basic hardware, the binding of symbolic memory addresses to actual physical addresses, and the distinction between logical and physical addresses.
We conclude the section with a discussion of dynamic linking and shared libraries.
Main memory and the registers built into the processor itself are the only general-purpose storage that the CPU can access directly.
There are machine instructions that take memory addresses as arguments, but none that take disk addresses.
Therefore, any instructions in execution, and any data being used by the instructions, must be in one of these direct-access storage devices.
If the data are not in memory, they must be moved there before the CPU can operate on them.
Registers that are built into the CPU are generally accessible within one cycle of the CPU clock.
Most CPUs can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick.
The same cannot be said of main memory, which is accessed via a transaction on the memory bus.
Completing a memory access may take many cycles of the CPU clock.
In such cases, the processor normally needs to stall, since it does not have the data required to complete the instruction that it is executing.
This situation is intolerable because of the frequency of memory accesses.
The remedy is to add fast memory between the CPUand main memory, typically on the CPU chip for fast access.
To manage a cache built into the CPU, the hardware automatically speeds up memory access without any operating-system control.
Not only are we concerned with the relative speed of accessing physical memory, but we also must ensure correct operation.
For proper system operation we must protect the operating system from access by user processes.
On multiuser systems, we must additionally protect user processes from one another.
This protection must be provided by the hardware because the operating system doesn’t usually intervene between the CPU and its memory accesses (because of the resulting performance penalty)
Hardware implements this production in several different ways, as we show throughout the chapter.
We ﬁrst need to make sure that each process has a separate memory space.
Separate per-process memory space protects the processes from each other and is fundamental to having multiple processes loaded in memory for concurrent execution.
To separate memory spaces, we need the ability to determine the range of legal addresses that the process may access and to ensure that the process can access only these legal addresses.
We can provide this protection by using two registers, usually a base and a limit, as illustrated in Figure 8.1
The base register holds the smallest legal physical memory address; the limit register speciﬁes the size of the range.
Figure 8.1 A base and a limit register deﬁne a logical address space.
Protection of memory space is accomplished by having the CPU hardware compare every address generated in user mode with the registers.
Any attempt by a program executing in user mode to access operating-system memory or other users’ memory results in a trap to the operating system, which treats the attempt as a fatal error (Figure 8.2)
This scheme prevents a user program from (accidentally or deliberately) modifying the code or data structures of either the operating system or other users.
The base and limit registers can be loaded only by the operating system, which uses a special privileged instruction.
Since privileged instructions can be executed only in kernel mode, and since only the operating system executes in kernel mode, only the operating system can load the base and limit registers.
Figure 8.2 Hardware address protection with base and limit registers.
This scheme allows the operating system to change the value of the registers but prevents user programs from changing the registers’ contents.
The operating system, executing in kernel mode, is given unrestricted access to both operating-system memory and users’ memory.
This provision allows the operating system to load users’ programs into users’ memory, to dump out those programs in case of errors, to access and modify parameters of system calls, to perform I/O to and from user memory, and to provide many other services.
Consider, for example, that an operating system for a multiprocessing system must execute context switches, storing the state of one process from the registers into main memory before loading the next process’s context from main memory into the registers.
Usually, a program resides on a disk as a binary executable ﬁle.
To be executed, the program must be brought into memory and placed within a process.
Depending on the memory management in use, the process may be moved between disk and memory during its execution.
The processes on the disk that are waiting to be brought into memory for execution form the input queue.
The normal single-tasking procedure is to select one of the processes in the input queue and to load that process into memory.
As the process is executed, it accesses instructions and data from memory.
Eventually, the process terminates, and its memory space is declared available.
Most systems allow a user process to reside in any part of the physical memory.
You will see later how a user program actually places a process in physical memory.
In most cases, a user program goes through several steps—some of which may be optional—before being executed (Figure 8.3)
Addresses may be represented in different ways during these steps.
Addresses in the source program are generally symbolic (such as the variable count)
A compiler typically binds these symbolic addresses to relocatable addresses (such as “14 bytes from the beginning of this module”)
The linkage editor or loader in turn binds the relocatable addresses to absolute addresses (such as 74014)
Each binding is a mapping from one address space to another.
Classically, the binding of instructions and data to memory addresses can be done at any step along the way:
If you know at compile time where the process will reside in memory, then absolute code can be generated.
For example, if you know that a user process will reside starting at location R, then the generated compiler code will start at that location and extend up from there.
If, at some later time, the starting location changes, then it will be necessary to recompile this code.
If it is not known at compile time where the process will reside in memory, then the compiler must generate relocatable code.
In this case, ﬁnal binding is delayed until load time.
If the starting address changes, we need only reload the user code to incorporate this changed value.
If the process can be moved during its execution from one memory segment to another, then binding must be delayed until run time.
Special hardware must be available for this scheme to work, as will be discussed in Section 8.1.3
A major portion of this chapter is devoted to showing how these various bindings can be implemented effectively in a computer system and to discussing appropriate hardware support.
An address generated by the CPU is commonly referred to as a logical address, whereas an address seen by the memory unit—that is, the one loaded into the memory-address register of the memory—is commonly referred to as a physical address.
The compile-time and load-time address-binding methods generate identical logical and physical addresses.
In this case, we usually refer to the logical address as a virtual address.
We use logical address and virtual address interchangeably in this text.
The set of all logical addresses generated by a program is a logical address space.
The set of all physical addresses corresponding to these logical addresses is a physical address space.
Thus, in the execution-time address-binding scheme, the logical and physical address spaces differ.
The run-time mapping from virtual to physical addresses is done by a hardware device called the memory-management unit (MMU)
For the time being, we illustrate this mapping with a simple MMU scheme that is a generalization of the base-register scheme described in Section 8.1.1
The value in the relocation register is added to every address generated by a user process at the time the address is sent to memory (see Figure 8.4)
Only when it is used as a memory address (in an indirect load or store, perhaps) is it relocated relative to the base register.
This form of execution-time binding was discussed in Section 8.1.2
The ﬁnal location of a referenced memory address is not determined until the reference is made.
The user program generates only logical addresses and thinks that the process runs in locations 0 to max.
However, these logical addresses must be mapped to physical addresses before they are used.
In our discussion so far, it has been necessary for the entire program and all data of a process to be in physical memory for the process to execute.
The size of a process has thus been limited to the size of physical memory.
To obtain better memory-space utilization, we can use dynamic loading.
With dynamic loading, a routine is not loaded until it is called.
All routines are kept on disk in a relocatable load format.
The main program is loaded into memory and is executed.
When a routine needs to call another routine, the calling routine ﬁrst checks to see whether the other routine has been loaded.
If it has not, the relocatable linking loader is called to load the desired routine into memory and to update the program’s address tables to reﬂect this change.
The advantage of dynamic loading is that a routine is loaded only when it is needed.
This method is particularly useful when large amounts of code are needed to handle infrequently occurring cases, such as error routines.
In this case, although the total program size may be large, the portion that is used (and hence loaded) may be much smaller.
Dynamic loading does not require special support from the operating system.
It is the responsibility of the users to design their programs to take advantage of such a method.
Operating systems may help the programmer, however, by providing library routines to implement dynamic loading.
Dynamically linked libraries are system libraries that are linked to user programs when the programs are run (refer back to Figure 8.3)
Some operating systems support only static linking, in which system libraries are treated like any other object module and are combined by the loader into the binary program image.
Here, though, linking, rather than loading, is postponed until execution time.
This feature is usually used with system libraries, such as language subroutine libraries.
Without this facility, each program on a system must include a copy of its language library (or at least the routines referenced by the program) in the executable image.
With dynamic linking, a stub is included in the image for each libraryroutine reference.
The stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present.
When the stub is executed, it checks to see whether the needed routine is already in memory.
If it is not, the program loads the routine into memory.
Either way, the stub replaces itself with the address of the routine and executes the routine.
Thus, the next time that particular code segment is reached, the library routine is executed directly, incurring no cost for dynamic linking.
Under this scheme, all processes that use a language library execute only one copy of the library code.
This feature can be extended to library updates (such as bug ﬁxes)
A library may be replaced by a new version, and all programs that reference the library will automatically use the new version.
So that programs will not accidentally execute new, incompatible versions of libraries, version information is included in both the program and the library.
More than one version of a library may be loaded into memory, and each program uses its version information to decide which copy of the library to use.
Versions with minor changes retain the same version number, whereas versions with major changes increment the number.
Thus, only programs that are compiled with the new library version are affected by any incompatible changes incorporated in it.
Other programs linked before the new library was installed will continue using the older library.
Unlike dynamic loading, dynamic linking and shared libraries generally require help from the operating system.
If the processes in memory are protected from one another, then the operating system is the only entity that can check to see whether the needed routine is in another process’s memory space or that can allow multiple processes to access the same memory addresses.
We elaborate on this concept when we discuss paging in Section 8.5.4
A process, however, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution (Figure 8.5)
Swapping makes it possible for the total physical address space of all processes to exceed the real physical memory of the system, thus increasing the degree of multiprogramming in a system.
Standard swapping involves moving processes between main memory and a backing store.
Figure 8.5 Swapping of two processes using a disk as a backing store.
The system maintains a ready queue consisting of all processes whose memory images are on the backing store or in memory and are ready to run.
Whenever the CPU scheduler decides to execute a process, it calls the dispatcher.
The dispatcher checks to see whether the next process in the queue is in memory.
If it is not, and if there is no free memory region, the dispatcher swaps out a process currently in memory and swaps in the desired process.
It then reloads registers and transfers control to the selected process.
The context-switch time in such a swapping system is fairly high.
The actual transfer of the 100-MB process to or from main memory takes.
Since we must swap both out and in, the total swap time is about 4,000 milliseconds.
Notice that the major part of the swap time is transfer time.
The total transfer time is directly proportional to the amount of memory swapped.
However, many user processes may be much smaller than this—say, 100 MB.
Clearly, it would be useful to know exactly how much memory a user process is using, not simply how much it might be using.
Then we would need to swap only what is actually used, reducing swap time.
For this method to be effective, the user must keep the system informed of any changes in memory requirements.
Thus, a process with dynamic memory requirements will need to issue system calls (request memory() and release memory()) to inform the operating system of its changing memory needs.
If we want to swap a process, we must be sure that it is completely idle.
A process may be waiting for an I/O operation when we want to swap that process to free up memory.
However, if the I/O is asynchronously accessing the user memory for I/O buffers, then the process cannot be swapped.
Assume that the I/O operation is queued because the device is busy.
There are two main solutions to this problem: never swap a process with pending I/O, or execute I/O operations only into operating-system buffers.
Transfers between operating-system buffers and process memory then occur only when the process is swapped in.
We now need to copy the data again, from kernel memory to user memory, before the user process can access it.
It requires too much swapping time and provides too little execution time to be a reasonable.
Modiﬁed versions of swapping, however, are found on many systems, including UNIX, Linux, and Windows.
In one common variation, swapping is normally disabled but will start if the amount of free memory (unused memory available for the operating system or processes to use) falls below a threshold amount.
Swapping is halted when the amount of free memory increases.
Another variation involves swapping portions of processes—rather than entire processes—to decrease swap time.
Although most operating systems for PCs and servers support some modiﬁed version of swapping, mobile systems typically do not support swapping in any form.
Mobile devices generally use ﬂash memory rather than more spacious hard disks as their persistent storage.
The resulting space constraint is one reason why mobile operating-system designers avoid swapping.
Other reasons include the limited number of writes that ﬂash memory can tolerate before it becomes unreliable and the poor throughput between main memory and ﬂash memory in these devices.
Instead of using swapping, when free memory falls below a certain threshold, Apple’s iOS asks applications to voluntarily relinquish allocated memory.
Read-only data (such as code) are removed from the system and later reloaded from ﬂash memory if necessary.
Data that have been modiﬁed (such as the stack) are never removed.
However, any applications that fail to free up sufﬁcient memory may be terminated by the operating system.
Android does not support swapping and adopts a strategy similar to that used by iOS.
It may terminate a process if insufﬁcient free memory is available.
However, before terminating a process, Android writes its application state to ﬂash memory so that it can be quickly restarted.
Because of these restrictions, developers for mobile systems must carefully allocate and release memory to ensure that their applications do not use too much memory or suffer from memory leaks.
Note that both iOS and Android support paging, so they do have memory-management abilities.
The main memory must accommodate both the operating system and the various user processes.
We therefore need to allocate main memory in the most efﬁcient way possible.
The memory is usually divided into two partitions: one for the resident operating system and one for the user processes.
We can place the operating system in either low memory or high memory.
The major factor affecting this decision is the location of the interrupt vector.
Since the interrupt vector is often in low memory, programmers usually place the operating system in low memory as well.
Thus, in this text, we discuss only the situation in which.
We usually want several user processes to reside in memory at the same time.
We therefore need to consider how to allocate available memory to the processes that are in the input queue waiting to be brought into memory.
In contiguous memory allocation, each process is contained in a single section of memory that is contiguous to the section containing the next process.
Before discussing memory allocation further, we must discuss the issue of memory protection.
We can prevent a process from accessing memory it does not own by combining two ideas previously discussed.
Each logical address must fall within the range speciﬁed by the limit register.
The MMU maps the logical address dynamically by adding the value in the relocation register.
When the CPU scheduler selects a process for execution, the dispatcher loads the relocation and limit registers with the correct values as part of the context switch.
Because every address generated by a CPU is checked against these registers, we can protect both the operating system and the other users’ programs and data from being modiﬁed by this running process.
For example, the operating system contains code and buffer space for device drivers.
If a device driver (or other operating-system service) is not commonly used, we do not want to keep the code and data in memory, as we might be able to use that space for other purposes.
Such code is sometimes called transient operating-system code; it comes and goes as needed.
Thus, using this code changes the size of the operating system during program execution.
In the variable-partition scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied.
Initially, all memory is available for user processes and is considered one large block of available memory, a hole.
Eventually, as you will see, memory contains a set of holes of various sizes.
As processes enter the system, they are put into an input queue.
The operating system takes into account the memory requirements of each process and the amount of available memory space in determining which processes are allocated memory.
When a process is allocated space, it is loaded into memory, and it can then compete for CPU time.
When a process terminates, it releases its memory, which the operating system may then ﬁll with another process from the input queue.
At any given time, then, we have a list of available block sizes and an input queue.
The operating system can order the input queue according to a scheduling algorithm.
Memory is allocated to processes until, ﬁnally, the memory requirements of the next process cannot be satisﬁed—that is, no available block of memory (or hole) is large enough to hold that process.
The operating system can then wait until a large enough block is available, or it can skip down the input queue to see whether the smaller memory requirements of some other process can be met.
In general, as mentioned, the memory blocks available comprise a set of holes of various sizes scattered throughout memory.
When a process arrives and needs memory, the system searches the set for a hole that is large enough for this process.
If the hole is too large, it is split into two parts.
One part is allocated to the arriving process; the other is returned to the set of holes.
When a process terminates, it releases its block of memory, which is then placed back in the set of holes.
If the new hole is adjacent to other holes, these adjacent holes are merged to form one larger hole.
At this point, the system may need to check whether there are processes waiting for memory and whether this newly freed and recombined memory could satisfy the demands of any of these waiting processes.
This procedure is a particular instance of the general dynamic storageallocation problem, which concerns how to satisfy a request of size n from a list of free holes.
The ﬁrst-ﬁt, best-ﬁt, and worst-ﬁt strategies are the ones most commonly used to select a free hole from the set of available holes.
Searching can start either at the beginning of the set of holes or at the location where the previous ﬁrst-ﬁt search ended.
We can stop searching as soon as we ﬁnd a free hole that is large enough.
We must search the entire list, unless the list is ordered by size.
Again, we must search the entire list, unless it is sorted by size.
This strategy produces the largest leftover hole, which may be more useful than the smaller leftover hole from a best-ﬁt approach.
Simulations have shown that both ﬁrst ﬁt and best ﬁt are better than worst ﬁt in terms of decreasing time and storage utilization.
Neither ﬁrst ﬁt nor best ﬁt is clearly better than the other in terms of storage utilization, but ﬁrst ﬁt is generally faster.
Both the ﬁrst-ﬁt and best-ﬁt strategies for memory allocation suffer from external fragmentation.
As processes are loaded and removed from memory, the free memory space is broken into little pieces.
External fragmentation exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous: storage is fragmented into a large number of small holes.
In the worst case, we could have a block of free (or wasted) memory between every two processes.
If all these small pieces of memory were in one big free block instead, we might be able to run several more processes.
Whether we are using the ﬁrst-ﬁt or best-ﬁt strategy can affect the amount of fragmentation.
First ﬁt is better for some systems, whereas best ﬁt is better for others.
Another factor is which end of a free block is allocated.
Depending on the total amount of memory storage and the average process size, external fragmentation may be a minor or a major problem.
Statistical analysis of ﬁrst ﬁt, for instance, reveals that, even with some optimization, given N allocated blocks, another 0.5 N blocks will be lost to fragmentation.
That is, one-third of memory may be unusable! This property is known as the 50-percent rule.
Consider a multiple-partition allocation scheme with a hole of 18,464 bytes.
If we allocate exactly the requested block, we are left with a hole of 2 bytes.
The overhead to keep track of this hole will be substantially larger than the hole itself.
The general approach to avoiding this problem is to break the physical memory into ﬁxed-sized blocks and allocate memory in units based on block size.
With this approach, the memory allocated to a process may be slightly larger than the requested memory.
One solution to the problem of external fragmentation is compaction.
The goal is to shufﬂe the memory contents so as to place all free memory together in one large block.
If relocation is static and is done at assembly or load time, compaction cannot be done.
It is possible only if relocation is dynamic and is done at execution time.
If addresses are relocated dynamically, relocation requires only moving the program and data and then changing the base register to reﬂect the new base address.
The simplest compaction algorithm is to move all processes toward one end of memory; all holes move in the other direction, producing one large hole of available memory.
Fragmentation is a general problem in computing that can occur wherever we must manage blocks of data.
As we’ve already seen, the user’s view of memory is not the same as the actual physical memory.
This is equally true of the programmer’s view of memory.
Indeed, dealing with memory in terms of its physical properties is inconvenient to both the operating system and the programmer.
What if the hardware could provide a memory mechanism that mapped the programmer’s view to the actual physical memory? The system would have more freedom to manage memory, while the programmer would have a more natural programming environment.
Do programmers think of memory as a linear array of bytes, some containing instructions and others containing data? Most programmers would say “no.” Rather, they prefer to view memory as a collection of variable-sized segments, with no necessary ordering among the segments (Figure 8.7)
When writing a program, a programmer thinks of it as a main program with a set of methods, procedures, or functions.
It may also include various data structures: objects, arrays, stacks, variables, and so on.
Each of these modules or data elements is referred to by name.
The programmer talks about “the stack,” “the math library,” and “the main program” without caring what addresses in memory these elements occupy.
She is not concerned with whether the stack is stored before or after the Sqrt() function.
Segments vary in length, and the length of each is intrinsically deﬁned by its purpose in the program.
Elements within a segment are identiﬁed by their offset from the beginning of the segment: the ﬁrst statement of the program, the seventh stack frame entry in the stack, the ﬁfth instruction of the Sqrt(), and so on.
Segmentation is a memory-management scheme that supports this programmer view of memory.
The addresses specify both the segment name and the offset within the segment.
The programmer therefore speciﬁes each address by two quantities: a segment name and an offset.
For simplicity of implementation, segments are numbered and are referred to by a segment number, rather than by a segment name.
Normally, when a program is compiled, the compiler automatically constructs segments reﬂecting the input program.
A C compiler might create separate segments for the following:
Libraries that are linked in during compile time might be assigned separate segments.
The loader would take all these segments and assign them segment numbers.
Although the programmer can now refer to objects in the program by a two-dimensional address, the actual physical memory is still, of course, a onedimensional sequence of bytes.
Thus, we must deﬁne an implementation to map two-dimensional user-deﬁned addresses into one-dimensional physical.
Each entry in the segment table has a segment base and a segment limit.
The segment base contains the starting physical address where the segment resides in memory, and the segment limit speciﬁes the length of the segment.
The use of a segment table is illustrated in Figure 8.8
A logical address consists of two parts: a segment number, s, and an offset into that segment, d.
The segment number is used as an index to the segment table.
The offset d of the logical address must be between 0 and the segment limit.
If it is not, we trap to the operating system (logical addressing attempt beyond end of segment)
When an offset is legal, it is added to the segment base to produce the address in physical memory of the desired byte.
The segment table is thus essentially an array of base–limit register pairs.
As an example, consider the situation shown in Figure 8.9
The segment table has a separate entry for each segment, giving the beginning address of the segment in physical memory (or base) and the length of that segment (or limit)
Segmentation permits the physical address space of a process to be noncontiguous.
It also solves the considerable problem of ﬁtting memory chunks of varying sizes onto the backing store.
Most memory-management schemes used before the introduction of paging suffered from this problem.
The problem arises because, when code fragments or data residing in main memory need to be swapped out, space must be found on the backing store.
The backing store has the same fragmentation problems discussed in connection with main memory, but access is much slower, so compaction is impossible.
Because of its advantages over earlier methods, paging in its various forms is used in most operating systems, from those for mainframes through those for smartphones.
Paging is implemented through cooperation between the operating system and the computer hardware.
The basic method for implementing paging involves breaking physical memory into ﬁxed-sized blocks called frames and breaking logical memory into blocks of the same size called pages.
When a process is to be executed, its pages are loaded into any available memory frames from their source (a ﬁle system or the backing store)
The backing store is divided into ﬁxed-sized blocks that are the same size as the memory frames or clusters of multiple frames.
This rather simple idea has great functionality and wide ramiﬁcations.
The hardware support for paging is illustrated in Figure 8.10
Every address generated by the CPU is divided into two parts: a page number (p) and a page.
The page number is used as an index into a page table.
The page table contains the base address of each page in physical memory.
This base address is combined with the page offset to deﬁne the physical memory address that is sent to the memory unit.
The paging model of memory is shown in Figure 8.11
As a concrete (although minuscule) example, consider the memory in Figure 8.12
On a Linux system, the page size varies according to architecture, and there are several ways of obtaining the page size.
Another strategy is to enter the following command on the command line:
Each of these techniques returns the page size as a number of bytes.
You may have noticed that paging itself is a form of dynamic relocation.
Every logical address is bound by the paging hardware to some physical address.
Using paging is similar to using a table of base (or relocation) registers, one for each frame of memory.
If process size is independent of page size, we expect internal fragmentation to average one-half page per process.
However, overhead is involved in each page-table entry, and this overhead is reduced as the size of the pages increases.
Also, disk I/O is more efﬁcient when the amount data being transferred is larger (Chapter 10)
Generally, page sizes have grown over time as processes, data sets, and main memory have become larger.
Today, pages typically are between CPUs and kernels even support multiple page sizes.
Researchers are now developing support for variable on-the-ﬂy page size.
We should note here that the size of physical memory in a paged memory system is different from the maximum logical size of a process.
As we further explore paging, we introduce other information that must be kept in the page-table entries.
Figure 8.13 Free frames (a) before allocation and (b) after allocation.
Thus, a system with 32-bit page-table entries may address less physical memory than the possible maximum.
Therefore, paging lets us use physical memory that is larger than what can be addressed by the CPU’s address pointer length.
When a process arrives in the system to be executed, its size, expressed in pages, is examined.
Thus, if the process requires n pages, at least n frames must be available in memory.
If n frames are available, they are allocated to this arriving process.
The ﬁrst page of the process is loaded into one of the allocated frames, and the frame number is put in the page table for this process.
The next page is loaded into another frame, its frame number is put into the page table, and so on (Figure 8.13)
An important aspect of paging is the clear separation between the programmer’s view of memory and the actual physical memory.
The programmer views memory as one single space, containing only this one program.
In fact, the user program is scattered throughout physical memory, which also holds other programs.
This mapping is hidden from the programmer and is controlled by the operating system.
Notice that the user process by deﬁnition is unable to access memory it does not own.
It has no way of addressing memory outside of its page table, and the table includes only those pages that the process owns.
Since the operating system is managing physical memory, it must be aware of the allocation details of physical memory—which frames are allocated, which frames are available, how many total frames there are, and so on.
This information is generally kept in a data structure called a frame table.
The frame table has one entry for each physical page frame, indicating whether the latter.
In addition, the operating system must be aware that user processes operate in user space, and all logical addresses must be mapped to produce physical addresses.
If a user makes a system call (to do I/O, for example) and provides an address as a parameter (a buffer, for instance), that address must be mapped to produce the correct physical address.
The operating system maintains a copy of the page table for each process, just as it maintains a copy of the instruction counter and register contents.
This copy is used to translate logical addresses to physical addresses whenever the operating system must map a logical address to a physical address manually.
It is also used by the CPU dispatcher to deﬁne the hardware page table when a process is to be allocated the CPU.
Each operating system has its own methods for storing page tables.
A pointer to the page table is stored with the other register values (like the instruction counter) in the process control block.
When the dispatcher is told to start a process, it must reload the user registers and deﬁne the correct hardware page-table values from the stored user page table.
Other operating systems provide one or at most a few page tables, which decreases the overhead involved when processes are context-switched.
The hardware implementation of the page table can be done in several ways.
In the simplest case, the page table is implemented as a set of dedicated registers.
These registers should be built with very high-speed logic to make the paging-address translation efﬁcient.
Every access to memory must go through the paging map, so efﬁciency is a major consideration.
The CPU dispatcher reloads these registers, just as it reloads the other registers.
Instructions to load or modify the page-table registers are, of course, privileged, so that only the operating system can change the memory map.
The DEC PDP-11 is an example of such an architecture.
The page table thus consists of eight entries that are kept in fast registers.
The use of registers for the page table is satisfactory if the page table is reasonably small (for example, 256 entries)
Most contemporary computers, however, allow the page table to be very large (for example, 1 million entries)
For these machines, the use of fast registers to implement the page table is not feasible.
Rather, the page table is kept in main memory, and a page-table base register (PTBR) points to the page table.
Changing page tables requires changing only this one register, substantially reducing context-switch time.
The problem with this approach is the time required to access a user memory location.
If we want to access location i, we must ﬁrst index into the page table, using the value in the PTBR offset by the page number for i.
It provides us with the frame number, which is combined with the page offset to produce the actual address.
With this scheme, two memory accesses are needed to access a byte (one for the page-table entry, one for the byte)
The standard solution to this problem is to use a special, small, fastlookup hardware cache called a translation look-aside buffer (TLB)
Each entry in the TLB consists of two parts: a key (or tag) and a value.
When the associative memory is presented with an item, the item is compared with all keys simultaneously.
If the item is found, the corresponding value ﬁeld is returned.
The search is fast; a TLB lookup in modern hardware is part of the instruction pipeline, essentially adding no performance penalty.
To be able to execute the search within a pipeline step, however, the TLB must be kept small.
That can double the number of TLB entries available, because those lookups occur in different pipeline steps.
We can see in this development an example of the evolution of CPU technology: systems have evolved from having no TLBs to having multiple levels of TLBs, just as they have multiple levels of caches.
The TLB is used with page tables in the following way.
The TLB contains only a few of the page-table entries.
When a logical address is generated by the CPU, its page number is presented to the TLB.
If the page number is found, its frame number is immediately available and is used to access memory.
As just mentioned, these steps are executed as part of the instruction pipeline within the CPU, adding no performance penalty compared with a system that does not implement paging.
If the page number is not in the TLB (known as a TLB miss), a memory reference to the page table must be made.
Depending on the CPU, this may be done automatically in hardware or via an interrupt to the operating system.
If the TLB is already full of entries, an existing entry must be selected for replacement.
Replacement policies range from least recently used (LRU) through round-robin to random.
Some CPUs allow the operating system to participate in LRU entry replacement, while others handle the matter themselves.
Furthermore, some TLBs allow certain entries to be wired down, meaning that they cannot be removed from the TLB.
Typically, TLB entries for key kernel code are wired down.
Some TLBs store address-space identiﬁers (ASIDs) in each TLB entry.
An ASID uniquely identiﬁes each process and is used to provide address-space protection for that process.
When the TLB attempts to resolve virtual page numbers, it ensures that the ASID for the currently running process matches the ASID associated with the virtual page.
If the ASIDs do not match, the attempt is treated as a TLB miss.
In addition to providing address-space protection, an ASID allows the TLB to contain entries for several different processes simultaneously.
If the TLB does not support separate ASIDs, then every time a new page table is selected (for instance, with each context switch), the TLB must be ﬂushed (or erased) to ensure that the next executing process does not use the wrong translation information.
Otherwise, the TLB could include old entries that contain valid virtual addresses but have incorrect or invalid physical addresses left over from the previous process.
The percentage of times that the page number of interest is found in the TLB is called the hit ratio.
We are assuming that a page-table lookup takes only one memory access, but it can take more, as we shall see.
To ﬁnd the effective memory-access time, we weight the case by its probability:
For a 99-percent hit ratio, which is much more realistic, we have.
This increased hit rate produces only a 1 percent slowdown in access time.
As we noted earlier, CPUs today may provide multiple levels of TLBs.
Calculating memory access times in modern CPUs is therefore much more complicated than shown in the example above.
A complete performance analysis of paging overhead in such a system would require miss-rate information about each TLB tier.
We can see from the general information above, however, that hardware features can have a significant effect on memory performance and that operating-system improvements (such as paging) can result in and, in turn, be affected by hardware changes (such as TLBs)
TLBs are a hardware feature and therefore would seem to be of little concern to operating systems and their designers.
But the designer needs to understand the function and features of TLBs, which vary by hardware platform.
For optimal operation, an operating-system design for a given platform must implement paging according to the platform’s TLB design.
Likewise, a change in the TLB design (for example, between generations of Intel CPUs) may necessitate a change in the paging implementation of the operating systems that use it.
Memory protection in a paged environment is accomplished by protection bits associated with each frame.
One bit can deﬁne a page to be read–write or read-only.
Every reference to memory goes through the page table to ﬁnd the correct frame number.
At the same time that the physical address is being computed, the protection bits can be checked to verify that no writes are being made to a read-only page.
An attempt to write to a read-only page causes a hardware trap to the operating system (or memory-protection violation)
We can easily expand this approach to provide a ﬁner level of protection.
We can create hardware to provide read-only, read–write, or execute-only protection; or, by providing separate protection bits for each kind of access, we can allow any combination of these accesses.
One additional bit is generally attached to each entry in the page table: a valid–invalid bit.
When this bit is set to valid, the associated page is in the process’s logical address space and is thus a legal (or valid) page.
When the bit is set toinvalid, the page is not in the process’s logical address space.
Illegal addresses are trapped by use of the valid–invalid bit.
The operating system sets this bit for each page to allow or disallow access to the page.
Because the program extends only to address 10468, any reference beyond that address is illegal.
Figure 8.15 Valid (v) or invalid (i) bit in a page table.
In fact, many processes use only a small fraction of the address space available to them.
It would be wasteful in these cases to create a page table with entries for every page in the address range.
Most of this table would be unused but would take up valuable memory space.
Some systems provide hardware, in the form of a page-table length register (PTLR), to indicate the size of the page table.
This value is checked against every logical address to verify that the address is in the valid range for the process.
Failure of this test causes an error trap to the operating system.
An advantage of paging is the possibility of sharing common code.
Consider a system that supports 40 users, each of whom executes a text editor.
If the code is reentrant code (or pure code), however, it can be shared, as shown in Figure 8.16
Here, we see three processes sharing a three-page editor—each page 50 KB in size (the large page size is used to simplify the ﬁgure)
Reentrant code is non-self-modifying code: it never changes during execution.
Thus, two or more processes can execute the same code at the same time.
Each process has its own copy of registers and data storage to hold the data for the process’s execution.
The data for two different processes will, of course, be different.
Only one copy of the editor need be kept in physical memory.
Each user’s page table maps onto the same physical copy of the editor, but data pages are mapped onto different frames.
Other heavily used programs can also be shared—compilers, window systems, run-time libraries, database systems, and so on.
The read-only nature of shared code should not be left to the correctness of the code; the operating system should enforce this property.
Furthermore, recall that in Chapter 3 we described shared memory as a method of interprocess communication.
Organizing memory according to pages provides numerous beneﬁts in addition to allowing several processes to share the same physical pages.
In this section, we explore some of the most common techniques for structuring the page table, including hierarchical paging, hashed page tables, and inverted page tables.
In such an environment, the page table itself becomes excessively large.
For example, consider a system with a 32-bit logical address space.
Clearly, we would not want to allocate the page table contiguously in main memory.
One simple solution to this problem is to divide the page table into smaller pieces.
One way is to use a two-level paging algorithm, in which the page table itself is also paged (Figure 8.17)
Because we page the page table, the page number is further divided.
Because address translation works from the outer page table inward, this scheme is also known as a forward-mapped page table.
Consider the memory management of one of the classic systems, the VAX minicomputer from Digital Equipment Corporation (DEC)
The logical address space of a process is divided into four equal sections, each of which consists of 230 bytes.
Each section represents a different part of the logical address space of a process.
The ﬁrst 2 high-order bits of the logical address designate the appropriate section.
By partitioning the page table in this manner, the operating system can leave partitions unused until a process needs them.
Entire sections of virtual address space are frequently unused, and multilevel page tables have no entries for these spaces, greatly decreasing the amount of memory needed to store virtual memory data structures.
To further reduce main-memory use, the VAX pages the user-process page tables.
For a system with a 64-bit logical address space, a two-level paging scheme is no longer appropriate.
In this case, the page table consists of up to 252 entries.
The obvious way to avoid such a large table is to divide the outer page table into smaller pieces.
This approach is also used on some 32-bit processors for added ﬂexibility and efﬁciency.
We can divide the outer page table in various ways.
For example, we can page the outer page table, giving us a three-level paging scheme.
In this case, a 64-bit address space is still daunting:
The next step would be a four-level paging scheme, where the second-level.
The 64-bit UltraSPARC would require seven levels of paging—a prohibitive number of memory accessesto translate each logical address.
You can see from this example why, for 64-bit architectures, hierarchical page tables are generally considered inappropriate.
A common approach for handling address spaces larger than 32 bits is to use a hashed page table, with the hash value being the virtual page number.
Each entry in the hash table contains a linked list of elements that hash to the same location (to handle collisions)
The algorithm works as follows: The virtual page number in the virtual address is hashed into the hash table.
The virtual page number is compared with ﬁeld 1 in the ﬁrst element in the linked list.
If there is a match, the corresponding page frame (ﬁeld 2) is used to form the desired physical address.
If there is no match, subsequent entries in the linked list are searched for a matching virtual page number.
A variation of this scheme that is useful for 64-bit address spaces has been proposed.
This variation uses clustered page tables, which are similar to.
Therefore, a single page-table entry can store the mappings for multiple physical-page frames.
Clustered page tables are particularly useful for sparse address spaces, where memory references are noncontiguous and scattered throughout the address space.
The page table has one entry for each page that the process is using (or one slot for each virtual address, regardless of the latter’s validity)
This table representation is a natural one, since processes reference pages through the pages’ virtual addresses.
The operating system must then translate this reference into a physical memory address.
Since the table is sorted by virtual address, the operating system is able to calculate where in the table the associated physical address entry is located and to use that value directly.
One of the drawbacks of this method is that each page table may consist of millions of entries.
These tables may consume large amounts of physical memory just to keep track of how other physical memory is being used.
To solve this problem, we can use an inverted page table.
An inverted page table has one entry for each real page (or frame) of memory.
Each entry consists of the virtual address of the page stored in that real memory location, with information about the process that owns the page.
Thus, only one page table is in the system, and it has only one entry for each page of physical memory.
Figure 8.20 shows the operation of an inverted page table.
Compare it with Figure 8.10, which depicts a standard page table in operation.
Inverted page tables often require that an address-space identiﬁer (Section 8.5.2) be stored in each entry of the page table, since the table usually contains several different address spaces mapping physical memory.
Storing the address-space identiﬁer ensures that a logical page for a particular process is mapped to the corresponding physical page frame.
Examples of systems using inverted page tables include the 64-bit UltraSPARC and PowerPC.
To illustrate this method, we describe a simpliﬁed version of the inverted page table used in the IBM RT.
For the IBM RT, each virtual address in the system consists of a triple:
The inverted page table is then searched for a match.
If no match is found, then an illegal address access has been attempted.
Although this scheme decreases the amount of memory needed to store each page table, it increases the amount of time needed to search the table when a page reference occurs.
Because the inverted page table is sorted by physical address, but lookups occur on virtual addresses, the whole table might need to be searched before a match is found.
To alleviate this problem, we use a hash table, as described in Section 8.6.2, to limit the search to one—or at most a few—page-table entries.
Of course, each access to the hash table adds a memory reference to the procedure, so one virtual memory reference requires at least two real memory reads—one for the hash-table entry and one for the page table.
Recall that the TLB is searched ﬁrst, before the hash table is consulted, offering some performance improvement.
Systems that use inverted page tables have difﬁculty implementing shared memory.
Shared memory is usually implemented as multiple virtual addresses (one for each process sharing the memory) that are mapped to one physical address.
This standard method cannot be used with inverted page tables; because there is only one virtual page entry for every physical page, one.
A simple technique for addressing this issue is to allow the page table to contain only one mapping of a virtual address to the shared physical address.
This means that references to virtual addresses that are not mapped result in page faults.
Consider as a ﬁnal example a modern 64-bit CPU and operating system that are tightly integrated to provide low-overhead virtual memory.
Solaris running on the SPARC CPU is a fully 64-bit operating system and as such has to solve the problem of virtual memory without using up all of its physical memory by keeping multiple levels of page tables.
Its approach is a bit complex but solves the problem efﬁciently using hashed page tables.
There are two hash tables—one for the kernel and one for all user processes.
Each hash-table entry represents a contiguous area of mapped virtual memory, which is more efﬁcient than having a separate hash-table entry for each page.
Each entry has a base address and a span indicating the number of pages the entry represents.
A cache of these TTEs reside in a translation storage buffer (TSB), which includes an entry per recently accessed page.
When a virtual address reference occurs, the hardware searches the TLB for a translation.
If none is found, the hardware walks through the in-memory TSB looking for the TTE that corresponds to the virtual address that caused the lookup.
This TLB walk functionality is found on many modern CPUs.
If a match is found in the TSB, the CPU copies the TSB entry into the TLB, and the memory translation completes.
If no match is found in the TSB, the kernel is interrupted to search the hash table.
The kernel then creates a TTE from the appropriate hash table and stores it in the TSB for automatic loading into the TLB by the CPU memory-management unit.
Finally, the interrupt handler returns control to the MMU, which completes the address translation and retrieves the requested byte or word from main memory.
The architecture of Intel chips has dominated the personal computer landscape for several years.
Currently, all the most popular PC operating systems run on Intel chips, including Windows, Mac OS X, and Linux (although Linux, of course, runs on several other architectures as well)
Notably, however, Intel’s dominance has not spread to mobile systems, where the ARM architecture currently enjoys considerable success (see Section 8.8)
Before we proceed, however, it is important to note that because Intel has released several versions—as well as variations—of its architectures over the years, we cannot provide a complete description of the memorymanagement structure of all its chips.
Nor can we provide all of the CPU details, as that information is best left to books on computer architecture.
Rather, we present the major memory-management concepts of these Intel CPUs.
The segmentation unit produces a linear address for each logical address.
The linear address is then given to the paging unit, which in turn generates the physical address in main memory.
Thus, the segmentation and paging units form the equivalent of the memory-management unit (MMU)
The logical address is a pair (selector, offset), where the selector is a 16-bit number:
The offset is a 32-bit number specifying the location of the byte within the segment in question.
The machine has six segment registers, allowing six segments to be addressed at any one time by a process.
It also has six 8-byte microprogram registers to hold the corresponding descriptors from either the LDT or GDT.
This cache lets the Pentium avoid having to read the descriptor from memory for every memory reference.
The segment register points to the appropriate entry in the LDT or GDT.
The base and limit information about the segment in question is used to generate a linear address.
First, the limit is used to check for address validity.
If the address is not valid, a memory fault is generated, resulting in a trap to the operating system.
If it is valid, then the value of the offset is added to the value of the base, resulting in a 32-bit linear address.
In the following section, we discuss how the paging unit turns this linear address into a physical address.
The CR3 register points to the page directory for the current process.
The page directory entry points to an inner page table that is indexed by the contents of the innermost 10 bits in the linear address.
To improve the efﬁciency of physical memory use, IA-32 page tables can be swapped to disk.
In this case, an invalid bit is used in the page directory entry to indicate whether the table to which the entry is pointing is in memory or on disk.
If the table is on disk, the operating system can use the other 31 bits to specify the disk location of the table.
The table can then be brought into memory on demand.
The fundamental difference introduced by PAE support was that paging went from a two-level scheme (as shown in Figure 8.23) to a three-level scheme, where the top two bits refer to a page directory pointer table.
It is important to note that operating system support is required to use PAE.
Intel has had an interesting history of developing 64-bit architectures.
Its initial entry was the IA-64 (later named Itanium) architecture, but that architecture was not widely adopted.
The x86-64 supported much larger logical and physical address spaces, as well as several other architectural advances.
Historically, AMD had often developed chips based on Intel’s architecture, but now the roles were reversed as Intel adopted AMD’s x86-64 architecture.
The representation of the linear address appears in Figure 8.25
History has taught us that even though memory capacities, CPU speeds, and similar computer capabilities seem large enough to satisfy demand for the foreseeable future, the growth of technology ultimately absorbs available capacities, and we ﬁnd ourselves in need of additional memory or processing power, often sooner than we think.
What might the future of technology bring that would make a 64-bit address space seem too small?
Interestingly, whereas Intel both designs and manufactures chips, ARM only designs them.
Apple has licensed the ARM design for its iPhone and iPad mobile devices, and several Android-based smartphones use ARM processors as well.
The paging system in use depends on whether a page or a section is being referenced.
Address translation with the ARM MMU is shown in Figure 8.26
At the outer level are two micro TLBs—a separate TLB for data and another for instructions.
In the case of a miss, the main TLB is then checked.
If both TLBs yield misses, a page table walk must be performed in hardware.
Memory-management algorithms for multiprogrammed operating systems range from the simple single-user system approach to segmentation and paging.
The most important determinant of the method used in a particular system is the hardware provided.
Every memory address generated by the CPU must be checked for legality and possibly mapped to a physical address.
The various memory-management algorithms (contiguous allocation, paging, segmentation, and combinations of paging and segmentation) differ in many aspects.
In comparing different memory-management strategies, we use the following considerations:
A simple base register or a base–limit register pair is sufﬁcient for the single- and multiple-partition schemes, whereas paging and segmentation need mapping tables to deﬁne the address map.
As the memory-management algorithm becomes more complex, the time required to map a logical address to a physical address increases.
For the simple systems, we need only compare or add to the logical address—operations that are fast.
Paging and segmentation can be as fast if the mapping table is implemented in fast registers.
If the table is in memory, however, user memory accesses can be degraded substantially.
A TLB can reduce the performance degradation to an acceptable level.
A multiprogrammed system will generally perform more efﬁciently if it has a higher level of multiprogramming.
For a given set of processes, we can increase the multiprogramming level only by packing more processes into memory.
To accomplish this task, we must reduce memory waste, or fragmentation.
Systems with ﬁxed-sized allocation units, such as the single-partition scheme and paging, suffer from internal fragmentation.
Systems with variable-sized allocation units, such as the multiple-partition scheme and segmentation, suffer from external fragmentation.
Compaction involves shifting a program in memory in such a way that the program does not notice the change.
This consideration requires that logical addresses be relocated dynamically, at execution time.
If addresses are relocated only at load time, we cannot compact storage.
At intervals determined by the operating system, usually dictated by CPU-scheduling policies, processes are copied from main memory to a backing store and later are copied back to main memory.
This scheme allows more processes to be run than can be ﬁt into memory at one time.
In general, PC operating systems support paging, and operating systems for mobile devices do not.
Another means of increasing the multiprogramming level is to share code and data among different processes.
Sharing generally requires that either paging or segmentation be used to provide small packets of.
Sharing is a means of running many processes with a limited amount of memory, but shared programs and data must be designed carefully.
If paging or segmentation is provided, different sections of a user program can be declared execute-only, read-only, or read–write.
This restriction is necessary with shared code or data and is generally useful in any case to provide simple run-time checks for common programming errors.
The CPU knows whether it wants an instruction (instruction fetch) or data (data fetch or store)
Therefore, two baselimit register pairs are provided: one for instructions and one for data.
The instruction base–limit register pair is automatically read-only, so programs can be shared among different users.
What effect would updating some byte on the one page have on the other page?
Deﬁne a system that allows static linking and sharing of segments without requiring that the segment numbers be the same.
Describe a paging scheme that allows pages to be shared without requiring that the page numbers be the same.
Each 2-K block of memory has a key (the storage key) associated with it.
The CPU also has a key (the protection key) associated with it.
Which of the following memory-management schemes could be used successfully with this hardware?
A compiler is used to generate the object code for individual modules, and a linkage editor is used to combine multiple object modules into a single program binary.
How does the linkage editor change the binding of instructions and data to memory addresses? What information needs to be passed from the compiler to the linkage editor to facilitate the memory-binding tasks of the linkage editor?
Allocation of data in the heap segments of programs is an example of such allocated memory.
What is required to support dynamic memory allocation in the following schemes?
Why? How could the operating system allow access to other memory? Why should it or should it not?
Why would Android disallow swapping on its boot disk yet allow it on a secondary disk?
The code segment is followed by the data segment that is used for storing the program variables.
When the program starts executing, the stack is allocated at the other end of the virtual address space and is allowed to grow toward lower virtual addresses.
What is the signiﬁcance of this structure for the following schemes?
How many entries are there in each of the following?
The system supports up to 512 MB of physical memory.
How many entries are there in each of the following?
If a memory reference takes 50 nanoseconds, how long does a paged memory reference take?
What are the physical addresses for the following logical addresses?
How many memory operations are performed when a user program executes a memory-load operation?
Under what circumstances is one scheme preferable to the other?
Describe all the steps taken by the Intel Pentium in translating a logical address into a physical address.
What are the advantages to the operating system of hardware that provides such complicated memory translation?
Write a C program that is passed a virtual address (in decimal) on the command line and have it output the page number and offset for the given address.
Writing this program will require using the appropriate data type to store 32 bits.
We encourage you to use unsigned data types as well.
The concept of segmentation was ﬁrst discussed by [Dennis (1965)]
Inverted page tables are discussed in an article about the IBM RT storage manager by [Chang and Mergen (1988)]
In Chapter 8, we discussed various memory-management strategies used in computer systems.
All these strategies have the same goal: to keep many processes in memory simultaneously to allow multiprogramming.
However, they tend to require that an entire process be in memory before it can execute.
Virtual memory is a technique that allows the execution of processes that are not completely in memory.
One major advantage of this scheme is that programs can be larger than physical memory.
Further, virtual memory abstracts main memory into an extremely large, uniform array of storage, separating logical memory as viewed by the user from physical memory.
This technique frees programmers from the concerns of memory-storage limitations.
Virtual memory also allows processes to share ﬁles easily and to implement shared memory.
In addition, it provides an efﬁcient mechanism for process creation.
Virtual memory is not easy to implement, however, and may substantially decrease performance if it is used carelessly.
In this chapter, we discuss virtual memory in the form of demand paging and examine its complexity and cost.
To explain the concepts of demand paging, page-replacement algorithms, and allocation of page frames.
The memory-management algorithms outlined in Chapter 8 are necessary because of one basic requirement: The instructions being executed must be.
The ﬁrst approach to meeting this requirement is to place the entire logical address space in physical memory.
Dynamic loading can help to ease this restriction, but it generally requires special precautions and extra work by the programmer.
The requirement that instructions must be in physical memory to be executed seems both necessary and reasonable; but it is also unfortunate, since it limits the size of a program to the size of physical memory.
In fact, an examination of real programs shows us that, in many cases, the entire program is not needed.
Since these errors seldom, if ever, occur in practice, this code is almost never executed.
Arrays, lists, and tables are often allocated more memory than they actually need.
Certain options and features of a program may be used rarely.
Even in those cases where the entire program is needed, it may not all be needed at the same time.
The ability to execute a program that is only partially in memory would confer many beneﬁts:
A program would no longer be constrained by the amount of physical memory that is available.
Users would be able to write programs for an extremely large virtual address space, simplifying the programming task.
Because each user program could take less physical memory, more programs could be run at the same time, with a corresponding increase in CPU utilization and throughput but with no increase in response time or turnaround time.
Less I/O would be needed to load or swap user programs into memory, so each user program would run faster.
Thus, running a program that is not entirely in memory would beneﬁt both the system and the user.
Virtual memory involves the separation of logical memory as perceived by users from physical memory.
This separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available (Figure 9.1)
Virtual memory makes the task of programming much easier, because the programmer no longer needs to worry about the amount of physical memory available; she can concentrate instead on the problem to be programmed.
The virtual address space of a process refers to the logical (or virtual) view of how a process is stored in memory.
Figure 9.1 Diagram showing virtual memory that is larger than physical memory.
It is up to the memorymanagement unit (MMU) to map logical pages to physical page frames in memory.
Note in Figure 9.2 that we allow the heap to grow upward in memory as it is used for dynamic memory allocation.
The large blank space (or hole) between the heap and the stack is part of the virtual address space but will require actual physical pages only if the heap or stack grows.
Virtual address spaces that include holes are known as sparse address spaces.
Using a sparse address space is beneﬁcial because the holes can be ﬁlled as the stack or heap segments grow or if we wish to dynamically link libraries (or possibly other shared objects) during program execution.
In addition to separating logical memory from physical memory, virtual memory allows ﬁles and memory to be shared by two or more processes through page sharing (Section 8.5.4)
System libraries can be shared by several processes through mapping of the shared object into a virtual address space.
Although each process considers the libraries to be part of its virtual address space, the actual pages where the libraries reside in physical memory are shared by all the processes (Figure 9.3)
Typically, a library is mapped read-only into the space of each process that is linked with it.
Recall from Chapter 3 that two or more processes can communicate through the use of shared memory.
Virtual memory allows one process to create a region of memory that it can share with another process.
Processes sharing this region consider it part of their virtual address space, yet the actual physical pages of memory are shared, much as is illustrated in Figure 9.3
Pages can be shared during process creation with the fork() system call, thus speeding up process creation.
We further explore these—and other—beneﬁts of virtual memory later in this chapter.
First, though, we discuss implementing virtual memory through demand paging.
Consider how an executable program might be loaded from disk into memory.
One option is to load the entire program in physical memory at program execution time.
However, a problem with this approach is that we may not initially need the entire program in memory.
Suppose a program starts with a list of available options from which the user is to select.
Loading the entire program into memory results in loading the executable code for all options, regardless of whether or not an option is ultimately selected by the user.
An alternative strategy is to load pages only as they are needed.
This technique is known as demand paging and is commonly used in virtual memory systems.
With demand-paged virtual memory, pages are loaded only when they are demanded during program execution.
Pages that are never accessed are thus never loaded into physical memory.
A demand-paging system is similar to a paging system with swapping (Figure 9.4) where processes reside in secondary memory (usually a disk)
When we want to execute a process, we swap it into memory.
Rather than swapping the entire process into memory, though, we use a lazy swapper.
A lazy swapper never swaps a page into memory unless that page will be needed.
In the context of a demand-paging system, use of the term “swapper” is technically incorrect.
A swapper manipulates entire processes, whereas a pager is concerned with the individual pages of a process.
We thus use “pager,” rather than “swapper,” in connection with demand paging.
Figure 9.4 Transfer of a paged memory to contiguous disk space.
When a process is to be swapped in, the pager guesses which pages will be used before the process is swapped out again.
Instead of swapping in a whole process, the pager brings only those pages into memory.
Thus, it avoids reading into memory pages that will not be used anyway, decreasing the swap time and the amount of physical memory needed.
With this scheme, we need some form of hardware support to distinguish between the pages that are in memory and the pages that are on the disk.
The valid–invalid bit scheme described in Section 8.5.3 can be used for this purpose.
This time, however, when this bit is set to “valid,” the associated page is both legal and in memory.
If the bit is set to “invalid,” the page either is not valid (that is, not in the logical address space of the process) or is valid but is currently on the disk.
The page-table entry for a page that is brought into memory is set as usual, but the page-table entry for a page that is not currently in memory is either simply marked invalid or contains the address of the page on disk.
Notice that marking a page invalid will have no effect if the process never attempts to access that page.
Hence, if we guess right and page in all pages that are actually needed and only those pages, the process will run exactly as though we had brought in all pages.
While the process executes and accesses pages that are memory resident, execution proceeds normally.
Figure 9.5 Page table when some pages are not in main memory.
But what happens if the process tries to access a page that was not brought into memory? Access to a page marked invalid causes a page fault.
The paging hardware, in translating the address through the page table, will notice that the invalid bit is set, causing a trap to the operating system.
This trap is the result of the operating system’s failure to bring the desired page into memory.
The procedure for handling this page fault is straightforward (Figure 9.6):
We check an internal table (usually kept with the process control block) for this process to determine whether the reference was a valid or an invalid memory access.
If it was valid but we have not yet brought in that page, we now page it in.
We ﬁnd a free frame (by taking one from the free-frame list, for example)
We schedule a disk operation to read the desired page into the newly allocated frame.
When the disk read is complete, we modify the internal table kept with the process and the page table to indicate that the page is now in memory.
We restart the instruction that was interrupted by the trap.
The process can now access the page as though it had always been in memory.
In the extreme case, we can start executing a process with no pages in memory.
When the operating system sets the instruction pointer to the ﬁrst.
After this page is brought into memory, the process continues to execute, faulting as necessary until every page that it needs is in memory.
At that point, it can execute with no more faults.
This scheme is pure demand paging: never bring a page into memory until it is required.
Theoretically, some programs could access several new pages of memory with each instruction execution (one page for the instruction and many for data), possibly causing multiple page faults per instruction.
Fortunately, analysis of running processes shows that this behavior is exceedingly unlikely.
Programs tend to have locality of reference, described in Section 9.6.1, which results in reasonable performance from demand paging.
The hardware to support demand paging is the same as the hardware for paging and swapping:
This table has the ability to mark an entry invalid through a valid–invalid bit or a special value of protection bits.
This memory holds those pages that are not present in main memory.
It is known as the swap device, and the section of disk used for this purpose is known as swap space.
A crucial requirement for demand paging is the ability to restart any instruction after a page fault.
Because we save the state (registers, condition code, instruction counter) of the interrupted process when the page fault occurs, we must be able to restart the process in exactly the same place and state, except that the desired page is now in memory and is accessible.
If the page fault occurs on the instruction fetch, we can restart by fetching the instruction again.
If a page fault occurs while we are fetching an operand, we must fetch and decode the instruction again and then fetch the operand.
As a worst-case example, consider a three-address instruction such as ADD the content of A to B, placing the result in C.
If we fault when we try to store in C (because C is in a page not currently in memory), we will have to get the desired page, bring it in, correct the page table, and restart the instruction.
The restart will require fetching the instruction again, decoding it again, fetching the two operands again, and then adding again.
The major difﬁculty arises when one instruction may modify several different locations.
If either block (source or destination) straddles a page boundary, a page fault might occur after the move is partially done.
In addition, if the source and destination blocks overlap, the source block may have been modiﬁed, in which case we cannot simply restart the instruction.
In one solution, the microcode computes and attempts to access both ends of both blocks.
If a page fault is going to occur, it will happen at this step, before anything is modiﬁed.
The move can then take place; we know that no page fault can occur, since all the relevant pages are in memory.
The other solution uses temporary registers to hold the values of overwritten locations.
If there is a page fault, all the old values are written back into memory before the trap occurs.
This action restores memory to its state before the instruction was started, so that the instruction can be repeated.
This is by no means the only architectural problem resulting from adding paging to an existing architecture to allow demand paging, but it illustrates some of the difﬁculties involved.
Paging is added between the CPU and the memory in a computer system.
Thus, people often assume that paging can be added to any system.
Although this assumption is true for a non-demand-paging environment, where a page fault represents a fatal error, it is not true where a page fault means only that an additional page must be brought into memory and the process restarted.
Demand paging can signiﬁcantly affect the performance of a computer system.
To see why, let’s compute the effective access time for a demand-paged memory.
As long as we have no page faults, the effective access time is equal to the memory access time.
If, however, a page fault occurs, we must ﬁrst read the relevant page from disk and then access the desired word.
Check that the page reference was legal and determine the location of the page on the disk.
Issue a read from the disk to a free frame:
Wait in a queue for this device until the read request is serviced.
Begin the transfer of the page to a free frame.
While waiting, allocate the CPU to some other user (CPU scheduling, optional)
Receive an interrupt from the disk I/O subsystem (I/O completed)
Save the registers and process state for the other user (if step 6 is executed)
Correct the page table and other tables to show that the desired page is now in memory.
Wait for the CPU to be allocated to this process again.
Restore the user registers, process state, and new page table, and then resume the interrupted instruction.
Not all of these steps are necessary in every case.
For example, we are assuming that, in step 6, the CPU is allocated to another process while the I/O occurs.
This arrangement allows multiprogramming to maintain CPU utilization but requires additional time to resume the page-fault service routine when the I/O transfer is complete.
In any case, we are faced with three major components of the page-fault service time:
The ﬁrst and third tasks can be reduced, with careful coding, to several hundred instructions.
The page-switch time, however, will probably be close to 8 milliseconds.
Thus, the total paging time is about 8 milliseconds, including hardware and software time.
Remember also that we are looking at only the device-service time.
If a queue of processes is waiting for the device, we have to add device-queueing time as we wait for the paging device to be free to service our request, increasing even more the time to swap.
We see, then, that the effective access time is directly proportional to the page-fault rate.
That is, to keep the slowdown due to paging at a reasonable level, we can allow fewer than one memory access out of 399,990 to page-fault.
In sum, it is important to keep the page-fault rate low in a demand-paging system.
Otherwise, the effective access time increases, slowing process execution dramatically.
An additional aspect of demand paging is the handling and overall use of swap space.
Disk I/O to swap space is generally faster than that to the ﬁle system.
It is a faster ﬁle system because swap space is allocated in much larger blocks, and ﬁle lookups and indirect allocation methods are not used (Chapter 10)
The system can therefore gain better paging throughput by copying an entire ﬁle image into the swap space at process startup and then performing demand paging from the swap space.
Another option is to demand pages from the ﬁle system initially but to write the pages to swap space as they are replaced.
This approach will ensure that only needed pages are read from the ﬁle system but that all subsequent paging is done from swap space.
Instead, these systems demand-page from the ﬁle system and reclaim read-only pages (such as code) from applications if memory becomes constrained.
Such data can be demand-paged from the ﬁle system if it is later needed.
Under iOS, anonymous memory pages are never reclaimed from an application unless the application is terminated or explicitly releases the memory.
In Section 9.2, we illustrated how a process can start quickly by demand-paging in the page containing the ﬁrst instruction.
However, process creation using the fork() system call may initially bypass the need for demand paging by using a technique similar to page sharing (covered in Section 8.5.4)
This technique provides rapid process creation and minimizes the number of new pages that must be allocated to the newly created process.
Recall that the fork() system call creates a child process that is a duplicate of its parent.
Traditionally, fork() worked by creating a copy of the parent’s address space for the child, duplicating the pages belonging to the parent.
However, considering that many child processes invoke the exec() system call immediately after creation, the copying of the parent’s address space may be unnecessary.
Instead, we can use a technique known as copy-on-write, which works by allowing the parent and child processes initially to share the same pages.
These shared pages are marked as copy-on-write pages, meaning that if either process writes to a shared page, a copy of the shared page is created.
For example, assume that the child process attempts to modify a page containing portions of the stack, with the pages set to be copy-on-write.
The operating system will create a copy of this page, mapping it to the address space of the child process.
The child process will then modify its copied page and not the page belonging to the parent process.
Obviously, when the copy-on-write technique is used, only the pages that are modiﬁed by either process are copied; all unmodiﬁed pages can be shared by the parent and child processes.
Note, too, that only pages that can be modiﬁed need be marked as copy-on-write.
Pages that cannot be modiﬁed (pages containing executable code) can be shared by the parent and child.
Copy-on-write is a common technique used by several operating systems, including Windows XP, Linux, and Solaris.
When it is determined that a page is going to be duplicated using copyon-write, it is important to note the location from which the free page will be allocated.
Many operating systems provide a pool of free pages for such requests.
These free pages are typically allocated when the stack or heap for a process must expand or when there are copy-on-write pages to be managed.
Operating systems typically allocate these pages using a technique known as zero-ﬁll-on-demand.
Zero-ﬁll-on-demand pages have been zeroed-out before being allocated, thus erasing the previous contents.
Several versions of UNIX (including Solaris and Linux) provide a variation of the fork() system call—vfork() (for virtual memory fork)—that operates differently from fork() with copy-on-write.
With vfork(), the parent process is suspended, and the child process uses the address space of the parent.
Because vfork() does not use copy-on-write, if the child process changes any pages of the parent’s address space, the altered pages will be visible to the parent once it resumes.
Because no copying of pages takes place, vfork() is an extremely efﬁcient method of process creation and is sometimes used to implement UNIX command-line shell interfaces.
In our earlier discussion of the page-fault rate, we assumed that each page faults at most once, when it is ﬁrst referenced.
If a process of ten pages actually uses only half of them, then demand paging saves the I/O necessary to load the ﬁve pages that are never used.
We could also increase our degree of multiprogramming by running twice as many processes.
Thus, if we had forty frames, we could run eight processes, rather than the four that could run if each required ten frames (ﬁve of which were never used)
If we increase our degree of multiprogramming, we are over-allocating memory.
If we run six processes, each of which is ten pages in size but actually uses only ﬁve pages, we have higher CPU utilization and throughput, with ten frames to spare.
It is possible, however, that each of these processes, for a particular data set, may suddenly try to use all ten of its pages, resulting in a need for sixty frames when only forty are available.
Further, consider that system memory is not used only for holding program pages.
Buffers for I/O also consume a considerable amount of memory.
Deciding how much memory to allocate to I/O and how much to program pages is a signiﬁcant challenge.
Some systems allocate a ﬁxed percentage of memory for I/O buffers, whereas others allow both user processes and the I/O subsystem to compete for all system memory.
While a user process is executing, a page fault occurs.
The operating system determines where the desired page is residing on the disk but then ﬁnds that there are no free frames on the free-frame list; all memory is in use (Figure 9.9)
However, demand paging is the operating system’s attempt to improve the computer system’s utilization and throughput.
Users should not be aware that their processes are running on a paged system—paging should be logically transparent to the user.
The operating system could instead swap out a process, freeing all its frames and reducing the level of multiprogramming.
This option is a good one in certain circumstances, and we consider it further in Section 9.6
If no frame is free, we ﬁnd one that is not currently being used and free it.
We can free a frame by writing its contents to swap space and changing the page table (and all other tables) to indicate that the page is no longer in memory (Figure 9.10)
We can now use the freed frame to hold the page for which the process faulted.
We modify the page-fault service routine to include page replacement:
Find the location of the desired page on the disk.
If there is no free frame, use a page-replacement algorithm to select a victim frame.
Write the victim frame to the disk; change the page and frame tables accordingly.
Read the desired page into the newly freed frame; change the page and frame tables.
Continue the user process from where the page fault occurred.
Notice that, if no frames are free, two page transfers (one out and one in) are required.
This situation effectively doubles the page-fault service time and increases the effective access time accordingly.
We can reduce this overhead by using a modify bit (or dirty bit)
When this scheme is used, each page or frame has a modify bit associated with it in the hardware.
The modify bit for a page is set by the hardware whenever any byte in the page is written into, indicating that the page has been modiﬁed.
When we select a page for replacement, we examine its modify bit.
If the bit is set, we know that the page has been modiﬁed since it was read in from the disk.
In this case, we must write the page to the disk.
If the modify bit is not set, however, the page has not been modiﬁed since it was read into memory.
In this case, we need not write the memory page to the disk: it is already there.
This technique also applies to read-only pages (for example, pages of binary code)
Such pages cannot be modiﬁed; thus, they may be discarded when desired.
This scheme can signiﬁcantly reduce the time required to service a page fault, since it reduces I/O time by one-half if the page has not been modiﬁed.
It completes the separation between logical memory and physical memory.
With this mechanism, an enormous virtual memory can be provided for programmers on a smaller physical memory.
With no demand paging, user addresses are mapped into physical addresses, and the two sets of addresses can be different.
All the pages of a process still must be in physical memory, however.
With demand paging, the size of the logical address space is no longer constrained by physical memory.
If we have a user process of twenty pages, we can execute it in ten frames simply by using demand paging and using a replacement algorithm to ﬁnd a free frame whenever necessary.
If a page that has been modiﬁed is to be replaced, its contents are copied to the disk.
A later reference to that page will cause a page fault.
At that time, the page will be brought back into memory, perhaps replacing some other page in the process.
We must solve two major problems to implement demand paging: we must develop a frame-allocation algorithm and a page-replacement algorithm.
That is, if we have multiple processes in memory, we must decide how many frames to allocate to each process; and when page replacement is required, we must select the frames that are to be replaced.
Designing appropriate algorithms to solve these problems is an important task, because disk I/O is so expensive.
Even slight improvements in demand-paging methods yield large gains in system performance.
How do we select a particular replacement algorithm? In general, we want the one with the lowest page-fault rate.
We evaluate an algorithm by running it on a particular string of memory references and computing the number of page faults.
The string of memory references is called a reference string.
We can generate reference strings artiﬁcially (by using a random-number generator, for example), or we can trace a given system and record the address of each memory reference.
The latter choice produces a large number of data (on the order of 1 million addresses per second)
To reduce the number of data, we use two facts.
First, for a given page size (and the page size is generally ﬁxed by the hardware or system), we need to consider only the page number, rather than the entire address.
Second, if we have a reference to a page p, then any references to page p that immediately follow will never cause a page fault.
Page p will be in memory after the ﬁrst reference, so the immediately following references will not fault.
For example, if we trace a particular process, we might record the following address sequence:
At 100 bytes per page, this sequence is reduced to the following reference string:
Figure 9.11 Graph of page faults versus number of frames.
To determine the number of page faults for a particular reference string and page-replacement algorithm, we also need to know the number of page frames available.
Obviously, as the number of frames available increases, the number of page faults decreases.
For the reference string considered previously, for example, if we had three or more frames, we would have only three faultsone fault for the ﬁrst reference to each page.
In contrast, with only one frame available, we would have a replacement with every reference, resulting in eleven faults.
In general, we expect a curve such as that in Figure 9.11
As the number of frames increases, the number of page faults drops to some minimal level.
Of course, adding physical memory increases the number of frames.
The simplest page-replacement algorithm is a ﬁrst-in, ﬁrst-out (FIFO) algorithm.
A FIFO replacement algorithm associates with each page the time when that page was brought into memory.
When a page must be replaced, the oldest page is chosen.
Notice that it is not strictly necessary to record the time when a page is brought in.
We can create a FIFO queue to hold all pages in memory.
We replace the page at the head of the queue.
When a page is brought into memory, we insert it at the tail of the queue.
For our example reference string, our three frames are initially empty.
Because of this replacement, the next reference, to 0, will.
Every time a fault occurs, we show which pages are in our three frames.
The FIFO page-replacement algorithm is easy to understand and program.
On the one hand, the page replaced may be an initialization module that was used a long time ago and is no longer needed.
On the other hand, it could contain a heavily used variable that was initialized early and is in constant use.
Notice that, even if we select for replacement a page that is in active use, everything still works correctly.
After we replace an active page with a new one, a fault occurs almost immediately to retrieve the active page.
Some other page must be replaced to bring the active page back into memory.
Thus, a bad replacement choice increases the page-fault rate and slows process execution.
To illustrate the problems that are possible with a FIFO page-replacement algorithm, consider the following reference string:
Figure 9.13 shows the curve of page faults for this reference string versus the number of available frames.
Notice that the number of faults for four frames (ten) is greater than the number of faults for three frames (nine)! This most unexpected result is known as Belady’s anomaly: for some page-replacement algorithms, the page-fault rate may increase as the number of allocated frames increases.
We would expect that giving more memory to a process would improve its performance.
In some early research, investigators noticed that this assumption was not always true.
One result of the discovery of Belady’s anomaly was the search for an optimal page-replacement algorithm—the algorithm that has the lowest page-fault rate of all algorithms and will never suffer from Belady’s anomaly.
Such an algorithm does exist and has been called OPT or MIN.
Replace the page that will not be used for the longest period of time.
Use of this page-replacement algorithm guarantees the lowest possible pagefault rate for a ﬁxed number of frames.
Figure 9.13 Page-fault curve for FIFO replacement on a reference string.
For example, on our sample reference string, the optimal page-replacement algorithm would yield nine page faults, as shown in Figure 9.14
The ﬁrst three references cause faults that ﬁll the three empty frames.
With only nine page faults, optimal replacement is much better than a FIFO algorithm, which results in ﬁfteen faults.
If we ignore the ﬁrst three, which all algorithms must suffer, then optimal replacement is twice as good as FIFO replacement.
In fact, no replacement algorithm can process this reference string in three frames with fewer than nine faults.
Unfortunately, the optimal page-replacement algorithm is difﬁcult to implement, because it requires future knowledge of the reference string.
We encountered a similar situation with the SJF CPU-scheduling algorithm in Section 6.3.2
As a result, the optimal algorithm is used mainly for comparison studies.
If the optimal algorithm is not feasible, perhaps an approximation of the optimal algorithm is possible.
The key distinction between the FIFO and OPT algorithms (other than looking backward versus forward in time) is that the FIFO algorithm uses the time when a page was brought into memory, whereas the OPT algorithm uses the time when a page is to be used.
If we use the recent past as an approximation of the near future, then we can replace the page that has not been used for the longest period of time.
When a page must be replaced, LRU chooses the page that has not been used for the longest period of time.
We can think of this strategy as the optimal page-replacement algorithm looking backward in time, rather than forward.
Strangely, if we let SR be the reverse of a reference string S, then the page-fault rate for the OPT algorithm on S is the same as the page-fault rate for the OPT algorithm on SR.
Similarly, the page-fault rate for the LRU algorithm on S is the same as the page-fault rate for the LRU algorithm on SR.
The result of applying LRU replacement to our example reference string is shown in Figure 9.15
Notice that the ﬁrst ﬁve faults are the same as those for optimal replacement.
Despite these problems, LRU replacement with twelve faults is much better than FIFO replacement with ﬁfteen.
The LRU policy is often used as a page-replacement algorithm and is considered to be good.
The problem is to determine an order for the frames deﬁned by the time of last use.
In the simplest case, we associate with each page-table entry a time-of-use ﬁeld and add to the CPU a logical clock or counter.
Whenever a reference to a page is made, the contents of the clock register are copied to the time-of-use ﬁeld in the page-table entry for that page.
This scheme requires a search of the page table to ﬁnd the LRU page and a write to memory (to the time-of-use ﬁeld in the page table) for each memory access.
The times must also be maintained when page tables are changed (due to CPU scheduling)
Another approach to implementing LRU replacement is to keep a stack of page numbers.
Whenever a page is referenced, it is removed from the stack and put on the top.
In this way, the most recently used page is always at the top of the stack and the least recently used page is always at the bottom (Figure 9.16)
Because entries must be removed from the middle of the stack, it is best to implement this approach by using a doubly linked list with a head pointer and a tail pointer.
Removing a page and putting it on the top of the stack then requires changing six pointers at worst.
Each update is a little more expensive, but there is no search for a replacement; the tail pointer points to the bottom of the stack, which is the LRU page.
This approach is particularly appropriate for software or microcode implementations of LRU replacement.
Like optimal replacement, LRU replacement does not suffer from Belady’s anomaly.
Both belong to a class of page-replacement algorithms, called stack algorithms, that can never exhibit Belady’s anomaly.
A stack algorithm is an algorithm for which it can be shown that the set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n + 1 frames.
For LRU replacement, the set of pages in memory would be the n most recently referenced pages.
If the number of frames is increased, these n pages will still be the most recently referenced and so will still be in memory.
Note that neither implementation of LRU would be conceivable without hardware assistance beyond the standard TLB registers.
The updating of the clock ﬁelds or stack must be done for every memory reference.
Figure 9.16 Use of a stack to record the most recent page references.
Few systems could tolerate that level of overhead for memory management.
Few computer systems provide sufﬁcient hardware support for true LRU page replacement.
In fact, some systems provide no hardware support, and other page-replacement algorithms (such as a FIFO algorithm) must be used.
Many systems provide some help, however, in the form of a reference bit.
The reference bit for a page is set by the hardware whenever that page is referenced (either a read or a write to any byte in the page)
Reference bits are associated with each entry in the page table.
Initially, all bits are cleared (to 0) by the operating system.
As a user process executes, the bit associated with each page referenced is set (to 1) by the hardware.
After some time, we can determine which pages have been used and which have not been used by examining the reference bits, although we do not know the order of use.
This information is the basis for many page-replacement algorithms that approximate LRU replacement.
We can gain additional ordering information by recording the reference bits at regular intervals.
We can keep an 8-bit byte for each page in a table in memory.
At regular intervals (say, every 100 milliseconds), a timer interrupt transfers control to the operating system.
These 8-bit shift registers contain the history of page use for the last eight time periods.
If the shift register contains 00000000, for example, then the page has not been used for eight time periods.
If we interpret these 8-bit bytes as unsigned integers, the page with the lowest number is the LRU page, and it can be replaced.
Notice that the numbers are not guaranteed to be unique, however.
We can either replace (swap out) all pages with the smallest value or use the FIFO method to choose among them.
The number of bits of history included in the shift register can be varied, of course, and is selected (depending on the hardware available) to make the updating as fast as possible.
In the extreme case, the number can be reduced to zero, leaving only the reference bit itself.
The basic algorithm of second-chance replacement is a FIFO replacement algorithm.
When a page has been selected, however, we inspect its reference bit.
When a page gets a second chance, its reference bit is cleared, and its arrival time is reset to the current time.
In addition, if a page is used often enough to keep its reference bit set, it will never be replaced.
One way to implement the second-chance algorithm (sometimes referred to as the clock algorithm) is as a circular queue.
A pointer (that is, a hand on the clock) indicates which page is to be replaced next.
When a frame is needed, the pointer advances until it ﬁnds a page with a 0 reference bit.
As it advances, it clears the reference bits (Figure 9.17)
Once a victim page is found, the page is replaced, and the new page is inserted in the circular queue in that position.
Notice that, in the worst case, when all bits are set, the pointer cycles through the whole queue, giving each page a second chance.
It clears all the reference bits before selecting the next page for replacement.
Second-chance replacement degenerates to FIFO replacement if all bits are set.
We can enhance the second-chance algorithm by considering the reference bit and the modify bit (described in Section 9.4.1) as an ordered pair.
With these two bits, we have the following four possible classes:
When page replacement is called for, we use the same scheme as in the clock algorithm; but instead of examining whether the page to which we are pointing has the reference bit set to 1, we examine the class to which that page belongs.
We replace the ﬁrst page encountered in the lowest nonempty class.
Notice that we may have to scan the circular queue several times before we ﬁnd a page to be replaced.
The major difference between this algorithm and the simpler clock algorithm is that here we give preference to those pages that have been modiﬁed in order to reduce the number of I/Os required.
There are many other algorithms that can be used for page replacement.
For example, we can keep a counter of the number of references that have been made to each page and develop the following two schemes.
The least frequently used (LFU) page-replacement algorithm requires that the page with the smallest count be replaced.
The reason for this selection is that an actively used page should have a large reference count.
A problem arises, however, when a page is used heavily during the initial phase of a process but then is never used again.
Since it was used heavily, it has a large count and remains in memory even though it is no longer needed.
One solution is to shift the counts right by 1 bit at regular intervals, forming an exponentially decaying average usage count.
The most frequently used (MFU) page-replacement algorithm is based on the argument that the page with the smallest count was probably just brought in and has yet to be used.
As you might expect, neither MFU nor LFU replacement is common.
The implementation of these algorithms is expensive, and they do not approximate OPT replacement well.
Other procedures are often used in addition to a speciﬁc page-replacement algorithm.
For example, systems commonly keep a pool of free frames.
When a page fault occurs, a victim frame is chosen as before.
However, the desired page is read into a free frame from the pool before the victim is written out.
This procedure allows the process to restart as soon as possible, without waiting for the victim page to be written out.
When the victim is later written out, its frame is added to the free-frame pool.
An expansion of this idea is to maintain a list of modiﬁed pages.
Whenever the paging device is idle, a modiﬁed page is selected and is written to the disk.
This scheme increases the probability that a page will be clean when it is selected for replacement and will not need to be written out.
Another modiﬁcation is to keep a pool of free frames but to remember which page was in each frame.
Since the frame contents are not modiﬁed when a frame is written to the disk, the old page can be reused directly from the free-frame pool if it is needed before that frame is reused.
When a page fault occurs, we ﬁrst check whether the desired page is in the free-frame pool.
If it is not, we must select a free frame and read into it.
This technique is used in the VAX/VMS system along with a FIFO replacement algorithm.
When the FIFO replacement algorithm mistakenly replaces a page that is still in active use, that page is quickly retrieved from the free-frame pool, and no I/O is necessary.
The free-frame buffer provides protection against the relatively poor, but simple, FIFO replacement algorithm.
This method is necessary because the early versions of VAX did not implement the reference bit correctly.
Some versions of the UNIX system use this method in conjunction with the second-chance algorithm.
It can be a useful augmentation to any pagereplacement algorithm, to reduce the penalty incurred if the wrong victim page is selected.
In certain cases, applications accessing data through the operating system’s virtual memory perform worse than if the operating system provided no buffering at all.
A typical example is a database, which provides its own memory management and I/O buffering.
Applications like this understand their memory use and disk use better than does an operating system that is implementing algorithms for general-purpose use.
If the operating system is buffering I/O and the application is doing so as well, however, then twice the memory is being used for a set of I/O.
In another example, data warehouses frequently perform massive sequential disk reads, followed by computations and writes.
The LRU algorithm would be removing old pages and preserving new ones, while the application would more likely be reading older pages than newer ones (as it starts its sequential reads again)
Because of such problems, some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks, without any ﬁle-system data structures.
This array is sometimes called the raw disk, and I/O to this array is termed raw I/O.
Raw I/O bypasses all the ﬁlesystem services, such as ﬁle I/O demand paging, ﬁle locking, prefetching, space allocation, ﬁle names, and directories.
Note that although certain applications are more efﬁcient when implementing their own special-purpose storage services on a raw partition, most applications perform better when they use the regular ﬁle-system services.
How do we allocate the ﬁxed amount of free memory among the various processes? If we have 93 free frames and two processes, how many frames does each process get?
Under pure demand paging, all 93 frames would initially be put on the free-frame list.
When a user process started execution, it would generate a sequence of page faults.
The ﬁrst 93 page faults would all get free frames from the free-frame list.
When the process terminated, the 93 frames would once again be placed on the free-frame list.
We can require that the operating system allocate all its buffer and table space from the free-frame list.
When this space is not in use by the operating system, it can be used to support user paging.
We can try to keep three free frames reserved on the free-frame list at all times.
Thus, when a page fault occurs, there is a free frame available to page into.
While the page swap is taking place, a replacement can be selected, which is then written to the disk as the user process continues to execute.
Other variants are also possible, but the basic strategy is clear: the user process is allocated any free frame.
Our strategies for the allocation of frames are constrained in various ways.
We cannot, for example, allocate more than the total number of available frames (unless there is page sharing)
We must also allocate at least a minimum number of frames.
One reason for allocating at least a minimum number of frames involves performance.
Obviously, as the number of frames allocated to each process decreases, the page-fault rate increases, slowing process execution.
In addition, remember that, when a page fault occurs before an executing instruction is complete, the instruction must be restarted.
Consequently, we must have enough frames to hold all the different pages that any single instruction can reference.
For example, consider a machine in which all memory-reference instructions may reference only one memory address.
In this case, we need at least one frame for the instruction and one frame for the memory reference.
Think about what might happen if a process had only two frames.
The minimum number of frames is deﬁned by the computer architecture.
For example, the move instruction for the PDP-11 includes more than one word for some addressing modes, and thus the instruction itself may straddle two pages.
In addition, each of its two operands may be indirect references, for a total of six frames.
Since the instruction is from storage location to storage location, it takes 6 bytes and can straddle two pages.
The block of characters to move and the area to which it is to be moved can each also straddle two pages.
The worst case occurs when the MVC instruction is the operand of an EXECUTE instruction that straddles a page boundary; in this case, we need eight frames.
Theoretically, a simple load instruction could reference an indirect address that could reference an indirect address (on another page) that could also reference an indirect address (on yet another page), and so on, until every page in virtual memory had been touched.
Thus, in the worst case, the entire virtual memory must be in physical memory.
To overcome this difﬁculty, we must place a limit on the levels of indirection (for example, limit an instruction to at most 16 levels of indirection)
When the ﬁrst indirection occurs, a counter is set to 16; the counter is then decremented for each successive indirection for this instruction.
If the counter is decremented to 0, a trap occurs (excessive indirection)
This limitation reduces the maximum number of memory references per instruction to 17, requiring the same number of frames.
Whereas the minimum number of frames per process is deﬁned by the architecture, the maximum number is deﬁned by the amount of available physical memory.
In between, we are still left with signiﬁcant choice in frame allocation.
The easiest way to split m frames among n processes is to give everyone an equal share, m/n frames (ignoring frames needed by the operating system for the moment)
The three leftover frames can be used as a free-frame buffer pool.
An alternative is to recognize that various processes will need differing amounts of memory.
To solve this problem, we can use proportional allocation, in which we allocate available memory to each process according to its size.
Let the size of the virtual memory for process pi be si , and deﬁne.
Then, if the total number of available frames is m, we allocate ai frames to process pi , where ai is approximately.
In this way, both processes share the available frames according to their “needs,” rather than equally.
In both equal and proportional allocation, of course, the allocation may vary according to the multiprogramming level.
If the multiprogramming level is increased, each process will lose some frames to provide the memory needed for the new process.
Conversely, if the multiprogramming level decreases, the frames that were allocated to the departed process can be spread over the remaining processes.
Notice that, with either equal or proportional allocation, a high-priority process is treated the same as a low-priority process.
By its deﬁnition, however, we may want to give the high-priority process more memory to speed its execution, to the detriment of low-priority processes.
One solution is to use a proportional allocation scheme wherein the ratio of frames depends not on the relative sizes of processes but rather on the priorities of processes or on a combination of size and priority.
Another important factor in the way frames are allocated to the various processes is page replacement.
With multiple processes competing for frames, we can classify page-replacement algorithms into two broad categories: global replacement and local replacement.
Global replacement allows a process to select a replacement frame from the set of all frames, even if that frame is currently allocated to some other process; that is, one process can take a frame from another.
Local replacement requires that each process select from only its own set of allocated frames.
For example, consider an allocation scheme wherein we allow high-priority processes to select frames from low-priority processes for replacement.
A process can select a replacement from among its own frames or the frames of any lower-priority process.
This approach allows a high-priority process to increase its frame allocation at the expense of a low-priority process.
With a local replacement strategy, the number of frames allocated to a process does not change.
With global replacement, a process may happen to select only frames allocated to other processes, thus increasing the number of frames allocated to it (assuming that other processes do not choose its frames for replacement)
One problem with a global replacement algorithm is that a process cannot control its own page-fault rate.
The set of pages in memory for a process depends not only on the paging behavior of that process but also on the paging behavior of other processes.
Such is not the case with a local replacement algorithm.
Under local replacement, the set of pages in memory for a process is affected by the paging behavior of only that process.
Local replacement might hinder a process, however, by not making available to it other, less used pages of memory.
Thus, global replacement generally results in greater system throughput and is therefore the more commonly used method.
Thus far in our coverage of virtual memory, we have assumed that all main memory is created equal—or at least that it is accessed equally.
Often, in systems with multiple CPUs (Section 1.3.2), a given CPU can access some sections of main memory faster than it can access others.
These performance differences are caused by how CPUs and memory are interconnected in the system.
Frequently, such a system is made up of several system boards, each containing multiple CPUs and some memory.
The system boards are interconnected in various ways, ranging from system buses to high-speed network connections like InﬁniBand.
As you might expect, the CPUs on a particular board can access the memory on that board with less delay than they can access memory on other boards in the system.
Systems in which memory access times vary signiﬁcantly are known collectively as non-uniform memory access (NUMA) systems, and without exception, they are slower than systems in which memory and CPUs are located on the same motherboard.
Managing which page frames are stored at which locations can signiﬁcantly affect performance in NUMA systems.
If we treat memory as uniform in such a system, CPUs may wait signiﬁcantly longer for memory access than if we modify memory allocation algorithms to take NUMA into account.
The goal of these changes is to have memory frames allocated “as close as possible” to the CPU on which the process is running.
The deﬁnition of “close” is “with minimum latency,” which typically means on the same system board as the CPU.
The algorithmic changes consist of having the scheduler track the last CPU on which each process ran.
If the scheduler tries to schedule each process onto its previous CPU, and the memory-management system tries to allocate frames for the process close to the CPU on which it is being scheduled, then improved cache hits and decreased memory access times will result.
For example, a process with many running threads may end up with those threads scheduled on many different system boards.
How is the memory to be allocated in this case? Solaris solves the problem by creating lgroups (for “latency groups”) in the kernel.
In fact, there is a hierarchy of lgroups based on the amount of latency between the groups.
Solaris tries to schedule all threads of a process and allocate all memory of a process within an lgroup.
If that is not possible, it picks nearby lgroups for the rest of the resources needed.
This practice minimizes overall memory latency and maximizes CPU cache hit rates.
If the number of frames allocated to a low-priority process falls below the minimum number required by the computer architecture, we must suspend that process’s execution.
We should then page out its remaining pages, freeing all its allocated frames.
This provision introduces a swap-in, swap-out level of intermediate CPU scheduling.
In fact, look at any process that does not have “enough” frames.
If the process does not have the number of frames it needs to support pages in active use, it will quickly page-fault.
However, since all its pages are in active use, it must replace a page that will be needed again right away.
Consequently, it quickly faults again, and again, and again, replacing pages that it must bring back in immediately.
A process is thrashing if it is spending more time paging than executing.
Consider the following scenario, which is based on the actual behavior of early paging systems.
If CPU utilization is too low, we increase the degree of multiprogramming by introducing a new process to the system.
A global page-replacement algorithm is used; it replaces pages without regard to the process to which they belong.
Now suppose that a process enters a new phase in its execution and needs more frames.
It starts faulting and taking frames away from other processes.
These processes need those pages, however, and so they also fault, taking frames from other processes.
These faulting processes must use the paging device to swap pages in and out.
As they queue up for the paging device, the ready queue empties.
As processes wait for the paging device, CPU utilization decreases.
The CPU scheduler sees the decreasing CPU utilization and increases the degree of multiprogramming as a result.
The new process tries to get started by taking frames from running processes, causing more page faults and a longer queue for the paging device.
As a result, CPU utilization drops even further, and the CPU scheduler tries to increase the degree of multiprogramming even more.
No work is getting done, because the processes are spending all their time paging.
This phenomenon is illustrated in Figure 9.18, in which CPU utilization is plotted against the degree of multiprogramming.
As the degree of multiprogramming increases, CPU utilization also increases, although more slowly, until a maximum is reached.
If the degree of multiprogramming is increased even further, thrashing sets in, and CPU utilization drops sharply.
At this point, to increase CPU utilization and stop thrashing, we must decrease the degree of multiprogramming.
We can limit the effects of thrashing by using a local replacement algorithm (or priority replacement algorithm)
With local replacement, if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash as well.
If processes are thrashing, they will be in the queue for the paging device most of the time.
The average service time for a page fault will increase because of the longer average queue for the paging device.
Thus, the effective access time will increase even for a process that is not thrashing.
To prevent thrashing, we must provide a process with as many frames as it needs.
But how do we know how many frames it “needs”? There are several techniques.
The working-set strategy (Section 9.6.2) starts by looking at how many frames a process is actually using.
The locality model states that, as a process executes, it moves from locality to locality.
A locality is a set of pages that are actively used together (Figure 9.19)
A program is generally composed of several different localities, which may overlap.
For example, when a function is called, it deﬁnes a new locality.
In this locality, memory references are made to the instructions of the function call, its local variables, and a subset of the global variables.
When we exit the function, the process leaves this locality, since the local variables and instructions of the function are no longer in active use.
Thus, we see that localities are deﬁned by the program structure and its data structures.
The locality model states that all programs will exhibit this basic memory reference structure.
Note that the locality model is the unstated principle behind the caching discussions so far in this book.
If accesses to any types of data were random rather than patterned, caching would be useless.
Suppose we allocate enough frames to a process to accommodate its current locality.
It will fault for the pages in its locality until all these pages are in memory; then, it will not fault again until it changes localities.
If we do not allocate enough frames to accommodate the size of the current locality, the process will thrash, since it cannot keep in memory all the pages that it is actively using.
The most important property of the working set, then, is its size.
If we compute the working-set size, WSSi , for each process in the system, we can then consider that.
Each process is actively using the pages in its working set.
If the total demand is greater than the total number of available frames (D> m), thrashing will occur, because some processes will not have enough frames.
If there are enough extra frames, another process can be initiated.
If the sum of the working-set sizes increases, exceeding the total number of available frames, the operating system selects a process to suspend.
The process’s pages are written out (swapped), and its frames are reallocated to other processes.
This working-set strategy prevents thrashing while keeping the degree of multiprogramming as high as possible.
The difﬁculty with the working-set model is keeping track of the working set.
At each memory reference, a new reference appears at one end, and the oldest reference drops off the other end.
A page is in the working set if it is referenced anywhere in the working-set window.
Note that this arrangement is not entirely accurate, because we cannot tell where, within an interval of 5,000, a reference occurred.
However, the cost to service these more frequent interrupts will be correspondingly higher.
The working-set model is successful, and knowledge of the working set can be useful for prepaging (Section 9.9.1), but it seems a clumsy way to control thrashing.
A strategy that uses the page-fault frequency (PFF) takes a more direct approach.
When it is too high, we know that the process needs more frames.
Conversely, if the page-fault rate is too low, then the process may have too many frames.
We can establish upper and lower bounds on the desired page-fault rate (Figure 9.21)
If the actual page-fault rate exceeds the upper limit, we allocate the process another.
If the page-fault rate falls below the lower limit, we remove a frame from the process.
Thus, we can directly measure and control the page-fault rate to prevent thrashing.
As with the working-set strategy, we may have to swap out a process.
If the page-fault rate increases and no free frames are available, we must select some process and swap it out to backing store.
The freed frames are then distributed to processes with high page-fault rates.
Practically speaking, thrashing and the resulting swapping have a disagreeably large impact on performance.
The current best practice in implementing a computer facility is to include enough physical memory, whenever possible, to avoid thrashing and swapping.
From smartphones through mainframes, providing enough memory to keep all working sets in memory concurrently, except under extreme conditions, gives the best user experience.
Consider a sequential read of a ﬁle on disk using the standard system calls open(), read(), and write()
Each ﬁle access requires a system call and disk access.
Alternatively, we can use the virtual memory techniques discussed so far to treat ﬁle I/O as routine memory accesses.
This approach, known as memory mapping a ﬁle, allows a part of the virtual address space to be logically associated with the ﬁle.
As we shall see, this can lead to signiﬁcant performance increases.
Memory mapping a ﬁle is accomplished by mapping a disk block to a page (or pages) in memory.
Initial access to the ﬁle proceeds through ordinary demand paging, resulting in a page fault.
There is a direct relationship between the working set of a process and its page-fault rate.
Typically, as shown in Figure 9.20, the working set of a process changes over time as references to data and code sections move from one locality to another.
Assuming there is sufﬁcient memory to store the working set of a process (that is, the process is not thrashing), the page-fault rate of the process will transition between peaks and valleys over time.
A peak in the page-fault rate occurs when we begin demand-paging a new locality.
However, once the working set of this new locality is in memory, the page-fault rate falls.
When the process moves to a new working set, the page-fault rate rises toward a peak once again, returning to a lower rate once the new working set is loaded into memory.
The span of time between the start of one peak and the start of the next peak represents the transition from one working set to another.
Subsequent reads and writes to the ﬁle are handled as routine memory accesses.
Manipulating ﬁles through memory rather than incurring the overhead of using the read() and write() system calls simpliﬁes and speeds up ﬁle access and usage.
Note that writes to the ﬁle mapped in memory are not necessarily immediate (synchronous) writes to the ﬁle on disk.
Some systems may choose to update the physical ﬁle when the operating system periodically checks whether the page in memory has been modiﬁed.
When the ﬁle is closed, all the memory-mapped data are written back to disk and removed from the virtual memory of the process.
Some operating systems provide memory mapping only through a speciﬁc system call and use the standard system calls to perform all other ﬁle I/O.
However, some systems choose to memory-map a ﬁle regardless of whether the ﬁle was speciﬁed as memory-mapped.
If a ﬁle is speciﬁed as memory-mapped (using the mmap() system call), Solaris maps the ﬁle into the address space of the process.
Solaris still memory-maps the ﬁle; however, the ﬁle is mapped to the kernel address space.
Regardless of how the ﬁle is opened, then, Solaris treats all ﬁle I/O as memory-mapped, allowing ﬁle access to take place via the efﬁcient memory subsystem.
Multiple processes may be allowed to map the same ﬁle concurrently, to allow sharing of data.
Writes by any of the processes modify the data in virtual memory and can be seen by all others that map the same section of the ﬁle.
Given our earlier discussions of virtual memory, it should be clear how the sharing of memory-mapped sections of memory is implemented: the virtual memory map of each sharing process points to the same page of physical memory—the page that holds a copy of the disk block.
The memory-mapping system calls can also support copy-on-write functionality, allowing processes to share a ﬁle in read-only mode but to have their own copies of any data they modify.
Quite often, shared memory is in fact implemented by memory mapping ﬁles.
Under this scenario, processes can communicate using shared memory by having the communicating processes memory-map the same ﬁle into their virtual address spaces.
The memory-mapped ﬁle serves as the region of shared memory between the communicating processes (Figure 9.23)
We have already seen this in Section 3.4.1, where a POSIX shared memory object is created and each communicating process memory-maps the object into its address space.
In the following section, we illustrate support in the Windows API for shared memory using memory-mapped ﬁles.
The general outline for creating a region of shared memory using memorymapped ﬁles in the Windows API involves ﬁrst creating a ﬁle mapping for the ﬁle to be mapped and then establishing a view of the mapped ﬁle in a process’s virtual address space.
A second process can then open and create a view of the mapped ﬁle in its virtual address space.
The mapped ﬁle represents the shared-memory object that will enable communication to take place between the processes.
In this example, a producer process ﬁrst creates a shared-memory object using the memory-mapping features available in the Windows API.
After that, a consumer process opens a mapping to the shared-memory object and reads the message written by the consumer.
To establish a memory-mapped ﬁle, a process ﬁrst opens the ﬁle to be mapped with the CreateFile() function, which returns a HANDLE to the opened ﬁle.
Once the ﬁle mapping is established, the process then establishes a view of the mapped ﬁle in its virtual address space with the MapViewOfFile() function.
The view of the mapped ﬁle represents the portion of the ﬁle being mapped in the virtual address space of the process —the entire ﬁle or only a portion of it may be mapped.
We illustrate this sequence in the program shown in Figure 9.24
We eliminate much of the error checking for code brevity.
The consumer process will communicate using this shared-memory segment by creating a mapping to the same named object.
The producer then creates a view of the memory-mapped ﬁle in its virtual address space.
By passing the last three parameters the value 0, it indicates that the mapped view is the entire ﬁle.
It could instead have passed values specifying an offset and size, thus creating a view containing only a subsection of the ﬁle.
It is important to note that the entire mapping may not be loaded into memory when the mapping is established.
Rather, the mapped ﬁle may be demand-paged, thus bringing pages into memory only as they are accessed.
The MapViewOfFile() function returns a pointer to the shared-memory object; any accesses to this memory location are thus accesses to the memory-mapped.
In this instance, the producer process writes the message “Shared memory message” to shared memory.
A program illustrating how the consumer process establishes a view of the named shared-memory object is shown in Figure 9.25
This program is somewhat simpler than the one shown in Figure 9.24, as all that is necessary is for the process to create a mapping to the existing named shared-memory object.
The consumer process must also create a view of the mapped ﬁle, just as the producer process did in the program in Figure 9.24
The consumer then reads from shared memory the message “Shared memory message” that was written by the producer process.
Finally, both processes remove the view of the mapped ﬁle with a call to UnmapViewOfFile()
We provide a programming exercise at the end of this chapter using shared memory with memory mapping in the Windows API.
In the case of I/O, as mentioned in Section 1.2.1, each I/O controller includes registers to hold commands and the data being transferred.
Usually, special I/O instructions allow data transfers between these registers and system memory.
To allow more convenient access to I/O devices, many computer architectures provide memory-mapped I/O.
In this case, ranges of memory addresses are set aside and are mapped to the device registers.
Reads and writes to these memory addresses cause the data to be transferred to and from the device registers.
This method is appropriate for devices that have fast response times, such as video controllers.
In the IBM PC, each location on the screen is mapped to a memory location.
Displaying text on the screen is almost as easy as writing the text into the appropriate memory-mapped locations.
Memory-mapped I/O is also convenient for other devices, such as the serial and parallel ports used to connect modems and printers to a computer.
The CPU transfers data through these kinds of devices by reading and writing a few device registers, called an I/O port.
To send out a long string of bytes through a memory-mapped serial port, the CPU writes one data byte to the data register and sets a bit in the control register to signal that the byte is available.
If the CPU uses polling to watch the control bit, constantly looping to see whether the device is ready, this method of operation is called programmed I/O (PIO)
If the CPU does not poll the control bit, but instead receives an interrupt when the device is ready for the next byte, the data transfer is said to be interrupt driven.
When a process running in user mode requests additional memory, pages are allocated from the list of free page frames maintained by the kernel.
This list is typically populated using a page-replacement algorithm such as those discussed in Section 9.4 and most likely contains free pages scattered throughout physical memory, as explained earlier.
Remember, too, that if a user process requests a single byte of memory, internal fragmentation will result, as the process will be granted an entire page frame.
Kernel memory is often allocated from a free-memory pool different from the list used to satisfy ordinary user-mode processes.
The kernel requests memory for data structures of varying sizes, some of which are less than a page in size.
As a result, the kernel must use memory conservatively and attempt to minimize waste due to fragmentation.
This is especially important because many operating systems do not subject kernel code or data to the paging system.
Pages allocated to user-mode processes do not necessarily have to be in contiguous physical memory.
However, certain hardware devices interact directly with physical memory—without the beneﬁt of a virtual memory interface—and consequently may require memory residing in physically contiguous pages.
In the following sections, we examine two strategies for managing free memory that is assigned to kernel processes: the “buddy system” and slab allocation.
The buddy system allocates memory from a ﬁxed-size segment consisting of physically contiguous pages.
The segment is initially divided into two buddies—which we will call AL and AR —each 128 KB in size.
One of these buddies is further divided into two 64-KB buddiesBL and BR.
An advantage of the buddy system is how quickly adjacent buddies can be combined to form larger segments using a technique known as coalescing.
This segment, BL , can in turn be coalesced with its buddy BR to form a 128-KB segment.
Ultimately, we can end up with the original 256-KB segment.
The obvious drawback to the buddy system is that rounding up to the next highest power of 2 is very likely to cause fragmentation within allocated segments.
In fact, we cannot guarantee that less than 50 percent of the allocated unit will be wasted due to internal fragmentation.
In the following section, we explore a memory allocation scheme where no space is lost due to fragmentation.
A second strategy for allocating kernel memory is known as slab allocation.
A slab is made up of one or more physically contiguous pages.
There is a single cache for each unique kernel data structure —for example, a separate cache for the data structure representing process descriptors, a separate cache for ﬁle objects, a separate cache for semaphores, and so forth.
Each cache is populated with objects that are instantiations of the kernel data structure the cache represents.
For example, the cache representing semaphores stores instances of semaphore objects, the cache representing process descriptors stores instances of process descriptor objects, and so forth.
The relationship among slabs, caches, and objects is shown in Figure 9.27
When a cache is created, a number of objects—which are initially marked as free—are allocated to the cache.
The number of objects in the cache depends on the size of the associated slab.
Initially, all objects in the cache are marked as free.
When a new object for a kernel data structure is needed, the allocator can assign any free object from the cache to satisfy the request.
The object assigned from the cache is marked as used.
Let’s consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor.
In Linux systems, a process descriptor is of the type struct task struct, which requires approximately 1.7 KB of memory.
When the Linux kernel creates a new task, it requests the necessary memory for the struct task struct object from its cache.
The cache will fulﬁll the request using a struct task struct object that has already been allocated in a slab and is marked as free.
In Linux, a slab may be in one of three possible states:
The slab allocator ﬁrst attempts to satisfy the request with a free object in a partial slab.
If none exists, a free object is assigned from an empty slab.
If no empty slabs are available, a new slab is allocated from contiguous physical pages and assigned to a cache; memory for the object is allocated from this slab.
Fragmentation is not an issue because each unique kernel data structure has an associated cache, and each cache is made up of one or more slabs that are divided into.
Thus, when the kernel requests memory for an object, the slab allocator returns the exact amount of memory required to represent the object.
The slab allocation scheme is thus particularly effective for managing memory when objects are frequently allocated and deallocated, as is often the case with requests from the kernel.
The act of allocating—and releasing—memory can be a time-consuming process.
However, objects are created in advance and thus can be quickly allocated from the cache.
Furthermore, when the kernel has ﬁnished with an object and releases it, it is marked as free and returned to its cache, thus making it immediately available for subsequent requests from the kernel.
The slab allocator ﬁrst appeared in the Solaris 2.4 kernel.
Because of its general-purpose nature, this allocator is now also used for certain user-mode memory requests in Solaris.
Linux originally used the buddy system; however, beginning with Version 2.2, the Linux kernel adopted the slab allocator.
Recent distributions of Linux now include two other kernel memory allocators—the SLOB and SLUB allocators.
The SLOB allocator is designed for systems with a limited amount of memory, such as embedded systems.
Memory requests are allocated from an object on an appropriately sized list using a ﬁrst-ﬁt policy.
Beginning with Version 2.6.24, the SLUB allocator replaced SLAB as the default allocator for the Linux kernel.
One change is to move the metadata that is stored with each slab under SLAB allocation to the page structure the Linux kernel uses for each page.
Additionally, SLUB removes the per-CPU queues that the SLAB allocator maintains for objects in each cache.
For systems with a large number of processors, the amount of memory allocated to these queues was not insigniﬁcant.
Thus, SLUB provides better performance as the number of processors on a system increases.
The major decisions that we make for a paging system are the selections of a replacement algorithm and an allocation policy, which we discussed earlier in this chapter.
There are many other considerations as well, and we discuss several of them here.
An obvious property of pure demand paging is the large number of page faults that occur when a process is started.
This situation results from trying to get the initial locality into memory.
Prepaging is an attempt to prevent this high level of initial paging.
The strategy is to bring into memory at one time all the pages that will be needed.
Some operating systems—notably Solaris—prepage the page frames for small ﬁles.
In a system using the working-set model, for example, we could keep with each process a list of the pages in its working set.
If we must suspend a process (due to an I/O wait or a lack of free frames), we remember the working set for that process.
When the process is to be resumed (because I/O has ﬁnished or enough free frames have become available), we automatically bring back into memory its entire working set before restarting the process.
The question is simply whether the cost of using prepaging is less than the cost of servicing the corresponding page faults.
It may well be the case that many of the pages brought back into memory by prepaging will not be used.
The designers of an operating system for an existing machine seldom have a choice concerning the page size.
However, when new machines are being designed, a decision regarding the best page size must be made.
As you might expect, there is no single best page size.
Rather, there is a set of factors that support various sizes.
How do we select a page size? One concern is the size of the page table.
For a given virtual memory space, decreasing the page size increases the number of pages and hence the size of the page table.
Because each active process must have its own copy of the page table, a large page size is desirable.
If a process is allocated memory starting at location 00000 and continuing until it has as much as it needs, it probably will not end exactly on a page boundary.
Thus, a part of the ﬁnal page must be allocated (because pages are the units of allocation) but will be unused (creating internal fragmentation)
Assuming independence of process size and page size, we can expect that, on the average, half of the ﬁnal page of each process will be wasted.
To minimize internal fragmentation, then, we need a small page size.
Another problem is the time required to read or write a page.
I/O time is composed of seek, latency, and transfer times.
Transfer time is proportional to the amount transferred (that is, the page size)—a fact that would seem to argue for a small page size.
However, as we shall see in Section 10.1.1, latency and seek time normally dwarf transfer time.
Doubling the page size increases I/O time to only 28.4 milliseconds.
Thus, a desire to minimize I/O time argues for a larger page size.
With a smaller page size, though, total I/O should be reduced, since locality will be improved.
A smaller page size allows each page to match program locality more accurately.
If we have only one large page, we must bring in the entire page, a total of 200 KB transferred and allocated.
With a smaller page size, then, we have better resolution, allowing us to isolate only the memory that is actually needed.
With a larger page size, we must allocate and transfer not only what is needed but also anything else that happens to be in the page, whether it is needed or not.
Thus, a smaller page size should result in less I/O and less total allocated memory.
Each page fault generates the large amount of overhead needed for processing the interrupt, saving registers, replacing a page, queueing for the paging device, and updating tables.
To minimize the number of page faults, we need to have a large page size.
Other factors must be considered as well (such as the relationship between page size and sector size on the paging device)
As we have seen, some factors (internal fragmentation, locality) argue for a small page size, whereas others (table size, I/O time) argue for a large page size.
Nevertheless, the historical trend is toward larger page sizes, even for mobile systems.
Modern systems may now use much larger page sizes, as we will see in the following section.
In Chapter 8, we introduced the hit ratio of the TLB.
Recall that the hit ratio for the TLB refers to the percentage of virtual address translations that are resolved in the TLB rather than the page table.
Clearly, the hit ratio is related to the number of entries in the TLB, and the way to increase the hit ratio is by increasing the number of entries in the TLB.
This, however, does not come cheaply, as the associative memory used to construct the TLB is both expensive and power hungry.
Related to the hit ratio is a similar metric: the TLB reach.
The TLB reach refers to the amount of memory accessible from the TLB and is simply the number of entries multiplied by the page size.
Ideally, the working set for a process is stored in the TLB.
If it is not, the process will spend a considerable amount of time resolving memory references in the page table rather than the TLB.
If we double the number of entries in the TLB, we double the TLB reach.
Another approach for increasing the TLB reach is to either increase the size of the page or provide multiple page sizes.
However, this may lead to an increase in fragmentation for some applications that do not require such a large page size.
Alternatively, an operating system may provide several different page sizes.
Solaris also allows applications —such as databases—to take advantage of the large 4-MB page size.
Providing support for multiple page sizes requires the operating system —not hardware—to manage the TLB.
For example, one of the ﬁelds in a TLB entry must indicate the size of the page frame corresponding to the TLB entry.
Managing the TLB in software and not hardware comes at a cost in performance.
However, the increased hit ratio and TLB reach offset the performance costs.
Indeed, recent trends indicate a move toward software-managed TLBs and operating-system support for multiple page sizes.
Section 8.6.3 introduced the concept of the inverted page table.
Because they keep information about which virtual memory page is stored in each physical frame, inverted page tables reduce the amount of physical memory needed to store this information.
However, the inverted page table no longer contains complete information about the logical address space of a process, and that information is required if a referenced page is not currently in memory.
For the information to be available, an external page table (one per process) must be kept.
Each such table looks like the traditional per-process page table and contains information on where each virtual page is located.
But do external page tables negate the utility of inverted page tables? Since these tables are referenced only when a page fault occurs, they do not need to be available quickly.
Instead, they are themselves paged in and out of memory as necessary.
Unfortunately, a page fault may now cause the virtual memory manager to generate another page fault as it pages in the external page table it needs to locate the virtual page on the backing store.
This special case requires careful handling in the kernel and a delay in the page-lookup processing.
Demand paging is designed to be transparent to the user program.
In many cases, the user is completely unaware of the paged nature of memory.
Careful selection of data structures and programming structures can increase locality and hence lower the page-fault rate and the number of pages in the working set.
For example, a stack has good locality, since access is always made to the top.
A hash table, in contrast, is designed to scatter references, producing bad locality.
Of course, locality of reference is just one measure of the efﬁciency of the use of a data structure.
Other heavily weighted factors include search speed, total number of memory references, and total number of pages touched.
At a later stage, the compiler and loader can have a signiﬁcant effect on paging.
Separating code and data and generating reentrant code means that code pages can be read-only and hence will never be modiﬁed.
Clean pages do not have to be paged out to be replaced.
The loader can avoid placing routines across page boundaries, keeping each routine completely in one page.
Routines that call each other many times can be packed into the same page.
This packaging is a variant of the bin-packing problem of operations research: try to pack the variable-sized load segments into the ﬁxed-sized pages so that interpage references are minimized.
Such an approach is particularly useful for large page sizes.
When demand paging is used, we sometimes need to allow some of the pages to be locked in memory.
One such situation occurs when I/O is done to or from user (virtual) memory.
For example, a controller for a USB storage device is generally given the number of bytes to transfer and a memory address for the buffer (Figure 9.28)
We must be sure the following sequence of events does not occur: A process issues an I/O request and is put in a queue for that I/O device.
These processes cause page faults, and one of them, using a global replacement algorithm, replaces the page containing the memory buffer for the waiting process.
Some time later, when the I/O request advances to the head of the device queue, the I/O occurs to the speciﬁed address.
However, this frame is now being used for a different page belonging to another process.
One solution is never to execute I/O to user memory.
Instead, data are always copied between system memory and user memory.
I/O takes place only between system memory and the I/O device.
To write a block on tape, we ﬁrst copy the block to system memory and then write it to tape.
Another solution is to allow pages to be locked into memory.
If the frame is locked, it cannot be selected for replacement.
Under this approach, to write a block on tape, we lock into memory the pages containing the block.
Figure 9.28 The reason why frames used for I/O must be in memory.
Frequently, some or all of the operating-system kernel is locked into memory.
Many operating systems cannot tolerate a page fault caused by the kernel or by a speciﬁc kernel module, including the one performing memory management.
User processes may also need to lock pages into memory.
A database process may want to manage a chunk of memory, for example, moving blocks between disk and memory itself because it has the best knowledge of how it is going to use its data.
Such pinning of pages in memory is fairly common, and most operating systems have a system call allowing an application to request that a region of its logical address space be pinned.
Note that this feature could be abused and could cause stress on the memory-management algorithms.
Therefore, an application frequently requires special privileges to make such a request.
Another use for a lock bit involves normal page replacement.
Consider the following sequence of events: A low-priority process faults.
Selecting a replacement frame, the paging system reads the necessary page into memory.
Ready to continue, the low-priority process enters the ready queue and waits for the CPU.
Since it is a low-priority process, it may not be selected by the CPU scheduler for a time.
Looking for a replacement, the paging system sees a page that is in memory but has not been referenced or modiﬁed: it is the page that the low-priority process just brought in.
This page looks like a perfect replacement: it is clean and will not need to be written out, and it apparently has not been used for a long time.
Whether the high-priority process should be able to replace the low-priority process is a policy decision.
After all, we are simply delaying the low-priority process for the beneﬁt of the high-priority process.
However, we are wasting the effort spent to bring in the page for the low-priority process.
If we decide to prevent replacement of a newly brought-in page until it can be used at least once, then we can use the lock bit to implement this mechanism.
When a page is selected for replacement, its lock bit is turned on.
It remains on until the faulting process is again dispatched.
Using a lock bit can be dangerous: the lock bit may get turned on but never turned off.
Should this situation occur (because of a bug in the operating system, for example), the locked frame becomes unusable.
On a single-user system, the overuse of locking would hurt only the user doing the locking.
For instance, Solaris allows locking “hints,” but it is free to disregard these hints if the free-frame pool becomes too small or if an individual process requests that too many pages be locked in memory.
In this section, we describe how Windows and Solaris implement virtual memory.
Clustering handles page faults by bringing in not only the faulting page but also.
When a process is ﬁrst created, it is assigned a working-set minimum and maximum.
The working-set minimum is the minimum number of pages the process is guaranteed to have in memory.
If sufﬁcient memory is available, a process may be assigned as many pages as its working-set maximum.
In some circumstances, a process may be allowed to exceed its working-set maximum.
The virtual memory manager maintains a list of free page frames.
Associated with this list is a threshold value that is used to indicate whether sufﬁcient free memory is available.
If a page fault occurs for a process that is below its working-set maximum, the virtual memory manager allocates a page from this list of free pages.
If a process that is at its working-set maximum incurs a page fault, it must select a page for replacement using a local LRU page-replacement policy.
When the amount of free memory falls below the threshold, the virtual memory manager uses a tactic known as automatic working-set trimming to restore the value above the threshold.
Automatic working-set trimming works by evaluating the number of pages allocated to processes.
If a process has been allocated more pages than its working-set minimum, the virtual memory manager removes pages until the process reaches its working-set minimum.
A process that is at its working-set minimum may be allocated pages from the free-page-frame list once sufﬁcient free memory is available.
Windows performs working-set trimming on both user mode and system processes.
In Solaris, when a thread incurs a page fault, the kernel assigns a page to the faulting thread from the list of free pages it maintains.
Therefore, it is imperative that the kernel keep a sufﬁcient amount of free memory available.
The lotsfree parameter is typically set to 1/64 the size of the physical memory.
Four times per second, the kernel checks whether the amount of free memory is less than lotsfree.
If the number of free pages falls below lotsfree, a process known as a pageout starts up.
The pageout process is similar to the second-chance algorithm described in Section 9.4.5.2, except that it uses two hands while scanning pages, rather than one.
Later, the back hand of the clock examines the reference bit for the pages in memory, appending each page whose reference bit is still set to 0 to the free list and writing to disk its contents if modiﬁed.
Solaris maintains a cache list of pages that have been “freed” but have not yet been overwritten.
Pages can be reclaimed from the cache list if they are accessed before being moved to the free list.
The pageout algorithm uses several parameters to control the rate at which pages are scanned (known as the scanrate)
The scanrate is expressed in pages per second and ranges from slowscan to fastscan.
When free memory falls below lotsfree, scanning occurs at slowscan pages per second and progresses to fastscan, depending on the amount of free memory available.
The default value of slowscan is 100 pages per second.
This is shown in Figure 9.29 (with fastscan set to the maximum)
The distance (in pages) between the hands of the clock is determined by a system parameter, handspread.
The amount of time between the front hand’s clearing a bit and the back hand’s investigating its value depends on the scanrate and the handspread.
However, because of the demands placed on the memory system, a scanrate of several thousand is not uncommon.
This means that the amount of time between clearing and investigating a bit is often a few seconds.
As mentioned above, the pageout process checks memory four times per second.
However, if free memory falls below the value ofdesfree (Figure 9.29), pageout will run a hundred times per second with the intention of keeping at least desfree free memory available.
If the pageout process is unable to keep the amount of free memory at desfree for a 30-second average, the kernel begins swapping processes, thereby freeing all pages allocated to swapped processes.
In general, the kernel looks for processes that have been idle for long periods of time.
If the system is unable to maintain the amount of free memory at minfree, the pageout process is called for every request for a new page.
Recent releases of the Solaris kernel have provided enhancements of the paging algorithm.
Pages belonging to libraries that are being shared by several processes—even if they are eligible to be claimed by the scannerare skipped during the page-scanning process.
This is known as priority paging and is covered in Section 12.6.2
It is desirable to be able to execute a process whose logical address space is larger than the available physical address space.
Virtual memory is a technique that enables us to map a large logical address space onto a smaller physical memory.
Virtual memory allows us to run extremely large processes and to raise the degree of multiprogramming, increasing CPU utilization.
Further, it frees application programmers from worrying about memory availability.
In addition, with virtual memory, several processes can share system libraries and memory.
With virtual memory, we can also use an efﬁcient type of process creation known as copy-on-write, wherein parent and child processes share actual pages of memory.
Pure demand paging never brings in a page until that page is referenced.
The ﬁrst reference causes a page fault to the operating system.
The operating-system kernel consults an internal table to determine where the page is located on the backing store.
It then ﬁnds a free frame and reads the page in from the backing store.
The page table is updated to reﬂect this change, and the instruction that caused the page fault is restarted.
This approach allows a process to run even though its entire memory image is not in main memory at once.
As long as the page-fault rate is reasonably low, performance is acceptable.
We can use demand paging to reduce the number of frames allocated to a process.
This arrangement can increase the degree of multiprogramming (allowing more processes to be available for execution at one time) and—in theory, at least—the CPU utilization of the system.
It also allows processes to be run even though their memory requirements exceed the total available physical memory.
If total memory requirements exceed the capacity of physical memory, then it may be necessary to replace pages from memory to free frames for new pages.
Most page-replacement algorithms, such as the second-chance algorithm, are approximations of LRU replacement.
In addition to a page-replacement algorithm, a frame-allocation policy is needed.
Allocation can be ﬁxed, suggesting local page replacement, or dynamic, suggesting global replacement.
The working set is the set of pages in the current locality.
Accordingly, each process should be allocated enough frames for its current working set.
If a process does not have enough memory for its working set, it will thrash.
Providing enough frames to each process to avoid thrashing may require process swapping and scheduling.
Most operating systems provide features for memory mapping ﬁles, thus allowing ﬁle I/O to be treated as routine memory access.
The Win32 API implements shared memory through memory mapping of ﬁles.
Kernel processes typically require memory to be allocated using pages that are physically contiguous.
The buddy system allocates memory to kernel processes in units sized according to a power of 2, which often results in fragmentation.
Slab allocators assign kernel data structures to caches associated with slabs, which are made up of one or more physically contiguous pages.
With slab allocation, no memory is wasted due to fragmentation, and memory requests can be satisﬁed quickly.
In addition to requiring us to solve the major problems of page replacement and frame allocation, the proper design of a paging system requires that we consider prepaging, page size, TLB reach, inverted page tables, program structure, I/O interlock and page locking, and other issues.
The page-reference string has length p, and n distinct page numbers occur in it.
What is a lower bound on the number of page faults?
What is an upper bound on the number of page faults?
The list of free page frames is D, E , F (that is, D is at the head of the list, E is second, and F is last)
Convert the following virtual addresses to their equivalent physical addresses in hexadecimal.
A dash for a page frame indicates that the page is not in memory.
Rank these algorithms on a ﬁve-point scale from “bad” to “perfect” according to their page-fault rate.
Separate those algorithms that suffer from Belady’s anomaly from those that do not.
The central processor has a cycle time of 1 microsecond.
It costs an additional 1 microsecond to access a page other than the current one.
One percent of all instructions executed accessed a page other than the current page.
Of the instructions that accessed another page, 80 percent accessed a page already in memory.
When a new page was required, the replaced page was modiﬁed 50 percent of the time.
Calculate the effective instruction time on this system, assuming that the system is running one process only and that the processor is idle during drum transfers.
How many page faults would occur for the following replacement algorithms, assuming one, two, three, four, ﬁve, six, and seven frames? Remember that all frames are initially empty, so your ﬁrst unique pages will cost one fault each.
Sketch how you could simulate a reference bit even if one were not provided by the hardware, or explain why it is not possible to do so.
If it is possible, calculate what the cost would be.
Remember that since segments are not the same size, the segment that is chosen for replacement may be too small to leave enough consecutive locations for the needed segment.
Consider strategies for systems where segments cannot be relocated and strategies for systems where they can.
The system was recently measured to determine utilization of the CPU and the paging disk.
For each case, what is happening? Can the degree of multiprogramming be increased to increase the CPU utilization? Is the paging helping?
Can the page tables be set up to simulate base and limit registers? How can they be, or why can they not be?
Describe a scenario in which each of the following can occur.
Assuming a thread is in the Running state, answer the following questions, and explain your answer:
Will the thread change state if it incurs a page fault? If so, to what state will it change?
Will the thread change state if it generates a TLB miss that is resolved in the page table? If so, to what state will it change?
Will the thread change state if an address reference is resolved in the page table? If so, to what state will it change?
When a process ﬁrst starts execution, how would you characterize the page-fault rate?
Once the working set for a process is loaded into memory, how would you characterize the page-fault rate?
Assume that a process changes its locality and the size of the new working set is too large to be stored in available free memory.
Identify some options system designers could choose from to handle this situation.
The virtual memory is implemented by paging, and the page size is 4,096 bytes.
Assume that the page to be replaced is modiﬁed 70 percent of the time.
What is the maximum acceptable page-fault rate for an effective access time of no more than 200 nanoseconds?
Assume that there exists a process with ﬁve user-level threads and that the mapping of user threads to kernel threads is one to one.
If one user thread incurs a page fault while accessing its stack, would the other user threads belonging to the same process also be affected by the page fault—that is, would they also have to wait for the faulting page to be brought into memory? Explain.
Assuming demand paging with three frames, how many page faults would occur for the following replacement algorithms?
The reference bit is set to 1 when the page has been referenced.
Periodically, a thread zeroes out all values of the reference bit.
A dash for a page frame indicates the page is not in memory.
The page-replacement algorithm is localized LRU, and all numbers are provided in decimal.
Convert the following virtual addresses (in hexadecimal) to the equivalent physical addresses.
Also set the reference bit for the appropriate entry in the page table.
Using the above addresses as a guide, provide an example of a logical address (in hexadecimal) that results in a page fault.
From what set of page frames will the LRU page-replacement algorithm choose in resolving a page fault?
What can you say about the system if you notice the following behavior:
Assume that the free-frame pool is managed using the LRU replacement policy.
If a page fault occurs and the page does not exist in the free-frame pool, how is free space generated for the newly requested page?
If a page fault occurs and the page exists in the free-frame pool, how is the resident page set and the free-frame pool managed to make space for the requested page?
What does the system degenerate to if the number of resident pages is set to one?
What does the system degenerate to if the number of pages in the free-frame pool is zero?
For each of the following, indicate whether it will (or is likely to) improve CPU utilization.
Install a faster hard disk or multiple controllers with multiple hard disks.
What sequence of page faults is incurred when all of the pages of a program are currently nonresident and the ﬁrst instruction of the program is an indirect memory-load operation? What happens when the operating system is using a per-process frame allocation technique and only two pages are allocated to this process?
What would you gain and what would you lose by using this policy rather than LRU or second-chance replacement?
We can achieve this minimization by distributing heavily used pages evenly over all of memory, rather than having them compete for a small number of page frames.
We can associate with each page frame a counter of the number of pages associated with that frame.
Then, to replace a page, we can search for the page frame with the smallest counter.
How many page faults occur for your algorithm for the following reference string with four page frames?
What is the minimum number of page faults for an optimal pagereplacement strategy for the reference string in part b with four page frames?
Addresses are translated through a page table in main memory, with an access time of 1 microsecond per memory access.
Thus, each memory reference through the page table takes two accesses.
To improve this time, we have added an associative memory that reduces access time to one memory reference if the page-table entry is in the associative memory.
Using Figure 9.26 as a guide, draw a tree illustrating how the following memory requests are allocated:
Next, modify the tree for the following releases of memory.
The mapping in this system is one to one (there is a corresponding kernel thread for each user thread)
Does a multithreaded process consist of (a) a working set for the entire process or (b) a working set for each thread? Explain.
Assuming there is one cache per object type, explain why this scheme doesn’t scale well with multiple CPUs.
What are the advantages of such a paging scheme? What modiﬁcations to the virtual memory system provide this functionality?
Apply the random page-reference string to each algorithm, and record the number of page faults incurred by each algorithm.
In particular, using the producer—consumer strategy, design two programs that communicate with shared memory using the Windows API as outlined in Section 9.7.2
The producer will generate the numbers speciﬁed in the Collatz conjecture and write them to a shared memory object.
The consumer will then read and output the sequence of numbers from shared memory.
In this instance, the producer will be passed an integer parameter on the command line specifying how many numbers to produce (for example, providing 5 on the command line means the producer process will generate the ﬁrst ﬁve numbers)
Your program will read from a ﬁle containing logical addresses and, using a TLB as well as a page table, will translate each logical address to its corresponding physical address and output the value of the byte stored at the translated physical address.
The goal behind this project is to simulate the steps involved in translating logical to physical addresses.
Your program will read a ﬁle containing several 32-bit integer numbers that represent logical addresses.
Hence, the addresses are structured as shown in Figure 9.33
Additionally, your program need only be concerned with reading logical addresses and translating them to their corresponding physical addresses.
You do not need to support writing to the logical address space.
Your program will translate logical to physical addresses using a TLB and page table as outlined in Section 8.5
First, the page number is extracted from the logical address, and the TLB is consulted.
In the case of a TLB-hit, the frame number is obtained from the TLB.
In the case of a TLB-miss, the page table must be consulted.
In the latter case, either the frame number is obtained.
Your program will implement demand paging as described in Section 9.2
The backing store is represented by the ﬁle BACKING STORE.bin, a binary ﬁle of size 65,536 bytes.
When a page fault occurs, you will read in a 256-byte page from the ﬁle BACKING STORE and store it in an available page frame in physical memory.
Once this frame is stored (and the page table and TLB are updated), subsequent accesses to page 15 will be resolved by either the TLB or the page table.
You will need to treat BACKING STORE.bin as a random-access ﬁle so that you can randomly seek to certain positions of the ﬁle for reading.
We suggest using the standard C library functions for performing I/O, including fopen(), fread(), fseek(), and fclose()
The size of physical memory is the same as the size of the virtual address space—65,536 bytes—so you do not need to be concerned about page replacements during a page fault.
Later, we describe a modiﬁcation to this project using a smaller amount of physical memory; at that point, a page-replacement strategy will be required.
First, write a simple program that extracts the page number and offset (based on Figure 9.33) from the following integer numbers:
Perhaps the easiest way to do this is by using the operators for bit-masking and bit-shifting.
Once you can correctly establish the page number and offset from an integer number, you are ready to begin.
Initially, we suggest that you bypass the TLB and use only a page table.
You can integrate the TLB once your page table is working properly.
Remember, address translation can work without a TLB; the TLB just makes it faster.
When you are ready to implement the TLB, recall that it has only 16 entries, so you will need to use a replacement strategy when you update a full TLB.
You may use either a FIFO or an LRU policy for updating your TLB.
Your program is to translate each logical address to a physical address and determine the contents of the signed byte stored at the correct physical address.
Recall that in the C language, the char data type occupies a byte of storage, so we suggest using char values.
The logical address being translated (the integer value being read from addresses.txt)
The corresponding physical address (what your program translates the logical address to)
The signed byte value stored at the translated physical address.
We also provide the ﬁle correct.txt, which contains the correct output values for the ﬁle addresses.txt.
You should use this ﬁle to determine if your program is correctly translating logical to physical addresses.
After completion, your program is to report the following statistics:
Page-fault rate—The percentage of address references that resulted in page faults.
Since the logical addresses in addresses.txt were generated randomly and do not reﬂect any memory access locality, do not expect to have a high TLB hit rate.
This project assumes that physical memory is the same size as the virtual address space.
In practice, physical memory is typically much smaller than a virtual address space.
A suggested modiﬁcation is to use a smaller physical address space.
This change will require modifying your program so that it keeps track of free page frames as well as implementing a page-replacement policy using either FIFO or LRU (Section 9.4)
Belady’s optimal algorithm is for a ﬁxed allocation; [Prieve and Fabry (1976)] presented an optimal algorithm for situations in which the allocation can vary.
The enhanced clock algorithm was discussed by [Carr and Hennessy (1981)]
Discussions concerning the working-set model were presented by [Denning (1980)]
Since main memory is usually too small to accommodate all the data and programs permanently, the computer system must provide secondary storage to back up main memory.
Modern computer systems use disks as the primary on-line storage medium for information (both programs and data)
The ﬁle system provides the mechanism for on-line storage of and access to both data and programs residing on the disks.
A ﬁle is a collection of related information deﬁned by its creator.
Files are normally organized into directories for ease of use.
The devices that attach to a computer vary in many aspects.
Some devices transfer a character or a block of characters at a time.
In many ways, they are also the slowest major component of the computer.
Because of all this device variation, the operating system needs to provide a wide range of functionality to applications, to allow them to control all aspects of the devices.
One key goal of an operating system’s I/O subsystem is to provide the simplest interface possible to the rest of the system.
Because devices are a performance bottleneck, another key is to optimize I/O for maximum concurrency.
The ﬁle system can be viewed logically as consisting of three parts.
In Chapter Chapter 12, we describe the internal data structures and algorithms used by the operating system to implement this interface.
In this chapter, we begin a discussion of ﬁle systems at the lowest level: the structure of secondary storage.
We ﬁrst describe the physical structure of magnetic disks and magnetic tapes.
We then describe disk-scheduling algorithms, which schedule the order of disk I/Os to maximize performance.
Next, we discuss disk formatting and management of boot blocks, damaged blocks, and swap space.
We conclude with an examination of the structure of RAID systems.
To describe the physical structure of secondary storage devices and its effects on the uses of the devices.
To discuss operating-system services provided for mass storage, including RAID.
In this section, we present a general overview of the physical structure of secondary and tertiary storage devices.
Magnetic disks provide the bulk of secondary storage for modern computer systems.
Each disk platter has a ﬂat circular shape, like a CD.
The two surfaces of a platter are covered with a magnetic material.
We store information by recording it magnetically on the platters.
A read–write head “ﬂies” just above each surface of every platter.
The heads are attached to a disk arm that moves all the heads as a unit.
The surface of a platter is logically divided into circular tracks, which are subdivided into sectors.
The set of tracks that are at one arm position makes up a cylinder.
There may be thousands of concentric cylinders in a disk drive, and each track may contain hundreds of sectors.
The storage capacity of common disk drives is measured in gigabytes.
When the disk is in use, a drive motor spins it at high speed.
The transfer rate is the rate at which data ﬂow between the drive and the computer.
The positioning time, or random-access time, consists of two parts: the time necessary to move the disk arm to the desired cylinder, called the seek time, and the time necessary for the desired sector to rotate to the disk head, called the rotational latency.
Typical disks can transfer several megabytes of data per second, and they have seek times and rotational latencies of several milliseconds.
Because the disk head ﬂies on an extremely thin cushion of air (measured in microns), there is a danger that the head will make contact with the disk surface.
Although the disk platters are coated with a thin protective layer, the head will sometimes damage the magnetic surface.
A head crash normally cannot be repaired; the entire disk must be replaced.
A disk can be removable, allowing different disks to be mounted as needed.
Removable magnetic disks generally consist of one platter, held in a plastic case to prevent damage while not in the disk drive.
Other forms of removable disks include CDs, DVDs, and Blu-ray discs as well as removable ﬂash-memory devices known as ﬂash drives (which are a type of solid-state drive)
A disk drive is attached to a computer by a set of wires called an I/O bus.
Several kinds of buses are available, including advanced technology attachment (ATA), serial ATA (SATA), eSATA, universal serial bus (USB), and ﬁbre channel (FC)
The data transfers on a bus are carried out by special electronic processors called controllers.
The host controller is the controller at the computer end of the bus.
To perform a disk I/O operation, the computer places a command into the host controller, typically using memory-mapped I/O ports, as described in Section 9.7.3
The host controller then sends the command via messages to the disk controller, and the disk controller operates the disk-drive hardware to carry out the command.
Data transfer at the disk drive happens between the cache and the disk surface, and data transfer to the host, at fast electronic speeds, occurs between the cache and the host controller.
Sometimes old technologies are used in new ways as economics change or the technologies evolve.
An example is the growing importance of solid-state disks, or SSDs.
Simply described, an SSD is nonvolatile memory that is used like a hard drive.
There are many variations of this technology, from DRAM with a battery to allow it to maintain its state in a power failure through ﬂash-memory technologies like single-level cell (SLC) and multilevel cell (MLC) chips.
SSDs have the same characteristics as traditional hard disks but can be more reliable because they have no moving parts and faster because they have no seek time or latency.
However, they are more expensive per megabyte than traditional hard disks, have less capacity than the larger hard disks, and may have shorter life spans than hard disks, so their uses are somewhat limited.
One use for SSDs is in storage arrays, where they hold ﬁle-system metadata that require high performance.
SSDs are also used in some laptop computers to make them smaller, faster, and more energy-efﬁcient.
Because SSDs can be much faster than magnetic disk drives, standard bus interfaces can cause a major limit on throughput.
Some SSDs are designed to connect directly to the system bus (PCI, for example)
SSDs are changing other traditional aspects of computer design as well.
Some systems use them as a direct replacement for disk drives, while others use them as a new cache tier, moving data between magnetic disks, SSDs, and memory to optimize performance.
In the remainder of this chapter, some sections pertain to SSDs, while others do not.
For example, because SSDs have no disk head, disk-scheduling algorithms largely do not apply.
Although it is relatively permanent and can hold large quantities of data, its access time is slow compared with that of main memory and magnetic disk.
In addition, random access to magnetic tape is about a thousand times slower than random access to magnetic disk, so tapes are not very useful for secondary storage.
As with many aspects of computing, published performance numbers for disks are not the same as real-world performance numbers.
Stated transfer rates are always lower than effective transfer rates, for example.
The transfer rate may be the rate at which bits can be read from the magnetic media by the disk head, but that is different from the rate at which blocks are delivered to the operating system.
Tapes are used mainly for backup, for storage of infrequently used information, and as a medium for transferring information from one system to another.
A tape is kept in a spool and is wound or rewound past a read–write head.
Moving to the correct spot on a tape can take minutes, but once positioned, tape drives can write data at speeds comparable to disk drives.
Tape capacities vary greatly, depending on the particular kind of tape drive, with current capacities exceeding several terabytes.
Some tapes have built-in compression that can more than double the effective storage.
Some are named according to technology, such as LTO-5 and SDLT.
Modern magnetic disk drives are addressed as large one-dimensional arrays of logical blocks, where the logical block is the smallest unit of transfer.
The one-dimensional array of logical blocks is mapped onto the sectors of the disk sequentially.
Sector 0 is the ﬁrst sector of the ﬁrst track on the outermost cylinder.
The mapping proceeds in order through that track, then through the rest of the tracks in that cylinder, and then through the rest of the cylinders from outermost to innermost.
By using this mapping, we can—at least in theory—convert a logical block number into an old-style disk address that consists of a cylinder number, a track number within that cylinder, and a sector number within that track.
In practice, it is difﬁcult to perform this translation, for two reasons.
First, most disks have some defective sectors, but the mapping hides this by substituting spare sectors from elsewhere on the disk.
Second, the number of sectors per track is not a constant on some drives.
On media that use constant linear velocity (CLV), the density of bits per track is uniform.
The farther a track is from the center of the disk, the greater its length, so the more sectors it can hold.
As we move from outer zones to inner zones, the number of sectors per track decreases.
Tracks in the outermost zone typically hold 40 percent more sectors than do tracks in the innermost zone.
The drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head.
Alternatively, the disk rotation speed can stay constant; in this case, the density of bits decreases from inner tracks to outer tracks to keep the data rate constant.
This method is used in hard disks and is known as constant angular velocity (CAV)
The number of sectors per track has been increasing as disk technology improves, and the outer zone of a disk usually has several hundred sectors per track.
Similarly, the number of cylinders per disk has been increasing; large disks have tens of thousands of cylinders.
The typical desktop PC uses an I/O bus architecture called IDE or ATA.
This architecture supports a maximum of two drives per I/O bus.
A newer, similar protocol that has simpliﬁed cabling is SATA.
High-end workstations and servers generally use more sophisticated I/O architectures such as ﬁbre channel (FC), a high-speed serial architecture that can operate over optical ﬁber or over a four-conductor copper cable.
One is a large switched fabric having a 24-bit address space.
This variant is expected to dominate in the future and is the basis of storage-area networks (SANs), discussed in Section 10.3.3
Because of the large address space and the switched nature of the communication, multiple hosts and storage devices can attach to the fabric, allowing great ﬂexibility in I/O communication.
The other FC variant is an arbitrated loop (FC-AL) that can address 126 devices (drives and controllers)
A wide variety of storage devices are suitable for use as host-attached storage.
Among these are hard disk drives, RAID arrays, and CD, DVD, and tape drives.
The I/O commands that initiate data transfers to a host-attached storage device are reads and writes of logical data blocks directed to speciﬁcally identiﬁed storage units (such as bus ID or target logical unit)
A network-attached storage (NAS) device is a special-purpose storage system that is accessed remotely over a data network (Figure 10.2)
The remote procedure calls (RPCs) are carried via TCP or UDP over an IP network—usually the same localarea network (LAN) that carries all data trafﬁc to the clients.
Thus, it may be easiest to think of NAS as simply another storage-access protocol.
The networkattached storage unit is usually implemented as a RAID array with software that implements the RPC interface.
Network-attached storage provides a convenient way for all the computers on a LAN to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage.
However, it tends to be less efﬁcient and have lower performance than some direct-attached storage options.
In essence, it uses the IP network protocol to carry the SCSI protocol.
Thus, networks—rather than SCSI cables—can be used as the interconnects between hosts and their storage.
As a result, hosts can treat their storage as if it were directly attached, even if the storage is distant from the host.
One drawback of network-attached storage systems is that the storage I/O operations consume bandwidth on the data network, thereby increasing the latency of network communication.
This problem can be particularly acute in large client–server installations—the communication between servers and clients competes for bandwidth with the communication among servers and storage devices.
A storage-area network (SAN) is a private network (using storage protocols rather than networking protocols) connecting servers and storage units, as shown in Figure 10.3
Multiple hosts and multiple storage arrays can attach to the same SAN, and storage can be dynamically allocated to hosts.
A SAN switch allows or prohibits access between the hosts and the storage.
As one example, if a host is running low on disk space, the SAN can be conﬁgured to allocate more storage to that host.
SANs make it possible for clusters of servers to share the same storage and for storage arrays to include multiple direct host connections.
SANs typically have more ports—as well as more expensive ports—than storage arrays.
Another SAN interconnect is InﬁniBand — a special-purpose bus architecture that provides hardware and software support for high-speed interconnection networks for servers and storage units.
One of the responsibilities of the operating system is to use the hardware efﬁciently.
For the disk drives, meeting this responsibility entails having fast.
For magnetic disks, the access time has two major components, as mentioned in Section 10.1.1
The seek time is the time for the disk arm to move the heads to the cylinder containing the desired sector.
The rotational latency is the additional time for the disk to rotate the desired sector to the disk head.
The disk bandwidth is the total number of bytes transferred, divided by the total time between the ﬁrst request for service and the completion of the last transfer.
We can improve both the access time and the bandwidth by managing the order in which disk I/O requests are serviced.
Whenever a process needs I/O to or from the disk, it issues a system call to the operating system.
If the desired disk drive and controller are available, the request can be serviced immediately.
If the drive or controller is busy, any new requests for service will be placed in the queue of pending requests for that drive.
For a multiprogramming system with many processes, the disk queue may often have several pending requests.
Thus, when one request is completed, the operating system chooses which pending request to service next.
How does the operating system make this choice? Any one of several disk-scheduling algorithms can be used, and we discuss them next.
The simplest form of disk scheduling is, of course, the ﬁrst-come, ﬁrst-served (FCFS) algorithm.
This algorithm is intrinsically fair, but it generally does not provide the fastest service.
Consider, for example, a disk queue with requests for I/O to blocks on cylinders.
It seems reasonable to service all the requests close to the current head position before moving the head far away to service other requests.
The SSTF algorithm selects the request with the least seek time from the current head position.
In other words, SSTF chooses the pending request closest to the current head position.
This scheduling method results in a total head movement of only 236 cylinders—little more than one-third of the distance needed for FCFS scheduling of this request queue.
This new request will be serviced next, making the request at 186 wait.
While this request is being serviced, another request close to 14 could arrive.
In theory, a continual stream of requests near one another could cause the request for cylinder 186 to wait indeﬁnitely.
This scenario becomes increasingly likely as the pending-request queue grows longer.
Although the SSTF algorithm is a substantial improvement over the FCFS algorithm, it is not optimal.
This strategy reduces the total head movement to 208 cylinders.
In the SCAN algorithm, the disk arm starts at one end of the disk and moves toward the other end, servicing requests as it reaches each cylinder, until it gets to the other end of the disk.
At the other end, the direction of head movement is reversed, and servicing continues.
The head continuously scans back and forth across the disk.
The SCAN algorithm is sometimes called the elevator algorithm, since the disk arm behaves just like an elevator in a building, ﬁrst servicing all the requests going up and then reversing to service requests the other way.
If a request arrives in the queue just in front of the head, it will be serviced almost immediately; a request arriving just behind the head will have to wait until the arm moves to the end of the disk, reverses direction, and comes back.
Assuming a uniform distribution of requests for cylinders, consider the density of requests when the head reaches one end and reverses direction.
At this point, relatively few requests are immediately in front of the head, since these cylinders have recently been serviced.
These requests have also waited the longest, so why not go there ﬁrst? That is the idea of the next algorithm.
Circular SCAN (C-SCAN) scheduling is a variant of SCAN designed to provide a more uniform wait time.
Like SCAN, C-SCAN moves the head from one end of the disk to the other, servicing requests along the way.
When the head reaches the other end, however, it immediately returns to the beginning of the disk without servicing any requests on the return trip (Figure 10.7)
The C-SCAN scheduling algorithm essentially treats the cylinders as a circular list that wraps around from the ﬁnal cylinder to the ﬁrst one.
As we described them, both SCAN and C-SCAN move the disk arm across the full width of the disk.
More commonly, the arm goes only as far as the ﬁnal request in each direction.
Then, it reverses direction immediately, without going all the way to the end of the disk.
Versions of SCAN and C-SCAN that follow this pattern are called LOOK and C-LOOK scheduling, because they look for a request before continuing to move in a given direction (Figure 10.8)
Given so many disk-scheduling algorithms, how do we choose the best one? SSTF is common and has a natural appeal because it increases performance over FCFS.
For any particular list of requests, we can deﬁne an optimal order of retrieval, but the computation needed to ﬁnd an optimal schedule may not justify the savings over SSTF or SCAN.
With any scheduling algorithm, however, performance depends heavily on the number and types of requests.
For instance, suppose that the queue usually has just one outstanding request.
Then, all scheduling algorithms behave the same, because they have only one choice of where to move the disk head: they all behave like FCFS scheduling.
Requests for disk service can be greatly inﬂuenced by the ﬁle-allocation method.
A program reading a contiguously allocated ﬁle will generate several requests that are close together on the disk, resulting in limited head movement.
A linked or indexed ﬁle, in contrast, may include blocks that are widely scattered on the disk, resulting in greater head movement.
The location of directories and index blocks is also important.
Since every ﬁle must be opened to be used, and opening a ﬁle requires searching the directory structure, the directories will be accessed frequently.
Suppose that a directory entry is on the ﬁrst cylinder and a ﬁle’s data are on the ﬁnal cylinder.
In this case, the disk head has to move the entire width of the disk.
The disk-scheduling algorithms discussed in this section focus primarily on minimizing the amount of disk head movement in magnetic disk drives.
SSDs—which do not contain moving disk heads—commonly use a simple FCFS policy.
For example, the Linux Noop scheduler uses an FCFS policy but modiﬁes it to merge adjacent requests.
The observed behavior of SSDs indicates that the time required to service reads is uniform but that, because of the properties of ﬂash memory, write service time is not uniform.
Some SSD schedulers have exploited this property and merge only adjacent write requests, servicing all read requests in FCFS order.
Caching the directories and index blocks in main memory can also help to reduce disk-arm movement, particularly for read requests.
Because of these complexities, the disk-scheduling algorithm should be written as a separate module of the operating system, so that it can be replaced with a different algorithm if necessary.
Either SSTF or LOOK is a reasonable choice for the default algorithm.
The scheduling algorithms described here consider only the seek distances.
For modern disks, the rotational latency can be nearly as large as the average seek time.
It is difﬁcult for the operating system to schedule for improved rotational latency, though, because modern disks do not disclose the physical location of logical blocks.
Disk manufacturers have been alleviating this problem by implementing disk-scheduling algorithms in the controller hardware built into the disk drive.
If the operating system sends a batch of requests to the controller, the controller can queue them and then schedule them to improve both the seek time and the rotational latency.
If I/O performance were the only consideration, the operating system would gladly turn over the responsibility of disk scheduling to the disk hardware.
In practice, however, the operating system may have other constraints on the service order for requests.
For instance, demand paging may take priority over application I/O, and writes are more urgent than reads if the cache is running out of free pages.
Also, it may be desirable to guarantee the order of a set of disk writes to make the ﬁle system robust in the face of system crashes.
Consider what could happen if the operating system allocated a disk page to a ﬁle and the application wrote data into that page before the operating system had a chance to ﬂush the ﬁle system metadata back to disk.
To accommodate such requirements, an operating system may choose to do its own disk scheduling and to spoon-feed the requests to the disk controller, one by one, for some types of I/O.
The operating system is responsible for several other aspects of disk management, too.
Here we discuss disk initialization, booting from disk, and bad-block recovery.
A new magnetic disk is a blank slate: it is just a platter of a magnetic recording material.
Before a disk can store data, it must be divided into sectors that the disk controller can read and write.
Low-level formatting ﬁlls the disk with a special data structure for each sector.
The data structure for a sector typically consists of a header, a data area (usually 512 bytes in size), and a trailer.
The header and trailer contain information used by the disk controller, such as a sector number and an error-correcting code (ECC)
When the controller writes a sector of data during normal I/O, the ECC is updated with a value calculated from all the bytes in the data area.
When the sector is read, the ECC is recalculated and compared with the stored value.
If the stored and calculated numbers are different, this mismatch indicates that the data area of the sector has become corrupted and that the disk sector may be bad (Section 10.5.3)
The ECC is an error-correcting code because it contains enough information, if only a few bits of data have been corrupted, to enable the controller to identify which bits have changed and calculate what their correct values should be.
The controller automatically does the ECC processing whenever a sector is read or written.
This formatting enables the manufacturer to test the disk and to initialize the mapping from logical block numbers to defect-free sectors on the disk.
For many hard disks, when the disk controller is instructed to low-level-format the disk, it can also be told how many bytes of data space to leave between the header and trailer of all sectors.
Formatting a disk with a larger sector size means that fewer sectors can ﬁt on each track; but it also means that fewer headers and trailers are written on each track and more space is available for user data.
Some operating systems can handle only a sector size of 512 bytes.
Before it can use a disk to hold ﬁles, the operating system still needs to record its own data structures on the disk.
The ﬁrst step is to partition the disk into one or more groups of cylinders.
The operating system can treat each partition as though it were a separate disk.
For instance, one partition can hold a copy of the operating system’s executable code, while another holds user ﬁles.
The second step is logical formatting, or creation of a ﬁle system.
In this step, the operating system stores the initial ﬁle-system data structures onto the disk.
These data structures may include maps of free and allocated space and an initial empty directory.
To increase efﬁciency, most ﬁle systems group blocks together into larger chunks, frequently called clusters.
Disk I/O is done via blocks, but ﬁle system I/O is done via clusters, effectively assuring that I/O has more sequential-access and fewer random-access characteristics.
Some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks, without any ﬁle-system data structures.
This array is sometimes called the raw disk, and I/O to this array is termed raw I/O.
For example, some database systems prefer raw I/O because it enables them to control the exact disk location where each database record is stored.
We can make certain applications more efﬁcient by allowing them to implement their own special-purpose storage services on a raw partition, but most applications perform better when they use the regular ﬁle-system services.
For a computer to start running—for instance, when it is powered up or rebooted—it must have an initial program to run.
It initializes all aspects of the system, from CPU registers to device controllers and the contents of main memory, and then starts the operating system.
To do its job, the bootstrap program ﬁnds the operating-system kernel on disk, loads that kernel into memory, and jumps to an initial address to begin the operating-system execution.
For most computers, the bootstrap is stored in read-only memory (ROM)
This location is convenient, because ROM needs no initialization and is at a ﬁxed location that the processor can start executing when powered up or reset.
And, since ROM is read only, it cannot be infected by a computer virus.
The problem is that changing this bootstrap code requires changing the ROM hardware chips.
For this reason, most systems store a tiny bootstrap loader program in the boot ROM whose only job is to bring in a full bootstrap program from disk.
The full bootstrap program can be changed easily: a new version is simply written onto the disk.
The full bootstrap program is stored in the “boot blocks” at a ﬁxed location on the disk.
A disk that has a boot partition is called a boot disk or system disk.
The code in the boot ROM instructs the disk controller to read the boot blocks into memory (no device drivers are loaded at this point) and then starts executing that code.
The full bootstrap program is more sophisticated than the bootstrap loader in the boot ROM.
It is able to load the entire operating system from a non-ﬁxed location on disk and to start the operating system running.
Let’s consider as an example the boot process in Windows.
First, note that Windows allows a hard disk to be divided into partitions, and one partition —identiﬁed as the boot partition—contains the operating system and device drivers.
The Windows system places its boot code in the ﬁrst sector on the hard disk, which it terms the master boot record, or MBR.
Booting begins by running code that is resident in the system’s ROM memory.
This code directs the system to read the boot code from the MBR.
In addition to containing boot code, the MBR contains a table listing the partitions for the hard disk and a ﬂag indicating which partition the system is to be booted from, as illustrated in Figure 10.9
Once the system identiﬁes the boot partition, it reads the ﬁrst sector from that partition (which is called the boot sector) and continues with the remainder of the boot process, which includes loading the various subsystems and system services.
Because disks have moving parts and small tolerances (recall that the disk head ﬂies just above the disk surface), they are prone to failure.
Sometimes the failure is complete; in this case, the disk needs to be replaced and its contents.
Most disks even come from the factory with bad blocks.
Depending on the disk and controller in use, these blocks are handled in a variety of ways.
On simple disks, such as some disks with IDE controllers, bad blocks are handled manually.
One strategy is to scan the disk to ﬁnd bad blocks while the disk is being formatted.
Any bad blocks that are discovered are ﬂagged as unusable so that the ﬁle system does not allocate them.
If blocks go bad during normal operation, a special program (such as the Linux badblocks command) must be run manually to search for the bad blocks and to lock them away.
Data that resided on the bad blocks usually are lost.
The controller maintains a list of bad blocks on the disk.
The list is initialized during the low-level formatting at the factory and is updated over the life of the disk.
Low-level formatting also sets aside spare sectors not visible to the operating system.
The controller can be told to replace each bad sector logically with one of the spare sectors.
The controller calculates the ECC and ﬁnds that the sector is bad.
The next time the system is rebooted, a special command is run to tell the controller to replace the bad sector with a spare.
After that, whenever the system requests logical block 87, the request is translated into the replacement sector’s address by the controller.
Note that such a redirection by the controller could invalidate any optimization by the operating system’s disk-scheduling algorithm! For this reason, most disks are formatted to provide a few spare sectors in each cylinder and a spare cylinder as well.
When a bad block is remapped, the controller uses a spare sector from the same cylinder, if possible.
As an alternative to sector sparing, some controllers can be instructed to replace a bad block by sector slipping.
The replacement of a bad block generally is not totally automatic, because the data in the bad block are usually lost.
Soft errors may trigger a process in which a copy of the block data is made and the block is spared or slipped.
Whatever ﬁle was using that block must be repaired (for instance, by restoration from a backup tape), and that requires manual intervention.
Swapping was ﬁrst presented in Section 8.2, where we discussed moving entire processes between disk and main memory.
Swapping in that setting occurs when the amount of physical memory reaches a critically low point and processes are moved from memory to swap space to free available memory.
In practice, very few modern operating systems implement swapping in this fashion.
Rather, systems now combine swapping with virtual memory techniques (Chapter 9) and swap pages, not necessarily entire processes.
In fact, some systems now use the terms “swapping” and “paging” interchangeably, reﬂecting the merging of these two concepts.
Swap-space management is another low-level task of the operating system.
Virtual memory uses disk space as an extension of main memory.
Since disk access is much slower than memory access, using swap space signiﬁcantly decreases system performance.
The main goal for the design and implementation of swap space is to provide the best throughput for the virtual memory system.
In this section, we discuss how swap space is used, where swap space is located on disk, and how swap space is managed.
Swap space is used in various ways by different operating systems, depending on the memory-management algorithms in use.
For instance, systems that implement swapping may use swap space to hold an entire process image, including the code and data segments.
Paging systems may simply store pages that have been pushed out of main memory.
The amount of swap space needed on a system can therefore vary from a few megabytes of disk space to gigabytes, depending on the amount of physical memory, the amount of virtual memory it is backing, and the way in which the virtual memory is used.
Note that it may be safer to overestimate than to underestimate the amount of swap space required, because if a system runs out of swap space it may be forced to abort processes or may crash entirely.
Overestimation wastes disk space that could otherwise be used for ﬁles, but it does no other harm.
Some systems recommend the amount to be set aside for swap space.
Solaris, for example, suggests setting swap space equal to the amount by which virtual memory exceeds pageable physical memory.
Today, that limitation is gone, and most Linux systems use considerably less swap space.
Some operating systems—including Linux—allow the use of multiple swap spaces, including both ﬁles and dedicated swap partitions.
These swap spaces are usually placed on separate disks so that the load placed on the I/O system by paging and swapping can be spread over the system’s I/O bandwidth.
A swap space can reside in one of two places: it can be carved out of the normal ﬁle system, or it can be in a separate disk partition.
If the swap space is simply a large ﬁle within the ﬁle system, normal ﬁle-system routines can be used to create it, name it, and allocate its space.
Navigating the directory structure and the diskallocation data structures takes time and (possibly) extra disk accesses.
External fragmentation can greatly increase swapping times by forcing multiple seeks during reading or writing of a process image.
We can improve performance by caching the block location information in physical memory and by using special tools to allocate physically contiguous blocks for the swap ﬁle, but the cost of traversing the ﬁle-system data structures remains.
Alternatively, swap space can be created in a separate raw partition.
No ﬁle system or directory structure is placed in this space.
Rather, a separate swap-space storage manager is used to allocate and deallocate the blocks from the raw partition.
This manager uses algorithms optimized for speed rather than for storage efﬁciency, because swap space is accessed much more frequently than ﬁle systems (when it is used)
Internal fragmentation may increase, but this trade-off is acceptable because the life of data in the swap space generally is much shorter than that of ﬁles in the ﬁle system.
Since swap space is reinitialized at boot time, any fragmentation is short-lived.
The raw-partition approach creates a ﬁxed amount of swap space during disk partitioning.
Adding more swap space requires either repartitioning the disk (which involves moving the other ﬁle-system partitions or destroying them and restoring them from backup) or adding another swap space elsewhere.
Some operating systems are ﬂexible and can swap both in raw partitions and in ﬁle-system space.
Linux is an example: the policy and implementation are separate, allowing the machine’s administrator to decide which type of swapping to use.
The trade-off is between the convenience of allocation and management in the ﬁle system and the performance of swapping in raw partitions.
We can illustrate how swap space is used by following the evolution of swapping and paging in various UNIX systems.
The traditional UNIX kernel started with an implementation of swapping that copied entire processes between contiguous disk regions and memory.
In Solaris 1 (SunOS), the designers changed standard UNIX methods to improve efﬁciency and reﬂect technological developments.
When a process executes, text-segment pages containing code are brought in from the ﬁle.
Figure 10.10 The data structures for swapping on Linux systems.
It is more efﬁcient to reread a page from the ﬁle system than to write it to swap space and then reread it from there.
Swap space is only used as a backing store for pages of anonymous memory, which includes memory allocated for the stack, heap, and uninitialized data of a process.
The biggest change is that Solaris now allocates swap space only when a page is forced out of physical memory, rather than when the virtual memory page is ﬁrst created.
This scheme gives better performance on modern computers, which have more physical memory than older systems and tend to page less.
Linux is similar to Solaris in that swap space is used only for anonymous memory—that is, memory not backed by any ﬁle.
Linux allows one or more swap areas to be established.
A swap area may be in either a swap ﬁle on a regular ﬁle system or a dedicated swap partition.
Each swap area consists of a series of 4-KB page slots, which are used to hold swapped pages.
Associated with each swap area is a swap map—an array of integer counters, each corresponding to a page slot in the swap area.
If the value of a counter is 0, the corresponding page slot is available.
Values greater than 0 indicate that the page slot is occupied by a swapped page.
The value of the counter indicates the number of mappings to the swapped page.
For example, a value of 3 indicates that the swapped page is mapped to three different processes (which can occur if the swapped page is storing a region of memory shared by three processes)
The data structures for swapping on Linux systems are shown in Figure 10.10
Disk drives have continued to get smaller and cheaper, so it is now economically feasible to attach many disks to a computer system.
Having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written, if the disks are operated in parallel.
Furthermore, this setup offers the potential for improving the reliability of data storage, because redundant information can be stored on multiple disks.
Thus, failure of one disk does not lead to loss of data.
A variety of disk-organization techniques, collectively called redundant arrays of independent disks (RAID), are commonly used to address the performance and reliability issues.
In the past, RAIDs composed of small, cheap disks were viewed as a cost-effective alternative to large, expensive disks.
For example, a system can have disks directly attached to its buses.
In this case, the operating system or system software can implement RAID functionality.
Alternatively, an intelligent host controller can control multiple attached disks and can implement RAID on those disks in hardware.
Finally, a storage array, or RAID array, can be used.
A RAID array is a standalone unit with its own controller, cache (usually), and disks.
It is attached to the host via one or more standard controllers (for example, FC)
This common setup allows an operating system or software without RAID functionality to have RAID-protected disks.
It is even used on systems that do have RAID software layers because of its simplicity and ﬂexibility.
Hence, the I in RAID, which once stood for “inexpensive,” now stands for “independent.”
The chance that some disk out of a set of N disks will fail is much higher than the chance that a speciﬁc single disk will fail.
Suppose that the mean time to failure of a single disk is 100,000 hours.
The solution to the problem of reliability is to introduce redundancy; we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information.
Thus, even if a disk fails, data are not lost.
The simplest (but most expensive) approach to introducing redundancy is to duplicate every disk.
With mirroring, a logical disk consists of two physical disks, and every write is carried out on both disks.
If one of the disks in the volume fails, the data can be read from the other.
Data will be lost only if the second disk fails before the ﬁrst failed disk is replaced.
You should be aware that we cannot really assume that disk failures will be independent.
Power failures and natural disasters, such as earthquakes, ﬁres, and ﬂoods, may result in damage to both disks at the same time.
Also, manufacturing defects in a batch of disks can cause correlated failures.
As disks age, the probability of failure grows, increasing the chance that a second disk will fail while the ﬁrst is being repaired.
In spite of all these considerations, however, mirrored-disk systems offer much higher reliability than do single-disk systems.
Power failures are a particular source of concern, since they occur far more frequently than do natural disasters.
Even with mirroring of disks, if writes are in progress to the same block in both disks, and power fails before both blocks are fully written, the two blocks can be in an inconsistent state.
One solution to this problem is to write one copy ﬁrst, then the next.
Another is to add a solid-state nonvolatile RAM (NVRAM) cache to the RAID array.
This write-back cache is protected from data loss during power failures, so the write can be considered complete at that point, assuming the NVRAM has some kind of error protection and correction, such as ECC or mirroring.
Now let’s consider how parallel access to multiple disks improves performance.
With disk mirroring, the rate at which read requests can be handled is doubled, since read requests can be sent to either disk (as long as both disks in a pair are functional, as is almost always the case)
The transfer rate of each read is the same as in a single-disk system, but the number of reads per unit time has doubled.
For example, if we use an array of four disks, bits i and 4 + i of each byte go to disk i.
Other levels of striping, such as bytes of a sector or sectors of a block, also are possible.
Parallelism in a disk system, as achieved through striping, has two main goals:
Increase the throughput of multiple small accesses (that is, page accesses) by load balancing.
Striping provides high data-transfer rates, but it does not improve reliability.
Numerous schemes to provide redundancy at lower cost by using disk striping combined with “parity” bits (which we describe shortly) have been proposed.
These schemes have different cost–performance trade-offs and are classiﬁed according to levels called RAID levels.
We describe the various levels here; Figure 10.11 shows them pictorially (in the ﬁgure, P indicates error-correcting bits and C indicates a second copy of the data)
In all cases depicted in the ﬁgure, four disks’ worth of data are stored, and the extra disks are used to store redundant information for failure recovery.
Memory systems have long detected certain errors by using parity bits.
Similarly, if the stored parity bit is damaged, it does not match the computed parity.
Thus, all single-bit errors are detected by the memory system.
Error-correcting schemes store two or more extra bits and can reconstruct the data if a single bit is damaged.
The idea of ECC can be used directly in disk arrays via striping of bytes across disks.
This scheme is shown in Figure 10.11(c), where the disks labeled P store the errorcorrection bits.
If one of the disks fails, the remaining bits of the byte and the associated error-correction bits can be read from other disks and used to reconstruct the damaged data.
On the negative side, RAID level 3 supports fewer I/Os per second, since every disk has to participate in every I/O request.
A further performance problem with RAID 3—and with all paritybased RAID levels—is the expense of computing and writing the parity.
This overhead results in signiﬁcantly slower writes than with non-parity RAID arrays.
To moderate this performance penalty, many RAID storage arrays include a hardware controller with dedicated parity hardware.
This controller ofﬂoads the parity computation from the CPU to the array.
The array has an NVRAM cache as well, to store the blocks while the parity is computed and to buffer the writes from the controller to the spindles.
This combination can make parity RAID almost as fast as non-parity.
In fact, a caching array doing parity RAID can outperform a non-caching non-parity RAID.
If one of the disks fails, the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk.
A block read accesses only one disk, allowing other requests to be processed by the other disks.
Thus, the data-transfer rate for each access is slower, but multiple read accesses can proceed in parallel, leading to a higher overall I/O rate.
The transfer rates for large reads are high, since all the disks can be read in parallel.
Large writes also have high transfer rates, since the data and parity can be written in parallel.
An operatingsystem write of data smaller than a block requires that the block be read, modiﬁed with the new data, and written back.
Thus, a single write requires four disk accesses: two to read the two old blocks and two to write the two new blocks.
If the added disks are initialized with blocks containing only zeros, then the parity value does not change, and the RAID set is still correct.
For each block, one of the disks stores the parity and the others store data.
For example, with an array of ﬁve disks, the parity for the nth block is stored in disk (n mod 5)+1
The nth blocks of the other four disks store actual data for that block.
This setup is shown in Figure 10.11(f), where the Ps are distributed across all the disks.
A parity block cannot store parity for blocks in the same disk, because a disk failure would result in loss of data as well as of parity, and hence the loss would not be recoverable.
Instead of parity, error-correcting codes such as the Reed–Solomon codes are used.
It is common in environments where both performance and reliability are important.
Unfortunately, like RAID 1, it doubles the number of disks needed for storage, so it is also relatively expensive.
Numerous variations have been proposed to the basic RAID schemes described here.
As a result, some confusion may exist about the exact deﬁnitions of the different RAID levels.
Consider the following layers at which RAID can be implemented.
Volume-management software can implement RAID within the kernel or at the system software layer.
In this case, the storage hardware can provide minimal features and still be part of a full RAID solution.
Only the disks directly connected to the HBA can be part of a given RAID set.
This solution is low in cost but not very ﬂexible.
The storage array can create RAID sets of various levels and can even slice these sets into smaller volumes, which are then presented to the operating system.
The operating system need only implement the ﬁle system on each of the volumes.
Arrays can have multiple connections available or can be part of a SAN, allowing multiple hosts to take advantage of the array’s features.
In this case, a device sits between the hosts and the storage.
It accepts commands from the servers and manages access to the storage.
It could provide mirroring, for example, by writing each block to two separate storage devices.
Other features, such as snapshots and replication, can be implemented at each of these levels as well.
A snapshot is a view of the ﬁle system before the last update took place.
Replication involves the automatic duplication of writes between separate sites for redundancy and disaster recovery.
In synchronous replication, each block must be written locally and remotely before the write is considered complete, whereas in asynchronous replication, the writes are grouped together and written periodically.
Asynchronous replication can result in data loss if the primary site fails, but it is faster and has no distance limitations.
The implementation of these features differs depending on the layer at which RAID is implemented.
For example, if RAID is implemented in software, then each host may need to carry out and manage its own replication.
If replication is implemented in the storage array or in the SAN interconnect, however, then whatever the host operating system or its features, the host’s data can be replicated.
One other aspect of most RAID implementations is a hot spare disk or disks.
A hot spare is not used for data but is conﬁgured to be used as a replacement in case of disk failure.
For instance, a hot spare can be used to rebuild a mirrored pair should one of the disks in the pair fail.
In this way, the RAID level can be reestablished automatically, without waiting for the failed disk to be replaced.
Allocating more than one hot spare allows more than one failure to be repaired without human intervention.
Given the many choices they have, how do system designers choose a RAID level? One consideration is rebuild performance.
If a disk fails, the time needed to rebuild its data can be signiﬁcant.
This may be an important factor if a continuous supply of data is required, as it is in high-performance or interactive database systems.
Rebuilding is easiest for RAID level 1, since data can be copied from another disk.
For the other levels, we need to access all the other disks in the array to rebuild data in a failed disk.
Rebuild times can be hours for RAID 5 rebuilds of large disk sets.
For example, how many disks should be in a given RAID set? How many bits should be protected by each parity bit? If more disks are in an array, data-transfer rates are higher, but the system is more expensive.
If more bits are protected by a parity bit, the space overhead due to parity bits is lower, but the chance that a second disk will fail before the ﬁrst failed disk is repaired is greater, and that will result in data loss.
The concepts of RAID have been generalized to other storage devices, including arrays of tapes, and even to the broadcast of data over wireless systems.
When applied to arrays of tapes, RAID structures are able to recover data even if one of the tapes in an array is damaged.
When applied to broadcast of data, a block of data is split into short units and is broadcast along with a parity unit.
If one of the units is not received for any reason, it can be reconstructed from the other units.
Commonly, tape-drive robots containing multiple tape drives will stripe data across all the drives to increase throughput and decrease backup time.
Unfortunately, RAID does not always assure that data are available for the operating system and its users.
A pointer to a ﬁle could be wrong, for example, or pointers within the ﬁle structure could be wrong.
Incomplete writes, if not properly recovered, could result in corrupt data.
Some other process could accidentally write over a ﬁle system’s structures, too.
As large as is the landscape of software and hardware bugs, that is how numerous are the potential perils for data on a system.
The Solaris ZFS ﬁle system takes an innovative approach to solving these problems through the use of checksums—a technique used to verify the.
Innovation, in an effort to provide better, faster, and less expensive solutions, frequently blurs the lines that separated previous technologies.
Unlike most other storage arrays, InServ does not require that a set of disks be conﬁgured at a speciﬁc RAID level.
Rather, each disk is broken into 256-MB “chunklets.” RAID is then applied at the chunklet level.
A disk can thus participate in multiple and various RAID levels as its chunklets are used for multiple volumes.
InServ also provides snapshots similar to those created by the WAFL ﬁle system.
The format of InServ snapshots can be read–write as well as readonly, allowing multiple hosts to mount copies of a given ﬁle system without needing their own copies of the entire ﬁle system.
Any changes a host makes in its own copy are copy-on-write and so are not reﬂected in the other copies.
On these systems, the original size is the only size, and any change requires copying data.
An administrator can conﬁgure InServ to provide a host with a large amount of logical storage that initially occupies only a small amount of physical storage.
As the host starts using the storage, unused disks are allocated to the host, up to the original logical level.
The host thus can believe that it has a large ﬁxed storage space, create its ﬁle systems there, and so on.
Disks can be added or removed from the ﬁle system by InServ without the ﬁle system’s noticing the change.
This feature can reduce the number of drives needed by hosts, or at least delay the purchase of disks until they are really needed.
These checksums are not kept with the block that is being checksummed.
Rather, they are stored with the pointer to that block.
Consider an inode — a data structure for storing ﬁle system metadata — with pointers to its data.
Within the inode is the checksum of each block of data.
If there is a problem with the data, the checksum will be incorrect, and the ﬁle system will know about it.
If the data are mirrored, and there is a block with a correct checksum and one with an incorrect checksum, ZFS will automatically update the bad block with the good one.
Similarly, the directory entry that points to the inode has a checksum for the inode.
Any problem in the inode is detected when the directory is accessed.
This checksumming takes places throughout all ZFS structures, providing a much higher level of consistency, error detection, and error correction than is found in RAID disk sets or standard ﬁle systems.
The extra overhead that is created by the checksum calculation and extra block read-modify-write cycles is not noticeable because the overall performance of ZFS is very fast.
Another issue with most RAID implementations is lack of ﬂexibility.
Consider a storage array with twenty disks divided into four sets of ﬁve disks.
Each set of ﬁve disks is a RAID level 5 set.
As a result, there are four separate volumes, each holding a ﬁle system.
But what if one ﬁle system is too large to ﬁt on a ﬁve-disk RAID level 5 set? And what if another ﬁle system needs very little space? If such factors are known ahead of time, then the disks and volumes.
Very frequently, however, disk use and requirements change over time.
Even if the storage array allowed the entire set of twenty disks to be created as one large RAID set, other issues could arise.
Several volumes of various sizes could be built on the set.
But some volume managers do not allow us to change a volume’s size.
In that case, we would be left with the same issue described above—mismatched ﬁle-system sizes.
Some volume managers allow size changes, but some ﬁle systems do not allow for ﬁle-system growth or shrinkage.
The volumes could change sizes, but the ﬁle systems would need to be recreated to take advantage of those changes.
Disks, or partitions of disks, are gathered together via RAID sets into pools of storage.
A pool can hold one or more ZFS ﬁle systems.
The entire pool’s free space is available to all ﬁle systems within that pool.
As a result, there are no artiﬁcial limits on storage use and no need to relocate ﬁle systems between volumes or resize volumes.
In Chapter 5, we introduced the write-ahead log, which requires the availability of stable storage.
By deﬁnition, information residing in stable storage is never lost.
To implement such storage, we need to replicate the required information.
We also need to coordinate the writing of updates in a way that guarantees that a failure during an update will not leave all the copies in a damaged state and that, when we are recovering from a failure, we can force all copies to a consistent and correct value, even if another failure occurs during the recovery.
In this section, we discuss how to meet these needs.
A failure occurred in the midst of transfer, so only some of the sectors were written with the new data, and the sector being written during the failure may have been corrupted.
The failure occurred before the disk write started, so the previous data values on the disk remain intact.
Whenever a failure occurs during writing of a block, the system needs to detect it and invoke a recovery procedure to restore the block to a consistent state.
To do that, the system must maintain two physical blocks for each logical block.
When the ﬁrst write completes successfully, write the same information onto the second physical block.
Declare the operation complete only after the second write completes successfully.
During recovery from a failure, each pair of physical blocks is examined.
If both are the same and no detectable error exists, then no further action is necessary.
If one block contains a detectable error then we replace its contents with the value of the other block.
If neither block contains a detectable error, but the blocks differ in content, then we replace the content of the ﬁrst block with that of the second.
This recovery procedure ensures that a write to stable storage either succeeds completely or results in no change.
We can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage.
Although having a large number of copies further reduces the probability of a failure, it is usually reasonable to simulate stable storage with only two copies.
The data in stable storage are guaranteed to be safe unless a failure destroys all the copies.
Because waiting for disk writes to complete (synchronous I/O) is time consuming, many storage arrays add NVRAM as a cache.
Since the memory is nonvolatile (it usually has battery power to back up the unit’s power), it can be trusted to store the data en route to the disks.
Writes to it are much faster than to disk, so performance is greatly improved.
Disk drives are the major secondary storage I/O devices on most computers.
Most secondary storage devices are either magnetic disks or magnetic tapes, although solid-state disks are growing in importance.
Modern disk drives are structured as large one-dimensional arrays of logical disk blocks.
Requests for disk I/O are generated by the ﬁle system and by the virtual memory system.
Each request speciﬁes the address on the disk to be referenced, in the form of a logical block number.
Disk-scheduling algorithms can improve the effective bandwidth, the average response time, and the variance in response time.
Algorithms such as SSTF, SCAN, C-SCAN, LOOK, and C-LOOK are designed to make such improvements through strategies for disk-queue ordering.
Performance of disk-scheduling algorithms can vary greatly on magnetic disks.
In contrast, because solid-state disks have no moving parts, performance varies little among algorithms, and quite often a simple FCFS strategy is used.
Some systems have utilities that scan the ﬁle system to identify fragmented ﬁles; they then move blocks around to decrease the fragmentation.
Defragmenting a badly fragmented ﬁle system can signiﬁcantly improve performance, but the system may have reduced performance while the defragmentation is in progress.
Sophisticated ﬁle systems, such as the UNIX Fast File System, incorporate many strategies to control fragmentation during space allocation so that disk reorganization is not needed.
First, a disk must be lowlevel-formatted to create the sectors on the raw hardware—new disks usually come preformatted.
Then, the disk is partitioned, ﬁle systems are created, and.
Finally, when a block is corrupted, the system must have a way to lock out that block or to replace it logically with a spare.
Because an efﬁcient swap space is a key to good performance, systems usually bypass the ﬁle system and use raw-disk access for paging I/O.
Some systems dedicate a raw-disk partition to swap space, and others use a ﬁle within the ﬁle system instead.
Still other systems allow the user or system administrator to make the decision by providing both options.
Because of the amount of storage required on large systems, disks are frequently made redundant via RAID algorithms.
These algorithms allow more than one disk to be used for a given operation and allow continued operation and even automatic recovery in the face of a disk failure.
In fact, the suitability of a storage device for random access depends on the transfer size.
The term “streaming transfer rate” denotes the rate for a data transfer that is underway, excluding the effect of access latency.
In contrast, the “effective transfer rate” is the ratio of total bytes per total seconds, including overhead time such as access latency.
Random access causes the effective transfer rate of a device to decrease, because no data are transferred during the access time.
The utilization of a device is the ratio of effective transfer rate to streaming transfer rate.
Calculate the utilization of the disk drive for each of the four transfer sizes given in part a.
Suppose that a utilization of 25 percent (or higher) is considered acceptable.
Using the performance ﬁgures given, compute the smallest transfer size for disk that gives acceptable utilization.
Complete the following sentence: A disk is a random-access device for transfers larger than bytes and is a sequentialaccess device for smaller transfers.
Compute the minimum transfer sizes that give acceptable utilization for cache, memory, and tape.
When is a tape a random-access device, and when is it a sequential-access device?
Describe a way to modify algorithms such as SCAN to ensure fairness.
Explain why fairness is an important goal in a time-sharing system.
Give three or more examples of circumstances in which it is important that the operating system be unfair in serving I/O requests.
Starting from the current head position, what is the total distance (in cylinders) that the disk arm moves to satisfy all the pending requests for each of the following disk-scheduling algorithms?
Suppose that, during a seek, the disk in Exercise 10.11 accelerates the disk arm at a constant rate for the ﬁrst half of the seek, then decelerates the disk arm at the same rate for the second half of the seek.
The distance of a seek is the number of cylinders over which the head moves.
Explain why the seek time is proportional to the square root of the seek distance.
Calculate the total seek time for each of the schedules in Exercise 10.11
Determine which schedule is the fastest (has the smallest total seek time)
The percentage speedup is the time saved divided by the original time.
What is the percentage speedup of the fastest schedule over FCFS?
What is the average rotational latency of this disk drive?
What seek distance can be covered in the time that you found for part a?
Consider the average response time (the time between the arrival of a request and the completion of that request’s service), the variation in response time, and the effective.
How does performance depend on the relative sizes of seek time and rotational latency?
For example, we can expect a cylinder containing the ﬁle-system metadata to be accessed more frequently than a cylinder containing only ﬁles.
Suppose you know that 50 percent of the requests are for a small, ﬁxed number of cylinders.
Would any of the scheduling algorithms discussed in this chapter be particularly good for this case? Explain your answer.
Propose a disk-scheduling algorithm that gives even better performance by taking advantage of this “hot spot” on the disk.
How many blocks are accessed in order to perform the following?
Assume that the system has ﬂexibility in deciding which disk organization to use for storing a particular ﬁle.
Although this quantity is called a “time,” the MTBF actually is measured in drive-hours per failure.
If a system contains 1,000 disk drives, each of which has a 750,000hour MTBF, which of the following best describes how often a drive failure will occur in that disk farm: once per thousand years, once per century, once per decade, once per year, once per month, once per week, once per day, once per hour, once per minute, or once per second?
What does this MTBF tell you about the expected lifetime of a 20-year-old?
The manufacturer guarantees a 1-million-hour MTBF for a certain model of disk drive.
What can you conclude about the number of years for which one of these drives is under warranty?
How could the operating system improve ﬁle-system performance with this knowledge?
The program will generate a random series of 1,000 cylinder requests and service them according to each of the algorithms listed above.
The program will be passed the initial position of the disk head (as a parameter on the command line) and report the total amount of head movement required by each algorithm.
The I/O size and randomness of the workload inﬂuence disk performance considerably.
The concept of a storage hierarchy has been studied for more than forty years.
For most users, the ﬁle system is the most visible aspect of an operating system.
It provides the mechanism for on-line storage of and access to both data and programs of the operating system and all the users of the computer system.
The ﬁle system consists of two distinct parts: a collection of ﬁles, each storing related data, and a directory structure, which organizes and provides information about all the ﬁles in the system.
File systems live on devices, which we described in the preceding chapter and will continue to discuss in the following one.
In this chapter, we consider the various aspects of ﬁles and the major directory structures.
We also discuss the semantics of sharing ﬁles among multiple processes, users, and computers.
Finally, we discuss ways to handle ﬁle protection, necessary when we have multiple users and we want to control who may access ﬁles and how ﬁles may be accessed.
To discuss ﬁle-system design tradeoffs, including access methods, ﬁle sharing, ﬁle locking, and directory structures.
Computers can store information on various storage media, such as magnetic disks, magnetic tapes, and optical disks.
So that the computer system will be convenient to use, the operating system provides a uniform logical view of stored information.
The operating system abstracts from the physical properties of its storage devices to deﬁne a logical storage unit, the ﬁle.
Files are mapped by the operating system onto physical devices.
These storage devices are usually nonvolatile, so the contents are persistent between system reboots.
A ﬁle is a named collection of related information that is recorded on secondary storage.
From a user’s perspective, a ﬁle is the smallest allotment of logical secondary storage; that is, data cannot be written to secondary storage unless they are within a ﬁle.
Commonly, ﬁles represent programs (both source and object forms) and data.
Files may be free form, such as text ﬁles, or may be formatted rigidly.
In general, a ﬁle is a sequence of bits, bytes, lines, or records, the meaning of which is deﬁned by the ﬁle’s creator and user.
The information in a ﬁle is deﬁned by its creator.
Many different types of information may be stored in a ﬁle—source or executable programs, numeric or text data, photos, music, video, and so on.
A ﬁle has a certain deﬁned structure, which depends on its type.
A text ﬁle is a sequence of characters organized into lines (and possibly pages)
A source ﬁle is a sequence of functions, each of which is further organized as declarations followed by executable statements.
An executable ﬁle is a series of code sections that the loader can bring into memory and execute.
A ﬁle is named, for the convenience of its human users, and is referred to by its name.
A name is usually a string of characters, such as example.c.
Some systems differentiate between uppercase and lowercase characters in names, whereas other systems do not.
When a ﬁle is named, it becomes independent of the process, the user, and even the system that created it.
For instance, one user might create the ﬁle example.c, and another user might edit that ﬁle by specifying its name.
The ﬁle’s owner might write the ﬁle to a USB disk, send it as an e-mail attachment, or copy it across a network, and it could still be called example.c on the destination system.
A ﬁle’s attributes vary from one operating system to another but typically consist of these:
The symbolic ﬁle name is the only information kept in humanreadable form.
This unique tag, usually a number, identiﬁes the ﬁle within the ﬁle system; it is the non-human-readable name for the ﬁle.
This information is needed for systems that support different types of ﬁles.
This information is a pointer to a device and to the location of the ﬁle on that device.
The current size of the ﬁle (in bytes, words, or blocks) and possibly the maximum allowed size are included in this attribute.
Access-control information determines who can do reading, writing, executing, and so on.
This information may be kept for creation, last modiﬁcation, and last use.
These data can be useful for protection, security, and usage monitoring.
Some newer ﬁle systems also support extended ﬁle attributes, including character encoding of the ﬁle and security features such as a ﬁle checksum.
Figure 11.1 illustrates a ﬁle info window on Mac OS X, which displays a ﬁle’s attributes.
The information about all ﬁles is kept in the directory structure, which also resides on secondary storage.
Typically, a directory entry consists of the ﬁle’s name and its unique identiﬁer.
It may take more than a kilobyte to record this information for each ﬁle.
In a system with many ﬁles, the size of the directory itself may be megabytes.
Because directories, like ﬁles, must be nonvolatile, they must be stored on the device and brought into memory piecemeal, as needed.
To deﬁne a ﬁle properly, we need to consider the operations that can be performed on ﬁles.
The operating system can provide system calls to create, write, read, reposition, delete, and truncate ﬁles.
Let’s examine what the operating system must do to perform each of these six basic ﬁle operations.
It should then be easy to see how other similar operations, such as renaming a ﬁle, can be implemented.
First, space in the ﬁle system must be found for the ﬁle.
Second, an entry for the new ﬁle must be made in the directory.
To write a ﬁle, we make a system call specifying both the name of the ﬁle and the information to be written to the ﬁle.
Given the name of the ﬁle, the system searches the directory to ﬁnd the ﬁle’s location.
The system must keep a write pointer to the location in the ﬁle where the next write is to take place.
The write pointer must be updated whenever a write occurs.
To read from a ﬁle, we use a system call that speciﬁes the name of the ﬁle and where (in memory) the next block of the ﬁle should be put.
Again, the directory is searched for the associated entry, and the system needs to keep a read pointer to the location in the ﬁle where the next read is to take place.
Once the read has taken place, the read pointer is updated.
Both the read and write operations use this same pointer, saving space and reducing system complexity.
Repositioning within a ﬁle need not involve any actual I/O.
This ﬁle operation is also known as a ﬁle seek.
To delete a ﬁle, we search the directory for the named ﬁle.
Having found the associated directory entry, we release all ﬁle space, so that it can be reused by other ﬁles, and erase the directory entry.
The user may want to erase the contents of a ﬁle but keep its attributes.
Rather than forcing the user to delete the ﬁle and then recreate it, this function allows all attributes to remain unchanged—except for ﬁle length—but lets the ﬁle be reset to length zero and its ﬁle space released.
These six basic operations comprise the minimal set of required ﬁle operations.
These primitive operations can then be combined to perform other ﬁle operations.
For instance, we can create a copy of a ﬁle—or copy the ﬁle to another I/O device, such as a printer or a display—by creating a new ﬁle and then reading from the old and writing to the new.
We also want to have operations that allow a user to get and set the various attributes of a ﬁle.
For example, we may want to have operations that allow a user to determine the status of a ﬁle, such as the ﬁle’s length, and to set ﬁle attributes, such as the ﬁle’s owner.
Most of the ﬁle operations mentioned involve searching the directory for the entry associated with the named ﬁle.
To avoid this constant searching, many systems require that an open() system call be made before a ﬁle is ﬁrst used.
The operating system keeps a table, called the open-ﬁle table, containing information about all open ﬁles.
When a ﬁle operation is requested, the ﬁle is speciﬁed via an index into this table, so no searching is required.
When the ﬁle is no longer being actively used, it is closed by the process, and the operating system removes its entry from the open-ﬁle table.
Some systems implicitly open a ﬁle when the ﬁrst reference to it is made.
The ﬁle is automatically closed when the job or program that opened the ﬁle terminates.
Most systems, however, require that the programmer open a ﬁle explicitly with the open() system call before that ﬁle can be used.
The open() operation takes a ﬁle name and searches the directory, copying the directory entry into the open-ﬁle table.
If the request mode is allowed, the ﬁle is opened for the process.
The open() system call typically returns a pointer to the entry in the open-ﬁle table.
This pointer, not the actual ﬁle name, is used in all I/O operations, avoiding any further searching and simplifying the system-call interface.
The implementation of the open() and close() operations is more complicated in an environment where several processes may open the ﬁle simultaneously.
This may occur in a system where several different applications open the same ﬁle at the same time.
Typically, the operating system uses two levels of internal tables: a per-process table and a system-wide table.
The perprocess table tracks all ﬁles that a process has open.
Stored in this table is information regarding the process’s use of the ﬁle.
For instance, the current ﬁle pointer for each ﬁle is found here.
Access rights to the ﬁle and accounting information can also be included.
Each entry in the per-process table in turn points to a system-wide open-ﬁle table.
Once a ﬁle has been opened by one process, the system-wide table includes an entry for the ﬁle.
When another process executes an open() call, a new entry is simply added to the process’s open-ﬁle table pointing to the appropriate entry in the system-wide table.
Typically, the open-ﬁle table also has an open count associated with each ﬁle to indicate how many processes have the ﬁle open.
Each close() decreases this open count, and when the open count reaches zero, the ﬁle is no longer in use, and the ﬁle’s entry is removed from the open-ﬁle table.
In summary, several pieces of information are associated with an open ﬁle.
This pointer is unique to each process operating on the ﬁle and therefore must be kept separate from the on-disk ﬁle attributes.
As ﬁles are closed, the operating system must reuse its open-ﬁle table entries, or it could run out of space in the table.
Multiple processes may have opened a ﬁle, and the system must wait for the last ﬁle to close before removing the open-ﬁle table entry.
The ﬁle-open count tracks the number of opens and closes and reaches zero on the last close.
Most ﬁle operations require the system to modify data within the ﬁle.
The information needed to locate the ﬁle on disk is kept in memory so that the system does not have to read it from disk for each operation.
This information is stored on the per-process table so the operating system can allow or deny subsequent I/O requests.
Some operating systems provide facilities for locking an open ﬁle (or sections of a ﬁle)
File locks allow one process to lock a ﬁle and prevent other processes from gaining access to it.
File locks are useful for ﬁles that are shared by several processes—for example, a system log ﬁle that can be accessed and modiﬁed by a number of processes in the system.
File locks provide functionality similar to reader–writer locks, covered in Section 5.7.2
A shared lock is akin to a reader lock in that several processes can acquire the lock concurrently.
An exclusive lock behaves like a writer lock; only one process at a time can acquire such a lock.
It is important to note that not all operating systems provide both types of locks: some systems only provide exclusive ﬁle locking.
In the Java API, acquiring a lock requires ﬁrst obtaining the FileChannel for the ﬁle to be locked.
The lock() method of the FileChannel is used to acquire the lock.
FileLock lock(long begin, long end, boolean shared) where begin and end are the beginning and ending positions of the region being locked.
Setting shared to true is for shared locks; setting shared to false acquires the lock exclusively.
The lock is released by invoking the release() of the FileLock returned by the lock() operation.
The program in Figure 11.2 illustrates ﬁle locking in Java.
The ﬁrst half of the ﬁle is acquired as an exclusive lock; the lock for the second half is a shared lock.
Furthermore, operating systems may provide either mandatory or advisory ﬁle-locking mechanisms.
If a lock is mandatory, then once a process acquires an exclusive lock, the operating system will prevent any other process.
For example, assume a process acquires an exclusive lock on the ﬁle system.log.
If we attempt to open system.log from another process—for example, a text editor—the operating system will prevent access until the exclusive lock is released.
This occurs even if the text editor is not written explicitly to acquire the lock.
Alternatively, if the lock is advisory, then the operating system will not prevent the text editor from acquiring access to system.log.
Rather, the text editor must be written so that it manually acquires the lock before accessing the ﬁle.
In other words, if the locking scheme is mandatory, the operating system ensures locking integrity.
For advisory locking, it is up to software developers to ensure that locks are appropriately acquired and released.
As a general rule, Windows operating systems adopt mandatory locking, and UNIX systems employ advisory locks.
The use of ﬁle locks requires the same precautions as ordinary process synchronization.
For example, programmers developing on systems with mandatory locking must be careful to hold exclusive ﬁle locks only while they are accessing the ﬁle.
Otherwise, they will prevent other processes from accessing the ﬁle as well.
Furthermore, some measures must be taken to ensure that two or more processes do not become involved in a deadlock while trying to acquire ﬁle locks.
When we design a ﬁle system—indeed, an entire operating system—we always consider whether the operating system should recognize and support ﬁle types.
If an operating system recognizes the type of a ﬁle, it can then operate on the ﬁle in reasonable ways.
For example, a common mistake occurs when a user tries to output the binary-object form of a program.
This attempt normally produces garbage; however, the attempt can succeed if the operating system has been told that the ﬁle is a binary-object program.
A common technique for implementing ﬁle types is to include the type as part of the ﬁle name.
The name is split into two parts—a name and an extension, usually separated by a period (Figure 11.3)
In this way, the user and the operating system can tell from the name alone what the type of a ﬁle is.
Most operating systems allow users to specify a ﬁle name as a sequence of characters followed by a period and terminated by an extension made up of additional characters.
The system uses the extension to indicate the type of the ﬁle and the type of operations that can be done on that ﬁle.
Only a ﬁle with a .com, .exe, or .sh extension can be executed, for instance.
The .com and .exe ﬁles are two forms of binary executable ﬁles, whereas the .sh ﬁle is a shell script containing, in ASCII format, commands to the operating system.
Application programs also use extensions to indicate ﬁle types in which they are interested.
For example, Java compilers expect source ﬁles to have a .java extension, and the Microsoft Word word processor expects its ﬁles to end with a .doc or .docx extension.
These extensions are not always required, so a user may specify a ﬁle without the extension (to save typing), and the application will look for a ﬁle with the given name and the extension it expects.
Because these extensions are not supported by the operating system, they can be considered “hints” to the applications that operate on them.
In this system, each ﬁle has a type, such as .app (for application)
Each ﬁle also has a creator attribute containing the name of the program that created it.
This attribute is set by the operating system during the create() call, so its use is enforced and supported by the system.
For instance, a ﬁle produced by a word processor has the word processor’s name as its creator.
When the user opens that ﬁle, by double-clicking the mouse on the icon representing the ﬁle, the word processor is invoked automatically and the ﬁle is loaded, ready to be edited.
The UNIX system uses a crude magic number stored at the beginning of some ﬁles to indicate roughly the type of the ﬁle—executable program, shell script, PDF ﬁle, and so on.
Not all ﬁles have magic numbers, so system features cannot be based solely on this information.
Extensions can be used or ignored by a given application, but that is up to the application’s programmer.
File types also can be used to indicate the internal structure of the ﬁle.
As mentioned in Section 11.1.3, source and object ﬁles have structures that match the expectations of the programs that read them.
For example, the operating system requires that an executable ﬁle have a speciﬁc structure so that it can determine where in memory to load the ﬁle and what the location of the ﬁrst instruction is.
Some operating systems extend this idea into a set of system-supported ﬁle structures, with sets of special operations for manipulating ﬁles with those structures.
This point brings us to one of the disadvantages of having the operating system support multiple ﬁle structures: the resulting size of the operating system is cumbersome.
If the operating system deﬁnes ﬁve different ﬁle structures, it needs to contain the code to support these ﬁle structures.
In addition, it may be necessary to deﬁne every ﬁle as one of the ﬁle types supported by the operating system.
When new applications require information structured in ways not supported by the operating system, severe problems may result.
For example, assume that a system supports two types of ﬁles: text ﬁles (composed of ASCII characters separated by a carriage return and line feed) and executable binary ﬁles.
Now, if we (as users) want to deﬁne an encrypted ﬁle to protect the contents from being read by unauthorized people, we may ﬁnd neither ﬁle type to be appropriate.
The encrypted ﬁle is not ASCII text lines but rather is (apparently) random bits.
Although it may appear to be a binary ﬁle, it is not executable.
As a result, we may have to circumvent or misuse the operating system’s ﬁle-type mechanism or abandon our encryption scheme.
Some operating systems impose (and support) a minimal number of ﬁle structures.
This approach has been adopted in UNIX, Windows, and others.
Each application program must include its own code to interpret an input ﬁle as to the appropriate structure.
However, all operating systems must support at least one structure—that of an executable ﬁle—so that the system is able to load and run programs.
Internally, locating an offset within a ﬁle can be complicated for the operating system.
Disk systems typically have a well-deﬁned block size determined by the size of a sector.
All disk I/O is performed in units of one block (physical record), and all blocks are the same size.
It is unlikely that the physical record size will exactly match the length of the desired logical record.
Packing a number of logical records into physical blocks is a common solution to this problem.
For example, the UNIX operating system deﬁnes all ﬁles to be simply streams of bytes.
Each byte is individually addressable by its offset from the beginning (or end) of the ﬁle.
In this case, the logical record size is 1 byte.
The ﬁle system automatically packs and unpacks bytes into physical disk blockssay, 512 bytes per block—as necessary.
The logical record size, physical block size, and packing technique determine how many logical records are in each physical block.
The packing can be done either by the user’s application program or by the operating system.
In either case, the ﬁle may be considered a sequence of blocks.
The conversion from logical records to physical blocks is a relatively simple software problem.
When it is used, this information must be accessed and read into computer memory.
The information in the ﬁle can be accessed in several ways.
Information in the ﬁle is processed in order, one record after the other.
This mode of access is by far the most common; for example, editors and compilers usually access ﬁles in this fashion.
Reads and writes make up the bulk of the operations on a ﬁle.
A read operation—read next()—reads the next portion of the ﬁle and automatically advances a ﬁle pointer, which tracks the I/O location.
Similarly, the write operation—write next()—appends to the end of the ﬁle and advances to the end of the newly written material (the new end of ﬁle)
Sequential access, which is depicted in Figure 11.4, is based on a tape model of a ﬁle and works as well on sequential-access devices as it does on random-access ones.
Here, a ﬁle is made up of ﬁxed-length logical records that allow programs to read and write records rapidly in no particular order.
The direct-access method is based on a disk model of a ﬁle, since disks allow random access to any ﬁle block.
There are no restrictions on the order of reading or writing for a direct-access ﬁle.
Direct-access ﬁles are of great use for immediate access to large amounts of information.
When a query concerning a particular subject arrives, we compute which block contains the answer and then read that block directly to provide the desired information.
To store information about a larger set, such as people, we might compute a hash function on the people’s names or search a small in-memory index to determine a block to read and search.
For the direct-access method, the ﬁle operations must be modiﬁed to include the block number as a parameter.
Thus, we have read(n), where n is the block number, rather than read next(), and write(n) rather than write next()
An alternative approach is to retain read next() and write next(), as with sequential access, and to add an operation position file(n) where n is the block number.
Then, to effect a read(n), we would position file(n) and then read next()
The block number provided by the user to the operating system is normally a relative block number.
A relative block number is an index relative to the beginning of the ﬁle.
The use of relative block numbers allows the operating system to decide where the ﬁle should be placed (called the allocation problem, as we discuss in Chapter 12) and helps to prevent the user from accessing portions of the ﬁle system that may not be part of her ﬁle.
Not all operating systems support both sequential and direct access for ﬁles.
Some systems allow only sequential ﬁle access; others allow only direct access.
Some systems require that a ﬁle be deﬁned as sequential or direct when it is created.
Such a ﬁle can be accessed only in a manner consistent with its declaration.
We can easily simulate sequential access on a direct-access ﬁle by simply keeping a variable cp that deﬁnes our current position, as shown in Figure 11.5
Simulating a direct-access ﬁle on a sequential-access ﬁle, however, is extremely inefﬁcient and clumsy.
Other access methods can be built on top of a direct-access method.
These methods generally involve the construction of an index for the ﬁle.
The index, like an index in the back of a book, contains pointers to the various blocks.
Figure 11.5 Simulation of sequential access on a direct-access ﬁle.
For example, a retail-price ﬁle might list the universal product codes (UPCs) for items, with the associated prices.
By keeping the ﬁle sorted by UPC, we can deﬁne an index consisting of the ﬁrst UPC in each block.
To ﬁnd the price of a particular item, we can make a binary search of the index.
From this search, we learn exactly which block contains the desired record and access that block.
This structure allows us to search a large ﬁle doing little I/O.
With large ﬁles, the index ﬁle itself may become too large to be kept in memory.
One solution is to create an index for the index ﬁle.
The primary index ﬁle contains pointers to secondary index ﬁles, which point to the actual data items.
For example, IBM’s indexed sequential-access method (ISAM) uses a small master index that points to disk blocks of a secondary index.
The secondary index blocks point to the actual ﬁle blocks.
To ﬁnd a particular item, we ﬁrst make a binary search of the master index, which provides the block number of the secondary index.
This block is read in, and again a binary search is used to ﬁnd the block containing the desired record.
In this way, any record can be located from its key by at most two direct-access reads.
Figure 11.6 shows a similar situation as implemented by VMS index and relative ﬁles.
There are typically thousands, millions, even billions of ﬁles within a computer.
Files are stored on random-access storage devices, including hard disks, optical disks, and solid-state (memory-based) disks.
A storage device can be used in its entirety for a ﬁle system.
For example, a disk can be partitioned into quarters, and each quarter can hold a separate ﬁle system.
Storage devices can also be collected together into RAID sets that provide protection from the failure of a single disk (as described in Section 10.7)
Sometimes, disks are subdivided and also collected into RAID sets.
Partitioning is useful for limiting the sizes of individual ﬁle systems, putting multiple ﬁle-system types on the same device, or leaving part of the device available for other uses, such as swap space or unformatted (raw) disk space.
A ﬁle system can be created on each of these parts of the disk.
Any entity containing a ﬁle system is generally known as a volume.
The volume may be a subset of a device, a whole device, or multiple devices linked together into a RAID set.
Each volume can be thought of as a virtual disk.
Volumes can also store multiple operating systems, allowing a system to boot and run more than one operating system.
Each volume that contains a ﬁle system must also contain information about the ﬁles in the system.
This information is kept in entries in a device directory or volume table of contents.
The device directory (more commonly known simply as the directory) records information—such as name, location, size, and type—for all ﬁles on that volume.
As we have just seen, a general-purpose computer system has multiple storage devices, and those devices can be sliced up into volumes that hold ﬁle systems.
Computer systems may have zero or more ﬁle systems, and the ﬁle systems may be of varying types.
For example, a typical Solaris system may have dozens of ﬁle systems of a dozen different types, as shown in the ﬁle system list in Figure 11.8
It is worth noting, though, that there are many special-purpose ﬁle systems.
Consider the types of ﬁle systems in the Solaris example mentioned above:
Even within a ﬁle system, it is useful to segregate ﬁles into groups and manage and act on those groups.
In the remainder of this section, we explore the topic of directory structure.
The directory can be viewed as a symbol table that translates ﬁle names into their directory entries.
If we take such a view, we see that the directory itself can be organized in many ways.
The organization must allow us to insert entries, to delete entries, to search for a named entry, and to list all the entries in the directory.
In this section, we examine several schemes for deﬁning the logical structure of the directory system.
When considering a particular directory structure, we need to keep in mind the operations that are to be performed on a directory:
We need to be able to search a directory structure to ﬁnd the entry for a particular ﬁle.
Since ﬁles have symbolic names, and similar names may indicate a relationship among ﬁles, we may want to be able to ﬁnd all ﬁles whose names match a particular pattern.
New ﬁles need to be created and added to the directory.
When a ﬁle is no longer needed, we want to be able to remove.
We need to be able to list the ﬁles in a directory and the contents of the directory entry for each ﬁle in the list.
Because the name of a ﬁle represents its contents to its users, we must be able to change the name when the contents or use of the ﬁle changes.
Renaming a ﬁle may also allow its position within the directory structure to be changed.
We may wish to access every directory and every ﬁle within a directory structure.
For reliability, it is a good idea to save the contents and structure of the entire ﬁle system at regular intervals.
Often, we do this by copying all ﬁles to magnetic tape.
This technique provides a backup copy in case of system failure.
In addition, if a ﬁle is no longer in use, the ﬁle can be copied to tape and the disk space of that ﬁle released for reuse by another ﬁle.
In the following sections, we describe the most common schemes for deﬁning the logical structure of a directory.
All ﬁles are contained in the same directory, which is easy to support and understand (Figure 11.9)
A single-level directory has signiﬁcant limitations, however, when the number of ﬁles increases or when the system has more than one user.
Since all ﬁles are in the same directory, they must have unique names.
Fortunately, most ﬁle systems support ﬁle names of up to 255 characters, so it is relatively easy to select unique ﬁle names.
Even a single user on a single-level directory may ﬁnd it difﬁcult to remember the names of all the ﬁles as the number of ﬁles increases.
It is not uncommon for a user to have hundreds of ﬁles on one computer system and an equal number of additional ﬁles on another system.
Keeping track of so many ﬁles is a daunting task.
As we have seen, a single-level directory often leads to confusion of ﬁle names among different users.
The standard solution is to create a separate directory for each user.
In the two-level directory structure, each user has his own user ﬁle directory (UFD)
The UFDs have similar structures, but each lists only the ﬁles of a single user.
When a user job starts or a user logs in, the system’s master ﬁle directory (MFD) is searched.
The MFD is indexed by user name or account number, and each entry points to the UFD for that user (Figure 11.10)
When a user refers to a particular ﬁle, only his own UFD is searched.
Thus, different users may have ﬁles with the same name, as long as all the ﬁle names within each UFD are unique.
To create a ﬁle for a user, the operating system searches only that user’s UFD to ascertain whether another ﬁle of that name exists.
To delete a ﬁle, the operating system conﬁnes its search to the local UFD; thus, it cannot accidentally delete another user’s ﬁle that has the same name.
The user directories themselves must be created and deleted as necessary.
A special system program is run with the appropriate user name and account information.
The program creates a new UFD and adds an entry for it to the MFD.
The execution of this program might be restricted to system administrators.
The allocation of disk space for user directories can be handled with the techniques discussed in Chapter 12 for ﬁles themselves.
Although the two-level directory structure solves the name-collision problem, it still has disadvantages.
Isolation is an advantage when the users are completely independent but is a disadvantage when the users want to cooperate on some task and to access one another’s ﬁles.
Some systems simply do not allow local user ﬁles to be accessed by other users.
If access is to be permitted, one user must have the ability to name a ﬁle in another user’s directory.
To name a particular ﬁle uniquely in a two-level directory, we must give both the user name and the ﬁle name.
Specifying a user name and a ﬁle name deﬁnes a path in the tree from the root (the MFD) to a leaf (the speciﬁed ﬁle)
Thus, a user name and a ﬁle name deﬁne a path name.
To name a ﬁle uniquely, a user must know the path name of the ﬁle desired.
For example, if user A wishes to access her own test ﬁle named test.txt, she can simply refer to test.txt.
To access the ﬁle named test.txt of user B (with directory-entry name userb), however, she might have to refer to /userb/test.txt.
Every system has its own syntax for naming ﬁles in directories other than the user’s own.
Additional syntax is needed to specify the volume of a ﬁle.
For instance, in Windows a volume is speciﬁed by a letter followed by a colon.
Some systems go even further and separate the volume, directory name, and ﬁle name parts of the speciﬁcation.
Other systems—such as UNIX and Linux—simply treat the volume name as part of the directory name.
The ﬁrst name given is that of the volume, and the rest is the directory and ﬁle.
For instance, /u/pbg/testmight specify volume u, directory pbg, and ﬁle test.
The standard solution is to complicate the search procedure slightly.
A special user directory is deﬁned to contain the system ﬁles (for example, user 0)
Whenever a ﬁle name is given to be loaded, the operating system ﬁrst searches the local UFD.
If it is not found, the system automatically searches the special user directory that contains the system ﬁles.
The sequence of directories searched when a ﬁle is named is called the search path.
The search path can be extended to contain an unlimited list of directories to search when a command name is given.
This method is the one most used in UNIX and Windows.
Systems can also be designed so that each user has his own search path.
Once we have seen how to view a two-level directory as a two-level tree, the natural generalization is to extend the directory structure to a tree of arbitrary height (Figure 11.11)
This generalization allows users to create their own subdirectories and to organize their ﬁles accordingly.
The tree has a root directory, and every ﬁle in the system has a unique path name.
A directory (or subdirectory) contains a set of ﬁles or subdirectories.
A directory is simply another ﬁle, but it is treated in a special way.
Special system calls are used to create and delete directories.
The current directory should contain most of the ﬁles that are of current interest to the process.
When reference is made to a ﬁle, the current directory is searched.
If a ﬁle is needed that is not in the current directory, then the user usually must.
To change directories, a system call is provided that takes a directory name as a parameter and uses it to redeﬁne the current directory.
Thus, the user can change her current directory whenever she wants.
From one change directory() system call to the next, all open() system calls search the current directory for the speciﬁed ﬁle.
Note that the search path may or may not contain a special entry that stands for “the current directory.”
The initial current directory of a user’s login shell is designated when the user job starts or the user logs in.
The operating system searches the accounting ﬁle (or some other predeﬁned location) to ﬁnd an entry for this user (for accounting purposes)
In the accounting ﬁle is a pointer to (or the name of) the user’s initial directory.
This pointer is copied to a local variable for this user that speciﬁes the user’s initial current directory.
The current directory of any subprocess is usually the current directory of the parent when it was spawned.
Path names can be of two types: absolute and relative.
An absolute path name begins at the root and follows a path down to the speciﬁed ﬁle, giving the directory names on the path.
A relative path name deﬁnes a path from the current directory.
Allowing a user to deﬁne her own subdirectories permits her to impose a structure on her ﬁles.
This structure might result in separate directories for ﬁles associated with different topics (for example, a subdirectory was created to hold the text of this book) or different forms of information (for example, the directory programs may contain source programs; the directory bin may store all the binaries)
An interesting policy decision in a tree-structured directory concerns how to handle the deletion of a directory.
If a directory is empty, its entry in the directory that contains it can simply be deleted.
However, suppose the directory to be deleted is not empty but contains several ﬁles or subdirectories.
Some systems will not delete a directory unless it is empty.
Thus, to delete a directory, the user must ﬁrst delete all the ﬁles in that directory.
If any subdirectories exist, this procedure must be applied recursively to them, so that they can be deleted also.
This approach can result in a substantial amount of work.
An alternative approach, such as that taken by the UNIX rm command, is to provide an option: when a request is made to delete a directory, all that directory’s ﬁles and subdirectories are also to be deleted.
Either approach is fairly easy to implement; the choice is one of policy.
The latter policy is more convenient, but it is also more dangerous, because an entire directory structure can be removed with one command.
If that command is issued in error, a large number of ﬁles and directories will need to be restored (assuming a backup exists)
With a tree-structured directory system, users can be allowed to access, in addition to their ﬁles, the ﬁles of other users.
For example, user B can access a ﬁle of user A by specifying its path names.
User B can specify either an absolute or a relative path name.
Alternatively, user B can change her current directory to be user A’s directory and access the ﬁle by its ﬁle names.
Consider two programmers who are working on a joint project.
The ﬁles associated with that project can be stored in a subdirectory, separating them from other projects and ﬁles of the two programmers.
But since both programmers are equally responsible for the project, both want the subdirectory to be in their own directories.
A shared directory or ﬁle exists in the ﬁle system in two (or more) places at once.
A tree structure prohibits the sharing of ﬁles or directories.
An acyclic graph —that is, a graph with no cycles—allows directories to share subdirectories and ﬁles (Figure 11.12)
The same ﬁle or subdirectory may be in two different directories.
The acyclic graph is a natural generalization of the tree-structured directory scheme.
It is important to note that a shared ﬁle (or directory) is not the same as two copies of the ﬁle.
With two copies, each programmer can view the copy rather than the original, but if one programmer changes the ﬁle, the changes will not appear in the other’s copy.
With a shared ﬁle, only one actual ﬁle exists, so any changes made by one person are immediately visible to the other.
Sharing is particularly important for subdirectories; a new ﬁle created by one person will automatically appear in all the shared subdirectories.
When people are working as a team, all the ﬁles they want to share can be put into one directory.
The UFD of each team member will contain this directory of shared ﬁles as a subdirectory.
Even in the case of a single user, the user’s ﬁle organization may require that some ﬁle be placed in different subdirectories.
For example, a program written for a particular project should be both in the directory of all programs and in the directory for that project.
Shared ﬁles and subdirectories can be implemented in several ways.
A common way, exempliﬁed by many of the UNIX systems, is to create a new directory entry called a link.
For example, a link may be implemented as an absolute or a relative path name.
When a reference to a ﬁle is made, we search the directory.
If the directory entry is marked as a link, then the name of the real ﬁle is included in the link information.
We resolve the link by using that path name to locate the real ﬁle.
Links are easily identiﬁed by their format in the directory entry (or by having a special type on systems that support types) and are effectively indirect pointers.
The operating system ignores these links when traversing directory trees to preserve the acyclic structure of the system.
Another common approach to implementing shared ﬁles is simply to duplicate all information about them in both sharing directories.
Consider the difference between this approach and the creation of a link.
The link is clearly different from the original directory entry; thus, the two are not equal.
Duplicate directory entries, however, make the original and the copy indistinguishable.
A major problem with duplicate directory entries is maintaining consistency when a ﬁle is modiﬁed.
An acyclic-graph directory structure is more ﬂexible than a simple tree structure, but it is also more complex.
Consequently, distinct ﬁle names may refer to the same ﬁle.
This situation is similar to the aliasing problem for programming languages.
If we are trying to traverse the entire ﬁle system—to ﬁnd a ﬁle, to accumulate statistics on all ﬁles, or to copy all ﬁles to backup storage—this problem becomes signiﬁcant, since we do not want to traverse shared structures more than once.
When can the space allocated to a shared ﬁle be deallocated and reused? One possibility is to remove the ﬁle whenever anyone deletes it, but this action may leave dangling pointers to the now-nonexistent ﬁle.
Worse, if the remaining ﬁle pointers contain actual disk addresses, and the space is subsequently reused for other ﬁles, these dangling pointers may point into the middle of other ﬁles.
In a system where sharing is implemented by symbolic links, this situation is somewhat easier to handle.
The deletion of a link need not affect the original ﬁle; only the link is removed.
If the ﬁle entry itself is deleted, the space for the ﬁle is deallocated, leaving the links dangling.
We can search for these links and remove them as well, but unless a list of the associated links is kept with each ﬁle, this search can be expensive.
Alternatively, we can leave the links until an attempt is made to use them.
At that time, we can determine that the ﬁle of the name given by the link does not exist and can fail to resolve the link name; the access is treated just as with any other illegal ﬁle name.
In this case, the system designer should consider carefully what to do when a ﬁle is deleted and another ﬁle of the same name is created, before a symbolic link to the original ﬁle is used.
In the case of UNIX, symbolic links are left when a ﬁle is deleted, and it is up to the user to realize that the original ﬁle is gone or has been replaced.
Another approach to deletion is to preserve the ﬁle until all references to it are deleted.
To implement this approach, we must have some mechanism for determining that the last reference to the ﬁle has been deleted.
We could keep a list of all references to a ﬁle (directory entries or symbolic links)
When a link or a copy of the directory entry is established, a new entry is added to the ﬁle-reference list.
When a link or directory entry is deleted, we remove its entry on the list.
The ﬁle is deleted when its ﬁle-reference list is empty.
The trouble with this approach is the variable and potentially large size of the ﬁle-reference list.
However, we really do not need to keep the entire list—we need to keep only a count of the number of references.
Adding a new link or directory entry increments the reference count.
When the count is 0, the ﬁle can be deleted; there are no remaining references to it.
The UNIX operating system uses this approach for nonsymbolic links (or hard links), keeping a reference count in the ﬁle information block (or inode; see Section A.7.2)
By effectively prohibiting multiple references to directories, we maintain an acyclic-graph structure.
To avoid problems such as the ones just discussed, some systems simply do not allow shared directories or links.
A serious problem with using an acyclic-graph structure is ensuring that there are no cycles.
If we start with a two-level directory and allow users to create subdirectories, a tree-structured directory results.
It should be fairly easy to see that simply adding new ﬁles and subdirectories to an existing tree-structured directory preserves the tree-structured nature.
However, when we add links, the tree structure is destroyed, resulting in a simple graph structure (Figure 11.13)
The primary advantage of an acyclic graph is the relative simplicity of the algorithms to traverse the graph and to determine when there are no more references to a ﬁle.
We want to avoid traversing shared sections of an acyclic graph twice, mainly for performance reasons.
If we have just searched a major shared subdirectory for a particular ﬁle without ﬁnding it, we want to avoid searching that subdirectory again; the second search would be a waste of time.
If cycles are allowed to exist in the directory, we likewise want to avoid searching any component twice, for reasons of correctness as well as performance.
A poorly designed algorithm might result in an inﬁnite loop continually searching through the cycle and never terminating.
A similar problem exists when we are trying to determine when a ﬁle can be deleted.
With acyclic-graph directory structures, a value of 0 in the reference count means that there are no more references to the ﬁle or directory, and the ﬁle can be deleted.
However, when cycles exist, the reference count may not be 0 even when it is no longer possible to refer to a directory or ﬁle.
This anomaly results from the possibility of self-referencing (or a cycle) in the directory structure.
In this case, we generally need to use a garbage collection scheme to determine when the last reference has been deleted and the disk space can be reallocated.
Garbage collection involves traversing the entire ﬁle system, marking everything that can be accessed.
Then, a second pass collects everything that is not marked onto a list of free space.
A similar marking procedure can be used to ensure that a traversal or search will cover everything in the ﬁle system once and only once.
Garbage collection for a disk-based ﬁle system, however, is extremely time consuming and is thus seldom attempted.
Garbage collection is necessary only because of possible cycles in the graph.
Thus, an acyclic-graph structure is much easier to work with.
The difﬁculty is to avoid cycles as new links are added to the structure.
How do we know when a new link will complete a cycle? There are algorithms to detect cycles in graphs; however, they are computationally expensive, especially when the graph is on disk storage.
A simpler algorithm in the special case of directories and links is to bypass links during directory traversal.
Just as a ﬁle must be opened before it is used, a ﬁle system must be mounted before it can be available to processes on the system.
More speciﬁcally, the directory structure may be built out of multiple volumes, which must be mounted to make them available within the ﬁle-system name space.
The operating system is given the name of the device and the mount point—the location within the ﬁle structure where the ﬁle system is to be attached.
Some operating systems require that a ﬁle system type be provided, while others inspect the structures of the device and determine the type of ﬁle system.
For instance, on a UNIX system, a ﬁle system containing a user’s home directories might be mounted as /home; then, to access the directory structure within that ﬁle system, we could precede the directory names with /home, as in /home/jane.
Mounting that ﬁle system under /users would result in the path name /users/jane, which we could use to reach the same directory.
Next, the operating system veriﬁes that the device contains a valid ﬁle system.
It does so by asking the device driver to read the device directory and verifying that the directory has the expected format.
Finally, the operating system notes in its directory structure that a ﬁle system is mounted at the speciﬁed mount point.
This scheme enables the operating system to traverse its directory structure, switching among ﬁle systems, and even ﬁle systems of varying types, as appropriate.
To illustrate ﬁle mounting, consider the ﬁle system depicted in Figure 11.14, where the triangles represent subtrees of directories that are of interest.
At this point, only the ﬁles on the existing ﬁle system can be accessed.
Figure 11.15 shows the effects of mounting the volume residing on /device/dsk over /users.
If the volume is unmounted, the ﬁle system is restored to the situation depicted in Figure 11.14
For example, a system may disallow a mount over a directory that contains ﬁles; or it may make the mounted ﬁle system available at that directory and obscure the directory’s existing ﬁles until the ﬁle system is unmounted, terminating the use of the ﬁle system and allowing access to the original ﬁles in that directory.
As another example, a system may allow the same ﬁle system to be mounted repeatedly, at different mount points; or it may only allow one mount per ﬁle system.
Consider the actions of the Mac OS X operating system.
Whenever the system encounters a disk for the ﬁrst time (either at boot time or while the system is running), the Mac OS X operating system searches for a ﬁle system on the device.
If it ﬁnds one, it automatically mounts the ﬁle system under the /Volumes directory, adding a folder icon labeled with the name of the ﬁle system (as stored in the device directory)
The user is then able to click on the icon and thus display the newly mounted ﬁle system.
The Microsoft Windows family of operating systems maintains an extended two-level directory structure, with devices and volumes assigned drive letters.
Volumes have a general graph directory structure associated with the drive letter.
The more recent versions of Windows allow a ﬁle system to be mounted anywhere in the directory tree, just as UNIX does.
Windows operating systems automatically discover all devices and mount all located ﬁle systems at boot time.
In some systems, like UNIX, the mount commands are explicit.
A system conﬁguration ﬁle contains a list of devices and mount points for automatic mounting at boot time, but other mounts may be executed manually.
In the previous sections, we explored the motivation for ﬁle sharing and some of the difﬁculties involved in allowing users to share ﬁles.
Such ﬁle sharing is very desirable for users who want to collaborate and to reduce the effort required to achieve a computing goal.
Therefore, user-oriented operating systems must accommodate the need to share ﬁles in spite of the inherent difﬁculties.
In this section, we examine more aspects of ﬁle sharing.
We begin by discussing general issues that arise when multiple users share ﬁles.
Once multiple users are allowed to share ﬁles, the challenge is to extend sharing to multiple ﬁle systems, including remote ﬁle systems; we discuss that challenge as well.
Finally, we consider what to do about conﬂicting actions occurring on shared ﬁles.
For instance, if multiple users are writing to a ﬁle, should all the writes be allowed to occur, or should the operating system protect the users’ actions from one another?
When an operating system accommodates multiple users, the issues of ﬁle sharing, ﬁle naming, and ﬁle protection become preeminent.
Given a directory structure that allows ﬁles to be shared by users, the system must mediate the ﬁle sharing.
The system can either allow a user to access the ﬁles of other users by default or require that a user speciﬁcally grant access to the ﬁles.
These are the issues of access control and protection, which are covered in Section 11.6
To implement sharing and protection, the system must maintain more ﬁle and directory attributes than are needed on a single-user system.
Although many approaches have been taken to meet this requirement, most systems have evolved to use the concepts of ﬁle (or directory) owner (or user) and group.
The owner is the user who can change attributes and grant access and who has.
The group attribute deﬁnes a subset of users who can share access to the ﬁle.
For example, the owner of a ﬁle on a UNIX system can issue all operations on a ﬁle, while members of the ﬁle’s group can execute one subset of those operations, and all other users can execute another subset of operations.
Exactly which operations can be executed by group members and other users is deﬁnable by the ﬁle’s owner.
More details on permission attributes are included in the next section.
The owner and group IDs of a given ﬁle (or directory) are stored with the other ﬁle attributes.
When a user requests an operation on a ﬁle, the user ID can be compared with the owner attribute to determine if the requesting user is the owner of the ﬁle.
The system then applies those permissions to the requested operation and allows or denies it.
Many systems have multiple local ﬁle systems, including volumes of a single disk or multiple volumes on multiple attached disks.
In these cases, the ID checking and permission matching are straightforward, once the ﬁle systems are mounted.
With the advent of networks (Chapter 17), communication among remote computers became possible.
Networking allows the sharing of resources spread across a campus or even around the world.
One obvious resource to share is data in the form of ﬁles.
Through the evolution of network and ﬁle technology, remote ﬁle-sharing methods have changed.
The ﬁrst implemented method involves manually transferring ﬁles between machines via programs like ftp.
The second major method uses a distributed ﬁle system (DFS) in which remote directories are visible from a local machine.
In some ways, the third method, the World Wide Web, is a reversion to the ﬁrst.
A browser is needed to gain access to the remote ﬁles, and separate operations (essentially a wrapper for ftp) are used to transfer ﬁles.
Increasingly, cloud computing (Section 1.11.7) is being used for ﬁle sharing as well.
Anonymous access allows a user to transfer ﬁles without having an account on the remote system.
The World Wide Web uses anonymous ﬁle exchange almost exclusively.
This integration adds complexity, as we describe in this section.
Remote ﬁle systems allow a computer to mount one or more ﬁle systems from one or more remote machines.
In this case, the machine containing the ﬁles is the server, and the machine seeking access to the ﬁles is the client.
Generally, the server declares that a resource is available to clients and speciﬁes exactly which resource (in this case, which ﬁles) and exactly which clients.
A server can serve multiple clients, and a client can use multiple servers, depending on the implementation details of a given client–server facility.
The server usually speciﬁes the available ﬁles on a volume or directory level.
A client can be speciﬁed by a network name or other identiﬁer, such as an IP address, but these can be spoofed, or imitated.
As a result of spooﬁng, an unauthorized client could be allowed access to the server.
More secure solutions include secure authentication of the client via encrypted keys.
Unfortunately, with security come many challenges, including ensuring compatibility of the client and server (they must use the same encryption algorithms) and security of key exchanges (intercepted keys could again allow unauthorized access)
Because of the difﬁculty of solving these problems, unsecure authentication methods are most commonly used.
In the case of UNIX and its network ﬁle system (NFS), authentication takes place via the client networking information, by default.
In this scheme, the user’s IDs on the client and server must match.
If they do not, the server will be unable to determine access rights to ﬁles.
Access is thus granted or denied based on incorrect authentication information.
The server must trust the client to present the correct user ID.
That is, many servers can provide ﬁles to many clients.
In fact, a given machine can be both a server to some NFS clients and a client of other NFS servers.
Once the remote ﬁle system is mounted, ﬁle operation requests are sent on behalf of the user across the network to the server via the DFS protocol.
Typically, a ﬁle-open request is sent along with the ID of the requesting user.
The server then applies the standard access checks to determine if the user has credentials to access the ﬁle in the mode requested.
If it is allowed, a ﬁle handle is returned to the client application, and the application then can perform read, write, and other operations on the ﬁle.
The operating system may apply semantics similar to those for a local ﬁle-system mount or may use different semantics.
To make client–server systems easier to manage, distributed information systems, also known as distributed naming services, provide uniﬁed access to the information needed for remote computing.
Before DNS became widespread, ﬁles containing the same information were sent via e-mail or ftp between all networked hosts.
Obviously, this methodology was not scalable! DNS is further discussed in Section 17.4.1
Other distributed information systems provide user name/password/user ID/group ID space for a distributed facility.
Sun Microsystems (now part of Oracle Corporation) introduced yellow pages (since renamed network information service, or NIS), and most of the industry adopted its use.
It centralizes storage of user names, host names, printer information, and the like.
Unfortunately, it uses unsecure authentication methods, including sending user passwords unencrypted (in clear text) and identifying hosts by IP address.
Sun’s NIS+ was a much more secure replacement for NIS but was much more complicated and was not widely adopted.
In the case of Microsoft’s common Internet ﬁle system (CIFS), network information is used in conjunction with user authentication (user name and password) to create a network login that the server uses to decide whether to allow or deny access to a requested ﬁle system.
For this authentication to be valid, the user names must match from machine to machine (as with NFS)
Microsoft uses active directory as a distributed naming structure to provide a single name space for users.
Once established, the distributed naming facility is used by all clients and servers to authenticate users.
The industry is moving toward use of the lightweight directory-access protocol (LDAP) as a secure distributed naming mechanism.
Oracle Solaris and most other major operating systems include LDAP and allow it to be employed for user authentication as well as system-wide retrieval of information, such as availability of printers.
Conceivably, one distributed LDAP directory could be used by an organization to store all user and resource information for all the organization’s computers.
The result would be secure single sign-on for users, who would enter their authentication information once for access to all computers within the organization.
Local ﬁle systems can fail for a variety of reasons, including failure of the disk containing the ﬁle system, corruption of the directory structure or other disk-management information (collectively called metadata), disk-controller failure, cable failure, and host-adapter failure.
Many of these failures will cause a host to crash and an error condition to be displayed, and human intervention will be required to repair the damage.
Because of the complexity of network systems and the required interactions between remote machines, many more problems can interfere with the proper operation of remote ﬁle systems.
In the case of networks, the network can be interrupted between two hosts.
Such interruptions can result from hardware failure, poor hardware conﬁguration, or networking implementation issues.
Although some networks have built-in resiliency, including multiple paths between hosts, many do not.
Any single failure can thus interrupt the ﬂow of DFS commands.
Consider a client in the midst of using a remote ﬁle system.
It has ﬁles open from the remote host; among other activities, it may be performing directory lookups to open ﬁles, reading or writing data to ﬁles, and closing ﬁles.
Now consider a partitioning of the network, a crash of the server, or even a scheduled shutdown of the server.
This scenario is rather common, so it would not be appropriate for the client system to act as it would if a local ﬁle system were lost.
Rather, the system can either terminate all operations to the lost server or delay operations until the.
These failure semantics are deﬁned and implemented as part of the remote-ﬁle-system protocol.
Termination of all operations can result in users’ losing data—and patience.
Thus, most DFS protocols either enforce or allow delaying of ﬁle-system operations to remote hosts, with the hope that the remote host will become available again.
To implement this kind of recovery from failure, some kind of state information may be maintained on both the client and the server.
If both server and client maintain knowledge of their current activities and open ﬁles, then they can seamlessly recover from a failure.
In the situation where the server crashes but must recognize that it has remotely mounted exported ﬁle systems and opened ﬁles, NFS takes a simple approach, implementing a stateless DFS.
In essence, it assumes that a client request for a ﬁle read or write would not have occurred unless the ﬁle system had been remotely mounted and the ﬁle had been previously open.
The NFS protocol carries all the information needed to locate the appropriate ﬁle and perform the requested operation.
Similarly, it does not track which clients have the exported volumes mounted, again assuming that if a request comes in, it must be legitimate.
While this stateless approach makes NFS resilient and rather easy to implement, it also makes it unsecure.
For example, forged read or write requests could be allowed by an NFS server.
These issues are addressed in the industry standard NFS Version 4, in which NFS is made stateful to improve its security, performance, and functionality.
Consistency semantics represent an important criterion for evaluating any ﬁle system that supports ﬁle sharing.
These semantics specify how multiple users of a system are to access a shared ﬁle simultaneously.
In particular, they specify when modiﬁcations of data by one user will be observable by other users.
These semantics are typically implemented as code with the ﬁle system.
However, the complex algorithms of that chapter tend not to be implemented in the case of ﬁle I/O because of the great latencies and slow transfer rates of disks and networks.
For example, performing an atomic transaction to a remote disk could involve several network communications, several disk reads and writes, or both.
Systems that attempt such a full set of functionalities tend to perform poorly.
A successful implementation of complex sharing semantics can be found in the Andrew ﬁle system.
For the following discussion, we assume that a series of ﬁle accesses (that is, reads and writes) attempted by a user to the same ﬁle is always enclosed between the open() and close() operations.
The series of accesses between the open() and close() operations makes up a ﬁle session.
To illustrate the concept, we sketch several prominent examples of consistency semantics.
The UNIX ﬁle system (Chapter 17) uses the following consistency semantics:
Writes to an open ﬁle by a user are visible immediately to other users who have this ﬁle open.
One mode of sharing allows users to share the pointer of current location into the ﬁle.
Thus, the advancing of the pointer by one user affects all sharing users.
Here, a ﬁle has a single image that interleaves all accesses, regardless of their origin.
In the UNIX semantics, a ﬁle is associated with a single physical image that is accessed as an exclusive resource.
Contention for this single image causes delays in user processes.
The Andrew ﬁle system (OpenAFS) uses the following consistency semantics:
Writes to an open ﬁle by a user are not visible immediately to other users that have the same ﬁle open.
Once a ﬁle is closed, the changes made to it are visible only in sessions starting later.
Already open instances of the ﬁle do not reﬂect these changes.
According to these semantics, a ﬁle may be associated temporarily with several (possibly different) images at the same time.
Consequently, multiple users are allowed to perform both read and write accesses concurrently on their images of the ﬁle, without delay.
Once a ﬁle is declared as shared by its creator, it cannot be modiﬁed.
An immutable ﬁle has two key properties: its name may not be reused, and its contents may not be altered.
Thus, the name of an immutable ﬁle signiﬁes that the contents of the ﬁle are ﬁxed.
The implementation of these semantics in a distributed system (Chapter 17) is simple, because the sharing is disciplined (read-only)
When information is stored in a computer system, we want to keep it safe from physical damage (the issue of reliability) and improper access (the issue of protection)
Many computers have systems programs that automatically (or through computer-operator intervention) copy disk ﬁles to tape at regular intervals (once per day or week or month) to maintain a copy should a ﬁle system be accidentally destroyed.
File systems can be damaged by hardware problems (such as errors in reading or writing), power surges or failures, head crashes, dirt, temperature extremes, and vandalism.
Bugs in the ﬁle-system software can also cause ﬁle contents to be lost.
For a single-user laptop system, we might provide protection by locking the computer in a desk drawer or ﬁle cabinet.
In a larger multiuser system, however, other mechanisms are needed.
The need to protect ﬁles is a direct result of the ability to access ﬁles.
Systems that do not permit access to the ﬁles of other users do not need protection.
Protection mechanisms provide controlled access by limiting the types of ﬁle access that can be made.
Access is permitted or denied depending on several factors, one of which is the type of access requested.
Delete the ﬁle and free its space for possible reuse.
Other operations, such as renaming, copying, and editing the ﬁle, may also be controlled.
For many systems, however, these higher-level functions may be implemented by a system program that makes lower-level system calls.
For instance, copying a ﬁle may be implemented simply by a sequence of read requests.
In this case, a user with read access can also cause the ﬁle to be copied, printed, and so on.
Each has advantages and disadvantages and must be appropriate for its intended application.
A small computer system that is used by only a few members of a research group, for example, may not need the same types of protection as a large corporate computer that is used for research, ﬁnance, and personnel operations.
The most common approach to the protection problem is to make access dependent on the identity of the user.
Different users may need different types of access to a ﬁle or directory.
The most general scheme to implement identitydependent access is to associate with each ﬁle and directory an access-control list (ACL) specifying user names and the types of access allowed for each user.
When a user requests access to a particular ﬁle, the operating system checks the access list associated with that ﬁle.
If that user is listed for the requested access, the access is allowed.
Otherwise, a protection violation occurs, and the user job is denied access to the ﬁle.
This approach has the advantage of enabling complex access methodologies.
If we want to allow everyone to read a ﬁle, we must list all users with read access.
Constructing such a list may be a tedious and unrewarding task, especially if we do not know in advance the list of users in the system.
The directory entry, previously of ﬁxed size, now must be of variable size, resulting in more complicated space management.
These problems can be resolved by use of a condensed version of the access list.
To condense the length of the access-control list, many systems recognize three classiﬁcations of users in connection with each ﬁle:
A set of users who are sharing the ﬁle and need similar access is a.
The most common recent approach is to combine access-control lists with the more general (and easier to implement) owner, group, and universe accesscontrol scheme just described.
For example, Solaris uses the three categories of access by default but allows access-control lists to be added to speciﬁc ﬁles and directories when more ﬁne-grained access control is desired.
To illustrate, consider a person, Sara, who is writing a new book.
She has hired three graduate students (Jim, Dawn, and Jill) to help with the project.
The text of the book is kept in a ﬁle named book.tex.
Sara should be able to invoke all operations on the ﬁle.
Jim, Dawn, and Jill should be able only to read and write the ﬁle; they.
All other users should be able to read, but not write, the ﬁle.
Sara is interested in letting as many people as possible read the text so that she can obtain feedback.
To achieve such protection, we must create a new group—say, textwith members Jim, Dawn, and Jill.
The name of the group, text, must then be associated with the ﬁle book.tex, and the access rights must be set in accordance with the policy we have outlined.
The visitor cannot be added to the text group because that would give him access to all chapters.
In the UNIX system, directory protection and ﬁle protection are handled similarly.
Associated with each subdirectory are three ﬁelds—owner, group, and universe—each consisting of the three bits rwx.
Thus, a user can list the content of a subdirectory only if the r bit is set in the appropriate ﬁeld.
Similarly, a user can change his current directory to another current directory (say, foo) only if the x bit associated with the foo subdirectory is set in the appropriate ﬁeld.
A sample directory listing from a UNIX environment is shown in below:
The ﬁrst ﬁeld describes the protection of the ﬁle or directory.
Also shown are the number of links to the ﬁle, the owner’s name, the group’s name, the size of the ﬁle in bytes, the date of last modiﬁcation, and ﬁnally the ﬁle’s name (with optional extension)
For this scheme to work properly, permissions and access lists must be controlled tightly.
For example, in the UNIX system, groups can be created and modiﬁed only by the manager of the facility (or by any superuser)
With the more limited protection classiﬁcation, only three ﬁelds are needed to deﬁne protection.
Often, each ﬁeld is a collection of bits, and each bit either allows or prevents the access associated with it.
For example, the UNIX system deﬁnes three ﬁelds of 3 bits each—rwx, where r controls read access, w controls write access, and x controls execution.
A separate ﬁeld is kept for the ﬁle owner, for the ﬁle’s group, and for all other users.
In this scheme, 9 bits per ﬁle are needed to record protection information.
Thus, for our example, the protection ﬁelds for the ﬁle book.tex are as follows: for the owner Sara, all bits are set; for the group text, the r and w bits are set; and for the universe, only the r bit is set.
One difﬁculty in combining approaches comes in the user interface.
Users must be able to tell when the optional ACL permissions are set on a ﬁle.
In the Solaris example, a “+” is appended to the regular permissions, as in:
A separate set of commands, setfacl and getfacl, is used to manage the ACLs.
In this example, user “guest” is speciﬁcally denied access to the ﬁle ListPanel.java.
Another difﬁculty is assigning precedence when permission and ACLs conﬂict.
For example, if Joe is in a ﬁle’s group, which has read permission, but the ﬁle has an ACL granting Joe read and write permission, should a write by Joe be granted or denied? Solaris gives ACLs precedence (as they are more ﬁne-grained and are not assigned by default)
This follows the general rule that speciﬁcity should have priority.
Another approach to the protection problem is to associate a password with each ﬁle.
Just as access to the computer system is often controlled by a.
If the passwords are chosen randomly and changed often, this scheme may be effective in limiting access to a ﬁle.
First, the number of passwords that a user needs to remember may become large, making the scheme impractical.
Second, if only one password is used for all the ﬁles, then once it is discovered, all ﬁles are accessible; protection is on an all-or-none basis.
Some systems allow a user to associate a password with a subdirectory, rather than with an individual ﬁle, to address this problem.
In a multilevel directory structure, we need to protect not only individual ﬁles but also collections of ﬁles in subdirectories; that is, we need to provide a mechanism for directory protection.
The directory operations that must be protected are somewhat different from the ﬁle operations.
We want to control the creation and deletion of ﬁles in a directory.
In addition, we probably want to control whether a user can determine the existence of a ﬁle in a directory.
Sometimes, knowledge of the existence and name of a ﬁle is signiﬁcant in itself.
Thus, listing the contents of a directory must be a protected operation.
Similarly, if a path name refers to a ﬁle in a directory, the user must be allowed access to both the directory and the ﬁle.
In systems where ﬁles may have numerous path names (such as acyclic and general graphs), a given user may have different access rights to a particular ﬁle, depending on the path name used.
A ﬁle is an abstract data type deﬁned and implemented by the operating system.
A logical record may be a byte, a line (of ﬁxed or variable length), or a more complex data item.
The operating system may speciﬁcally support various record types or may leave that support to the application program.
The major task for the operating system is to map the logical ﬁle concept onto physical storage devices such as magnetic disk or tape.
Since the physical record size of the device may not be the same as the logical record size, it may be necessary to order logical records into physical records.
Again, this task may be supported by the operating system or left for the application program.
Each device in a ﬁle system keeps a volume table of contents or a device directory listing the location of the ﬁles on the device.
In addition, it is useful to create directories to allow ﬁles to be organized.
A single-level directory in a multiuser system causes naming problems, since each ﬁle must have a unique name.
A two-level directory solves this problem by creating a separate directory for each user’s ﬁles.
The directory lists the ﬁles by name and includes the ﬁle’s location on the disk, length, type, owner, time of creation, time of last use, and so on.
The natural generalization of a two-level directory is a tree-structured directory.
A tree-structured directory allows a user to create subdirectories to organize ﬁles.
Acyclic-graph directory structures enable users to share subdirectories and ﬁles but complicate searching and deletion.
A general graph structure allows complete ﬂexibility in the sharing of ﬁles and directories but sometimes requires garbage collection to recover unused disk space.
Disks are segmented into one or more volumes, each containing a ﬁle system or left “raw.” File systems may be mounted into the system’s naming.
Once mounted, the ﬁles within the volume are available for use.
File systems may be unmounted to disable access or for maintenance.
File sharing depends on the semantics provided by the system.
Files may have multiple readers, multiple writers, or limits on sharing.
Distributed ﬁle systems allow client hosts to mount volumes or directories from servers, as long as they can access each other across a network.
Remote ﬁle systems present challenges in reliability, performance, and security.
Distributed information systems maintain user, host, and access information so that clients and servers can share state information to manage use and access.
Access to ﬁles can be controlled separately for each type of access—read, write, execute, append, delete, list directory, and so on.
File protection can be provided by access lists, passwords, or other techniques.
Other systems keep all ﬁles unless the user explicitly deletes them.
If your answer is no, explain what prevents your simulation’s success.
How would your answer change if ﬁle names were limited to seven characters?
Suggest a scheme for dealing with each of these protection problems.
Suppose that you want to allow 4,990 of these users to be able to access one ﬁle.
Can you suggest another protection scheme that can be used more effectively for this purpose than the scheme provided by UNIX?
What problems may occur if a new ﬁle is created in the same storage area or with the same absolute path name? How can these problems be avoided?
Should the operating system maintain a separate table for each user or maintain just one table that contains references to ﬁles that are currently being accessed by all users? If the same ﬁle is being accessed by two different programs or users, should there be separate entries in the open-ﬁle table? Explain.
Discuss the advantages and disadvantages of this scheme compared with the more traditional one, where the user has to open and close the ﬁle explicitly.
Other systems maintain several copies, one for each of the users sharing the ﬁle.
A multilevel directory structure was ﬁrst implemented on the MULTICS system ([Organick (1972)])
The network ﬁle system (NFS), designed by Sun Microsystems, allows directory structures to be spread across networked computer systems.
As we saw in Chapter 11, the ﬁle system provides the mechanism for on-line storage and access to ﬁle contents, including data and programs.
The ﬁle system resides permanently on secondary storage, which is designed to hold a large amount of data permanently.
This chapter is primarily concerned with issues surrounding ﬁle storage and access on the most common secondary-storage medium, the disk.
We explore ways to structure ﬁle use, to allocate disk space, to recover freed space, to track the locations of data, and to interface other parts of the operating system to secondary storage.
To describe the details of implementing local ﬁle systems and directory structures.
Disks provide most of the secondary storage on which ﬁle systems are maintained.
A disk can be rewritten in place; it is possible to read a block from the disk, modify the block, and write it back into the same place.
A disk can access directly any block of information it contains.
Thus, it is simple to access any ﬁle either sequentially or randomly, and switching from one ﬁle to another requires only moving the read–write heads and waiting for the disk to rotate.
To improve I/O efﬁciency, I/O transfers between memory and disk are.
File systems provide efﬁcient and convenient access to the disk by allowing data to be stored, located, and retrieved easily.
The ﬁrst problem is deﬁning how the ﬁle system should look to the user.
This task involves deﬁning a ﬁle and its attributes, the operations allowed on a ﬁle, and the directory structure for organizing ﬁles.
The second problem is creating algorithms and data structures to map the logical ﬁle system onto the physical secondary-storage devices.
The ﬁle system itself is generally composed of many different levels.
The structure shown in Figure 12.1 is an example of a layered design.
Each level in the design uses the features of lower levels to create new features for use by higher levels.
The I/O control level consists of device drivers and interrupt handlers to transfer information between the main memory and the disk system.
A device driver can be thought of as a translator.
Its input consists of highlevel commands such as “retrieve block 123.” Its output consists of low-level, hardware-speciﬁc instructions that are used by the hardware controller, which interfaces the I/O device to the rest of the system.
The device driver usually writes speciﬁc bit patterns to special locations in the I/O controller’s memory to tell the controller which device location to act on and what actions to take.
The basic ﬁle system needs only to issue generic commands to the appropriate device driver to read and write physical blocks on the disk.
This layer also manages the memory buffers and caches that hold various ﬁle-system, directory, and data blocks.
A block in the buffer is allocated before the transfer of a disk block can occur.
When the buffer is full, the buffer manager must ﬁnd more buffer memory or free.
Caches are used to hold frequently used ﬁle-system metadata to improve performance, so managing their contents is critical for optimum system performance.
The ﬁle-organization module knows about ﬁles and their logical blocks, as well as physical blocks.
By knowing the type of ﬁle allocation used and the location of the ﬁle, the ﬁle-organization module can translate logical block addresses to physical block addresses for the basic ﬁle system to transfer.
Since the physical blocks containing the data usually do not match the logical numbers, a translation is needed to locate each block.
The ﬁle-organization module also includes the free-space manager, which tracks unallocated blocks and provides these blocks to the ﬁle-organization module when requested.
Metadata includes all of the ﬁle-system structure except the actual data (or contents of the ﬁles)
The logical ﬁle system manages the directory structure to provide the ﬁle-organization module with the information the latter needs, given a symbolic ﬁle name.
A ﬁlecontrol block (FCB) (an inode in UNIX ﬁle systems) contains information about the ﬁle, including ownership, permissions, and location of the ﬁle contents.
When a layered structure is used for ﬁle-system implementation, duplication of code is minimized.
The I/O control and sometimes the basic ﬁle-system code can be used by multiple ﬁle systems.
Each ﬁle system can then have its own logical ﬁle-system and ﬁle-organization modules.
Unfortunately, layering can introduce more operating-system overhead, which may result in decreased performance.
The use of layering, including the decision about how many layers to use and what each layer should do, is a major challenge in designing new systems.
Many ﬁle systems are in use today, and most operating systems support more than one.
For example, most CD-ROMs are written in the ISO 9660 format, a standard format agreed on by CD-ROM manufacturers.
In addition to removable-media ﬁle systems, each operating system has one or more diskbased ﬁle systems.
There are also distributed ﬁle systems in which a ﬁle system on a server is mounted by one or more client computers across a network.
File-system research continues to be an active area of operating-system design and implementation.
Google created its own ﬁle system to meet the company’s speciﬁc storage and retrieval needs, which include highperformance access from many clients across a very large number of disks.
Another interesting project is the FUSE ﬁle system, which provides ﬂexibility in ﬁle-system development and use by implementing and executing ﬁle systems as user-level rather than kernel-level code.
Using FUSE, a user can add a new ﬁle system to a variety of operating systems and can use that ﬁle system to manage her ﬁles.
As was described in Section 11.1.2, operating systems implement open() and close() systems calls for processes to request access to ﬁle contents.
In this section, we delve into the structures and operations used to implement ﬁle-system operations.
Several on-disk and in-memory structures are used to implement a ﬁle system.
These structures vary depending on the operating system and the ﬁle system, but some general principles apply.
On disk, the ﬁle system may contain information about how to boot an operating system stored there, the total number of blocks, the number and location of free blocks, the directory structure, and individual ﬁles.
Many of these structures are detailed throughout the remainder of this chapter.
A boot control block (per volume) can contain information needed by the system to boot an operating system from that volume.
If the disk does not contain an operating system, this block can be empty.
A volume control block (per volume) contains volume (or partition) details, such as the number of blocks in the partition, the size of the blocks, a free-block count and free-block pointers, and a free-FCB count and FCB pointers.
In NTFS, it is stored in the master ﬁle table.
A directory structure (per ﬁle system) is used to organize the ﬁles.
In UFS, this includes ﬁle names and associated inode numbers.
In NTFS, it is stored in the master ﬁle table.
It has a unique identiﬁer number to allow association with a directory entry.
In NTFS, this information is actually stored within the master ﬁle table, which uses a relational database structure, with a row per ﬁle.
The in-memory information is used for both ﬁle-system management and performance improvement via caching.
The data are loaded at mount time, updated during ﬁle-system operations, and discarded at dismount.
An in-memory mount table contains information about each mounted volume.
For directories at which volumes are mounted, it can contain a pointer to the volume table.
The system-wide open-ﬁle table contains a copy of the FCB of each open ﬁle, as well as other information.
The per-process open-ﬁle table contains a pointer to the appropriate entry in the system-wide open-ﬁle table, as well as other information.
Buffers hold ﬁle-system blocks when they are being read from disk or written to disk.
To create a new ﬁle, an application program calls the logical ﬁle system.
The logical ﬁle system knows the format of the directory structures.
To create a new ﬁle, it allocates a new FCB.
Alternatively, if the ﬁle-system implementation creates all FCBs at ﬁle-system creation time, an FCB is allocated from the set of free FCBs.
The system then reads the appropriate directory into memory, updates it with the new ﬁle name and FCB, and writes it back to the disk.
Some operating systems, including UNIX, treat a directory exactly the same as a ﬁle—one with a “type”ﬁeld indicating that it is a directory.
Other operating systems, including Windows, implement separate system calls for ﬁles and directories and treat directories as entities separate from ﬁles.
Whatever the larger structural issues, the logical ﬁle system can call the ﬁle-organization module to map the directory I/O into disk-block numbers, which are passed on to the basic ﬁle system and I/O control system.
Now that a ﬁle has been created, it can be used for I/O.
The open() call passes a ﬁle name to the logical ﬁle system.
The open() system call ﬁrst searches the system-wide open-ﬁle table to see if the ﬁle is already in use by another process.
If it is, a per-process open-ﬁle table entry is created pointing to the existing system-wide open-ﬁle table.
If the ﬁle is not already open, the directory structure is searched for the given ﬁle name.
Parts of the directory structure are usually cached in memory to speed directory operations.
Once the ﬁle is found, the FCB is copied into a system-wide open-ﬁle table in memory.
This table not only stores the FCB but also tracks the number of processes that have the ﬁle open.
Next, an entry is made in the per-process open-ﬁle table, with a pointer to the entry in the system-wide open-ﬁle table and some other ﬁelds.
These other ﬁelds may include a pointer to the current location in the ﬁle (for the next read() or write() operation) and the access mode in which the ﬁle is open.
The open() call returns a pointer to the appropriate entry in the per-process.
The ﬁle name may not be part of the open-ﬁle table, as the system has no use for it once the appropriate FCB is located on disk.
It could be cached, though, to save time on subsequent opens of the same ﬁle.
When a process closes the ﬁle, the per-process table entry is removed, and the system-wide entry’s open count is decremented.
When all users that have opened the ﬁle close it, any updated metadata is copied back to the disk-based directory structure, and the system-wide open-ﬁle table entry is removed.
Some systems complicate this scheme further by using the ﬁle system as an interface to other system aspects, such as networking.
For example, in UFS, the system-wide open-ﬁle table holds the inodes and other information for ﬁles and directories.
It also holds similar information for network connections and devices.
In this way, one mechanism can be used for multiple purposes.
The caching aspects of ﬁle-system structures should not be overlooked.
Most systems keep all information about an open ﬁle, except for its actual data blocks, in memory.
The BSD UNIX system is typical in its use of caches wherever disk I/O can be saved.
Its average cache hit rate of 85 percent shows that these techniques are well worth implementing.
The operating structures of a ﬁle-system implementation are summarized in Figure 12.3
The layout of a disk can have many variations, depending on the operating system.
A disk can be sliced into multiple partitions, or a volume can span multiple partitions on multiple disks.
The former layout is discussed here, while the latter, which is more appropriately considered a form of RAID, is covered in Section 10.7
Each partition can be either “raw,” containing no ﬁle system, or “cooked,” containing a ﬁle system.
Raw disk is used where no ﬁle system is appropriate.
Likewise, some databases use raw disk and format the data to suit their needs.
Raw disk can also hold information needed by disk RAID systems, such as bit maps indicating which blocks are mirrored and which have changed and need to be mirrored.
Similarly, raw disk can contain a miniature database holding RAID conﬁguration information, such as which disks are members of each RAID set.
Boot information can be stored in a separate partition, as described in Section 10.5.2
Again, it has its own format, because at boot time the system does not have the ﬁle-system code loaded and therefore cannot interpret the ﬁle-system format.
Rather, boot information is usually a sequential series of blocks, loaded as an image into memory.
Execution of the image starts at a predeﬁned location, such as the ﬁrst byte.
This boot loader in turn knows enough about the ﬁle-system structure to be able to ﬁnd and load the kernel and start it executing.
It can contain more than the instructions for how to boot a speciﬁc operating system.
For instance, many systems can be dual-booted, allowing us to install multiple operating systems on a single system.
How does the system know which one to boot? A boot loader that understands multiple ﬁle systems and multiple operating systems can occupy the boot space.
Once loaded, it can boot one of the operating systems available on the disk.
The disk can have multiple partitions, each containing a different type of ﬁle system and a different operating system.
The root partition, which contains the operating-system kernel and sometimes other system ﬁles, is mounted at boot time.
Other volumes can be automatically mounted at boot or manually mounted later, depending on the operating system.
As part of a successful mount operation, the operating system veriﬁes that the device contains a valid ﬁle system.
It does so by asking the device driver to read the device directory and verifying that the directory has the expected format.
If the format is invalid, the partition must have its consistency checked and possibly corrected, either with or without user intervention.
Finally, the operating system notes in its in-memory mount table that a ﬁle system is mounted, along with the type of the ﬁle system.
The details of this function depend on the operating system.
Microsoft Windows–based systems mount each volume in a separate name space, denoted by a letter and a colon.
To record that a ﬁle system is mounted at F:, for example, the operating system places a pointer to the ﬁle system in a ﬁeld of the device structure corresponding to F:
When a process speciﬁes the driver letter, the operating system ﬁnds the appropriate ﬁle-system pointer and traverses the directory structures on that device to ﬁnd the speciﬁed ﬁle.
Later versions of Windows can mount a ﬁle system at any point within the existing directory structure.
On UNIX, ﬁle systems can be mounted at any directory.
Mounting is implemented by setting a ﬂag in the in-memory copy of the inode for that directory.
The ﬂag indicates that the directory is a mount point.
A ﬁeld then points to an entry in the mount table, indicating which device is mounted there.
The mount table entry contains a pointer to the superblock of the ﬁle system on that device.
This scheme enables the operating system to traverse its directory structure, switching seamlessly among ﬁle systems of varying types.
The previous section makes it clear that modern operating systems must concurrently support multiple types of ﬁle systems.
But how does an operating system allow multiple types of ﬁle systems to be integrated into a directory structure? And how can users seamlessly move between ﬁle-system types as they navigate the ﬁle-system space? We now discuss some of these implementation details.
An obvious but suboptimal method of implementing multiple types of ﬁle systems is to write directory and ﬁle routines for each type.
Instead, however, most operating systems, including UNIX, use object-oriented techniques to simplify, organize, and modularize the implementation.
The use of these methods allows very dissimilar ﬁle-system types to be implemented within the same structure, including network ﬁle systems, such as NFS.
Users can access ﬁles contained within multiple ﬁle systems on the local disk or even on ﬁle systems available across the network.
Data structures and procedures are used to isolate the basic systemcall functionality from the implementation details.
Thus, the ﬁle-system implementation consists of three major layers, as depicted schematically in Figure 12.4
The ﬁrst layer is the ﬁle-system interface, based on the open(), read(), write(), and close() calls and on ﬁle descriptors.
The second layer is called the virtual ﬁle system (VFS) layer.
It separates ﬁle-system-generic operations from their implementation by deﬁning a clean VFS interface.
Several implementations for the VFS interface may coexist on the same machine, allowing transparent access to different types of ﬁle systems mounted locally.
It provides a mechanism for uniquely representing a ﬁle throughout a network.
The VFS is based on a ﬁle-representation structure, called a vnode, that contains a numerical designator for a network-wide unique ﬁle.
This network-wide uniqueness is required for support of network ﬁle systems.
The kernel maintains one vnode structure for each active node (ﬁle or directory)
Thus, the VFS distinguishes local ﬁles from remote ones, and local ﬁles are further distinguished according to their ﬁle-system types.
The VFS activates ﬁle-system-speciﬁc operations to handle local requests according to their ﬁle-system types and calls the NFS protocol procedures for.
File handles are constructed from the relevant vnodes and are passed as arguments to these procedures.
The layer implementing the ﬁle-system type or the remote-ﬁle-system protocol is the third layer of the architecture.
The four main object types deﬁned by the Linux VFS are:
For each of these four object types, the VFS deﬁnes a set of operations that may be implemented.
Every object of one of these types contains a pointer to a function table.
The function table lists the addresses of the actual functions that implement the deﬁned operations for that particular object.
For example, an abbreviated API for some of the operations for the ﬁle object includes:
An implementation of the ﬁle object for a speciﬁc ﬁle type is required to implement each function speciﬁed in the deﬁnition of the ﬁle object.
Thus, the VFS software layer can perform an operation on one of these objects by calling the appropriate function from the object’s function table, without having to know in advance exactly what kind of object it is dealing with.
The VFS does not know, or care, whether an inode represents a disk ﬁle, a directory ﬁle, or a remote ﬁle.
The appropriate function for that ﬁle’s read() operation will always be at the same place in its function table, and the VFS software layer will call that function without caring how the data are actually read.
In this section, we discuss the trade-offs involved in choosing one of these algorithms.
The simplest method of implementing a directory is to use a linear list of ﬁle names with pointers to the data blocks.
This method is simple to program but time-consuming to execute.
To create a new ﬁle, we must ﬁrst search the directory to be sure that no existing ﬁle has the same name.
Then, we add a new entry at the end of the directory.
To delete a ﬁle, we search the directory for the named ﬁle and then release the space allocated to it.
To reuse the directory entry, we can do one of several things.
We can mark the entry as unused (by assigning it a special name, such as an all-blank name, or by including a usedunused bit in each entry), or we can attach it to a list of free directory entries.
A third alternative is to copy the last entry in the directory into the freed location and to decrease the length of the directory.
A linked list can also be used to decrease the time required to delete a ﬁle.
The real disadvantage of a linear list of directory entries is that ﬁnding a ﬁle requires a linear search.
Directory information is used frequently, and users will notice if access to it is slow.
In fact, many operating systems implement a software cache to store the most recently used directory information.
A cache hit avoids the need to constantly reread the information from disk.
A sorted list allows a binary search and decreases the average search time.
However, the requirement that the list be kept sorted may complicate creating and deleting ﬁles, since we may have to move substantial amounts of directory information to maintain a sorted directory.
A more sophisticated tree data structure, such as a balanced tree, might help here.
An advantage of the sorted list is that a sorted directory listing can be produced without a separate sort step.
Another data structure used for a ﬁle directory is a hash table.
Here, a linear list stores the directory entries, but a hash data structure is also used.
The hash table takes a value computed from the ﬁle name and returns a pointer to the ﬁle.
The major difﬁculties with a hash table are its generally ﬁxed size and the dependence of the hash function on that size.
For example, assume that we make a linear-probing hash table that holds 64 entries.
Each hash entry can be a linked list instead of an individual value, and we can resolve collisions by adding the new entry to the linked list.
Lookups may be somewhat slowed, because searching for a name might require stepping through a linked list of colliding table entries.
Still, this method is likely to be much faster than a linear search through the entire directory.
The direct-access nature of disks gives us ﬂexibility in the implementation of ﬁles.
In almost every case, many ﬁles are stored on the same disk.
The main problem is how to allocate space to these ﬁles so that disk space is utilized effectively and ﬁles can be accessed quickly.
Three major methods of allocating disk space are in wide use: contiguous, linked, and indexed.
Although some systems support all three, it is more common for a system to use one method for all ﬁles within a ﬁle-system type.
Contiguous allocation requires that each ﬁle occupy a set of contiguous blocks on the disk.
With this ordering, assuming that only one job is accessing the disk, accessing block b + 1 after block b normally requires no head movement.
When head movement is needed (from the last sector of one cylinder to the ﬁrst sector of the next cylinder), the head need only move from one track to the next.
Thus, the number of disk seeks required for accessing contiguously allocated ﬁles is minimal, as is seek time when a seek is ﬁnally needed.
Accessing a ﬁle that has been allocated contiguously is easy.
For sequential access, the ﬁle system remembers the disk address of the last block referenced and, when necessary, reads the next block.
Thus, both sequential and direct access can be supported by contiguous allocation.
The system chosen to manage free space determines how this task is accomplished; these management systems are discussed in Section 12.5
Any management system can be used, but some are slower than others.
First ﬁt and best ﬁt are the most common strategies used to select a free hole from the set of available holes.
Simulations have shown that both ﬁrst ﬁt and best ﬁt are more efﬁcient than worst ﬁt in terms of both time and storage utilization.
Neither ﬁrst ﬁt nor best ﬁt is clearly best in terms of storage utilization, but ﬁrst ﬁt is generally faster.
All these algorithms suffer from the problem of external fragmentation.
As ﬁles are allocated and deleted, the free disk space is broken into little pieces.
External fragmentation exists whenever free space is broken into chunks.
It becomes a problem when the largest contiguous chunk is insufﬁcient for a request; storage is fragmented into a number of holes, none of which is large enough to store the data.
Depending on the total amount of disk storage and the average ﬁle size, external fragmentation may be a minor or a major problem.
One strategy for preventing loss of signiﬁcant amounts of disk space to external fragmentation is to copy an entire ﬁle system onto another disk.
The original disk is then freed completely, creating one large contiguous free space.
We then copy the ﬁles back onto the original disk by allocating contiguous space from this one large hole.
This scheme effectively compacts all free space into one contiguous space, solving the fragmentation problem.
Compacting these disks may take hours and may be necessary on a weekly basis.
Some systems require that this function be done off-line, with the ﬁle system unmounted.
During this down time, normal system operation generally cannot be permitted, so such compaction is avoided at all costs on production machines.
Most modern systems that need defragmentation can perform it on-line during normal system operations, but the performance penalty can be substantial.
Another problem with contiguous allocation is determining how much space is needed for a ﬁle.
When the ﬁle is created, the total amount of space it will need must be found and allocated.
How does the creator (program or person) know the size of the ﬁle to be created? In some cases, this determination may be fairly simple (copying an existing ﬁle, for example)
In general, however, the size of an output ﬁle may be difﬁcult to estimate.
If we allocate too little space to a ﬁle, we may ﬁnd that the ﬁle cannot be extended.
Especially with a best-ﬁt allocation strategy, the space on both sides of the ﬁle may be in use.
First, the user program can be terminated, with an appropriate error message.
The user must then allocate more space and run the program again.
To prevent them, the user will normally overestimate the amount of space needed, resulting in considerable wasted space.
The other possibility is to ﬁnd a larger hole, copy the contents of the ﬁle to the new space, and release the previous space.
This series of actions can be repeated as long as space exists, although it can be time consuming.
The user need never be informed explicitly about what is happening, however; the system continues despite the problem, although more and more slowly.
Even if the total amount of space needed for a ﬁle is known in advance, preallocation may be inefﬁcient.
A ﬁle that will grow slowly over a long period (months or years) must be allocated enough space for its ﬁnal size, even though much of that space will be unused for a long time.
The ﬁle therefore has a large amount of internal fragmentation.
Then, if that amount proves not to be large enough, another chunk of contiguous space, known as an extent, is added.
The location of a ﬁle’s blocks is then recorded as a location and a block count, plus a link to the ﬁrst block of the next extent.
On some systems, the owner of the ﬁle can set the extent size, but this setting results in inefﬁciencies if the owner is incorrect.
Internal fragmentation can still be a problem if the extents are too large, and external fragmentation can become a problem as extents of varying sizes are allocated and deallocated.
The commercial Veritas ﬁle system uses extents to optimize performance.
Veritas is a high-performance replacement for the standard UNIX UFS.
With linked allocation, each ﬁle is a linked list of disk blocks; the disk blocks may be scattered anywhere on the disk.
To create a new ﬁle, we simply create a new entry in the directory.
With linked allocation, each directory entry has a pointer to the ﬁrst disk block of the ﬁle.
This pointer is initialized to null (the end-of-list pointer value) to signify an empty ﬁle.
A write to the ﬁle causes the free-space management system to ﬁnd a free block, and this new block is written to and is linked to the end of the ﬁle.
To read a ﬁle, we simply read blocks by following the pointers from block to block.
There is no external fragmentation with linked allocation, and any free block on the free-space list can be used to satisfy a request.
The size of a ﬁle need not be declared when the ﬁle is created.
A ﬁle can continue to grow as long as free blocks are available.
The major problem is that it can be used effectively only for sequential-access ﬁles.
To ﬁnd the ith block of a ﬁle, we must start at the beginning of that ﬁle and follow the pointers until we get to the ith block.
Each access to a pointer requires a disk read, and some require a disk seek.
Consequently, it is inefﬁcient to support a direct-access capability for linked-allocation ﬁles.
Each ﬁle requires slightly more space than it would otherwise.
The usual solution to this problem is to collect blocks into multiples, called clusters, and to allocate clusters rather than blocks.
Pointers then use a much smaller percentage of the ﬁle’s disk space.
The cost of this approach is an increase in internal fragmentation, because more space is wasted when a cluster is partially full than when a block is partially full.
Clusters can be used to improve the disk-access time for many other algorithms as well, so they are used in most ﬁle systems.
Recall that the ﬁles are linked together by pointers scattered all over the disk, and consider what would happen if a pointer were lost or damaged.
A bug in the operating-system software or a disk hardware failure might result in picking up the wrong pointer.
This error could in turn result in linking into the free-space list or into another ﬁle.
One partial solution is to use doubly linked lists, and another is to store the ﬁle name and relative block number in each block.
However, these schemes require even more overhead for each ﬁle.
An important variation on linked allocation is the use of a ﬁle-allocation table (FAT)
This simple but efﬁcient method of disk-space allocation was used by the MS-DOS operating system.
A section of disk at the beginning of each volume is set aside to contain the table.
The table has one entry for each disk block and is indexed by block number.
The FAT is used in much the same way as a linked list.
The directory entry contains the block number of the ﬁrst block of the ﬁle.
The table entry indexed by that block number contains the block number of the next block in the ﬁle.
This chain continues until it reaches the last block, which has a special end-of-ﬁle value as the table entry.
Allocating a new block to a ﬁle is a simple matter of ﬁnding the ﬁrst 0-valued table entry and replacing the previous end-of-ﬁle value with the address of the new block.
The FAT allocation scheme can result in a signiﬁcant number of disk head seeks, unless the FAT is cached.
The disk head must move to the start of the volume to read the FAT and ﬁnd the location of the block in question, then move to the location of the block itself.
In the worst case, both moves occur for each of the blocks.
A beneﬁt is that random-access time is improved, because the disk head can ﬁnd the location of any block by reading the information in the FAT.
However, in the absence of a FAT, linked allocation cannot support efﬁcient direct access, since the pointers to the blocks are scattered with the blocks themselves all over the disk and must be retrieved in order.
Indexed allocation solves this problem by bringing all the pointers together into one location: the index block.
Each ﬁle has its own index block, which is an array of disk-block addresses.
The i th entry in the index block points to the i th block of the ﬁle.
To ﬁnd and read the i th block, we use the pointer in the i th index-block entry.
This scheme is similar to the paging scheme described in Section 8.5
When the ﬁle is created, all pointers in the index block are set to null.
When the i th block is ﬁrst written, a block is obtained from the free-space manager, and its address is put in the ith index-block entry.
Indexed allocation supports direct access, without suffering from external fragmentation, because any free block on the disk can satisfy a request for more space.
Consider a common case in which we have a ﬁle of only one or two blocks.
With linked allocation, we lose the space of only one pointer per block.
With indexed allocation, an entire index block must be allocated, even if only one or two pointers will be non-null.
This point raises the question of how large the index block should be.
Every ﬁle must have an index block, so we want the index block to be as small as possible.
If the index block is too small, however, it will not be able to hold enough pointers for a large ﬁle, and a mechanism will have to be available to deal with this issue.
Thus, it can be read and written directly by itself.
To allow for large ﬁles, we can link together several index blocks.
For example, an index block might contain a small header giving the name of the ﬁle and a set of the ﬁrst 100 disk-block addresses.
The next address (the last word in the index block) is null (for a small ﬁle) or is a pointer to another index block (for a large ﬁle)
A variant of linked representation uses a ﬁrst-level index block to point to a set of second-level index blocks, which in turn point to the ﬁle blocks.
To access a block, the operating system uses the ﬁrst-level index to ﬁnd a second-level index block and then uses that block to ﬁnd the desired data block.
This approach could be continued to a third or fourth level, depending on the desired maximum ﬁle size.
Another alternative, used in UNIX-based ﬁle systems, is to keep the ﬁrst, say, 15 pointers of the index block in the ﬁle’s inode.
The ﬁrst 12 of these pointers point to direct blocks; that is, they contain addresses of blocks that contain data of the ﬁle.
Thus, the data for small ﬁles (of no more than 12 blocks) do not need a separate index block.
The ﬁrst points to a single indirect block, which is an index block containing not data but the addresses of blocks that do contain data.
The second points to a double indirect block, which contains the address of a block that contains the addresses of blocks that contain pointers to the actual data blocks.
The last pointer contains the address of a triple indirect block.
Under this method, the number of blocks that can be allocated to a ﬁle exceeds the amount of space addressable by the 4-byte ﬁle pointers used by many operating systems.
Many UNIX and Linux implementations now support 64-bit ﬁle pointers, which allows ﬁles and ﬁle systems to be several exbibytes in size.
Indexed-allocation schemes suffer from some of the same performance problems as does linked allocation.
Speciﬁcally, the index blocks can be cached in memory, but the data blocks may be spread all over a volume.
The allocation methods that we have discussed vary in their storage efﬁciency and data-block access times.
Both are important criteria in selecting the proper method or methods for an operating system to implement.
Before selecting an allocation method, we need to determine how the systems will be used.
A system with mostly sequential access should not use the same method as a system with mostly random access.
For any type of access, contiguous allocation requires only one access to get a disk block.
Since we can easily keep the initial address of the ﬁle in memory, we can calculate immediately the disk address of the i th block (or the next block) and read it directly.
For linked allocation, we can also keep the address of the next block in memory and read it directly.
This method is ﬁne for sequential access; for direct access, however, an access to the i th block might require i disk reads.
This problem indicates why linked allocation should not be used for an application requiring direct access.
As a result, some systems support direct-access ﬁles by using contiguous allocation and sequential-access ﬁles by using linked allocation.
For these systems, the type of access to be made must be declared when the ﬁle is created.
A ﬁle created for sequential access will be linked and cannot be used for direct access.
A ﬁle created for direct access will be contiguous and can support both direct access and sequential access, but its maximum length must be declared when it is created.
In this case, the operating system must have appropriate data structures and algorithms to support both allocation methods.
Files can be converted from one type to another by the creation of a new ﬁle of the desired type, into which the contents of the old ﬁle are copied.
The old ﬁle may then be deleted and the new ﬁle renamed.
If the index block is already in memory, then the access can be made directly.
However, keeping the index block in memory requires considerable space.
If this memory space is not available, then we may have to read ﬁrst the index block and then the desired data block.
For a two-level index, two index-block reads might be necessary.
For an extremely large ﬁle, accessing a block near the end of the ﬁle would require reading in all the index blocks before the needed data block ﬁnally could be read.
Thus, the performance of indexed allocation depends on the index structure, on the size of the ﬁle, and on the position of the block desired.
Some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small ﬁles (up to three or four blocks) and automatically switching to an indexed allocation if the ﬁle grows large.
Since most ﬁles are small, and contiguous allocation is efﬁcient for small ﬁles, average performance can be quite good.
Given the disparity between CPU speed and disk speed, it is not unreasonable to add thousands of extra instructions to the operating system to save just a few disk-head movements.
Furthermore, this disparity is increasing over time, to the point where hundreds of thousands of instructions could reasonably be used to optimize head movements.
Since disk space is limited, we need to reuse the space from deleted ﬁles for new ﬁles, if possible.
Write-once optical disks allow only one write to any given sector, and thus reuse is not physically possible.
To keep track of free disk space, the system maintains a free-space list.
The free-space list records all free disk blocks—those not allocated to some ﬁle or directory.
To create a ﬁle, we search the free-space list for the required amount of space and allocate that space to the new ﬁle.
When a ﬁle is deleted, its disk space is added to the free-space list.
The free-space list, despite its name, may not be implemented as a list, as we discuss next.
Frequently, the free-space list is implemented as a bit map or bit vector.
The main advantage of this approach is its relative simplicity and its efﬁciency in ﬁnding the ﬁrst free block or n consecutive free blocks on the disk.
Indeed, many computers supply bit-manipulation instructions that can be used effectively for that purpose.
One technique for ﬁnding the ﬁrst free block on a system that uses a bit-vector to allocate disk space is to sequentially check each word in the bit map to see whether that value is not 0, since a.
Another approach to free-space management is to link together all the free disk blocks, keeping a pointer to the ﬁrst free block in a special location on the disk and caching it in memory.
This ﬁrst block contains a pointer to the next free disk block, and so on.
In this situation, we would keep a pointer to block 2 as the ﬁrst free block.
This scheme is not efﬁcient; to traverse the list, we must read each block, which requires substantial I/O time.
Usually, the operating system simply needs a free block so that it can allocate that block to a ﬁle, so the ﬁrst block in the free list is used.
The FAT method incorporates free-block accounting into the allocation data structure.
Thus, rather than keeping a list of n free disk addresses, we can keep the address of the ﬁrst free block and the number (n) of free contiguous blocks that follow the ﬁrst block.
Each entry in the free-space list then consists of a disk address and a count.
Note that this method of tracking free space is similar to the extent method of allocating blocks.
These entries can be stored in a balanced tree, rather than a linked list, for efﬁcient lookup, insertion, and deletion.
Oracle’s ZFS ﬁle system (found in Solaris and other operating systems) was designed to encompass huge numbers of ﬁles, directories, and even ﬁle systems (in ZFS, we can create ﬁle-system hierarchies)
On these scales, metadata I/O can have a large performance impact.
Consider, for example, that if the free-space list is implemented as a bit map, bit maps must be modiﬁed both when blocks are allocated and when they are freed.
Clearly, the data structures for such a system could be large and inefﬁcient.
In its management of free space, ZFS uses a combination of techniques to control the size of data structures and minimize the I/O needed to manage those structures.
First, ZFS creates metaslabs to divide the space on the device into chunks of manageable size.
Rather than write counting structures to disk, it uses log-structured ﬁle-system techniques to record them.
The space map is a log of all block activity (allocating and freeing), in time order, in counting format.
When ZFS decides to allocate or free space from a metaslab, it loads the associated space map into memory in a balanced-tree structure (for very efﬁcient operation), indexed by offset, and replays the log into that structure.
The in-memory space map is then an accurate representation of the allocated and free space in the metaslab.
During the collection and sorting phase, block requests can still occur, and ZFS satisﬁes these requests from the log.
In essence, the log plus the balanced tree is the free list.
Disks tend to represent a major bottleneck in system performance, since they are the slowest main computer component.
In this section, we discuss a variety of techniques used to improve the efﬁciency and performance of secondary storage.
The efﬁcient use of disk space depends heavily on the disk-allocation and directory algorithms in use.
Even an empty disk has a percentage of its space lost to inodes.
However, by preallocating the inodes and spreading them across the volume, we improve the ﬁle system’s performance.
This improved performance results from the UNIX allocation and free-space algorithms, which try to keep a ﬁle’s data blocks near that ﬁle’s inode block to reduce seek time.
As another example, let’s reconsider the clustering scheme discussed in Section 12.4, which improves ﬁle-seek and ﬁle-transfer performance at the cost of internal fragmentation.
To reduce this fragmentation, BSD UNIX varies the cluster size as a ﬁle grows.
Large clusters are used where they can be ﬁlled, and small clusters are used for small ﬁles and the last cluster of a ﬁle.
The types of data normally kept in a ﬁle’s directory (or inode) entry also require consideration.
Commonly, a “last write date” is recorded to supply information to the user and to determine whether the ﬁle needs to be backed up.
Some systems also keep a “last access date,” so that a user can determine when the ﬁle was last read.
The result of keeping this information is that, whenever the ﬁle is read, a ﬁeld in the directory structure must be written to.
That means the block must be read into memory, a section changed, and the block written back out to disk, because operations on disks occur only in block (or cluster) chunks.
So any time a ﬁle is opened for reading, its directory entry must be read and written as well.
This requirement can be inefﬁcient for frequently accessed ﬁles, so we must weigh its beneﬁt against its performance cost when designing a ﬁle system.
Generally, every data item associated with a ﬁle needs to be considered for its effect on efﬁciency and performance.
Consider, for instance, how efﬁciency is affected by the size of the pointers used to access data.
One of the difﬁculties in choosing a pointer size—or, indeed, any ﬁxed allocation size within an operating system—is planning for the effects of changing technology.
As hard disks with capacities of over 100 MB became common, the disk data structures and algorithms in MS-DOS had to be modiﬁed to allow larger ﬁle systems.
The initial ﬁle-system decisions were made for efﬁciency reasons; however, with the advent of MS-DOS Version 4, millions of computer users were inconvenienced when they had to switch to the new, larger ﬁle system.
Solaris’ ZFS ﬁle system uses 128-bit pointers, which theoretically should never need to be extended.
As another example, consider the evolution of the Solaris operating system.
Originally, many data structures were of ﬁxed length, allocated at system startup.
These structures included the process table and the open-ﬁle table.
When the process table became full, no more processes could be created.
When the ﬁle table became full, no more ﬁles could be opened.
Table sizes could be increased only by recompiling the kernel and rebooting the system.
With later releases of Solaris, almost all kernel structures were allocated dynamically, eliminating these artiﬁcial limits on system performance.
Of course, the algorithms that manipulate these tables are more complicated, and the operating system is a little slower because it must dynamically allocate and deallocate table entries; but that price is the usual one for more general functionality.
Even after the basic ﬁle-system algorithms have been selected, we can still improve performance in several ways.
As will be discussed in Chapter 13, most disk controllers include local memory to form an on-board cache that is large enough to store entire tracks at a time.
Once a seek is performed, the track is read into the disk cache starting at the sector under the disk head (reducing latency time)
The disk controller then transfers any sector requests to the operating system.
Once blocks make it from the disk controller into main memory, the operating system may cache the blocks there.
Some systems maintain a separate section of main memory for a buffer cache, where blocks are kept under the assumption that they will be used again shortly.
Caching ﬁle data using virtual addresses is far more efﬁcient than caching through physical disk blocks, as accesses interface with virtual memory rather than the ﬁle system.
Several systems—including Solaris, Linux, and Windows —use page caching to cache both process pages and ﬁle data.
Regardless of whether we are caching disk blocks or pages (or both), LRU (Section 9.4.4) seems a reasonable general-purpose algorithm for block or page replacement.
However, the evolution of the Solaris page-caching algorithms reveals the difﬁculty in choosing an algorithm.
Solaris allows processes and the page cache to share unused memory.
Versions earlier than Solaris 2.5.1 made no distinction between allocating pages to a process and allocating them to the page cache.
As a result, a system performing many I/O operations used most of the available memory for caching pages.
Because of the high rates of I/O, the page scanner (Section 9.10.2) reclaimed pages from processes—rather than from the page cache—when free memory ran low.
Solaris 8 applied a ﬁxed limit to process pages and the ﬁle-system page cache, preventing either from forcing the other out of memory.
Another issue that can affect the performance of I/O is whether writes to the ﬁle system occur synchronously or asynchronously.
Synchronous writes occur in the order in which the disk subsystem receives them, and the writes are not buffered.
Thus, the calling routine must wait for the data to reach the disk drive before it can proceed.
In an asynchronous write, the data are stored in the cache, and control returns to the caller.
Operating systems frequently include a ﬂag in the open system call to allow a process to request that writes be performed synchronously.
For example, databases use this feature for atomic transactions, to assure that data reach stable storage in the required order.
Some systems optimize their page cache by using different replacement algorithms, depending on the access type of the ﬁle.
A ﬁle being read or written sequentially should not have its pages replaced in LRU order, because the most recently used page will be used last, or perhaps never again.
Instead, sequential access can be optimized by techniques known as free-behind and read-ahead.
Free-behind removes a page from the buffer as soon as the next page is requested.
The previous pages are not likely to be used again and waste buffer space.
With read-ahead, a requested page and several subsequent pages are read and cached.
These pages are likely to be requested after the current page is processed.
Retrieving these data from the disk in one transfer and caching them saves a considerable amount of time.
One might think that a track cache on the controller would eliminate the need for read-ahead on a multiprogrammed system.
However, because of the high latency and overhead involved in making many small transfers from the track cache to main memory, performing a read-ahead remains beneﬁcial.
The page cache, the ﬁle system, and the disk drivers have some interesting interactions.
When data are written to a disk ﬁle, the pages are buffered in the cache, and the disk driver sorts its output queue according to disk address.
These two actions allow the disk driver to minimize disk-head seeks and to.
Unless synchronous writes are required, a process writing to disk simply writes into the cache, and the system asynchronously writes the data to disk when convenient.
When data are read from a disk ﬁle, the block I/O system does some read-ahead; however, writes are much more nearly asynchronous than are reads.
Thus, output to the disk through the ﬁle system is often faster than is input for large transfers, counter to intuition.
Files and directories are kept both in main memory and on disk, and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency.
We also consider how a system can recover from such a failure.
A system crash can cause inconsistencies among on-disk ﬁle-system data structures, such as directory structures, free-block pointers, and free FCB pointers.
Many ﬁle systems apply changes to these structures in place.
A typical operation, such as creating a ﬁle, can involve many structural changes within the ﬁle system on the disk.
Directory structures are modiﬁed, FCBs are allocated, data blocks are allocated, and the free counts for all of these blocks are decreased.
These changes can be interrupted by a crash, and inconsistencies among the structures can result.
For example, the free FCB count might indicate that an FCB had been allocated, but the directory structure might not point to the FCB.
Compounding this problem is the caching that operating systems do to optimize I/O performance.
Some changes may go directly to disk, while others may be cached.
If the cached changes do not reach disk before a crash occurs, more corruption is possible.
In addition to crashes, bugs in ﬁle-system implementation, disk controllers, and even user applications can corrupt a ﬁle system.
File systems have varying methods to deal with corruption, depending on the ﬁle-system data structures and algorithms.
Whatever the cause of corruption, a ﬁle system must ﬁrst detect the problems and then correct them.
For detection, a scan of all the metadata on each ﬁle system can conﬁrm or deny the consistency of the system.
Unfortunately, this scan can take minutes or hours and should occur every time the system boots.
Alternatively, a ﬁle system can record its state within the ﬁle-system metadata.
At the start of any metadata change, a status bit is set to indicate that the metadata is in ﬂux.
If all updates to the metadata complete successfully, the ﬁle system can clear that bit.
If, however, the status bit remains set, a consistency checker is run.
The consistency checker—a systems program such as fsck in UNIXcompares the data in the directory structure with the data blocks on disk and tries to ﬁx any inconsistencies it ﬁnds.
For instance, if linked allocation is used and there is a link from any block to its next block, then the entire ﬁle can be.
In contrast, the loss of a directory entry on an indexed allocation system can be disastrous, because the data blocks have no knowledge of one another.
For this reason, UNIX caches directory entries for reads; but any write that results in space allocation, or other metadata changes, is done synchronously, before the corresponding data blocks are written.
Of course, problems can still occur if a synchronous write is interrupted by a crash.
Computer scientists often ﬁnd that algorithms and technologies originally used in one area are equally useful in other areas.
Such is the case with the database log-based recovery algorithms.
These logging algorithms have been applied successfully to the problem of consistency checking.
The consistency check may not be able to recover the structures, resulting in loss of ﬁles and even entire directories.
Consistency checking can require human intervention to resolve conﬂicts, and that is inconvenient if no human is available.
The system can remain unavailable until the human tells it how to proceed.
To check terabytes of data, hours of clock time may be required.
The solution to this problem is to apply log-based recovery techniques to ﬁle-system metadata updates.
Both NTFS and the Veritas ﬁle system use this method, and it is included in recent versions of UFS on Solaris.
In fact, it is becoming common on many operating systems.
Fundamentally, all metadata changes are written sequentially to a log.
Each set of operations for performing a speciﬁc task is a transaction.
Once the changes are written to this log, they are considered to be committed, and the system call can return to the user process, allowing it to continue execution.
Meanwhile, these log entries are replayed across the actual ﬁlesystem structures.
As the changes are made, a pointer is updated to indicate which actions have completed and which are still incomplete.
When an entire committed transaction is completed, it is removed from the log ﬁle, which is actually a circular buffer.
A circular buffer writes to the end of its space and then continues at the beginning, overwriting older values as it goes.
We would not want the buffer to write over data that had not yet been saved, so that scenario is avoided.
The log may be in a separate section of the ﬁle system or even on a separate disk spindle.
It is more efﬁcient, but more complex, to have it under separate read and write heads, thereby decreasing head contention and seek times.
If the system crashes, the log ﬁle will contain zero or more transactions.
Any transactions it contains were not completed to the ﬁle system, even though they were committed by the operating system, so they must now be completed.
The transactions can be executed from the pointer until the work is complete.
The only problem occurs when a transaction was aborted—that is, was not committed before the system crashed.
Any changes from such a transaction that were applied to the ﬁle system must be undone, again preserving the consistency of the ﬁle system.
This recovery is all that is needed after a crash, eliminating any problems with consistency checking.
A side beneﬁt of using logging on disk metadata updates is that those updates proceed much faster than when they are applied directly to the ondisk data structures.
The reason is found in the performance advantage of sequential I/O over random I/O.
The costly synchronous random metadata writes are turned into much less costly synchronous sequential writes to the log-structured ﬁle system’s logging area.
Those changes, in turn, are replayed asynchronously via random writes to the appropriate structures.
The overall result is a signiﬁcant gain in performance of metadata-oriented operations, such as ﬁle creation and deletion.
Another alternative to consistency checking is employed by Network Appliance’s WAFL ﬁle system and the Solaris ZFS ﬁle system.
Rather, a transaction writes all data and metadata changes to new blocks.
When the transaction is complete, the metadata structures that pointed to the old versions of these blocks are updated to point to the new blocks.
The ﬁle system can then remove the old pointers and the old blocks and make them available for reuse.
If the old pointers and blocks are kept, a snapshot is created; the snapshot is a view of the ﬁle system before the last update took place.
This solution should require no consistency checking if the pointer update is done atomically.
See Section 12.9 for details of the WAFL ﬁle system.
However, ZFS goes further and provides checksumming of all metadata and data blocks.
This solution (when combined with RAID) assures that data are always correct.
Magnetic disks sometimes fail, and care must be taken to ensure that the data lost in such a failure are not lost forever.
To this end, system programs can be used to back up data from disk to another storage device, such as a magnetic tape or other hard disk.
Recovery from the loss of an individual ﬁle, or of an entire disk, may then be a matter of restoring the data from backup.
To minimize the copying needed, we can use information from each ﬁle’s directory entry.
For instance, if the backup program knows when the last backup of a ﬁle was done, and the ﬁle’s last write date in the directory indicates that the ﬁle has not changed since that date, then the ﬁle does not need to be copied again.
Copy to a backup medium all ﬁles from the disk.
The new cycle can have its backup written over the previous set or onto a new set of backup media.
Using this method, we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups.
Of course, the larger the value of N, the greater the number of media that must be read for a complete restore.
An added advantage of this backup cycle is that we can restore any ﬁle accidentally deleted during the cycle by retrieving the deleted ﬁle from the backup of the previous day.
The length of the cycle is a compromise between the amount of backup medium needed and the number of days covered by a restore.
To decrease the number of tapes that must be read to do a restore, an option is to perform a full backup and then each day back up all ﬁles that have changed since the full backup.
In this way, a restore can be done via the most recent incremental backup and the full backup, with no other incremental backups needed.
The trade-off is that more ﬁles will be modiﬁed each day, so each successive incremental backup involves more ﬁles and more backup media.
A user may notice that a particular ﬁle is missing or corrupted long after the damage was done.
For this reason, we usually plan to take a full backup from time to time that will be saved “forever.” It is a good idea to store these permanent backups far away from the regular backups to protect against hazard, such as a ﬁre that destroys the computer and all the backups too.
And if the backup cycle reuses media, we must take care not to reuse the media too many times—if the media wear out, it might not be possible to restore any data from the backups.
They are typically integrated with the overall directory structure and interface of the client system.
Here, we use it as an example to explore the implementation details of network ﬁle systems.
The implementation described here is part of the Solaris operating system, which is a modiﬁed version of UNIX SVR4
The speciﬁcation and the implementation are intertwined in our description of NFS.
Whenever detail is needed, we refer to the Solaris implementation; whenever the description is general, it applies to the speciﬁcation also.
Here, we describe Version 3, as that is the one most commonly deployed.
The goal is to allow some degree of sharing among these ﬁle systems (on explicit request) in a transparent manner.
A machine may be, and often is, both a client and a server.
To ensure machine independence, sharing of a remote ﬁle system affects only the client machine and no other machine.
So that a remote directory will be accessible in a transparent manner from a particular machine—say, from M1—a client of that machine must ﬁrst carry out a mount operation.
The semantics of the operation involve mounting a remote directory over a directory of a local ﬁle system.
Once the mount operation is completed, the mounted directory looks like an integral subtree of the local ﬁle system, replacing the subtree descending from the local directory.
The local directory becomes the name of the root of the newly mounted directory.
Speciﬁcation of the remote directory as an argument for the mount operation is not done transparently; the location (or host name) of the remote directory has to be provided.
However, from then on, users on machine M1 can access ﬁles in the remote directory in a totally transparent manner.
To illustrate ﬁle mounting, consider the ﬁle system depicted in Figure 12.13, where the triangles represent subtrees of directories that are of interest.
At this point, on each machine, only the local ﬁles can be accessed.
This ﬁgure depicts the view users on U have of their ﬁle system.
After the mount is complete, they can access any ﬁle within the dir1 directory using the.
The original directory /usr/local on that machine is no longer visible.
Subject to access-rights accreditation, any ﬁle system, or any directory within a ﬁle system, can be mounted remotely on top of any local directory.
Diskless workstations can even mount their own roots from servers.
That is, a ﬁle system can be mounted over another ﬁle system that is remotely mounted, not local.
A machine is affected by only those mounts that it has itself invoked.
Mounting a remote ﬁle system does not give the client access to other ﬁle systems that were, by chance, mounted over the former ﬁle system.
Thus, the mount mechanism does not exhibit a transitivity property.
If a shared ﬁle system is mounted over a user’s home directories on all machines in a network, the user can log into any workstation and get their home environment.
One of the design goals of NFS was to operate in a heterogeneous environment of different machines, operating systems, and network architectures.
Hence, if the system’s heterogeneous machines and ﬁle systems are properly interfaced to NFS, ﬁle systems of different types can be mounted both locally and remotely.
The NFS speciﬁcation distinguishes between the services provided by a mount mechanism and the actual remote-ﬁle-access services.
Accordingly, two separate protocols are speciﬁed for these services: a mount protocol and a protocol for remote ﬁle accesses, the NFS protocol.
These RPCs are the building blocks used to implement transparent remote ﬁle access.
The mount protocol establishes the initial logical connection between a server and a client.
In Solaris, each machine has a server process, outside the kernel, performing the protocol functions.
A mount operation includes the name of the remote directory to be mounted and the name of the server machine storing it.
The mount request is mapped to the corresponding RPC and is forwarded to the mount server running on the speciﬁc server machine.
The server maintains an export list that speciﬁes local ﬁle systems that it exports for mounting, along with names of machines that are permitted to mount them.
In Solaris, this list is the /etc/dfs/dfstab, which can be edited only by a superuser.
The speciﬁcation can also include access rights, such as read only.
To simplify the maintenance of export lists and mount tables, a distributed naming scheme can be used to hold this information and make it available to appropriate clients.
Recall that any directory within an exported ﬁle system can be mounted remotely by an accredited machine.
When the server receives a mount request that conforms to its export list, it returns to the client a ﬁle handle that serves as the key for further accesses to ﬁles within the mounted ﬁle system.
The ﬁle handle contains all the information that the server needs to distinguish an individual ﬁle it stores.
In UNIX terms, the ﬁle handle consists of a ﬁle-system identiﬁer and an inode number to identify the exact mounted directory within the exported ﬁle system.
The server also maintains a list of the client machines and the corresponding currently mounted directories.
This list is used mainly for administrative purposes—for instance, for notifying all clients that the server is going down.
Only through addition and deletion of entries in this list can the server state be affected by the mount protocol.
The NFS protocol provides a set of RPCs for remote ﬁle operations.
These procedures can be invoked only after a ﬁle handle for the remotely mounted directory has been established.
A prominent feature of NFS servers is that they are stateless.
Servers do not maintain information about their clients from one access to another.
UNIX’s open-ﬁles table or ﬁle structures exist on the server side.
Consequently, each request has to provide a full set of arguments, including a unique ﬁle identiﬁer and an absolute offset inside the ﬁle for the appropriate operations.
The resulting design is robust; no special measures need be taken to recover a server after a crash.
File operations must be idempotent for this purpose, that is, the same operation performed multiple times has the same effect as if it were only performed once.
To achieve idempotence, every NFS request has a sequence number, allowing the server to determine if a request has been duplicated or if any are missing.
Maintaining the list of clients that we mentioned seems to violate the statelessness of the server.
However, this list is not essential for the correct operation of the client or the server, and hence it does not need to be restored after a server crash.
Consequently, it may include inconsistent data and is treated as only a hint.
A further implication of the stateless-server philosophy and a result of the synchrony of an RPC is that modiﬁed data (including indirection and status blocks) must be committed to the server’s disk before results are returned to the client.
That is, a client can cache write blocks, but when it ﬂushes them to the server, it assumes that they have reached the server’s disks.
Thus, a server crash and recovery will be invisible to a client; all blocks that the server is managing for the client will be intact.
The resulting performance penalty can be large, because the advantages of caching are lost.
Performance can be increased by using storage with its own nonvolatile cache (usually battery-backed-up memory)
The disk controller acknowledges the disk write when the write is stored in the nonvolatile cache.
In essence, the host sees a very fast synchronous write.
These blocks remain intact even after a system crash and are written from this stable storage to disk periodically.
A single NFS write procedure call is guaranteed to be atomic and is not intermixed with other write calls to the same ﬁle.
As a result, two users writing to the same remote ﬁle may get their data intermixed.
The claim is that, because lock management is inherently stateful, a service outside the NFS should provide locking (and Solaris does)
Users are advised to coordinate access to shared ﬁles using mechanisms outside the scope of NFS.
As an illustration of the architecture, let’s trace how an operation on an already-open remote ﬁle is handled (follow the example in Figure 12.15)
The client initiates the operation with a regular system call.
The operating-system layer maps this call to a VFS operation on the appropriate vnode.
The VFS layer identiﬁes the ﬁle as a remote one and invokes the appropriate NFS procedure.
An RPC call is made to the NFS service layer at the remote server.
This call is reinjected to the VFS layer on the remote system, which ﬁnds that it is local and invokes the appropriate ﬁle-system operation.
An advantage of this architecture is that the client and the server are identical; thus, a machine may be a client, or a server, or both.
The actual service on each server is performed by kernel threads.
Path-name translation is done by breaking the path into component names and performing a separate NFS lookup call for every pair of component name and directory vnode.
Once a mount point is crossed, every component lookup causes a separate RPC to the server.
It would be much more efﬁcient to hand a server a path name and receive a target vnode once a mount point is encountered.
At any point, however, there might be another mount point for the particular client of which the stateless server is unaware.
This cache speeds up references to ﬁles with the same initial path name.
The directory cache is discarded when attributes returned from the server do not match the attributes of the cached vnode.
Recall that some implementations of NFS allow mounting a remote ﬁle system on top of another already-mounted remote ﬁle system (a cascading mount)
When a client has a cascading mount, more than one server can be involved in a path-name traversal.
However, when a client does a lookup on a directory on which the server has mounted a ﬁle system, the client sees the underlying directory instead of the mounted directory.
With the exception of opening and closing ﬁles, there is an almost one-to-one correspondence between the regular UNIX system calls for ﬁle operations and the NFS protocol RPCs.
Thus, a remote ﬁle operation can be translated directly to the corresponding RPC.
Conceptually, NFS adheres to the remote-service paradigm; but in practice, buffering and caching techniques are employed for the sake of performance.
No direct correspondence exists between a remote operation and an RPC.
Instead, ﬁle blocks and ﬁle attributes are fetched by the RPCs and are cached locally.
Future remote operations use the cached data, subject to consistency constraints.
When a ﬁle is opened, the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes.
The cached ﬁle blocks are used only if the corresponding cached attributes are up to date.
The attribute cache is updated whenever new attributes arrive from the server.
Both read-ahead and delayed-write techniques are used between the server and the client.
Clients do not free delayed-write blocks until the server conﬁrms that the data have been written to disk.
Delayed-write is retained even when a ﬁle is opened concurrently, in conﬂicting modes.
Tuning the system for performance makes it difﬁcult to characterize the consistency semantics of NFS.
New ﬁles created on a machine may not be visible elsewhere for 30 seconds.
Furthermore, writes to a ﬁle at one site may or may not be visible at other sites that have this ﬁle open for reading.
New opens of a ﬁle observe only the changes that have already been ﬂushed to the server.
Thus, NFS provides neither strict emulation of UNIX semantics nor the session semantics of Andrew (Section 11.5.3.2)
Because disk I/O has such a huge impact on system performance, ﬁle-system design and implementation command quite a lot of attention from system designers.
Some ﬁle systems are general purpose, in that they can provide reasonable performance and functionality for a wide variety of ﬁle sizes, ﬁle types, and I/O loads.
Others are optimized for speciﬁc tasks in an attempt to provide better performance in those areas than general-purpose ﬁle systems.
The write-anywhere ﬁle layout (WAFL) from Network Appliance is an example of this sort of optimization.
It can provide ﬁles to clients via the NFS, CIFS, ftp, and http protocols, although it was designed just for NFS and CIFS.
When many clients use these protocols to talk to a ﬁle server, the server may see a very large demand for random reads and an even larger demand for random writes.
The NFS and CIFS protocols cache data from read operations, so writes are of the greatest concern to ﬁle-server creators.
The WAFL designers took advantage of running on a speciﬁc architecture to optimize the ﬁle system for random I/O, with a stable-storage cache in front.
Ease of use is one of the guiding principles of WAFL.
Its creators also designed it to include a new snapshot functionality that creates multiple read-only copies of the ﬁle system at different points in time, as we shall see.
The ﬁle system is similar to the Berkeley Fast File System, with many modiﬁcations.
Each inode contains 16 pointers to blocks (or indirect blocks) belonging to the ﬁle described by the inode.
All inodes are in one ﬁle, the free-block map in another, and the free-inode map in a third, as shown in Figure 12.16
Because these are standard ﬁles, the data blocks are not limited in location and can be placed anywhere.
If a ﬁle system is expanded by addition of disks, the lengths of the metadata ﬁles are automatically expanded by the ﬁle system.
Thus, a WAFL ﬁle system is a tree of blocks with the root inode as its base.
To take a snapshot, WAFL creates a copy of the root inode.
Any ﬁle or metadata updates after that go to new blocks rather than overwriting their existing blocks.
The new root inode points to metadata and data changed as a result of these writes.
Meanwhile, the snapshot (the old root inode) still points to the old blocks, which have not been updated.
It therefore provides access to the ﬁle system just as it was at the instant the snapshot was made—and takes very little disk space to do so.
In essence, the extra disk space occupied by a snapshot consists of just the blocks that have been modiﬁed since the snapshot was taken.
An important change from more standard ﬁle systems is that the free-block map has more than one bit per block.
It is a bitmap with a bit set for each snapshot that is using the block.
When all snapshots that have been using the block are deleted, the bit map for that block is all zeros, and the block is free to be reused.
Used blocks are never overwritten, so writes are very fast, because a write can occur at the free block nearest the current head location.
There are many other performance optimizations in WAFL as well.
Many snapshots can exist simultaneously, so one can be taken each hour of the day and each day of the month.
A user with access to these snapshots can access ﬁles as they were at any of the times the snapshots were taken.
The snapshot facility is also useful for backups, testing, versioning, and so on.
WAFL’s snapshot facility is very efﬁcient in that it does not even require that copy-on-write copies of each data block be taken before the block is modiﬁed.
Other ﬁle systems provide snapshots, but frequently with less efﬁciency.
Newer versions of WAFL actually allow read–write snapshots, known as clones.
Clones are also efﬁcient, using the same techniques as shapshots.
In this case, a read-only snapshot captures the state of the ﬁle system, and a clone refers back to that read-only snapshot.
Any writes to the clone are stored in new blocks, and the clone’s pointers are updated to refer to the new blocks.
The original snapshot is unmodiﬁed, still giving a view into the ﬁle system as it was before the clone was updated.
Clones can also be promoted to replace the original ﬁle system; this involves throwing out all of the old pointers and any associated old blocks.
Clones are useful for testing and upgrades, as the original version is left untouched and the clone deleted when the test is done or if the upgrade fails.
Another feature that naturally results from the WAFL ﬁle system implementation is replication, the duplication and synchronization of a set of data over a network to another system.
First, a snapshot of a WAFL ﬁle system is duplicated to another system.
When another snapshot is taken on the source system, it is relatively easy to update the remote system just by sending over all blocks contained in the new snapshot.
The remote system adds these blocks to the ﬁle system and updates its pointers, and the new system then is a duplicate of the source system as of the time of the second snapshot.
Repeating this process maintains the remote system as a nearly up-to-date copy of the ﬁrst system.
Should the ﬁrst system be destroyed, most of its data are available for use on the remote system.
Finally, we should note that the ZFS ﬁle system supports similarly efﬁcient snapshots, clones, and replication.
The ﬁle system resides permanently on secondary storage, which is designed to hold a large amount of data permanently.
Physical disks may be segmented into partitions to control media use and to allow multiple, possibly varying, ﬁle systems on a single spindle.
These ﬁle systems are mounted onto a logical ﬁle system architecture to make them available for use.
File systems are often implemented in a layered or modular structure.
The lower levels deal with the physical properties of storage devices.
Upper levels deal with symbolic ﬁle names and logical properties of ﬁles.
Intermediate levels map the logical ﬁle concepts into physical device properties.
A VFS layer allows the upper layers to deal with each ﬁle-system type uniformly.
Even remote ﬁle systems can be integrated into the system’s directory structure and acted on by standard system calls via the VFS interface.
The various ﬁles can be allocated space on the disk in three ways: through contiguous, linked, or indexed allocation.
Indexed allocation may require substantial overhead for its index block.
Contiguous space can be enlarged through extents to increase ﬂexibility and to decrease external fragmentation.
Indexed allocation can be done in clusters of multiple blocks to increase throughput and to reduce the number of index entries needed.
Indexing in large clusters is similar to contiguous allocation with extents.
Free-space allocation methods also inﬂuence the efﬁciency of disk-space use, the performance of the ﬁle system, and the reliability of secondary storage.
Optimizations include grouping, counting, and the FAT, which places the linked list in one contiguous area.
A hash table is a commonly used method, as it is fast and efﬁcient.
Unfortunately, damage to the table or a system crash can result in inconsistency between the directory information and the disk’s contents.
A consistency checker can be used to repair the damage.
Operating-system backup tools allow disk data to be copied to tape, enabling the user to recover from data or even disk loss due to hardware failure, operating system bug, or user error.
Network ﬁle systems, such as NFS, use client–server methodology to allow users to access ﬁles and directories from remote machines as if they were on local ﬁle systems.
System calls on the client are translated into network protocols and retranslated into ﬁle-system operations on the server.
Networking and multiple-client access create challenges in the areas of data consistency and performance.
Due to the fundamental role that ﬁle systems play in system operation, their performance and reliability are crucial.
Techniques such as log structures and caching help improve performance, while log structures and RAID improve reliability.
The WAFL ﬁle system is an example of optimization of performance to match a speciﬁc I/O load.
Assume that the ﬁlecontrol block (and the index block, in the case of indexed allocation) is already in memory.
Calculate how many disk I/O operations are required for contiguous, linked, and indexed (single-level) allocation strategies, if, for one block, the following conditions hold.
Also assume that the block information to be added is stored in memory.
What criteria should be used in deciding which strategy is best utilized for a particular ﬁle?
If the ﬁle grows to be larger than the space allocated for it, special actions must be taken.
One solution to this problem is to deﬁne a ﬁle structure consisting of an initial contiguous area (of a speciﬁed size)
If this area is ﬁlled, the operating system automatically deﬁnes an overﬂow area that is linked to the initial contiguous area.
If the overﬂow area is ﬁlled, another overﬂow area is allocated.
Compare this implementation of a ﬁle with the standard contiguous and linked implementations.
A ﬁle is a collection of extents, with each extent corresponding to a contiguous set of blocks.
A key issue in such systems is the degree of variability in the size of the extents.
What are the advantages and disadvantages of the following schemes?
All extents are of the same size, and the size is predetermined.
Extents can be of any size and are allocated dynamically.
Extents can be of a few ﬁxed sizes, and these sizes are predetermined.
Suppose that the pointer to the free-space list is lost.
Can the system reconstruct the free-space list? Explain your answer.
Consider a ﬁle system similar to the one used by UNIX with indexed allocation.
How many disk I/O operations might be required to read the contents of a small local ﬁle at /a/b/c? Assume that none of the disk blocks is currently being cached.
Suggest a scheme to ensure that the pointer is never lost as a result of memory failure.
How could we take advantage of this ﬂexibility to improve performance? What modiﬁcations would have to be made to the free-space management scheme in order to support this feature?
Assume that the information about each ﬁle is already in memory.
For each of the three allocation strategies (contiguous, linked, and indexed), answer these questions:
This ﬁle system has 12 direct disk blocks, as well as single, double, and triple indirect disk blocks.
What is the maximum size of a ﬁle that can be stored in this ﬁle system?
Typical disk devices do not have relocation or base registers (such as those used when memory is to be compacted), so how can we relocate ﬁles? Give three reasons why recompacting and relocation of ﬁles are often avoided.
What issues should we take into account in implementing the name cache?
Copy to a backup medium all ﬁles from the disk.
This differs from the schedule given in Section 12.7.4 by having all subsequent backups copy all ﬁles modiﬁed since the ﬁrst full backup.
What are the beneﬁts of this system over the one in Section 12.7.4? What are the drawbacks? Are restore operations made easier or more difﬁcult? Explain your answer.
The following exercise examines the relationship between ﬁles and inodes on a UNIX or Linux system.
That is, an inode is a ﬁle (and vice versa)
You can complete this exercise on the Linux virtual machine that is provided with this text.
You can also complete the exercise on any Linux, UNIX, or.
Next, obtain the inode number of this ﬁle with the command.
The inode number offile1.txt is likely to be different on your system.
The UNIX ln command creates a link between a source and target ﬁle.
A hard link creates a separate target ﬁle that has the same inode as the source ﬁle.
After you have done so, examine the contents of file1.txt.
Does file2.txt still exist as well? Now examine the man pages for both the rm and unlink commands.
The strace command traces the execution of system calls as the command rm file2.txt is run.
A soft link (or symbolic link) creates a new ﬁle that “points” to the name of the ﬁle it is linking to.
In the source code available with this text, create a soft link to file3.txt by entering the following command:
Are the inodes the same, or is each unique? Next, edit the contents of file4.txt.
After you have done so, explain what happens when you attempt to edit file4.txt.
The MS-DOS FAT system is explained in [Norton and Wilton (1988)]
The internals of the BSD UNIX system are covered in full in [McKusick and Neville-Neil (2005)]
Details concerning ﬁle systems for Linux can be found in [Love (2010)]
The ZFS source code for space maps can be found at http://src.opensolaris.org/source/xref/onnv/onnv-gate/usr/src/uts/common/ fs/zfs/space map.c.
The network ﬁle system (NFS) is discussed in [Callaghan (2000)]
The two main jobs of a computer are I/O and processing.
In many cases, the main job is I/O, and the processing is merely incidental.
For instance, when we browse a web page or edit a ﬁle, our immediate interest is to read or enter some information, not to compute an answer.
The role of the operating system in computer I/O is to manage and control I/O operations and I/O devices.
Although related topics appear in other chapters, here we bring together the pieces to paint a complete picture of I/O.
First, we describe the basics of I/O hardware, because the nature of the hardware interface places constraints on the internal facilities of the operating system.
Next, we discuss the I/O services provided by the operating system and the embodiment of these services in the application I/O interface.
Then, we explain how the operating system bridges the gap between the hardware interface and the application interface.
We also discuss the UNIX System V STREAMS mechanism, which enables an application to assemble pipelines of driver code dynamically.
Finally, we discuss the performance aspects of I/O and the principles of operating-system design that improve I/O performance.
To explore the structure of an operating system’s I/O subsystem.
To explain the performance aspects of I/O hardware and software.
The control of devices connected to the computer is a major concern of operating-system designers.
Because I/O devices vary so widely in their function and speed (consider a mouse, a hard disk, and a tape robot), varied methods are needed to control them.
These methods form the I/O subsystem of the kernel, which separates the rest of the kernel from the complexities of managing I/O devices.
On the one hand, we see increasing standardization of software and hardware interfaces.
This trend helps us to incorporate improved device generations into existing computers and operating systems.
On the other hand, we see an increasingly broad variety of I/O devices.
Some new devices are so unlike previous devices that it is a challenge to incorporate them into our computers and operating systems.
This challenge is met by a combination of hardware and software techniques.
The basic I/O hardware elements, such as ports, buses, and device controllers, accommodate a wide variety of I/O devices.
To encapsulate the details and oddities of different devices, the kernel of an operating system is structured to use device-driver modules.
The device drivers present a uniform deviceaccess interface to the I/O subsystem, much as system calls provide a standard interface between the application and the operating system.
Most ﬁt into the general categories of storage devices (disks, tapes), transmission devices (network connections, Bluetooth), and human-interface devices (screen, keyboard, mouse, audio in and out)
Other devices are more specialized, such as those involved in the steering of a jet.
In these aircraft, a human gives input to the ﬂight computer via a joystick and foot pedals, and the computer sends output commands that cause motors to move rudders and ﬂaps and fuels to the engines.
Despite the incredible variety of I/O devices, though, we need only a few concepts to understand how the devices are attached and how the software can control the hardware.
A device communicates with a computer system by sending signals over a cable or even through the air.
The device communicates with the machine via a connection point, or port—for example, a serial port.
If devices share a common set of wires, the connection is called a bus.
A bus is a set of wires and a rigidly deﬁned protocol that speciﬁes a set of messages that can be sent on the wires.
In terms of the electronics, the messages are conveyed by patterns of electrical voltages applied to the wires with deﬁned timings.
When device A has a cable that plugs into device B, and device B has a cable that plugs into device C, and device C plugs into a port on the computer, this arrangement is called a daisy chain.
Buses are used widely in computer architecture and vary in their signaling methods, speed, throughput, and connection methods.
In the ﬁgure, a PCI bus (the common PC system bus) connects the processor–memory subsystem to fast devices, and an expansion bus connects relatively slow devices, such as the keyboard and serial and USB ports.
In the upper-right portion of the ﬁgure, four disks are connected together on a Small Computer System Interface (SCSI) bus plugged into a SCSI controller.
A controller is a collection of electronics that can operate a port, a bus, or a device.
It is a single chip (or portion of a chip) in the computer that controls the signals on the.
Because the SCSI protocol is complex, the SCSI bus controller is often implemented as a separate circuit board (or a host adapter) that plugs into the computer.
It typically contains a processor, microcode, and some private memory to enable it to process the SCSI protocol messages.
If you look at a disk drive, you will see a circuit board attached to one side.
It implements the disk side of the protocol for some kind of connection—SCSI or Serial Advanced Technology Attachment (SATA), for instance.
It has microcode and a processor to do many tasks, such as bad-sector mapping, prefetching, buffering, and caching.
How can the processor give commands and data to a controller to accomplish an I/O transfer? The short answer is that the controller has one or more registers for data and control signals.
The processor communicates with the controller by reading and writing bit patterns in these registers.
One way in which this communication can occur is through the use of special I/O instructions that specify the transfer of a byte or word to an I/O port address.
The I/O instruction triggers bus lines to select the proper device and to move bits into or out of a device register.
In this case, the device-control registers are mapped into the address space of the processor.
The CPU executes I/O requests using the standard data-transfer instructions to read and write the device-control registers at their mapped locations in physical memory.
For instance, PCs use I/O instructions to control some devices and memory-mapped I/O to control others.
The process sends output to the screen by writing data into the memory-mapped region.
The controller generates the screen image based on the contents of this memory.
Moreover, writing millions of bytes to the graphics memory is faster than issuing millions of I/O instructions.
But the ease of writing to a memory-mapped I/O controller is offset by a disadvantage.
Because a common type of software fault is a write through an incorrect pointer to an unintended region of memory, a memory-mapped device register is vulnerable to accidental modiﬁcation.
An I/O port typically consists of four registers, called the status, control, data-in, and data-out registers.
The data-in register is read by the host to get input.
The data-out register is written by the host to send output.
The status register contains bits that can be read by the host.
The control register can be written by the host to start a command or to change the mode of a device.
Some controllers have FIFO chips that can hold several bytes of input or output data to expand the capacity of the controller beyond the size of the data register.
A FIFO chip can hold a small burst of data until the device or host is able to receive those data.
The complete protocol for interaction between the host and a controller can be intricate, but the basic handshaking notion is simple.
Assume that 2 bits are used to coordinate the producer–consumer relationship between the controller and the host.
The controller indicates its state through the busy bit in the status register.
The controller sets the busy bit when it is busy working and clears the busy bit when it is ready to accept the next command.
The host signals its wishes via the command-ready bit in the command register.
The host sets the command-ready bit when a command is available for the controller to execute.
For this example, the host writes output through a port, coordinating with the controller by handshaking as follows.
The host repeatedly reads the busy bit until that bit becomes clear.
The host sets the write bit in the command register and writes a byte into the data-out register.
When the controller notices that the command-ready bit is set, it sets the busy bit.
The controller reads the command register and sees the write command.
It reads the data-out register to get the byte and does the I/O to the device.
The controller clears the command-ready bit, clears the error bit in the status register to indicate that the device I/O succeeded, and clears the busy bit to indicate that it is ﬁnished.
In step 1, the host is busy-waiting or polling: it is in a loop, reading the.
If the controller and device are fast, this method is a reasonable one.
But if the wait may be long, the host should probably switch to another task.
How, then, does the host know when the controller has become idle? For some devices, the host must service the device quickly, or data will be lost.
For instance, when data are streaming in on a serial port or from a keyboard, the small buffer on the controller will overﬂow and data will be lost if the host waits too long before returning to read the bytes.
In many computer architectures, three CPU-instruction cycles are sufﬁcient to poll a device: read a device register, logical--and to extract a status bit, and branch if not zero.
But polling becomes inefﬁcient when it is attempted repeatedly yet rarely ﬁnds a device ready for service, while other useful CPU processing remains undone.
In such instances, it may be more efﬁcient to arrange for the hardware controller to notify the CPU when the device becomes ready for service, rather than to require the CPU to poll repeatedly for an I/O completion.
The hardware mechanism that enables a device to notify the CPU is called an interrupt.
The CPU hardware has a wire called the interrupt-request line that the CPU senses after executing every instruction.
When the CPU detects that a controller has asserted a signal on the interrupt-request line, the CPU performs a state save and jumps to the interrupt-handler routine at a ﬁxed address in memory.
The interrupt handler determines the cause of the interrupt, performs the necessary processing, performs a state restore, and executes a return from interrupt instruction to return the CPU to the execution state prior to the interrupt.
We say that the device controller raises an interrupt by asserting a signal on the interrupt request line, the CPU catches the interrupt and dispatches it to the interrupt handler, and the handler clears the interrupt by servicing the device.
We stress interrupt management in this chapter because even single-user modern systems manage hundreds of interrupts per second and servers hundreds of thousands per second.
The basic interrupt mechanism just described enables the CPU to respond to an asynchronous event, as when a device controller becomes ready for service.
In a modern operating system, however, we need more sophisticated interrupt-handling features.
We need the ability to defer interrupt handling during critical processing.
We need an efﬁcient way to dispatch to the proper interrupt handler for a device without ﬁrst polling all the devices to see which one raised the interrupt.
We need multilevel interrupts, so that the operating system can distinguish between high- and low-priority interrupts and can respond with the appropriate degree of urgency.
One is the nonmaskable interrupt, which is reserved for events such as unrecoverable memory errors.
The second interrupt line is maskable: it can be turned off by the CPU before the execution of critical instruction sequences that must not be interrupted.
The maskable interrupt is used by device controllers to request service.
The interrupt mechanism accepts an address—a number that selects a speciﬁc interrupt-handling routine from a small set.
In most architectures, this address is an offset in a table called the interrupt vector.
This vector contains the memory addresses of specialized interrupt handlers.
The purpose of a vectored interrupt mechanism is to reduce the need for a single interrupt handler to search all possible sources of interrupts to determine which one needs service.
In practice, however, computers have more devices (and, hence, interrupt handlers) than they have address elements in the interrupt vector.
A common way to solve this problem is to use interrupt chaining, in which each element in the interrupt vector points to the head of a list of interrupt handlers.
When an interrupt is raised, the handlers on the corresponding list are called one by one, until one is found that can service the request.
This structure is a compromise between the overhead of a huge interrupt table and the inefﬁciency of dispatching to a single interrupt handler.
Figure 13.4 illustrates the design of the interrupt vector for the Intel Pentium processor.
The interrupt mechanism also implements a system of interrupt priority levels.
These levels enable the CPU to defer the handling of low-priority interrupts without masking all interrupts and makes it possible for a highpriority interrupt to preempt the execution of a low-priority interrupt.
A modern operating system interacts with the interrupt mechanism in several ways.
At boot time, the operating system probes the hardware buses to determine what devices are present and installs the corresponding interrupt handlers into the interrupt vector.
During I/O, the various device controllers raise interrupts when they are ready for service.
These interrupts signify that output has completed, or that input data are available, or that a failure has been detected.
The interrupt mechanism is also used to handle a wide variety of exceptions, such as dividing by 0, accessing a protected or nonexistent memory address, or attempting to execute a privileged instruction from user mode.
The events that trigger interrupts have a common property: they are occurrences that induce the operating system to execute an urgent, self-contained routine.
An operating system has other good uses for an efﬁcient hardware and software mechanism that saves a small amount of processor state and then calls a privileged routine in the kernel.
For example, many operating systems use the interrupt mechanism for virtual memory paging.
A page fault is an exception that raises an interrupt.
The interrupt suspends the current process and jumps to the page-fault handler in the kernel.
This handler saves the state of the process, moves the process to the wait queue, performs page-cache management, schedules an I/O operation to fetch the page, schedules another process to resume execution, and then returns from the interrupt.
Another example is found in the implementation of system calls.
Usually, a program uses library calls to issue system calls.
The library routines check the arguments given by the application, build a data structure to convey the arguments to the kernel, and then execute a special instruction called a software interrupt, or trap.
This instruction has an operand that identiﬁes the desired kernel service.
When a process executes the trap instruction, the interrupt hardware saves the state of the user code, switches to kernel mode, and dispatches to the kernel routine that implements the requested service.
Interrupts can also be used to manage the ﬂow of control within the kernel.
For example, consider one example of the processing required to complete.
One step is to copy data from kernel space to the user buffer.
This copying is time consuming but not urgent—it should not block other high-priority interrupt handling.
Another step is to start the next pending I/O for that disk drive.
If the disks are to be used efﬁciently, we need to start the next I/O as soon as the previous one completes.
Consequently, a pair of interrupt handlers implements the kernel code that completes a disk read.
The high-priority handler records the I/O status, clears the device interrupt, starts the next pending I/O, and raises a low-priority interrupt to complete the work.
Later, when the CPU is not occupied with highpriority work, the low-priority interrupt will be dispatched.
The corresponding handler completes the user-level I/O by copying data from kernel buffers to the application space and then calling the scheduler to place the application on the ready queue.
A threaded kernel architecture is well suited to implement multiple interrupt priorities and to enforce the precedence of interrupt handling over background processing in kernel and application routines.
A range of high priorities is reserved for these threads.
These priorities give interrupt handlers precedence over application code and kernel housekeeping and implement the priority relationships among interrupt handlers.
The priorities cause the Solaris thread scheduler to preempt lowpriority interrupt handlers in favor of higher-priority ones, and the threaded implementation enables multiprocessor hardware to run several interrupt handlers concurrently.
In summary, interrupts are used throughout modern operating systems to handle asynchronous events and to trap to supervisor-mode routines in the kernel.
To enable the most urgent work to be done ﬁrst, modern computers use a system of interrupt priorities.
Device controllers, hardware faults, and system calls all raise interrupts to trigger kernel routines.
Because interrupts are used so heavily for time-sensitive processing, efﬁcient interrupt handling is required for good system performance.
For a device that does large transfers, such as a disk drive, it seems wasteful to use an expensive general-purpose processor to watch status bits and to feed data into a controller register one byte at a time—a process termed programmed I/O (PIO)
To initiate a DMA transfer, the host writes a DMA command block into memory.
This block contains a pointer to the source of a transfer, a pointer to the destination of the transfer, and a count of the number of bytes to be transferred.
The CPU writes the address of this command block to the DMA controller, then goes on with other work.
The DMA controller proceeds to operate the memory bus directly, placing addresses on the bus to perform transfers without the help of the main CPU.
A simple DMA controller is a standard component in all modern computers, from smartphones to mainframes.
Handshaking between the DMA controller and the device controller is performed via a pair of wires called DMA-request and DMA-acknowledge.
The device controller places a signal on the DMA-request wire when a word of data is available for transfer.
This signal causes the DMA controller to seize the memory bus, place the desired address on the memory-address wires, and place a signal on the DMA-acknowledge wire.
When the device controller receives the DMA-acknowledge signal, it transfers the word of data to memory and removes the DMA-request signal.
When the entire transfer is ﬁnished, the DMA controller interrupts the CPU.
When the DMA controller seizes the memory bus, the CPU is momentarily prevented from accessing main memory, although it can still access data items in its primary and secondary caches.
Although this cycle stealing can slow down the CPU computation, ofﬂoading the data-transfer work to a DMA controller generally improves the total system performance.
Some computer architectures use physical memory addresses for DMA, but others perform direct virtual memory access (DVMA), using virtual addresses that undergo translation to physical addresses.
On protected-mode kernels, the operating system generally prevents processes from issuing device commands directly.
This discipline protects data from access-control violations and also protects the system from erroneous use of device controllers that could cause a system crash.
Instead, the operating system exports functions that a sufﬁciently privileged process can use to access low-level operations on the underlying hardware.
On kernels without memory protection, processes can access device controllers directly.
This direct access can be used to achieve high performance, since it can avoid kernel communication, context switches, and layers of kernel software.
The trend in general-purpose operating systems is to protect memory and devices so that the system can try to guard against erroneous or malicious applications.
The handshaking relationship between the host and a device controller.
The execution of this handshaking in a polling loop or via interrupts.
The ofﬂoading of this work to a DMA controller for large transfers.
We gave a basic example of the handshaking that takes place between a device controller and the host earlier in this section.
In reality, the wide variety of available devices poses a problem for operating-system implementers.
Each kind of device has its own set of capabilities, control-bit deﬁnitions, and protocols for interacting with the host—and they are all different.
How can the operating system be designed so that we can attach new devices to the computer without rewriting the operating system? And when the devices vary so widely, how can the operating system give a convenient, uniform I/O interface to applications? We address those questions next.
In this section, we discuss structuring techniques and interfaces for the operating system that enable I/O devices to be treated in a standard, uniform way.
We explain, for instance, how an application can open a ﬁle on a disk without knowing what kind of disk it is and how new disks and other devices can be added to a computer without disruption of the operating system.
Speciﬁcally, we can abstract away the detailed differences in I/O devices by identifying a few general kinds.
Each general kind is accessed through a standardized set of functions—an interface.
The differences are encapsulated in kernel modules called device drivers that internally are custom-tailored to speciﬁc devices but that export one of the standard interfaces.
Figure 13.6 illustrates how the I/O-related portions of the kernel are structured in software layers.
The purpose of the device-driver layer is to hide the differences among device controllers from the I/O subsystem of the kernel, much as the I/O system calls encapsulate the behavior of devices in a few generic classes that hide hardware differences from applications.
They either design new devices to be compatible with an existing host controller interface (such as SATA), or they write device drivers to interface the new hardware to popular operating systems.
Thus, we can attach new peripherals to a computer without waiting for the operating-system vendor to develop support code.
Unfortunately for device-hardware manufacturers, each type of operating system has its own standards for the device-driver interface.
A given device may ship with multiple device drivers—for instance, drivers for Windows, Linux, AIX, and Mac OS X.
Devices vary on many dimensions, as illustrated in Figure 13.7
A character-stream device transfers bytes one by one, whereas a block device transfers a block of bytes as a unit.
A sequential device transfers data in a ﬁxed order determined by the device, whereas the user of a random-access device can instruct the device to seek to any of the available data storage locations.
A synchronous device performs data transfers with predictable response times, in coordination with other aspects of the system.
An asynchronous device exhibits irregular or unpredictable response times not coordinated with other computer events.
A sharable device can be used concurrently by several processes or threads; a dedicated device cannot.
Device speeds range from a few bytes per second to a few gigabytes per second.
Some devices perform both input and output, but others support only one data transfer direction.
For the purpose of application access, many of these differences are hidden by the operating system, and the devices are grouped into a few conventional types.
The resulting styles of device access have been found to be useful and broadly applicable.
Although the exact system calls may differ across operating systems, the device categories are fairly standard.
The major access conventions include block I/O, character-stream I/O, memory-mapped ﬁle access, and network sockets.
Operating systems also provide special system calls to access a few additional devices, such as a time-of-day clock and a timer.
Some operating systems provide a set of system calls for graphical display, video, and audio devices.
Most operating systems also have an escape (or back door) that transparently passes arbitrary commands from an application to a device driver.
In UNIX, this system call is ioctl() (for “I/O control”)
The ioctl() system call enables an application to access any functionality that can be implemented by any device driver, without the need to invent a new system call.
The ﬁrst is a ﬁle descriptor that connects the application to the driver by referring to a hardware device managed by that driver.
The second is an integer that selects one of the commands implemented in the driver.
The third is a pointer to an arbitrary data structure in memory that enables the application and driver to communicate any necessary control information or data.
The block-device interface captures all the aspects necessary for accessing disk drives and other block-oriented devices.
The device is expected to understand commands such as read() and write()
If it is a random-access device, it is also expected to have a seek() command to specify which block to transfer next.
Applications normally access such a device through a ﬁle-system interface.
We can see that read(), write(), and seek() capture the essential behaviors of block-storage devices, so that applications are insulated from the low-level differences among those devices.
The operating system itself, as well as special applications such as databasemanagement systems, may prefer to access a block device as a simple linear array of blocks.
If the application performs its own buffering, then using a ﬁle system would cause extra, unneeded buffering.
Likewise, if an application provides its own locking of ﬁle blocks or regions, then any operating-system locking services would be redundant at the least and contradictory at the worst.
To avoid these conﬂicts, raw-device access passes control of the device directly to the application, letting the operating system step out of the way.
Unfortunately, no operating-system services are then performed on this device.
A compromise that is becoming common is for the operating system to allow a mode of operation on a ﬁle that disables buffering and locking.
Memory-mapped ﬁle access can be layered on top of block-device drivers.
Rather than offering read and write operations, a memory-mapped interface provides access to disk storage via an array of bytes in main memory.
The system call that maps a ﬁle into memory returns the virtual memory address that contains a copy of the ﬁle.
The actual data transfers are performed only when needed to satisfy access to the memory image.
Because the transfers are handled by the same mechanism as that used for demand-paged virtual memory access, memory-mapped I/O is efﬁcient.
Memory mapping is also convenient for programmers—access to a memory-mapped ﬁle is as simple as reading from and writing to memory.
Operating systems that offer virtual memory commonly use the mapping interface for kernel services.
For instance, to execute a program, the operating system maps the executable into memory and then transfers control to the entry address of the executable.
The mapping interface is also commonly used for kernel access to swap space on disk.
A keyboard is an example of a device that is accessed through a characterstream interface.
The basic system calls in this interface enable an application to get() or put() one character.
On top of this interface, libraries can be built that offer line-at-a-time access, with buffering and editing services (for example, when a user types a backspace, the preceding character is removed from the input stream)
This style of access is convenient for input devices such as keyboards, mice, and modems that produce data for input “spontaneously” —that is, at times that cannot necessarily be predicted by the application.
This access style is also good for output devices such as printers and audio boards, which naturally ﬁt the concept of a linear stream of bytes.
Because the performance and addressing characteristics of network I/O differ signiﬁcantly from those of disk I/O, most operating systems provide a network.
One interface available in many operating systems, including UNIX and Windows, is the network socket interface.
Think of a wall socket for electricity: any electrical appliance can be plugged in.
By analogy, the system calls in the socket interface enable an application to create a socket, to connect a local socket to a remote address (which plugs this application into a socket created by another application), to listen for any remote application to plug into the local socket, and to send and receive packets over the connection.
To support the implementation of servers, the socket interface also provides a function called select() that manages a set of sockets.
A call to select() returns information about which sockets have a packet waiting to be received and which sockets have room to accept a packet to be sent.
The use of select() eliminates the polling and busy waiting that would otherwise be necessary for network I/O.
These functions encapsulate the essential behaviors of networks, greatly facilitating the creation of distributed applications that can use any underlying network hardware and protocol stack.
Many other approaches to interprocess communication and network communication have been implemented.
For instance, Windows provides one interface to the network interface card and a second interface to the network protocols.
In UNIX, which has a long history as a proving ground for network technology, we ﬁnd half-duplex pipes, full-duplex FIFOs, full-duplex STREAMS, message queues, and sockets.
Most computers have hardware clocks and timers that provide three basic functions:
Set a timer to trigger operation X at time T.
These functions are used heavily by the operating system, as well as by timesensitive applications.
Unfortunately, the system calls that implement these functions are not standardized across operating systems.
The hardware to measure elapsed time and to trigger operations is called a programmable interval timer.
It can be set to wait a certain amount of time and then generate an interrupt, and it can be set to do this once or to repeat the process to generate periodic interrupts.
The scheduler uses this mechanism to generate an interrupt that will preempt a process at the end of its time slice.
The disk I/O subsystem uses it to invoke the periodic ﬂushing of dirty cache buffers to disk, and the network subsystem uses it to cancel operations that are proceeding too slowly because of network congestion or failures.
The operating system may also provide an interface for user processes to use timers.
The operating system can support more timer requests than the number of timer hardware channels by simulating virtual clocks.
To do so, the kernel (or the timer device driver) maintains a list of interrupts wanted by its own routines and by user requests, sorted in earliest-time-ﬁrst order.
When the timer interrupts, the kernel signals the requester and reloads the timer with the next earliest time.
This resolution is coarse, since a modern computer can execute hundreds of millions of instructions per second.
The precision of triggers is limited by the coarse resolution of the timer, together with the overhead of maintaining virtual clocks.
Furthermore, if the timer ticks are used to maintain the system time-of-day clock, the system clock can drift.
In most computers, the hardware clock is constructed from a highfrequency counter.
In some computers, the value of this counter can be read from a device register, in which case the counter can be considered a highresolution clock.
Although this clock does not generate interrupts, it offers accurate measurements of time intervals.
Another aspect of the system-call interface relates to the choice between blocking I/O and nonblocking I/O.
When an application issues a blocking system call, the execution of the application is suspended.
The application is moved from the operating system’s run queue to a wait queue.
After the system call completes, the application is moved back to the run queue, where it is eligible to resume execution.
When it resumes execution, it will receive the values returned by the system call.
The physical actions performed by I/O devices are generally asynchronous—they take a varying or unpredictable amount of time.
Nevertheless, most operating systems use blocking system calls for the application interface, because blocking application code is easier to understand than nonblocking application code.
One example is a user interface that receives keyboard and mouse input while processing and displaying data on the screen.
Another example is a video application that reads frames from a ﬁle on disk while simultaneously decompressing and displaying the output on the display.
One way an application writer can overlap execution with I/O is to write a multithreaded application.
Some threads can perform blocking system calls, while others continue executing.
A nonblocking call does not halt the execution of the application for an extended time.
Instead, it returns quickly, with a return value that indicates how many bytes were transferred.
An alternative to a nonblocking system call is an asynchronous system call.
An asynchronous call returns immediately, without waiting for the I/O to complete.
The completion of the I/O at some future time is communicated to the application, either through the setting of some variable in the address space of the application or through the triggering of a signal or software interrupt or a call-back routine that is executed outside the linear control ﬂow of the application.
The difference between nonblocking and asynchronous system calls is that a nonblocking read() returns immediately with whatever data are available—the full number of bytes requested, fewer, or none at all.
An asynchronous read() call requests a transfer that will be performed in its entirety but will complete at some future time.
Figure 13.8 Two I/O methods: (a) synchronous and (b) asynchronous.
Frequently, they are not exposed to users or applications but rather are contained within the operating-system operation.
By default, when an application issues a network send request or a disk write request, the operating system notes the request, buffers the I/O, and returns to the application.
When possible, to optimize overall system performance, the operating system completes the request.
If a system failure occurs in the interim, the application will lose any “in-ﬂight” requests.
Therefore, operating systems usually put a limit on how long they will buffer a request.
Data consistency within applications is maintained by the kernel, which reads data from its buffers before issuing I/O requests to devices, assuring that data not yet written are nevertheless returned to a requesting reader.
Note that multiple threads performing I/O to the same ﬁle might not receive consistent data, depending on how the kernel implements its I/O.
In this situation, the threads may need to use locking protocols.
Some I/O requests need to be performed immediately, so I/O system calls usually have a way to indicate that a given request, or I/O to a speciﬁc device, should be performed synchronously.
A good example of nonblocking behavior is the select() system call for network sockets.
This system call takes an argument that speciﬁes a maximum waiting time.
By setting it to 0, an application can poll for network activity without blocking.
But using select() introduces extra overhead, because the select() call only checks whether I/O is possible.
For a data transfer, select() must be followed by some kind of read() or write() command.
A variation on this approach, found in Mach, is a blocking multiple-read call.
It speciﬁes desired reads for several devices in one system call and returns as soon as any one of them completes.
Some operating systems provide another major variation of I/O via their applications interfaces.
The same transfer could be caused by several individual invocations of system calls, but this scattergather method is useful for a variety of reasons.
Multiple separate buffers can have their contents transferred via one system call, avoiding context-switching and system-call overhead.
Without vectored I/O, the data might ﬁrst need to be transferred to a larger buffer in the right order and then transmitted, which is inefﬁcient.
In addition, some versions of scatter–gather provide atomicity, assuring that all the I/O is done without interruption (and avoiding corruption of data if other threads are also performing I/Oinvolving those buffers)
When possible, programmers make use of scatter–gather I/O features to increase throughput and decrease system overhead.
The I/O subsystem is also responsible for protecting itself from errant processes and malicious users.
To schedule a set of I/O requests means to determine a good order in which to execute them.
The order in which applications issue system calls rarely is the best choice.
Scheduling can improve overall system performance, can share device access fairly among processes, and can reduce the average waiting time for I/O to complete.
Suppose that a disk arm is near the beginning of a disk and that three applications issue blocking read calls to that disk.
Rearranging the order of service in this way is the essence of I/O scheduling.
Operating-system developers implement scheduling by maintaining a wait queue of requests for each device.
When an application issues a blocking I/O system call, the request is placed on the queue for that device.
The I/Oscheduler rearranges the order of the queue to improve the overall system efﬁciency and the average response time experienced by applications.
The operating system may also try to be fair, so that no one application receives especially poor service, or it may give priority service for delay-sensitive requests.
For instance, requests from the virtual memory subsystem may take priority over application requests.
Several scheduling algorithms for disk I/O are detailed in Section 10.4
When a kernel supports asynchronous I/O, it must be able to keep track of many I/O requests at the same time.
For this purpose, the operating system might attach the wait queue to a device-status table.
The kernel manages this table, which contains an entry for each I/O device, as shown in Figure 13.9
Each table entry indicates the device’s type, address, and state (not functioning, idle, or busy)
If the device is busy with a request, the type of request and other parameters will be stored in the table entry for that device.
Scheduling I/O operations is one way in which the I/O subsystem improves the efﬁciency of the computer.
Another way is by using storage space in main memory or on disk via buffering, caching, and spooling.
A buffer, of course, is a memory area that stores data being transferred between two devices or between a device and an application.
One reason is to cope with a speed mismatch between the producer and consumer of a data stream.
Suppose, for example, that a ﬁle is being received via modem for storage on the hard disk.
The modem is about a thousand times slower than the hard disk.
So a buffer is created in main memory to accumulate the bytes received from the modem.
When an entire buffer of data has arrived, the buffer can be written to disk in a single operation.
Since the disk write is not instantaneous and the modem still needs a place to store additional incoming data, two buffers are used.
After the modem ﬁlls the ﬁrst buffer, the disk write is requested.
The modem then starts to ﬁll the second buffer while the ﬁrst buffer is written to disk.
By the time the modem has ﬁlled the second buffer, the disk write from the ﬁrst one should have completed, so the modem can switch back to the ﬁrst buffer while the disk writes the second one.
This double buffering decouples the producer of data from the consumer, thus relaxing timing requirements between them.
The need for this decoupling is illustrated in Figure 13.10, which lists the enormous differences in device speeds for typical computer hardware.
A second use of buffering is to provide adaptations for devices that have different data-transfer sizes.
Such disparities are especially common in computer networking, where buffers are used widely for fragmentation and reassembly of messages.
The packets are sent over the network, and the receiving side places them in a reassembly buffer to form an image of the source data.
A third use of buffering is to support copy semantics for application I/O.
An example will clarify the meaning of “copy semantics.” Suppose that an application has a buffer of data that it wishes to write to disk.
It calls the write() system call, providing a pointer to the buffer and an integer specifying the number of bytes to write.
After the system call returns, what happens if the application changes the contents of the buffer? With copy semantics, the version of the data written to disk is guaranteed to be the version at the time of the application system call, independent of any subsequent changes in the application’s buffer.
A simple way in which the operating system can guarantee copy semantics is for the write() system call to copy the application data into a kernel buffer before returning control to the application.
The disk write is performed from the kernel buffer, so that subsequent changes to the application buffer have no effect.
Copying of data between kernel buffers and application data space is common in operating systems, despite the overhead that this operation introduces, because of the clean semantics.
The same effect can be obtained more efﬁciently by clever use of virtual memory mapping and copy-on-write page protection.
A cache is a region of fast memory that holds copies of data.
Access to the cached copy is more efﬁcient than access to the original.
The difference between a buffer and a cache is that a buffer may hold the only existing copy of a data item, whereas a cache, by deﬁnition, holds a copy on faster storage of an item that resides elsewhere.
Caching and buffering are distinct functions, but sometimes a region of memory can be used for both purposes.
For instance, to preserve copy semantics and to enable efﬁcient scheduling of disk I/O, the operating system uses buffers in main memory to hold disk data.
These buffers are also used as a cache, to improve the I/O efﬁciency for ﬁles that are shared by applications or that are being written and reread rapidly.
When the kernel receives a ﬁle I/O request, the kernel ﬁrst accesses the buffer cache to see whether that region of the ﬁle is already available in main memory.
If it is, a physical disk I/O can be avoided or deferred.
Also, disk writes are accumulated in the buffer cache for several seconds, so that large transfers are gathered to allow efﬁcient write schedules.
This strategy of delaying writes to improve I/O efﬁciency is discussed, in the context of remote ﬁle access, in Section 17.9.2
A spool is a buffer that holds output for a device, such as a printer, that cannot accept interleaved data streams.
Although a printer can serve only one job at a time, several applications may wish to print their output concurrently, without having their output mixed together.
The operating system solves this problem by intercepting all output to the printer.
Each application’s output is spooled to a separate disk ﬁle.
When an application ﬁnishes printing, the spooling system queues the corresponding spool ﬁle for output to the printer.
The spooling system copies the queued spool ﬁles to the printer one at a time.
In some operating systems, spooling is managed by a system daemon process.
In either case, the operating system provides a control interface that enables users and system administrators to display the queue, remove unwanted jobs before those jobs print, suspend printing while the printer is serviced, and so on.
Some devices, such as tape drives and printers, cannot usefully multiplex the I/O requests of multiple concurrent applications.
Spooling is one way operating systems can coordinate concurrent output.
Another way to deal with concurrent device access is to provide explicit facilities for coordination.
Some operating systems (including VMS) provide support for exclusive device access by enabling a process to allocate an idle device and to deallocate that device when it is no longer needed.
Other operating systems enforce a limit of one open ﬁle handle to such a device.
Many operating systems provide functions that enable processes to coordinate exclusive access among themselves.
For instance, Windows provides system calls to wait until a device object becomes available.
It also has a parameter to theOpenFile() system call that declares the types of access to be permitted to other concurrent threads.
On these systems, it is up to the applications to avoid deadlock.
An operating system that uses protected memory can guard against many kinds of hardware and application errors, so that a complete system failure is.
Devices and I/O transfers can fail in many ways, either for transient reasons, as when a network becomes overloaded, or for “permanent” reasons, as when a disk controller becomes defective.
For instance, a disk read() failure results in a read() retry, and a network send() error results in a resend(), if the protocol so speciﬁes.
Unfortunately, if an important component experiences a permanent failure, the operating system is unlikely to recover.
As a general rule, an I/O system call will return one bit of information about the status of the call, signifying either success or failure.
In the UNIX operating system, an additional integer variable named errno is used to return an error code—one of about a hundred values—indicating the general nature of the failure (for example, argument out of range, bad pointer, or ﬁle not open)
By contrast, some hardware can provide highly detailed error information, although many current operating systems are not designed to convey this information to the application.
For instance, a failure of a SCSI device is reported by the SCSI protocol in three levels of detail: a sense key that identiﬁes the general nature of the failure, such as a hardware error or an illegal request; an additional sense code that states the category of failure, such as a bad command parameter or a self-test failure; and an additional sense-code qualiﬁer that gives even more detail, such as which command parameter was in error or which hardware subsystem failed its self-test.
Further, many SCSI devices maintain internal pages of error-log information that can be requested by the host—but seldom are.
A user process may accidentally or purposely attempt to disrupt the normal operation of a system by attempting to issue illegal I/O instructions.
We can use various mechanisms to ensure that such disruptions cannot take place in the system.
To prevent users from performing illegal I/O, we deﬁne all I/O instructions to be privileged instructions.
Thus, users cannot issue I/O instructions directly; they must do it through the operating system.
To do I/O, a user program executes a system call to request that the operating system perform I/O on its behalf (Figure 13.11)
The operating system, executing in monitor mode, checks that the request is valid and, if it is, does the I/O requested.
In addition, any memory-mapped and I/O port memory locations must be protected from user access by the memory-protection system.
Note that a kernel cannot simply deny all user access.
Most graphics games and video editing and playback software need direct access to memory-mapped graphics controller memory to speed the performance of the graphics, for example.
The kernel might in this case provide a locking mechanism to allow a section of graphics memory (representing a window on screen) to be allocated to one process at a time.
The kernel needs to keep state information about the use of I/O components.
It does so through a variety of in-kernel data structures, such as the open-ﬁle.
Figure 13.11 Use of a system call to perform I/O.
The kernel uses many similar structures to track network connections, character-device communications, and other I/O activities.
Although each of these entities supports a read() operation, the semantics differ.
For instance, to read a user ﬁle, the kernel needs to probe the buffer cache before deciding whether to perform a disk I/O.
To read a raw disk, the kernel needs to ensure that the request size is a multiple of the disk sector size and is aligned on a sector boundary.
To read a process image, it is merely necessary to copy data from memory.
The open-ﬁle record, shown in Figure 13.12, contains a dispatch table that holds pointers to the appropriate routines, depending on the type of ﬁle.
An I/O request is converted into a message that is sent through the kernel to the I/O manager and then to the device driver, each of which may change the message contents.
For output, the message contains the data to be written.
For input, the message contains a buffer to receive the data.
The message-passing approach can add overhead, by comparison with procedural techniques that use shared data structures, but it simpliﬁes the structure and design of the I/O system and adds ﬂexibility.
In summary, the I/O subsystem coordinates an extensive collection of services that are available to applications and to other parts of the kernel.
The upper levels of the I/O subsystem access devices via the uniform interface provided by the device drivers.
Earlier, we described the handshaking between a device driver and a device controller, but we did not explain how the operating system connects an application request to a set of network wires or to a speciﬁc disk sector.
The application refers to the data by a ﬁle name.
Within a disk, the ﬁle system maps from the ﬁle name through the ﬁle-system directories to obtain the space allocation of the ﬁle.
For instance, in MS-DOS, the name maps to a number that indicates an entry in the ﬁle-access table, and that table entry tells which disk blocks are allocated to the ﬁle.
In UNIX, the name maps to an inode number, and the corresponding inode contains the space-allocation information.
But how is the connection made from the ﬁle name to the disk controller (the hardware port address or the memory-mapped controller registers)?
One method is that used by MS-DOS, a relatively simple operating system.
The ﬁrst part of an MS-DOS ﬁle name, preceding the colon, is a string that identiﬁes a speciﬁc hardware device.
For example, C: is the ﬁrst part of every ﬁle name on the primary hard disk.
The fact that C: represents the primary hard disk is built into the operating system; C: is mapped to a speciﬁc port address through a device table.
Because of the colon separator, the device name space is separate from the ﬁle-system name space.
This separation makes it easy for the operating system to associate extra functionality with each device.
For instance, it is easy to invoke spooling on any ﬁles written to the printer.
If, instead, the device name space is incorporated in the regular ﬁle-system name space, as it is in UNIX, the normal ﬁle-system name services are provided automatically.
If the ﬁle system provides ownership and access control to all ﬁle names, then devices have owners and access control.
Since ﬁles are stored on devices, such an interface provides access to the I/O system at two levels.
Names can be used to access the devices themselves or to access the ﬁles stored on the devices.
Unlike an MS-DOS ﬁle name, which has a colon separator, a UNIX path name has no clear separation of the device portion.
In fact, no part of the path name is the name of a device.
To resolve a path name, UNIX looks up the name in the mount table to ﬁnd the longest matching preﬁx; the corresponding entry in the mount table gives the device name.
This device name also has the form of a name in the ﬁle-system name space.
The major device number identiﬁes a device driver that should be called to handle I/O to this device.
The minor device number is passed to the device driver to index into a device table.
The corresponding device-table entry gives the port address or the memory-mapped address of the device controller.
Modern operating systems gain signiﬁcant ﬂexibility from the multiple stages of lookup tables in the path between a request and a physical device controller.
The mechanisms that pass requests between applications and drivers are general.
Thus, we can introduce new devices and drivers into a computer without recompiling the kernel.
In fact, some operating systems have the ability to load device drivers on demand.
It then loads in the necessary drivers, either immediately or when ﬁrst required by an I/O request.
We next describe the typical life cycle of a blocking read request, as depicted in Figure 13.13
The ﬁgure suggests that an I/O operation requires a great many steps that together consume a tremendous number of CPU cycles.
A process issues a blocking read() system call to a ﬁle descriptor of a ﬁle that has been opened previously.
The system-call code in the kernel checks the parameters for correctness.
In the case of input, if the data are already available in the buffer cache, the data are returned to the process, and the I/O request is completed.
The process is removed from the run queue and is placed on the wait queue for the device, and the I/O request is scheduled.
Eventually, the I/O subsystem sends the request to the device driver.
Depending on the operating system, the request is sent via a subroutine call or an in-kernel message.
The device driver allocates kernel buffer space to receive the data and schedules the I/O.
Eventually, the driver sends commands to the device controller by writing into the device-control registers.
The device controller operates the device hardware to perform the data transfer.
The driver may poll for status and data, or it may have set up a DMA transfer into kernel memory.
We assume that the transfer is managed by a DMA controller, which generates an interrupt when the transfer completes.
The correct interrupt handler receives the interrupt via the interruptvector table, stores any necessary data, signals the device driver, and returns from the interrupt.
The device driver receives the signal, determines which I/O request has completed, determines the request’s status, and signals the kernel I/O subsystem that the request has been completed.
The kernel transfers data or return codes to the address space of the requesting process and moves the process from the wait queue back to the ready queue.
Moving the process to the ready queue unblocks the process.
When the scheduler assigns the process to the CPU, the process resumes execution at the completion of the system call.
A stream is a full-duplex connection between a device driver and a user-level process.
It consists of a stream head that interfaces with the user process, a driver end that controls the device, and zero or more stream modules between the stream head and the driver end.
Each of these components contains a pair of queues —a read queue and a write queue.
Modules provide the functionality of STREAMS processing; they are pushed onto a stream by use of the ioctl() system call.
For example, a process can open a serial-port device via a stream and can push on a module to handle input editing.
Because messages are exchanged between queues in adjacent modules, a queue in one module may overﬂow an adjacent queue.
To prevent this from occurring, a queue may support ﬂow control.
Without ﬂow control, a queue accepts all messages and immediately sends them on to the queue in the adjacent module without buffering them.
This process involves exchanges of control messages between queues in adjacent modules.
A user process writes data to a device using either the write()orputmsg() system call.
The write() system call writes raw data to the stream, whereas putmsg() allows the user process to specify a message.
Regardless of the system call used by the user process, the stream head copies the data into a message and delivers it to the queue for the next module in line.
This copying of messages continues until the message is copied to the driver end and hence the device.
Similarly, the user process reads data from the stream head using either the read() or getmsg() system call.
If read() is used, the stream head gets a message from its adjacent queue and returns ordinary data (an unstructured byte stream) to the process.
If getmsg() is used, a message is returned to the process.
When writing to the stream, the user process will block, assuming the next queue uses ﬂow control, until there is room to copy the message.
Likewise, the user process will block when reading from the stream until data are available.
As mentioned, the driver end—like the stream head and modules—has a read and write queue.
However, the driver end must respond to interrupts, such as one triggered when a frame is ready to be read from a network.
Unlike the stream head, which may block if it is unable to copy a message to the next queue in line, the driver end must handle all incoming data.
The network card must simply drop further messages until there is enough buffer space to store incoming messages.
The beneﬁt of using STREAMS is that it provides a framework for a modular and incremental approach to writing device drivers and network protocols.
Modules may be used by different streams and hence by different devices.
For example, a networking module may be used by both an Ethernet network card and a 802.11 wireless network card.
Furthermore, rather than treating character-device I/O as an unstructured byte stream, STREAMS allows support for message boundaries and control information when communicating between modules.
Most UNIX variants support STREAMS, and it is the preferred method for writing protocols and device drivers.
It places heavy demands on the CPU to execute device-driver code and to schedule processes fairly and efﬁciently as they block and unblock.
The resulting context switches stress the CPU and its hardware caches.
I/O also exposes any inefﬁciencies in the interrupt-handling mechanisms in the kernel.
In addition, I/O loads down the memory bus during data copies between controllers and physical memory and again during copies between kernel buffers and application data space.
Coping gracefully with all these demands is one of the major concerns of a computer architect.
Although modern computers can handle many thousands of interrupts per second, interrupt handling is a relatively expensive task.
Each interrupt causes the system to perform a state change, to execute the interrupt handler, and then to restore state.
Programmed I/O can be more efﬁcient than interrupt-driven I/O, if the number of cycles spent in busy waiting is not excessive.
An I/O completion typically unblocks a process, leading to the full overhead of a context switch.
Consider, for instance, a remote login from one machine to another.
Each character typed on the local machine must be transported to the remote machine.
On the local machine, the character is typed; a keyboard interrupt is generated; and the character is passed through the interrupt handler to the device driver, to the kernel, and then to the user process.
The user process issues a network I/O system call to send the character to the remote machine.
The character then ﬂows into the local kernel, through the network layers that construct a network packet, and into the network device driver.
The network device driver transfers the packet to the network controller, which sends the character and generates an interrupt.
The interrupt is passed back up through the kernel to cause the network I/O system call to complete.
Now, the remote system’s network hardware receives the packet, and an interrupt is generated.
The character is unpacked from the network protocols and is given to the appropriate network daemon.
The network daemon identiﬁes which remote login session is involved and passes the packet to the appropriate subdaemon for that session.
Usually, the receiver echoes the character back to the sender; that approach doubles the work.
To eliminate the context switches involved in moving each character between daemons and the kernel, the Solaris developers reimplemented the telnet daemon using in-kernel threads.
Sun estimated that this improvement increased the maximum number of network logins from a few hundred to a few thousand on a large server.
Other systems use separate front-end processors for terminal I/O to reduce the interrupt burden on the main CPU.
For instance, a terminal concentrator can multiplex the trafﬁc from hundreds of remote terminals into one port on a large computer.
An I/O channel is a dedicated, special-purpose CPU found in mainframes and in other high-end systems.
The job of a channel is to ofﬂoad I/O work from the main CPU.
The idea is that the channels keep the data ﬂowing smoothly, while the main CPU remains free to process the data.
Like the device controllers and DMA controllers found in smaller computers, a channel can process more general and sophisticated programs, so channels can be tuned for particular workloads.
We can employ several principles to improve the efﬁciency of I/O:
Reduce the number of times that data must be copied in memory while.
Reduce the frequency of interrupts by using large transfers, smart controllers, and polling (if busy waiting can be minimized)
Increase concurrency by using DMA-knowledgeable controllers or channels to ofﬂoad simple data copying from the CPU.
Move processing primitives into hardware, to allow their operation in device controllers to be concurrent with CPU and bus operation.
Balance CPU, memory subsystem, bus, and I/O performance, because an overload in any one area will cause idleness in others.
The mouse movements and button clicks are converted into numeric values that are passed from hardware, through the mouse device driver, to the application.
By contrast, the functionality provided by the Windows disk device driver is complex.
It not only manages individual disks but also implements RAID arrays (Section 10.7)
To do so, it converts an application’s read or write request into a coordinated set of disk I/O operations.
Moreover, it implements sophisticated error-handling and data-recovery algorithms and takes many steps to optimize disk performance.
Where should the I/O functionality be implemented—in the device hardware, in the device driver, or in application software? Sometimes we observe the progression depicted in Figure 13.16
Initially, we implement experimental I/O algorithms at the application level, because application code is ﬂexible and application bugs are unlikely to cause system crashes.
Furthermore, by developing code at the application level, we avoid the need to reboot or reload device drivers after every change to the code.
An application-level implementation can be inefﬁcient, however, because of the overhead of context switches and because the application cannot take advantage of internal kernel data structures and kernel functionality (such as efﬁcient in-kernel messaging, threading, and locking)
When an application-level algorithm has demonstrated its worth, we may reimplement it in the kernel.
This can improve performance, but the development effort is more challenging, because an operating-system kernel is a large, complex software system.
Moreover, an in-kernel implementation must be thoroughly debugged to avoid data corruption and system crashes.
The highest performance may be obtained through a specialized implementation in hardware, either in the device or in the controller.
The disadvantages of a hardware implementation include the difﬁculty and expense of making further improvements or of ﬁxing bugs, the increased development time (months rather than days), and the decreased ﬂexibility.
For instance, a hardware RAID controller may not provide any means for the kernel to inﬂuence the order or location of individual block reads and writes, even if the kernel has special information about the workload that would enable it to improve the I/O performance.
The basic hardware elements involved in I/O are buses, device controllers, and the devices themselves.
The work of moving data between devices and main memory is performed by the CPU as programmed I/O or is ofﬂoaded to a DMA controller.
The kernel module that controls a device is a device driver.
The system-call interface provided to applications is designed to handle several basic categories of hardware, including block devices, character devices, memory-mapped ﬁles, network sockets, and programmed interval timers.
The system calls usually block the processes that issue them, but nonblocking and asynchronous calls are used by the kernel itself and by applications that must not sleep while waiting for an I/O operation to complete.
Among these are I/O scheduling, buffering, caching, spooling, device reservation, and error handling.
Another service, name translation, makes the connections between hardware devices and the symbolic ﬁle names used by applications.
It involves several levels of mapping that translate from character-string names, to speciﬁc device drivers and device addresses, and then to physical addresses of I/Oports or bus controllers.
This mapping may occur within the ﬁle-system name space, as it does in UNIX, or in a separate device name space, as it does in MS-DOS.
Through streams, drivers can be stacked, with data passing through them sequentially and bidirectionally for processing.
I/O system calls are costly in terms of CPU consumption because of the many layers of software between a physical device and an application.
These layers imply overhead from several sources: context switching to cross the kernel’s protection boundary, signal and interrupt handling to service the I/O devices, and the load on the CPU and memory system to copy data between kernel buffers and application space.
Is it possible to implement this handshaking with only one bit? If it is, describe the protocol.
If it is not, explain why one bit is insufﬁcient.
But if the I/O device is ready for service, polling can be much more efﬁcient than is catching and dispatching an interrupt.
Describe a hybrid strategy that combines polling, sleeping, and interrupts for I/O device service.
For each of these three strategies (pure polling, pure interrupts, hybrid), describe a computing environment in which that strategy is more efﬁcient than is either of the others.
Discuss what issues need to be considered in assigning priorities to different interrupts.
A tape drive on a multitasking operating system (with no device preallocation available)
A graphics card with direct bus connection, accessible through memory-mapped I/O.
For each of these scenarios, would you design the operating system to use buffering, spooling, caching, or a combination? Would you use polled I/O or interrupt-driven I/O? Give reasons for your choices.
What are the implications of this design for the initiation of I/O operations by the user program and their execution by the operating system?
Describe three circumstances under which nonblocking I/O should be used.
Why not just implement nonblocking I/O and have processes busy-wait until their devices are ready?
In certain settings, however, the code that is to be executed at the completion of the I/O can be broken into two separate pieces.
The ﬁrst piece executes immediately after the I/O completes and schedules a second interrupt for the remaining piece of code to be executed at a later time.
What is the purpose of using this strategy in the design of interrupt handlers?
How does this design complicate the design of the DMA controller? What are the advantages of providing such functionality?
Protection mechanisms control access to a system by limiting the types of ﬁle access permitted to users.
In addition, protection must ensure that only processes that have gained proper authorization from the operating system can operate on memory segments, the CPU, and other resources.
Protection is provided by a mechanism that controls the access of programs, processes, or users to the resources deﬁned by a computer system.
Thismechanismmust provideameans for specifying thecontrols to be imposed, together with a means of enforcing them.
Security ensures the authentication of system users to protect the integrity of the information stored in the system (both data and code), as well as the physical resources of the computer system.
The security system prevents unauthorized access, malicious destruction or alteration of data, and accidental introduction of inconsistency.
The processes in an operating system must be protected from one another’s activities.
To provide such protection, we can use various mechanisms to ensure that only processes that have gained proper authorization from the operating system can operate on the ﬁles, memory segments, CPU, and other resources of a system.
Protection refers to a mechanism for controlling the access of programs, processes, or users to the resources deﬁned by a computer system.
This mechanism must provide a means for specifying the controls to be imposed, together with a means of enforcement.
We distinguish between protection and security, which is a measure of conﬁdence that the integrity of a system and its data will be preserved.
To discuss the goals and principles of protection in a modern computer system.
To explain how protection domains, combined with an access matrix, are used to specify the resources a process may access.
As computer systems have become more sophisticated and pervasive in their applications, the need to protect their integrity has also grown.
Protection was originally conceived as an adjunct to multiprogramming operating systems, so that untrustworthy users might safely share a common logical name space, such as a directory of ﬁles, or share a common physical name space, such as memory.
Modern protection concepts have evolved to increase the reliability of any complex system that makes use of shared resources.
The most obvious is the need to prevent the mischievous, intentional violation of an access restriction.
Of more general importance, however, is the need to ensure that each program component active in a system uses system resources only in ways consistent with stated policies.
This requirement is an absolute one for a reliable system.
Protection can improve reliability by detecting latent errors at the interfaces between component subsystems.
Early detection of interface errors can often prevent contamination of a healthy subsystem by a malfunctioning subsystem.
Also, an unprotected resource cannot defend against use (or misuse) by an unauthorized or incompetent user.
The role of protection in a computer system is to provide a mechanism for the enforcement of the policies governing resource use.
These policies can be established in a variety of ways.
Some are ﬁxed in the design of the system, while others are formulated by the management of a system.
Still others are deﬁned by the individual users to protect their own ﬁles and programs.
A protection system must have the ﬂexibility to enforce a variety of policies.
Policies for resource use may vary by application, and they may change over time.
For these reasons, protection is no longer the concern solely of the designer of an operating system.
The application programmer needs to use protection mechanisms as well, to guard resources created and supported by an application subsystem against misuse.
In this chapter, we describe the protection mechanisms the operating system should provide, but application designers can use them as well in designing their own protection software.
Mechanisms determine how something will be done; policies decide what will be done.
The separation of policy and mechanism is important for ﬂexibility.
Policies are likely to change from place to place or time to time.
In the worst case, every change in policy would require a change in the underlying mechanism.
Using general mechanisms enables us to avoid such a situation.
Frequently, a guiding principle can be used throughout a project, such as the design of an operating system.
Following this principle simpliﬁes design decisions and keeps the system consistent and easy to understand.
A key, time-tested guiding principle for protection is the principle of least privilege.
It dictates that programs, users, and even systems be given just enough privileges to perform their tasks.
Consider the analogy of a security guard with a passkey.
If this key allows the guard into just the public areas that she guards, then misuse of the key will result in minimal damage.
If, however, the passkey allows access to all areas, then damage from its being lost, stolen, misused, copied, or otherwise compromised will be much greater.
An operating system following the principle of least privilege implements its features, programs, system calls, and data structures so that failure or compromise of a component does the minimum damage and allows the minimum damage to be done.
The overﬂow of a buffer in a system daemon might cause the daemon process to fail, for example, but should not allow the execution of code from the daemon process’s stack that would enable a remote.
Such an operating system also provides system calls and services that allow applications to be written with ﬁne-grained access controls.
It provides mechanisms to enable privileges when they are needed and to disable them when they are not needed.
Also beneﬁcial is the creation of audit trails for all privileged function access.
The audit trail allows the programmer, system administrator, or law-enforcement ofﬁcer to trace all protection and security activities on the system.
Managing users with the principle of least privilege entails creating a separate account for each user, with just the privileges that the user needs.
An operator who needs to mount tapes and back up ﬁles on the system has access to just those commands and ﬁles needed to accomplish the job.
Some systems implement role-based access control (RBAC) to provide this functionality.
Computers implemented in a computing facility under the principle of least privilege can be limited to running speciﬁc services, accessing speciﬁc remote hosts via speciﬁc services, and doing so during speciﬁc times.
The principle of least privilege can help produce a more secure computing environment.
For example, Windows 2000 has a complex protection scheme at its core and yet has many security holes.
By comparison, Solaris is considered relatively secure, even though it is a variant of UNIX, which historically was designed with little protection in mind.
One reason for the difference may be that Windows 2000 has more lines of code and more services than Solaris and thus has more to secure and protect.
Another reason could be that the protection scheme in Windows 2000 is incomplete or protects the wrong aspects of the operating system, leaving other areas vulnerable.
A computer system is a collection of processes and objects.
By objects, we mean both hardware objects (such as the CPU, memory segments, printers, disks, and tape drives) and software objects (such as ﬁles, programs, and semaphores)
Each object has a unique name that differentiates it from all other objects in the system, and each can be accessed only through well-deﬁned and meaningful operations.
The operations that are possible may depend on the object.
Memory segments can be read and written, whereas a CD-ROM or DVD-ROM can only be read.
Data ﬁles can be created, opened, read, written, closed, and deleted; program ﬁles can be read, written, executed, and deleted.
A process should be allowed to access only those resources for which it has authorization.
Furthermore, at any time, a process should be able to access only those resources that it currently requires to complete its task.
This second requirement, commonly referred to as the need-to-know principle, is useful in limiting the amount of damage a faulty process can cause in the system.
For example, when process p invokes procedure A(), the procedure should be allowed to access only its own variables and the formal parameters passed to it; it should not be able to access all the variables of process p.
Similarly, consider the case in which process p invokes a compiler to compile a particular ﬁle.
The compiler should not be able to access ﬁles arbitrarily but should have access only to a well-deﬁned subset of ﬁles (such as the source ﬁle, listing ﬁle, and so on) related to the ﬁle to be compiled.
Conversely, the compiler may have private ﬁles used for accounting or optimization purposes that process p should not be able to access.
The need-to-know principle is similar to the principle of least privilege discussed in Section 14.2 in that the goals of protection are to minimize the risks of possible security violations.
The association between a process and a domain may be either static, if the set of resources available to the process is ﬁxed throughout the process’s lifetime, or dynamic.
As might be expected, establishing dynamic protection domains is more complicated than establishing static protection domains.
If the association between processes and domains is ﬁxed, and we want to adhere to the need-to-know principle, then a mechanism must be available to change the content of a domain.
The reason stems from the fact that a process may execute in two different phases and may, for example, need read access in one phase and write access in another.
If a domain is static, we must deﬁne the domain to include both read and write access.
However, this arrangement provides more rights than are needed in each of the two phases, since we have read access in the phase where we need only write access, and vice versa.
We must allow the contents of a domain to be modiﬁed so that the domain always reﬂects the minimum necessary access rights.
If the association is dynamic, a mechanism is available to allow domain switching, enabling the process to switch from one domain to another.
We may also want to allow the content of a domain to be changed.
If we cannot change the content of a domain, we can provide the same effect by creating a new domain with the changed content and switching to that new domain when we want to change the domain content.
A domain can be realized in a variety of ways:
In this case, the set of objects that can be accessed depends on the identity of the user.
Domain switching occurs when the user is changed—generally when one user logs out and another user logs in.
In this case, the set of objects that can be accessed depends on the identity of the process.
Domain switching occurs when one process sends a message to another process and then waits for a response.
In this case, the set of objects that can be accessed corresponds to the local variables deﬁned within the procedure.
We discuss domain switching in greater detail in Section 14.4
When a process executes in monitor mode, it can execute privileged instructions and thus gain complete control of the computer system.
In contrast, when a process executes in user mode, it can invoke only nonprivileged instructions.
Consequently, it can execute only within its predeﬁned memory space.
These two modes protect the operating system (executing in monitor domain) from the user processes (executing in user domain)
In a multiprogrammed operating system, two protection domains are insufﬁcient, since users also want to be protected from one another.
We illustrate such a scheme by examining two inﬂuential operating systems—UNIX and MULTICS —to see how they implement these concepts.
In the UNIX operating system, a domain is associated with the user.
Switching the domain corresponds to changing the user identiﬁcation temporarily.
This change is accomplished through the ﬁle system as follows.
An owner identiﬁcation and a domain bit (known as the setuid bit) are associated with each ﬁle.
When the setuid bit is on, and a user executes that ﬁle, the userID is set to that of the owner of the ﬁle.
When the bit is off, however, the userID does not change.
For example, when a user A (that is, a user with userID = A) starts executing a ﬁle owned by B, whose associated domain bit is off, the userID of the process is set to A.
When the setuid bit is on, the userID is set to.
Other methods are used to change domains in operating systems in which userIDs are used for domain deﬁnition, because almost all systems need to provide such a mechanism.
This mechanism is used when an otherwise privileged facility needs to be made available to the general user population.
For instance, it might be desirable to allow users to access a network without letting them write their own networking programs.
In such a case, on a UNIX system, the setuid bit on a networking program would be set, causing the userID to change when the program was run.
The userID would change to that of a user with network access privilege (such as root, the most powerful userID)
One problem with this method is that if a user manages to create a ﬁle with userID root and with its setuid bit on, that user can become root and do anything and everything on the system.
An alternative to this method used in some other operating systems is to place privileged programs in a special directory.
The operating system is designed to change the userID of any program run from this directory, either to the equivalent of root or to the userID of the owner of the directory.
This eliminates one security problem, which occurs when intruders create programs to manipulate the setuid feature and hide the programs in the system for later use (using obscure ﬁle or directory names)
This method is less ﬂexible than that used in UNIX, however.
Even more restrictive, and thus more protective, are systems that simply do not allow a change of userID.
In these instances, special techniques must be used to allow users access to privileged facilities.
For instance, a daemon process may be started at boot time and run as a special userID.
Users then run a separate program, which sends requests to this process whenever they need to use the facility.
In any of these systems, great care must be taken in writing privileged programs.
Any oversight can result in a total lack of protection on the system.
Generally, these programs are the ﬁrst to be attacked by people trying to break into a system.
For example, security has been breached on many UNIX systems because of the setuid feature.
In the MULTICS system, the protection domains are organized hierarchically into a ring structure.
If j < i, then Di is a subset of Dj.
That is, a process executing in domain Dj has more privileges than does a process executing in domain Di.
A process executing in domain D0 has the most privileges.
A segment description includes an entry that identiﬁes the ring number.
The association between segments and rings is a policy decision with which we are not concerned here.
Domain switching in MULTICS occurs when a process crosses from one ring to another by calling a procedure in a different ring.
Obviously, this switch must be done in a controlled manner; otherwise, a process could start executing in ring 0, and no protection would be provided.
To allow controlled domain switching, we modify the ring ﬁeld of the segment descriptor to include the following:
However, if parameters are passed that refer to segments in a lower ring (that is, segments not accessible to the called procedure), then these segments must be copied into an area that can be accessed by the called procedure.
This scheme allows processes with limited access rights to call procedures in lower rings that have more access rights, but only in a carefully controlled manner.
The main disadvantage of the ring (or hierarchical) structure is that it does not allow us to enforce the need-to-know principle.
In particular, if an object must be accessible in domain Dj but not accessible in domain Di , then we must have j < i.
But this requirement means that every segment accessible in Di is also accessible in Dj.
The MULTICS protection system is generally more complex and less efﬁcient than are those used in current operating systems.
If protection interferes with the ease of use of the system or signiﬁcantly decreases system performance, then its use must be weighed carefully against the purpose of the system.
For instance, we would want to have a complex protection system on a computer used by a university to process students’ grades and also used by students for classwork.
A similar protection system would not be suited to a computer being used for number crunching, in which performance is of utmost importance.
We would prefer to separate the mechanism from the protection policy, allowing the same system to have complex or simple protection depending on the needs of its users.
To separate mechanism from policy, we require a more general model of protection.
Our general model of protection can be viewed abstractly as a matrix, called an access matrix.
The rows of the access matrix represent domains, and the columns represent objects.
Each entry in the matrix consists of a set of access rights.
Because the column deﬁnes objects explicitly, we can omit the object name from the access right.
The entry access(i,j) deﬁnes the set of operations that a process executing in domain Di can invoke on object Oj.
To illustrate these concepts, we consider the access matrix shown in Figure 14.3
A process executing in domain D4 has the same privileges as one executing in.
The laser printer can be accessed only by a process executing in domain D2
The access-matrix scheme provides us with the mechanism for specifying a variety of policies.
The mechanism consists of implementing the access matrix and ensuring that the semantic properties we have outlined hold.
More speciﬁcally, we must ensure that a process executing in domain Di can access only those objects speciﬁed in row i, and then only as allowed by the access-matrix entries.
The policy decisions involve which rights should be included in the (i, j)th entry.
We must also decide the domain in which each process executes.
This last policy is usually decided by the operating system.
The users normally decide the contents of the access-matrix entries.
When a user creates a new object Oj , the column Oj is added to the access matrix with the appropriate initialization entries, as dictated by the creator.
The user may decide to enter some rights in some entries in column j and other rights in other entries, as needed.
The access matrix provides an appropriate mechanism for deﬁning and implementing strict control for both static and dynamic association between processes and domains.
When we switch a process from one domain to another, we are executing an operation (switch) on an object (the domain)
We can control domain switching by including domains among the objects of the access matrix.
Similarly, when we change the content of the access matrix, we are performing an operation on an object: the access matrix.
Again, we can control these changes by including the access matrix itself as an object.
Actually, since each entry in the access matrix can be modiﬁed individually, we must consider each entry in the access matrix as an object to be protected.
Now, we need to consider only the operations possible on these new objects (domains and the access matrix) and decide how we want processes to be able to execute these operations.
Allowing controlled change in the contents of the access-matrix entries requires three additional operations: copy, owner, and control.
The ability to copy an access right from one domain (or row) of the access matrix to another is denoted by an asterisk (*) appended to the access right.
The copy right allows the access right to be copied only within the column (that is, for the object) for which the right is deﬁned.
A system may select only one of these three copy rights, or it may provide all three by identifying them as separate rights: copy, transfer, and limited copy.
We also need a mechanism to allow addition of new rights and removal of some rights.
If access(i, j) includes the owner right, then a process executing in domain Di can add and remove.
The copy and owner rights allow a process to change the entries in a column.
A mechanism is also needed to change the entries in a row.
If access(i, j) includes the control right, then a process executing in domain Di can remove any access right from row j.
The copy and owner rights provide us with a mechanism to limit the propagation of access rights.
However, they do not give us the appropriate tools for preventing the propagation (or disclosure) of information.
The problem of guaranteeing that no information initially held in an object can migrate outside of its execution environment is called the conﬁnement problem.
This problem is in general unsolvable (see the bibliographical notes at the end of the chapter)
These operations on the domains and the access matrix are not in themselves important, but they illustrate the ability of the access-matrix model to allow us to implement and control dynamic protection requirements.
New objects and new domains can be created dynamically and included in the.
However, we have shown only that the basic mechanism exists.
System designers and users must make the policy decisions concerning which domains are to have access to which objects in which ways.
How can the access matrix be implemented effectively? In general, the matrix will be sparse; that is, most of the entries will be empty.
Although datastructure techniques are available for representing sparse matrices, they are not particularly useful for this application, because of the way in which the protection facility is used.
Here, we ﬁrst describe several methods of implementing the access matrix and then compare the methods.
The table is usually large and thus cannot be kept in main memory, so additional I/O is needed.
Virtual memory techniques are often used for managing this table.
In addition, it is difﬁcult to take advantage of special groupings of objects or domains.
For example, if everyone can read a particular object, this object must have a separate entry in every domain.
Each column in the access matrix can be implemented as an access list for one object, as described in Section 11.6.2
This approach can be extended easily to deﬁne a list plus a default set of access rights.
When an operation M on an object Oj is attempted in domain.
Rather than associating the columns of the access matrix with the objects as access lists, we can associate each row with its domain.
A capability list for a domain is a list of objects together with the operations allowed on those objects.
An object is often represented by its physical name or address, called a capability.
To execute operation M on object Oj , the process executes the operation M, specifying the capability (or pointer) for object Oj as a parameter.
Simple possession of the capability means that access is allowed.
The capability list is associated with a domain, but it is never directly accessible to a process executing in that domain.
Rather, the capability list is itself a protected object, maintained by the operating system and accessed by the user only indirectly.
Capability-based protection relies on the fact that the capabilities are never allowed to migrate into any address space directly accessible by a user process (where they could be modiﬁed)
If all capabilities are secure, the object they protect is also secure against unauthorized access.
Capabilities were originally proposed as a kind of secure pointer, to meet the need for resource protection that was foreseen as multiprogrammed computer systems came of age.
The idea of an inherently protected pointer provides a foundation for protection that can be extended up to the application level.
To provide inherent protection, we must distinguish capabilities from other kinds of objects, and they must be interpreted by an abstract machine on which higher-level programs run.
Capabilities are usually distinguished from other data in one of two ways:
Each object has a tag to denote whether it is a capability or accessible data.
The tags themselves must not be directly accessible by an application program.
Hardware or ﬁrmware support may be used to enforce this restriction.
Although only one bit is necessary to distinguish between capabilities and other objects, more bits are often used.
This extension allows all objects to be tagged with their types by the hardware.
Thus, the hardware can distinguish integers, ﬂoating-point numbers, pointers, Booleans, characters, instructions, capabilities, and uninitialized values by their tags.
Alternatively, the address space associated with a program can be split into two parts.
One part is accessible to the program and contains the program’s normal data and instructions.
The other part, containing the capability list, is accessible only by the operating system.
A segmented memory space (Section 8.4) is useful to support this approach.
Several capability-based protection systems have been developed; we describe them brieﬂy in Section 14.8
The Mach operating system also uses a version of capability-based protection; it is described in Appendix B.
The lock–key scheme is a compromise between access lists and capability lists.
Each object has a list of unique bit patterns, called locks.
Similarly, each domain has a list of unique bit patterns, called keys.
A process executing in a domain can access an object only if that domain has a key that matches one of the locks of the object.
As with capability lists, the list of keys for a domain must be managed by the operating system on behalf of the domain.
Users are not allowed to examine or modify the list of keys (or locks) directly.
As you might expect, choosing a technique for implementing an access matrix involves various trade-offs.
Using a global table is simple; however, the table can be quite large and often cannot take advantage of special groupings of objects or domains.
When a user creates an object, he can specify which domains can access the object, as well as what operations are allowed.
However, because access-right information for a particular domain is not localized, determining the set of access rights for each domain is difﬁcult.
In addition, every access to the object must be checked, requiring a search of the access list.
In a large system with long access lists, this search can be time consuming.
Capability lists do not correspond directly to the needs of users, but they are useful for localizing information for a given process.
The process attempting access must present a capability for that access.
Then, the protection system needs only to verify that the capability is valid.
The lock–key mechanism, as mentioned, is a compromise between access lists and capability lists.
The mechanism can be both effective and ﬂexible, depending on the length of the keys.
The keys can be passed freely from domain to domain.
In addition, access privileges can be effectively revoked by the simple technique of changing some of the locks associated with the object (Section 14.7)
Most systems use a combination of access lists and capabilities.
When a process ﬁrst tries to access an object, the access list is searched.
Otherwise, a capability is created and attached to the process.
Additional references use the capability to demonstrate swiftly that access is allowed.
This strategy is used in the MULTICS system and in the CAL system.
As an example of how such a strategy works, consider a ﬁle system in which each ﬁle has an associated access list.
When a process opens a ﬁle, the directory structure is searched to ﬁnd the ﬁle, access permission is checked, and buffers are allocated.
All this information is recorded in a new entry in a ﬁle table associated with the process.
The operation returns an index into this table for the newly opened ﬁle.
All operations on the ﬁle are made by speciﬁcation of the index into the ﬁle table.
The entry in the ﬁle table then points to the ﬁle and its buffers.
When the ﬁle is closed, the ﬁle-table entry is deleted.
Since the ﬁle table is maintained by the operating system, the user cannot accidentally corrupt it.
Thus, the user can access only those ﬁles that have been opened.
Since access is checked when the ﬁle is opened, protection is ensured.
The right to access must still be checked on each access, and the ﬁle-table entry has a capability only for the allowed operations.
If a ﬁle is opened for reading, then a capability for read access is placed in the ﬁle-table entry.
If an attempt is made to write onto the ﬁle, the system identiﬁes this protection violation by comparing the requested operation with the capability in the ﬁle-table entry.
In Section 11.6.2, we described how access controls can be used on ﬁles within a ﬁle system.
Each ﬁle and directory is assigned an owner, a group, or possibly a list of users, and for each of those entities, access-control information is assigned.
A similar function can be added to other aspects of a computer system.
Solaris 10 advances the protection available in the operating system by explicitly adding the principle of least privilege via role-based access control (RBAC)
A privilege is the right to execute a system call or to use an option within that system call (such as opening a ﬁle with write access)
Privileges can be assigned to processes, limiting them to exactly the access they need to perform their work.
Users are assigned roles or can take roles based on passwords to the roles.
In this way, a user can take a role that enables a privilege, allowing the user to run a program to accomplish a speciﬁc task, as depicted in Figure 14.8
This implementation of privileges decreases the security risk associated with superusers and setuid programs.
Notice that this facility is similar to the access matrix described in Section 14.4
This relationship is further explored in the exercises at the end of the chapter.
In a dynamic protection system, we may sometimes need to revoke access rights to objects shared by different users.
Does revocation occur immediately, or is it delayed? If revocation is delayed, can we ﬁnd out when it will take place?
When an access right to an object is revoked, does it affect all the users who have an access right to that object, or can we specify a select group of users whose access rights should be revoked?
Can a subset of the rights associated with an object be revoked, or must we revoke all access rights for this object?
Can access be revoked permanently (that is, the revoked access right will never again be available), or can access be revoked and later be obtained again?
The access list is searched for any access rights to be revoked, and they are deleted from the list.
Revocation is immediate and can be general or selective, total or partial, and permanent or temporary.
Capabilities, however, present a much more difﬁcult revocation problem, as mentioned earlier.
Since the capabilities are distributed throughout the system, we must ﬁnd them before we can revoke them.
If a process wants to use a capability, it may ﬁnd that that capability has been deleted.
If access has been revoked, the process will not be able to reacquire the capability.
A list of pointers is maintained with each object, pointing to all capabilities associated with that object.
When revocation is required, we can follow these pointers, changing the capabilities as necessary.
Each capability points to a unique entry in a global table, which in turn points to the object.
We implement revocation by searching the global table for the desired entry and deleting it.
Then, when an access is attempted, the capability is found to point to an illegal table entry.
Table entries can be reused for other capabilities without difﬁculty, since both the capability and the table entry contain the unique name of the object.
A key is a unique bit pattern that can be associated with a capability.
This key is deﬁned when the capability is created, and it can be neither modiﬁed nor inspected by the process that owns the capability.
A master key is associated with each object; it can be deﬁned or replaced with the set-key operation.
When a capability is created, the current value of the master key is associated with the capability.
When the capability is exercised, its key is compared with the master key.
If the keys match, the operation is allowed to continue; otherwise, an exception condition is raised.
Revocation replaces the master key with a new value via the set-key operation, invalidating all previous capabilities for this object.
This scheme does not allow selective revocation, since only one master key is associated with each object.
If we associate a list of keys with each object, then selective revocation can be implemented.
Finally, we can group all keys into one global table of keys.
A capability is valid only if its key matches some key in the global table.
We implement revocation by removing the matching key from the table.
With this scheme, a key can be associated with several objects, and several keys can be associated with each object, providing maximum ﬂexibility.
In key-based schemes, the operations of deﬁning keys, inserting them into lists, and deleting them from lists should not be available to all users.
In particular, it would be reasonable to allow only the owner of an object to set the keys for that object.
This choice, however, is a policy decision that the protection system can implement but should not deﬁne.
These systems differ in their complexity and in the types of policies that can be implemented on them.
Neither system is widely used, but both provide interesting proving grounds for protection theories.
Hydra is a capability-based protection system that provides considerable ﬂexibility.
The system implements a ﬁxed set of possible access rights, including such basic forms of access as the right to read, write, or execute a memory segment.
In addition, a user (of the protection system) can declare other rights.
The interpretation of user-deﬁned rights is performed solely by the user’s program, but the system provides access protection for the use of these rights, as well as for the use of system-deﬁned rights.
The procedures that implement such operations are themselves a form of object, and they are accessed indirectly by capabilities.
The names of user-deﬁned procedures must be identiﬁed to the protection system if it is to deal with objects of the userdeﬁned type.
When the deﬁnition of an object is made known to Hydra, the names of operations on the type become auxiliary rights.
For a process to perform an operation on a typed object, the capability it holds for that object must contain the name of the operation being invoked among its auxiliary rights.
This scheme allows a procedure to be certiﬁed as trustworthy to act on a formal parameter of a speciﬁed type on behalf of any process that holds a right to execute the procedure.
The rights held by a trustworthy procedure are independent of, and may exceed, the rights held by the calling process.
However, such a procedure must not be regarded as universally trustworthy (the procedure is not allowed to act on other types, for instance), and the trustworthiness must not be extended to any other procedures or program segments that might be executed by a process.
Ampliﬁcation allows implementation procedures access to the representation variables of an abstract data type.
If a process holds a capability to a typed object A, for instance, this capability may include an auxiliary right to invoke some operation P but does not include any of the so-called kernel rights, such as read, write, or execute, on the segment that represents A.
Such a capability gives a process a means of indirect access (through the operation P) to the representation of A, but only for speciﬁc purposes.
When a process invokes the operation P on an object A, however, the capability for access to A may be ampliﬁed as control passes to the code body of P.
This ampliﬁcation may be necessary to allow P the right to access the storage segment representing A so as to implement the operation that P deﬁnes on the abstract data type.
The code body of P may be allowed to read or to write to the segment of A directly, even though the calling process cannot.
On return from P, the capability for A is restored to its original, unampliﬁed state.
This case is a typical one in which the rights held by a process for access to a protected segment must change dynamically, depending on the task to be performed.
The dynamic adjustment of rights is performed to guarantee consistency of a programmer-deﬁned abstraction.
Ampliﬁcation of rights can be stated explicitly in the declaration of an abstract type to the Hydra operating system.
When a user passes an object as an argument to a procedure, we may need to ensure that the procedure cannot modify the object.
We can implement this restriction readily by passing an access right that does not have the modiﬁcation (write) right.
However, if ampliﬁcation may occur, the right to modify may be reinstated.
In general, of course, a user may trust that a procedure performs its task correctly.
This assumption is not always correct, however, because of hardware or software errors.
The procedure-call mechanism of Hydra was designed as a direct solution to the problem of mutually suspicious subsystems.
Suppose that a program can be invoked as a service by a number of different users (for example, a sort routine, a compiler, a game)
When users invoke this service program, they take the risk that the program will malfunction and will either damage the given data or retain some access right to the data to be used (without authority) later.
Similarly, the service program may have some private ﬁles (for accounting purposes, for example) that should not.
A Hydra subsystem is built on top of its protection kernel and may require protection of its own components.
A subsystem interacts with the kernel through calls on a set of kernel-deﬁned primitives that deﬁne access rights to resources deﬁned by the subsystem.
The subsystem designer can deﬁne policies for use of these resources by user processes, but the policies are enforced by use of the standard access protection provided by the capability system.
Programmers can make direct use of the protection system after acquainting themselves with its features in the appropriate reference manual.
Hydra provides a large library of system-deﬁned procedures that can be called by user programs.
Programmers can explicitly incorporate calls on these system procedures into their program code or can use a program translator that has been interfaced to Hydra.
A different approach to capability-based protection has been taken in the design of the Cambridge CAP system.
CAP’s capability system is simpler and superﬁcially less powerful than that of Hydra.
However, closer examination shows that it, too, can be used to provide secure protection of user-deﬁned objects.
It can be used to provide access to objects, but the only rights provided are the standard read, write, and execute of the individual storage segments associated with the object.
Data capabilities are interpreted by microcode in the CAP machine.
The second kind of capability is the so-called software capability, which is protected, but not interpreted, by the CAP microcode.
It is interpreted by a protected (that is, privileged) procedure, which may be written by an application programmer as part of a subsystem.
A particular kind of rights ampliﬁcation is associated with a protected procedure.
When executing the code body of such a procedure, a process temporarily acquires the right to read or write the contents of a software capability itself.
This speciﬁc kind of rights ampliﬁcation corresponds to an implementation of the seal and unseal primitives on capabilities.
Of course, this privilege is still subject to type veriﬁcation to ensure that only software capabilities for a speciﬁed abstract type are passed to any such procedure.
Universal trust is not placed in any code other than the CAP machine’s microcode.
See the bibliographical notes at the end of the chapter for references.
The interpretation of a software capability is left completely to the subsystem, through the protected procedures it contains.
This scheme allows a variety of protection policies to be implemented.
Although programmers can deﬁne their own protected procedures (any of which might be incorrect), the security of the overall system cannot be compromised.
The basic protection system will not allow an unveriﬁed, user-deﬁned, protected procedure access to any storage segments (or capabilities) that do not belong to the protection environment in which it resides.
The most serious consequence of an insecure protected procedure is a protection breakdown of the subsystem for which that procedure has responsibility.
The designers of the CAP system have noted that the use of software capabilities allowed them to realize considerable economies in formulating and implementing protection policies commensurate with the requirements of abstract resources.
However, subsystem designers who want to make use of this facility cannot simply study a reference manual, as is the case with Hydra.
Instead, they must learn the principles and techniques of protection, since the system provides them with no library of procedures.
To the degree that protection is provided in existing computer systems, it is usually achieved through an operating-system kernel, which acts as a security agent to inspect and validate each attempt to access a protected resource.
Since comprehensive access validation may be a source of considerable overhead, either we must give it hardware support to reduce the cost of each validation, or we must allow the system designer to compromise the goals of protection.
Satisfying all these goals is difﬁcult if the ﬂexibility to implement protection policies is restricted by the support mechanisms provided or if protection environments are made larger than necessary to secure greater operational efﬁciency.
As operating systems have become more complex, and particularly as they have attempted to provide higher-level user interfaces, the goals of protection have become much more reﬁned.
The designers of protection systems have drawn heavily on ideas that originated in programming languages and especially on the concepts of abstract data types and objects.
Protection systems are now concerned not only with the identity of a resource to which access is attempted but also with the functional nature of that access.
In the newest protection systems, concern for the function to be invoked extends beyond a set of system-deﬁned functions, such as standard ﬁle-access methods, to include functions that may be user-deﬁned as well.
Policies for resource use may also vary, depending on the application, and they may be subject to change over time.
For these reasons, protection can no longer be considered a matter of concern only to the designer of an operating system.
It should also be available as a tool for use by the application designer, so that resources of an application subsystem can be guarded against tampering or the inﬂuence of an error.
Specifying the desired control of access to a shared resource in a system is making a declarative statement about the resource.
This kind of statement can be integrated into a language by an extension of its typing facility.
When protection is declared along with data typing, the designer of each subsystem can specify its requirements for protection, as well as its need for use of other resources in a system.
Such a speciﬁcation should be given directly as a program is composed, and in the language in which the program itself is stated.
Protection needs are simply declared, rather than programmed as a sequence of calls on procedures of an operating system.
Protection requirements can be stated independently of the facilities provided by a particular operating system.
The means for enforcement need not be provided by the designer of a subsystem.
A declarative notation is natural because access privileges are closely related to the linguistic concept of data type.
For example, suppose a language is used to generate code to run on the Cambridge CAP system.
On this system, every storage reference made on the underlying hardware occurs indirectly through a capability.
This restriction prevents any process from accessing a resource outside of its protection environment at any time.
However, a program may impose arbitrary restrictions on how a resource can be used during execution of a particular code segment.
We can implement such restrictions most readily by using the software capabilities provided by CAP.
A language implementation might provide standard protected procedures to interpret software capabilities that would realize the protection policies that could be speciﬁed in the language.
This scheme puts policy speciﬁcation at the disposal of the programmers, while freeing them from implementing its enforcement.
Even if a system does not provide a protection kernel as powerful as those of Hydra or CAP, mechanisms are still available for implementing protection speciﬁcations given in a programming language.
The principal distinction is that the security of this protection will not be as great as that supported by a protection kernel, because the mechanism must rely on more assumptions about the operational state of the system.
A compiler can separate references for which it can certify that no protection violation could occur from those for which a violation might be possible, and it can treat them differently.
The security provided by this form of protection rests on the assumption that the code generated by the compiler will not be modiﬁed prior to or during its execution.
What, then, are the relative merits of enforcement based solely on a kernel, as opposed to enforcement provided largely by a compiler?
Enforcement by a kernel provides a greater degree of security of the protection system itself than does the generation of protectionchecking code by a compiler.
In a compiler-supported scheme, security rests on correctness of the translator, on some underlying mechanism of storage management that protects the segments from which compiled code is executed, and, ultimately, on the security of ﬁles from which a program is loaded.
Some of these considerations also apply to a softwaresupported protection kernel, but to a lesser degree, since the kernel may reside in ﬁxed physical storage segments and may be loaded only from a designated ﬁle.
Hardware-supported protection is also relatively immune to protection violations that might occur as a result of either hardware or system software malfunction.
There are limits to the ﬂexibility of a protection kernel in implementing a user-deﬁned policy, although it may supply adequate facilities for the system to provide enforcement of its own policies.
With a programming language, protection policy can be declared and enforcement provided as needed by an implementation.
If a language does not provide sufﬁcient ﬂexibility, it can be extended or replaced with less disturbance than would be caused by the modiﬁcation of an operating-system kernel.
The greatest efﬁciency is obtained when enforcement of protection is supported directly by hardware (or microcode)
Insofar as software support is required, language-based enforcement has the advantage that static access enforcement can be veriﬁed off-line at compile time.
Also, since an intelligent compiler can tailor the enforcement mechanism to meet the speciﬁed need, the ﬁxed overhead of kernel calls can often be avoided.
In summary, the speciﬁcation of protection in a programming language allows the high-level description of policies for the allocation and use of resources.
A language implementation can provide software for protection enforcement when automatic hardware-supported checking is unavailable.
In addition, it can interpret protection speciﬁcations to generate calls on whatever protection system is provided by the hardware and the operating system.
One way of making protection available to the application program is through the use of a software capability that could be used as an object of computation.
Inherent in this concept is the idea that certain program components might have the privilege of creating or examining these software capabilities.
Such components might copy the data structure or pass its address to other program components, but they could not gain access to its contents.
The reason for introducing such software capabilities is to bring a protection mechanism into the programming language.
The only problem with the concept as proposed is that the use of the seal and unseal operations takes a procedural approach to specifying protection.
A nonprocedural or declarative notation seems a preferable way to make protection available to the application programmer.
What is needed is a safe, dynamic access-control mechanism for distributing capabilities to system resources among user processes.
To contribute to the overall reliability of a system, the access-control mechanism should be safe to use.
To be useful in practice, it should also be reasonably efﬁcient.
This requirement has led to the development of a number of language constructs that allow the programmer to declare various restrictions on the use of a speciﬁc managed resource.
In particular, mechanisms ensure that a user process will use the managed resource only if it was granted a capability to that resource.
Specifying the type of operations that a particular process may invoke on an allocated resource (for example, a reader of a ﬁle should be allowed only to read the ﬁle, whereas a writer should be able both to read and to write)
It should not be necessary to grant the same set of rights to every user process, and it should be impossible for a process to enlarge its set of access rights, except with the authorization of the access-control mechanism.
Specifying the order in which a particular process may invoke the various operations of a resource (for example, a ﬁle must be opened before it can be read)
It should be possible to give two processes different restrictions on the order in which they can invoke the operations of the allocated resource.
The incorporation of protection concepts into programming languages, as a practical tool for system design, is in its infancy.
Protection will likely become a matter of greater concern to the designers of new systems with distributed architectures and increasingly stringent requirements on data security.
Then the importance of suitable language notations in which to express protection requirements will be recognized more widely.
Because Java was designed to run in a distributed environment, the Java virtual machine—or JVM—has many built-in protection mechanisms.
Java programs are composed of classes, each of which is a collection of data ﬁelds and functions (called methods) that operate on those ﬁelds.
The JVM loads a class in response to a request to create instances (or objects) of that class.
One of the most novel and useful features of Java is its support for dynamically loading untrusted classes over a network and for executing mutually distrusting classes within the same JVM.
Classes running in the same JVM may be from different sources and may not be equally trusted.
As a result, enforcing protection at the granularity of the JVM process is insufﬁcient.
Intuitively, whether a request to open a ﬁle should be allowed will generally depend on which class has requested the open.
When the JVM loads a class, it assigns the class to a protection domain that gives the permissions of that class.
The protection domain to which the class is assigned depends on the URL from which the class was loaded and any digital signatures on the class ﬁle.
A conﬁgurable policy ﬁle determines the permissions granted to the domain (and its classes)
For example, classes loaded from a trusted server might be placed in a protection domain that allows them to access ﬁles in the user’s home directory, whereas classes loaded from an untrusted server might have no ﬁle access permissions at all.
It can be complicated for the JVM to determine what class is responsible for a request to access a protected resource.
Accesses are often performed indirectly, through system libraries or other classes.
For example, consider a class that is not allowed to open network connections.
It could call a system library to request the load of the contents of a URL.
The JVM must decide whether or not to open a network connection for this request.
But which class should be used to determine if the connection should be allowed, the application or the system library?
The philosophy adopted in Java is to require the library class to explicitly permit a network connection.
More generally, in order to access a protected resource, some method in the calling sequence that resulted in the request must explicitly assert the privilege to access the resource.
By doing so, this method takes responsibility for the request.
Presumably, it will also perform whatever checks are necessary to ensure the safety of the request.
Of course, not every method is allowed to assert a privilege; a method can assert a privilege only if its class is in a protection domain that is itself allowed to exercise the privilege.
Every thread in the JVM has an associated stack of its ongoing method invocations.
When a caller may not be trusted, a method executes an access request within a doPrivileged block to perform the access to a protected resource directly or indirectly.
When the doPrivileged block is entered, the stack frame for this method is annotated to indicate this fact.
When an access to a protected resource is subsequently requested, either by this method or a method it calls, a call to checkPermissions() is used to invoke stack inspection to determine if the request should be allowed.
The inspection examines stack frames on the calling thread’s stack, starting from the most recently added frame and working toward the oldest.
If a stack frame is ﬁrst found that has the doPrivileged() annotation, then checkPermissions() returns immediately and silently, allowing the access.
If the stack inspection exhausts the stack without ﬁnding either type of frame, then whether access is allowed depends on the implementation (for example, some implementations of the JVM may allow access, while other implementations may not)
Here, the gui() method of a class in the untrusted applet protection domain performs two operations, ﬁrst a get() and then an open()
For this reason, the untrusted applet’s get() invocation will succeed: the checkPermissions() call in the networking library encounters the stack frame of the get() method, which performed its open() in a doPrivileged block.
However, the untrusted applet’s open() invocation will result in an exception, because the checkPermissions() call ﬁnds no doPrivileged annotation before encountering the stack frame of the gui() method.
Of course, for stack inspection to work, a program must be unable to modify the annotations on its own stack frame or to otherwise manipulate stack inspection.
This is one of the most important differences between Java and many other languages (including C++)
A Java program cannot directly access memory; it can manipulate only an object for which it has a reference.
References cannot be forged, and manipulations are made only through welldeﬁned interfaces.
Compliance is enforced through a sophisticated collection of load-time and run-time checks.
As a result, an object cannot manipulate its runtime stack, because it cannot get a reference to the stack or other components of the protection system.
More generally, Java’s load-time and run-time checks enforce type safety of Java classes.
Type safety ensures that classes cannot treat integers as pointers, write past the end of an array, or otherwise access memory in arbitrary ways.
Rather, a program can access an object only via the methods deﬁned on that object by its class.
This is the foundation of Java protection, since it enables a class to effectively encapsulate and protect its data and methods from other classes loaded in the same JVM.
For example, a variable can be deﬁned as private so that only the class that contains it can access it or protected so that it can be accessed only by the class that contains it, subclasses of that class, or classes in the same package.
Computer systems contain many objects, and they need to be protected from misuse.
Objects may be hardware (such as memory, CPU time, and I/O devices) or software (such as ﬁles, programs, and semaphores)
An access right is permission to perform an operation on an object.
Processes execute in domains and may use any of the access rights in the domain to access and manipulate objects.
During its lifetime, a process may be either bound to a protection domain or allowed to switch from one domain to another.
The access matrix is a general model of protection that provides a mechanism for protection without imposing a particular protection policy on the system or its users.
The separation of policy and mechanism is an important design property.
It is normally implemented either as access lists associated with each object or as capability lists associated with each domain.
We can include dynamic protection in the access-matrix model by considering domains and the access matrix itself as objects.
Revocation of access rights in a dynamic protection model is typically easier to implement with an access-list scheme than with a capability list.
Real systems are much more limited than the general model and tend to provide protection only for ﬁles.
Hydra, the Cambridge CAP system, and Mach are capability systems that extend protection to user-deﬁned software objects.
Solaris 10 implements the principle of least privilege via role-based access control, a form of the access matrix.
Language-based protection provides ﬁner-grained arbitration of requests and privileges than the operating system is able to provide.
For example, a single Java JVM can run several threads, each in a different protection class.
It enforces the resource requests through sophisticated stack inspection and via the type safety of the language.
When such a ﬁle is deleted, its storage area is overwritten by some random bits.
The access rights of a program at a particular level in the ring structure are considered a set of capabilities.
What is the relationship between the capabilities of a domain at level j and a domain at level i to an object (for j > i)?
Thus, a descendant can never have the ability to do anything that its ancestors cannot do.
The root of the tree is the operating system, which has the ability to do anything.
Assume that the set of access rights is represented by an access matrix, A.
A(x,y) deﬁnes the access rights of process x to object y.
If x is a descendant of z, what is the relationship between A(x,y) and A(z,y) for an arbitrary object y?
Suppose that we allow a process with number n to access an object with number m only if n > m.
At this point, the object should also be deleted, and the space it occupies should be returned to the system.
How does the system ensure that the user cannot modify the contents of the list?
If we were to implement the system calls of a typical operating system and store them in a segment associated with ring 0, what should be the values stored in the ring ﬁeld of the segment descriptor? What happens during a system call when a process executing in a higher-numbered ring invokes a procedure in ring 0?
Is this approach equivalent to including the access privileges of domain B in those of domain A?
How does this practice compare with the cross-ring calls in a ring-protection scheme?
Java program were allowed to directly alter the annotations of its stack frame.
The concept of a capability evolved from Iliffe’s and Jodeit’s codewords, which were implemented in the Rice University computer ([Iliffe and Jodeit (1962)])
The term capability was introduced by [Dennis and Horn (1966)]
The CAP system was described by [Needham and Walker (1977)]
Protection, as we discussed in Chapter 14, is strictly an internal problem: How do we provide controlled access to programs and data stored in a computer system? Security, on the other hand, requires not only an adequate protection system but also consideration of the external environment within which the system operates.
A protection system is ineffective if user authentication is compromised or a program is run by an unauthorized user.
Computer resources must be guarded against unauthorized access, malicious destruction or alteration, and accidental introduction of inconsistency.
These resources include information stored in the system (both data and code), as well as the CPU, memory, disks, tapes, and networking that are the computer.
In this chapter, we start by examining ways in which resources may be accidentally or purposely misused.
Finally, we look at mechanisms to guard against or detect attacks.
In many applications, ensuring the security of the computer system is worth considerable effort.
Large commercial systems containing payroll or other ﬁnancial data are inviting targets to thieves.
Systems that contain data pertaining to corporate operations may be of interest to unscrupulous competitors.
Furthermore, loss of such data, whether by accident or fraud, can seriously impair the ability of the corporation to function.
In Chapter 14, we discussed mechanisms that the operating system can provide (with appropriate aid from the hardware) that allow users to protect.
These mechanisms work well only as long as the users conform to the intended use of and access to these resources.
We say that a system is secure if its resources are used and accessed as intended under all circumstances.
Nonetheless, we must have mechanisms to make security breaches a rare occurrence, rather than the norm.
Security violations (or misuse) of the system can be categorized as intentional (malicious) or accidental.
It is easier to protect against accidental misuse than against malicious misuse.
For the most part, protection mechanisms are the core of protection from accidents.
The following list includes several forms of accidental and malicious security violations.
We should note that in our discussion of security, we use the terms intruder and cracker for those attempting to breach security.
In addition, a threat is the potential for a security violation, such as the discovery of a vulnerability, whereas an attack is the attempt to break security.
This type of violation involves unauthorized reading of data (or theft of information)
Typically, a breach of conﬁdentiality is the goal of an intruder.
Capturing secret data from a system or a data stream, such as credit-card information or identity information for identity theft, can result directly in money for the intruder.
Such attacks can, for example, result in passing of liability to an innocent party or modiﬁcation of the source code of an important commercial application.
Some crackers would rather wreak havoc and gain status or bragging rights than gain ﬁnancially.
Website defacement is a common example of this type of security breach.
For example, an intruder (or intrusion program) may install a daemon on a system that acts as a ﬁle server.
The original Internet worm turned into a DOS attack when a bug failed to delay its rapid spread.
Attackers use several standard methods in their attempts to breach security.
The most common is masquerading, in which one participant in a communication pretends to be someone else (another host or another person)
By masquerading, attackers breach authentication, the correctness of identiﬁcation; they can then gain access that they would not normally be allowed or escalate their privileges—obtain privileges to which they would not normally be entitled.
Another common attack is to replay a captured exchange of data.
A replay attack consists of the malicious or fraudulent repeat of a valid data transmission.
Sometimes the replay comprises the entire attackfor example, in a repeat of a request to transfer money.
But frequently it is done along with message modiﬁcation, again to escalate privileges.
Yet another kind of attack is the man-in-the-middle attack, in which an attacker sits in the data ﬂow of a communication, masquerading as the sender to the receiver, and vice versa.
In a network communication, a man-in-the-middle attack may be preceded by a session hijacking, in which an active communication session is intercepted.
As we have already suggested, absolute protection of the system from malicious abuse is not possible, but the cost to the perpetrator can be made sufﬁciently high to deter most intruders.
In some cases, such as a denial-ofservice attack, it is preferable to prevent the attack but sufﬁcient to detect the attack so that countermeasures can be taken.
To protect a system, we must take security measures at four levels:
The site or sites containing the computer systems must be physically secured against armed or surreptitious entry by intruders.
Both the machine rooms and the terminals or workstations that have access to the machines must be secured.
Authorization must be done carefully to assure that only appropriate users have access to the system.
Even authorized users, however, may be “encouraged” to let others use their access (in exchange for a bribe, for example)
They may also be tricked into allowing access via social engineering.
Here, a legitimate-looking e-mail or web page misleads a user into entering conﬁdential information.
Another technique is dumpster diving, a general term for attempting to gather information in order to gain unauthorized access to the computer (by looking through trash, ﬁnding phone books, or ﬁnding notes containing passwords, for example)
These security problems are management and personnel issues, not problems pertaining to operating systems.
The system must protect itself from accidental or purposeful security breaches.
A stack overﬂow could allow the launching of an unauthorized process.
Much computer data in modern systems travels over private leased lines, shared lines like the Internet, wireless connections, or dial-up lines.
Intercepting these data could be just as harmful as breaking into a computer, and interruption of communications could constitute a remote denial-of-service attack, diminishing users’ use of and trust in the system.
Security at the ﬁrst two levels must be maintained if operating-system security is to be ensured.
A weakness at a high level of security (physical or human) allows circumvention of strict low-level (operating-system) security measures.
Thus, the old adage that a chain is only as strong as its weakest link is especially true of system security.
All of these aspects must be addressed for security to be maintained.
Furthermore, the system must provide protection (Chapter 14) to allow the implementation of security features.
Without the ability to authorize users and processes, to control their access, and to log their activities, it would be impossible for an operating system to implement security measures or to run securely.
Hardware protection features are needed to support an overall protection scheme.
For example, a system without memory protection cannot be secure.
New hardware features are allowing systems to be made more secure, as we shall discuss.
As intruders exploit security vulnerabilities, security countermeasures are created and deployed.
This causes intruders to become more sophisticated in their attacks.
For example, recent security incidents include the use of spyware to provide a conduit for spam through innocent systems (we discuss this practice in Section 15.2)
This cat-and-mouse game is likely to continue, with more security tools needed to block the escalating intruder techniques and activities.
In the remainder of this chapter, we address security at the network and operating-system levels.
Security at the physical and human levels, although important, is for the most part beyond the scope of this text.
Security within the operating system and between operating systems is implemented in several.
Processes, along with the kernel, are the only means of accomplishing work on a computer.
Therefore, writing a program that creates a breach of security, or causing a normal process to change its behavior and create a breach, is a common goal of crackers.
In fact, even most nonprogram security events have as their goal causing a program threat.
For example, while it is useful to log in to a system without authorization, it is quite a lot more useful to leave behind a back-door daemon that provides information or allows easy access even if the original exploit is blocked.
In this section, we describe common methods by which programs cause security breaches.
Note that there is considerable variation in the naming conventions for security holes and that we use the most common or descriptive terms.
Many systems have mechanisms for allowing programs written by users to be executed by other users.
If these programs are executed in a domain that provides the access rights of the executing user, the other users may misuse these rights.
A text-editor program, for example, may include code to search the ﬁle to be edited for certain keywords.
If any are found, the entire ﬁle may be copied to a special area accessible to the creator of the text editor.
A code segment that misuses its environment is called a Trojan horse.
Long search paths, such as are common on UNIX systems, exacerbate the Trojanhorse problem.
The search path lists the set of directories to search when an ambiguous program name is given.
The path is searched for a ﬁle of that name, and the ﬁle is executed.
All the directories in such a search path must be secure, or a Trojan horse could be slipped into the user’s path and executed accidentally.
For instance, consider the use of the “.” character in a search path.
The “.” tells the shell to include the current directory in the search.
Thus, if a user has “.” in her search path, has set her current directory to a friend’s directory, and enters the name of a normal system command, the command may be executed from the friend’s directory.
The program will run within the user’s domain, allowing the program to do anything that the user is allowed to do, including deleting the user’s ﬁles, for instance.
A variation of the Trojan horse is a program that emulates a login program.
An unsuspecting user starts to log in at a terminal and notices that he has apparently mistyped his password.
What has happened is that his authentication key and password have been stolen by the login emulator, which was left running on the terminal by the thief.
The emulator stored away the password, printed out a login error message, and exited; the user was then provided with a genuine login prompt.
Spyware sometimes accompanies a program that the user has chosen to install.
Most frequently, it comes along with freeware or shareware programs, but sometimes it is included with commercial software.
The goal of spyware is to download ads to display on the user’s system, create pop-up browser windows when certain sites are visited, or capture information from the user’s system and return it to a central site.
This latter practice is an example of a general category of attacks known as covert channels, in which surreptitious communication occurs.
For example, the installation of an innocuous-seeming program on a Windows system could result in the loading of a spyware daemon.
The spyware could contact a central site, be given a message and a list of recipient addresses, and deliver a spam message to those users from the Windows machine.
This theft of service is not even considered a crime in most countries!
Spyware is a micro example of a macro problem: violation of the principle of least privilege.
Under most circumstances, a user of an operating system does not need to install network daemons.
First, a user may run with more privileges than necessary (for example, as the administrator), allowing programs that she runs to have more access to the system than is necessary.
This is a case of human error—a common security weakness.
Second, an operating system may allow by default more privileges than a normal user needs.
An operating system (and, indeed, software in general) should allow ﬁne-grained control of access and security, but it must also be easy to manage and understand.
Inconvenient or inadequate security measures are bound to be circumvented, causing an overall weakening of the security they were designed to implement.
The designer of a program or system might leave a hole in the software that only she is capable of using.
This type of security breach (or trap door) was shown in the movieWarGames.
For instance, the code might check for a speciﬁc user ID or password, and it might circumvent normal security procedures.
Programmers have been arrested for embezzling from banks by including rounding errors in their code and having the occasional half-cent credited to their accounts.
This account crediting can add up to a large amount of money, considering the number of transactions that a large bank executes.
A clever trap door could be included in a compiler.
The compiler could generate standard object code as well as a trap door, regardless of the source code being compiled.
This activity is particularly nefarious, since a search of the source code of the program will not reveal any problems.
Only the source code of the compiler would contain the information.
Trap doors pose a difﬁcult problem because, to detect them, we have to analyze all the source code for all components of a system.
Given that software systems may consist of millions of lines of code, this analysis is not done frequently, and frequently it is not done at all!
Consider a program that initiates a security incident only under certain circumstances.
It would be hard to detect because under normal operations, there would be no security hole.
However, when a predeﬁned set of parameters was met, the security hole would be created.
A programmer, for example, might write code to detect whether he was still employed; if that check failed, a daemon could be spawned to allow remote access, or code could be launched to cause damage to the site.
The stack- or buffer-overﬂow attack is the most common way for an attacker outside the system, on a network or dial-up connection, to gain unauthorized access to the target system.
An authorized user of the system may also use this exploit for privilege escalation.
The bug can be a simple case of poor programming, in which the programmer neglected to code bounds checking on an input ﬁeld.
In this case, the attacker sends more data than the program was expecting.
By using trial and error, or by examining the source code of the attacked program if it is available, the attacker determines the vulnerability and writes a program to do the following:
Overﬂow an input ﬁeld, command-line argument, or input buffer—for example, on a network daemon—until it writes into the stack.
Write a simple set of code for the next space in the stack that includes the commands that the attacker wishes to execute—for instance, spawn a shell.
The result of this attack program’s execution will be a root shell or other privileged command execution.
For instance, if a web-page form expects a user name to be entered into a ﬁeld, the attacker could send the user name, plus extra characters to overﬂow the buffer and reach the stack, plus a new return address to load onto the stack, plus the code the attacker wants to run.
When the buffer-reading subroutine returns from execution, the return address is the exploit code, and the code is run.
This program creates a character array of size BUFFER SIZE and copies the contents of the parameter provided on the command line—argv[1]
As long as the size of this parameter is less than BUFFER SIZE (we need one byte to store the null terminator), this program works properly.
But consider what happens if the parameter provided on the command line is longer than BUFFER SIZE.
Thus, this program suffers from a potential buffer-overﬂow problem in which copied data overﬂow the buffer array.
Furthermore, lack of bounds checking is not the only possible cause of the behavior of the program in Figure 15.2
The program could instead have been carefully designed to compromise the integrity of the system.
We now consider the possible security vulnerabilities of a buffer overﬂow.
When a function is invoked in a typical computer architecture, the variables deﬁned locally to the function (sometimes known as automatic variables), the parameters passed to the function, and the address to which control returns once the function exits are stored in a stack frame.
The layout for a typical stack frame is shown in Figure 15.3
Examining the stack frame from top to bottom, we ﬁrst see the parameters passed to the function, followed by any automatic variables declared in the function.
We next see the frame pointer, which is the address of the beginning of the stack frame.
The frame pointer must be saved on the stack, as the value of the stack pointer can vary during the function call.
The saved frame pointer allows relative access to parameters and automatic variables.
Given this standard memory layout, a cracker could execute a bufferoverﬂow attack.
Her goal is to replace the return address in the stack frame so that it now points to the code segment containing the attacking program.
The programmer ﬁrst writes a short code segment such as the following:
Using the execvp() system call, this code segment creates a shell process.
If the program being attacked runs with system-wide permissions, this newly created shell will gain complete access to the system.
Of course, the code segment could do anything allowed by the privileges of the attacked process.
This code segment is then compiled so that the assembly language instructions can be modiﬁed.
The primary modiﬁcation is to remove unnecessary features in the code, thereby reducing the code size so that it can ﬁt into a stack frame.
This assembled code fragment is now a binary sequence that will be at the heart of the attack.
Let’s assume that when the main() function is called in that program, the stack frame appears as shown in Figure 15.4(a)
That address is the location of the code the attacker wants executed.
The binary sequence is appended with the necessary amount of NO-OP instructions (for NO-OPeration) to ﬁll the stack frame up to the location of the return address, and the location of buffer[0], the new return address, is added.
The attack is complete when the attacker gives this constructed binary sequence as input to the process.
Now, when control returns from main(), instead of returning to the location speciﬁed by the old value of the return address, we return to the modiﬁed shell code, which runs with the access rights of the attacked process! Figure 15.4(b) contains the modiﬁed shell code.
In this example, we considered the possibility that the program being attackedthe code shown in Figure 15.2—ran with system-wide permissions.
However, the code segment that runs once the value of the return address has been modiﬁed might perform any type of malicious act, such as deleting ﬁles, opening network ports for further exploitation, and so on.
This example buffer-overﬂow attack reveals that considerable knowledge and programming skill are needed to recognize exploitable code and then to exploit it.
Unfortunately, it does not take great programmers to launch security attacks.
Rather, one cracker can determine the bug and then write an exploit.
Anyone with rudimentary computer skills and access to the exploita so-called script kiddie—can then try to launch the attack at target systems.
The buffer-overﬂow attack is especially pernicious because it can be run between systems and can travel over allowed communication channels.
Such attacks can occur within protocols that are expected to be used to communicate with the target machine, and they can therefore be hard to detect and prevent.
They can even bypass the security added by ﬁrewalls (Section 15.7)
One solution to this problem is for the CPU to have a feature that disallows execution of code in a stack section of memory.
Recent versions of Sun’s SPARC chip include this setting, and recent versions of Solaris enable it.
The return address of the overﬂowed routine can still be modiﬁed; but when the return address is within the stack and the code there attempts to execute, an exception is generated, and the program is halted with an error.
Recent versions of AMD and Intel x86 chips include the NX feature to prevent this type of attack.
The hardware implementation involves the use of a new bit in the page tables of the CPUs.
This bit marks the associated page as nonexecutable, so that instructions cannot be read from it and executed.
As this feature becomes more prevalent, buffer-overﬂow attacks should greatly diminish.
A virus is a fragment of code embedded in a legitimate program.
Viruses are self-replicating and are designed to “infect” other programs.
They can wreak havoc in a system by modifying or destroying ﬁles and causing system crashes and program malfunctions.
As with most penetration attacks, viruses are very speciﬁc to architectures, operating systems, and applications.
Even if a virus does infect such a program, its powers usually are limited because other aspects of the system are protected.
Viruses are usually borne via e-mail, with spam the most common vector.
They can also spread when users download viral programs from Internet ﬁle-sharing services or exchange infected disks.
Another common form of virus transmission uses Microsoft Ofﬁce ﬁles, such as Microsoft Word documents.
These documents can contain macros (or Visual Basic programs) that programs in the Ofﬁce suite (Word, PowerPoint, and Excel) will execute automatically.
Because these programs run under the user’s own account, the macros can run largely unconstrained (for example, deleting user ﬁles at will)
Commonly, the virus will also e-mail itself to others in the user’s contact list.
Here is a code sample that shows how simple it is to write a Visual Basic macro that a virus could use to format the hard drive of a Windows computer as soon as the ﬁle containing the macro was opened:
How do viruses work? Once a virus reaches a target machine, a program known as a virus dropper inserts the virus into the system.
The virus dropper is usually a Trojan horse, executed for other reasons but installing the virus as its core activity.
Once installed, the virus may do any one of a number of things.
There are literally thousands of viruses, but they fall into several main categories.
Note that many viruses belong to more than one category.
A standard ﬁle virus infects a system by appending itself to a ﬁle.
It changes the start of the program so that execution jumps to its code.
After it executes, it returns control to the program so that its execution is not noticed.
File viruses are sometimes known as parasitic viruses, as they leave no full ﬁles behind and leave the host program still functional.
A boot virus infects the boot sector of the system, executing every time the system is booted and before the operating system is loaded.
These viruses are also known as memory viruses, because they do not appear in the ﬁle system.
Most viruses are written in a low-level language, such as assembly or C.
Macro viruses are written in a high-level language, such as Visual Basic.
These viruses are triggered when a program capable of executing the macro is run.
For example, a macro virus could be contained in a spreadsheet ﬁle.
A source code virus looks for source code and modiﬁes it to include the virus and to help spread the virus.
A polymorphic virus changes each time it is installed to avoid detection by antivirus software.
The changes do not affect the virus’s functionality but rather change the virus’s signature.
A virus signature is a pattern that can be used to identify a virus, typically a series of bytes that make up the virus code.
An encrypted virus includes decryption code along with the encrypted virus, again to avoid detection.
This tricky virus attempts to avoid detection by modifying parts of the system that could be used to detect it.
For example, it could modify the read system call so that if the ﬁle it has modiﬁed is read, the original form of the code is returned rather than the infected code.
This virus attempts to bypass detection by an antivirus scanner by installing itself in the interrupt-handler chain.
A virus of this type is able to infect multiple parts of a system, including boot sectors, memory, and ﬁles.
An armored virus is coded to make it hard for antivirus researchers to unravel and understand.
It can also be compressed to avoid detection and disinfection.
In addition, virus droppers and other full ﬁles that are part of a virus infestation are frequently hidden via ﬁle attributes or unviewable ﬁle names.
For example, in 2004 a new and widespread virus was detected.
This virus started by infecting hundreds of Windows servers (including many trusted sites) running Microsoft Internet Information Server (IIS)
Any vulnerable Microsoft Explorer web browser visiting those sites received a browser virus with any download.
The browser virus installed several back-door programs, including a keystroke logger, which records everything entered on the keyboard (including passwords and credit-card numbers)
It also installed a daemon to allow unlimited remote access by an intruder and another that allowed an intruder to route spam through the infected desktop computer.
Generally, viruses are the most disruptive security attacks, and because they are effective, they will continue to be written and to spread.
An active security-related debate within the computing community concerns the existence of a monoculture, in which many systems run the same hardware, operating system, and application software.
One question is whether such a monoculture even exists today.
Another question is whether, if it does, it increases the threat of and damage caused by viruses and other security intrusions.
Program threats typically use a breakdown in the protection mechanisms of a system to attack programs.
In contrast, system and network threats involve the abuse of services and network connections.
System and network threats create a situation in which operating-system resources and user ﬁles are misused.
Sometimes, a system and network attack is used to launch a program attack, and vice versa.
The more open an operating system is—the more services it has enabled and the more functions it allows—the more likely it is that a bug is available to exploit.
For example, Solaris 10 moved from a model in which many services (FTP, telnet, and others) were enabled by default when the system was installed to a model in which almost all services are disabled at installation time and must speciﬁcally be enabled by system administrators.
Such changes reduce the system’s attack surface—the set of ways in which an attacker can try to break into the system.
In the remainder of this section, we discuss some examples of system and network threats, including worms, port scanning, and denial-of-service.
It is important to note that masquerading and replay attacks are also commonly launched over networks between systems.
In fact, these attacks are more effective and harder to counter when multiple systems are involved.
For example, within a computer, the operating system usually can determine the sender and receiver of a message.
Even if the sender changes to the ID of someone else, there may be a record of that ID change.
When multiple systems are involved, especially systems controlled by attackers, then such tracing is much more difﬁcult.
In general, we can say that sharing secrets (to prove identity and as keys to encryption) is required for authentication and encryption, and sharing secrets is easier in environments (such as a single operating system) in which secure sharing methods exist.
A worm is a process that uses the spawn mechanism to duplicate itself.
The worm spawns copies of itself, using up system resources and perhaps locking out all other processes.
On computer networks, worms are particularly potent, since they may reproduce themselves among systems and thus shut down an entire network.
Within a few hours of its release, it had consumed system resources to the point of bringing down the infected machines.
Although Morris designed the self-replicating program for rapid reproduction and distribution, some of the features of the UNIX networking environment provided the means to propagate the worm throughout the system.
It is likely that Morris chose for initial infection an Internet host left open for and accessible to outside users.
From there, the worm program exploited ﬂaws in the UNIX operating system’s security routines and took advantage of UNIX utilities that simplify resource sharing in local-area networks to gain unauthorized access to thousands of other connected sites.
The worm was made up of two programs, a grappling hook (also called a bootstrap or vector) program and the main program.
Once established on the computer system under attack, the grappling hook connected to the machine where it originated and uploaded a copy of the main worm onto the hooked system (Figure 15.6)
The main program proceeded to search for other machines to which the newly infected system could connect easily.
In these actions, Morris exploited the UNIX networking utility rsh for easy remote task execution.
By setting up special ﬁles that list host–login name pairs, users can omit entering a password each time.
The worm searched these special ﬁles for site names that would allow remote execution without a password.
Where remote shells were established, the worm program was uploaded and began executing anew.
The attack via remote access was one of three infection methods built into the worm.
The other two methods involved operating-system bugs in the UNIX finger and sendmail programs.
Finger runs as a background process (or daemon) at each BSD site and responds to queries throughout the Internet.
The program queried finger with a 536-byte string crafted to exceed the buffer allocated for input and to overwrite the stack frame.
Instead of returning to the main routine where it resided before Morris’s call, the finger daemon was routed to a procedure within the invading 536-byte string now residing on the stack.
The new procedure executed /bin/sh, which, if successful, gave the worm a remote shell on the machine under attack.
The bug exploited in sendmail also involved using a daemon process for malicious entry.
Debugging code in the utility permits testers to verify and display the state of the mail system.
The debugging option was useful to system administrators and was often left on.
Morris included in his attack arsenal a call to debug that —instead of specifying a user address, as would be normal in testing—issued a set of commands that mailed and executed a copy of the grappling-hook program.
Once in place, the main worm systematically attempted to discover user passwords.
It began by trying simple cases of no password or passwords constructed of account–user-name combinations, then used comparisons with an internal dictionary of 432 favorite password choices, and then went to the.
This elaborate and efﬁcient three-stage password-cracking algorithm enabled the worm to gain access to other user accounts on the infected system.
The worm then searched for rsh data ﬁles in these newly broken accounts and used them as described previously to gain access to user accounts on remote systems.
With each new access, the worm program searched for already active copies of itself.
If it found one, the new copy exited, except in every seventh instance.
Had the worm exited on all duplicate sightings, it might have remained undetected.
Allowing every seventh duplicate to proceed (possibly to confound efforts to stop its spread by baiting with “fake” worms) created a wholesale infestation of Sun and VAX systems on the Internet.
The very features of the UNIX network environment that assisted in the worm’s propagation also helped to stop its advance.
Ease of electronic communication, mechanisms to copy source and binary ﬁles to remote machines, and access to both source code and human expertise allowed cooperative efforts to develop solutions quickly.
By the evening of the next day, November 3, methods of halting the invading program were circulated to system administrators via the Internet.
Within days, speciﬁc software patches for the exploited security ﬂaws were available.
Why did Morris unleash the worm? The action has been characterized as both a harmless prank gone awry and a serious criminal offense.
Based on the complexity of the attack, it is unlikely that the worm’s release or the scope of its spread was unintentional.
The worm program took elaborate steps to cover its tracks and to repel efforts to stop its spread.
Yet the program contained no code aimed at damaging or destroying the systems on which it ran.
The author clearly had the expertise to include such commands; in fact, data structures were present in the bootstrap code that could have been used to transfer Trojan-horse or virus programs.
The behavior of the program may lead to interesting observations, but it does not provide a sound basis for inferring motive.
Security experts continue to evaluate methods to decrease or eliminate worms.
A more recent event, though, shows that worms are still a fact of life on the Internet.
It also shows that as the Internet grows, the damage that even “harmless” worms can do also grows and can be signiﬁcant.
It was the fastest-spreading worm released to date, at its peak infecting hundreds of thousands of computers and one in seventeen e-mail messages on the Internet.
It clogged e-mail inboxes, slowed networks, and took a huge number of hours to clean up.
Sobig.F was launched by being uploaded to a pornography newsgroup via an account created with a stolen credit card.
The virus targeted Microsoft Windows systems and used its own SMTP engine to e-mail itself to all the addresses found on an infected system.
It used a variety of subject lines to help avoid detection, including “Thank You!” “Your details,”
Sobig.F included an attachment for the target e-mail reader to click on, again with a variety of names.
If this payload was executed, it stored a program called WINPPR32.EXE in the default Windows directory, along with a text ﬁle.
The code included in the attachment was also programmed to periodically attempt to connect to one of twenty servers and download and execute a program from them.
Fortunately, the servers were disabled before the code could be downloaded.
The content of the program from these servers has not yet been determined.
If the code was malevolent, untold damage to a vast number of machines could have resulted.
Port scanning is not an attack but rather a means for a cracker to detect a system’s vulnerabilities to attack.
Port scanning typically is automated, involving a tool that attempts to create a TCP/IP connection to a speciﬁc port or a range of ports.
For example, suppose there is a known vulnerability (or bug) in sendmail.
A cracker could launch a port scanner to try to connect, say, to port 25 of a particular system or to a range of systems.
If the connection was successful, the cracker (or tool) could attempt to communicate with the answering service to determine if the service was indeed sendmail and, if so, if it was the version with the bug.
Now imagine a tool in which each bug of every service of every operating system was encoded.
The tool could attempt to connect to every port of one or more systems.
For every service that answered, it could try to use each known bug.
Frequently, the bugs are buffer overﬂows, allowing the creation of a privileged command shell on the system.
From there, of course, the cracker could install Trojan horses, back-door programs, and so on.
There is no such tool, but there are tools that perform subsets of that functionality.
For example, nmap (from http://www.insecure.org/nmap/) is a very versatile open-source utility for network exploration and security auditing.
When pointed at a target, it will determine what services are running, including application names and versions.
It can also provide information about defenses, such as what ﬁrewalls are defending the target.
Because port scans are detectable (Section 15.6.3), they frequently are launched from zombie systems.
Such systems are previously compromised, independent systems that are serving their owners while being used for nefarious purposes, including denial-of-service attacks and spam relay.
Zombies make crackers particularly difﬁcult to prosecute because determining the source of the attack and the person that launched it is challenging.
This is one of many reasons for securing “inconsequential” systems, not just systems containing “valuable” information or services.
As mentioned earlier, denial-of-service attacks are aimed not at gaining information or stealing resources but rather at disrupting legitimate use of a system or facility.
Launching an attack that prevents legitimate use is frequently easier than breaking into a machine or facility.
Attacks in the ﬁrst category use so many facility resources that, in essence, no useful work can be done.
For example, a website click could download a Java applet that proceeds to use all available CPU time or to pop up windows inﬁnitely.
The second category involves disrupting the network of the facility.
There have been several successful denial-of-service attacks of this kind against major websites.
These attacks result from abuse of some of the fundamental functionality of TCP/IP.
For instance, if the attacker sends the part of the protocol that says “I want to start a TCP connection,” but never follows with the standard “The connection is now complete,” the result can be partially started TCP sessions.
If enough of these sessions are launched, they can eat up all the network resources of the system, disabling any further legitimate TCP connections.
Such attacks, which can last hours or days, have caused partial or full failure of attempts to use the target facility.
The attacks are usually stopped at the network level until the operating systems can be updated to reduce their vulnerability.
Even more difﬁcult to prevent and resolve are distributed denial-of-service (DDOS) attacks.
These attacks are launched from multiple sites at once, toward a common target, typically by zombies.
A site comes under attack, and the attackers offer to halt the attack in exchange for money.
Sometimes a site does not even know it is under attack.
It can be difﬁcult to determine whether a system slowdown is an attack or just a surge in system use.
Consider that a successful advertising campaign that greatly increases trafﬁc to a site could be considered a DDOS.
For example, if an authentication algorithm locks an account for a period of time after several incorrect attempts to access the account, then an attacker could cause all authentication to be blocked by purposely making incorrect attempts to access all accounts.
Similarly, a ﬁrewall that automatically blocks certain kinds of trafﬁc could be induced to block that trafﬁc when it should not.
These examples suggest that programmers and systems managers need to fully understand the algorithms and technologies they are deploying.
Finally, computer science classes are notorious sources of accidental system DOS attacks.
Consider the ﬁrst programming exercises in which students learn to create subprocesses or threads.
The system’s free memory and CPU resources don’t stand a chance.
There are many defenses against computer attacks, running the gamut from methodology to technology.
The broadest tool available to system designers and users is cryptography.
In this section, we discuss cryptography and its use in computer security.
Note that the cryptography discussed here has been simpliﬁed for educational purposes; readers are cautioned against using any.
Good cryptography libraries are widely available and would make a good basis for production applications.
In an isolated computer, the operating system can reliably determine the sender and recipient of all interprocess communication, since it controls all communication channels in the computer.
In a network of computers, the situation is quite different.
A networked computer receives bits “from the wire” with no immediate and reliable way of determining what machine or application sent those bits.
Similarly, the computer sends bits onto the network with no way of knowing who might eventually receive them.
Additionally, when either sending or receiving, the system has no way of knowing if an eavesdropper listened to the communication.
Commonly, network addresses are used to infer the potential senders and receivers of network messages.
Network packets arrive with a source address, such as an IP address.
And when a computer sends a message, it names the intended receiver by specifying a destination address.
However, for applications where security matters, we are asking for trouble if we assume that the source or destination address of a packet reliably determines who sent or received that packet.
A rogue computer can send a message with a falsiﬁed source address, and numerous computers other than the one speciﬁed by the destination address can (and typically do) receive a packet.
For example, all of the routers on the way to the destination will receive the packet, too.
How, then, is an operating system to decide whether to grant a request when it cannot trust the named source of the request? And how is it supposed to provide protection for a request or data when it cannot determine who will receive the response or message contents it sends over the network?
It is generally considered infeasible to build a network of any scale in which the source and destination addresses of packets can be trusted in this sense.
Therefore, the only alternative is somehow to eliminate the need to trust the network.
Abstractly, cryptography is used to constrain the potential senders and/or receivers of a message.
Modern cryptography is based on secrets called keys that are selectively distributed to computers in a network and used to process messages.
Cryptography enables a recipient of a message to verify that the message was created by some computer possessing a certain key.
Similarly, a sender can encode its message so that only a computer with a certain key can decode the message.
Unlike network addresses, however, keys are designed so that it is not computationally feasible to derive them from the messages they were used to generate or from any other public information.
Thus, they provide a much more trustworthy means of constraining senders and receivers of messages.
Note that cryptography is a ﬁeld of study unto itself, with large and small complexities and subtleties.
Here, we explore the most important aspects of the parts of cryptography that pertain to operating systems.
Because it solves a wide variety of communication security problems, encryption is used frequently in many aspects of modern computing.
It is used to send messages securely across across a network, as well as to protect database data, ﬁles, and even entire disks from having their contents read by unauthorized entities.
An encryption algorithm enables the sender of a message to ensure that.
Encryption of messages is an ancient practice, of course, and there have been many encryption algorithms, dating back to ancient times.
In this section, we describe important modern encryption principles and algorithms.
Both E and Ek for any k should be efﬁciently computable functions.
Generally, Ek is a randomized mapping from messages to ciphertexts.
There are two main types of encryption algorithms: symmetric and asymmetric.
In a symmetric encryption algorithm, the same key is used to encrypt and to decrypt.
Figure 15.7 shows an example of two users communicating securely via symmetric encryption over an insecure channel.
Note that the key exchange can take place directly between the two parties or via a trusted third party (that is, a certiﬁcate authority), as discussed in Section 15.4.1.4
For the past several decades, the most commonly used symmetric encryption algorithm in the United States for civilian applications has been the data-encryption standard (DES) cipher adopted by the National Institute of Standards and Technology (NIST)
Because DES works on a block of bits at a time, is known as a block cipher, and its transformations are typical of block ciphers.
With block ciphers, if the same key is used for encrypting an extended amount of data, it becomes vulnerable to attack.
Rather than giving up on DES, NIST created a modiﬁcation called triple DES, in which the DES algorithm is repeated three times (two encryptions and one decryption) on the same plaintext using two.
When three keys are used, the effective key length is 168 bits.
In 2001, NIST adopted a new block cipher, called the advanced encryption standard (AES), to replace DES.
In particular, they do not directly handle messages longer than their required block sizes.
However, there are many modes of encryption that are based on stream ciphers, which can be used to securely encrypt longer messages.
A stream cipher is designed to encrypt and decrypt a stream of bytes or bits rather than a block.
This is useful when the length of a communication would make a block cipher too slow.
The key is input into a pseudo–random-bit generator, which is an algorithm that attempts to produce random bits.
The output of the generator when fed a key is a keystream.
A keystream is an inﬁnite set of bits that can be used to encrypt a plaintext stream by simply XORing it with the plaintext.
XOR, for “eXclusive OR” is an operation that compares two input bits and generates one output bit.
RC4 is used in encrypting steams of data, such as in WEP, the wireless LAN protocol.
In an asymmetric encryption algorithm, there are different encryption and decryption keys.
An entity preparing to receive encrypted communication creates two keys and makes one of them (called the public key) available to anyone who wants it.
Any sender can use that key to encrypt a communication, but only the key creator can decrypt the communication.
This scheme, known as public-key encryption, was a breakthrough in cryptography.
No longer must a key be kept secret and delivered securely.
Instead, anyone can encrypt a message to the receiving entity, and no matter who else is listening, only that entity can decrypt the message.
As an example of how public-key encryption works, we describe an algorithm known as RSA, after its inventors, Rivest, Shamir, and Adleman.
Asymmetric algorithms based on elliptic curves are gaining ground, however, because the key length of such an algorithm can be shorter for the same amount of cryptographic strength.
The use of asymmetric encryption begins with the publication of the public key of the destination.
For bidirectional communication, the source also must publish its public key.
The private key (or “secret key”) must be zealously guarded, as anyone holding that key can decrypt any message created by the matching public key.
We should note that the seemingly small difference in key use between asymmetric and symmetric cryptography is quite large in practice.
It is much faster for a computer to encode and decode ciphertext by using the usual symmetric algorithms than by using asymmetric algorithms.
Why, then, use an asymmetric algorithm? In truth, these algorithms are not used for generalpurpose encryption of large amounts of data.
However, they are used not only for encryption of small amounts of data but also for authentication, conﬁdentiality, and key distribution, as we show in the following sections.
We have seen that encryption offers a way of constraining the set of possible receivers of a message.
Constraining the set of potential senders of a message is called authentication.
Authentication is also useful for proving that a message has not been modiﬁed.
In this section, we discuss authentication as a constraint on possible senders of a message.
Note that this sort of authentication is similar to but distinct from user authentication, which we discuss in Section 15.5
An authentication algorithm using symmetric keys consists of the following components:
Both S and Sk for any k should be efﬁciently computable functions.
However, a computer not holding k cannot generate authenticators on messages that can be veriﬁed using Vk.
Since authenticators are generally exposed (for example, sent on a network with the messages themselves), it must not be feasible to derive k from the authenticators.
Practically, if Vk(m, a ) = true, then we know that m has not been modiﬁed, and that the sender of the message has k.
If we share k with only one entity, then we know that the message originated from k.
The ﬁrst main type of authentication algorithm uses symmetric encryption.
A MAC provides a way to securely authenticate short values.
If we use it to authenticate H(m) for an H that is collision resistant, then we obtain a way to securely authenticate long messages by hashing them ﬁrst.
Note that k is needed to compute both Sk and Vk , so anyone able to compute one can compute the other.
The second main type of authentication algorithm is a digital-signature algorithm, and the authenticators thus produced are called digital signatures.
Digital signatures are very useful in that they enable anyone to verify the authenticity of the message.
In a digital-signature algorithm, it is computationally infeasible to derive ks from kv.
Thus, kv is the public key, and ks is the private key.
Note that encryption and authentication may be used together or separately.
For example, a company could provide a software patch and could “sign” that patch to prove that it came from the company and that it hasn’t been modiﬁed.
For example, digital signatures are the core of nonrepudiation, which supplies proof that an entity performed an action.
Nonrepudiation assures that a person ﬁlling out an electronic form cannot deny that he did so.
Certainly, a good part of the battle between cryptographers (those inventing ciphers) and cryptanalysts (those trying to break them) involves keys.
With symmetric algorithms, both parties need the key, and no one else should have it.
The delivery of the symmetric key is a huge challenge.
Sometimes it is performed out-of-band—say, via a paper document or a conversation.
Suppose a user wanted to communicate with N other users privately.
That user would need N keys and, for more security, would need to change those keys frequently.
These are the very reasons for efforts to create asymmetric key algorithms.
Not only can the keys be exchanged in public, but a given user needs only one private key, no matter how many other people she wants to communicate with.
There is still the matter of managing a public key for each recipient of the communication, but since public keys need not be secured, simple storage can be used for that key ring.
Unfortunately, even the distribution of public keys requires some care.
Here, the person who wants to receive an encrypted message sends out his public key, but an attacker also sends her “bad” public key (which matches her private key)
The person who wants to send the encrypted message knows no better and so uses the bad key to encrypt the message.
One way to solve that problem involves the use of digital certiﬁcates.
A digital certiﬁcate is a public key digitally signed by a trusted party.
The trusted party receives proof of identiﬁcation from some entity and certiﬁes that the public key belongs to that entity.
But how do we know we can trust the certiﬁer? These certiﬁcate authorities have their public keys included within web browsers (and other consumers of certiﬁcates) before they are distributed.
The certiﬁcate authorities can then vouch for other authorities (digitally signing the public keys of these other authorities), and so on, creating a web of trust.
The certiﬁcates can be distributed in a standard X.509 digital certiﬁcate format that can be parsed by computer.
This scheme is used for secure web communication, as we discuss in Section 15.4.3
Network protocols are typically organized in layers, like an onion or a parfait, with each layer acting as a client of the one below it.
That is, when one protocol generates a message to send to its protocol peer on another machine, it hands its message to the protocol below it in the network-protocol stack for delivery to its peer on that machine.
For example, in an IP network, TCP (a transportlayer protocol) acts as a client of IP (a network-layer protocol): TCP packets are passed down to IP for delivery to the IP peer at the other end of the connection.
This IP peer then delivers the TCP packet up to the TCP peer on that machine.
Cryptography can be inserted at almost any layer in the OSI model.
Networklayer security generally has been standardized on IPSec, which deﬁnes IP packet formats that allow the insertion of authenticators and the encryption of packet contents.
IPSec uses symmetric encryption and uses the Internet Key Exchange (IKE) protocol for key exchange.
IPSec is becoming widely used as the basis for virtual private networks (VPNs), in which all trafﬁc between two IPSec endpoints is encrypted to make a private network out of one that may otherwise be public.
Numerous protocols also have been developed for use by applications, such as PGP for encrypting e-mail, but then the applications themselves must be coded to implement security.
Where is cryptographic protection best placed in a protocol stack? In general, there is no deﬁnitive answer.
On the one hand, more protocols beneﬁt from protections placed lower in the stack.
For example, since IP packets encapsulate TCP packets, encryption of IP packets (using IPSec, for example) also.
Similarly, authenticators on IP packets detect the modiﬁcation of contained TCP header information.
On the other hand, protection at lower layers in the protocol stack may give insufﬁcient protection to higher-layer protocols.
For example, an application server that accepts connections encrypted with IPSec might be able to authenticate the client computers from which requests are received.
However, to authenticate a user at a client computer, the server may need to use an application-level protocol—the user may be required to type a password.
E-mail delivered via the industry-standard SMTP protocol is stored and forwarded, frequently multiple times, before it is delivered.
Each of these transmissions could go over a secure or an insecure network.
For e-mail to be secure, the e-mail message needs to be encrypted so that its security is independent of the transports that carry it.
It is perhaps the most commonly used cryptographic protocol on the Internet today, since it is the standard protocol by which web browsers communicate securely with web servers.
For completeness, we should note that SSL was designed by Netscape and that it evolved into the industry- standard TLS protocol.
In this discussion, we use SSL to mean both SSL and TLS.
Even then, we describe it in a very simpliﬁed and abstract form, so as to maintain focus on its use of cryptographic primitives.
What we are about to see is a complex dance in which asymmetric cryptography is used so that a client and a server can establish a secure session key that can be used for symmetric encryption of the session between the two—all of this while avoiding man-in-the-middle and replay attacks.
For added cryptographic strength, the session keys are forgotten once a session is completed.
Another communication between the two will require generation of new session keys.
The SSL protocol is initiated by a client c to communicate securely with a server.
Prior to the protocol’s use, the server s is assumed to have obtained a certiﬁcate, denoted certs , from certiﬁcation authority CA.
Various attributes (attrs) of the server, such as its unique distinguished name and its common (DNS) name.
The identity of a asymmetric encryption algorithm E () for the server.
A validity interval (interval) during which the certiﬁcate should be considered valid.
In the case of the Web, the user’s browser is shipped from its vendor containing the veriﬁcation.
The user can add or delete these as she chooses.
A symmetric encryption key kcryptcs for encrypting messages from the client to the server.
A symmetric encryption key kcryptsc for encrypting messages from the server to the client.
A MAC generation key kmaccs for generating authenticators on messages from the client to the server.
A MAC generation key kmacsc for generating authenticators on messages from the server to the client.
To send a message m to the server, the client sends.
This protocol enables the server to limit the recipients of its messages to the client that generated pms and to limit the senders of the messages it accepts to that same client.
Similarly, the client can limit the recipients of the messages it sends and the senders of the messages it accepts to the party that knows kd (that is, the party that can decrypt cpms)
In many applications, such as web transactions, the client needs to verify the identity of the party that knows kd.
In particular, the attrs ﬁeld contains information that the client can use to determine the identity—for example, the.
For applications in which the server also needs information about the client, SSL supports an option by which a client can send a certiﬁcate to the server.
In addition to its use on the Internet, SSL is being used for a wide variety of tasks.
IPSec is good for point-to-point encryption of trafﬁc—say, between two company ofﬁces.
But what about users? If a system cannot authenticate a user, then authenticating that a message came from that user is pointless.
Thus, a major security problem for operating systems is user authentication.
The protection system depends on the ability to identify the programs and processes currently executing, which in turn depends on the ability to identify each user of the system.
How do we determine whether a user’s identity is authentic? Generally, user authentication is based on one or more of three things: the user’s possession of something (a key or card), the user’s knowledge of something (a user identiﬁer and password), or an attribute of the user (ﬁngerprint, retina pattern, or signature)
The most common approach to authenticating a user identity is the use of passwords.
When the user identiﬁes herself by user ID or account name, she is asked for a password.
If the user-supplied password matches the password stored in the system, the system assumes that the account is being accessed by the owner of that account.
Passwords are often used to protect objects in the computer system, in the absence of more complete protection schemes.
They can be considered a special case of either keys or capabilities.
For instance, a password may be associated with each resource (such as a ﬁle)
Whenever a request is made to use the resource, the password must be given.
For example, different passwords may be used for reading ﬁles, appending ﬁles, and updating ﬁles.
In practice, most systems require only one password for a user to gain full rights.
Although more passwords theoretically would be more secure, such systems tend not to be implemented due to the classic trade-off between security and convenience.
If security makes something inconvenient, then the security is frequently bypassed or otherwise circumvented.
Passwords are extremely common because they are easy to understand and use.
Unfortunately, passwords can often be guessed, accidentally exposed, sniffed (read by an eavesdropper), or illegally transferred from an authorized user to an unauthorized one, as we show next.
One way is for the intruder (either human or program) to know the user or to have information about the user.
All too frequently, people use obvious information (such as the names of their cats or spouses) as their passwords.
The other way is to use brute force, trying enumeration—or all possible combinations of valid password characters (letters, numbers, and punctuation on some systems)—until the password is found.
On average, guessing 5,000 times would produce a correct hit.
A program that could try a password every millisecond would take only about 5 seconds to guess a four-character password.
Enumeration is less successful where systems allow longer passwords that include both uppercase and lowercase letters, along with numbers and all punctuation characters.
Of course, users must take advantage of the large password space and must not, for example, use only lowercase letters.
In addition to being guessed, passwords can be exposed as a result of visual or electronic monitoring.
An intruder can look over the shoulder of a user (shoulder surﬁng) when the user is logging in and can learn the password easily by watching the keyboard.
Alternatively, anyone with access to the network on which a computer resides can seamlessly add a network monitor, allowing him to sniff, or watch, all data being transferred on the network, including user IDs and passwords.
Encrypting the data stream containing the password solves this problem.
For example, if a ﬁle is used to contain the passwords, it could be copied for off-system analysis.
Or consider a Trojan-horse program installed on the system that captures every keystroke before sending it on to the application.
Exposure is a particularly severe problem if the password is written down where it can be read or lost.
Some systems force users to select hard-toremember or long passwords, or to change their password frequently, which may cause a user to record the password or to reuse it.
As a result, such systems provide much less security than systems that allow users to select easy passwords!
The ﬁnal type of password compromise, illegal transfer, is the result of human nature.
Most computer installations have a rule that forbids users to share accounts.
This rule is sometimes implemented for accounting reasons but is often aimed at improving security.
For instance, suppose one user ID is shared by several users, and a security breach occurs from that user ID.
It is impossible to know who was using the ID at the time the break occurred or even whether the user was an authorized one.
With one user per user ID, any user can be questioned directly about use of the account; in addition, the user might notice something different about the account and detect the break-in.
Sometimes, users break account-sharing rules to help friends or to circumvent accounting, and this behavior can result in a system’s being accessed by unauthorized users —possibly harmful ones.
Passwords can be either generated by the system or selected by a user.
System-generated passwords may be difﬁcult to remember, and thus users may write them down.
As mentioned, however, user-selected passwords are often easy to guess (the user’s name or favorite car, for example)
Some systems will check a proposed password for ease of guessing or cracking before accepting.
Some systems also age passwords, forcing users to change their passwords at regular intervals (every three months, for instance)
This method is not foolproof either, because users can easily toggle between two passwords.
The solution, as implemented on some systems, is to record a password history for each user.
For instance, the system could record the last N passwords and not allow their reuse.
Several variants on these simple password schemes can be used.
At the extreme, the password is changed from session to session.
A new password is selected (either by the system or by the user) at the end of each session, and that password must be used for the next session.
In such a case, even if a password is used by an unauthorized person, that person can use it only once.
When the legitimate user tries to use a now-invalid password at the next session, he discovers the security violation.
Steps can then be taken to repair the breached security.
One problem with all these approaches is the difﬁculty of keeping the password secret within the computer.
How can the system store a password securely yet allow its use for authentication when the user presents her password? The UNIX system uses secure hashing to avoid the necessity of keeping its password list secret.
Because the list is hashed rather than encrypted, it is impossible for the system to decrypt the stored value and determine the original password.
The system contains a function that is extremely difﬁcult—the designers hope impossible —to invert but is simple to compute.
That is, given a value x, it is easy to compute the hash function value f (x)
Given a function value f (x), however, it is impossible to compute x.
When a user presents a password, it is hashed and compared against the stored encoded password.
Even if the stored encoded password is seen, it cannot be decoded, so the password cannot be determined.
Thus, the password ﬁle does not need to be kept secret.
The ﬂaw in this method is that the system no longer has control over the passwords.
Although the passwords are hashed, anyone with a copy of the password ﬁle can run fast hash routines against it—hashing each word in a dictionary, for instance, and comparing the results against the passwords.
If the user has selected a password that is also a word in the dictionary, the password is cracked.
On sufﬁciently fast computers, or even on clusters of slow computers, such a comparison may take only a few hours.
Furthermore, because UNIX systems use a well-known hashing algorithm, a cracker might keep a cache of passwords that have been cracked previously.
For these reasons, systems include a “salt,” or recorded random number, in the hashing algorithm.
The salt value is added to the password to ensure that if two plaintext passwords are the same, they result in different hash values.
In addition, the salt value makes hashing a dictionary ineffective, because each dictionary term would need to be combined with each salt value for comparison to the stored passwords.
Newer versions of UNIX also store the hashed password entries in a ﬁle readable only by the superuser.
Another weakness in the UNIX password methods is that many UNIX systems treat only the ﬁrst eight characters as signiﬁcant.
It is therefore extremely important for users to take advantage of the available password space.
Complicating the issue further is the fact that some systems do not allow the use of dictionary words as passwords.
A good technique is to generate your password by using the ﬁrst letter of each word of an easily remembered phrase using both upper and lower characters with a number or punctuation mark thrown in for good measure.
For example, the phrase “My mother’s name is Katherine” might yield the password “Mmn.isK!”
The password is hard to crack but easy for the user to remember.
A more secure system would allow more characters in its passwords.
Indeed, a system might also allow passwords to include the space character, so that a user could create a passphrase.
To avoid the problems of password snifﬁng and shoulder surﬁng, a system can use a set of paired passwords.
When a session begins, the system randomly selects and presents one part of a password pair; the user must supply the other part.
In this system, the user is challenged and must respond with the correct answer to that challenge.
This approach can be generalized to the use of an algorithm as a password.
That is, a user can type in a password, and no entity intercepting that password will be able to reuse it.
In this scheme, the system and the user share a symmetric password.
The password pw is never transmitted over a medium that allows exposure.
Rather, the password is used as input to the function, along with a challenge ch presented by the system.
The result of this function is transmitted as the authenticator to the computer.
Because the computer also knows pw and ch, it can perform the same computation.
The next time the user needs to be authenticated, another ch is generated, and the same steps ensue.
This one-time password system is one of only a few ways to prevent improper authentication due to password exposure.
Another variation on one-time passwords uses a code book, or one-time pad, which is a list of single-use passwords.
Each password on the list is used once and then is crossed out or erased.
The commonly used S/Key system uses either a software calculator or a code book based on these calculations as a source of one-time passwords.
Of course, the user must protect his code book, and it is helpful if the code book does not identify the system to which the codes are authenticators.
Yet another variation on the use of passwords for authentication involves the use of biometric measures.
Palm- or hand-readers are commonly used to secure physical access—for example, access to a data center.
These readers match stored parameters against what is being read from hand-reader pads.
The parameters can include a temperature map, as well as ﬁnger length, ﬁnger width, and line patterns.
These devices are currently too large and expensive to be used for normal computer authentication.
Fingerprint readers have become accurate and cost-effective and should become more common in the future.
These devices read ﬁnger ridge patterns and convert them into a sequence of numbers.
Over time, they can store a set of sequences to adjust for the location of the ﬁnger on the reading pad and other factors.
Software can then scan a ﬁnger on the pad and compare its features with these stored sequences to determine if they match.
Of course, multiple users can have proﬁles stored, and the scanner can differentiate among them.
A very accurate two-factor authentication scheme can result from requiring a password as well as a user name and ﬁngerprint scan.
If this information is encrypted in transit, the system can be very resistant to spooﬁng or replay attack.
Consider how strong authentication can be with a USB device that must be plugged into the system, a PIN, and a ﬁngerprint scan.
Except for having to place ones ﬁnger on a pad and plug the USB into the system, this authentication method is no less convenient than that using normal passwords.
Recall, though, that strong authentication by itself is not sufﬁcient to guarantee the ID of the user.
An authenticated session can still be hijacked if it is not encrypted.
Just as there are myriad threats to system and network security, there are many security solutions.
The solutions range from improved user education, through technology, to writing bug-free software.
Most security professionals subscribe to the theory of defense in depth, which states that more layers of defense are better than fewer layers.
Of course, this theory applies to any kind of security.
Consider the security of a house without a door lock, with a door lock, and with a lock and an alarm.
In this section, we look at the major methods, tools, and techniques that can be used to improve resistance to threats.
The ﬁrst step toward improving the security of any aspect of computing is to have a security policy.
For example, a policy might state that all outsideaccessible applications must have a code review before being deployed, or that users should not share their passwords, or that all connection points between a company and the outside must have port scans run every six months.
Without a policy in place, it is impossible for users and administrators to know what is permissible, what is required, and what is not allowed.
The policy is a road map to security, and if a site is trying to move from less secure to more secure, it needs a map to know how to get there.
Once the security policy is in place, the people it affects should know it well.
The policy should also be a living document that is reviewed and updated periodically to ensure that it is still pertinent and still followed.
How can we determine whether a security policy has been correctly implemented? The best way is to execute a vulnerability assessment.
Such assessments can cover broad ground, from social engineering through risk assessment to port scans.
Risk assessment, for example, attempts to value the assets of the entity in question (a program, a management team, a system, or a facility) and determine the odds that a security incident will affect the entity and decrease its value.
When the odds of suffering a loss and the amount of the potential loss are known, a value can be placed on trying to secure the entity.
The core activity of most vulnerability assessments is a penetration test, in which the entity is scanned for known vulnerabilities.
Because this book is concerned with operating systems and the software that runs on them, we concentrate on those aspects of vulnerability assessment.
Vulnerability scans typically are done at times when computer use is relatively low, to minimize their impact.
When appropriate, they are done on test systems rather than production systems, because they can induce unhappy behavior from the target systems or network devices.
A scan within an individual system can check a variety of aspects of the system:
Improper protections on system data ﬁles, such as the password ﬁle, device.
Dangerous entries in the program search path (for example, the Trojan horse discussed in Section 15.2.1)
Any problems found by a security scan can be either ﬁxed automatically or reported to the managers of the system.
Networked computers are much more susceptible to security attacks than are standalone systems.
Rather than attacks from a known set of access points, such as directly connected terminals, we face attacks from an unknown and large set of access points—a potentially severe security problem.
To a lesser extent, systems connected to telephone lines via modems are also more exposed.
For instance, a top-secret system may be accessed only from within a building also considered top-secret.
The system loses its topsecret rating if any form of communication can occur outside that environment.
The connectors that plug a terminal into the secure computer are locked in a safe in the ofﬁce when the terminal is not in use.
A person must have proper ID to gain access to the building and her ofﬁce, must know a physical lock combination, and must know authentication information for the computer itself to gain access to the computer—an example of multifactor authentication.
Unfortunately for system administrators and computer-security professionals, it is frequently impossible to lock a machine in a room and disallow all remote access.
For instance, the Internet currently connects millions of computers and has become a mission-critical, indispensable resource for many companies and individuals.
If you consider the Internet a club, then, as in any club with millions of members, there are many good members and some bad members.
The bad members have many tools they can use to attempt to gain access to the interconnected computers, just as Morris did with his worm.
Vulnerability scans can be applied to networks to address some of the problems with network security.
The scans search a network for ports that respond to a request.
If services are enabled that should not be, access to them can be blocked, or they can be disabled.
The scans then determine the details of the application listening on that port and try to determine if it has any known vulnerabilities.
Testing those vulnerabilities can determine if the system is misconﬁgured or lacks needed patches.
Finally, though, consider the use of port scanners in the hands of a cracker rather than someone trying to improve security.
Fortunately, it is possible to detect port scans through anomaly detection, as we discuss next.
It is a general challenge to security that the same tools can be used for good and for harm.
In fact, some people advocate security through obscurity, stating that no tools should be written to test security, because such tools can be used to ﬁnd (and exploit) security holes.
Others believe that this approach to security is not a valid one, pointing out, for example, that crackers could write their own tools.
It seems reasonable that security through obscurity be considered one of the layers of security only so long as it is not the only layer.
For example, a company could publish its entire network conﬁguration, but keeping that information secret makes it harder for intruders to know what to attack or to determine what might be detected.
Even here, though, a company assuming that such information will remain a secret has a false sense of security.
Securing systems and facilities is intimately linked to intrusion detection.
Intrusion detection, as its name suggests, strives to detect attempted or successful.
Intrusion detection encompasses a wide array of techniques that vary on a number of axes, including the following:
Detection can occur in real time (while the intrusion is occurring) or after the fact.
These may include user-shell commands, process system calls, and network packet headers or contents.
Some forms of intrusion might be detected only by correlating information from several such sources.
Simple forms of response include alerting an administrator to the potential intrusion or somehow halting the potentially intrusive activity—for example, killing a process engaged in such activity.
In a sophisticated form of response, a system might transparently divert an intruder’s activity to a honeypot—a false resource exposed to the attacker.
The resource appears real to the attacker and enables the system to monitor and gain information about the attack.
But just what constitutes an intrusion? Deﬁning a suitable speciﬁcation of intrusion turns out to be quite difﬁcult, and thus automatic IDSs and IDPs today typically settle for one of two less ambitious approaches.
In the ﬁrst, called signature-based detection, system input or network trafﬁc is examined for speciﬁc behavior patterns (or signatures) known to indicate attacks.
A simple example of signature-based detection is scanning network packets for the string /etc/passwd/ targeted for a UNIX system.
Another example is virus-detection software, which scans binaries or network packets for known viruses.
The second approach, typically called anomaly detection, attempts through various techniques to detect anomalous behavior within computer systems.
Of course, not all anomalous system activity indicates an intrusion, but the presumption is that intrusions often induce anomalous behavior.
An example of anomaly detection is monitoring system calls of a daemon process to detect whether the system-call behavior deviates from normal patterns, possibly indicating that a buffer overﬂow has been exploited in the daemon to corrupt its behavior.
Another example is monitoring shell commands to detect anomalous commands for a given user or detecting an anomalous login time for a user, either of which may indicate that an attacker has succeeded in gaining access to that user’s account.
Signature-based detection and anomaly detection can be viewed as two sides of the same coin.
Signature-based detection attempts to characterize dangerous behaviors and to detect when one of these behaviors occurs, whereas anomaly detection attempts to characterize normal (or nondangerous) behaviors and to detect when something other than these behaviors occurs.
These different approaches yield IDSs and IDPs with very different properties, however.
Signature-based detection, in contrast, will identify only known attacks that can be codiﬁed in a recognizable pattern.
Thus, new attacks that were not contemplated when the signatures were generated will evade signature-based detection.
This problem is well known to vendors of virus-detection software, who must release new signatures with great frequency as new viruses are detected manually.
Anomaly detection is not necessarily superior to signature-based detection, however.
Indeed, a signiﬁcant challenge for systems that attempt anomaly detection is to benchmark “normal” system behavior accurately.
If the system has already been penetrated when it is benchmarked, then the intrusive activity may be included in the “normal” benchmark.
Even if the system is benchmarked cleanly, without inﬂuence from intrusive behavior, the benchmark must give a fairly complete picture of normal behavior.
Otherwise, the number of false positives (false alarms) or, worse, false negatives (missed intrusions) will be excessive.
To illustrate the impact of even a marginally high rate of false alarms, consider an installation consisting of a hundred UNIX workstations from which security-relevant events are recorded for purposes of intrusion detection.
A small installation such as this could easily generate a million audit records per day.
Only one or two might be worthy of an administrator’s investigation.
If we suppose, optimistically, that each actual attack is reﬂected in ten audit records, we can roughly compute the rate of occurrence of audit records reﬂecting truly intrusive activity as follows:
This example illustrates a general principle for IDSs and IDPs: for usability, they must offer an extremely low false-alarm rate.
Achieving a sufﬁciently low false-alarm rate is an especially serious challenge for anomaly-detection systems, as mentioned, because of the difﬁculties of adequately benchmarking normal system behavior.
Intrusion detection software is evolving to implement signatures, anomaly algorithms, and other algorithms and to combine the results to arrive at a more accurate anomaly-detection rate.
As we have seen, viruses can and do wreak havoc on systems.
Some of these programs are effective against only particular known viruses.
They work by searching all the programs on a system for the speciﬁc pattern of instructions known to make up the virus.
When they ﬁnd a known pattern, they remove the instructions, disinfecting the program.
Antivirus programs may have catalogs of thousands of viruses for which they search.
Both viruses and antivirus software continue to become more sophisticated.
Some viruses modify themselves as they infect other software to avoid the basic pattern-match approach of antivirus programs.
Antivirus programs in turn now look for families of patterns rather than a single pattern to identify a virus.
In fact, some antivirus programs implement a variety of detection algorithms.
They can decompress compressed viruses before checking for a signature.
A process opening an executable ﬁle for writing is suspicious, for example, unless it is a compiler.
Another popular technique is to run a program in a sandbox, which is a controlled or emulated section of the system.
The antivirus software analyzes the behavior of the code in the sandbox before letting it run unmonitored.
Some antivirus programs also put up a complete shield rather than just scanning ﬁles within a ﬁle system.
They search boot sectors, memory, inbound and outbound e-mail, ﬁles as they are downloaded, ﬁles on removable devices or media, and so on.
The best protection against computer viruses is prevention, or the practice of safe computing.
Purchasing unopened software from vendors and avoiding free or pirated copies from public sources or disk exchange offer the safest route to preventing infection.
However, even new copies of legitimate software applications are not immune to virus infection: in a few cases, disgruntled employees of a software company have infected the master copies of software programs to do economic harm to the company.
For macro viruses, one defense is to exchange Microsoft Word documents in an alternative ﬁle format called rich text format (RTF)
Unlike the native Word format, RTF does not include the capability to attach macros.
Another defense is to avoid opening any e-mail attachments from unknown users.
Unfortunately, history has shown that e-mail vulnerabilities appear as fast as they are ﬁxed.
For example, in 2000, the love bug virus became very widespread by traveling in e-mail messages that pretended to be love notes sent by friends of the receivers.
Once a receiver opened the attached Visual Basic script, the virus propagated by sending itself to the ﬁrst addresses in the receiver’s e-mail contact list.
An example of an anomaly-detection tool is the Tripwire ﬁle system integritychecking tool for UNIX, developed at Purdue University.
Tripwire operates on the premise that many intrusions result in modiﬁcation of system directories and ﬁles.
For example, an attacker might modify the system programs, perhaps inserting copies with Trojan horses, or might insert new programs into directories commonly found in user-shell search paths.
Or an intruder might remove system log ﬁles to cover his tracks.
Tripwire is a tool to monitor ﬁle systems for added, deleted, or changed ﬁles and to alert system administrators to these modiﬁcations.
The operation of Tripwire is controlled by a conﬁguration ﬁle tw.config that enumerates the directories and ﬁles to be monitored for changes, deletions, or additions.
Each entry in this conﬁguration ﬁle includes a selection mask to specify the ﬁle attributes (inode attributes) that will be monitored for changes.
For example, the selection mask might specify that a ﬁle’s permissions be monitored but its access time be ignored.
In addition, the selection mask can instruct that the ﬁle be monitored for changes.
Monitoring the hash of a ﬁle for changes is as good as monitoring the ﬁle itself, and storing hashes of ﬁles requires far less room than copying the ﬁles themselves.
When run initially, Tripwire takes as input the tw.config ﬁle and computes a signature for each ﬁle or directory consisting of its monitored attributes (inode attributes and hash values)
When run subsequently, Tripwire inputs both tw.config and the previously stored database, recomputes the signature for each ﬁle or directory named in tw.config, and compares this signature with the signature (if any) in the previously computed database.
Events reported to an administrator include any monitored ﬁle or directory whose signature differs from that in the database (a changed ﬁle), any ﬁle or directory in a monitored directory for which a signature does not exist in the database (an added ﬁle), and any signature in the database for which the corresponding ﬁle or directory no longer exists (a deleted ﬁle)
Although effective for a wide class of attacks, Tripwire does have limitations.
Perhaps the most obvious is the need to protect the Tripwire program and its associated ﬁles, especially the database ﬁle, from unauthorized modiﬁcation.
For this reason, Tripwire and its associated ﬁles should be stored on some tamper-proof medium, such as a write-protected disk or a secure server where logins can be tightly controlled.
Unfortunately, this makes it less convenient to update the database after authorized updates to monitored directories and ﬁles.
A second limitation is that some security-relevant ﬁles —for example, system log ﬁles—are supposed to change over time, and Tripwire does not provide a way to distinguish between an authorized and an unauthorized change.
So, for example, an attack that modiﬁes (without deleting) a system log that would normally change anyway would escape Tripwire’s detection capabilities.
The best Tripwire can do in this case is to detect certain obvious inconsistencies (for example, a shrinking log ﬁle)
Free and commercial versions of Tripwire are available from http://tripwire.org and http://tripwire.com.
It did, however, effectively negate the defensive strategy of opening attachments only from people known to the receiver.
A more effective defense method is to avoid opening any e-mail attachment that contains executable code.
Some companies now enforce this as policy by removing all incoming attachments to e-mail messages.
Another safeguard, although it does not prevent infection, does permit early detection.
A user must begin by completely reformatting the hard disk, especially the boot sector, which is often targeted for viral attack.
Only secure software is uploaded, and a signature of each program is taken via a secure message-digest computation.
The resulting ﬁle name and associated messagedigest list must then be kept free from unauthorized access.
Periodically, or each time a program is run, the operating system recomputes the signature and compares it with the signature on the original list; any differences serve as a warning of possible infection.
For example, a high-overhead antivirus scan, such as a sandbox, can be used; and if a program passes the test, a signature can be created for it.
If the signatures match the next time the program is run, it does not need to be virus-scanned again.
Auditing, accounting, and logging can decrease system performance, but they are useful in several areas, including security.
All system-call executions can be logged for analysis of program behavior (or misbehavior)
Authentication failures and authorization failures can tell us quite a lot about break-in attempts.
Accounting is another potential tool in a security administrator’s kit.
It can be used to ﬁnd performance changes, which in turn can reveal security problems.
One of the early UNIX computer break-ins was detected by Cliff Stoll when he was examining accounting logs and spotted an anomaly.
We turn next to the question of how a trusted computer can be connected safely to an untrustworthy network.
One solution is the use of a ﬁrewall to separate trusted and untrusted systems.
A ﬁrewall is a computer, appliance, or router that sits between the trusted and the untrusted.
A network ﬁrewall limits network access between the two security domains and monitors and logs all connections.
It can also limit connections based on source or destination address, source or destination port, or direction of the connection.
For instance, web servers use HTTP to communicate with web browsers.
A ﬁrewall therefore may allow only HTTP to pass from all hosts outside the ﬁrewall to the web server within the ﬁrewall.
The Morris Internet worm used the finger protocol to break into computers, so finger would not be allowed to pass, for example.
In fact, a network ﬁrewall can separate a network into multiple domains.
Connections are allowed from the Internet to the DMZ computers and from the company computers to the Internet but are not allowed from the Internet or DMZ computers to the company computers.
Optionally, controlled communications may be allowed between the DMZ and one company computer or more.
For instance, a web server on the DMZ may need to query a database server on the corporate network.
With a ﬁrewall, however, access is contained, and any DMZ systems that are broken into still are unable to access the company computers.
Of course, a ﬁrewall itself must be secure and attack-proof.
Furthermore, ﬁrewalls do not prevent attacks that tunnel, or travel within protocols or connections that the ﬁrewall allows.
A buffer-overﬂow attack to a web server will not be stopped by the ﬁrewall, for example, because the HTTP connection is allowed; it is the contents of the HTTP connection that house the attack.
Likewise, denialof-service attacks can affect ﬁrewalls as much as any other machines.
Another vulnerability of ﬁrewalls is spooﬁng, in which an unauthorized host pretends to be an authorized host by meeting some authorization criterion.
For example, if a ﬁrewall rule allows a connection from a host and identiﬁes that host by its IP address, then another host could send packets using that same address and be allowed through the ﬁrewall.
In addition to the most common network ﬁrewalls, there are other, newer kinds of ﬁrewalls, each with its pros and cons.
A personal ﬁrewall is a software layer either included with the operating system or added as an application.
Rather than limiting communication between security domains, it limits communication to (and possibly from) a given host.
A user could add a personal ﬁrewall to her PC so that a Trojan horse would be denied access to the network to which the PC is connected, for example.
An application proxy ﬁrewall understands the protocols that applications speak across the network.
An application proxy accepts a connection just as an SMTP server would and then initiates a connection to the original destination SMTP server.
It can monitor the trafﬁc as it forwards the message, watching for and disabling illegal commands, attempts to exploit.
An XML ﬁrewall, for example, has the speciﬁc purpose of analyzing XML trafﬁc and blocking disallowed or malformed XML.
System-call ﬁrewalls sit between applications and the kernel, monitoring system-call execution.
For example, in Solaris 10, the “least privilege” feature implements a list of more than ﬁfty system calls that processes may or may not be allowed to make.
A process that does not need to spawn other processes can have that ability taken away, for instance.
This speciﬁcation is widely used to determine the security of a facility and to model security solutions, so we explore it here.
Division D includes only one class and is used for systems that have failed to meet the requirements of any of the other security classes.
For instance, MS-DOS and Windows 3.1 are in division D.
Division C, the next level of security, provides discretionary protection and accountability of users and their actions through the use of audit capabilities.
A C1-class system incorporates some form of controls that allow users to protect private information and to keep other users from accidentally reading or destroying their data.
A C1 environment is one in which cooperating users access data at the same levels of sensitivity.
The total of all protection systems within a computer system (hardware, software, ﬁrmware) that correctly enforce a security policy is known as a trusted computer base (TCB)
The TCB of a C1 system controls access between users and ﬁles by allowing the user to specify and control sharing of objects by named individuals or deﬁned groups.
In addition, the TCB requires that the users identify themselves before they start any activities that the TCB is expected to mediate.
This identiﬁcation is accomplished via a protected mechanism or password.
The TCB protects the authentication data so that they are inaccessible to unauthorized users.
For example, access rights of a ﬁle can be speciﬁed to the level of a single individual.
In addition, the system administrator can selectively audit the actions of any one or more users based on individual identity.
The TCB also protects itself from modiﬁcation of its code or data structures.
In addition, no information produced by a prior user is available to another user who accesses a storage object that has been released back to the system.
Some special, secure versions of UNIX have been certiﬁed at the C2 level.
In addition, they attach a sensitivity label to each object in the system.
The B1-class TCB maintains these labels, which are used for decisions pertaining to mandatory access control.
For example, a user at the conﬁdential level could not access a ﬁle at the more sensitive secret level.
The TCB also denotes the sensitivity level at the top and bottom of each.
In addition to the normal user-namepassword authentication information, the TCB also maintains the clearance and authorizations of individual users and will support at least two levels of security.
These levels are hierarchical, so that a user may access any objects that carry sensitivity labels equal to or lower than his security clearance.
For example, a secret-level user could access a ﬁle at the conﬁdential level in the absence of other access controls.
Processes are also isolated through the use of distinct address spaces.
A B2-class system extends the sensitivity labels to each system resource, such as storage objects.
Physical devices are assigned minimum and maximum security levels that the system uses to enforce constraints imposed by the physical environments in which the devices are located.
In addition, a B2 system supports covert channels and the auditing of events that could lead to the exploitation of a covert channel.
A B3-class system allows the creation of access-control lists that denote users or groups not granted access to a given named object.
The TCB also contains a mechanism to monitor events that may indicate a violation of security policy.
The mechanism notiﬁes the security administrator and, if necessary, terminates the event in the least disruptive manner.
A system beyond class A1 might be designed and developed in a trusted facility by trusted personnel.
The use of a TCB merely ensures that the system can enforce aspects of a security policy; the TCB does not specify what the policy should be.
Typically, a given computing environment develops a security policy for certiﬁcation and has the plan accredited by a security agency, such as the National Computer Security Center.
Certain computing environments may require other certiﬁcation, such as that supplied by TEMPEST, which guards against electronic eavesdropping.
For example, a TEMPEST-certiﬁed system has terminals that are shielded to prevent electromagnetic ﬁelds from escaping.
This shielding ensures that equipment outside the room or building where the terminal is housed cannot detect what information is being displayed by the terminal.
Microsoft Windows 7 is a general-purpose operating system designed to support a variety of security features and methods.
In this section, we examine features that Windows 7 uses to perform security functions.
The Windows 7 security model is based on the notion of user accounts.
Windows 7 allows the creation of any number of user accounts, which can be grouped in any manner.
Access to system objects can then be permitted or denied as desired.
Users are identiﬁed to the system by a unique security ID.
When a user logs on, Windows 7 creates a security access token that includes the security ID for the user, security IDs for any groups of which the user is a member, and a list of any special privileges that the user has.
Examples of special privileges include backing up ﬁles and directories, shutting down.
Every process that Windows 7 runs on behalf of a user will receive a copy of the access token.
The system uses the security IDs in the access token to permit or deny access to system objects whenever the user, or a process on behalf of the user, attempts to access the object.
Authentication of a user account is typically accomplished via a user name and password, although the modular design of Windows 7 allows the development of custom authentication packages.
For example, a retinal (or eye) scanner might be used to verify that the user is who she says she is.
Windows 7 uses the idea of a subject to ensure that programs run by a user do not get greater access to the system than the user is authorized to have.
A subject is used to track and manage permissions for each program that a user runs.
It is composed of the user’s access token and the program acting on behalf of the user.
Since Windows 7 operates with a client–server model, two classes of subjects are used to control access: simple subjects and server subjects.
An example of a simple subject is the typical application program that a user executes after she logs on.
The simple subject is assigned a security context based on the security access token of the user.
A server subject is a process implemented as a protected server that uses the security context of the client when acting on the client’s behalf.
As mentioned in Section 15.7, auditing is a useful security technique.
Windows 7 has built-in auditing that allows many common security threats to be monitored.
Examples include failure auditing for login and logoff events to detect random password break-ins, success auditing for login and logoff events to detect login activity at strange hours, success and failure write-access auditing for executable ﬁles to track a virus outbreak, and success and failure auditing for ﬁle access to detect access to sensitive ﬁles.
Windows added mandatory integrity control, which works by assigning an integrity label to each securable object and subject.
In order for a given subject to have access to an object, it must have the access requested in the discretionary access-control list, and its integrity label must be equal to or higher than that of the secured object (for the given operation)
The integrity labels in Windows 7 are (in ascending order): untrusted, low, medium, high, and system.
In addition, three access mask bits are permitted for integrity labels: NoReadUp, NoWriteUp, and NoExecuteUp.
NoWriteUp is automatically enforced, so a lower-integrity subject cannot perform a write operation on a higher-integrity object.
However, unless explictly blocked by the security descriptor, it can perform read or execute operations.
For securable objects without an explicit integrity label, a default label of medium is assigned.
The label for a given subject is assigned during logon.
For instance, a nonadministrative user will have an integrity label of medium.
In addition to integrity labels, Windows Vista also added User Account Control (UAC), which represents an administrative account (not the built-in Administrators account) with two separate tokens.
One, for normal usage, has the built-in Administrators group disabled and has an integrity label of medium.
The other, for elevated usage, has the built-in Administrators group enabled and an integrity label of high.
Security attributes of an object in Windows 7 are described by a security descriptor.
The security descriptor contains the security ID of the owner of the object (who can change the access permissions), a group security ID used.
Optionally, the system access-control list can set the integrity of the object and identify which operations to block from lower-integrity subjects: read, write (always enforced), or execute.
For example, the security descriptor of the ﬁle foo.bar might have owner avi and this discretionary access-control list:
In addition, it might have a system access-control list that tells the system to audit writes by everyone, along with an integrity label of medium that denies read, write, and execute to lower-integrity subjects.
An access-control list is composed of access-control entries that contain the security ID of the individual and an access mask that deﬁnes all possible actions on the object, with a value of AccessAllowed or AccessDenied for each action.
We can see how this allows a ﬁne degree of control over access to objects.
Windows 7 classiﬁes objects as either container objects or noncontainer objects.
Container objects, such as directories, can logically contain other objects.
By default, when an object is created within a container object, the new object inherits permissions from the parent object.
Similarly, if the user copies a ﬁle from one directory to a new directory, the ﬁle will inherit the permissions of the destination directory.
Furthermore, if a permission is changed on a directory, the new permissions do not automatically apply to existing ﬁles and subdirectories; the user may explicitly apply them if he so desires.
The system administrator can prohibit printing to a printer on the system for all or part of a day and can use the Windows 7 Performance Monitor to help her spot approaching problems.
In general, Windows 7 does a good job of providing features to help ensure a secure computing environment.
Many of these features are not enabled by default, however, which may be one reason for the myriad security breaches on Windows 7 systems.
For a real multiuser environment, the system administrator should formulate a security plan and implement it, using the features that Windows 7 provides and other security tools.
The data stored in the computer system must be protected from unauthorized access, malicious destruction or alteration, and accidental introduction of inconsistency.
It is easier to protect against accidental loss of data consistency than to protect against malicious access to the data.
Absolute protection of the information stored in a computer system from malicious abuse is not possible; but the cost to the perpetrator can be made sufﬁciently high to deter most, if not all, attempts to access that information without proper authority.
Several types of attacks can be launched against programs and against individual computers or the masses.
Stack- and buffer-overﬂow techniques allow successful attackers to change their level of system access.
Viruses and worms are self-perpetuating, sometimes infecting thousands of computers.
Encryption limits the domain of receivers of data, while authentication limits the domain of senders.
Encryption is used to provide conﬁdentiality of data being stored or transferred.
Symmetric encryption requires a shared key, while asymmetric encryption provides a public key and a private key.
Authentication, when combined with hashing, can prove that data have not been changed.
User authentication methods are used to identify legitimate users of a system.
In addition to standard user-name and password protection, several authentication methods are used.
One-time passwords, for example, change from session to session to avoid replay attacks.
Two-factor authentication requires two forms of authentication, such as a hardware calculator with an activation PIN.
Methods of preventing or detecting security incidents include intrusiondetection systems, antivirus software, auditing and logging of system events, monitoring of system software changes, system-call monitoring, and ﬁrewalls.
Is there a simple method for detecting that such an event has occurred? Explain your answer.
Thus, if a user manages to read this list, password protection is no longer provided.
The watchdog then either grants or denies access to the ﬁle.
Discuss two pros and two cons of using watchdogs for security.
What are two potential hazards of using such a system for security? How can these problems be limited or eliminated?
Internet could design their systems to limit or eliminate the damage done by worms.
What are the drawbacks of making the change that you suggest?
Morris, Jr., for his creation and execution of the Internet worm discussed in Section 15.3.1
For each item on your list, state whether this concern relates to physical, human, or operating-system security.
Authentication: the receiver knows that only the sender could have generated the message.
Authentication and secrecy: only the receiver can decrypt the message, and the receiver knows that only the sender could have generated the message.
A security kernel for a multiprocessor microcomputer is described by [Schell (1983)]
A distributed secure system is described by [Rushby and Randell (1983)]
Password authentication with insecure communications is considered by [Lamport (1981)]
The issue of password cracking is examined by [Seely (1989)]
Issues related to trusting computer programs are discussed in [Thompson (1984)]
Security problems associated with the TCP/IP protocol suite are described in [Bellovin (1989)]
Another approach to protecting networks from insider attacks is to secure topology or route discovery.
A paper on the dangers of a computer monoculture can be found at http://cryptome.org/cyberinsecurity.htm.
Information about NIST’s AES activities can be found at http://www.nist.gov/aes; information about other cryptographic standards for the United States can also be found.
The Department of Defense Trusted Computer System Evaluation Criteria ([DoD (1985)]), known also as the Orange Book, describes a set of security levels and the features that an operating system must have to qualify for each security rating.
Reading it is a good starting point for understanding security concerns.
The Microsoft Windows NT Workstation Resource Kit ([Microsoft (1996)]) describes the security model of NT and how to use that model.
Generally, with a virtual machine, guest operating systems and applications run in an environment that appears to them to be native hardware.
This environment behaves toward them as native hardware would but also protects, manages, and limits them.
A distributed system is a collection of processors that do not share memory or a clock.
Instead, each processor has its own local memory, and the processors communicatewith one another through communication lines such as local-area or wide-area networks.
Distributed systems offer several beneﬁts: they give users access to more of the resources maintained by the system, speed computation, and improve data availability and reliability.
The term virtualization has many meanings, and aspects of virtualization permeate all aspects of computing.
Generally, with a virtual machine, guest operating systems and applications run in an environment that appears to them to be native hardware and that behaves toward them as native hardware would but that also protects, manages, and limits them.
This chapter delves into the uses, features, and implementation of virtual machines.
Virtual machines can be implemented in several ways, and this chapter describes these options.
One option is to add virtual machine support to the kernel.
Because that implementation method is the most pertinent to this book, we explore it most fully.
Additionally, hardware features provided by the CPU and even by I/O devices can support virtual machine implementation, so we discuss how those features are used by the appropriate kernel modules.
To show the most common hardware features that support virtualization and explain how they are used by operating-system modules.
The fundamental idea behind a virtual machine is to abstract the hardware of a single computer (the CPU, memory, disk drives, network interface cards, and so forth) into several different execution environments, thereby creating the illusion that each separate environment is running on its own private computer.
This concept may seem similar to the layered approach of operating system implementation (see Section 2.7.2), and in some ways it is.
In the case of virtualization, there is a layer that creates a virtual system on which operating systems or applications can run.
At the base is the host, the underlying hardware system that runs the virtual machines.
Each guest process is provided with a virtual copy of the host (Figure 16.1)
Usually, the guest process is in fact an operating system.
A single physical machine can thus run multiple operating systems concurrently, each in its own virtual machine.
Take a moment to note that with virtualization, the deﬁnition of “operating system” once again blurs.
This virtualization software is installed on the hardware, runs when the hardware boots, and provides services to applications.
The services include traditional ones, such as scheduling and memory management, along with new types, such as migration of applications between systems.
Is the VMware ESX VMM an operating system that, in turn, runs other operating systems? Certainly it acts like an operating system.
For clarity, however, we call the component that provides virtual environments a VMM.
Hardware-based solutions that provide support for virtual machine creation and management via ﬁrmware.
These VMMs, which are commonly found in mainframe and large to midsized servers, are generally known as type 0 hypervisors.
All problems in computer science can be solved by another level of indirection”—David Wheeler “
General-purpose operating systems that provide standard functions as well as VMM functions, including Microsoft Windows Server with HyperV and RedHat Linux with the KVM feature.
Applications that run on standard operating systems but provide VMM features to guest operating systems.
Emulators that allow applications written for one hardware environment to run on a very different hardware environment, such as a different type of CPU.
The variety of virtualization techniques in use today is a testament to the breadth, depth, and importance of virtualization in modern computing.
Virtualization is invaluable for data-center operations, efﬁcient application development, and software testing, among many other uses.
In addition, many of its original concepts are found in other systems, making it worth exploring.
A major difﬁculty with the VM approach involved disk systems.
Suppose that the physical machine had three disk drives but wanted to support seven virtual machines.
Clearly, it could not allocate a disk drive to each virtual machine.
The solution was to provide virtual diskstermed minidisks in IBM’s VM operating system.
The system implemented each minidisk by allocating as many tracks on the physical disks as the minidisk needed.
Once the virtual machines were created, users could run any of the operating systems or software packages that were available on the underlying machine.
For the IBM VM system, a user normally ran CMS—a single-user interactive operating system.
For many years after IBM introduced this technology, virtualization remained in its domain.
However, a formal deﬁnition of virtualization helped to establish system requirements and a target for functionality.
A VMM provides an environment for programs that is essentially identical to the original machine.
Programs running within that environment show only minor performance decreases.
These requirements of ﬁdelity, performance, and safety still guide virtualization efforts today.
Accordingly, developers launched multiple efforts to implement virtualization on that platform.
Both Xen and VMware created technologies, still used today, to allow guest operating systems to run on the 80x86
Since that time, virtualization has expanded to include all common CPUs, many commercial and open-source tools, and many operating systems.
Most of them are fundamentally related to the ability to share the same hardware yet run several different execution environments (that is, different operating systems) concurrently.
One important advantage of virtualization is that the host system is protected from the virtual machines, just as the virtual machines are protected from each other.
A virus inside a guest operating system might damage that operating system but is unlikely to affect the host or the other guests.
Because each virtual machine is almost completely isolated from all other virtual machines, there are almost no protection problems.
A potential disadvantage of isolation is that it can prevent sharing of resources.
First, it is possible to share a ﬁle-system volume and thus to share ﬁles.
Second, it is possible to deﬁne a network of virtual machines, each of which can.
The network is modeled after physical communication networks but is implemented in software.
Of course, the VMM is free to allow any number of its guests to use physical resources, such as a physical network connection (with sharing provided by the VMM), in which case the allowed guests could communicate with each other via the physical network.
One feature common to most virtualization implementations is the ability to freeze, or suspend, a running virtual machine.
Many operating systems provide that basic feature for processes, but VMMs go one step further and allow copies and snapshots to be made of the guest.
The copy can be used to create a new VM or to move a VM from one machine to another with its current state intact.
The guest can then resume where it was, as if on its original machine, creating a clone.
The snapshot records a point in time, and the guest can be reset to that point if necessary (for example, if a change was made but is no longer wanted)
For example, snapshots might record a guest’s state every day for a month, making restoration to any of those snapshot states possible.
These abilities are used to good advantage in virtual environments.
A virtual machine system is a perfect vehicle for operating-system research and development.
Operating systems are large and complex programs, and a change in one part may cause obscure bugs to appear in some other part.
The power of the operating system makes changing it particularly dangerous.
Because the operating system executes in kernel mode, a wrong change in a pointer could cause an error that would destroy the entire ﬁle system.
Thus, it is necessary to test all changes to the operating system carefully.
Furthermore, the operating system runs on and controls the entire machine, meaning that the system must be stopped and taken out of use while changes are made and tested.
Since it makes the system unavailable to users, system-development time on shared systems is often scheduled late at night or on weekends, when system load is low.
A virtual-machine system can eliminate much of this latter problem.
System programmers are given their own virtual machine, and system development is done on the virtual machine instead of on a physical machine.
Normal system operation is disrupted only when a completed and tested change is ready to be put into production.
Another advantage of virtual machines for developers is that multiple operating systems can run concurrently on the developer’s workstation.
This virtualized workstation allows for rapid porting and testing of programs in varying environments.
In addition, multiple versions of a program can run, each in its own isolated operating system, within one system.
Similarly, qualityassurance engineers can test their applications in multiple environments without buying, powering, and maintaining a computer for each environment.
A major advantage of virtual machines in production data-center use is system consolidation, which involves taking two or more separate systems and running them in virtual machines on one system.
Consider, too, that management tools that are part of the VMM allow system administrators to manage many more systems than they otherwise could.
With virtualization and its tools, the same work can be managed by one or two administrators.
One of the tools that make this possible is templating, in which one standard virtual machine image, including an installed and conﬁgured guest operating system and applications, is saved and used as a source for multiple running VMs.
Other features include managing the patching of all guests, backing up and restoring the guests, and monitoring their resource use.
Virtualization can improve not only resource utilization but also resource management.
Some VMMs include a live migration feature that moves a running guest from one physical server to another without interrupting its operation or active network connections.
If a server is overloaded, live migration can thus free resources on the source host while not disrupting the guest.
Similarly, when host hardware must be repaired or upgraded, guests can be migrated to other servers, the evacuated host can be maintained, and then the guests can be migrated back.
This operation occurs without downtime and without interruption to users.
Think about the possible effects of virtualization on how applications are deployed.
If a system can easily add, remove, and move a virtual machine, then why install applications on that system directly? Instead, the application could be preinstalled on a tuned and customized operating system in a virtual machine.
Application management would become easier, less tuning would be required, and technical support of the application would be more straightforward.
System administrators would ﬁnd the environment easier to manage as well.
Installation would be simple, and redeploying the application to another system would be much easier than the usual steps of uninstalling and reinstalling.
For widespread adoption of this methodology to occur, though, the format of virtual machines must be standardized so that any virtual machine will run on any virtualization platform.
The “Open Virtual Machine Format” is an attempt to provide such standardization, and it could succeed in unifying virtual machine formats.
Virtualization has laid the foundation for many other advances in computer facility implementation, management, and monitoring.
Cloud computing, for example, is made possible by virtualization in which resources such as CPU, memory, and I/O are provided as services to customers using Internet technologies.
By using APIs, a program can tell a cloud computing facility to create thousands of VMs, all running a speciﬁc guest operating system and application, which others can access via the Internet.
Many multiuser games, photo-sharing sites, and other web services use this functionality.
In the area of desktop computing, virtualization is enabling desktop and laptop computer users to connect remotely to virtual machines located in remote data centers and access their applications as if they were local.
This practice can increase security, because no data are stored on local disks at the user’s site.
The cost of the user’s computing resource may also decrease.
Other uses of virtualization are sure to follow as it becomes more prevalent and hardware support continues to improve.
Although the virtual machine concept is useful, it is difﬁcult to implement.
Much work is required to provide an exact duplicate of the underlying machine.
This is especially a challenge on dual-mode systems, where the underlying machine has only user mode and kernel mode.
In this section, we examine the building blocks that are needed for efﬁcient virtualization.
The ability to virtualize depends on the features provided by the CPU.
If the features are sufﬁcient, then it is possible to write a VMM that provides a guest environment.
VMMs use several techniques to implement virtualization, including trap-and-emulate and binary translation.
We discuss each of these techniques in this section, along with the hardware support needed to support virtualization.
One important concept found in most virtualization options is the implementation of a virtual CPU (VCPU)
Rather, it represents the state of the CPU as the guest machine believes it to be.
For each guest, the VMM maintains a VCPU representing that guest’s current CPU state.
When the guest is context-switched onto a CPU by the VMM, information from the VCPU is used to load the right context, much as a general-purpose operating system would use the PCB.
On a typical dual-mode system, the virtual machine guest can execute only in user mode (unless extra hardware support is provided)
The kernel, of course, runs in kernel mode, and it is not safe to allow user-level code to run in kernel mode.
Just as the physical machine has two modes, however, so must the virtual machine.
Consequently, we must have a virtual user mode and a virtual kernel mode, both of which run in physical user mode.
Those actions that cause a transfer from user mode to kernel mode on a real machine (such as a system call, an interrupt, or an attempt to execute a privileged instruction) must also cause a transfer from virtual user mode to virtual kernel mode in the virtual machine.
How can such a transfer be accomplished? The procedure is as follows: When the kernel in the guest attempts to execute a privileged instruction, that is an error (because the system is in user mode) and causes a trap to the VMM in the real machine.
The VMM gains control and executes (or “emulates”) the action that was attempted by the guest kernel on the part of the guest.
This is called the trap-and-emulate method and is shown in Figure 16.2
Most virtualization products use this method to one extent or other.
All nonprivileged instructions run natively on the hardware, providing the same performance.
Privileged instructions create extra overhead, however, causing the guest to run more slowly than it would natively.
In addition, the CPU is being multiprogrammed among many virtual machines, which can further slow down the virtual machines in unpredictable ways.
Only the privileged instructions (needed mainly for I/O) must be emulated and hence execute more slowly.
In general, with the evolution of hardware, the performance of trap-and-emulate functionality has been improved, and cases in which it is needed have been reduced.
For example, many CPUs now have extra modes added to their standard dual-mode operation.
The VCPU need not keep track of what mode the guest operating system is in, because the physical CPU performs that function.
In fact, some CPUs provide guest CPU state management in hardware, so the VMM need not supply that functionality, removing the extra overhead.
Some CPUs do not have a clean separation of privileged and nonprivileged instructions.
Unfortunately for virtualization implementers, the Intel x86 CPU line is one of them.
No thought was given to running virtualization on the x86 when it was designed.
The chip has maintained backward compatibility throughout its lifetime, preventing changes that would have made virtualization easier through many generations.
The command popf loads the ﬂag register from the contents of the stack.
If the CPU is in privileged mode, all of the ﬂags are replaced from the stack.
If the CPU is in user mode, then only some ﬂags are replaced, and others are ignored.
Because no trap is generated if popf is executed in user mode, the trap-and-emulate procedure is rendered.
For the purposes of this discussion, we will call this set of instructions special instructions.
This previously insurmountable problem was solved with the implementation of the binary translation technique.
Binary translation is fairly simple in concept but complex in implementation.
If the guest VCPU is in user mode, the guest can run its instructions natively on a physical CPU.
If the guest VCPU is in kernel mode, then the guest believes that it is running in kernel mode.
The VMM examines every instruction the guest executes in virtual kernel mode by reading the next few instructions that the guest is going to execute, based on the guest’s program counter.
Special instructions are translated into a new set of instructions that perform the equivalent task—for example changing the ﬂags in the VCPU.
The code reads native binary instructions dynamically from the guest, on demand, and generates native binary code that executes in place of the original code.
The basic method of binary translation just described would execute correctly but perform poorly.
But how could performance be improved for the other instructions? We can turn to a speciﬁc implementation of binary translation, the VMware method, to see one way of improving performance.
The replacement code for each instruction that needs to be translated is cached.
All later executions of that instruction run from the translation cache and need not be translated again.
If the cache is large enough, this method can greatly improve performance.
Let’s consider another issue in virtualization: memory management, specifically the page tables.
How can the VMM keep page-table state both for guests that believe they are managing the page tables and for the VMM itself? A common method, used with both trap-and-emulate and binary translation, is to use nested page tables (NPTs)
Each guest operating system maintains one or more page tables to translate from virtual to physical memory.
The VMM maintains NPTs to represent the guest’s page-table state, just as it creates a VCPU to represent the guest’s CPU state.
The VMM knows when the guest tries to change its page table, and it makes the equivalent change in the NPT.
When the guest is on the CPU, the VMM puts the pointer to the appropriate NPT into the appropriate CPU register to make that table the active page table.
If the guest needs to modify the page table (for example, fulﬁlling a page fault), then that operation must be intercepted by the VMM and appropriate changes made to the nested and system page tables.
Unfortunately, the use of NPTs can cause TLB misses to increase, and many other complexities need to be addressed to achieve reasonable performance.
Although it might seem that the binary translation method creates large amounts of overhead, it performed well enough to launch a new industry aimed at virtualizing Intel x86-based systems.
VMware tested the performance impact of binary translation by booting one such system, Windows XP, and immediately shutting it down while monitoring the elapsed time and the number of translations produced by the binary translation method.
To achieve that result, developers used many performance improvements that we do not discuss here.
For more information, consult the bibliographical notes at the end of this chapter.
Without some level of hardware support, virtualization would be impossible.
The more hardware support available within a system, the more feature-rich and stable the virtual machines can be and the better they can perform.
In fact, all major general-purpose CPUs are providing extended amounts of hardware support for virtualization.
It deﬁnes two new modes of operation—host and guest—thus moving from a dual-mode to a multimode processor.
The VMM can enable host mode, deﬁne the characteristics of each guest virtual machine, and then switch the system to guest mode, passing control of the system to a guest operating system that is running in the virtual machine.
In guest mode, the virtualized operating system thinks it is running on native hardware and sees whatever devices are included in the host’s deﬁnition of the guest.
The functionality in Intel VT-x is similar, providing root and nonroot modes, equivalent to host and guest modes.
Both provide guest VCPU state data structures to load and save guest CPU state automatically during guest context switches.
In addition, virtual machine control structures (VMCSs) are provided to manage guest and host state, as well as the various guest execution controls, exit controls, and information about why guests exit back to the host.
In the latter case, for example, a nested page-table violation caused by an attempt to access unavailable memory can result in the guest’s exit.
In essence, these CPUs implement nested page tables in hardware to allow the VMM to fully control paging while the CPUs accelerate the translation from virtual to physical addresses.
The CPU page-table walking function includes this new layer as necessary, walking through the guest table to the VMM table to ﬁnd the physical address desired.
A TLB miss results in a performance penalty, because more tables must be traversed (the guest and host page tables) to complete the lookup.
Figure 16.4 shows the extra translation work performed by the hardware to translate from a guest virtual address to a ﬁnal physical address.
Without hardware assistance, a guest might try to set up a DMA transfer that affects the memory of the VMM or other guests.
In CPUs that provide hardware-assisted DMA (such as Intel CPUs with VT-d), even DMA has a level of indirection.
First, the VMM sets up protection domains to tell the CPU which physical memory belongs to each guest.
Next, it assigns the I/O devices to the protection domains, allowing them direct access to those memory regions and only those regions.
The hardware then transforms the address in a DMA request issued by an I/O device to the host physical memory address associated with the I/O.
In this manner DMA transfers are passed through between a guest and a device without VMM interference.
Similarly, interrupts must be delivered to the appropriate guest and must not be visible to other guests.
By providing an interrupt remapping feature, CPUs with virtualization hardware assistance automatically deliver an interrupt destined for a guest to a core that is currently running a thread of that guest.
That way, the guest receives interrupts without the VMM’s needing to intercede in their delivery.
Without interrupt remapping, malicious guests can generate interrupts that can be used to gain control of the host system.
See the bibliographical notes at the end of this chapter for more details.
We’ve now looked at some of the techniques used to implement virtualization.
Next, we consider the major types of virtual machines, their implementation, their functionality, and how they use the building blocks just described to.
Of course, the hardware on which the virtual machines are running can cause great variation in implementation methods.
Here, we discuss the implementations in general, with the understanding that VMMs take advantage of hardware assistance where it is available.
Whatever the hypervisor type, at the time a virtual machine is created, its creator gives the VMM certain parameters.
These parameters usually include the number of CPUs, amount of memory, networking details, and storage details that the VMM will take into account when creating the guest.
The VMM then creates the virtual machine with those parameters.
In the case of a type 0 hypervisor, the resources are usually dedicated.
In this situation, if there are not two virtual CPUs available and unallocated, the creation request.
For other hypervisor types, the resources are dedicated or virtualized, depending on the type.
Certainly, an IPaddress cannot be shared, but the virtual CPUs are usually multiplexed on the physical CPUs as discussed in Section 16.6.1
Similarly, memory management usually involves allocating more memory to guests than actually exists in physical memory.
This is more complicated and is described in Section 16.6.2
Finally, when the virtual machine is no longer needed, it can be deleted.
When this happens, the VMM ﬁrst frees up any used disk space and then removes the conﬁguration associated with the virtual machine, essentially forgetting the virtual machine.
These steps are quite simple compared with building, conﬁguring, running, and removing physical machines.
Creating a virtual machine from an existing one can be as easy as clicking the “clone” button and providing a new name and IP address.
This ease of creation can lead to virtual machine sprawl, which occurs when there are so many virtual machines on a system that their use, history, and state become confusing and difﬁcult to track.
Type 0 hypervisors have existed for many years under many names, including “partitions” and “domains”
They are a hardware feature, and that brings its own positives and negatives.
Operating systems need do nothing special to take advantage of their features.
The VMM itself is encoded in the ﬁrmware and loaded at boot time.
In turn, it loads the guest images to run in each partition.
The feature set of a type 0 hypervisor tends to be smaller than those of the other types because it is implemented in hardware.
For example, a system might be split into four virtual systems, each with dedicated CPUs, memory, and I/O devices.
Each guest believes that it has dedicated hardware because it does, simplifying many implementation details.
I/O presents some difﬁculty, because it is not easy to dedicate I/O devices to guests if there are not enough.
What if a system has two Ethernet ports and more than two guests, for example? Either all guests must get their own I/O devices, or the system must provided I/O device sharing.
In these cases, the hypervisor manages shared access or grants all devices to a control partition.
In the control partition, a guest operating system provides services (such as networking) via daemons to other guests, and the hypervisor routes I/O requests appropriately.
Some type 0 hypervisors are even more sophisticated and can move physical CPUs and memory between running guests.
In these cases, the guests are paravirtualized, aware of the virtualization and assisting in its execution.
For example, a guest must watch for signals from the hardware or VMM that a hardware change has occurred, probe its hardware devices to detect the change, and add or subtract CPUs or memory from its available resources.
Because type 0 virtualization is very close to raw hardware execution, it should be considered separately from the other methods discussed here.
A type 0 hypervisor can run multiple guest operating systems (one in each hardware partition)
All of those guests, because they are running on raw hardware, can in turn be VMMs.
Essentially, the guest operating systems in a type 0 hypervisor are native operating systems with a subset of hardware made available to them.
Because of that, each can have its own guest operating.
Type 1 hypervisors are commonly found in company data centers and are in a sense becoming “the data-center operating system.” They are special-purpose operating systems that run natively on the hardware, but rather than providing system calls and other interfaces for running programs, they create, run, and manage guest operating systems.
Whatever the platform, guests generally do not know they are running on anything but the native hardware.
Type 1 hypervisors run in kernel mode, taking advantage of hardware protection.
Where the host CPU allows, they use multiple modes to give guest operating systems their own control and improved performance.
They implement device drivers for the hardware they run on, because no other component could do so.
Because they are operating systems, they must also provide CPU scheduling, memory management, I/O management, protection, and even security.
Frequently, they provide APIs, but those APIs support applications in guests or external applications that supply features like backups, monitoring, and security.
Many type 1 hypervisors are closed-source commercial offerings, such as VMware ESX while some are open source or hybrids of open and closed source, such as Citrix XenServer and its open Xen counterpart.
By using type 1 hypervisors, data-center managers can control and manage the operating systems and applications in new and sophisticated ways.
An important beneﬁt is the ability to consolidate more operating systems and applications onto fewer systems.
For example, rather than having ten systems running at 10 percent utilization each, a data center might have one server manage the entire load.
If utilization increases, guests and their applications can be moved to less-loaded systems live, without interruption of service.
Using snapshots and cloning, the system can save the states of guests and duplicate those states—a much easier task than restoring from backups or installing manually or via scripts and tools.
Another type of type 1 hypervisor includes various general-purpose operating systems with VMM functionality.
In this instance, an operating system such as RedHat Enterprise Linux, Windows, or Oracle Solaris performs its normal duties as well as providing a VMM allowing other operating systems to run as guests.
Because of their extra duties, these hypervisors typically provide fewer virtualization features than other type 1 hypervisors.
In many ways, they treat a guest operating system as just another process, albeit with special handling provided when the guest tries to execute special instructions.
Type 2 hypervisors are less interesting to us as operating-system explorers, because there is very little operating-system involvement in these applicationlevel virtual machine managers.
This type of VMM is simply another process run and managed by the host, and even the host does not know virtualization is happening within the VMM.
Type 2 hypervisors have limits not associated with some of the other types.
For example, a user needs administrative privileges to access many of the hardware assistance features of modern CPUs.
If the VMM is being run by a standard user without additional privileges, the VMM cannot take advantage of these features.
As is often the case, the limitations of type 2 hypervisors also provide some beneﬁts.
They run on a variety of general-purpose operating systems, and running them requires no changes to the host operating system.
A student can use a type 2 hypervisor, for example, to test a non-native operating system without replacing the native operating system.
In fact, on an Apple laptop, a student could have versions of Windows, Linux, Unix, and less common operating systems all available for learning and experimentation.
As we’ve seen, paravirtualization takes a different tack than the other types of virtualization.
Rather than try to trick a guest operating system into believing it has a system to itself, paravirtualization presents the guest with a system that is similar but not identical to the guest’s preferred system.
The guest must be modiﬁed to run on the paravirtualized virtual hardware.
The gain for this extra work is more efﬁcient use of resources and a smaller virtualization layer.
For example, as we have seen, some VMMs present virtual devices to guests that appear to be real devices.
Instead of taking that approach, the Xen VMM presents clean and simple device abstractions that allow efﬁcient I/O, as well as good communication between the guest and the VMM about device I/O.
For each device used by each guest, there is a circular buffer shared by the guest and the VMM via shared memory.
Read and write data are placed in this buffer, as shown in Figure 16.6
Request queue - Descriptors queued by the VM but not yet accepted by Xen Outstanding descriptors - Descriptor slots awaiting a response from Xen Response queue - Descriptors returned by Xen in response to serviced requests Unused descriptors.
For memory management, Xen does not implement nested page tables.
Rather, each guest has its own set of page tables, set to read-only.
Xen requires the guest to use a speciﬁc mechanism, a hypercall from the guest to the hypervisor VMM, when a page-table change is needed.
This means that the guest operating system’s kernel code must be changed from the default code to these Xen-speciﬁc methods.
To optimize performance, Xen allows the guest to queue up multiple page-table changes asynchronously via hypercalls and then check to ensure that the changes are complete before continuing operation.
Xen allowed virtualization of x86 CPUs without the use of binary translation, instead requiring modiﬁcations in the guest operating systems like the one described above.
Over time, Xen has taken advantage of hardware features supporting virtualization.
As a result, it no longer requires modiﬁed guests and essentially does not need the paravirtualization method.
Paravirtualization is still used in other solutions, however, such as type 0 hypervisors.
Another kind of virtualization, based on a different execution model, is the virtualization of programming environments.
Here, a programming language is designed to run within a custom-built virtualized environment.
For example, Oracle’s Java has many features that depend on its running in the Java virtual machine (JVM), including speciﬁc methods for security and memory management.
If we deﬁne virtualization as including only duplication of hardware, this is not really virtualization at all.
Instead, we can deﬁne a virtual environment, based on APIs, that provides a set of features that we want to have available for a particular language and programs written in that language.
This arrangement means that Java programs are written once and then can run on any system (including all of the major operating systems) on which a JVM is available.
The same can be said for interpreted languages, which run inside programs that read each instruction and interpret it into native operations.
Virtualization is probably the most common method for running applications designed for one operating system on a different operating system, but on the same CPU.
This method works relatively efﬁciently because the applications were compiled for the same instruction set as the target system uses.
But what if an application or operating system needs to run on a different CPU? Here, it is necessary to translate all of the source CPU’s instructions so that they are turned into the equivalent instructions of the target CPU.
Such an environment is no longer virtualized but rather is fully emulated.
Emulation is useful when the host system has one system architecture and the guest system was compiled for a different architecture.
For example, suppose a company has replaced its outdated computer system with a new system but would like to continue to run certain important programs that were compiled for the old system.
The programs could be run in an emulator that translates each of the outdated system’s instructions into the native instruction set of the new system.
Emulation can increase the life of programs and allow us to explore old architectures without having an actual old machine.
As may be expected, the major challenge of emulation is performance.
Instruction-set emulation can run an order of magnitude slower than native instructions, because it may take ten instructions on the new system to read, parse, and simulate an instruction from the old system.
Thus, unless the new machine is ten times faster than the old, the program running on the new machine will run more slowly than it did on its native hardware.
Another challenge for emulator writers is that it is difﬁcult to create a correct emulator because, in essence, this task involves writing an entire CPU in software.
In spite of these challenges, emulation is very popular, particularly in gaming circles.
Many popular video games were written for platforms that are no longer in production.
Users who want to run those games frequently can ﬁnd an emulator of such a platform and then run the game unmodiﬁed within the emulator.
Modern systems are so much faster than old game consoles that even the Apple iPhone has game emulators and games available to run within them.
The goal of virtualization in some instances is to provide a method to segregate applications, manage their performance and resource use, and create an easy way to start, stop, move, and manage them.
If the applications are all compiled for the same operating system, then we do not need complete virtualization to provide these features.
Starting with version 10, Oracle Solaris has included containers, or zones, that create a virtual layer between the operating system and the applications.
In this system, only one kernel is installed, and the hardware is not virtualized.
Rather, the operating system and its devices are virtualized, providing processes within a zone with the impression that they are the only processes on the system.
One or more containers can be created, and each can have its own applications, network stacks, network address and ports, user accounts, and so on.
Each zone in fact can run its own scheduler to optimize the performance of its applications on the allotted resources.
Thus far, we have explored the building blocks of virtualization and the various types of virtualization.
In this section, we take a deeper dive into the operatingsystem aspects of virtualization, including how the VMM provides core operating-system functions like scheduling, I/O, and memory management.
Here, we answer questions such as these: How do VMMs schedule CPU use when guest operating systems believe they have dedicated CPUs? How can memory management work when many guests require large amounts of memory?
A system with virtualization, even a single-CPU system, frequently acts like a multiprocessor system.
The virtualization software presents one or more virtual CPUs to each of the virtual machines running on the system and then schedules the use of the physical CPUs among the virtual machines.
The signiﬁcant variations among virtualization technologies make it difﬁcult to summarize the effect of virtualization on scheduling.
The VMM has a number of physical CPUs available and a number of threads to run on those CPUs.
Guests are conﬁgured with a certain number of virtual CPUs at creation time, and that number can be adjusted throughout the life of the VM.
When there are enough CPUs to allocate the requested number to each guest, the VMM can treat the CPUs as dedicated and schedule only a given guest’s threads on that guest’s CPUs.
In this situation, the guests act much like native operating systems running on native CPUs.
Of course, in other situations, there may not be enough CPUs to go around.
The VMM itself needs some CPU cycles for guest management and I/O management and can steal cycles from the guests by scheduling its threads across all of the system CPUs, but the impact of this action is relatively minor.
More difﬁcult is the case of overcommitment, in which the guests are conﬁgured for more CPUs than exist in the system.
Here, a VMM can use standard scheduling algorithms to make progress on each thread but can also add a fairness aspect to those algorithms.
For example, if there are six hardware CPUs and 12 guest-allocated CPUs, the VMM could allocate CPU resources proportionally, giving each guest half of the CPU resources it believes it has.
The VMM can still present all 12 virtual CPUs to the guests, but in mapping them onto physical CPUs, the VMM can use its scheduler to share them appropriately.
Even given a scheduler that provides fairness, any guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively affected by virtualization.
Consider a timesharing operating system that tries to allot 100 milliseconds to each time slice to give users a reasonable response time.
Within a virtual machine, this operating system is at the mercy of the virtualization system as to what CPU resources it actually receives.
Depending on how busy the system is, the time slice may take a second or more, resulting in very poor response times for users logged into that virtual machine.
The effect on a real-time operating system can be even more serious.
The net effect of such scheduling layering is that individual virtualized operating systems receive only a portion of the available CPU cycles, even though they believe they are receiving all of the cycles and indeed that they are scheduling all of those cycles.
Commonly, the time-of-day clocks in virtual machines are incorrect because timers take longer to trigger than they would on dedicated CPUs.
To correct for this, a VMM will have an application available for each type of operating system that system administrators install into the guests.
Efﬁcient memory use in general-purpose operating systems is one of the major keys to performance.
In virtualized environments, there are more users of memory (the guests and their applications, as well as the VMM), leading to more pressure on memory use.
Further adding to this pressure is that VMMs typically overcommit memory, so that the total memory with which guests are conﬁgured exceeds the amount of memory that physically exists in the system.
The extra need for efﬁcient memory use is not lost on the implementers of VMMs, who take great measures to ensure the optimal use of memory.
For example, VMware ESX uses at least three methods of memory management.
Before memory optimization can occur, the VMM must establish how much real memory each guest should use.
To do that, the VMM ﬁrst evaluates the maximum memory size of each guest as dictated when it is conﬁgured.
General-purpose operating systems do not expect the amount of memory in the system to change, so VMMs must maintain the illusion that the guest has that amount of memory.
Next, the VMM computes a target real memory allocation for each guest based on the conﬁgured memory for that guest and other factors, such as overcommitment and system load.
It then uses the three low-level mechanisms below to reclaim memory from the guests.
The overall effect is to enable guests to behave and perform as if they had the full amount of memory requested although in reality they have less.
Recall that a guest believes it controls memory allocation via its pagetable management, whereas in reality the VMM maintains a nested page table that re-translates the guest page table to the real page table.
The VMM can use this extra level of indirection to optimize the guest’s use of memory without the guest’s knowledge or help.
One approach is to provide double paging, in which the VMM has its own page-replacement algorithms and pages to backing-store pages that the guest believes are in physical memory.
Of course, the VMM has knows less about the guest’s memory access patterns than the guest does, so its paging is less efﬁcient, creating performance problems.
VMMs do use this method when other methods are not available or are not providing enough free memory.
A common solution is for the VMM to install in each guest a pseudodevice driver or kernel module that it controls.
A pseudo–device driver uses device-driver interfaces, appearing to the kernel to be a device driver, but does not actually control a device.
Rather, it is an easy way to add kernel-mode code without directly modifying the kernel.
This balloon memory manager communicates with the VMM and is told to allocate or deallocate memory.
If told to allocate, it allocates memory and tells the operating system to pin the allocated pages into physical memory.
Recall that pinning locks a page into physical memory so that it cannot be moved or paged out.
The guest sees memory pressure becauses of these pinned pages, essentially decreasing the amount of physical memory it has available to use.
Meanwhile, the VMM, knowing that the pages pinned by the balloon process will never be used, removes those physical pages from the guest and allocates them to another guest.
At the same time, the guest is using its own memorymanagement and paging algorithms to manage the available memory, which is the most efﬁcient option.
If memory pressure within the entire system decreases, the VMM will tell the balloon process within the guest to unpin and free some or all of the memory, allowing the guest more pages for its use.
Another common method for reducing memory pressure is for the VMM to determine if the same page has been loaded more than once.
If this is the case, to the VMM reduces the number of copies of the page to one and maps the other users of the page to that one copy.
VMware, for example, randomly samples guest memory and creates a hash for each page sampled.
The hash of every page examined is compared with other hashes already stored in a hash table.
If there is a match, the pages are compared byte by byte to see if they really are identical.
If they are, one page is freed, and its logical address is mapped to the other’s physical address.
This technique might seem at ﬁrst to be ineffective, but consider that guests run operating systems.
If multiple guests run the same operating system, then only one copy of the active operating-system pages need be in memory.
Similarly, multiple guests could be running the same set of applications, again a likely source of memory sharing.
In the area of I/O, hypervisors have some leeway and can be less concerned with exactly representing the underlying hardware to their guests.
Because of all the variation in I/O devices, operating systems are used to dealing with varying and ﬂexible I/O mechanisms.
For example, operating systems have a device-driver mechanism that provides a uniform interface to the operating system whatever the I/O device.
Device-driver interfaces are designed to allow third-party hardware manufacturers to provide device drivers connecting their devices to the operating system.
Virtualization takes advantage of such built-in ﬂexibility by providing speciﬁc virtualized devices to guest operating systems.
As described in Section 16.5, VMMs vary greatly in how they provide I/O to their guests.
I/O devices may be dedicated to guests, for example, or the VMM may have device drivers onto which it maps guest I/O.
The VMM may also provide idealized device drivers to guests, which allows easy provision and management of guest I/O.
In this case, the guest sees an easy-to-control device, but in reality that simple device driver communicates to the VMM which sends those requests to a more complicated real device through a more complex real device driver.
I/O in virtual environments is complicated and requires careful VMM design and implementation.
Consider the case of a hypervisor and hardware combination that allows devices to be dedicated to a guest and allows the guest to access those devices directly.
Of course, a device dedicated to one guest is not available to any other guests, but this direct access can still be useful in some circumstances.
The reason to allow direct access is to improve I/O performance.
The less the hypervisor has to do to enable I/O for its guests, the faster the I/O can occur.
With Type 0 hypervisors that provide direct device access, guests can often run at the same speed as native operating systems.
When type 0 hypervisors instead provide shared devices, performance can suffer by comparison.
The hardware needs to provide DMA pass-through with facilities like VT-d, as well as direct interrupt delivery to speciﬁc guests.
Given how frequently interrupts occur, it should be no surprise that the guests on hardware without these features have worse performance than if they were running natively.
In addition to direct access, VMMs provide shared access to devices.
Consider a disk drive to which multiple guests have access.
The VMM must provide protection while sharing the device, assuring that a guest can access only the blocks speciﬁed in the guest’s conﬁguration.
In such instances, the VMM must be part of every I/O, checking it for correctness as well as routing the data to and from the appropriate devices and guests.
In the area of networking, VMMs also have work to do.
General-purpose operating systems typically have one Internet protocol (IP) address, although they sometimes have more than one—for example, to connect to a management network, backup network, and production network.
With virtualization, each guest needs at least one IP address, because that is the guest’s main mode of communication.
Therefore, a server running a VMM may have dozens of addresses, and the VMM acts as a virtual switch to route the network packets to the addressed guest.
The guests can be “directly” connected to the network by an IP address that is seen by the broader network (this is known as bridging)
Alternatively, the VMM can provide a network address translation (NAT) address.
The NAT address is local to the server on which the guest is running, and the VMM provides routing between the broader network and the guest.
The VMM also provides ﬁrewalling, moderating connections between guests within the system and between guests and external systems.
An important question in determining how virtualization works is this: If multiple operating systems have been installed, what and where is the boot disk? Clearly, virtualized environments need to approach the area of storage management differently from native operating systems.
Even the standard multiboot method of slicing the root disk into partitions, installing a boot manager in one partition, and installing each other operating system in another partition is not sufﬁcient, because partitioning has limits that would prevent it from working for tens or hundreds of virtual machines.
Once again, the solution to this problem depends on the type of hypervisor.
Type 0 hypervisors do tend to allow root disk partitioning, partly because these systems tend to run fewer guests than other systems.
Alternatively, they may have a disk manager as part of the control partition, and that disk manager provides disk space (including boot disks) to the other partitions.
Type 1 hypervisors store the guest root disk (and conﬁguration information) in one or more ﬁles within the ﬁle systems provided by the VMM.
Type 2 hypervisors store the same information within the host operating system’s ﬁle systems.
In essence, a disk image, containing all of the contents of the root disk of the guest, is contained within one ﬁle in the VMM.
Aside from the potential performance problems that causes, it is a clever solution, because it simpliﬁes copying and moving guests.
If the administrator wants a duplicate of the guest (for testing, for example), she simply copies the associated disk image of the guest and tells the VMM about the new copy.
Moving a virtual machine from one system to another that runs the same VMM is as simple as halting the guest, copying the image to the other system, and starting the guest there.
Guests sometimes need more disk space than is available in their root disk image.
For example, a nonvirtualized database server might use several ﬁle systems spread across many disks to store various parts of the database.
Virtualizing such a database usually involves creating several ﬁles and having the VMM present those to the guest as disks.
The guest then executes as usual, with the VMM translating the disk I/O requests coming from the guest into ﬁle I/O commands to the correct ﬁles.
Frequently, VMMs provide a mechanism to capture a physical system as it is currently conﬁgured and convert it to a guest that the VMM can manage and run.
Based on the discussion above, it should be clear that this physicalto-virtual (P-to-V) conversion reads the disk blocks of the physical system’s disks and stores them within ﬁles on the VMM’s system or on shared storage that the VMM can access.
Perhaps not as obvious is the need for a virtual-tophysical (V-to-P) procedure for converting a guest to a physical system.
This step is sometimes needed for debugging: a problem could be caused by the VMM or associated components, and the administrator could attempt to solve the problem by removing virtualization from the problem variables.
V-to-P conversion can take the ﬁles containing all of the guest data and generate disk blocks on a system’s disk, recreating the guest as a native operating system and applications.
Once the testing is concluded, the native system can be reused for other purposes when the virtual machine returns to service, or the virtual machine can be deleted and the native system can continue to run.
Here, we explore the details of how live migration works and why VMMs have a relatively easy time implementing it while general-purpose operating systems, in spite of some research attempts, do not.
A running guest on one system is copied to another system running the same VMM.
The copy occurs with so little interruption of service that users logged in to the guest, and network connections to the guest, continue without noticeable impact.
This rather astonishing ability is very powerful in resource management and hardware administration.
After all, compare it with the steps necessary without virtualization: warning users, shutting down the processes, possibly moving the.
With live migration, an overloaded system can have its load decreased live with no discernible disruption.
Similarly, a system needing hardware or system changes (for example, a ﬁrmware upgrade, hardware addition or removal, or hardware repair) can have guests migrated off, the work done, and guests migrated back without noticeable impact on users or remote connections.
Live migration is made possible because of the well-deﬁned interfaces between guests and VMMs and the limited state the VMM maintains for the guest.
The source VMM establishes a connection with the target VMM and conﬁrms that it is allowed to send a guest.
The target creates a new guest by creating a new VCPU, new nested page table, and other state storage.
The source sends all read-only memory pages to the target.
The source sends all read-write pages to the target, marking them as clean.
The source repeats step 4, as during that step some pages were probably modiﬁed by the guest and are now dirty.
These pages need to be sent again and marked again as clean.
Once the target acknowledges that the guest is running, the source terminates the guest.
We conclude this discussion with a few interesting details and limitations concerning live migration.
First, for network connections to continue uninterrupted, the network infrastructure needs to understand that a MAC.
Figure 16.8 Live migration of a guest between two servers.
Before virtualization, this did not happen, as the MAC address was tied to physical hardware.
With virtualization, the MAC must be movable for existing networking connections to continue without resetting.
Modern network switches understand this and route trafﬁc wherever the MAC address is, even accommodating a move.
A limitation of live migration is that no disk state is transferred.
One reason live migration is possible is that most of the guest’s state is maintained within the guest—for example, open ﬁle tables, system-call state, kernel state, and so on.
Because disk I/O is so much slower than memory access, and used disk space is usually much larger than used memory, disks associated with the guest cannot be moved as part of a live migration.
Rather, the disk must be remote to the guest, accessed over the network.
In that case, disk access state is maintained within the guest, and network connections are all that matter to the VMM.
The network connections are maintained during the migration, so remote disk access continues.
Typically, NFS, CIFS, or iSCSI is used to store virtual machine images and any other storage a guest needs access to.
Those network-based storage accesses simply continue when the network connections are continued once the guest has been migrated.
Live migration enables entirely new ways of managing data centers.
For example, virtualization management tools can monitor all the VMMs in an environment and automatically balance resource use by moving guests between the VMMs.
They can also optimize the use of electricity and cooling by migrating all guests off selected servers if other servers can handle the load and powering down the selected servers entirely.
If the load increases, these tools can power up the servers and migrate guests back to them.
Despite the advantages of virtual machines, they received little attention for a number of years after they were ﬁrst developed.
Today, however, virtual machines are coming into fashion as a means of solving system compatibility problems.
In this section, we explore two popular contemporary virtual machines: the VMware Workstation and the Java virtual machine.
As you will see, these virtual machines can typically run on top of operating systems of any of the design types discussed in earlier chapters.
Thus, operating-system design methods—simple layers, microkernels, modules, and virtual machines —are not mutually exclusive.
VMware Workstation is a popular commercial application that abstracts Intel X86 and compatible hardware into isolated virtual machines.
VMware Workstation is a prime example of a Type 2 hypervisor.
It runs as an application on a host operating system such as Windows or Linux and allows this host system to run several different guest operating systems concurrently as independent virtual machines.
The architecture of such a system is shown in Figure 16.9
In this scenario, Linux is running as the host operating system, and FreeBSD, Windows NT, and.
At the heart of VMware is the virtualization layer, which abstracts the physical hardware into isolated virtual machines running as guest operating systems.
Each virtual machine has its own virtual CPU, memory, disk drives, network interfaces, and so forth.
The physical disk that the guest owns and manages is really just a ﬁle within the ﬁle system of the host operating system.
To create an identical guest, we can simply copy the ﬁle.
Copying the ﬁle to another location protects the guest against a disaster at the original site.
Moving the ﬁle to another location moves the guest system.
These scenarios show how virtualization can improve the efﬁciency of system administration as well as system resource use.
In addition to a language speciﬁcation and a large API library, Java provides a speciﬁcation for a Java virtual machine, or JVM.
Java objects are speciﬁed with the class construct; a Java program consists of one or more classes.
The class loader loads the compiled .class ﬁles from both the Java program and the Java API for execution by the Java interpreter.
After a class is loaded, the veriﬁer checks that the .class ﬁle is valid Java bytecode and that it does not overﬂow or underﬂow the stack.
If the class passes veriﬁcation, it is run by the Java interpreter.
The JVM also automatically manages memory by performing garbage collection—the practice of reclaiming memory from objects no longer in use and returning it to the system.
Much research focuses on garbage collection algorithms for increasing the performance of Java programs in the virtual machine.
The JVM may be implemented in software on top of a host operating system, such as Windows, Linux, or Mac OS X, or as part of a Web browser.
Alternatively, the JVM may be implemented in hardware on a chip speciﬁcally designed to run Java programs.
If the JVM is implemented in software, the Java interpreter interprets the bytecode operations one at a time.
A faster software technique is to use a just-in-time (JIT) compiler.
Here, the ﬁrst time a Java method is invoked, the bytecodes for the method are turned into native machine language for the host system.
These operations are then cached so that subsequent invocations of a method are performed using the native machine instructions, and the bytecode operations need not be interpreted all over again.
Here, a special Java chip executes the Java bytecode operations as native code, thus bypassing the need for either a software interpreter or a just-in-time compiler.
Virtualization is a method of providing a guest with a duplicate of a system’s underlying hardware.
Multiple guests can run on a given system, each believing it is the native operating system in full control of the system.
Virtualization started as a method to allow IBM to segregate users and provide them with their own execution environments on IBM mainframes.
Since then, with improvements in system and CPU performance and through innovative software techniques, virtualization has become a common feature in data centers and even on personal computers.
Because of the popularity of virtualization, CPU designers have added features to support virtualization.
This snowball effect is likely to continue, with virtualization and its hardware support increasing over time.
Type 0 virtualization is implemented in the hardware and requires modiﬁcations to the operating system to ensure proper operation.
In Type 1 virtualization, a host virtual machine monitor (VMM) provides the environment and features needed to create, run, and destroy guest virtual machines.
Each guest includes all of the software typically associated with a full native system, including the operating system, device drivers, applications, user accounts, and so on.
Type 2 hypervisors are simply applications that run on other operating systems, which do not know that virtualization is taking place.
These hypervisors do not enjoy hardware or host support so must perform all virtualization activities in the context of a process.
Other facilities that are similar to virtualization but do not meet the full deﬁnition of replicating hardware exactly are also common.
The language speciﬁes a containing application in which programs run, and this application provides services to the programs.
Emulation is used when a host system has one architecture and the guest was compiled for a different architecture.
Every instruction the guest wants to execute must be translated from its instruction set to that of the native hardware.
Although this method involves some perform penalty, it is balanced by the usefulness of being able to run old programs on newer, incompatible hardware or run games designed for old consoles on modern hardware.
Implementing virtualization is challenging, especially when hardware support is minimal.
Some hardware support must exist for virtualization, but the more features provided by the system, the easier virtualization is to implement and the better the performance of the guests.
VMMs take advantage of whatever hardware support is available when optimizing CPU scheduling, memory management, and I/O modules to provide guests with optimum resource use while protecting the VMM from the guests and the guests from one another.
The original IBM VM system was described in [Meyer and Seawright (1970)]
Virtualization has been an active research area for many years.
Oracle Solaris containers are similar to BSD jails, as described in [Poul-henning Kamp (2000)]
Some protection challenges and attacks in virtual environments are discussed in [Wojtczuk and Ruthkowska (2011)]
VMware shipped the vMotion live migration feature as part of VMware vCenter, as described in VMware VirtualCenter User’s Manual Version 1.0 (http://www.vmware.com/pdf/VirtualCenter Users Manual.pdf)
Research showing that, without interrupt remapping, malicious guests can generate interrupts that can be used to gain control of the host system is discussed in [Wojtczuk and Ruthkowska (2011)]
A distributed system is a collection of processors that do not share memory or a clock.
The nodes communicate with one another through various networks, such as high-speed buses and the Internet.
In this chapter, we discuss the general structure of distributed systems and the networks that interconnect them.
We also contrast the main differences in operating-system design between these systems and centralized systems.
To provide a high-level overview of distributed systems and the networks that interconnect them.
A distributed system is a collection of loosely coupled nodes interconnected by a communication network.
From the point of view of a speciﬁc node in a distributed system, the rest of the nodes and their respective resources are remote, whereas its own resources are local.
The nodes in a distributed system may vary in size and function.
They may include small microprocessors, personal computers, and large general-purpose computer systems.
These processors are referred to by a number of names, such as processors, sites, machines, and hosts, depending on the context in which they are mentioned.
We mainly use site to indicate the location of a machine and node to refer to a speciﬁc system at a site.
Generally, one node at one site, the server, has a resource that another node at another site, the client (or user), would like to use.
A general structure of a distributed system is shown in Figure 17.1
There are four major reasons for building distributed systems: resource sharing, computation speedup, reliability, and communication.
If a number of different sites (with different capabilities) are connected to one another, then a user at one site may be able to use the resources available at another.
For example, a user at site A may be using a laser printer located at site B.
Meanwhile, a user at B may access a ﬁle that resides at A.
In general, resource sharing in a distributed system provides mechanisms for sharing ﬁles at remote sites, processing information in a distributed database, printing ﬁles at remote sites, using remote specialized hardware devices (such as a supercomputer), and performing other operations.
If a particular computation can be partitioned into subcomputations that can run concurrently, then a distributed system allows us to distribute the subcomputations among the various sites.
The subcomputations can be run concurrently and thus provide computation speedup.
In addition, if a particular site is currently overloaded with jobs, some of them can be moved to other, lightly loaded sites.
This movement of jobs is called load sharing or job migration.
Automated load sharing, in which the distributed operating system automatically moves jobs, is not yet common in commercial systems.
If one site fails in a distributed system, the remaining sites can continue operating, giving the system better reliability.
If the system is composed of multiple large autonomous installations (that is, general-purpose computers), the failure of one of them should not affect the rest.
If, however, the system is composed of small machines, each of which is responsible for some crucial system function (such as the web server or the ﬁle system), then a single failure may halt the operation of the whole system.
The failure of a site must be detected by the system, and appropriate action may be needed to recover from the failure.
The system must no longer use the services of that site.
In addition, if the function of the failed site can be taken over by another site, the system must ensure that the transfer of function occurs correctly.
Finally, when the failed site recovers or is repaired, mechanisms must be available to integrate it back into the system smoothly.
When several sites are connected to one another by a communication network, users at the various sites have the opportunity to exchange information.
At a low level, messages are passed between systems, much as messages are passed between processes in the single-computer message system discussed in Section 3.4
Given message passing, all the higher-level functionality found in standalone systems can be expanded to encompass the distributed system.
Such functions include ﬁle transfer, login, mail, and remote procedure calls (RPCs)
The advantage of a distributed system is that these functions can be carried out over great distances.
Two people at geographically distant sites can collaborate on a project, for example.
By transferring the ﬁles of the project, logging in to each other’s remote systems to run programs, and exchanging mail to coordinate the work, users minimize the limitations inherent in longdistance work.
We wrote this book by collaborating in such a manner.
The advantages of distributed systems have resulted in an industry-wide trend toward downsizing.
Many companies are replacing their mainframes with networks of workstations or personal computers.
Companies get a bigger bang for the buck (that is, better functionality for the cost), more ﬂexibility in locating resources and expanding facilities, better user interfaces, and easier maintenance.
In this section, we describe the two general categories of network-oriented operating systems: network operating systems and distributed operating systems.
Network operating systems are simpler to implement but generally more difﬁcult for users to access and utilize than are distributed operating systems, which provide more features.
A network operating system provides an environment in which users, who are aware of the multiplicity of machines, can access remote resources by either logging in to the appropriate remote machine or transferring data from the remote machine to their own machines.
Currently, all general-purpose operating systems, and even embedded operating systems such as Android and iOS, are network operating systems.
An important function of a network operating system is to allow users to log in remotely.
To illustrate, let’s suppose that a user at Westminster College wishes to compute on cs.yale.edu, a computer that is located at Yale University.
To do so, the user must have a valid account on that machine.
This command results in the formation of an encrypted socket connection between the local machine at Westminster College and the “cs.yale.edu” computer.
After this connection has been established, the networking software creates a transparent, bidirectional link so that all characters entered by the user are sent to a process on “cs.yale.edu” and all the output from that process is sent back to the user.
The process on the remote machine asks the user for a login name and a password.
Once the correct information has been received, the process acts as a proxy for the user, who can compute on the remote machine just as any local user can.
Another major function of a network operating system is to provide a mechanism for remote ﬁle transfer from one machine to another.
In such an environment, each computer maintains its own local ﬁle system.
If a user at one site (say, cs.uvm.edu) wants to access a ﬁle located on another computer (say, cs.yale.edu), then the ﬁle must be copied explicitly from the computer at Yale to the computer at the University of Vermont.
The Internet provides a mechanism for such a transfer with the ﬁle transfer protocol (FTP) program and the more private secure ﬁle transfer protocol (SFTP) program.
The program then asks the user for a login name and a password.
Once the correct information has been received, the user must connect to the subdirectory where the ﬁle Server.java resides and then copy the ﬁle by executing.
In this scheme, the ﬁle location is not transparent to the user; users must know exactly where each ﬁle is.
Moreover, there is no real ﬁle sharing, because a user can only copy a ﬁle from one site to another.
Thus, several copies of the same ﬁle may exist, resulting in a waste of space.
In addition, if these copies are modiﬁed, the various copies will be inconsistent.
This remote copying is accomplished through the “anonymous FTP” method, which works as follows.
The ﬁle to be copied (that is, Server.java) must be placed in a special subdirectory (say, ftp) with the protection set to allow the public to read the ﬁle.
A user who wishes to copy the ﬁle uses the ftp command.
When the user is asked for the login name, the user supplies the name “anonymous” and an arbitrary password.
Once anonymous login is accomplished, the system must ensure that this partially authorized user does not access inappropriate ﬁles.
Generally, the user is allowed to access only those ﬁles that are in the directory tree of user “anonymous.” Any ﬁles placed here are accessible to any anonymous users, subject to the usual ﬁle-protection scheme used on that machine.
Anonymous users, however, cannot access ﬁles outside of this directory tree.
Implementation of the FTP mechanism is similar to ssh implementation.
A daemon on the remote site watches for requests to connect to the system’s FTP port.
Login authentication is accomplished, and the user is allowed to execute transfer commands remotely.
Unlike the ssh daemon, which executes any command for the user, the FTP daemon responds only to a predeﬁned set of ﬁle-related commands.
There are also various commands to change transfer modes (for binary or ASCII ﬁles) and to determine connection status.
An important point about ssh and FTP is that they require the user to change paradigms.
With ssh, the user must know appropriate commands on the remote system.
For instance, a user on a Windows machine who connects remotely to a UNIX machine must switch to UNIX commands for the duration of the ssh session.
In networking, a session is a complete round of communication, frequently beginning with a login to authenticate and ending with a logoff to terminate the communication.
Obviously, users would ﬁnd it more convenient not to be required to use a different set of commands.
In a distributed operating system, users access remote resources in the same way they access local resources.
Data and process migration from one site to another is under the control of the distributed operating system.
Suppose a user on site A wants to access data (such as a ﬁle) that reside at site B.
The system can transfer the data by one of two basic methods.
From that point on, all access to the ﬁle is local.
When the user no longer needs access to the ﬁle, a copy of the ﬁle (if it has been modiﬁed) is sent back to site B.
Even if only a modest change has been made to a large ﬁle, all the data must be transferred.
This mechanism can be thought of as an automated FTP system.
This approach was used in the Andrew ﬁle system, but it was found to be too inefﬁcient.
The other approach is to transfer to site A only those portions of the ﬁle that are actually necessary for the immediate task.
If another portion is required later, another transfer will take place.
When the user no longer wants to access the ﬁle, any part of it that has been modiﬁed must be sent back to site B.
The Sun Microsystems network ﬁle system (NFS) protocol uses this method (Section 12.8), as do newer versions of Andrew.
The Microsoft SMB protocol (also known as Common Internet File System, or CIFS) also allows ﬁle sharing over a network.
Clearly, if only a small part of a large ﬁle is being accessed, the latter approach is preferable.
If signiﬁcant portions of the ﬁle are being accessed, however, it is more efﬁcient to copy the entire ﬁle.
Whichever method is used, data migration includes more than the mere transfer of data from one site to another.
The system must also perform various data translations if the two sites involved are not directly compatible (for instance, if they use different character-code representations or represent integers with a different number or order of bits)
In some circumstances, we may want to transfer the computation, rather than the data, across the system; this process is called computation migration.
For example, consider a job that needs to access various large ﬁles that reside at different sites, to obtain a summary of those ﬁles.
It would be more efﬁcient to access the ﬁles at the sites where they reside and return the desired results to the site that initiated the computation.
Generally, if the time to transfer the data is longer than the time to execute the remote command, the remote command should be used.
Such a computation can be carried out in different ways.
Suppose that process P wants to access a ﬁle at site A.
Access to the ﬁle is carried out at site A and could be initiated by an RPC.
An RPC uses network protocols to execute a routine on a remote system (Section 3.6.2)
The procedure executes appropriately and then returns the results to P.
Alternatively, process P can send a message to site A.
The operating system at site A then creates a new process Q whose function is to carry out the designated task.
When process Q completes its execution, it sends the needed result back to P via the message system.
In this scheme, process P may execute concurrently with process Q.
In fact, it may have several processes running concurrently on several sites.
Either method could be used to access several ﬁles residing at various sites.
One RPC might result in the invocation of another RPC or even in the transfer of messages to another site.
Similarly, process Q could, during the course of its execution, send a message to another site, which in turn would create another process.
This process might either send a message back to Q or repeat the cycle.
When a process is submitted for execution, it is not always executed at the site at which it is initiated.
The entire process, or parts of it, may be executed at different sites.
The processes (or subprocesses) may be distributed across the network to even the workload.
If a single process can be divided into a number of subprocesses that can run concurrently on different sites, then the total process turnaround time can be reduced.
The process may have characteristics that make it more suitable for execution on some specialized processor (such as matrix inversion on an array processor) rather than on a microprocessor.
The process may require software that is available at only a particular site, and either the software cannot be moved, or it is less expensive to move the process.
Just as in computation migration, if the data being used in the computation are numerous, it may be more efﬁcient to have a process run remotely than to transfer all the data.
We use two complementary techniques to move processes in a computer network.
In the ﬁrst, the system can attempt to hide the fact that the process has migrated from the client.
The client then need not code her program explicitly to accomplish the migration.
This method is usually employed for achieving load balancing and computation speedup among homogeneous systems, as they do not need user input to help them execute programs remotely.
The other approach is to allow (or require) the user to specify explicitly how the process should migrate.
This method is usually employed when the process must be moved to satisfy a hardware or software preference.
You have probably realized that the World Wide Web has many aspects of a distributed computing environment.
Certainly it provides data migration (between a web server and a web client)
For instance, a web client could trigger a database operation on a web server.
Finally, with Java, Javascript, and similar languages, it provides a form of process migration: Java applets and Javascript scripts are sent from the server to the client, where they are executed.
A network operating system provides most of these features, but a distributed operating system makes them seamless and easily accessible.
The result is a powerful and easy-to-use facility —one of the reasons for the huge growth of the World Wide Web.
There are basically two types of networks: local-area networks (LAN) and wide-area networks (WAN)
The main difference between the two is the way in which they are geographically distributed.
These differences imply major variations in the speed and reliability of the communications networks, and they are reﬂected in the distributed operating-system design.
Local-area networks emerged in the early 1970s as a substitute for large mainframe computer systems.
For many enterprises, it is more economical to have a number of small computers, each with its own self-contained applications, than to have a single large system.
Because each small computer is likely to need a full complement of peripheral devices (such as disks and printers), and because some form of data sharing is likely to occur in a single enterprise, it was a natural step to connect these small systems into a network.
LANs, as mentioned, are usually designed to cover a small geographical area, and they are generally used in an ofﬁce environment.
All the sites in such systems are close to one another, so the communication links tend to have a higher speed and lower error rate than do their counterparts in wide-area networks.
The most common links in a local-area network are twisted-pair and ﬁberoptic cabling.
In a star network, the nodes connect to one or more switches, and the switches connect to each other, enabling any two nodes to communicate.
Ten megabits per second is the speed of 10BaseT Ethernet.
The use of opticalﬁber cabling is growing; it provides higher communication rates over longer distances than are possible with copper.
A typical LAN may consist of a number of different computers (from mainframes to laptops or other mobile devices), various shared peripheral devices (such as laser printers and storage arrays), and one or more routers (specialized network communication processors) that provide access to other networks (Figure 17.2)
An Ethernet network has no central controller, because it is a multiaccess bus, so new hosts can be added easily to the network.
The Ethernet protocol is deﬁned by the IEEE 802.3 standard.
The wireless spectrum is increasingly used for designing local-area networks.
Wireless (or WiFi) technology allows us to construct a network using only a wireless router to transmit signals between hosts.
Each host has a wireless transmitter and receiver that it uses to participate in the network.
Whereas Ethernet systems often run at 1 gigabit per second, WiFi networks typically run considerably slower.
The recent 802.11n standard provides theoretically much higher data rates.
Data rates of wireless networks are heavily inﬂuenced by the distance between the wireless router and the host, as well as interference in the wireless spectrum.
On the positive side, wireless networks often have a physical advantage over wired Ethernet networks because they require no cabling to connect communicating hosts.
As a result, wireless networks are popular in homes and businesses, as well as public areas such as libraries, Internet cafes, sports arenas, and even buses and airplanes.
Wide-area networks emerged in the late 1960s, mainly as an academic research project to provide efﬁcient communication among sites, allowing hardware and software to be shared conveniently and economically by a wide community of users.
The ﬁrst WAN to be designed and developed was the Arpanet.
Begun in 1968, the Arpanet has grown from a four-site experimental network to a worldwide network of networks, the Internet, comprising millions of computer systems.
Because the sites in a WAN are physically distributed over a large geographical area, the communication links are, by default, relatively slow and unreliable.
Typical links are telephone lines, leased (dedicated data) lines, optical cable, microwave links, radio waves, and satellite channels.
These communication links are controlled by special communication processors (Figure 17.3), commonly known as gateway routers or simply routers, that are responsible for deﬁning the interface through which the sites communicate over the network, as well as for transferring information among the various sites.
For example, the Internet WAN enables hosts at geographically separated sites to communicate with one another.
The host computers typically differ from one another in speed, CPU type, operating system, and so on.
Hosts are generally on LANs, which are, in turn, connected to the Internet via regional networks.
The regional networks, such as NSFnet in the northeast United States, are interlinked with routers (Section 17.4.2) to form the worldwide.
Residences can connect to the Internet by either telephone, cable, or specialized Internet service providers that install routers to connect the residences to central services.
A company might create its own private WAN for increased security, performance, or reliability.
As mentioned, WANs are generally slower than LANs, although backbone WAN connections that link major cities may have transfer rates of over 40 gigabits per second.
Frequently, WANs and LANs interconnect, and it is difﬁcult to tell where one ends and the other starts.
Cell phones are used for both voice and data communications.
Cell phones in a given area connect via radio waves to a cell tower that contains receivers and transmitters.
This part of the network is similar to a LAN except that the cell phones do not communicate with each other (unless two people talking or exchanging data happen to be connected to the same tower)
Rather, the towers are connected to other towers and to hubs that connect the tower communications to land lines or other communication mediums and route the packets toward their destinations.
Once the appropriate tower receives the packets, it uses its transmitters to send them to the correct recipient.
Now that we have discussed the physical aspects of networking, we turn to the internal workings.
The designer of a communication network must address ﬁve basic issues:
In the following sections, we elaborate on each of these issues.
The ﬁrst issue in network communication involves the naming of the systems in the network.
For a process at site A to exchange information with a process at site B, each must be able to specify the other.
Within a computer system, each process has a process identiﬁer, and messages may be addressed with the process identiﬁer.
Because networked systems share no memory, however, a host within the system initially has no knowledge about the processes on other hosts.
A host name is usually an alphanumeric identiﬁer, rather than a number, to make it easier for users to specify.
For instance, site A might have hosts named homer, marge, bart, and lisa.
Names are convenient for humans to use, but computers prefer numbers for speed and simplicity.
For this reason, there must be a mechanism to resolve the host name into a host-id that describes the destination system to the networking hardware.
This mechanism is similar to the name-to-address binding that occurs during program compilation, linking, loading, and execution (Chapter 8)
First, every host may have a data ﬁle containing the names and addresses of all the other hosts reachable on the network (similar to binding at compile time)
The problem with this model is that adding or removing a host from the network requires updating the data ﬁles on all the hosts.
The alternative is to distribute the information among systems on the network.
The network must then use a protocol to distribute and retrieve the information.
The ﬁrst method was the one originally used on the Internet.
The second method, the domain-name system (DNS), is the one now in use.
Hosts on the Internet are logically addressed with multipart names known as IP addresses.
The parts of an IP address progress from the most.
Other top-level domains include com for commercial sites and org for organizations, as well as a domain for each country connected to the network, for systems speciﬁed by country rather than organization type.
Generally, the system resolves addresses by examining the host-name components in reverse order.
Each component has a name server—simply a process on a system—that accepts a name and returns the address of the name server responsible for that name.
As the ﬁnal step, the name server for the host in question is contacted, and a host-id is returned.
The system library or the kernel on system A issues a request to the name server for the edu domain, asking for the address of the name server for brown.edu.
The name server for the edu domain must be at a known address, so that it can be queried.
The edu name server returns the address of the host on which the brown.edu name server resides.
System A then queries the name server at this address and asks about cs.brown.edu.
This protocol may seem inefﬁcient, but individual hosts cache the IP addresses they have already resolved to speed the process.
Of course, the contents of these caches must be refreshed over time in case the name server is moved or its address changes.
In fact, the protocol is so important that it has been optimized many times and has had many safeguards added.
Consider what would happen if the primary edu name server crashed.
It is possible that no edu hosts would be able to have their addresses resolved, making them all unreachable! The solution is to use secondary, backup name servers that duplicate the contents of the primary servers.
Before the domain-name service was introduced, all hosts on the Internet needed to have copies of a ﬁle that contained the names and addresses of each host on the network.
All changes to this ﬁle had to be registered at one site (host SRI-NIC), and periodically all hosts had to copy the updated ﬁle from SRI-NIC to be able to contact new systems or ﬁnd hosts whose addresses had changed.
Under the domain-name service, each name-server site is responsible for updating the host information for that domain.
For instance, any host changes at Brown University are the responsibility of the name server for brown.edu and need not be reported anywhere else.
Domains may contain autonomous subdomains to further distribute the responsibility for host-name and host-id changes.
Java provides the necessary API to design a program that maps IP names to IP addresses.
An InetAddress is a Java class representing an IP name or address.
The static method getByName() belonging to the InetAddress class is passed a string representation of an IP name, and it returns the corresponding InetAddress.
The program then invokes the getHostAddress() method, which internally uses DNS to look up the IP address of the designated host.
The kernel on the destination host is then responsible for transferring the message to the process named by the identiﬁer.
This exchange is by no means trivial; it is described in Section 17.4.4
When a process at site A wants to communicate with a process at site B, how is the message sent? If there is only one physical path from A to B, the message must be sent through that path.
However, if there are multiple physical paths from A to B, then several routing options exist.
Each site has a routing table indicating the alternative paths that can be used to send a message to other sites.
The table may include information about the speed and cost of the various communication paths, and it may be updated as necessary, either manually or via programs that exchange routing information.
The three most common routing schemes are ﬁxed routing, virtual routing, and dynamic routing.
A path from A to B is speciﬁed in advance and does not change unless a hardware failure disables it.
Usually, the shortest path is chosen, so that communication costs are minimized.
A path from A to B is ﬁxed for the duration of one session.
Different sessions involving messages from A to B may use different paths.
A session could be as short as a ﬁle transfer or as long as a remote-login period.
The path used to send a message from site A to site B is chosen only when the message is sent.
Because the decision is made dynamically, separate messages may be assigned different paths.
Site A will make a decision to send the message to site C.
C, in turn, will decide to send it to site D, and so on.
Usually, a site sends a message to another site on whatever link is the least used at that particular time.
Fixed routing cannot adapt to link failures or load changes.
In other words, if a path has been established between A and B, the messages must be sent along this path, even if the path is down or is used more heavily than another possible path.
We can partially remedy this problem by using virtual routing and can avoid it completely by using dynamic routing.
Fixed routing and virtual routing ensure that messages from A to B will be delivered in the order in which they were sent.
We can remedy this problem by appending a sequence number to each message.
Dynamic routing is the most complicated to set up and run; however, it is the best way to manage routing in complicated environments.
Within a site, the hosts may just need to know how to reach the system that connects the local network to other networks (such as company-wide networks or the Internet)
Each individual host has a static route to the gateway, but the gateway itself uses dynamic routing to reach any host on the rest of the network.
A router is the communications processor within the computer network responsible for routing messages.
A router can be a host computer with routing software or a special-purpose device.
Either way, a router must have at least two network connections, or else it would have nowhere to route messages.
A router decides whether any given message needs to be passed from the network on which it is received to any other network connected to the router.
It makes this determination by examining the destination Internet address of the message.
The router checks its tables to determine the location of the destination host, or at least of the network to which it will send the message toward the destination host.
In the case of static routing, this table is changed only by manual update (a new ﬁle is loaded onto the router)
With dynamic routing, a routing protocol is used between routers to inform them of network changes and to allow them to update their routing tables automatically.
Gateways and routers have typically been dedicated hardware devices that run code out of ﬁrmware.
More recently, routing has been managed by software that directs multiple network devices more intelligently than a single router could.
For example, the OpenFlow standard allows developers to introduce new networking efﬁciencies and features by decoupling data-routing decisions from the underlying networking devices.
To simplify the system design, we commonly implement communication with ﬁxed-length messages called packets, frames, or datagrams.
A communication implemented in one packet can be sent to its destination in a connectionless message.
A connectionless message can be unreliable, in which case the sender has no guarantee that, and cannot tell whether, the packet reached its destination.
Usually, in this case, an acknowledgement packet is returned from the destination indicating that the original packet arrived.
Of course, the return packet could be lost along the way.
If a message is too long to ﬁt within one packet, or if the packets need to ﬂow back and forth between the two communicators, a connection is established to allow the reliable exchange of multiple packets.
Once messages are able to reach their destinations, processes can institute communications sessions to exchange information.
Pairs of processes that want to communicate over the network can be connected in a number of ways.
The three most common schemes are circuit switching, message switching, and packet switching.
If two processes want to communicate, a permanent physical link is established between them.
This link is allocated for the duration of the communication session, and no other process can use that link during this period (even if the two processes are not actively communicating for a while)
This scheme is similar to that used in the telephone system.
Once a communication line has been opened between two parties (that is, party A calls party B), no one else can use this circuit until the communication is terminated explicitly (for example, when the parties hang up)
If two processes want to communicate, a temporary link is established for the duration of one message transfer.
Physical links are allocated dynamically among correspondents as needed and are allocated for only short periods.
Each message is a block of data with system information—such as the source, the destination, and errorcorrection codes (ECC)—that allows the communication network to deliver the message to the destination correctly.
Each letter is a message that contains both the destination address and source (return) address.
Many messages (from different users) can be shipped over the same link.
One logical message may have to be divided into a number of packets.
Each packet may be sent to its destination separately, and each therefore must include a source and a destination address with its data.
Furthermore, the various packets may take different paths through the network.
The packets must be reassembled into messages as they arrive.
Note that it is not harmful for data to be broken into packets, possibly routed separately, and reassembled at the destination.
Circuit switching requires substantial setup time and may waste network bandwidth, but it incurs less overhead for shipping each message.
Conversely, message and packet switching require less setup time but incur more overhead per message.
Also, in packet switching, each message must be divided into packets and later reassembled.
Packet switching is the method most commonly used on data networks because it makes the best use of network bandwidth.
When we are designing a communication network, we must deal with the inherent complexity of coordinating asynchronous operations communicating in a potentially slow and error-prone environment.
In addition, the systems on the network must agree on a protocol or a set of protocols for determining host names, locating hosts on the network, establishing connections, and so on.
We can simplify the design problem (and related implementation) by partitioning the problem into multiple layers.
Each layer on one system communicates with the equivalent layer on other systems.
Typically, each layer has its own protocols, and communication takes place between peer layers using a speciﬁc protocol.
For instance, Figure 17.5 shows the logical communications between two computers, with the three lowest-level layers implemented in hardware.
The International Standards Organization created the OSI model for describing the various layers of networking.
While these layers are not implemented in practice, they are useful for understanding how networking logically works, and we describe them below:
Figure 17.5 Two computers communicating via the OSI network model.
The physical layer is responsible for handling both the mechanical and the electrical details of the physical transmission of a bit stream.
This layer is implemented in the hardware of the networking device.
The data-link layer is responsible for handling frames, or ﬁxed-length parts of packets, including any error detection and recovery that occurs in the physical layer.
The network layer is responsible for breaking messages into packets, providing connections between logical addresses, and routing packets in the communication network, including handling the addresses of outgoing packets, decoding the addresses of incoming packets, and maintaining routing information for proper response to changing load levels.
The transport layer is responsible for transfer of messages between nodes, including partitioning messages into packets, maintaining packet order, and controlling ﬂow to avoid congestion.
The session layer is responsible for implementing sessions, or process-to-process communication protocols.
The presentation layer is responsible for resolving the differences in formats among the various sites in the network, including character conversions and half duplex–full duplex modes (character echoing)
The application layer is responsible for interacting directly with users.
This layer deals with ﬁle transfer, remote-login protocols, and electronic mail, as well as with schemas for distributed databases.
Figure 17.6 summarizes the OSI protocol stack—a set of cooperating protocols—showing the physical ﬂow of data.
As mentioned, logically each layer of a protocol stack communicates with the equivalent layer on other systems.
But physically, a message starts at or above the application layer and is passed through each lower level in turn.
Each layer may modify the message and include message-header data for the equivalent layer on the receiving side.
Ultimately, the message reaches the data-network layer and is transferred as one or more packets (Figure 17.7)
The data-link layer of the target system receives these data, and the message is moved up through the protocol stack.
It is analyzed, modiﬁed, and stripped of headers as it progresses.
It ﬁnally reaches the application layer for use by the receiving process.
The OSI model formalizes some of the earlier work done in network protocols but was developed in the late 1970s and is currently not in widespread use.
Perhaps the most widely adopted protocol stack is the TCP/IP model, which has been adopted by virtually all Internet sites.
The TCP/IP protocol stack has fewer layers than the OSI model.
The relationship between the OSI and TCP/IP models is shown in Figure 17.8
The Internet protocol (IP) is responsible for routing IP datagrams through the Internet.
The TCP/IP model does not formally identify a link or physical layer, allowing TCP/IP trafﬁc to run across any physical network.
In Section 17.6, we consider the TCP/IP model running over an Ethernet network.
Security should be a concern in the design and implementation of any modern communication protocol.
Both strong authentication and encryption are needed for secure communication.
Strong authentication ensures that the sender and receiver of a communication are who or what they are.
Weak authentication and clear-text communication are still very common, however, for a variety of reasons.
When most of the common protocols were designed, security was frequently less important than performance, simplicity, and efﬁciency.
Strong authentication requires a multistep handshake protocol or authentication devices, adding complexity to a protocol.
Modern CPUs can efﬁciently perform encryption, frequently including cryptographic acceleration instructions, so system performance is not compromised.
Long-distance communication can be made secure by authenticating the endpoints and encrypting the stream of packets in a virtual private network, as discussed in Section 15.4.2
We now return to the name-resolution issue raised in Section 17.4.1 and examine its operation with respect to the TCP/IP protocol stack on the Internet.
Then we consider the processing needed to transfer a packet between hosts on different Ethernet networks.
We base our description on the IPV4 protocols, which are the type most commonly used today.
In a TCP/IP network, every host has a name and an associated IP address (or host-id)
Both of these strings must be unique; and so that the name space can be managed, they are segmented.
The name is hierarchical (as explained in Section 17.4.1), describing the host name and then the organization with which the host is associated.
The host-id is split into a network number and a host number.
The proportion of the split varies, depending on the size of the network.
Once the Internet administrators assign a network number, the site with that number is free to assign host-ids.
The sending system checks its routing tables to locate a router to send the frame on its way.
The routers use the network part of the host-id to transfer the packet from its source network to the destination network.
The packet may be a complete message, or it may just be a component of a message, with more packets needed before the message can be reassembled and passed to the TCP/UDP layer for transmission to the destination process.
Within a network, how does a packet move from sender (host or router) to receiver? Every Ethernet device has a unique byte number, called the medium access control (MAC) address, assigned to it for addressing.
Two devices on a LAN communicate with each other only with this number.
If a system needs to send data to another system, the networking software generates an address resolution protocol (ARP) packet containing the IP address of the destination system.
This packet is broadcast to all other systems on that Ethernet network.
A broadcast uses a special network address (usually, the maximum address) to signal that all hosts should receive and process the packet.
The broadcast is not re-sent by gateways, so only systems on the local network receive it.
Only the system whose IP address matches the IP address of the ARP request responds and sends back its MAC address to the system that initiated the query.
For efﬁciency, the host caches the IP– MAC address pair in an internal table.
The cache entries are aged, so that an entry is eventually removed from the cache if an access to that system is not required within a given time.
For added performance, ARP entries for heavily used hosts may be pinned in the ARP cache.
Once an Ethernet device has announced its host-id and address, communication can begin.
A process may specify the name of a host with which to communicate.
Networking software takes that name and determines the IP address of the target, using a DNS lookup.
The message is passed from the application layer, through the software layers, and to the hardware layer.
At the hardware layer, the packet (or packets) has the Ethernet address at its start; a trailer indicates the end of the packet and contains a checksum for detection of packet damage (Figure 17.9)
The packet is placed on the network by the Ethernet device.
The data section of the packet may contain some or all of the data of the original message, but it may also contain some of the upper-level headers that compose the message.
In other words, all parts of the original message must be sent from source to destination, and all headers above the 802.3 layer (data-link layer) are included as data in the Ethernet packets.
If the destination is on the same local network as the source, the system can look in its ARP cache, ﬁnd the Ethernet address of the host, and place the packet on the wire.
The destination Ethernet device then sees its address in the packet and reads in the packet, passing it up the protocol stack.
If the destination system is on a network different from that of the source, the source system ﬁnds an appropriate router on its network and sends the packet there.
Routers then pass the packet along the WAN until it reaches its destination network.
The router that connects the destination network checks its ARP cache, ﬁnds the Ethernet number of the destination, and sends the packet to that host.
Through all of these transfers, the data-link-layer header may change as the Ethernet address of the next router in the chain is used, but the other headers of the packet remain the same until the packet is received and processed by the protocol stack and ﬁnally passed to the receiving process by the kernel.
A distributed system may suffer from various types of hardware failure.
The failure of a link, the failure of a site, and the loss of a message are the most common types.
To ensure that the system is robust, we must detect any of these failures, reconﬁgure the system so that computation can continue, and recover when a site or a link is repaired.
In an environment with no shared memory, we are generally unable to differentiate among link failure, site failure, and message loss.
We can usually detect only that one of these failures has occurred.
Once a failure has been detected, appropriate action must be taken.
To detect link and site failure, we use a heartbeat procedure.
Suppose that sites A and B have a direct physical link between them.
At ﬁxed intervals, the sites send each other an I-am-up message.
If site A does not receive this message within a predetermined time period, it can assume that site B has failed, that the link between A and B has failed, or that the message from B has been lost.
It can wait for another time period to receive an I-am-up message from B, or it can send an Are-you-up? message to B.
If time goes by and site A still has not received an I-am-up message, or if site A has sent an Are-you-up? message and has not received a reply, the procedure can be repeated.
Again, the only conclusion that site A can draw safely is that some type of failure has occurred.
Site A can try to differentiate between link failure and site failure by sending an Are-you-up? message to B by another route (if one exists)
If and when B receives this message, it immediately replies positively.
This positive reply tells A that B is up and that the failure is in the direct link between them.
Since we do not know in advance how long it will take the message to travel from A to B and back, we must use a time-out scheme.
At the time A sends the Are-you-up? message, it speciﬁes a time interval during which it is willing to wait for the reply from B.
If A receives the reply message within that time interval, then it can safely conclude that B is up.
If not, however (that is, if a time-out occurs), then A may conclude only that one or more of the following situations has occurred:
The direct link (if one exists) from A to B is down.
Site A cannot, however, determine which of these events has occurred.
Suppose that site A has discovered, through the mechanism just described, that a failure has occurred.
It must then initiate a procedure that will allow the system to reconﬁgure and to continue its normal mode of operation.
If a direct link from A to B has failed, this information must be broadcast to every site in the system, so that the various routing tables can be updated accordingly.
If the system believes that a site has failed (because that site can be reached no longer), then all sites in the system must be notiﬁed, so that they will no longer attempt to use the services of the failed site.
The failure of a site that serves as a central coordinator for some activity (such as deadlock detection) requires the election of a new coordinator.
Similarly, if the failed site is part of a logical ring, then a new logical ring must be constructed.
Note that, if the site has not failed (that is, if it is up but cannot be reached), then we may have the undesirable situation in which two sites serve as the coordinator.
When the network is partitioned, the two coordinators (each for its own partition) may initiate conﬂicting actions.
For example, if the coordinators are responsible for implementing mutual exclusion, we may have a situation in which two processes are executing simultaneously in their critical sections.
When a failed link or site is repaired, it must be integrated into the system gracefully and smoothly.
Suppose that a link between A and B has failed.
When it is repaired, both A and B must be notiﬁed.
We can accomplish this notiﬁcation by continuously repeating the heartbeat procedure described in Section 17.7.1
When it recovers, it must notify all other sites that it is up again.
Site B then may have to receive information from the other sites to update its local tables.
For example, it may need routingtable information, a list of sites that are down, undelivered messages, a transaction log of unexecuted transactions, and mail.
If the site has not failed but simply could not be reached, then it still needs this information.
A distributed system must tolerate a certain level of failure and continue to function normally when faced with various types of failures.
Making a facility fault tolerant starts at the protocol level, as described above, but continues through all aspects of the system.
We use the term fault tolerance in a broad sense.
Communication faults, certain machine failures, storage-device crashes, and decays of storage media should all be tolerated to some extent.
A faulttolerant system should continue to function, perhaps in a degraded form, when faced with such failures.
It should be proportional, however, to the failures that caused it.
A system that grinds to a halt when only one of its components fails is certainly not fault tolerant.
Unfortunately, fault tolerance can be difﬁcult and expensive to implement.
At the network layer, multiple redundant communication paths and network devices such as switches and routers are needed to avoid a communication failure.
A storage failure can cause loss of the operating system, applications, or data.
In addition, RAID systems can ensure continued access to the data even in the event of one or more disk failures (Section 10.7)
A system failure without redundancy can cause an application or an entire facility to stop operation.
The most simple system failure involves a system running only stateless applications.
These applications can be restarted without compromising the operation; so as long as the applications can run on more than one computer (node), operation can continue.
Such a facility is commonly known as a compute cluster because it centers on computation.
In contrast, datacentric systems involve running applications that access and modify shared data.
As a result, datacentric computing facilities are more difﬁcult to make fault tolerant.
For instance, high-availability clusters include two or more computers and a set of shared disks.
Any given application can be stored on the computers or on the shared disk, but the data must be stored on the shared disk.
The running application’s node has exclusive access to the application’s data on disk.
The application is monitored by the cluster software, and if it fails it is automatically restarted.
If it cannot be restarted, or if the entire computer fails, the node’s exclusive access to the application’s data is terminated and is granted to another node in the cluster.
The application loses whatever state information was in the failed system’s memory but can continue based on whatever state it last wrote to the shared disk.
From a user’s point of view, a service was interrupted and then restarted, possibly with some data missing.
Speciﬁc applications may improve on this functionality by implementing lock management along with clustering.
With lock management, the application can run on multiple nodes and can use the same data on shared disks concurrently.
If a node fails, transactions can continue on other nodes, and users notice no interruption of service, as long as the client is able to automatically locate the other nodes in the cluster.
Any noncommitted transactions on the failed node are lost, but again, client applications can be designed to retry noncommitted transactions if they detect a failure of their database node.
Making the multiplicity of processors and storage devices transparent to the users has been a key challenge to many designers.
Ideally, a distributed system should look to its users like a conventional, centralized system.
The user interface of a transparent distributed system should not distinguish between local and remote resources.
That is, users should be able to access remote resources as though these resources were local, and the distributed system should be responsible for locating the resources and for arranging for the appropriate interaction.
It would be convenient to allow users to log into any machine in the system rather than forcing them to use a speciﬁc machine.
A transparent distributed system facilitates user mobility by bringing over the user’s environment (for example, home directory) to wherever he logs in.
Once the authentication is complete, facilities like desktop virtualization allow users to see their desktop sessions at remote facilities.
Still another issue is scalability—the capability of a system to adapt to increased service load.
Systems have bounded resources and can become completely saturated under increased load.
For example, with respect to a ﬁle system, saturation occurs either when a server’s CPU runs at a high utilization rate or when disks’ I/O requests overwhelm the I/O subsystem.
Scalability is a relative property, but it can be measured accurately.
A scalable system reacts more gracefully to increased load than does a nonscalable one.
First, its performance degrades more moderately; and second, its resources reach a saturated state later.
Adding new resources might solve the problem, but it might generate additional indirect load on other resources (for example, adding machines to a distributed system can clog the network and increase service loads)
Even worse, expanding the system can call for expensive design modiﬁcations.
A scalable system should have the potential to grow without these problems.
In a distributed system, the ability to scale up gracefully is of special importance, since expanding the network by adding new machines or interconnecting two networks is commonplace.
In short, a scalable design should withstand high service load, accommodate growth of the user community, and allow simple integration of added resources.
A heavily loaded component can become paralyzed and behave like a faulty component.
In addition, shifting the load from a faulty component to that component’s backup can saturate the latter.
Generally, having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully.
Thus, the multiple resources in a distributed system represent an inherent advantage, giving the system a greater potential for fault tolerance and scalability.
Fault-tolerance and scalability considerations call for a design demonstrating distribution of control and data.
Facilities like the Hadoop distributed ﬁle system were created with this problem in mind.
Hadoop is based on Google’s MapReduce and Google File System projects that created a facility to track every web page on the Internet.
Hadoop is an open-source programming framework that supports the processing of large data sets in distributed computing environments.
Traditional systems with traditional databases cannot scale to the capacity and performance needed by “big data” projects (at least not at reasonable prices)
Examples of big data projects include mining Twitter for information pertinent to a company and sifting ﬁnancial data to look for trends in stock pricing.
With Hadoop and its related tools, thousands of systems can work together to manage a distributed database of petabytes of information.
Although the World Wide Web is the predominant distributed system in use today, it is not the only one.
Another important and popular use of distributed computing is the distributed ﬁle system, or DFS.
In doing so, we use two running examples—OpenAFS, an open-source distributed ﬁle system, and NFS, the most common UNIX-based DFS.
To explain the structure of a DFS, we need to deﬁne the terms service, server, and client in the DFS context.
A service is a software entity running on one or more machines and providing a particular type of function to clients.
A server is the service software running on a single machine.
A client is a process that can invoke a service using a set of operations that form its client interface.
Sometimes a lower-level interface is deﬁned for the actual cross-machine interaction; it is the intermachine interface.
Using this terminology, we say that a ﬁle system provides ﬁle services to clients.
A client interface for a ﬁle service is formed by a set of primitive ﬁle operations, such as create a ﬁle, delete a ﬁle, read from a ﬁle, and write to a ﬁle.
The primary hardware component that a ﬁle server controls is a set of local secondary-storage devices (usually, magnetic disks) on which ﬁles are stored and from which they are retrieved according to the clients’ requests.
A DFS is a ﬁle system whose clients, servers, and storage devices are dispersed among the machines of a distributed system.
Accordingly, service activity has to be carried out across the network.
Instead of a single centralized data repository, the system frequently has multiple and independent storage devices.
As you will see, the concrete conﬁguration and implementation of a DFS may vary from system to system.
In others, a machine can be both a server and a client.
A DFS can be implemented as part of a distributed operating system or, alternatively, by a software layer whose task is to manage the communication between conventional operating systems and ﬁle systems.
The distinctive features of a DFS are the multiplicity and autonomy of clients and servers in the system.
Ideally, though, a DFS should appear to its clients to be a conventional, centralized ﬁle system.
That is, the client interface of a DFS should not distinguish between local and remote ﬁles.
It is up to the DFS to locate the ﬁles and to arrange for the transport of the data.
The most important performance measure of a DFS is the amount of time needed to satisfy service requests.
In conventional systems, this time consists of disk-access time and a small amount of CPU-processing time.
In a DFS, however, a remote access has the additional overhead associated with the distributed structure.
This overhead includes the time to deliver the request to a server, as well as the time to get the response across the network back to the client.
For each direction, in addition to the transfer of the information, there is the CPU overhead of running the communication protocol software.
The performance of a DFS can be viewed as another dimension of the DFS’s transparency.
That is, the performance of an ideal DFS would be comparable to that of a conventional ﬁle system.
The fact that a DFS manages a set of dispersed storage devices is the DFS’s key distinguishing feature.
The overall storage space managed by a DFS is composed of different and remotely located smaller storage spaces.
Usually, these constituent storage spaces correspond to sets of ﬁles.
All ﬁles belonging to the same component unit must reside in the same location.
For instance, users deal with logical data objects represented by ﬁle names, whereas the system manipulates physical blocks of data stored on disk tracks.
Usually, a user refers to a ﬁle by a textual name.
The latter is mapped to a lower-level numerical identiﬁer that in turn is mapped to disk blocks.
This multilevel mapping provides users with an abstraction of a ﬁle that hides the details of how and where on the disk the ﬁle is stored.
In a transparent DFS, a new dimension is added to the abstraction: that of hiding where in the network the ﬁle is located.
In a conventional ﬁle system, the range of the naming mapping is an address within a disk.
In a DFS, this range is expanded to include the speciﬁc machine on whose disk the ﬁle is stored.
Going one step further with the concept of treating ﬁles as abstractions leads to the possibility of ﬁle replication.
Given a ﬁle name, the mapping returns a set of the locations of this ﬁle’s replicas.
In this abstraction, both the existence of multiple copies and their locations are hidden.
We need to differentiate two related notions regarding name mappings in a DFS:
The name of a ﬁle does not reveal any hint of the ﬁle’s physical storage location.
The name of a ﬁle does not need to be changed when the ﬁle’s physical storage location changes.
Both deﬁnitions relate to the level of naming discussed previously, since ﬁles have different names at different levels (that is, user-level textual names and system-level numerical identiﬁers)
Therefore, location independence is a stronger property than is location transparency.
A few aspects can further differentiate location independence and static location transparency:
Divorce of data from location, as exhibited by location independence, provides a better abstraction for ﬁles.
A ﬁle name should denote the ﬁle’s most signiﬁcant attributes, which are its contents rather than its location.
If only static location transparency is supported, the ﬁle name still denotes a speciﬁc, although hidden, set of physical disk blocks.
Static location transparency provides users with a convenient way to share data.
Location independence promotes sharing the storage space itself, as well as the data objects.
When ﬁles can be mobilized, the overall, system-wide storage space looks like a single virtual resource.
A possible beneﬁt is the ability to balance the utilization of storage across the system.
Location independence separates the naming hierarchy from the storagedevices hierarchy and from the intercomputer structure.
By contrast, if static location transparency is used (although names are transparent), we can easily expose the correspondence between component units and machines.
The machines are conﬁgured in a pattern similar to the naming structure.
This conﬁguration may restrict the architecture of the system unnecessarily and conﬂict with other considerations.
A server in charge of a root directory is an example of a structure that is dictated by the naming hierarchy and contradicts decentralization guidelines.
Once the separation of name and location has been completed, clients can access ﬁles residing on remote server systems.
In fact, these clients may be diskless and rely on servers to provide all ﬁles, including the operatingsystem kernel.
Consider the problem of getting the kernel to a diskless workstation.
The diskless workstation has no kernel, so it cannot use the DFS code to retrieve the kernel.
Instead, a special boot protocol, stored in read-only memory (ROM) on the client, is invoked.
It enables networking and retrieves only one special ﬁle (the kernel or boot code) from a ﬁxed location.
Once the kernel is copied over the network and loaded, its DFS makes all the other operating-system ﬁles available.
The advantages of diskless clients are many, including lower cost (because the client machines require no disks) and greater convenience (when an operating-system upgrade occurs, only the server needs to be modiﬁed)
The disadvantages are the added complexity of the boot protocols and the performance loss resulting from the use of a network rather than a local disk.
There are three main approaches to naming schemes in a DFS.
In the simplest approach, a ﬁle is identiﬁed by some combination of its host name and local name, which guarantees a unique system-wide name.
This naming scheme is neither location transparent nor location independent.
The DFS is structured as a collection of isolated component units, each of which is an entire conventional ﬁle system.
Component units remain isolated, although means are provided to refer to remote ﬁles.
The second approach was popularized by Sun’s network ﬁle system, NFS.
Early NFS versions allowed only previously mounted remote directories to be accessed transparently.
The advent of the automount feature allowed mounts to be done on demand based on a table of mount points and ﬁle-structure names.
Components are integrated to support transparent sharing, but this integration is limited and is not uniform, because each machine may attach different remote directories to its tree.
We can achieve total integration of the component ﬁle systems by using the third approach.
Here, a single global name structure spans all the ﬁles in the system.
Ideally, the composed ﬁle-system structure is the same as the structure of a conventional ﬁle system.
In practice, however, the many special ﬁles (for example, UNIX device ﬁles and machine-speciﬁc binary directories) make this goal difﬁcult to attain.
To evaluate naming structures, we look at their administrative complexity.
Because any remote directory can be attached anywhere onto the local directory tree, the resulting hierarchy can be highly unstructured.
If a server becomes unavailable, some arbitrary set of directories on different machines becomes unavailable.
In addition, a separate accreditation mechanism controls which machine is allowed to attach which directory to its tree.
Thus, a user might be able to access a remote directory tree on one client but be denied access on another client.
Implementation of transparent naming requires a provision for the mapping of a ﬁle name to the associated location.
To keep this mapping manageable, we must aggregate sets of ﬁles into component units and provide the mapping on a component-unit basis rather than on a single-ﬁle basis.
UNIX-like systems use the hierarchical directory tree to provide name-to-location mapping and to aggregate ﬁles recursively into directories.
To enhance the availability of the crucial mapping information, we can use replication, local caching, or both.
As we noted, location independence means that the mapping changes over time.
Hence, replicating the mapping makes a simple yet consistent update of this information impossible.
Textual ﬁle names are mapped to lower-level ﬁle identiﬁers that indicate to which component unit the ﬁle belongs.
They can be replicated and cached freely without being invalidated by migration of component.
The inevitable price is the need for a second level of mapping, which maps component units to locations and needs a simple yet consistent update mechanism.
The only aspect that does change is the componentunit location mapping.
A common way to implement low-level identiﬁers is to use structured names.
These names are bit strings that usually have two parts.
The ﬁrst part identiﬁes the component unit to which the ﬁle belongs; the second part identiﬁes the particular ﬁle within the unit.
The invariant of structured names, however, is that individual parts of the name are unique at all times only within the context of the rest of the parts.
We can obtain uniqueness at all times by taking care not to reuse a name that is still in use, by adding sufﬁciently more bits (this method is used in OpenAFS), or by using a timestamp as one part of the name (as was done in Apollo Domain)
Next, let’s consider a user who requests access to a remote ﬁle.
The server storing the ﬁle has been located by the naming scheme, and now the actual data transfer must take place.
One way to achieve this transfer is through a remote-service mechanism, whereby requests for accesses are delivered to the server, the server machine performs the accesses, and their results are forwarded back to the user.
A direct analogy exists between disk-access methods in conventional ﬁle systems and the remote-service method in a DFS: using the remote-service method is analogous to performing a disk access for each access request.
To ensure reasonable performance of a remote-service mechanism, we can use a form of caching.
In conventional ﬁle systems, the rationale for caching is to reduce disk I/O (thereby increasing performance), whereas in DFSs, the goal is to reduce both network trafﬁc and disk I/O.
In the following discussion, we describe the implementation of caching in a DFS and contrast it with the basic remote-service paradigm.
If the data needed to satisfy the access request are not already cached, then a copy of those data is brought from the server to the client system.
The idea is to retain recently accessed disk blocks in the cache, so that repeated accesses to the same information can be handled locally, without additional network trafﬁc.
No direct correspondence exists between accesses and trafﬁc to the server.
Files are still identiﬁed with one master copy residing at the server machine, but copies (or parts) of the ﬁle are scattered in different caches.
The problem of keeping the cached copies consistent with the master ﬁle is the cache-consistency problem, which we discuss in Section 17.9.2.4
It acts similarly to demand-paged virtual memory, except that the backing store usually is a remote server rather than a local disk.
The granularity of the cached data in a DFS can vary from blocks of a ﬁle to an entire ﬁle.
Usually, more data are cached than are needed to satisfy a single access, so that many accesses can be served by the cached data.
The other systems discussed here support caching of individual blocks driven by client demand.
Increasing the caching unit increases the hit ratio, but it also increases the miss penalty, because each miss requires more data to be transferred.
Selecting the unit of caching involves considering parameters such as the network transfer unit and the RPC protocol service unit (if an RPC protocol is used)
The network transfer unit (for Ethernet, a packet) is about 1.5 KB, so larger units of cached data need to be disassembled for delivery and reassembled on reception.
Block size and total cache size are obviously of importance for blockcaching schemes.
For smaller caches, large block sizes are less beneﬁcial because they result in fewer blocks in the cache and a lower hit ratio.
Where should the cached data be stored—on disk or in main memory? Disk caches have one clear advantage over main-memory caches: they are reliable.
Modiﬁcations to cached data are lost in a crash if the cache is kept in volatile memory.
Moreover, if the cached data are kept on disk, they are still there during recovery, and there is no need to fetch them again.
Data can be accessed more quickly from a cache in main memory than.
The resulting performance speedup is predicted to outweigh the advantages of disk caches.
The server caches (used to speed up disk I/O) will be in main memory regardless of where user caches are located; if we use main-memory caches on the user machine, too, we can build a single caching mechanism for use by both servers and users.
Many remote-access implementations can be thought of as hybrids of caching and remote service.
Similarly, Sprite’s implementation is based on caching, but under certain circumstances, a remote-service method is adopted.
Thus, to evaluate the two methods, we must evaluate the degree to which either method is emphasized.
The NFS protocol and most implementations do not provide disk caching.
The policy used to write modiﬁed data blocks back to the server’s master copy has a critical effect on the system’s performance and reliability.
The simplest policy is to write data through to disk as soon as they are placed in any cache.
The advantage of a write-through policy is reliability: little information is lost when a client system crashes.
However, this policy requires each write access to wait until the information is sent to the server, so it causes poor write performance.
Caching with write-through is equivalent to using remote service for write accesses and exploiting caching only for read accesses.
An alternative is the delayed-write policy, also known as write-back caching, where we delay updates to the master copy.
Modiﬁcations are written to the cache and then are written through to the server at a later time.
First, because writes are made to the cache, write accesses complete much more quickly.
Second, data may be overwritten before they are written back, in which case only the last update needs to be written at all.
Unfortunately, delayed-write schemes introduce reliability problems, since unwritten data are lost whenever a user machine crashes.
Variations of the delayed-write policy differ in when modiﬁed data blocks are ﬂushed to the server.
One alternative is to ﬂush a block when it is about to be ejected from the client’s cache.
This option can result in good performance, but some blocks can reside in the client’s cache a long time before they are written back to the server.
A compromise between this alternative and the write-through policy is to scan the cache at regular intervals and to ﬂush blocks that have been modiﬁed since the most recent scan, just as UNIX scans its local cache.
Yet another variation on delayed write is to write data back to the server when the ﬁle is closed.
In the case of ﬁles that are open for short periods or are modiﬁed rarely, this policy does not signiﬁcantly reduce network trafﬁc.
In addition, the write-on-close policy requires the closing process to delay while the ﬁle is written through, which reduces the performance advantages of delayed writes.
For ﬁles that are open for long periods and are modiﬁed frequently, however, the performance advantages of this policy over delayed write with more frequent ﬂushing are apparent.
A client machine is sometimes faced with the problem of deciding whether a locally cached copy of data is consistent with the master copy (and hence can be used)
If the client machine determines that its cached data are out of date, it must cache an up-to-date copy of the data before allowing further accesses.
There are two approaches to verifying the validity of cached data:
The client initiates a validity check, in which it contacts the server and checks whether the local data are consistent with the master copy.
The frequency of the validity checking is the crux of this approach and determines the resulting consistency semantics.
It can range from a check before every access to a check only on ﬁrst access to a ﬁle (on ﬁle open, basically)
Every access coupled with a validity check is delayed, compared with an access served immediately by the cache.
Depending on its frequency, the validity check can load both the network and the server.
The server records, for each client, the ﬁles (or parts of ﬁles) that it caches.
When the server detects a potential inconsistency, it must react.
A potential for inconsistency occurs when two different clients in conﬂicting modes cache a ﬁle.
If UNIX semantics (Section 11.5.3) is implemented, we can resolve the potential inconsistency by having the server play an active role.
The server must be notiﬁed whenever a ﬁle is opened, and the intended mode (read or write) must be indicated for every open.
The server can then act when it detects that a ﬁle has been opened simultaneously in conﬂicting modes by disabling caching for that particular ﬁle.
Actually, disabling caching results in switching to a remote-service mode of operation.
Distributed ﬁle systems are in common use today, providing ﬁle sharing within LANs and across WANs as well.
The complexity of implementing such a system should not be underestimated, especially considering that it must be operating-system independent for widespread adoption and must provide availability and good performance in the presence of long distances and sometimes-frail networking.
A distributed system is a collection of processors that do not share memory or a clock.
Instead, each processor has its own local memory, and the processors communicate with one another through various communication lines, such as high-speed buses and the Internet.
The processors in a distributed system vary in size and function.
They may include small microprocessors, personal computers, and large general-purpose computer systems.
The processors in the system are connected through a communication network.
A distributed system provides the user with access to all system resources.
Access to a shared resource can be provided by data migration, computation.
The access can be speciﬁed by the user or implicitly supplied by the operating system and applications.
Communications within a distributed system may occur via circuit switching, message switching, or packet switching.
Packet switching is the method most commonly used on data networks.
Through these methods, messages can be exchanged by nodes in the system.
Protocol stacks, as speciﬁed by network layering models, add information to a message to ensure that it reaches its destination.
A naming system (such as DNS) must be used to translate from a host name to a network address, and another protocol (such as ARP) may be needed to translate the network number to a network device address (an Ethernet address, for instance)
If systems are located on separate networks, routers are needed to pass packets from source network to destination network.
There are many challenges to overcome for a distributed system to work correctly.
Issues include naming of nodes and processes in the system, fault tolerance, error recovery, and scalability.
A DFS is a ﬁle-service system whose clients, servers, and storage devices are dispersed among the sites of a distributed system.
Accordingly, service activity has to be carried out across the network; instead of a single centralized data repository, there are multiple independent storage devices.
Ideally, a DFS should look to its clients like a conventional, centralized ﬁle system.
The multiplicity and dispersion of its servers and storage devices should be transparent.
A transparent DFS facilitates client mobility by bringing the client’s environment to the site where the client logs in.
There are several approaches to naming schemes in a DFS.
In the simplest approach, ﬁles are named by some combination of their host name and local name, which guarantees a unique system-wide name.
Another approach, popularized by NFS, provides a means to attach remote directories to local directories, thus giving the appearance of a coherent directory tree.
Requests to access a remote ﬁle are usually handled by two complementary methods.
With remote service, requests for accesses are delivered to the server.
The server machine performs the accesses, and the results are forwarded back to the client.
With caching, if the data needed to satisfy the access request are not already cached, then a copy of the data is brought from the server to the client.
The problem of keeping the cached copies consistent with the master ﬁle is the cache-consistency problem.
Describe a method for process migration across different architectures running:
List three possible types of failure in a distributed system.
Specify which of the entries in your list also are applicable to a centralized system.
What implications does your answer have for recovery in distributed systems?
Why do they use fewer layers? What problems could the use of fewer layers cause?
A transport-layer protocol such as TCP is used to provide reliability.
Discuss the advantages and disadvantages of supporting reliable message delivery at the lowest possible layer.
For each page, graphic, or applet, a separate TCP session was constructed, used, and torn down.
Because of the overhead of building and destroying TCP/IP connections, performance problems resulted from this implementation method.
Would using UDP rather than TCP be a good alternative? What other changes could you make to improve HTTP performance?
What actions need to be performed to recover from a server crash in order to preserve the consistency guaranteed by the system?
Discuss three techniques used to make OpenAFS a scalable system.
Case Studies In the ﬁnal part of the book, we integrate the concepts described earlier by examining real operating systems.
We chose Linux for several reasons: it is popular, it is freely available, and it represents a full-featured UNIX system.
This gives a student of operating systems an opportunity to read—and modify—real operating-system source code.
This recent operating system from Microsoft is gaining popularity not only in the standalone-machine market but also in the workgroup–server market.
We chose Windows 7 because it provides an opportunity to study a modern operating system that has a design and implementation drastically different from those of UNIX.
In addition, we brieﬂy discuss other highly inﬂuential operating systems.
Finally, we provide on-line coverage of twomore systems: FreeBSD and Mach.
However, whereas Linux combines features from several UNIX systems, FreeBSD is based on the BSD model.
FreeBSD source code, like Linux source code, is freely available.Mach is amodern operating system that provides compatibility with BSD UNIX.
This chapter presents an in-depth examination of the Linux operating system.
By examining a complete, real system, we can see how the concepts we have discussed relate both to one another and to practice.
Linux is a variant of UNIX that has gained popularity over the last several decades, powering devices as small as mobile phones and as large as roomﬁlling supercomputers.
In this chapter, we look at the history and development of Linux and cover the user and programmer interfaces that Linux presents —interfaces that owe a great deal to the UNIX tradition.
We also discuss the design and implementation of these interfaces.
To explore the history of the UNIX operating system from which Linux is derived and the principles upon which Linux’s design is based.
To examine the Linux process model and illustrate how Linux schedules processes and provides interprocess communication.
To explore how Linux implements ﬁle systems and manages I/O devices.
Linux looks and feels much like any other UNIX system; indeed, UNIX compatibility has been a major design goal of the Linux project.
Early in its development, the Linux source code was made available freeboth at no cost and with minimal distributional restrictions—on the Internet.
As a result, Linux’s history has been one of collaboration by many developers from all around the world, corresponding almost exclusively over the Internet.
From an initial kernel that partially implemented a small subset of the UNIX system services, the Linux system has grown to include all of the functionality expected of a modern UNIX system.
In its early days, Linux development revolved largely around the central operating-system kernel—the core, privileged executive that manages all system resources and interacts directly with the computer hardware.
We need much more than this kernel, of course, to produce a full operating system.
We thus need to make a distinction between the Linux kernel and a complete Linux system.
The Linux kernel is an original piece of software developed from scratch by the Linux community.
The Linux system, as we know it today, includes a multitude of components, some written from scratch, others borrowed from other development projects, and still others created in collaboration with other teams.
The basic Linux system is a standard environment for applications and user programming, but it does not enforce any standard means of managing the available functionality as a whole.
As Linux has matured, a need has arisen for another layer of functionality on top of the Linux system.
A Linux distribution includes all the standard components of the Linux system, plus a set of administrative tools to simplify the initial installation and subsequent upgrading of Linux and to manage installation and removal of other packages on the system.
A modern distribution also typically includes tools for management of ﬁle systems, creation and management of user accounts, administration of networks, Web browsers, word processors, and so on.
It had no networking, ran only on 80386-compatible Intel processors and PC hardware, and had extremely limited device-driver support.
The virtual memory subsystem was also fairly basic and included no support for memory-mapped ﬁles; however, even this early incarnation supported shared pages with copy-on-write and protected address spaces.
The only ﬁle system supported was the Minix ﬁle system, as the ﬁrst Linux kernels were cross-developed on a Minix platform.
This release culminated three years of rapid development of the Linux kernel.
Perhaps the single biggest new feature was networking: 1.0 included support for UNIX’s standard TCP/IP networking protocols, as well as a BSD-compatible socket interface for networking programming.
Device-driver support was added for running IP over Ethernet or (via the PPP or SLIP protocols) over serial lines or modems.
The 1.0 kernel also included a new, much enhanced ﬁle system without the limitations of the original Minix ﬁle system, and it supported a range of SCSI controllers for high-performance disk access.
The developers extended the virtual memory subsystem to support paging to swap ﬁles and memory mapping.
A range of extra hardware support was included in this release.
Although still restricted to the Intel PC platform, hardware support had grown to include ﬂoppy-disk and CD-ROM devices, as well as sound cards, a range of mice, and international keyboards.
System V UNIX-style interprocess communication (IPC), including shared memory, semaphores, and message queues, was implemented.
A pattern was adopted as the standard numbering convention for Linux kernels.
Updates for the stable kernels are intended only as remedial versions, whereas the development kernels may include newer and relatively untested functionality.
This release did not offer nearly the same improvement in functionality as the 1.0 release, but it did support a much wider variety of hardware, including the new PCI hardware bus architecture.
They also updated the IP implementation with support for accounting and ﬁrewalling.
Simple support for dynamically loadable and unloadable kernel modules was supplied as well.
The Linux 1.2 release concentrated on wider hardware support and more complete implementations of existing functionality.
Much new functionality was under development at the time, but integration of the new code into the main kernel source code was deferred until after the stable 1.2 kernel was released.
As a result, the 1.3 development stream saw a great deal of new functionality added to the kernel.
This release was given a major version-number increment because of two major new capabilities: support for multiple architectures, including a 64-bit native Alpha port, and symmetric multiprocessing (SMP) support.
Additionally, the memorymanagement code was substantially improved to provide a uniﬁed cache for ﬁle-system data independent of the caching of block devices.
As a result of this change, the kernel offered greatly increased ﬁle-system and virtualmemory performance.
For the ﬁrst time, ﬁle-system caching was extended to networked ﬁle systems, and writable memory-mapped regions were also supported.
Other major improvements included the addition of internal kernel threads, a mechanism exposing dependencies between loadable modules, support for the automatic loading of modules on demand, ﬁle-system quotas, and POSIX-compatible real-time process-scheduling classes.
Networking was enhanced with more ﬂexible ﬁrewalling, improved routing and trafﬁc management, and support for TCP large window and selective acknowledgement.
Acorn, Apple, and NT disks could now be read, and NFS was enhanced with a new kernel-mode NFS daemon.
Signal handling, interrupts, and some I/O were locked at a ﬁner level than before to improve symmetric multiprocessor (SMP) performance.
In addition, the 2.6 kernel was preemptive, allowing a process to be preempted even while running in kernel mode.
New features include improved virtualization support, a new page write-back facility, improvements to the memory-management system, and yet another new process scheduler—the Completely Fair Scheduler (CFS)
We focus on this newest kernel in the remainder of this chapter.
As we noted earlier, the Linux kernel forms the core of the Linux project, but other components make up a complete Linux operating system.
Whereas the Linux kernel is composed entirely of code written from scratch speciﬁcally for the Linux project, much of the supporting software that makes up the Linux system is not exclusive to Linux but is common to a number of UNIX-like operating systems.
The main system libraries of Linux were originated by the GNU project, but the Linux community greatly improved the libraries by addressing omissions, inefﬁciencies, and bugs.
Other components, such as the GNU C compiler (gcc), were already of sufﬁciently high quality to be used directly in Linux.
The network administration tools under Linux were derived from code ﬁrst developed for 4.3 BSD, but more recent BSD derivatives, such as FreeBSD, have borrowed code from Linux in return.
The Linux system as a whole is maintained by a loose network of developers collaborating over the Internet, with small groups or individuals having responsibility for maintaining the integrity of speciﬁc components.
The File System Hierarchy Standard document is also maintained by the Linux community as a means of ensuring compatibility across the various system components.
This standard speciﬁes the overall layout of a standard Linux ﬁle system; it determines under which directory names conﬁguration ﬁles, libraries, system binaries, and run-time data ﬁles should be stored.
In theory, anybody can install a Linux system by fetching the latest revisions of the necessary system components from the FTP sites and compiling them.
In Linux’s early days, this is precisely what a Linux user had to do.
As Linux has matured, however, various individuals and groups have attempted to make this job less painful by providing standard, precompiled sets of packages for easy installation.
These collections, or distributions, include much more than just the basic Linux system.
The ﬁrst distributions managed these packages by simply providing a means of unpacking all the ﬁles into the appropriate places.
One of the important contributions of modern distributions, however, is advanced package management.
Today’s Linux distributions include a package-tracking database that allows packages to be installed, upgraded, or removed painlessly.
The SLS distribution, dating back to the early days of Linux, was the ﬁrst collection of Linux packages that was recognizable as a complete distribution.
Although it could be installed as a single entity, SLS lacked the packagemanagement tools now expected of Linux distributions.
The Slackware distribution represented a great improvement in overall quality, even though it also had poor package management.
In fact, it is still one of the most widely installed distributions in the Linux community.
Since Slackware’s release, many commercial and noncommercial Linux distributions have become available.
Red Hat and Debian are particularly popular distributions; the ﬁrst comes from a commercial Linux support company and the second from the free-software Linux community.
Other commercially supported versions of Linux include distributions from Canonical and SuSE, and others too numerous to list here.
There are too many Linux distributions in circulation for us to list all of them here.
The variety of distributions does not prevent Linux distributions from being compatible, however.
The RPM package ﬁle format is used, or at least understood, by the majority of distributions, and commercial applications distributed in this format can be installed and run on any distribution that can accept RPM ﬁles.
The Linux kernel is distributed under version 2.0 of the GNU General Public License (GPL), the terms of which are set out by the Free Software Foundation.
Public domain implies that the authors have waived copyright rights in the software, but copyright rights in Linux code are still held by the code’s various authors.
Linux is free software, however, in the sense that people can copy it, modify it, use it in any manner they want, and give away (or sell) their own copies.
The main implication of Linux’s licensing terms is that nobody using Linux, or creating a derivative of Linux (a legitimate exercise), can distribute the derivative without including the source code.
Software released under the GPL cannot be redistributed as a binary-only product.
If you release software that includes any components covered by the GPL, then, under the GPL, you must.
This restriction does not prohibit making—or even selling—binary software distributions, as long as anybody who receives binaries is also given the opportunity to get the originating source code for a reasonable distribution charge.
In its overall design, Linux resembles other traditional, nonmicrokernel UNIX implementations.
It is a multiuser, preemptively multitasking system with a full set of UNIX-compatible tools.
Linux’s ﬁle system adheres to traditional UNIX semantics, and the standard UNIX networking model is fully implemented.
The internal details of Linux’s design have been inﬂuenced heavily by the history of this operating system’s development.
Although Linux runs on a wide variety of platforms, it was originally developed exclusively on PC architecture.
A great deal of that early development was carried out by individual enthusiasts rather than by well-funded development or research facilities, so from the start Linux attempted to squeeze as much functionality as possible from limited resources.
Today, Linux can run happily on a multiprocessor machine with many gigabytes of main memory and many terabytes of disk space, but it is still capable of operating usefully in under 16 MB of RAM.
As PCs became more powerful and as memory and hard disks became cheaper, the original, minimalist Linux kernels grew to implement more UNIX functionality.
Speed and efﬁciency are still important design goals, but much recent and current work on Linux has concentrated on a third major design goal: standardization.
One of the prices paid for the diversity of UNIX implementations currently available is that source code written for one may not necessarily compile or run correctly on another.
Even when the same system calls are present on two different UNIX systems, they do not necessarily behave in exactly the same way.
The POSIX standards comprise a set of speciﬁcations for different aspects of operating-system behavior.
There are POSIX documents for common operating-system functionality and for extensions such as process threads and real-time operations.
Linux is designed to comply with the relevant POSIX documents, and at least two Linux distributions have achieved ofﬁcial POSIX certiﬁcation.
Because it gives standard interfaces to both the programmer and the user, Linux presents few surprises to anybody familiar with UNIX.
By default, however, the Linux programming interface adheres to SVR4 UNIX semantics, rather than to BSD behavior.
A separate set of libraries is available to implement BSD semantics in places where the two behaviors differ signiﬁcantly.
Many other standards exist in the UNIX world, but full certiﬁcation of Linux with respect to these standards is sometimes slowed because certiﬁcation is often available only for a fee, and the expense involved in certifying an operating system’s compliance with most standards is substantial.
However, supporting a wide base of applications is important for any operating system, so implementation of standards is a major goal for Linux development, even if the implementation is not formally certiﬁed.
The Linux system is composed of three main bodies of code, in line with most traditional UNIX implementations:
The kernel is responsible for maintaining all the important abstractions of the operating system, including such things as virtual memory and processes.
The system libraries deﬁne a standard set of functions through which applications can interact with the kernel.
These functions implement much of the operating-system functionality that does not need the full privileges of kernel code.
The most important system library is the C library, known as libc.
In addition to providing the standard C library, libc implements the user mode side of the Linux system call interface, as well as other critical system-level interfaces.
The system utilities are programs that perform individual, specialized management tasks.
Some system utilities are invoked just once to initialize and conﬁgure some aspect of the system.
Others —known as daemons in UNIX terminology—run permanently, handling such tasks as responding to incoming network connections, accepting logon requests from terminals, and updating log ﬁles.
Figure 18.1 illustrates the various components that make up a full Linux system.
The most important distinction here is between the kernel and everything else.
All the kernel code executes in the processor’s privileged mode with full access to all the physical resources of the computer.
Under Linux, no user code is built into the kernel.
Unlike kernel mode, user mode has access only to a controlled subset of the system’s resources.
Although various modern operating systems have adopted a messagepassing architecture for their kernel internals, Linux retains UNIX’s historical model: the kernel is created as a single, monolithic binary.
Because all kernel code and data structures are kept in a single address space, no context switches are necessary when a process calls an operating-system function or when a hardware interrupt is delivered.
Moreover, the kernel can pass data and make requests between various subsystems using relatively cheap C function invocation and not more complicated interprocess communication (IPC)
This single address space contains not only the core scheduling and virtual memory code but all kernel code, including all device drivers, ﬁle systems, and networking code.
Even though all the kernel components share this same melting pot, there is still room for modularity.
In the same way that user applications can load shared libraries at run time to pull in a needed piece of code, so the Linux kernel can load (and unload) modules dynamically at run time.
The kernel does not need to know in advance which modules may be loaded—they are truly independent loadable components.
The Linux kernel forms the core of the Linux operating system.
It provides all the functionality necessary to run processes, and it provides system services to give arbitrated and protected access to hardware resources.
The kernel implements all the features required to qualify as an operating system.
On its own, however, the operating system provided by the Linux kernel is not a complete UNIX system.
It lacks much of the functionality and behavior of UNIX, and the features that it does provide are not necessarily in the format in which a UNIX application expects them to appear.
The operating-system interface visible to running applications is not maintained directly by the kernel.
Rather, applications make calls to the system libraries, which in turn call the operating-system services as necessary.
At the simplest level, they allow applications to make system calls to the Linux kernel.
Making a system call involves transferring control from unprivileged user mode to privileged kernel mode; the details of this transfer vary from architecture to architecture.
The libraries take care of collecting the system-call arguments and, if necessary, arranging those arguments in the special form necessary to make the system call.
The libraries may also provide more complex versions of the basic system calls.
For example, the C language’s buffered ﬁle-handling functions are all implemented in the system libraries, providing more advanced control of ﬁle I/O than the basic kernel system calls.
All the functions necessary to support the running of UNIX or POSIX applications are implemented in the system libraries.
The Linux system includes a wide variety of user-mode programs—both system utilities and user utilities.
The system utilities include all the programs necessary to initialize and then administer the system, such as those to set up networking interfaces and to add and remove users from the system.
User utilities are also necessary to the basic operation of the system but do not require elevated privileges to run.
They include simple ﬁle-management utilities such as those to copy ﬁles, create directories, and edit text ﬁles.
Linux supports many shells; the most common is the bourne-Again shell (bash)
The Linux kernel has the ability to load and unload arbitrary sections of kernel code on demand.
These loadable kernel modules run in privileged kernel mode and as a consequence have full access to all the hardware capabilities of the machine on which they run.
In theory, there is no restriction on what a kernel module is allowed to do.
Among other things, a kernel module can implement a device driver, a ﬁle system, or a networking protocol.
Linux’s source code is free, so anybody wanting to write kernel code is able to compile a modiﬁed kernel and to reboot into that new functionality.
However, recompiling, relinking, and reloading the entire kernel is a cumbersome cycle to undertake when you are developing a new driver.
If you use kernel modules, you do not have to make a new kernel to test a new driver—the driver can be compiled on its own and loaded into the already running kernel.
Of course, once a new driver is written, it can be distributed as a module so that other users can beneﬁt from it without having to rebuild their kernels.
Because it is covered by the GPL license, the Linux kernel cannot be released with proprietary components added to it unless those new components are also released under the GPL and the source code for them is made available on demand.
The kernel’s module interface allows third parties to write and distribute, on their own terms, device drivers or ﬁle systems that could not be distributed under the GPL.
Kernel modules allow a Linux system to be set up with a standard minimal kernel, without any extra device drivers built in.
Any device drivers that the user needs can be either loaded explicitly by the system at startup or loaded automatically by the system on demand and unloaded when not in use.
For example, a mouse driver can be loaded when a USB mouse is plugged into the system and unloaded when the mouse is unplugged.
The module-management system allows modules to be loaded into memory and to communicate with the rest of the kernel.
The module loader and unloader, which are user-mode utilities, work with the module-management system to load a module into memory.
Loading a module requires more than just loading its binary contents into kernel memory.
The system must also make sure that any references the.
Linux deals with this reference updating by splitting the job of module loading into two separate sections: the management of sections of module code in kernel memory and the handling of symbols that modules are allowed to reference.
This symbol table does not contain the full set of symbols deﬁned in the kernel during the latter’s compilation; rather, a symbol must be explicitly exported.
The set of exported symbols constitutes a well-deﬁned interface by which a module can interact with the kernel.
Although exporting symbols from a kernel function requires an explicit request by the programmer, no special effort is needed to import those symbols into a module.
A module writer just uses the standard external linking of the C language.
Any external symbols referenced by the module but not declared by it are simply marked as unresolved in the ﬁnal module binary produced by the compiler.
When a module is to be loaded into the kernel, a system utility ﬁrst scans the module for these unresolved references.
All symbols that still need to be resolved are looked up in the kernel’s symbol table, and the correct addresses of those symbols in the currently running kernel are substituted into the module’s code.
Only then is the module passed to the kernel for loading.
If the system utility cannot resolve all references in the module by looking them up in the kernel’s symbol table, then the module is rejected.
The loading of the module is performed in two stages.
First, the moduleloader utility asks the kernel to reserve a continuous area of virtual kernel memory for the module.
The kernel returns the address of the memory allocated, and the loader utility can use this address to relocate the module’s machine code to the correct loading address.
A second system call then passes the module, plus any symbol table that the new module wants to export, to the kernel.
The module itself is now copied verbatim into the previously allocated space, and the kernel’s symbol table is updated with the new symbols for possible use by other modules not yet loaded.
The kernel deﬁnes a communication interface to which a module-management program can connect.
With this connection established, the kernel will inform the management process whenever a process requests a device driver, ﬁle system, or network service that is not currently loaded and will give the manager the opportunity to load that service.
The original service request will complete once the module is loaded.
The manager process regularly queries the kernel to see whether a dynamically loaded module is still in use and unloads that module when it is no longer actively needed.
Once a module is loaded, it remains no more than an isolated region of memory until it lets the rest of the kernel know what new functionality it provides.
The kernel maintains dynamic tables of all known drivers and provides a set of routines to allow drivers to be added to or removed from these tables at any time.
The kernel makes sure that it calls a module’s startup routine when that module is loaded and calls the module’s cleanup routine before.
A module may register many types of functionality; it is not limited to only one type.
For example, a device driver might want to register two separate mechanisms for accessing the device.
These drivers include character devices (such as printers, terminals, and mice), block devices (including all disk drives), and network interface devices.
The ﬁle system may be anything that implements Linux’s virtual ﬁle system calling routines.
It might implement a format for storing ﬁles on a disk, but it might equally well be a network ﬁle system, such as NFS, or a virtual ﬁle system whose contents are generated on demand, such as Linux’s /proc ﬁle system.
A module may implement an entire networking protocol, such as TCP or simply a new set of packet-ﬁltering rules for a network ﬁrewall.
This format speciﬁes a way of recognizing, loading, and executing a new type of executable ﬁle.
In addition, a module can register a new set of entries in the sysctl and /proc tables, to allow that module to be conﬁgured dynamically (Section 18.7.4)
Commercial UNIX implementations are usually sold to run on a vendor’s own hardware.
One advantage of a single-supplier solution is that the software vendor has a good idea about what hardware conﬁgurations are possible.
The problem of managing the hardware conﬁguration becomes more severe when modular device drivers are supported, since the currently active set of devices becomes dynamically variable.
Linux provides a central conﬂict-resolution mechanism to help arbitrate access to certain hardware resources.
To prevent modules from clashing over access to hardware resources.
To resolve conﬂicts among multiple drivers trying to access the same hardware—as, for example, when both the parallel printer driver and the parallel line IP (PLIP) network driver try to talk to the parallel port.
To these ends, the kernel maintains lists of allocated hardware resources.
The PC has a limited number of possible I/O ports (addresses in its hardware I/O address space), interrupt lines, and DMA channels.
When any device driver wants to access such a resource, it is expected to reserve the resource with.
This requirement incidentally allows the system administrator to determine exactly which resources have been allocated by which driver at any given point.
A module is expected to use this mechanism to reserve in advance any hardware resources that it expects to use.
If the reservation is rejected because the resource is not present or is already in use, then it is up to the module to decide how to proceed.
It may fail in its initialization attempt and request that it be unloaded if it cannot continue, or it may carry on, using alternative hardware resources.
A process is the basic context in which all user-requested activity is serviced within the operating system.
To be compatible with other UNIX systems, Linux must use a process model similar to those of other versions of UNIX.
Linux operates differently from UNIX in a few key places, however.
In this section, we review the traditional UNIX process model (Section A.3.2) and introduce Linux’s threading model.
The basic principle of UNIX process management is to separate into two steps two operations that are usually combined into one: the creation of a new process and the running of a new program.
A new process is created by the fork() system call, and a new program is run after a call to exec()
We can create a new process with fork() without running a new program—the new subprocess simply continues to execute exactly the same program, at exactly the same point, that the ﬁrst (parent) process was running.
In the same way, running a new program does not require that a new process be created ﬁrst.
A new binary object is loaded into the process’s address space and the new executable starts executing in the context of the existing process.
It is not necessary to specify every detail of the environment of a new program in the system call that runs that program.
If a parent process wishes to modify the environment in which a new program is to be run, it can fork and then, still running the original executable in a child process, make any system calls it requires to modify that child process before ﬁnally executing the new program.
Under UNIX, then, a process encompasses all the information that the operating system must maintain to track the context of a single execution of a single program.
Under Linux, we can break down this context into a number of speciﬁc sections.
Broadly, process properties fall into three groups: the process identity, environment, and context.
The PID is used to specify the process to the operating system when an application makes a.
Additional identiﬁers associate the process with a process group (typically, a tree of processes forked by a single user command) and login session.
Each process must have an associated user ID and one or more group IDs (user groups are discussed in Section 11.6.2) that determine the rights of a process to access system resources and ﬁles.
Process personalities are not traditionally found on UNIX systems, but under Linux each process has an associated personality identiﬁer that can slightly modify the semantics of certain system calls.
Personalities are primarily used by emulation libraries to request that system calls be compatible with certain varieties of UNIX.
Each process is associated with a speciﬁc view of the ﬁlesystem hierarchy, called its namespace.
Most processes share a common namespace and thus operate on a shared ﬁle-system hierarchy.
Processes and their children can, however, have different namespaces, each with a unique ﬁle-system hierarchy—their own root directory and set of mounted ﬁle systems.
Most of these identiﬁers are under the limited control of the process itself.
The process group and session identiﬁers can be changed if the process wants to start a new group or session.
Its credentials can be changed, subject to appropriate security checks.
However, the primary PID of a process is unchangeable and uniquely identiﬁes that process until termination.
A process’s environment is inherited from its parent and is composed of two null-terminated vectors: the argument vector and the environment vector.
The argument vector simply lists the command-line arguments used to invoke the running program; it conventionally starts with the name of the program itself.
The environment vector is a list of “NAME=VALUE” pairs that associates named environment variables with arbitrary textual values.
The environment is not held in kernel memory but is stored in the process’s own user-mode address space as the ﬁrst datum at the top of the process’s stack.
The argument and environment vectors are not altered when a new process is created.
The new child process will inherit the environment of its parent.
However, a completely new environment is set up when a new program is invoked.
On calling exec(), a process must supply the environment for the new program.
The kernel passes these environment variables to the next program, replacing the process’s current environment.
The kernel otherwise leaves the environment and command-line vectors alone—their interpretation is left entirely to the user-mode libraries and applications.
The passing of environment variables from one process to the next and the inheriting of these variables by the children of a process provide ﬂexible ways to pass information to components of the user-mode system software.
Various important environment variables have conventional meanings to related parts of the system software.
For example, the TERM variable is set up to name the type of terminal connected to a user’s login session.
Programs with multilingual support use the LANG variable to determine the language in which to display system messages for programs that include multilingual support.
Users can choose their own languages or select their own editors independently of one another.
The process identity and environment properties are usually set up when a process is created and not changed until that process exits.
A process may choose to change some aspects of its identity if it needs to do so, or it may alter its environment.
In contrast, process context is the state of the running program at any one time; it changes constantly.
The most important part of the process context is its scheduling context—the information that the scheduler needs to suspend and restart the process.
This information includes saved copies of all the process’s registers.
Floating-point registers are stored separately and are restored only when needed.
Thus, processes that do not use ﬂoating-point arithmetic do not incur the overhead of saving that state.
The scheduling context also includes information about scheduling priority and about any outstanding signals waiting to be delivered to the process.
A key part of the scheduling context is the process’s kernel stack, a separate area of kernel memory reserved for use by kernel-mode code.
Both system calls and interrupts that occur while the process is executing will use this stack.
The kernel maintains accounting information about the resources currently being consumed by each process and the total resources consumed by the process in its entire lifetime so far.
The ﬁle table is an array of pointers to kernel ﬁle structures representing open ﬁles.
When making ﬁle-I/O system calls, processes refer to ﬁles by an integer, known as a ﬁle descriptor (fd), that the kernel uses to index into this table.
Whereas the ﬁle table lists the existing open ﬁles, the ﬁle-system context applies to requests to open new ﬁles.
The ﬁle-system context includes the process’s root directory, current working directory, and namespace.
The signal-handler table deﬁnes the action to take in response to a speciﬁc signal.
Valid actions include ignoring the signal, terminating the process, and invoking a routine in the process’s address space.
The virtual memory context describes the full contents of a process’s private address space; we discuss it in Section 18.6
Linux provides the fork() system call, which duplicates a process without loading a new executable image.
Linux also provides the ability to create threads via the clone() system call.
In fact, Linux generally uses the term task —rather than process or thread—when referring to a ﬂow of control within a program.
The clone() system call behaves identically to fork(), except that it accepts as arguments a set of ﬂags that dictate what resources are shared between the parent and child (whereas a process created with fork() shares no resources with its parent)
Thus, if clone() is passed the ﬂags CLONE FS, CLONE VM, CLONE SIGHAND, and CLONE FILES, the parent and child tasks will share the same ﬁle-system information (such as the current working directory), the same memory space, the same signal handlers, and the same set of open ﬁles.
Using clone() in this fashion is equivalent to creating a thread in other systems, since the parent task shares most of its resources with its child task.
If none of these ﬂags is set when clone() is invoked, however, the associated resources are not shared, resulting in functionality similar to that of the fork() system call.
The lack of distinction between processes and threads is possible because Linux does not hold a process’s entire context within the main process data structure.
Thus, a process’s ﬁle-system context, ﬁle-descriptor table, signal-handler table, and virtual memory context are held in separate data structures.
The process data structure simply contains pointers to these other structures, so any number of processes can easily share a subcontext by pointing to the same subcontext and incrementing a reference count.
The arguments to the clone() system call tell it which subcontexts to copy and which to share.
The new process is always given a new identity and a new scheduling context—these are the essentials of a Linux process.
According to the arguments passed, however, the kernel may either create new subcontext data structures initialized so as to be copies of the parent’s or set up the new process to use the same subcontext data structures being used by the parent.
The fork() system call is nothing more than a special case of clone() that copies all subcontexts, sharing none.
Scheduling is the job of allocating CPU time to different tasks within an operating system.
In such a system, the process scheduler decides which process runs and when.
Making these decisions in a way that balances fairness and performance across many different workloads is one of the more complicated challenges in modern operating systems.
Normally, we think of scheduling as the running and interrupting of user processes, but another aspect of scheduling is also important to Linux: the running of the various kernel tasks.
Kernel tasks encompass both tasks that are requested by a running process and tasks that execute internally on behalf of the kernel itself, such as tasks spawned by Linux’s I/O subsystem.
One is a time-sharing algorithm for fair, preemptive scheduling among multiple processes.
The other is designed for real-time tasks, where absolute priorities are more important than fairness.
The scheduling algorithm used for routine time-sharing tasks received a major overhaul with version 2.6 of the kernel.
Earlier versions ran a variation of the traditional UNIX scheduling algorithm.
This algorithm does not provide adequate support for SMP systems, does not scale well as the number of tasks on the system grows, and does not maintain fairness among interactive tasks, particularly on systems such as desktops and mobile devices.
The process scheduler was ﬁrst overhauled with version 2.5 of the kernel.
The new scheduler also provided increased support for SMP, including processor afﬁnity and load balancing.
These changes, while improving scalability, did not improve interactive performance or fairness—and, in fact, made these problems worse under certain workloads.
Consequently, the process scheduler was overhauled a second time, with Linux kernel version 2.6
In the latter, the core variables in the scheduling algorithm are priority and time slice.
The time slice is the length of time—the slice of the processorthat a process is afforded.
Traditional UNIX systems give processes a ﬁxed time slice, perhaps with a boost or penalty for high- or low-priority processes, respectively.
A process may run for the length of its time slice, and higherpriority processes run before lower-priority processes.
It is a simple algorithm that many non-UNIX systems employ.
Such simplicity worked well for early time-sharing systems but has proved incapable of delivering good interactive performance and fairness on today’s modern desktops and mobile devices.
Instead of time slices, all processes are allotted a proportion of the processor’s time.
To start, CFS says that if there are N runnable processes, then each should be afforded 1/N of the processor’s time.
Processes with the default nice value have a weight of 1—their priority is unchanged.
Processes with a smaller nice value (higher priority) receive a higher weight, while processes with a larger nice value (lower priority) receive a lower weight.
To calculate the actual length of time a process runs, CFS relies on a conﬁgurable variable called target latency, which is the interval of time during which every runnable task should run at least once.
For example, assume that the target latency is 10 milliseconds.
Further assume that we have two runnable processes of the same priority.
Each of these processes has the same weight and therefore receives the same proportion of the processor’s time.
If we have 10 runnable processes, then CFS will run each for a millisecond before repeating.
Due to switching costs, scheduling processes for such short lengths of time is inefﬁcient.
All processes, regardless of the target latency, will run for at least the minimum granularity.
In this manner, CFS ensures that switching costs do not grow unacceptably large when the number of runnable processes grows too large.
In the usual case, however, the number of runnable processes remains reasonable, and both fairness and switching costs are maximized.
With the switch to fair scheduling, CFS behaves differently from traditional UNIX process schedulers in several ways.
Most notably, as we have seen, CFS eliminates the concept of a static time slice.
Instead, each process receives a proportion of the processor’s time.
How long that allotment is depends on how many other processes are runnable.
This approach solves several problems in mapping priorities to time slices inherent in preemptive, priority-based scheduling algorithms.
It is possible, of course, to solve these problems in other ways without abandoning the classic UNIX scheduler.
CFS, however, solves the problems with a simple algorithm that performs well on interactive workloads such as mobile devices without compromising throughput performance on the largest of servers.
Linux’s real-time scheduling algorithm is signiﬁcantly simpler than the fair scheduling employed for standard time-sharing processes.
In both cases, each process has a priority in addition to its scheduling class.
The scheduler always runs the process with the highest priority.
Among processes of equal priority, it runs the process that has been waiting longest.
The scheduler offers strict guarantees about the relative priorities of real-time processes, but the kernel does not offer any guarantees about how quickly a real-time process will be scheduled once that process becomes runnable.
In contrast, a hard real-time system can guarantee a minimum latency between when a process becomes runnable and when it actually runs.
The way the kernel schedules its own operations is fundamentally different from the way it schedules processes.
A request for kernel-mode execution can occur in two ways.
A running program may request an operating-system service, either explicitly via a system call or implicitly—for example, when a page fault occurs.
Alternatively, a device controller may deliver a hardware interrupt that causes the CPU to start executing a kernel-deﬁned handler for that interrupt.
The problem for the kernel is that all these tasks may try to access the same internal data structures.
If one kernel task is in the middle of accessing some data structure when an interrupt service routine executes, then that service routine cannot access or modify the same data without risking data corruption.
This fact relates to the idea of critical sections—portions of code that access shared data and thus must not be allowed to execute concurrently.
As a result, kernel synchronization involves much more than just process scheduling.
A framework is required that allows kernel tasks to run without violating the integrity of shared data.
Prior to version 2.6, Linux was a nonpreemptive kernel, meaning that a process running in kernel mode could not be preempted—even if a higherpriority process became available to run.
Now, a task can be preempted when it is running in the kernel.
The Linux kernel provides spinlocks and semaphores (as well as readerwriter versions of these two locks) for locking in the kernel.
On SMP machines, the fundamental locking mechanism is a spinlock, and the kernel is designed so that spinlocks are held for only short durations.
On single-processor machines, spinlocks are not appropriate for use and are replaced by enabling and disabling kernel preemption.
That is, rather than holding a spinlock, the task disables kernel preemption.
When the task would otherwise release the spinlock, it enables kernel preemption.
Linux uses an interesting approach to disable and enable kernel preemption.
It provides two simple kernel interfaces—preempt disable() and preempt enable()
In addition, the kernel is not preemptible if a kernel-mode task is holding a spinlock.
To enforce this rule, each task in the system has a thread-info structure that includes the ﬁeld preempt count, which is a counter indicating the number of locks being held by the task.
The counter is incremented when a lock is acquired and decremented when a lock is released.
If the value of preempt count for the task currently running is greater than zero, it is not safe to preempt the kernel, as this task currently holds a lock.
If the count is zero, the kernel can safely be interrupted, assuming there are no outstanding calls to preempt disable()
Spinlocks—along with the enabling and disabling of kernel preemptionare used in the kernel only when the lock is held for short durations.
When a lock must be held for longer periods, semaphores are used.
The second protection technique used by Linux applies to critical sections that occur in interrupt service routines.
By disabling interrupts (or using spinlocks) during a critical section, the kernel guarantees that it can proceed without the risk of concurrent access to shared data structures.
On most hardware architectures, interrupt enable and disable instructions are not cheap.
More importantly, as long as interrupts remain disabled, all I/O is suspended, and any device waiting for servicing will have to wait until interrupts are reenabled; thus, performance degrades.
To address this problem, the Linux kernel uses a synchronization architecture that allows long critical sections to run for their entire duration without having interrupts disabled.
An interrupt in a network device driver can signal the arrival of an entire network packet, which may result in a great deal of code being executed to disassemble, route, and forward that packet within the interrupt service routine.
Linux implements this architecture by separating interrupt service routines into two sections: the top half and the bottom half.
The top half is the standard interrupt service routine that runs with recursive interrupts disabled.
Interrupts of the same number (or line) are disabled, but other interrupts may run.
The bottom half of a service routine is run, with all interrupts enabled, by a miniature scheduler that ensures that bottom halves never interrupt themselves.
The bottom-half scheduler is invoked automatically whenever an interrupt service routine exits.
This separation means that the kernel can complete any complex processing that has to be done in response to an interrupt without worrying about being interrupted itself.
If another interrupt occurs while a bottom half is executing, then that interrupt can request that the same bottom half execute, but the execution will be deferred until the one currently running completes.
Each execution of the bottom half can be interrupted by a top half but can never be interrupted by a similar bottom half.
The kernel can code critical sections easily using this system.
Interrupt handlers can code their critical sections as bottom halves; and when the foreground kernel wants to enter a critical section, it can disable any relevant.
At the end of the critical section, the kernel can reenable the bottom halves and run any bottom-half tasks that have been queued by top-half interrupt service routines during the critical section.
Figure 18.2 summarizes the various levels of interrupt protection within the kernel.
Each level may be interrupted by code running at a higher level but will never be interrupted by code running at the same or a lower level.
Except for user-mode code, user processes can always be preempted by another process when a time-sharing scheduling interrupt occurs.
The Linux 2.0 kernel was the ﬁrst stable Linux kernel to support symmetric multiprocessor (SMP) hardware, allowing separate processes to execute in parallel on separate processors.
The original implementation of SMP imposed the restriction that only one processor at a time could be executing kernel code.
In version 2.2 of the kernel, a single kernel spinlock (sometimes termed BKL for “big kernel lock”) was created to allow multiple processes (running on different processors) to be active in the kernel concurrently.
However, the BKL provided a very coarse level of locking granularity, resulting in poor scalability to machines with many processors and processes.
Later releases of the kernel made the SMP implementation more scalable by splitting this single kernel spinlock into multiple locks, each of which protects only a small subset of the kernel’s data structures.
The 3.0 kernel provides additional SMP enhancements, including ever-ﬁner locking, processor afﬁnity, and load-balancing algorithms.
The ﬁrst deals with allocating and freeing physical memory—pages, groups of pages, and small blocks of RAM.
The second handles virtual memory, which is memory-mapped into the address space of running processes.
In this section, we describe these two components and then examine the mechanisms by which the loadable components of a new program are brought into a process’s virtual memory in response to an exec() system call.
Due to speciﬁc hardware constraints, Linux separates physical memory into four different zones, or regions:
On these systems, the ﬁrst 16 MB of physical memory comprise ZONE DMA.
Finally, ZONE NORMAL comprises everything else—the normal, regularly mapped pages.
Whether an architecture has a given zone depends on its constraints.
The kernel maintains a list of free pages for each zone.
When a request for physical memory arrives, the kernel satisﬁes the request using the appropriate zone.
The primary physical-memory manager in the Linux kernel is the page allocator.
Each zone has its own allocator, which is responsible for allocating and freeing all physical pages for the zone and is capable of allocating ranges of physically contiguous pages on request.
The allocator uses a buddy system (Section 9.8.1) to keep track of available physical pages.
In this scheme, adjacent units of allocatable memory are paired together (hence its name)
Each allocatable memory region has an adjacent partner (or buddy)
Whenever two allocated partner regions are freed up, they are combined to form a larger region—a buddy heap.
That larger region also has a partner, with which it can combine to form a still larger free region.
Separate linked lists are used to record the free memory regions of each allowable size.
Under Linux, the smallest size allocatable under this mechanism is a single physical page.
The region is broken up recursively until a piece of the desired size is available.
Ultimately, all memory allocations in the Linux kernel are made either statically, by drivers that reserve a contiguous area of memory during system boot time, or dynamically, by the page allocator.
However, kernel functions do not have to use the basic allocator to reserve memory.
Several specialized memory-management subsystems use the underlying page allocator to manage their own pools of memory.
The most important are the virtual memory system, described in Section 18.6.2; the kmalloc() variable-length allocator; the slab allocator, used for allocating memory for kernel data structures; and the page cache, used for caching pages belonging to ﬁles.
Another strategy adopted by Linux for allocating kernel memory is known as slab allocation.
A slab is used for allocating memory for kernel data structures and is made up of one or more physically contiguous pages.
There is a single cache for each unique kernel data structure—for example, a cache for the data structure representing process descriptors, a cache for ﬁle objects, a cache for inodes, and so forth.
Each cache is populated with objects that are instantiations of the kernel data structure the cache represents.
For example, the cache representing inodes stores instances of inode structures, and the cache representing process descriptors stores instances of process descriptor structures.
The relationship among slabs, caches, and objects is shown in Figure 18.5
When a cache is created, a number of objects are allocated to the cache.
The number of objects in the cache depends on the size of the associated slab.
Initially, all the objects in the cache are marked as free.
When a new object for a kernel data structure is needed, the allocator can assign any free object from the cache to satisfy the request.
The object assigned from the cache is marked as used.
Let’s consider a scenario in which the kernel requests memory from the slab allocator for an object representing a process descriptor.
In Linux systems, a process descriptor is of the type struct task struct, which requires approximately 1.7 KB of memory.
When the Linux kernel creates a new task, it requests the necessary memory for the struct task struct object from its cache.
The cache will fulﬁll the request using a struct task struct object that has already been allocated in a slab and is marked as free.
In Linux, a slab may be in one of three possible states:
The slab allocator ﬁrst attempts to satisfy the request with a free object in a partial slab.
If none exist, a free object is assigned from an empty slab.
If no empty slabs are available, a new slab is allocated from contiguous physical.
Two other main subsystems in Linux do their own management of physical pages: the page cache and the virtual memory system.
The page cache is the kernel’s main cache for ﬁles and is the main mechanism through which I/O to block devices (Section 18.8.1) is performed.
File systems of all types, including the native Linux disk-based ﬁle systems and the NFS networked ﬁle system, perform their I/O through the page cache.
The page cache stores entire pages of ﬁle contents and is not limited to block devices.
The virtual memory system manages the contents of each process’s virtual address space.
These two systems interact closely with each other because reading a page of data into the page cache requires mapping pages in the page cache using the virtual memory system.
In the following section, we look at the virtual memory system in greater detail.
The Linux virtual memory system is responsible for maintaining the address space accessible to each process.
It creates pages of virtual memory on demand and manages loading those pages from disk and swapping them back out to disk as required.
Under Linux, the virtual memory manager maintains two separate views of a process’s address space: as a set of separate regions and as a set of pages.
The ﬁrst view of an address space is the logical view, describing instructions that the virtual memory system has received concerning the layout of the address space.
In this view, the address space consists of a set of nonoverlapping regions, each region representing a continuous, page-aligned subset of the address space.
Each region is described internally by a single vm area struct structure that deﬁnes the properties of the region, including the process’s read, write, and execute permissions in the region as well as information about any ﬁles associated with the region.
The regions for each address space are linked into a balanced binary tree to allow fast lookup of the region corresponding to any virtual address.
The kernel also maintains a second, physical view of each address space.
This view is stored in the hardware page tables for the process.
The pagetable entries identify the exact current location of each page of virtual memory, whether it is on disk or in physical memory.
The physical view is managed by a set of routines, which are invoked from the kernel’s software-interrupt handlers whenever a process tries to access a page that is not currently present in the page tables.
Each vm area struct in the address-space description contains a ﬁeld pointing to a table of functions that implement the key page-management functionality for any given virtual memory region.
All requests to read or write an unavailable page are eventually dispatched to the appropriate handler in the function table for the vm area struct, so that the central memorymanagement routines do not have to know the details of managing each possible type of memory region.
One property that characterizes virtual memory is the backing store for the region, which describes where the pages for the region come from.
Most memory regions are backed either by a ﬁle or by nothing.
A region backed by nothing is the simplest type of virtual memory region.
Such a region represents demand-zero memory: when a process tries to read a page in such a region, it is simply given back a page of memory ﬁlled with zeros.
A region backed by a ﬁle acts as a viewport onto a section of that ﬁle.
Whenever the process tries to access a page within that region, the page table is ﬁlled with the address of a page within the kernel’s page cache corresponding to the appropriate offset in the ﬁle.
The same page of physical memory is used by both the page cache and the process’s page tables, so any changes made to the ﬁle by the ﬁle system are immediately visible to any processes that have mapped that ﬁle into their address space.
Any number of processes can map the same region of the same ﬁle, and they will all end up using the same page of physical memory for the purpose.
A virtual memory region is also deﬁned by its reaction to writes.
The mapping of a region into the process’s address space can be either private or shared.
If a process writes to a privately mapped region, then the pager detects that a copy-on-write is necessary to keep the changes local to the process.
In contrast, writes to a shared region result in updating of the object mapped into that region, so that the change will be visible immediately to any other process that is mapping that object.
The kernel creates a new virtual address space in two situations: when a process runs a new program with the exec() system call and when a new process is created by the fork() system call.
When a new program is executed, the process is given a new, completely empty virtual address space.
It is up to the routines for loading the program to populate the address space with virtual memory regions.
The second case, creating a new process with fork(), involves creating a complete copy of the existing process’s virtual address space.
The kernel copies the parent process’s vm area struct descriptors, then creates a new set of page tables for the child.
The parent’s page tables are copied directly into the child’s, and the reference count of each page covered is incremented.
Thus, after the fork, the parent and child share the same physical pages of memory in their address spaces.
A special case occurs when the copying operation reaches a virtual memory region that is mapped privately.
Any pages to which the parent process has written within such a region are private, and subsequent changes to these pages by either the parent or the child must not update the page in the other process’s address space.
When the page-table entries for such regions are copied, they are set to be read only and are marked for copy-on-write.
As long as neither process modiﬁes these pages, the two processes share the same page of physical memory.
However, if either process tries to modify a copy-on-write page, the reference count on the page is checked.
This mechanism ensures that private data pages are shared between processes whenever possible and copies are made only when absolutely necessary.
An important task for a virtual memory system is to relocate pages of memory from physical memory out to disk when that memory is needed.
Early UNIX systems performed this relocation by swapping out the contents of entire processes at once, but modern versions of UNIX rely more on paging—the movement of individual pages of virtual memory between physical memory and disk.
Linux does not implement whole-process swapping; it uses the newer paging mechanism exclusively.
First, the policy algorithm decides which pages to write out to disk and when to write them.
Second, the paging mechanism carries out the transfer and pages data back into physical memory when they are needed again.
Linux’s pageout policy uses a modiﬁed version of the standard clock (or second-chance) algorithm described in Section 9.4.5.2
Under Linux, a multiplepass clock is used, and every page has an age that is adjusted on each pass of the clock.
The age is more precisely a measure of the page’s youthfulness, or how much activity the page has seen recently.
Frequently accessed pages will attain a higher age value, but the age of infrequently accessed pages will drop toward zero with each pass.
This age valuing allows the pager to select pages to page out based on a least frequently used (LFU) policy.
The paging mechanism supports paging both to dedicated swap devices and partitions and to normal ﬁles, although swapping to a ﬁle is signiﬁcantly slower due to the extra overhead incurred by the ﬁle system.
Blocks are allocated from the swap devices according to a bitmap of used blocks, which is maintained in physical memory at all times.
The allocator uses a next-ﬁt algorithm to try to write out pages to continuous runs of disk blocks for improved performance.
The allocator records the fact that a page has been paged out to disk by using a feature of the page tables on modern processors: the page-table entry’s page-not-present bit is set, allowing the rest of the pagetable entry to be ﬁlled with an index identifying where the page has been written.
The page-table entries that map to these kernel pages are marked as protected, so that the pages are not visible or modiﬁable when the processor is running in user mode.
The ﬁrst is a static area that contains page-table references to every available physical page of memory in the system, so that a simple translation from physical to virtual addresses occurs when kernel code is run.
The core of the kernel, along with all pages allocated by the normal page allocator, resides in this region.
The remainder of the kernel’s reserved section of address space is not reserved for any speciﬁc purpose.
Page-table entries in this address range can be modiﬁed by the kernel to point to any other areas of memory.
The kernel provides a pair of facilities that allow kernel code to use this virtual memory.
The vmalloc() function allocates an arbitrary number of physical pages of memory that may not be physically contiguous into a single region of virtually contiguous kernel memory.
The vremap() function maps a sequence of virtual addresses to point to an area of memory used by a device driver for memory-mapped I/O.
The Linux kernel’s execution of user programs is triggered by a call to the exec() system call.
This exec() call commands the kernel to run a new program within the current process, completely overwriting the current execution context with the initial context of the new program.
The ﬁrst job of this system service is to verify that the calling process has permission rights to the ﬁle being executed.
Once that matter has been checked, the kernel invokes a loader routine to start running the program.
The loader does not necessarily load the contents of the program ﬁle into physical memory, but it does at least set up the mapping of the program into virtual memory.
There is no single routine in Linux for loading a new program.
Instead, Linux maintains a table of possible loader functions, and it gives each such function the opportunity to try loading the given ﬁle when an exec() system call is made.
Older Linux kernels understood the a.out format for binary ﬁles—a relatively simple format common on older UNIX systems.
Newer Linux systems use the more modern ELF format, now supported by most current UNIX implementations.
New sections can be added to an ELF binary (for example, to add extra debugging information) without causing the loader routines to become confused.
By allowing registration of multiple loader routines, Linux can easily support the ELF and a.out binary formats in a single running system.
The procedure for loading a.out binaries is simpler but similar in operation.
Under Linux, the binary loader does not load a binary ﬁle into physical memory.
Rather, the pages of the binary ﬁle are mapped into regions of virtual memory.
Only when the program tries to access a given page will a page fault result in the loading of that page into physical memory using demand paging.
It is the responsibility of the kernel’s binary loader to set up the initial memory mapping.
An ELF-format binary ﬁle consists of a header followed by several page-aligned sections.
The ELF loader works by reading the header and mapping the sections of the ﬁle into separate regions of virtual memory.
Figure 18.6 shows the typical layout of memory regions set up by the ELF loader.
In a reserved region at one end of the address space sits the kernel, in.
The rest of virtual memory is available to applications, which can use the kernel’s memory-mapping functions to create regions that map a portion of a ﬁle or that are available for application data.
The loader’s job is to set up the initial memory mapping to allow the execution of the program to start.
The regions that need to be initialized include the stack and the program’s text and data regions.
The stack is created at the top of the user-mode virtual memory; it grows downward toward lower-numbered addresses.
It includes copies of the arguments and environment variables given to the program in the exec() system call.
The other regions are created near the bottom end of virtual memory.
The sections of the binary ﬁle that contain program text or read-only data are mapped into memory as a write-protected region.
Writable initialized data are mapped next; then any uninitialized data are mapped in as a private demand-zero region.
Directly beyond these ﬁxed-sized regions is a variable-sized region that programs can expand as needed to hold data allocated at run time.
Each process has a pointer, brk, that points to the current extent of this data region, and processes can extend or contract their brk region with a single system call —sbrk()
Once these mappings have been set up, the loader initializes the process’s program-counter register with the starting point recorded in the ELF header, and the process can be scheduled.
Once the program has been loaded and has started running, all the necessary contents of the binary ﬁle have been loaded into the process’s virtual address.
However, most programs also need to run functions from the system libraries, and these library functions must also be loaded.
In the simplest case, the necessary library functions are embedded directly in the program’s executable binary ﬁle.
Such a program is statically linked to its libraries, and statically linked executables can commence running as soon as they are loaded.
The main disadvantage of static linking is that every program generated must contain copies of exactly the same common system library functions.
It is much more efﬁcient, in terms of both physical memory and disk-space usage, to load the system libraries into memory only once.
Linux implements dynamic linking in user mode through a special linker library.
Every dynamically linked program contains a small, statically linked function that is called when the program starts.
This static function just maps the link library into memory and runs the code that the function contains.
The link library determines the dynamic libraries required by the program and the names of the variables and functions needed from those libraries by reading the information contained in sections of the ELF binary.
It then maps the libraries into the middle of virtual memory and resolves the references to the symbols contained in those libraries.
In UNIX, a ﬁle does not have to be an object stored on disk or fetched over a network from a remote ﬁle server.
Rather, UNIX ﬁles can be anything capable of handling the input or output of a stream of data.
The Linux kernel handles all these types of ﬁles by hiding the implementation details of any single ﬁle type behind a layer of software, the virtual ﬁle system (VFS)
Here, we ﬁrst cover the virtual ﬁle system and then discuss the standard Linux ﬁle system—ext3
It has two components: a set of deﬁnitions that specify what ﬁle-system objects are allowed to look like and a layer of software to manipulate the objects.
For each of these four object types, the VFS deﬁnes a set of operations.
Every object of one of these types contains a pointer to a function table.
The function table lists the addresses of the actual functions that implement the deﬁned operations for that object.
For example, an abbreviated API for some of the ﬁle object’s operations includes:
An implementation of the ﬁle object (for a speciﬁc ﬁle type) is required to implement each function speciﬁed in the deﬁnition of the ﬁle object.
The VFS software layer can perform an operation on one of the ﬁle-system objects by calling the appropriate function from the object’s function table, without having to know in advance exactly what kind of object it is dealing with.
The VFS does not know, or care, whether an inode represents a networked ﬁle, a disk ﬁle, a network socket, or a directory ﬁle.
The appropriate function for that ﬁle’s read() operation will always be at the same place in its function table, and the VFS software layer will call that function without caring how the data are actually read.
The inode and ﬁle objects are the mechanisms used to access ﬁles.
An inode object is a data structure containing pointers to the disk blocks that contain the actual ﬁle contents, and a ﬁle object represents a point of access to the data in an open ﬁle.
A process cannot access an inode’s contents without ﬁrst obtaining a ﬁle object pointing to the inode.
The ﬁle object keeps track of where in the ﬁle the process is currently reading or writing, to keep track of sequential ﬁle I/O.
It also remembers the permissions (for example, read or write) requested when the ﬁle was opened and tracks the process’s activity if necessary to perform adaptive read-ahead, fetching ﬁle data into memory before the process requests the data, to improve performance.
File objects typically belong to a single process, but inode objects do not.
There is one ﬁle object for every instance of an open ﬁle, but always only a single inode object.
Even when a ﬁle is no longer in use by any process, its inode object may still be cached by the VFS to improve performance if the ﬁle is used again in the near future.
All cached ﬁle data are linked onto a list in the ﬁle’s inode object.
The inode also maintains standard information about each ﬁle, such as the owner, size, and time most recently modiﬁed.
Directory ﬁles are dealt with slightly differently from other ﬁles.
The UNIX programming interface deﬁnes a number of operations on directories, such as creating, deleting, and renaming a ﬁle in a directory.
The system calls for these directory operations do not require that the user open the ﬁles concerned, unlike the case for reading or writing data.
The VFS therefore deﬁnes these directory operations in the inode object, rather than in the ﬁle object.
The superblock object represents a connected set of ﬁles that form a self-contained ﬁle system.
The main responsibility of the superblock object is to provide access to inodes.
The VFS identiﬁes every inode by a unique ﬁle-system/inode number pair, and it ﬁnds the inode corresponding to a particular inode number by asking the superblock object to return the inode with that number.
Finally, a dentry object represents a directory entry, which may include the name of a directory in the path name of a ﬁle (such as /usr) or the actual ﬁle (such as stdio.h)
Each of these values is represented by a separate dentry object.
Because Linux treats directory names as ﬁles, translating this path requires ﬁrst obtaining the inode for the root/
The operating system must then read through this ﬁle to obtain the inode for the ﬁle include.
It must continue this process until it obtains the inode for the ﬁle stdio.h.
Because path-name translation can be a time-consuming task, Linux maintains a cache of dentry objects, which is consulted during path-name translation.
Obtaining the inode from the dentry cache is considerably faster than having to read the on-disk ﬁle.
The standard on-disk ﬁle system used by Linux is called ext3, for historical reasons.
The Minix ﬁle system was superseded by a new ﬁle system, which was christened the extended ﬁle system (extfs)
A later redesign to improve performance and scalability and to add a few missing features led to the second extended ﬁle system (ext2)
Further development added journaling capabilities, and the system was renamed the third extended ﬁle system (ext3)
Linux kernel developers are working on augmenting ext3 with modern ﬁle-system features such as extents.
This new ﬁle system is called the fourth extended ﬁle system (ext4)
The rest of this section discusses ext3, however, since it remains the most-deployed Linux ﬁle system.
It uses a similar mechanism for locating the data blocks belonging to a speciﬁc ﬁle, storing data-block pointers in indirect blocks throughout the ﬁle system with up to three levels of indirection.
As in FFS, directory ﬁles are stored on disk just like normal ﬁles, although their contents are interpreted differently.
Each block in a directory ﬁle consists of a linked list of entries.
In turn, each entry contains the length of the entry, the name of a ﬁle, and the inode number of the inode to which that entry refers.
The main differences between ext3 and FFS lie in their disk-allocation policies.
In FFS, the disk is allocated to ﬁles in blocks of 8 KB.
These blocks are subdivided into fragments of 1 KB for storage of small ﬁles or partially ﬁlled blocks at the ends of ﬁles.
The default block size on ext3 varies as a function of the total size of the ﬁle system.
To maintain high performance, the operating system must try to perform I/O operations in large chunks whenever possible by clustering physically adjacent I/O requests.
Clustering reduces the per-request overhead incurred by device drivers, disks, and disk-controller hardware.
A block-sized I/O request size is too small to maintain good performance, so ext3 uses allocation policies designed to place logically adjacent blocks of a ﬁle into physically adjacent blocks on disk, so that it can submit an I/O request for several disk blocks as a single operation.
Note that modern disk-drive technology packs sectors onto the disk at different densities, and thus with different cylinder sizes, depending on how far the disk head is from the center of the disk.
Therefore, ﬁxed-sized cylinder groups do not necessarily correspond to the disk’s geometry.
When allocating a ﬁle, ext3 must ﬁrst select the block group for that ﬁle.
For data blocks, it attempts to allocate the ﬁle to the block group to which the ﬁle’s inode has been allocated.
For inode allocations, it selects the block group in which the ﬁle’s parent directory resides for nondirectory ﬁles.
Directory ﬁles are not kept together but rather are dispersed throughout the available block groups.
These policies are designed not only to keep related information within the same block group but also to spread out the disk load among the disk’s block groups to reduce the fragmentation of any one area of the disk.
Within a block group, ext3 tries to keep allocations physically contiguous if possible, reducing fragmentation if it can.
It maintains a bitmap of all free blocks in a block group.
When allocating the ﬁrst blocks for a new ﬁle, it starts searching for a free block from the beginning of the block group.
When extending a ﬁle, it continues the search from the block most recently allocated to the ﬁle.
First, ext3 searches for an entire free byte in the bitmap; if it fails to ﬁnd one, it looks for any free bit.
The search for free bytes aims to allocate disk space in chunks of at least eight blocks where possible.
Once a free block has been identiﬁed, the search is extended backward until an allocated block is encountered.
When a free byte is found in the bitmap, this backward extension prevents ext3 from leaving a hole between the most recently allocated block in the previous nonzero byte and the zero byte found.
Once the next block to be allocated has been found by either bit or byte search, ext3 extends the allocation forward for up to eight blocks and preallocates these extra blocks to the ﬁle.
This preallocation helps to reduce fragmentation during interleaved writes to separate ﬁles and also reduces the CPU cost of disk allocation by allocating multiple blocks simultaneously.
The preallocated blocks are returned to the free-space bitmap when the ﬁle is closed.
Each row represents a sequence of set and unset bits in an allocation bitmap, indicating used and free blocks on disk.
In the ﬁrst case, if we can ﬁnd any free blocks sufﬁciently near the start of the search, then we allocate them no matter how fragmented.
The fragmentation is partially compensated for by the fact that the blocks are close together and can probably all be read without any disk seeks.
Furthermore, allocating them all to one ﬁle is better in the long run than allocating isolated blocks to separate ﬁles once large free areas become scarce on disk.
In the second case, we have not immediately found a free block close by, so we search forward for an entire free byte in the bitmap.
If we allocated that byte as a whole, we would end up creating a fragmented area of free space between it and the allocation preceding it.
Thus, before allocating, we back up to make this allocation ﬂush with the allocation preceding it, and then we allocate forward to satisfy the default allocation of eight blocks.
The ext3 ﬁle system supports a popular feature called journaling, whereby modiﬁcations to the ﬁle system are written sequentially to a journal.
A set of operations that performs a speciﬁc task is a transaction.
Once a transaction is written to the journal, it is considered to be committed.
Meanwhile, the journal entries relating to the transaction are replayed across the actual ﬁlesystem structures.
As the changes are made, a pointer is updated to indicate which actions have completed and which are still incomplete.
When an entire committed transaction is completed, it is removed from the journal.
The journal, which is actually a circular buffer, may be in a separate section of the ﬁle system, or it may even be on a separate disk spindle.
It is more efﬁcient, but more complex, to have it under separate read–write heads, thereby decreasing head contention and seek times.
If the system crashes, some transactions may remain in the journal.
Those transactions were never completed to the ﬁle system even though they were committed by the operating system, so they must be completed once the system.
The transactions can be executed from the pointer until the work is complete, and the ﬁle-system structures remain consistent.
The only problem occurs when a transaction has been aborted—that is, it was not committed before the system crashed.
Any changes from those transactions that were applied to the ﬁle system must be undone, again preserving the consistency of the ﬁle system.
This recovery is all that is needed after a crash, eliminating all problems with consistency checking.
Journaling ﬁle systems may perform some operations faster than nonjournaling systems, as updates proceed much faster when they are applied to the in-memory journal rather than directly to the on-disk data structures.
The reason for this improvement is found in the performance advantage of sequential I/O over random I/O.
Costly synchronous random writes to the ﬁle system are turned into much less costly synchronous sequential writes to the ﬁle system’s journal.
Those changes, in turn, are replayed asynchronously via random writes to the appropriate structures.
The overall result is a signiﬁcant gain in performance of ﬁle-system metadata-oriented operations, such as ﬁle creation and deletion.
Due to this performance improvement, ext3 can be conﬁgured to journal only metadata and not ﬁle data.
The ﬂexibility of the Linux VFS enables us to implement a ﬁle system that does not store data persistently at all but rather provides an interface to some other functionality.
The Linux process ﬁle system, known as the /proc ﬁle system, is an example of a ﬁle system whose contents are not actually stored anywhere but are computed on demand according to user ﬁle I/O requests.
SVR4 UNIX introduced a /proc ﬁle system as an efﬁcient interface to the kernel’s process debugging support.
Each subdirectory of the ﬁle system corresponded not to a directory on any disk but rather to an active process on the current system.
A listing of the ﬁle system reveals one directory per process, with the directory name being the ASCII decimal representation of the process’s unique process identiﬁer (PID)
Linux implements such a /proc ﬁle system but extends it greatly by adding a number of extra directories and text ﬁles under the ﬁle system’s root directory.
These new entries correspond to various statistics about the kernel and the associated loaded drivers.
The /proc ﬁle system provides a way for programs to access this information as plain text ﬁles; the standard UNIX user environment provides powerful tools to process such ﬁles.
For example, in the past, the traditional UNIX ps command for listing the states of all running processes has been implemented as a privileged process that reads the process state directly from the kernel’s virtual memory.
Under Linux, this command is implemented as an entirely unprivileged program that simply parses and formats the information from /proc.
The /proc ﬁle system must implement two things: a directory structure and the ﬁle contents within.
Because a UNIX ﬁle system is deﬁned as a set of ﬁle and directory inodes identiﬁed by their inode numbers, the /proc ﬁle system must deﬁne a unique and persistent inode number for each directory and the associated ﬁles.
Once such a mapping exists, the ﬁle system can use this inode number to identify just what operation is required when a user tries to read from a particular ﬁle inode or to perform a lookup in a particular directory.
When data are read from one of these ﬁles, the /proc ﬁle system will collect the appropriate information, format it into textual form, and place it into the requesting process’s read buffer.
The mapping from inode number to information type splits the inode number into two ﬁelds.
The top 16 bits of the inode number are interpreted as a PID, and the remaining bits deﬁne what type of information is being requested about that process.
Separate global ﬁles exist in /proc to report information such as the kernel version, free memory, performance statistics, and drivers currently running.
Not all the inode numbers in this range are reserved.
The kernel can allocate new /proc inode mappings dynamically, maintaining a bitmap of allocated inode numbers.
It also maintains a tree data structure of registered global /proc ﬁle-system entries.
Each entry contains the ﬁle’s inode number, ﬁle name, and access permissions, along with the special functions used to generate the ﬁle’s contents.
Drivers can register and deregister entries in this tree at any time, and a special section of the tree—appearing under the /proc/sys directory —is reserved for kernel variables.
Files under this tree are managed by a set of common handlers that allow both reading and writing of these variables, so a system administrator can tune the value of kernel parameters simply by writing out the new desired values in ASCII decimal to the appropriate ﬁle.
To allow efﬁcient access to these variables from within applications, the /proc/sys subtree is made available through a special system call, sysctl(), that reads and writes the same variables in binary, rather than in text, without the overhead of the ﬁle system.
To the user, the I/O system in Linux looks much like that in any UNIX system.
That is, to the extent possible, all device drivers appear as normal ﬁles.
Users can open an access channel to a device in the same way they open any other ﬁle—devices can appear as objects within the ﬁle system.
The system administrator can create special ﬁles within a ﬁle system that contain references to a speciﬁc device driver, and a user opening such a ﬁle will be able to read from and write to the device referenced.
By using the normal ﬁle-protection system, which determines who can access which ﬁle, the administrator can set access permissions for each device.
Linux splits all devices into three classes: block devices, character devices, and network devices.
Figure 18.8 illustrates the overall structure of the devicedriver system.
Block devices include all devices that allow random access to completely independent, ﬁxed-sized blocks of data, including hard disks and ﬂoppy disks, CD-ROMs and Blu-ray discs, and ﬂash memory.
Applications can also access these block devices directly if they wish.
For example, a database application may prefer to perform its own ﬁne-tuned layout of data onto a disk rather than using the general-purpose ﬁle system.
Character devices include most other devices, such as mice and keyboards.
The fundamental difference between block and character devices is random access—block devices are accessed randomly, while character devices are accessed serially.
For example, seeking to a certain position in a ﬁle might be supported for a DVD but makes no sense for a pointing device such as a mouse.
Network devices are dealt with differently from block and character devices.
Instead, they must communicate indirectly by opening a connection to the kernel’s networking subsystem.
We discuss the interface to network devices separately in Section 18.10
Block devices provide the main interface to all disk devices in a system.
Performance is particularly important for disks, and the block-device system must provide functionality to ensure that disk access is as fast as possible.
This functionality is achieved through the scheduling of I/O operations.
In the context of block devices, a block represents the unit with which the kernel performs I/O.
When a block is read into memory, it is stored in a buffer.
The request manager is the layer of software that manages the reading and writing of buffer contents to and from a block-device driver.
A separate list of requests is kept for each block-device driver.
The request lists are maintained in sorted order of increasing starting-sector number.
When a request is accepted for processing by a block-device driver, it is not removed from the list.
It is removed only after the I/O is complete, at which point the driver continues with the next request in the list, even if new requests have been inserted in the list before the active.
As new I/O requests are made, the request manager attempts to merge requests in the lists.
Linux kernel version 2.6 introduced a new I/O scheduling algorithm.
Although a simple elevator algorithm remains available, the default I/O scheduler is now the Completely Fair Queueing (CFQ) scheduler.
The CFQ I/O scheduler is fundamentally different from elevator-based algorithms.
Instead of sorting requests into a list, CFQ maintains a set of lists—by default, one for each process.
Requests originating from a process go in that process’s list.
For example, if two processes are issuing I/O requests, CFQ will maintain two separate lists of requests, one for each process.
Where a traditional C-SCAN algorithm is indifferent to a speciﬁc process, CFQ services each process’s list round-robin.
It pulls a conﬁgurable number of requests (by default, four) from each list before moving on to the next.
This method results in fairness at the process level—each process receives an equal fraction of the disk’s bandwidth.
The result is beneﬁcial with interactive workloads where I/O latency is important.
A character-device driver can be almost any device driver that does not offer random access to ﬁxed blocks of data.
Any character-device drivers registered to the Linux kernel must also register a set of functions that implement the ﬁle I/O operations that the driver can handle.
The kernel performs almost no preprocessing of a ﬁle read or write request to a character device.
It simply passes the request to the device in question and lets the device deal with the request.
The main exception to this rule is the special subset of character-device drivers that implement terminal devices.
The kernel maintains a standard interface to these drivers by means of a set of tty struct structures.
Each of these structures provides buffering and ﬂow control on the data stream from the terminal device and feeds those data to a line discipline.
A line discipline is an interpreter for the information from the terminal device.
The most common line discipline is the tty discipline, which glues the terminal’s data stream onto the standard input and output streams of a user’s running processes, allowing those processes to communicate directly with the user’s terminal.
This job is complicated by the fact that several such processes may be running simultaneously, and the tty line discipline is responsible for attaching and detaching the terminal’s input and output from the various processes connected to it as those processes are suspended or awakened by the user.
Other line disciplines also are implemented that have nothing to do with I/O to a user process.
The PPP and SLIP networking protocols are ways of encoding a networking connection over a terminal device such as a serial line.
These protocols are implemented under Linux as drivers that at one end appear to the terminal system as line disciplines and at the other end appear to the networking system as network-device drivers.
After one of these line disciplines has been enabled on a terminal device, any data appearing on that terminal will be routed directly to the appropriate network-device driver.
Linux provides a rich environment for processes to communicate with each other.
Communication may be just a matter of letting another process know that some event has occurred, or it may involve transferring data from one process to another.
The standard Linux mechanism for informing a process that an event has occurred is the signal.
Signals can be sent from any process to any other process, with restrictions on signals sent to processes owned by another user.
However, a limited number of signals are available, and they cannot carry information.
Only the fact that a signal has occurred is available to a process.
For example, it can send a signal to a server process when data arrive on a network channel, to a parent process when a child terminates, or to a waiting process when a timer expires.
Internally, the Linux kernel does not use signals to communicate with processes running in kernel mode.
If a kernel-mode process is expecting an event to occur, it will not use signals to receive notiﬁcation of that event.
Rather, communication about incoming asynchronous events within the kernel takes place through the use of scheduling states and wait queue structures.
These mechanisms allow kernel-mode processes to inform one another about relevant events, and they also allow events to be generated by device drivers or by the networking system.
Whenever a process wants to wait for some event to complete, it places itself on a wait queue associated with that event and tells the scheduler that it is no longer eligible for execution.
Once the event has completed, every process on the wait queue will be awoken.
This procedure allows multiple processes to wait for a single event.
For example, if several processes are trying to read a ﬁle from a disk, then they will all be awakened once the data have been read into memory successfully.
Although signals have always been the main mechanism for communicating asynchronous events among processes, Linux also implements the semaphore mechanism of System V UNIX.
A process can wait on a semaphore as easily as it can wait for a signal, but semaphores have two advantages: large numbers of semaphores can be shared among multiple independent processes, and operations on multiple semaphores can be performed atomically.
Internally, the standard Linux wait queue mechanism synchronizes processes that are communicating with semaphores.
The standard UNIX pipe mechanism allows a child process to inherit a communication channel from its parent; data written to one end of the pipe can be read at the other.
Under Linux, pipes appear as just another type of inode to virtual ﬁle system software, and each pipe has a pair of wait queues to synchronize the reader and writer.
Another process communications method, shared memory, offers an extremely fast way to communicate large or small amounts of data.
Any data written by one process to a shared memory region can be read immediately by any other process that has mapped that region into its address space.
The main disadvantage of shared memory is that, on its own, it offers no synchronization.
A process can neither ask the operating system whether a piece of shared memory has been written to nor suspend execution until such a write occurs.
A shared-memory region in Linux is a persistent object that can be created or deleted by processes.
Such an object is treated as though it were a small, independent address space.
The Linux paging algorithms can elect to page shared-memory pages out to disk, just as they can page out a process’s data pages.
The shared-memory object acts as a backing store for shared-memory regions, just as a ﬁle can act as a backing store for a memory-mapped memory region.
When a ﬁle is mapped into a virtual address space region, then any page faults that occur cause the appropriate page of the ﬁle to be mapped into virtual memory.
Similarly, shared-memory mappings direct page faults to map in pages from a persistent shared-memory object.
Also just as for ﬁles, sharedmemory objects remember their contents even if no processes are currently mapping them into virtual memory.
Not only does Linux support the standard Internet protocols used for most UNIX-to-UNIX communications, but it also implements a number of protocols native to other, non-UNIX operating systems.
In particular, since Linux was originally implemented primarily on PCs, rather than on large workstations or on server-class systems, it supports many of the protocols typically used on PC networks, such as AppleTalk and IPX.
Internally, networking in the Linux kernel is implemented by three layers of software:
User applications perform all networking requests through the socket interface.
This interface is designed to look like the 4.3 BSD socket layer, so that any programs designed to make use of Berkeley sockets will run on Linux without any source-code changes.
The BSD socket interface is sufﬁciently general to represent network addresses for a wide range of networking protocols.
This single interface is used in Linux to access not just those protocols implemented on standard BSD systems but all the protocols supported by the system.
The next layer of software is the protocol stack, which is similar in organization to BSD’s own framework.
Whenever any networking data arrive at this layer, either from an application’s socket or from a network-device driver, the data are expected to have been tagged with an identiﬁer specifying which network protocol they contain.
Protocols can communicate with one another if they desire; for example, within the Internet protocol set, separate protocols manage routing, error reporting, and reliable retransmission of lost data.
The protocol layer may rewrite packets, create new packets, split or reassemble packets into fragments, or simply discard incoming data.
Ultimately, once the protocol layer has ﬁnished processing a set of packets, it passes them on, either upward to the socket interface if the data are destined for a local connection or downward to a device driver if the data need to be transmitted remotely.
The protocol layer decides to which socket or device it will send the packet.
All communication between the layers of the networking stack is performed by passing single skbuff (socket buffer) structures.
Each of these structures contains a set of pointers into a single continuous area of memory, representing a buffer inside which network packets can be constructed.
The valid data in a skbuff do not need to start at the beginning of the skbuff’s buffer, and they do not need to run to the end.
The networking code can add data to or trim data from either end of the packet, as long as the result still ﬁts into the skbuff.
This capacity is especially important on modern microprocessors, where improvements in CPU speed have far outstripped the performance of main memory.
The skbuff architecture allows ﬂexibility in manipulating packet headers and checksums while avoiding any unnecessary data copying.
The most important set of protocols in the Linux networking system is the TCP/IP protocol suite.
The IP protocol implements routing between different hosts anywhere on the network.
On top of the routing protocol are the UDP, TCP, and ICMP protocols.
The TCP protocol implements reliable connections between hosts with guaranteed in-order delivery of packets and automatic retransmission of lost data.
The ICMP protocol carries various error and status messages between hosts.
Each packet (skbuff) arriving at the networking stack’s protocol software is expected to be already tagged with an internal identiﬁer indicating the protocol to which the packet is relevant.
Different networking-device drivers encode the protocol type in different ways; thus, the protocol for incoming data must be identiﬁed in the device driver.
New protocols can be added to the hash table as kernel-loadable modules.
It performs the routing decision using two tables: the persistent forwarding information base (FIB) and a cache of recent routing decisions.
The FIB is organized as a set of hash tables indexed by destination address; the tables representing the most speciﬁc routes are always searched ﬁrst.
Successful lookups from this table are added to the route-caching table, which caches routes only by speciﬁc destination.
No wildcards are stored in the cache, so lookups can be made quickly.
An entry in the route cache expires after a ﬁxed period with no hits.
The ﬁrewall manager maintains a number of separate ﬁrewall chains and allows a skbuff to be matched against any chain.
Chains are reserved for separate purposes: one is used for forwarded packets, one for packets being input to this host, and one for data generated at this host.
Each chain is held as an ordered list of rules, where a rule speciﬁes one of a number of possible ﬁrewall-decision functions plus some arbitrary data for matching purposes.
Two other functions performed by the IP driver are disassembly and reassembly of large packets.
If an outgoing packet is too large to be queued to a device, it is simply split up into smaller fragments, which are all queued to the driver.
The IP driver maintains an ipfrag object for each fragment awaiting reassembly and an ipq for each datagram being assembled.
If a match is found, the fragment is added to it; otherwise, a new ipq is created.
Once the ﬁnal fragment has arrived for a ipq, a completely new skbuff is constructed to hold the new packet, and this packet is passed back into the IP driver.
Packets identiﬁed by the IP as destined for this host are passed on to one of the other protocol drivers.
The UDP and TCP protocols share a means of associating packets with source and destination sockets: each connected pair of sockets is uniquely identiﬁed by its source and destination addresses and by the source and destination port numbers.
The socket lists are linked to hash tables keyed on these four address and port values for socket lookup on incoming packets.
The TCP protocol has to deal with unreliable connections, so it maintains ordered lists of unacknowledged outgoing packets to retransmit after a timeout and of incoming out-of-order packets to be presented to the socket when the missing data have arrived.
Linux’s security model is closely related to typical UNIX security mechanisms.
Making sure that nobody can access the system without ﬁrst proving that she has entry rights.
Providing a mechanism for checking whether a user has the right to access a certain object and preventing access to objects as required.
Authentication in UNIX has typically been performed through the use of a publicly readable password ﬁle.
A user’s password is combined with a random “salt” value, and the result is encoded with a one-way transformation function and stored in the password ﬁle.
The use of the one-way function means that the original password cannot be deduced from the password ﬁle except by trial and error.
When a user presents a password to the system, the password is recombined with the salt value stored in the password ﬁle and passed through the same one-way transformation.
If the result matches the contents of the password ﬁle, then the password is accepted.
Historically, UNIX implementations of this mechanism have had several drawbacks.
Passwords were often limited to eight characters, and the number of possible salt values was so low that an attacker could easily combine a dictionary of commonly used passwords with every possible salt value and have a good chance of matching one or more passwords in the password ﬁle, gaining unauthorized access to any accounts compromised as a result.
Extensions to the password mechanism have been introduced that keep the encrypted password secret in a ﬁle that is not publicly readable, that allow longer passwords, or that use more secure methods of encoding the password.
Other authentication mechanisms have been introduced that limit the periods during which a user is permitted to connect to the system.
Also, mechanisms exist to distribute authentication information to all the related systems in a network.
A new security mechanism has been developed by UNIX vendors to address authentication problems.
The pluggable authentication modules (PAM) system is based on a shared library that can be used by any system component that needs to authenticate users.
If a new authentication mechanism is added at a later date, it can be added to the conﬁguration ﬁle, and all system components will immediately be able to take advantage of it.
Access control under UNIX systems, including Linux, is performed through the use of unique numeric identiﬁers.
A user identiﬁer (UID) identiﬁes a single user or a single set of access rights.
A group identiﬁer (GID) is an extra identiﬁer that can be used to identify rights belonging to more than one user.
Access control is applied to various objects in the system.
Every ﬁle available in the system is protected by the standard access-control mechanism.
In addition, other shared objects, such as shared-memory sections and semaphores, employ the same access system.
Every object in a UNIX system under user and group access control has a single UID and a single GID associated with it.
User processes also have a single UID, but they may have more than one GID.
If a process’s UID matches the UID of an object, then the process has user rights or owner rights to that object.
If the UIDs do not match but any GID of the process matches the object’s GID, then group rights are conferred; otherwise, the process has world rights to the object.
Linux performs access control by assigning objects a protection mask that speciﬁes which access modes—read, write, or execute—are to be granted to processes with owner, group, or world access.
Thus, the owner of an object might have full read, write, and execute access to a ﬁle; other users in a certain group might be given read access but denied write access; and everybody else might be given no access at all.
A process with this special UID is granted automatic access to any object in the system, bypassing normal access checks.
Such processes are also granted permission to perform privileged operations, such as reading any physical memory or opening reserved network sockets.
This mechanism allows the kernel to prevent normal users from accessing these resources: most of the kernel’s key internal resources are implicitly owned by the root UID.
Linux implements the standard UNIX setuid mechanism described in Section A.3.2
This mechanism allows a program to run with privileges different from those of the user running the program.
For example, the lpr program (which submits a job to a print queue) has access to the system’s print queues even if the user running that program does not.
The UNIX implementation of setuid distinguishes between a process’s real and effective UID.
The real UID is that of the user running the program; the effective UID is that of the ﬁle’s owner.
First, Linux implements the POSIX speciﬁcation’s saved user-id mechanism, which allows a process to drop and reacquire its effective UID repeatedly.
For security reasons, a program may want to perform most of its operations in a safe mode, waiving the privileges granted by its setuid status; but it may wish to perform selected operations with all its privileges.
Standard UNIX implementations achieve this capacity only by swapping the real and effective UIDs.
When this is done, the previous effective UID is remembered, but the program’s real UID does not always correspond to the UID of the user running the program.
Saved UIDs allow a process to set its effective UID to its real UID and then return to the previous value of its effective UID without having to modify the real UID at any time.
The second enhancement provided by Linux is the addition of a process characteristic that grants just a subset of the rights of the effective UID.
The fsuid and fsgid process properties are used when access rights are granted to ﬁles.
The appropriate property is set every time the effective UID or GID is set.
However, the fsuid and fsgid can be set independently of the effective ids, allowing a process to access ﬁles on behalf of another user without taking on the identity of that other user in any other way.
Speciﬁcally, server processes can use this mechanism to serve ﬁles to a certain user without becoming vulnerable to being killed or suspended by that user.
Finally, Linux provides a mechanism for ﬂexible passing of rights from one program to another—a mechanism that has become common in modern versions of UNIX.
When a local network socket has been set up between any two processes on the system, either of those processes may send to the other process a ﬁle descriptor for one of its open ﬁles; the other process receives a.
This mechanism allows a client to pass access to a single ﬁle selectively to some server process without granting that process any other privileges.
For example, it is no longer necessary for a print server to be able to read all the ﬁles of a user who submits a new print job.
The print client can simply pass the server ﬁle descriptors for any ﬁles to be printed, denying the server access to any of the user’s other ﬁles.
Linux is a modern, free operating system based on UNIX standards.
It has been designed to run efﬁciently and reliably on common PC hardware; it also runs on a variety of other platforms, such as mobile phones.
It provides a programming interface and user interface compatible with standard UNIX systems and can run a large number of UNIX applications, including an increasing number of commercially supported applications.
A complete Linux system includes many components that were developed independently of Linux.
The core Linux operating-system kernel is entirely original, but it allows much existing free UNIX software to run, resulting in an entire UNIX-compatible operating system free from proprietary code.
The Linux kernel is implemented as a traditional monolithic kernel for performance reasons, but it is modular enough in design to allow most drivers to be dynamically loaded and unloaded at run time.
Linux is a multiuser system, providing protection between processes and running multiple processes according to a time-sharing scheduler.
Newly created processes can share selective parts of their execution environment with their parent processes, allowing multithreaded programming.
Interprocess communication is supported by both System V mechanisms—message queues, semaphores, and shared memory—and BSD’s socket interface.
Multiple networking protocols can be accessed simultaneously through the socket interface.
The memory-management system uses page sharing and copy-on-write to minimize the duplication of data shared by different processes.
Pages are loaded on demand when they are ﬁrst referenced and are paged back out to backing store according to an LFU algorithm if physical memory needs to be reclaimed.
To the user, the ﬁle system appears as a hierarchical directory tree that obeys UNIX semantics.
Internally, Linux uses an abstraction layer to manage multiple ﬁle systems.
Device-oriented ﬁle systems access disk storage through a page cache that is uniﬁed with the virtual memory system.
Describe three different ways to implement threads, and compare these three methods with the Linux clone() mechanism.
When might using each alternative mechanism be better or worse than using clones?
What effect does this restriction have on the kernel’s design? What are two advantages and two disadvantages of this design decision?
What are the advantages of each method? When might each be preferred?
Why do they do so? Of what hardware characteristics does sequential access take advantage? Why is rotational optimization no longer so useful?
What features necessary for certain real-time programming tasks are missing? How might they be added to the kernel? What are the costs (downsides) of such features?
What is the advantage of keeping this functionality out of the kernel? Are there any drawbacks? Explain your answer.
How might the need to support different ﬁle-system types affect the structure of the Linux kernel?
What are three implications of this availability for the security of the Linux system?
The Linux system is a product of the Internet; as a result, much of the available documentation on Linux is available in some form on the Internet.
The following key sites reference most of the useful information available:
The Linux Cross-Reference Page (LXR) (http://lxr.linux.no) maintains current listings of the Linux kernel, browsable via the Web and fully crossreferenced.
The Kernel Hackers’ Guide provides a helpful overview of the Linux kernel components and internals and is located at http://tldp.org/LDP/tlk/tlk.html.
The Linux Weekly News (LWN) (http://lwn.net) provides weekly Linuxrelated news, including a very well researched subsection on Linux kernel news.
Send e-mail to this address with the single line “help” in the mail’s body for information on how to access the list server and to subscribe to any lists.
Finally, the Linux system itself can be obtained over the Internet.
Complete Linux distributions are available from the home sites of the companies concerned, and the Linux community also maintains archives of current system components at several places on the Internet.
In this chapter, we discuss the key goals of Windows 7, the layered architecture of the system that has made it so easy to use, the ﬁle system, the networking features, and the programming interface.
To explore the principles underlying Windows 7’s design and the speciﬁc components of the system.
To provide a detailed discussion of the Windows 7 ﬁle system.
To describe the interface available in Windows 7 to system and application programmers.
October 1988, Dave Cutler, the architect of the DEC VAX/VMS operating system, was hired and given the charter of building Microsoft’s new operating system.
In addition, user-interface routines and all graphics code were moved into the kernel to improve performance, with the side effect of decreased system reliability.
It added Active Directory (an X.500-based directory service), better networking and laptop support, support for plug-and-play devices, a distributed ﬁle system, and support for more processors and more memory.
Windows XP updated the graphical user interface (GUI) with a visual design that took advantage of more recent hardware advances and many new ease-of-use features.
Numerous features were added to automatically repair problems in applications and the operating system itself.
The long-awaited update to Windows XP, called Windows Vista, was released in November 2006, but it was not well received.
Although Windows Vista included many improvements that later showed up in Windows 7, these improvements were overshadowed by Windows Vista’s perceived sluggishness and compatibility problems.
Microsoft responded to criticisms of Windows Vista by improving its engineering processes and working more closely with the makers of Windows hardware and applications.
Among the signiﬁcant engineering changes is the increased use of execution tracing rather than counters or proﬁling to analyze system behavior.
Tracing runs constantly in the system, watching hundreds of scenarios execute.
When one of these scenarios fails, or when it succeeds but does not perform well, the traces can be analyzed to determine the cause.
The subsystem architecture allows enhancements to be made to one operating-system personality without affecting the application compatibility of the other.
Windows 7 is a multiuser operating system, supporting simultaneous access through distributed services or through multiple instances of the GUI via the Windows terminal services.
The server editions of Windows 7 support simultaneous terminal server sessions from Windows desktop systems.
The desktop editions of terminal server multiplex the keyboard, mouse, and monitor between virtual terminal sessions for each logged-on user.
This feature, called fast user switching, allows users to preempt each other at the console of a PC without having to log off and log on.
We noted earlier that some GUI implementation moved into kernel mode in Windows NT 4.0
It started to move into user mode again with Windows Vista, which included the desktop window manager (DWM) as a user-mode process.
DirectX continues to run in the kernel, as does the code implementing Windows’ previous windowing and graphics models (Win32k and GDI)
Windows 7 made substantial changes to the DWM, signiﬁcantly reducing its memory footprint and improving its performance.
However, 64-bit editions of Windows also support much larger physical memories.
They are based on the same core components and run the same binary ﬁles for the kernel and most drivers.
Similarly, although Microsoft ships a variety of different editions of each release to address different market price points, few of the differences between editions are reﬂected in the core of the system.
Microsoft’s design goals for Windows included security, reliability, Windows and POSIX application compatibility, high performance, extensibility, portability, and international support.
Some additional goals, energy efﬁciency and dynamic device support, have recently been added to this list.
Extensive code review and testing were combined with sophisticated automatic analysis tools to identify and investigate potential defects that might represent security vulnerabilities.
System objects, including ﬁles, registry settings, and kernel objects, are protected by accesscontrol lists (ACLs) (see Section 11.6.2)
ACLs are vulnerable to user and programmer errors, however, as well as to the most common attacks on consumer systems, in which the user is tricked into running code, often while browsing the Web.
Windows 7 includes a mechanism called integrity levels that acts as a rudimentary capability system for controlling access.
Objects and processes are marked as having low, medium, or high integrity.
Windows does not allow a process to modify an object with a higher integrity level, no matter what the setting of the ACL.
Other security measures include address-space layout randomization (ASLR), nonexecutable stacks and heaps, and encryption and digital signature facilities.
This safeguard makes it likely that a system under attack will fail or crash rather than let the attacking code take control.
Recent chips from both Intel and AMD are based on the AMD64 architecture, which allows memory pages to be marked so that they cannot contain executable instruction code.
Windows tries to mark stacks and memory heaps so that they cannot be used to execute code, thus preventing attacks in which a program bug allows a buffer to overﬂow and then is tricked into executing the contents of the buffer.
This technique cannot be applied to all programs, because some rely on modifying data and executing it.
A column labeled “data execution prevention” in the Windows task manager shows which processes are marked to prevent these attacks.
Windows uses encryption as part of common protocols, such as those used to communicate securely with websites.
Encryption is also used to protect user ﬁles stored on disk from prying eyes.
Windows 7 allows users to easily encrypt virtually a whole disk, as well as removable storage devices such as USB ﬂash drives, with a feature called BitLocker.
If a computer with an encrypted disk is stolen, the thieves will need very sophisticated technology (such as an electron microscope) to gain access to any of the computer’s ﬁles.
Windows uses digital signatures to sign operating system binaries so it can verify that the ﬁles were produced by Microsoft or another known company.
In some editions of Windows, a code integrity module is activated at boot to ensure that all the loaded modules in the kernel have valid signatures, assuring that they have not been tampered with by an off-line attack.
At the same time, its reliability increased due to such factors as maturity in the source code, extensive stress testing of the system, improved CPU architectures, and automatic detection of many serious errors in drivers.
Other improvements in reliability have resulted from moving more code out of the kernel and into user-mode services.
Windows provides extensive support for writing drivers in user mode.
System facilities that were once in the kernel and are now in user mode include the Desktop Window Manager and much of the software stack for audio.
One of the most signiﬁcant improvements in the Windows experience came from adding memory diagnostics as an option at boot time.
This addition is especially valuable because so few consumer PCs have errorcorrecting memory.
When bad RAM starts to drop bits here and there, the result is frustratingly erratic behavior in the system.
The availability of memory diagnostics has greatly reduced the stress levels of users with bad RAM.
The heap learns from application crashes and automatically inserts mitigations into future execution of an application that has crashed.
This makes the application more reliable even if it contains common bugs such as using memory after freeing it or accessing past the end of the allocation.
Achieving high reliability in Windows is particularly challenging because almost one billion computers run Windows.
Even reliability problems that affect only a small percentage of users still impact tremendous numbers of human beings.
The complexity of the Windows ecosystem also adds to the challenges.
Millions of instances of applications, drivers, and other software are being constantly downloaded and run on Windows systems.
Of course, there is also a constant stream of malware attacks.
As Windows itself has become harder to attack directly, exploits increasingly target popular applications.
To cope with these challenges, Microsoft is increasingly relying on communications from customer machines to collect large amounts of data from the ecosystem.
Machines can be sampled to see how they are performing, what software they are running, and what problems they are encountering.
Customers can send data to Microsoft when systems or software crashes or hangs.
This constant stream of data from customer machines is collected very carefully, with the users’ consent and without invading privacy.
The result is that Microsoft is building an ever-improving picture of what is happening in the Windows ecosystem that allows continuous improvements through software updates, as well as providing data to guide future releases of Windows.
The requirements for Windows XP included a much higher compatibility with the consumer applications that ran on Windows 95/98
Application compatibility is difﬁcult to achieve because many applications check for a particular version of Windows, may depend to some extent on the quirks of the implementation of APIs, may have latent application bugs that were masked in the previous system, and so.
Applications may also have been compiled for a different instruction set.
Windows 7 implements several strategies to run applications despite incompatibilities.
This layer makes Windows 7 look (almost) bug-for-bug compatible with previous versions of Windows.
The Windows subsystem model allows multiple operating-system personalities to be supported.
This allows applications to get bug-for-bug compatibility with Windows XP.
Windows was designed to provide high performance on desktop systems (which are largely constrained by I/O performance), server systems (where the CPU is often the bottleneck), and large multithreaded and multiprocessor environments (where locking performance and cache-line management are keys to scalability)
To satisfy performance requirements, NT used a variety of techniques, such as asynchronous I/O, optimized protocols for networks, kernel-based graphics rendering, and sophisticated caching of ﬁle-system data.
The memory-management and synchronization algorithms were designed with an awareness of the performance considerations related to cache lines and multiprocessors.
The subsystems that constitute Windows NT communicate with one another efﬁciently through a local procedure call (LPC) facility that provides high-performance message passing.
When a thread requests a synchronous service from another process through an LPC, the servicing thread is marked ready, and its priority is temporarily boosted to avoid the scheduling delays that would occur if it had to wait for threads already in the queue.
Windows XP further improved performance by reducing the code-path length in critical functions, using better algorithms and per-processor data structures, using memory coloring for non-uniform memory access (NUMA) machines, and implementing more scalable locking protocols, such as queued spinlocks.
The new locking protocols helped reduce system bus cycles and included lock-free lists and queues, atomic read–modify–write operations (like interlocked increment), and other advanced synchronization techniques.
By the time Windows 7 was developed, several major changes had come to computing.
Client/server computing had increased in importance, so an advanced local procedure call (ALPC) facility was introduced to provide higher performance and more reliability than LPC.
The number of CPUs and the amount of physical memory available in the largest multiprocessors had increased substantially, so quite a lot of effort was put into improving operating-system scalability.
The implementation of SMP in Windows NT used bitmasks to represent collections of processors and to identify, for example, which set of processors a particular thread could be scheduled on.
Windows 7 added the concept of processor groups to represent arbitrary numbers of CPUs, thus accommodating more CPU cores.
The number of CPU cores within single systems has continued to increase not only because of more cores but also because of cores that support more than one logical thread of execution at a time.
All these additional CPUs created a great deal of contention for the locks used for scheduling CPUs and memory.
For example, before Windows 7, a single lock was used by the Windows scheduler to synchronize access to the queues containing threads waiting for events.
In Windows 7, each object has its own lock, allowing the queues to be accessed concurrently.
Also, many execution paths in the scheduler were rewritten to be lock-free.
This change resulted in good scalability performance for Windows even on systems with 256 hardware threads.
Other changes are due to the increasing importance of support for parallel computing.
For years, the computer industry has been dominated by Moore’s Law, leading to higher densities of transistors that manifest themselves as faster clock rates for each CPU.
Moore’s Law continues to hold true, but limits have been reached that prevent CPU clock rates from increasing further.
Instead, transistors are being used to build more and more CPUs into each chip.
New programming models for achieving parallel execution, such as Microsoft’s Concurrency RunTime (ConcRT) and Intel’s Threading Building Blocks (TBB), are being used to express parallelism in C++ programs.
Where Moore’s Law has governed computing for forty years, it now seems that Amdahl’s Law, which governs parallel computing, will rule the future.
To support task-based parallelism, Windows 7 provides a new form of user-mode scheduling (UMS)
The advent of multiple CPUs on the smallest computers is only part of the shift taking place to parallel computing.
Graphics processing units (GPUs) accelerate the computational algorithms needed for graphics by using SIMD architectures to execute a single instruction for multiple data at the same time.
This has given rise to the use of GPUs for general computing, not just graphics.
Operating-system support for software like OpenCL and CUDA is allowing programs to take advantage of the GPUs.
Windows supports use of GPUs through software in its DirectX graphics support.
This software, called DirectCompute, allows programs to specify computational kernels using the same HLSL (high-level shader language) programming model used to program the SIMD hardware for graphics shaders.
Extensibility refers to the capacity of an operating system to keep up with advances in computing technology.
To facilitate change over time, the developers implemented Windows using a layered architecture.
The Windows executive runs in kernel mode and provides the basic system services and abstractions that support shared use of the system.
On top of the executive, several server subsystems operate in user mode.
Among them are environmental subsystems that emulate different operating systems.
Thus, programs written for the Win32 APIs and POSIX all run on Windows in the appropriate environment.
Because of the modular structure, additional environmental subsystems can be added without affecting the executive.
In addition, Windows uses loadable drivers in the I/O system, so new ﬁle systems, new kinds of I/O devices, and new kinds of networking can be added while the system is running.
Windows uses a client–server model like the Mach operating system and supports distributed processing by remote procedure calls (RPCs) as deﬁned by the Open Software Foundation.
An operating system is portable if it can be moved from one CPU architecture to another with relatively few changes.
Like the UNIX operating system, Windows is written primarily in C and C++
Porting Windows to a new architecture mostly affects the Windows kernel, since the user-mode code in Windows is almost exclusively written to be architecture independent.
The entire Windows system must then be recompiled for the new CPU instruction set.
Operating systems are sensitive not only to CPU architecture but also to CPU support chips and hardware boot programs.
The CPU and support chips are collectively known as a chipset.
These chipsets and the associated boot code determine how interrupts are delivered, describe the physical characteristics of each system, and provide interfaces to deeper aspects of the CPU architecture, such as error recovery and power management.
It would be burdensome to have to port Windows to each type of support chip as well as to each CPU architecture.
The Windows kernel depends on the HAL interfaces rather than on the underlying chipset details.
This allows the single set of kernel and driver binaries for a particular CPU to be used with different chipsets simply by loading a different version of the HAL.
The NLS API provides specialized routines to format dates, time, and money in accordance with national customs.
String comparisons are specialized to account for varying character sets.
System text strings are kept in resource ﬁles that can be replaced to localize the system for different languages.
Multiple locales can be used concurrently, which is important to multilingual individuals and businesses.
Increasing energy efﬁciency for computers causes batteries to last longer for laptops and netbooks, saves signiﬁcant operating costs for power and cooling of data centers, and contributes to green initiatives aimed at lowering energy consumption by businesses and consumers.
For some time, Windows has implemented several strategies for decreasing energy use.
The CPUs are moved to lower power states—for example, by lowering clock frequency—whenever possible.
In addition, when a computer is not being actively used, Windows may put the entire computer into a low-power state (sleep) or may even save all of memory to disk and shut the computer off (hibernation)
When the user returns, the computer powers up and continues from its previous state, so the user does not need to reboot and restart applications.
The longer a CPU can stay unused, the more energy can be saved.
Because computers are so much faster than human beings, a lot of energy can be saved just while humans are thinking.
The problem is that too many programs are constantly polling to see what is happening in the system.
A swarm of software timers are ﬁring, keeping the CPU from staying idle long enough to save much energy.
Windows 7 extends CPU idle time by skipping clock ticks, coalescing software timers into smaller numbers of events, and “parking” entire CPUs when systems are not heavily loaded.
Early in the history of the PC industry, computer conﬁgurations were fairly static.
Occasionally, new devices might be plugged into the serial, printer, or game ports on the back of a computer, but that was it.
The next steps toward dynamic conﬁguration of PCs were laptop docks and PCMIA cards.
A PC could suddenly be connected to or disconnected from a whole set of peripherals.
PCs are designed to enable users to plug and unplug a huge host of peripherals all the time; external disks, thumb drives, cameras, and the like are constantly coming and going.
Support for dynamic conﬁguration of devices is continually evolving in Windows.
The system can automatically recognize devices when they are plugged in and can ﬁnd, install, and load the appropriate driversoften without user intervention.
When devices are unplugged, the drivers automatically unload, and system execution continues without disrupting other software.
The architecture of Windows is a layered system of modules, as shown in Figure 19.1
The main layers are the HAL, the kernel, and the executive, all of which run in kernel mode, and a collection of subsystems and services that run in user mode.
The user-mode subsystems fall into two categories: the environmental subsystems, which emulate different operating systems, and the protection subsystems, which provide security functions.
One of the chief advantages of this type of architecture is that interactions between modules are kept simple.
The remainder of this section describes these layers and subsystems.
The HAL is the layer of software that hides hardware chipset differences from upper levels of the operating system.
Only a single version of each device driver is required for each CPU architecture, no matter what support chips might be present.
The kernel layer of Windows has four main responsibilities: thread scheduling, low-level processor synchronization, interrupt and exception handling, and switching between user mode and kernel mode.
The kernel is implemented in the C language, using assembly language only where absolutely necessary to interface with the lowest level of the hardware architecture.
An object type in Windows is a system-deﬁned data type that has a set of attributes (data values) and a set of methods (for example, functions or operations)
The kernel performs its job by using a set of kernel objects whose attributes store the kernel data and whose methods perform the kernel activities.
The kernel dispatcher provides the foundation for the executive and the subsystems.
Most of the dispatcher is never paged out of memory, and its execution is never preempted.
Its main responsibilities are thread scheduling and context switching, implementation of synchronization primitives, timer management, software interrupts (asynchronous and deferred procedure calls), and exception dispatching.
Like many other modern operating systems, Windows uses processes and threads for executable code.
Each process has one or more threads, and each thread has its own scheduling state, including actual priority, processor afﬁnity, and CPU usage information.
There are six possible thread states: ready, standby, running, waiting, transition, and terminated.
The highest-priority ready thread is moved to the standby state, which means it is the next thread to run.
In a multiprocessor system, each processor keeps one thread in a standby state.
A thread is running when it is executing on a processor.
It runs until it is preempted by a higher-priority thread, until it terminates, until its allotted execution time (quantum) ends, or until it waits on a dispatcher object, such as an event signaling I/O completion.
A thread is in the waiting state when it is waiting for a dispatcher object to be signaled.
A thread is in the transition state while it waits for resources necessary for execution; for example, it may be waiting for its kernel stack to be swapped in from disk.
A thread enters the terminated state when it ﬁnishes execution.
The dispatcher uses a 32-level priority scheme to determine the order of thread execution.
Priorities are divided into two classes: variable class and real-time class.
The variable class contains threads having priorities from 1 to.
The dispatcher uses a queue for each scheduling priority and traverses the set of queues from highest to lowest until it ﬁnds a thread that is ready to run.
If a thread has a particular processor afﬁnity but that processor is not available, the dispatcher skips past it and continues looking for a ready thread that is willing to run on the available processor.
If no ready thread is found, the dispatcher executes a special thread called the idle thread.
When a thread’s time quantum runs out, the clock interrupt queues a quantum-end deferred procedure call (DPC) to the processor.
Queuing the DPC results in a software interrupt when the processor returns to normal interrupt priority.
The software interrupt causes the dispatcher to reschedule the processor to execute the next available thread at the preempted thread’s priority level.
The priority of the preempted thread may be modiﬁed before it is placed back on the dispatcher queues.
If the preempted thread is in the variablepriority class, its priority is lowered.
Lowering the thread’s priority tends to limit the CPUconsumption of compute-bound threads versus I/O-bound threads.
When a variable-priority thread is released from a wait operation, the dispatcher boosts the priority.
The amount of the boost depends on the device for which the thread was waiting.
For example, a thread waiting for keyboard I/O would get a large priority increase, whereas a thread waiting for a disk operation would get a moderate one.
This strategy tends to give good response times to interactive threads using a mouse and windows.
It also enables I/O-bound threads to keep the I/O devices busy while permitting compute-bound threads to use spare CPU cycles in the background.
In addition, the thread associated with the user’s active GUI window receives a priority boost to enhance its response time.
Scheduling occurs when a thread enters the ready or wait state, when a thread terminates, or when an application changes a thread’s priority or processor afﬁnity.
If a higher-priority thread becomes ready while a lowerpriority thread is running, the lower-priority thread is preempted.
This preemption gives the higher-priority thread preferential access to the CPU.
Windows is not a hard real-time operating system, however, because it does not guarantee that a real-time thread will start to execute within a particular time limit; threads are blocked indeﬁnitely while DPCs and interrupt service routines (ISRs) are running (as further discussed below)
Traditionally, operating-system schedulers used sampling to measure CPU utilization by threads.
The system timer would ﬁre periodically, and the timer interrupt handler would take note of what thread was currently scheduled and whether it was executing in user or kernel mode when the interrupt occurred.
This sampling technique was necessary because either the CPU did not have a high-resolution clock or the clock was too expensive or unreliable to access frequently.
Although efﬁcient, sampling was inaccurate and led to anomalies such as incorporating interrupt servicing time as thread time and dispatching threads that had run for only a fraction of the quantum.
Starting with Windows Vista, CPU time in Windows has been tracked using the hardware timestamp counter (TSC) included in recent processors.
Using the TSC results in more accurate accounting of CPU usage, and the scheduler will not preempt threads before they have run for a full quantum.
Key operating-system data structures are managed as objects using common facilities for allocation, reference counting, and security.
The event object is used to record an event occurrence and to synchronize this occurrence with some action.
Notiﬁcation events signal all waiting threads, and synchronization events signal a single waiting thread.
The mutant provides kernel-mode or user-mode mutual exclusion associated with the notion of ownership.
The mutex, available only in kernel mode, provides deadlock-free mutual exclusion.
The semaphore object acts as a counter or gate to control the number of threads that access a resource.
The thread object is the entity that is scheduled by the kernel dispatcher.
It is associated with a process object, which encapsulates a virtual address space.
The thread object is signaled when the thread exits, and the process object, when the process exits.
The timer object is used to keep track of time and to signal timeouts when operations take too long and need to be interrupted or when a periodic activity needs to be scheduled.
Many of the dispatcher objects are accessed from user mode via an open operation that returns a handle.
The user-mode code polls or waits on handles to synchronize with other threads as well as with the operating system (see Section 19.7.1)
The dispatcher implements two types of software interrupts: asynchronous procedure calls (APCs) and deferred procedure calls (DPCs, mentioned earlier)
An asynchronous procedure call breaks into an executing thread and calls a procedure.
APCs are used to begin execution of new threads, suspend or resume existing threads, terminate threads or processes, deliver notiﬁcation that an asynchronous I/O has completed, and extract the contents of the CPU registers from a running thread.
APCs are queued to speciﬁc threads and allow the system to execute both system and user code within a process’s context.
User-mode execution of an APC cannot occur at arbitrary times, but only when the thread is waiting in the kernel and marked alertable.
After handling all urgent device-interrupt processing, the ISR schedules the remaining processing by queuing a DPC.
The associated software interrupt will not occur until the CPU is next at a priority lower than the priority of all I/O device interrupts but higher than the priority at which threads run.
DPCs to process timer expirations and to preempt thread execution at the end of the scheduling quantum.
Execution of DPCs prevents threads from being scheduled on the current processor and also keeps APCs from signaling the completion of I/O.
This is done so that completion of DPC routines does not take an extended amount of time.
As an alternative, the dispatcher maintains a pool of worker threads.
ISRs and DPCs may queue work items to the worker threads where they will be executed using normal thread scheduling.
Unlike APCs, DPC routines make no assumptions about what process context the processor is executing.
The kernel dispatcher also provides trap handling for exceptions and interrupts generated by hardware or software.
Elaborate exception handling is performed by the kernel’s exception dispatcher.
The exception dispatcher creates an exception record containing the reason for the exception and ﬁnds an exception handler to deal with it.
When an exception occurs in kernel mode, the exception dispatcher simply calls a routine to locate the exception handler.
If no handler is found, a fatal system error occurs, and the user is left with the infamous “blue screen of death” that signiﬁes system failure.
Exception handling is more complex for user-mode processes, because an environmental subsystem (such as the POSIX system) sets up a debugger port and an exception port for every process it creates.
If a debugger port is registered, the exception handler sends the exception to the port.
If the debugger port is not found or does not handle that exception, the dispatcher attempts to ﬁnd an appropriate exception handler.
If no handler is found, the debugger is called again to catch the error for debugging.
If no debugger is running, a message is sent to the process’s exception port to give the environmental subsystem a chance to translate the exception.
For example, the POSIX environment translates Windows exception messages into POSIX signals before sending them to the thread that caused the exception.
Finally, if nothing else works, the kernel simply terminates the process containing the thread that caused the exception.
When Windows fails to handle an exception, it may construct a description of the error that occurred and request permission from the user to send the information back to Microsoft for further analysis.
In some cases, Microsoft’s automated analysis may be able to recognize the error immediately and suggest a ﬁx or workaround.
The interrupt dispatcher in the kernel handles interrupts by calling either an interrupt service routine (ISR) supplied by a device driver or a kernel trap-handler routine.
The interrupt is represented by an interrupt object that contains all the information needed to handle the interrupt.
Using an interrupt object makes it easy to associate interrupt-service routines with an interrupt without having to access the interrupt hardware directly.
Different processor architectures have different types and numbers of interrupts.
For portability, the interrupt dispatcher maps the hardware interrupts into a standard set.
The interrupts are prioritized and are serviced in priority order.
The kernel uses an interrupt-dispatch table to bind each interrupt level to a service routine.
In a multiprocessor computer, Windows keeps a separate interrupt-dispatch table (IDT) for each processor, and each processor’s IRQL can be set independently to mask out interrupts.
All interrupts that occur at a level equal to or less than the IRQL of a processor are blocked until the IRQL is lowered.
Windows takes advantage of this property and uses software interrupts to deliver APCs and DPCs, to perform system functions such as synchronizing threads with I/O completion, to start thread execution, and to handle timers.
What the programmer thinks of as a thread in traditional Windows is actually two threads: a user-mode thread (UT) and a kernel-mode thread (KT)
Each has its own stack, register values, and execution context.
A UT requests a system service by executing an instruction that causes a trap to kernel mode.
The kernel layer runs a trap handler that switches between the UT and the corresponding KT.
When a KT has completed its kernel execution and is ready to switch back to the corresponding UT, the kernel layer is called to make the switch to the UT, which continues its execution in user mode.
Windows 7 modiﬁes the behavior of the kernel layer to support usermode scheduling of the UTs.
A UT can explicitly yield to another UT by calling the user-mode scheduler; it is not necessary to enter the kernel.
User-mode scheduling is explained in more detail in Section 19.7.3.7
The Windows executive provides a set of services that all environmental subsystems use.
The services are grouped as follows: object manager, virtual memory manager, process manager, advanced local procedure call facility, I/O manager, cache manager, security reference monitor, plug-and-play and power managers, registry, and booting.
For managing kernel-mode entities, Windows uses a generic set of interfaces that are manipulated by user-mode programs.
Windows calls these entities objects, and the executive component that manipulates them is the object manager.
Examples of objects are semaphores, mutexes, events, processes, and threads; all these are dispatcher objects.
Threads can block in the kernel dispatcher waiting for any of these objects to be signaled.
The process, thread, and virtual memory APIs use process and thread handles to identify the process or thread to be operated on.
Other examples of objects include ﬁles, sections, ports, and various internal I/O objects.
File objects are used to maintain the open state of ﬁles and devices.
User-mode code accesses these objects using an opaque value called a handle, which is returned by many APIs.
Each process has a handle table containing entries that track the objects used by the process.
The system process, which contains the kernel, has its own handle table, which is protected from user code.
Kernel-mode code can access an object by using either a handle or a referenced pointer.
A process gets a handle by creating an object, by opening an existing object, by receiving a duplicated handle from another process, or by inheriting a handle from the parent process.
When a process exits, all its open handles are implicitly closed.
Since the object manager is the only entity that generates object handles, it is the natural place to check security.
The object manager checks whether a process has the right to access an object when the process tries to open the object.
The object manager also enforces quotas, such as the maximum amount of memory a process may use, by charging a process for the memory occupied by all its referenced objects and refusing to allocate more memory when the accumulated charges exceed the process’s quota.
The object manager keeps track of two counts for each object: the number of handles for the object and the number of referenced pointers.
The handle count is the number of handles that refer to the object in the handle tables of all processes, including the system process that contains the kernel.
The referenced pointer count is incremented whenever a new pointer is needed by the kernel and decremented when the kernel is done with the pointer.
The purpose of these reference counts is to ensure that an object is not freed while it is still referenced by either a handle or an internal kernel pointer.
In contrast to UNIX, which roots the system name space in the ﬁle system, Windows uses an abstract name space and connects the ﬁle systems as devices.
Whether a Windows object has a name is up to its creator.
Processes and threads are created without names and referenced either by handle or through a separate numerical identiﬁer.
Synchronization events usually have names, so that they can be opened by unrelated processes.
A permanent name represents an entity, such as a disk drive, that remains even if no process is accessing it.
A temporary name exists only while a process holds a handle to the object.
The object manager supports directories and symbolic links in the name space.
Each object, as mentioned earlier, is an instance of an object type.
The object type speciﬁes how instances are to be allocated, how the data ﬁelds are to be deﬁned, and how the standard set of virtual functions used for all objects are to be implemented.
The standard functions implement operations such as mapping names to objects, closing and deleting, and applying security checks.
Functions that are speciﬁc to a particular type of object are implemented by system services designed to operate on that particular object type, not by the methods speciﬁed in the object type.
The parse() function is the most interesting of the standard object functions.
The ﬁle systems, the registry conﬁguration store, and GUI objects are the most notable users of parse functions to extend the Windows name space.
Returning to our Windows naming example, device objects used to represent ﬁle-system volumes provide a parse function.
We can illustrate how naming, parse functions, objects, and handles work together by looking at the steps to open the ﬁle in Windows:
An application requests that a ﬁle named C:\foo\bar.doc be opened.
IopParseDevice() allocates a ﬁle object and passes it to the ﬁle system, which ﬁlls in the details of how to access C:\foo\bar.doc on the volume.
When the ﬁle system returns, IopParseDevice() allocates an entry for the ﬁle object in the handle table for the current process and returns the handle to the application.
If the ﬁle cannot successfully be opened, IopParseDevice() deletes the ﬁle object it allocated and returns an error indication to the application.
The executive component that manages the virtual address space, physical memory allocation, and paging is the virtual memory (VM) manager.
Pages of data allocated to a process that are not in physical memory are either stored in the paging ﬁles on disk or mapped directly to a regular ﬁle on a local or remote ﬁle system.
The upper 2 GB are mostly identical for all processes and are used by Windows in kernel mode to access the operating-system code and data structures.
Key areas of the kernel-mode region that are not identical for all processes are the self-map, hyperspace, and session space.
The hardware references a process’s page table using physical page-frame numbers, and the page table self-map makes the contents of the process’s page table accessible using virtual addresses.
Hyperspace maps the current process’s working-set information into the kernel-mode address space.
Session space is used to share an instance of the Win32 and other session-speciﬁc drivers among all the processes in the same terminal-server (TS) session.
Different TS sessions share different instances of these drivers, yet they are mapped at the same virtual addresses.
The lower, user-mode region of virtual address space is speciﬁc to each process and accessible by both user- and kernel-mode threads.
The Windows VM manager uses a two-step process to allocate virtual memory.
The ﬁrst step reserves one or more pages of virtual addresses in the process’s virtual address space.
The second step commits the allocation by assigning virtual memory space (physical memory or space in the paging ﬁles)
Windows limits the amount of virtual memory space a process consumes by enforcing a quota on committed memory.
The APIs used to reserve virtual addresses and commit virtual memory take a handle on a process object as a parameter.
This allows one process to control the virtual memory of another.
Environmental subsystems manage the memory of their client processes in this way.
After getting a handle to a section object, a process maps the memory of the section to a range of addresses, called a view.
A process can establish a view of the entire section or only the portion it needs.
Windows allows sections to be mapped not just into the current process but into any process for which the caller has a handle.
A section can be backed by disk space either in the system-paging ﬁle or in a regular ﬁle (a memory-mapped ﬁle)
A section can be based, meaning that it appears at the same virtual address for all processes attempting to access it.
Sections can also represent physical memory, allowing a 32-bit process to access more physical memory than can ﬁt in its virtual address space.
Let’s look more closely at the last two of these protection settings:
The exception can be used, for example, to check whether a faulty program iterates beyond the end of an array or simply to detect that the program attempted to access virtual addresses that are not committed to memory.
User- and kernel-mode stacks use no-access pages as guard pages to detect stack overﬂows.
Both the usermode memory allocator and the special kernel allocator used by the device veriﬁer can be conﬁgured to map each allocation onto the end of a page, followed by a no-access page to detect programming errors that access beyond the end of an allocation.
The copy-on-write mechanism enables the VM manager to use physical memory more efﬁciently.
When two processes want independent copies of data from the same section object, the VM manager places a single shared copy into virtual memory and activates the copy-on-write property for that region of memory.
If one of the processes tries to modify data in a copy-on-write page, the VM manager makes a private copy of the page for the process.
The virtual address translation in Windows uses a multilevel page table.
Each PTE points to a 4-KB page frame in physical memory.
For a variety of reasons, the hardware requires that the page directories or PTE tables at each level of a multilevel page table occupy a single page.
Thus, the number of PDEs or PTEs that ﬁt in a page determine how many virtual addresses are translated by that page.
The structure described so far can be used to represent only 1 GB of virtual address translation.
For AMD64, Windows uses a total of four full levels.
The VM manager allocates pages of PDEs and PTEs as needed and moves page-table pages to disk when not in use.
The page-table pages are faulted back into memory when referenced.
We next consider how virtual addresses are translated into physical addresses on IA-32-compatible processors.
Two bits are used to index into the four PDEs at the top level of the page table.
The selected PDE will contain the physical page number for each of the four page-directory pages that map 1 GB of the address space.
Nine bits are used to select another PDE, this time from a second-level page directory.
This PDE will contain the physical page numbers of up to 512 PTE-table pages.
Nine bits are used to select one of 512 PTEs from the selected PTE-table page.
The selected PTE will contain the physical page number for the byte we are accessing.
Twelve bits are used as the byte offset into the page.
The physical address of the byte we are accessing is constructed by appending the lowest 12 bits of the virtual address to the end of the physical page number we found in the selected PTE.
The number of bits in a physical address may be different from the number of bits in a virtual address.
Such systems could address only 4 GB of physical memory.
These systems could support 64 GB and were used on server systems.
Of course, once upon a time 4 GB seemed optimistically large for physical memory.
To improve performance, the VM manager maps the page-directory and PTE-table pages into the same contiguous region of virtual addresses in every process.
This self-map allows the VM manager to use the same pointer to access the current PDE or PTE corresponding to a particular virtual address no matter what process is running.
Although the self-map occupies signiﬁcant address space, it does not require any additional virtual memory pages.
It also allows the page table’s pages to be automatically paged in and out of physical memory.
In the creation of a self-map, one of the PDEs in the top-level page directory refers to the page-directory page itself, forming a “loop” in the page-table translations.
The virtual pages are accessed if the loop is not taken, the PTE-table pages are accessed if the loop is taken once, the lowest-level page-directory pages are accessed if the loop is taken twice, and so forth.
The additional levels of page directories used for 64-bit virtual memory are translated in the same way except that the virtual address pointer is broken up into even more values.
To avoid the overhead of translating every virtual address by looking up the PDE and PTE, processors use translation look-aside buffer (TLB) hardware, which contains an associative memory cache for mapping virtual pages to PTEs.
The TLB is part of the memory-management unit (MMU) within each processor.
The MMU needs to “walk” (navigate the data structures of) the page table in memory only when a needed translation is missing from the TLB.
The PDEs and PTEs contain more than just physical page numbers.
They also have bits reserved for operating-system use and bits that control how the hardware uses memory, such as whether hardware caching should be used for.
In addition, the entries specify what kinds of access are allowed for both user and kernel modes.
A PDE can also be marked to say that it should function as a PTE rather than a PDE.
If the selected PDE is marked to act as a PTE, then the remaining 21 bits of the pointer are used as the offset of the byte.
Also, the large pages can result in very signiﬁcant internal fragmentation.
Because of these problems, it is typically only Windows itself, along with large server applications, that use large pages to improve the performance of the TLB.
They are better suited to do so because operating-system and server applications start running when the system boots, before memory has become fragmented.
Windows manages physical memory by associating each physical page with one of seven states: free, zeroed, modiﬁed, standby, bad, transition, or valid.
A free page is a page that has no particular content.
A zeroed page is a free page that has been zeroed out and is ready for.
A modiﬁed page has been written by a process and must be sent to the disk before it is allocated for another process.
A standby page is a copy of information already stored on disk.
Standby pages may be pages that were not modiﬁed, modiﬁed pages that have already been written to the disk, or pages that were prefetched because they are expected to be used soon.
A bad page is unusable because a hardware error has been detected.
A transition page is on its way in from disk to a page frame allocated in.
A valid page is part of the working set of one or more processes and is contained within these processes’ page tables.
While valid pages are contained in processes’ page tables, pages in other states are kept in separate lists according to state type.
The lists are constructed by linking the corresponding entries in the page frame number (PFN) database, which includes an entry for each physical memory page.
The PFN entries also include information such as reference counts, locks, and NUMA information.
Note that the PFN database represents pages of physical memory, whereas the PTEs represent pages of virtual memory.
When the valid bit in a PTE is zero, hardware ignores all the other bits, and the VM manager can deﬁne them for its own use.
Invalid pages can have a number of states represented by bits in the PTE.
Pages mapped through section objects encode a pointer to the appropriate section object.
PTEs for pages that have been written to the page ﬁle contain enough information to locate the page on disk, and so forth.
The structure of the page-ﬁle PTE is shown in Figure 19.5
The T, P, and V bits are all zero for this type of PTE.
When a process is started, it is assigned a default minimum working-set size.
The working set of each process is allowed to grow until the amount of remaining physical memory starts to run low, at which point the VM manager starts to track the age of the pages in each working set.
Eventually, when the available memory runs critically low, the VM manager trims the working set to remove older pages.
How old a page is depends not on how long it has been in memory but on when it was last referenced.
This is determined by periodically making a pass through the working set of each process and incrementing the age for pages that have not been marked in the PTE as referenced since the last pass.
When it becomes necessary to trim the working sets, the VM manager uses heuristics to decide how much to trim from each process and then removes the oldest pages ﬁrst.
A process can have its working set trimmed even when plenty of memory is available, if it was given a hard limit on how much physical memory it could use.
In Windows 7, the VM manager will also trim processes that are growing rapidly, even if memory is plentiful.
This policy change signiﬁcantly improves the responsiveness of the system for other processes.
Windows tracks working sets not only for user-mode processes but also for the system process, which includes all the pageable data structures and code that run in kernel mode.
Windows 7 created additional working sets for the system process and associated them with particular categories of kernel memory; the ﬁle cache, kernel heap, and kernel code now have their own working sets.
The distinct working sets allow the VM manager to use different policies to trim the different categories of kernel memory.
The VM manager does not fault in only the page immediately needed.
Research shows that the memory referencing of a thread tends to have a locality property.
That is, when a page is used, it is likely that adjacent pages will be referenced in the near future.
Think of iterating over an array or fetching sequential instructions that form the executable code for a thread.
Because of locality, when the VM manager faults in a page, it also faults in a few adjacent pages.
This prefetching tends to reduce the total number of page faults and allows reads to be clustered to improve I/O performance.
In addition to managing committed memory, the VM manager manages each process’s reserved memory, or virtual address space.
Each process has an associated tree that describes the ranges of virtual addresses in use and what the uses are.
This allows the VM manager to fault in page-table pages as needed.
If the PTE for a faulting address is uninitialized, the VM manager searches for the address in the process’s tree of virtual address descriptors (VADs) and uses this information to ﬁll in the PTE and retrieve the page.
In some cases, a PTE-table page itself may not exist; such a page must be transparently allocated and initialized by the VM manager.
In other cases, the page may be shared as part of a section object, and the VAD will contain a pointer to that section object.
The section object contains information on how to ﬁnd the shared virtual page so that the PTE can be initialized to point at it directly.
The Windows process manager provides services for creating, deleting, and using processes, threads, and jobs.
It has no knowledge about parent–child relationships or process hierarchies; those reﬁnements are left to the particular environmental subsystem that owns the process.
The process manager is also not involved in the scheduling of processes, other than setting the priorities and afﬁnities in processes and threads when they are created.
Processes themselves can be collected into larger units called job objects.
The use of job objects allows limits to be placed on CPU usage, working-set size, and processor afﬁnities that control multiple processes at once.
An example of process creation in the Win32 environment is as follows:
A message is sent to the Win32 subsystem to notify it that the process is being created.
CreateProcess() in the original process then calls an API in the process manager of the NT executive to actually create the process.
The process manager calls the object manager to create a process object and returns the object handle to Win32
Win32 calls the process manager again to create a thread for the process and returns handles to the new process and thread.
The Windows APIs for manipulating virtual memory and threads and for duplicating handles take a process handle, so subsystems can perform.
Once a new process is created, the initial thread is created, and an asynchronous procedure call is delivered to the thread to prompt the start of execution at the user-mode image loader.
The loader is in ntdll.dll, which is a link library automatically mapped into every newly created process.
Windows also supports a UNIXfork() style of process creation in order to support the POSIX environmental subsystem.
Although the Win32 environment calls the process manager directly from the client process, POSIX uses the cross-process nature of the Windows APIs to create the new process from within the subsystem process.
The process manager relies on the asynchronous procedure calls (APCs) implemented by the kernel layer.
APCs are used to initiate thread execution, suspend and resume threads, access thread registers, terminate threads and processes, and support debuggers.
The debugger support in the process manager includes the APIs to suspend and resume threads and to create threads that begin in suspended mode.
There are also process-manager APIs that get and set a thread’s register context and access another process’s virtual memory.
Threads can be created in the current process; they can also be injected into another process.
The debugger makes use of thread injection to execute code within a process being debugged.
While running in the executive, a thread can temporarily attach to a different process.
Thread attach is used by kernel worker threads that need to execute in the context of the process originating a work request.
For example, the VM manager might use thread attach when it needs access to a process’s working set or page tables, and the I/O manager might use it in updating the status variable in a process for asynchronous I/O operations.
When the login process authenticates a user, the security token is attached to the user’s process and inherited by its child processes.
The token contains the security identity (SID) of the user, the SIDs of the groups the user belongs to, the privileges the user has, and the integrity level of the process.
By default, all threads within a process share a common token, representing the user and the application that started the process.
However, a thread running in a process with a security token belonging to one user can set a thread-speciﬁc token belonging to another user to impersonate that user.
The impersonation facility is fundamental to the client–server RPC model, where services must act on behalf of a variety of clients with different security IDs.
The right to impersonate a user is most often delivered as part of an RPC connection from a client process to a server process.
Impersonation allows the server to access system services as if it were the client in order to access or create objects and ﬁles on behalf of the client.
The server process must be trustworthy and must be carefully written to be robust against attacks.
Otherwise, one client could take over a server process and then impersonate any user who made a subsequent client request.
The environmental subsystems are servers that implement particular operatingsystem personalities.
To reduce the memory footprint, multiple services are often collected into a few processes running the svchost.exe program.
Each service is loaded as a dynamic-link library (DLL), which implements the service by relying on the user-mode thread-pool facilities to share threads and wait for messages (see Section 19.3.3.3)
The normal implementation paradigm for client–server computing is to use RPCs to communicate requests.
When an RPC always occurs between a client and server on the local system, the advanced local procedure call facility (ALPC) can be used as the transport.
At the lowest level of the system, in the implementation of the environmental systems, and for services that must be available in the early stages of booting, RPC is not available.
When a client wants services from a subsystem or service, it opens a handle to the server’s connection-port object and sends a connection request to the port.
The server creates a channel and returns a handle to the client.
The channel consists of a pair of private communication ports: one for client-to-server messages and the other for server-to-client messages.
Communication channels support a callback mechanism, so the client and server can accept requests when they would normally be expecting a reply.
When an ALPC channel is created, one of three message-passing techniques is chosen.
The ﬁrst technique is suitable for small to medium messages (up to 63 KB)
In this case, the port’s message queue is used as intermediate storage, and the messages are copied from one process to the other.
In this case, a sharedmemory section object is created for the channel.
Messages sent through the port’s message queue contain a pointer and size information referring to the section object.
The sender places data into the shared section, and the receiver views them directly.
The third technique uses APIs that read and write directly into a process’s address space.
This technique is normally used by RPC to achieve higher performance for speciﬁc scenarios.
The Win32 window manager uses its own form of message passing, which is independent of the executive ALPC facilities.
An event-pair object is a synchronization object used by the Win32 subsystem to provide notiﬁcation when the client thread.
The section object is used to pass the messages, and the event-pair object provides synchronization.
The section object eliminates message copying, since it represents a region of shared memory.
The event-pair object eliminates the overhead of using the port object to pass messages containing pointers and lengths.
The dedicated server thread eliminates the overhead of determining which client thread is calling the server, since there is one server thread per client thread.
The kernel gives scheduling preference to these dedicated server threads to improve performance.
The I/O manager is responsible for managing ﬁle systems, device drivers, and network drivers.
It keeps track of which device drivers, ﬁlter drivers, and ﬁle systems are loaded, and it also manages buffers for I/O requests.
It works with the VM manager to provide memory-mapped ﬁle I/O and controls the Windows cache manager, which handles caching for the entire I/O system.
The I/O manager is fundamentally asynchronous, providing synchronous I/O by explicitly waiting for an I/O operation to complete.
The I/O manager provides several models of asynchronous I/O completion, including setting of events, updating of a status variable in the calling process, delivery of APCs to initiating threads, and use of I/O completion ports, which allow a single thread to process I/O completions from many other threads.
Device drivers are arranged in a list for each device (called a driver or I/O stack)
A driver is represented in the system as a driver object.
Because a single driver can operate on multiple devices, the drivers are represented in the I/O stack by a device object, which contains a link to the driver object.
The I/O manager converts the requests it receives into a standard form called an I/O request packet (IRP)
It then forwards the IRP to the ﬁrst driver in the targeted I/O stack for processing.
After a driver processes the IRP, it calls the I/O manager either to forward the IRP to the next driver in the stack or, if all processing is ﬁnished, to complete the operation represented by the IRP.
The I/O request may be completed in a context different from the one in which it was made.
For example, if a driver is performing its part of an I/O operation and is forced to block for an extended time, it may queue the IRP to a worker thread to continue processing in the system context.
In the original thread, the driver returns a status indicating that the I/O request is pending so that the thread can continue executing in parallel with the I/O operation.
An IRP may also be processed in interrupt-service routines and completed in an arbitrary process context.
Because some ﬁnal processing may need to take place in the context that initiated the I/O, the I/O manager uses an APC to do ﬁnal I/O-completion processing in the process context of the originating thread.
As a driver stack is built, various drivers have the opportunity to insert themselves into the stack as ﬁlter drivers.
Filter drivers can examine and potentially modify each I/O operation.
File-system ﬁlter drivers execute above the ﬁle system and have been used to implement functionalities such as hierarchical storage management, single instancing of ﬁles for remote boot, and dynamic format conversion.
Third parties also use ﬁle-system ﬁlter drivers to implement virus detection.
Device drivers for Windows are written to the Windows Driver Model (WDM) speciﬁcation.
This model lays out all the requirements for device drivers, including how to layer ﬁlter drivers, share common code for handling power and plug-and-play requests, build correct cancellation logic, and so forth.
Because of the richness of the WDM, writing a full WDM device driver for each new hardware device can involve a great deal of work.
Fortunately, the port/miniport model makes it unnecessary to do this.
Within a class of similar devices, such as audio drivers, SATA devices, or Ethernet controllers, each instance of a device shares a common driver for that class, called a port driver.
The port driver implements the standard operations for the class and then calls device-speciﬁc routines in the device’s miniport driver to implement device-speciﬁc functionality.
The TCP/IP network stack is implemented in this way, with the ndis.sys class driver implementing much of the network driver functionality and calling out to the network miniport drivers for speciﬁc hardware.
Recent versions of Windows, including Windows 7, provide additional simpliﬁcations for writing device drivers for hardware devices.
Kernel-mode drivers can now be written using the Kernel-Mode Driver Framework (KMDF), which provides a simpliﬁed programming model for drivers on top of WDM.
Many drivers do not need to operate in kernel mode, and it is easier to develop and deploy drivers in user mode.
It also makes the system more reliable, because a failure in a user-mode driver does not cause a kernel-mode crash.
In many operating systems, caching is done by the ﬁle system.
The cache manager works closely with the VM manager to provide cache services for all components under the control of the I/O manager.
Caching in Windows is based on ﬁles rather than raw blocks.
The size of the cache changes dynamically according to how much free memory is available in the system.
The cache manager maintains a private working set rather than sharing the system process’s working set.
The cache manager memory-maps ﬁles into kernel memory and then uses special interfaces to the VM manager to fault pages into or trim them from this private working set.
Each cache block can hold a view (that is, a memory-mapped region) of a ﬁle.
Each cache block is described by a virtual address control block (VACB) that stores the virtual address and ﬁle offset for the view, as well as the number of processes using the view.
The VACBs reside in a single array maintained by the cache manager.
When the I/O manager receives a ﬁle’s user-level read request, the I/O manager sends an IRP to the I/O stack for the volume on which the ﬁle resides.
For ﬁles that are marked as cacheable, the ﬁle system calls the cache manager to look up the requested data in its cached ﬁle views.
The cache manager calculates which entry of that ﬁle’s VACB index array corresponds to the byte offset of the request.
The entry either points to the view in the cache or is invalid.
If it is invalid, the cache manager allocates a cache block (and the corresponding entry in the VACB array) and maps the view into the cache block.
The cache manager then attempts to copy data from the mapped ﬁle to the caller’s buffer.
If the copy fails, it does so because of a page fault, which causes the VM manager to send a noncached read request to the I/O manager.
The I/O manager sends another request down the driver stack, this time requesting a paging operation, which bypasses the cache manager and reads the data from the ﬁle directly into the page allocated for the cache manager.
Upon completion, the VACB is set to point at the page.
The data, now in the cache, are copied to the caller’s buffer, and the original I/O request is completed.
A kernel-level read operation is similar, except that the data can be accessed directly from the cache rather than being copied to a buffer in user space.
To use ﬁle-system metadata (data structures that describe the ﬁle system), the kernel uses the cache manager’s mapping interface to read the metadata.
To modify the metadata, the ﬁle system uses the cache manager’s pinning interface.
Pinning a page locks the page into a physical-memory page frame so that the VM manager cannot move the page or page it out.
After updating the metadata, the ﬁle system asks the cache manager to unpin the page.
A modiﬁed page is marked dirty, and so the VM manager ﬂushes the page to disk.
To improve performance, the cache manager keeps a small history of read requests and from this history attempts to predict future requests.
If the cache manager ﬁnds a pattern in the previous three requests, such as sequential access forward or backward, it prefetches data into the cache before the next.
In this way, the application may ﬁnd its data already cached and not need to wait for disk I/O.
The cache manager is also responsible for telling the VM manager to ﬂush the contents of the cache.
When write-through caching is needed, a process can set a ﬂag when opening the ﬁle, or the process can call an explicit cache-ﬂush function.
A fast-writing process could potentially ﬁll all the free cache pages before the cache-writer thread had a chance to wake up and ﬂush the pages to disk.
The cache writer prevents a process from ﬂooding the system in the following way.
When the amount of free cache memory becomes low, the cache manager temporarily blocks processes attempting to write data and wakes the cachewriter thread to ﬂush pages to disk.
If the fast-writing process is actually a network redirector for a network ﬁle system, blocking it for too long could cause network transfers to time out and be retransmitted.
To prevent such waste, network redirectors can instruct the cache manager to limit the backlog of writes in the cache.
Because a network ﬁle system needs to move data between a disk and the network interface, the cache manager also provides a DMA interface to move the data directly.
Moving data directly avoids the need to copy data through an intermediate buffer.
Centralizing management of system entities in the object manager enables Windows to use a uniform mechanism to perform run-time access validation and audit checks for every user-accessible entity in the system.
Whenever a process opens a handle to an object, the security reference monitor (SRM) checks the process’s security token and the object’s access-control list to see whether the process has the necessary access rights.
The SRM is also responsible for manipulating the privileges in security tokens.
Special privileges are required for users to perform backup or restore operations on ﬁle systems, debug processes, and so forth.
Tokens can also be marked as being restricted in their privileges so that they cannot access objects that are available to most users.
Restricted tokens are used primarily to limit the damage that can be done by execution of untrusted code.
The integrity level of the code executing in a process is also represented by a token.
Integrity levels are a type of capability mechanism, as mentioned earlier.
A process cannot modify an object with an integrity level higher than that of the code executing in the process, whatever other permissions have been granted.
Integrity levels were introduced to make it harder for code that successfully attacks outward-facing software, like Internet Explorer, to take over a system.
Another responsibility of the SRM is logging security audit events.
The Department of Defense’s Common Criteria (the 2005 successor to the Orange Book) requires that a secure system have the ability to detect and log all attempts to access system resources so that it can more easily trace attempts at unauthorized access.
Because the SRM is responsible for making access checks, it generates most of the audit records in the security-event log.
The operating system uses the plug-and-play (PnP) manager to recognize and adapt to changes in the hardware conﬁguration.
PnP devices use standard protocols to identify themselves to the system.
The PnP manager automatically recognizes installed devices and detects changes in devices as the system operates.
The manager also keeps track of hardware resources used by a device, as well as potential resources that could be used, and takes care of loading the appropriate drivers.
This management of hardware resourcesprimarily interrupts and I/O memory ranges—has the goal of determining a hardware conﬁguration in which all devices are able to operate successfully.
First, it gets a list of devices from each bus driver (for example, PCI or USB)
It loads the installed driver (after ﬁnding one, if necessary) and sends an add-device request to the appropriate driver for each device.
The PnP manager then ﬁgures out the optimal resource assignments and sends a start-device request to each driver specifying the resource assignments for the device.
If a device needs to be reconﬁgured, the PnP manager sends a query-stop request, which asks the driver whether the device can be temporarily disabled.
If the driver can disable the device, then all pending operations are completed, and new operations are prevented from starting.
Finally, the PnP manager sends a stop request and can then reconﬁgure the device with a new start-device request.
For example, queryremove, which operates similarly to query-stop, is employed when a user is getting ready to eject a removable device, such as a USB storage device.
The surprise-remove request is used when a device fails or, more likely, when a user removes a device without telling the system to stop it ﬁrst.
Finally, the remove request tells the driver to stop using a device permanently.
Many programs in the system are interested in the addition or removal of devices, so the PnP manager supports notiﬁcations.
Such a notiﬁcation, for example, gives GUI ﬁle menus the information they need to update their list of disk volumes when a new storage device is attached or removed.
Installing devices often results in adding new services to the svchost.exe processes in the system.
These services frequently set themselves up to run whenever the system boots and continue to run even if the original device is never plugged into the system.
Windows 7 introduced a service-trigger mechanism in the service control manager (SCM), which manages the system services.
With this mechanism, services can register themselves to start only when SCM receives a notiﬁcation from the PnP manager that the device of interest has been added to the system.
Windows works with the hardware to implement sophisticated strategies for energy efﬁciency, as described in Section 19.2.8
The policies that drive these strategies are implemented by the power manager.
The power manager detects current system conditions, such as the load on CPUs or I/O devices, and improves energy efﬁciency by reducing the performance and responsiveness of the system when need is low.
The power manager can also put the entire system into a very efﬁcient sleep mode and can even write all the contents of memory to disk and turn off the power to allow the system to go into hibernation.
The primary advantage of sleep is that the system can enter fairly quickly, perhaps just a few seconds after the lid closes on a laptop.
The power is turned down low on the CPUs and I/O devices, but the memory continues to be powered enough that its contents are not lost.
Hibernation takes considerably longer because the entire contents of memory must be transferred to disk before the system is turned off.
However, the fact that the system is, in fact, turned off is a signiﬁcant advantage.
If there is a loss of power to the system, as when the battery is swapped on a laptop or a desktop system is unplugged, the saved system data will not be lost.
Unlike shutdown, hibernation saves the currently running system so a user can resume where she left off, and because hibernation does not require power, a system can remain in hibernation indeﬁnitely.
Like the PnP manager, the power manager provides notiﬁcations to the rest of the system about changes in the power state.
Some applications want to know when the system is about to be shut down so they can start saving their states to disk.
Windows keeps much of its conﬁguration information in internal databases, called hives, that are managed by the Windows conﬁguration manager, which is commonly known as the registry.
There are separate hives for system information, default user preferences, software installation, security, and boot options.
Because the information in the system hive is required to boot the system, the registry manager is implemented as a component of the executive.
The registry represents the conﬁguration state in each hive as a hierarchical namespace of keys (directories), each of which can contain a set of typed values, such as UNICODE string, ANSI string, integer, or untyped binary data.
In theory, new keys and values are created and initialized as new software is installed; then they are modiﬁed to reﬂect changes in the conﬁguration of that software.
Restarting applications, or even the system, every time a conﬁguration change was made would be a nuisance.
Instead, programs rely on various kinds of notiﬁcations, such as those provided by the PnP and power managers, to learn about changes in the system conﬁguration.
The registry also supplies notiﬁcations; it allows threads to register to be notiﬁed when changes are made to some part of the registry.
The threads can thus detect and adapt to conﬁguration changes recorded in the registry itself.
Whenever signiﬁcant changes are made to the system, such as when updates to the operating system or drivers are installed, there is a danger that the conﬁguration data may be corrupted (for example, if a working driver is replaced by a nonworking driver or an application fails to install correctly and leaves partial information in the registry)
Windows creates a system restore point before making such changes.
The restore point contains a copy of the hives before the change and can be used to return to this version of the hives and thereby get a corrupted system working again.
To improve the stability of the registry conﬁguration, Windows added a transaction mechanism beginning with Windows Vista that can be used to prevent the registry from being partially updated with a collection of related conﬁguration changes.
Registry transactions can be part of more general transactions administered by the kernel transaction manager (KTM), which can also include ﬁle-system transactions.
The booting of a Windows PC begins when the hardware powers on and ﬁrmware begins executing from ROM.
In older machines, this ﬁrmware was known as the BIOS, but more modern systems use UEFI (the Uniﬁed Extensible Firmware Interface), which is faster and more general and makes better use of the facilities in contemporary processors.
The ﬁrmware runs power-on self-test (POST) diagnostics; identiﬁes many of the devices attached to the system and initializes them to a clean, power-up state; and then builds the description used by the advanced conﬁguration and power interface (ACPI)
Next, the ﬁrmware ﬁnds the system disk, loads the Windows bootmgr program, and begins executing it.
In a machine that has been hibernating, the winresume program is loaded next.
It restores the running system from disk, and the system continues execution at the point it had reached right before hibernating.
In a machine that has been shut down, the bootmgr performs further initialization of the system and then loads winload.
This program loads hal.dll, the kernel (ntoskrnl.exe), any drivers needed in booting, and the system hive.
The system process contains all the internal kernel worker threads and never executes in user mode.
The ﬁrst user-mode process created is SMSS, for session manager subsystem, which is similar to the INIT (initialization) process in UNIX.
Each session is used to represent a logged-on user, except for session 0, which is used to run system-wide background services, such as LSASS and SERVICES.
A session is anchored by an instance of the CSRSS process.
Each session other than 0 initially runs the WINLOGON process.
This process logs on a user and then launches the EXPLORER process, which implements the Windows GUI experience.
The following list itemizes some of these aspects of booting:
LSASS, the security subsystem, implements facilities such as authentication of users.
A number of services will have registered to start when the system boots.
Others will be started only on demand or when triggered by an event such as the arrival of a device.
It is started in every session—unlike the POSIX subsystem, which is started only on demand when a POSIX process is created.
The system optimizes the boot process by prepaging from ﬁles on disk based on previous boots of the system.
Disk access patterns at boot are also used to lay out system ﬁles on disk to reduce the number of I/O operations required.
The processes necessary to start the system are reduced by grouping services into fewer processes.
All of these approaches contribute to a dramatic reduction in system boot time.
Of course, system boot time is less important than it once was because of the sleep and hibernation capabilities of Windows.
Windows supports a GUI-based console that interfaces with the user via keyboard, mouse, and display.
Audio input is used by Windows voice-recognition software; voice recognition makes the system more convenient and increases its accessibility for users with disabilities.
Windows 7 added support for multi-touch hardware, allowing users to input data by touching the screen and making gestures with one or more ﬁngers.
Eventually, the video-input capability, which is currently used for communication applications, is likely to be used for visually interpreting gestures, as Microsoft has demonstrated for its Xbox 360 Kinect product.
Other future input experiences may evolve from Microsoft’s surface computer.
Most often installed at public venues, such as hotels and conference centers, the surface computer is a table surface with special cameras underneath.
It can track the actions of multiple users at once and recognize objects that are placed on top.
The PC was, of course, envisioned as a personal computer—an inherently single-user machine.
Modern Windows, however, supports the sharing of a PC among multiple users.
Each user that is logged on using the GUI has a session created to represent the GUI environment he will be using and to contain all the processes created to run his applications.
Windows allows multiple sessions to exist at the same time on a single machine.
However, Windows only supports a single console, consisting of all the monitors, keyboards, and mice connected to the PC.
Only one session can be connected to the console at a time.
From the logon screen displayed on the console, users can create new sessions or attach to an existing session that was previously created.
This allows multiple users to share a single PC without having to log off and on between users.
Users can also create new sessions, or connect to existing sessions, on one PC from a session running on another Windows PC.
The terminal server (TS) connects one of the GUI windows in a user’s local session to the new or existing session, called a remote desktop, on the remote computer.
The most common use of remote desktops is for users to connect to a session on their work PC from their home PC.
Many corporations use corporate terminal-server systems maintained in data centers to run all user sessions that access corporate resources, rather than allowing users to access those resources from the PCs in each user’s ofﬁce.
Each server computer may handle many dozens of remote-desktop sessions.
This is a form of thin-client computing, in which individual computers rely on a server for many functions.
Relying on data-center terminal servers improves reliability, manageability, and security of the corporate computing resources.
The TS is also used by Windows to implement remote assistance.
A remote user can be invited to share a session with the user logged on to the session on the console.
The remote user can watch the user’s actions and even be given control of the desktop to help resolve computing problems.
However, associated USB thumb drives, ﬂash memory on cameras, and external disks may be formatted with the 32-bit FAT ﬁle system for portability.
A disadvantage is that the FAT ﬁle system does not restrict ﬁle access to authorized users.
The only solution for securing data with FAT is to run an application to encrypt the data before storing it on the ﬁle system.
In contrast, NTFS uses ACLs to control access to individual ﬁles and supports implicit encryption of individual ﬁles or entire volumes (using Windows BitLocker feature)
A volume is created by the Windows logical disk management utility and is based on a logical disk partition.
A volume may occupy a portion of a disk or an entire disk, or may span several disks.
The cluster size is conﬁgured when an NTFS ﬁle system is formatted.
Given the size of today’s disks, it may make sense to use cluster sizes larger than the Windows defaults to achieve better performance, although these performance gains will come at the expense of more internal fragmentation.
It assigns them by numbering clusters from the beginning of the disk to the end.
A ﬁle in NTFS is not a simple byte stream as it is in UNIX; rather, it is a structured object consisting of typed attributes.
Each attribute of a ﬁle is an independent byte stream that can be created, deleted, read, and written.
Some attribute types are standard for all ﬁles, including the ﬁle name (or names, if the ﬁle has aliases, such as an MS-DOS short name), the creation time, and the security descriptor that speciﬁes the access control list.
Most traditional data ﬁles have an unnamed data attribute that contains all the ﬁle’s data.
However, additional data streams can be created with explicit names.
For instance, in Macintosh ﬁles stored on a Windows server, the resource fork is a named data stream.
The IProp interfaces of the Component Object Model (COM) use a named data stream to store properties on ordinary ﬁles, including thumbnails of images.
In general, attributes may be added as necessary and are accessed using a ﬁle-name:attribute syntax.
Every ﬁle in NTFS is described by one or more records in an array stored in a special ﬁle called the master ﬁle table (MFT)
Small attributes are stored in the MFT record itself and are called resident attributes.
Large attributes, such as the unnamed bulk data, are called nonresident attributes and are stored in one or more contiguous extents on the disk.
A pointer to each extent is stored in the MFT record.
For a small ﬁle, even the data attribute may ﬁt inside the MFT record.
If a ﬁle has many attributes—or if it is highly fragmented, so that many pointers are needed to point to all the fragments —one record in the MFT might not be large enough.
In this case, the ﬁle is described by a record called the base ﬁle record, which contains pointers to overﬂow records that hold the additional pointers and attributes.
Each ﬁle in an NTFS volume has a unique ID called a ﬁle reference.
The ﬁle number is the record number (that is, the array slot) in the MFT that describes the ﬁle.
The sequence number is incremented every time an MFT entry is reused.
The sequence number enables NTFS to perform internal consistency checks, such as catching a stale reference to a deleted ﬁle after the MFT entry has been reused for a new ﬁle.
As in UNIX, the NTFS namespace is organized as a hierarchy of directories.
Each directory uses a data structure called a B+ tree to store an index of the ﬁle names in that directory.
In a B+ tree, the length of every path from the root of the tree to a leaf is the same, and the cost of reorganizing the tree is eliminated.
The index root of a directory contains the top level of the B+ tree.
For a large directory, this top level contains pointers to disk extents that hold the remainder of the tree.
Each entry in the directory contains the name and ﬁle reference of the ﬁle, as well as a copy of the update timestamp and ﬁle size taken from the ﬁle’s resident attributes in the MFT.
Copies of this information are stored in the directory so that a directory listing can be efﬁciently generated.
Because all the ﬁle names, sizes, and update times are available from the directory itself, there is no need to gather these attributes from the MFT entries for each of the ﬁles.
The second ﬁle, which is used during recovery if the MFT is damaged, contains a copy of the ﬁrst 16 entries of the MFT.
The log ﬁle records all metadata updates to the ﬁle system.
The volume ﬁle contains the name of the volume, the version of NTFS that.
The root directory is the top-level directory in the ﬁle-system hierarchy.
The bitmap ﬁle indicates which clusters on a volume are allocated to ﬁles.
The boot ﬁle contains the startup code for Windows and must be located at a particular disk address so that it can be found easily by a simple ROM bootstrap loader.
The boot ﬁle also contains the physical address of the MFT.
The bad-cluster ﬁle keeps track of any bad areas on the volume; NTFS uses this record for error recovery.
Keeping all the NTFS metadata in actual ﬁles has a useful property.
As discussed in Section 19.3.3.6, the cache manager caches ﬁle data.
Since all the NTFS metadata reside in ﬁles, these data can be cached using the same mechanisms used for ordinary data.
In many simple ﬁle systems, a power failure at the wrong time can damage the ﬁle-system data structures so severely that the entire volume is scrambled.
Many UNIX ﬁle systems, including UFS but not ZFS, store redundant metadata on the disk, and they recover from crashes by using the fsck program to check all the ﬁle-system data structures and restore them forcibly to a consistent state.
Restoring them often involves deleting damaged ﬁles and freeing data clusters that had been written with user data but not properly recorded in the ﬁle system’s metadata structures.
This checking can be a slow process and can cause the loss of signiﬁcant amounts of data.
In NTFS, all ﬁlesystem data-structure updates are performed inside transactions.
Before a data structure is altered, the transaction writes a log record that contains redo and undo information.
After the data structure has been changed, the transaction writes a commit record to the log to signify that the transaction succeeded.
After a crash, the system can restore the ﬁle-system data structures to a consistent state by processing the log records, ﬁrst redoing the operations for committed transactions and then undoing the operations for transactions.
Periodically (usually every 5 seconds), a checkpoint record is written to the log.
The system does not need log records prior to the checkpoint to recover from a crash.
They can be discarded, so the log ﬁle does not grow without bounds.
The ﬁrst time after system startup that an NTFS volume is accessed, NTFS automatically performs ﬁle-system recovery.
This scheme does not guarantee that all the user-ﬁle contents are correct after a crash.
It ensures only that the ﬁle-system data structures (the metadata ﬁles) are undamaged and reﬂect some consistent state that existed prior to the crash.
It would be possible to extend the transaction scheme to cover user ﬁles, and Microsoft took some steps to do this in Windows Vista.
The log is stored in the third metadata ﬁle at the beginning of the volume.
It is created with a ﬁxed maximum size when the ﬁle system is formatted.
It has two sections: the logging area, which is a circular queue of log records, and the restart area, which holds context information, such as the position in the logging area where NTFS should start reading during a recovery.
In fact, the restart area holds two copies of its information, so recovery is still possible if one copy is damaged during the crash.
In addition to writing the log records and performing recovery actions, the log-ﬁle service keeps track of the free space in the log ﬁle.
If the free space gets too low, the log-ﬁle service queues pending transactions, and NTFS halts all new I/O operations.
After the in-progress operations complete, NTFS calls the cache manager to ﬂush all data and then resets the log ﬁle and performs the queued transactions.
The security of an NTFS volume is derived from the Windows object model.
Each NTFS ﬁle references a security descriptor, which speciﬁes the owner of the ﬁle, and an access-control list, which contains the access permissions granted or denied to each user or group listed.
Early versions of NTFS used a separate security descriptor as an attribute of each ﬁle.
In normal operation, NTFS does not enforce permissions on traversal of directories in ﬁle path names.
However, for compatibility with POSIX, these checks can be enabled.
The preﬁx-matching cache allows path-name traversal to begin much deeper in the tree, saving many steps.
Enforcing traversal checks means that the user’s access must be checked at each directory level.
When installed, it provides several ways to combine multiple disk drives into one logical volume so as to improve performance, capacity, or reliability.
One way to combine multiple disks is to concatenate them logically to form a large logical volume, as shown in Figure 19.7
In Windows, this logical volume, called a volume set, can consist of up to 32 physical partitions.
A volume set that contains an NTFS volume can be extended without disturbance of the data already stored in the ﬁle system.
The bitmap metadata on the NTFS volume are simply extended to cover the newly added space.
Another way to combine multiple physical partitions is to interleave their blocks in round-robin fashion to form a stripe set.
This scheme is also called RAID level 0, or disk striping.
For more on RAID (redundant arrays of inexpensive disks), see Section 10.7
Then, the allocation wraps around to the ﬁrst disk, allocating the second 64-KB block.
A stripe set forms one large logical volume, but the physical layout can improve the I/O bandwidth, because for a large I/O, all the disks can transfer data in parallel.
To deal with disk sectors that go bad, FtDisk uses a hardware technique called sector sparing, and NTFS uses a software technique called cluster remapping.
Sector sparing is a hardware capability provided by many disk drives.
When a disk drive is formatted, it creates a map from logical block numbers to good sectors on the disk.
If a sector fails, FtDisk instructs the disk drive to substitute a spare.
Cluster remapping is a software technique performed by the ﬁle system.
If a disk block goes bad, NTFS substitutes a different, unallocated block by changing any affected pointers in the MFT.
When a disk block goes bad, the usual outcome is a data loss.
But sector sparing or cluster remapping can be combined with fault-tolerant volumes to mask the failure of a disk block.
If a read fails, the system reconstructs the missing data by reading the mirror or by calculating the exclusive or parity in a stripe set with parity.
The reconstructed data are stored in a new location that is obtained by sector sparing or cluster remapping.
To compress a ﬁle, NTFS divides the ﬁle’s data into compression units, which are blocks of 16 contiguous clusters.
When a compression unit is written, a data-compression algorithm is applied.
If the result ﬁts into fewer than 16 clusters, the compressed version is stored.
When reading, NTFS can determine whether data have been compressed: if they have been, the length of the stored compression unit is less than 16 clusters.
To improve performance when reading contiguous compression units, NTFS prefetches and decompresses ahead of the application requests.
For sparse ﬁles or ﬁles that contain mostly zeros, NTFS uses another technique to save space.
Clusters that contain only zeros because they have never been written are not actually allocated or stored on disk.
Instead, gaps are left in the sequence of virtual-cluster numbers stored in the MFT entry for the ﬁle.
When reading a ﬁle, if NTFS ﬁnds a gap in the virtual-cluster numbers, it just zero-ﬁlls that portion of the caller’s buffer.
They provide a mechanism for organizing disk volumes that is more ﬂexible than the use of global names (like drive letters)
A mount point is implemented as a symbolic link with associated data that contains the true volume name.
Ultimately, mount points will supplant drive letters completely, but there will be a long transition due to the dependence of many applications on the drive-letter scheme.
Windows Vista introduced support for a more general form of symbolic links, similar to those found in UNIX.
The links can be absolute or relative, can point to objects that do not exist, and can point to both ﬁles and directories.
User-mode services can receive notiﬁcations of changes to the journal and then identify what ﬁles have changed by reading from the journal.
The search indexer service uses the change journal to identify ﬁles that need to be re-indexed.
The ﬁle-replication service uses it to identify ﬁles that need to be replicated across the network.
Windows implements the capability of bringing a volume to a known state and then creating a shadow copy that can be used to back up a consistent view of the volume.
This technique is known as snapshots in some other ﬁle systems.
Making a shadow copy of a volume is a form of copy-on-write, where blocks modiﬁed after the shadow copy is created are stored in their original form in the copy.
To achieve a consistent state for the volume requires the cooperation of applications, since the system cannot know when the data used by the application are in a stable state from which the application could be safely restarted.
The server version of Windows uses shadow copies to efﬁciently maintain old versions of ﬁles stored on ﬁle servers.
This allows users to see documents stored on ﬁle servers as they existed at earlier points in time.
The user can use this feature to recover ﬁles that were accidentally deleted or simply to look at a previous version of the ﬁle, all without pulling out backup media.
The networking components in Windows provide data transport, interprocess communication, ﬁle sharing across a network, and the ability to send print jobs to remote printers.
To describe networking in Windows, we must ﬁrst mention two of the internal networking interfaces: the network device interface speciﬁcation (NDIS) and the transport driver interface (TDI)
This interface enables any session-layer component to use any available transport mechanism.
The TDI supports both connection-based and connectionless transport and has functions to send any type of data.
These drivers can be loaded and unloaded from the system dynamically, although in practice the system typically has to be rebooted after a change.
The system uses the protocol to send I/O requests over the network.
Session control messages are commands that start and end a redirector connection to a shared resource at the server.
A redirector uses File messages to access ﬁles at the server.
Printer messages are used to send data to a remote print queue and to receive status information from the queue, and Message messages are used to communicate with another workstation.
A version of the SMB protocol was published as the common Internet ﬁle system (CIFS) and is supported on a number of operating systems.
The transmission control protocol/Internet protocol (TCP/IP) suite that is used on the Internet has become the de facto standard networking infrastructure.
Windows uses TCP/IP to connect to a wide variety of operating systems and hardware platforms.
The Windows TCP/IP package includes the simple network-management protocol (SNM), the dynamic host-conﬁguration protocol (DHCP), and the older Windows Internet name service (WINS)
This new implementation also supports ofﬂoading of the network stack onto advanced hardware, to achieve very high performance for servers.
Windows provides a software ﬁrewall that limits the TCP ports that can be used by programs for network communication.
Network ﬁrewalls are commonly implemented in routers and are a very important security measure.
Having a ﬁrewall built into the operating system makes a hardware router unnecessary, and it also provides more integrated management and easier use.
The point-to-point tunneling protocol (PPTP) is a protocol provided by Windows to communicate between remote-access server modules running on Windows server machines and other client systems that are connected over the Internet.
The remote-access servers can encrypt data sent over the connection, and they support multiprotocol virtual private networks (VPNs) over the Internet.
The HTTP protocol is used to get/put information using the World Wide Web.
Windows implements HTTP using a kernel-mode driver, so web servers can operate with a low-overhead connection to the networking stack.
Web-distributed authoring and versioning (WebDAV) is an HTTP-based protocol for collaborative authoring across a network.
Being built directly into the ﬁle system enables WebDAV to work with other ﬁle-system features, such as encryption.
Personal ﬁles can then be stored securely in a public place.
Because WebDAV uses HTTP, which is aget/putprotocol, Windows has to cache the ﬁles locally so programs can use read and write operations on parts of the ﬁles.
A process can use named pipes to communicate with other processes on the same machine.
Since named pipes are accessed through the ﬁle-system interface, the security mechanisms used for ﬁle objects also apply to named pipes.
The SMB protocol supports named pipes, so named pipes can also be used for communication between processes on different systems.
The format of pipe names follows the uniform naming convention (UNC)
A UNC name looks like a typical remote ﬁle name.
A remote procedure call (RPC) is a client–server mechanism that enables an application on one machine to make a procedure call to code on another machine.
The client calls a local procedure—a stub routine—that packs its arguments into a message and sends them across the network to a particular server process.
Meanwhile, the server unpacks the message, calls the procedure, packs the return results into a message, and sends them back to the client stub.
The client stub unblocks, receives the message, unpacks the results of the RPC, and returns them to the caller.
The client stub code and the descriptors necessary to pack and unpack the arguments for an RPC are compiled from a speciﬁcation written in the Microsoft Interface Deﬁnition Language.
It hides many of the architectural differences among computers, such as the sizes of binary numbers and the order of bytes and bits in computer words, by specifying standard data formats for RPC messages.
The component object model (COM) is a mechanism for interprocess communication that was developed for Windows.
For instance, COM is the infrastructure used by Microsoft’s object linking and embedding (OLE) technology for inserting spreadsheets into Microsoft Word documents.
Windows has a distributed extension called DCOM that can be used over a network utilizing RPC to provide a transparent method of developing distributed applications.
In Windows, an application can use the Windows I/O API to access ﬁles from a remote computer as though they were local, provided that the remote computer is running a CIFS server such as those provided by Windows.
A redirector is the client-side object that forwards I/O requests to a remote system, where they are satisﬁed by a server.
For performance and security, the redirectors and servers run in kernel mode.
In more detail, access to a remote ﬁle occurs as follows:
The application calls the I/O manager to request that a ﬁle be opened with a ﬁle name in the standard UNC format.
The MUP sends the I/O request packet asynchronously to all registered redirectors.
A redirector that can satisfy the request responds to the MUP.
To avoid asking all the redirectors the same question in the future, the MUP uses a cache to remember which redirector can handle this ﬁle.
The redirector sends the network request to the remote system.
The remote-system network drivers receive the request and pass it to the server driver.
The server driver hands the request to the proper local ﬁle-system driver.
The proper device driver is called to access the data.
The results are returned to the server driver, which sends the data back to the requesting redirector.
The redirector then returns the data to the calling application via the I/O manager.
A similar process occurs for applications that use the Win32 network API, rather than the UNC services, except that a module called a multi-provider router is used instead of a MUP.
For portability, redirectors and servers use the TDI API for network transport.
The list of redirectors is maintained in the system hive of the registry.
Windows supports a distributed ﬁle-system (DFS) protocol that allows a network administrator to serve up ﬁles from multiple servers using a single distributed name space.
To improve the PC experience for users who frequently switch among computers, Windows allows administrators to give users roaming proﬁles, which keep users’ preferences and other settings on servers.
Folder redirection is then used to automatically store a user’s documents and other ﬁles on a server.
This works well until one of the computers is no longer attached to the network, as when a user takes a laptop onto an airplane.
To give users off-line access to their redirected ﬁles, Windows uses client-side caching (CSC)
The ﬁles are pushed up to the server as they are changed.
If the computer becomes disconnected, the ﬁles are still available, and the update of the server is deferred until the next time the computer is online.
Many networked environments have natural groups of users, such as students in a computer laboratory at school or employees in one department in a business.
Frequently, we want all the members of the group to be able to access shared resources on their various computers in the group.
To manage the global access rights within such groups, Windows uses the concept of a domain.
Previously, these domains had no relationship whatsoever to the domain-name system (DNS) that maps Internet host names to IP addresses.
Active Directory is the Windows implementation of lightweight directoryaccess protocol (LDAP) services.
Active Directory stores the topology information about the domain, keeps the domain-based user and group accounts and passwords, and provides a domain-based store for Windows features that need it, such as Windows group policy.
Administrators use group policies to establish uniform standards for desktop preferences and software.
The Win32 API is the fundamental interface to the capabilities of Windows.
This section describes ﬁve main aspects of the Win32 API: access to kernel objects, sharing of objects between processes, process management, interprocess communication, and memory management.
The Windows kernel provides many services that application programs can use.
A process gains access to a kernel object named XXX by calling the CreateXXX function to open a handle to an instance of XXX.
Depending on which object is being opened, if the Create() function fails, it may return 0, or it may return a special constant named INVALID HANDLE VALUE.
A process can close any handle by calling the CloseHandle() function, and the system may delete the object if the count of handles referencing the object in all processes drops to zero.
The ﬁrst way is for a child process to inherit a handle to the object.
When the parent calls the CreateXXX function, the parent supplies a SECURITIES ATTRIBUTES structure with the bInheritHandle ﬁeld set to TRUE.
Next, the child process is created, passing a value of TRUE to the CreateProcess() function’s bInheritHandle argument.
Figure 19.8 shows a code sample that creates a semaphore handle inherited by a child process.
Assuming the child process knows which handles are shared, the parent and child can achieve interprocess communication through the shared objects.
In the example in Figure 19.8, the child process gets the value of the handle from the ﬁrst command-line argument and then shares the semaphore with the parent process.
The second way to share objects is for one process to give the object a name when the object is created and for the second process to open the name.
This method has two drawbacks: Windows does not provide a way to check whether an object with the chosen name already exists, and the object name space is global, without regard to the object type.
Figure 19.8 Code enabling a child to share an object by inheriting a handle.
Named objects have the advantage that unrelated processes can readily share them.
The ﬁrst process calls one of the CreateXXX functions and supplies a name as a parameter.
The second process gets a handle to share the object by calling OpenXXX() (or CreateXXX) with the same name, as shown in the example in Figure 19.9
The third way to share objects is via the DuplicateHandle() function.
This method requires some other method of interprocess communication to pass the duplicated handle.
Given a handle to a process and the value of a handle within that process, a second process can get a handle to the same object and thus share it.
An example of this method is shown in Figure 19.10
In Windows, a process is a loaded instance of an application and a thread is an executable unit of code that can be scheduled by the kernel dispatcher.
A process is created when a thread in some other process calls the CreateProcess() API.
This routine loads any dynamic link libraries used by the process and creates an initial thread in the process.
Each thread is created with its own stack, which defaults to 1 MB unless otherwise speciﬁed in an argument to CreateThread()
Figure 19.9 Code for sharing an object by name lookup.
Process A wants to give Process B access to a semaphore.
Figure 19.10 Code for sharing an object by passing a handle.
Priorities in the Win32 environment are based on the native kernel (NT) scheduling model, but not all priority values may be chosen.
Processes are typically members of the NORMAL PRIORITY CLASS unless the parent of the process was of the IDLE PRIORITY CLASS or another class was speciﬁed when CreateProcess was called.
The priority class of a process is the default for all threads that execute in the process.
It can be changed with the SetPriorityClass() function or by passing an argument to the START command.
Only users with the increase scheduling priority privilege can move a process into the REALTIME PRIORITY CLASS.
When a user is running an interactive process, the system needs to schedule the process’s threads to provide good responsiveness.
For this reason, Windows has a special scheduling rule for processes in the NORMAL PRIORITY CLASS.
Windows distinguishes between the process associated with the foreground window on the screen and the other (background) processes.
When a process moves into the foreground, Windows increases the scheduling quantum for all its threads by a factor of 3; CPU-bound threads in the foreground process will run three times longer than similar threads in background processes.
A thread starts with an initial priority determined by its class.
This function takes an argument that speciﬁes a priority relative to the base priority of its class:
Two other designations are also used to adjust the priority.
As discussed in Section 19.3.2.2, the kernel adjusts the priority of a variable class thread dynamically depending on whether the thread is I/O bound or CPU bound.
A thread can be created in a suspended state or can be placed in a suspended state later by use of the SuspendThread() function.
Before a suspended thread can be scheduled by the kernel dispatcher, it must be moved out of the suspended state by use of the ResumeThread() function.
Both functions set a counter so that if a thread is suspended twice, it must be resumed twice before it can run.
To synchronize concurrent access to shared objects by threads, the kernel provides synchronization objects, such as semaphores and mutexes.
Threads can also synchronize with kernel services operating on kernel objects—such as threads, processes, and ﬁles—because these are also dispatcher objects.
Another method of synchronization is available to threads within the same process that want to execute code exclusively.
The Win32 critical section object is a user-mode mutex object that can often be acquired and released without entering the kernel.
On a multiprocessor, a Win32 critical section will attempt to spin while waiting for a critical section held by another thread to be released.
If the spinning takes too long, the acquiring thread will allocate a kernel mutex and yield its CPU.
Critical sections are particularly efﬁcient because the kernel mutex is allocated only when there is contention and then used only after.
Most mutexes in programs are never actually contended, so the savings are signiﬁcant.
For programs that want user-mode reader–writer locks rather than a mutex, Win32 supports slim reader–writer (SRW) locks.
The Win32 API also supports condition variables, which can be used with either critical sections or SRW locks.
Repeatedly creating and deleting threads can be expensive for applications and services that perform small amounts of work in each instantiation.
The goal of using a thread pool is to increase performance and reduce memory footprint.
Threads are relatively expensive, and each processor can only be executing one thread at a time no matter how many threads are available.
The thread pool attempts to reduce the number of runnable threads by slightly delaying work requests (reusing each thread for many requests) while providing enough threads to effectively utilize the machine’s CPUs.
The wait and I/O- and timer-callback APIs allow the thread pool to further reduce the number of threads in a process, using far fewer threads than would be necessary if a process were to devote separate threads to servicing each waitable handle, timer, or completion port.
A ﬁber is user-mode code that is scheduled according to a user-deﬁned scheduling algorithm.
Fibers are completely a user-mode facility; the kernel is not aware that they exist.
The ﬁber mechanism uses Windows threads as if they were CPUs to execute the ﬁbers.
Fibers are cooperatively scheduled, meaning that they are never preempted but must explicitly yield the thread on which they are running.
When a ﬁber yields a thread, another ﬁber can be scheduled on it by the run-time system (the programming language run-time code)
The primary difference between these functions is that CreateFiber() does not begin executing the ﬁber that was created.
Fibers are not recommended for threads that use Win32 APIs rather than standard C-library functions because of potential incompatibilities.
Fibers must share the TEB of the thread on which they are running.
This can lead to problems when a Win32 interface puts state information into the TEB for one ﬁber and then the information is overwritten by a different ﬁber.
Fibers are included in the Win32 API to facilitate the porting of legacy UNIX applications that were written for a user-mode thread model such as Pthreads.
A new mechanism in Windows 7, user-mode scheduling (UMS), addresses several limitations of ﬁbers.
First, recall that ﬁbers are unreliable for executing Win32 APIs because they do not have their own TEBs.
When a thread running a ﬁber blocks in the kernel, the user scheduler loses control of the CPU for a time as the kernel dispatcher takes over scheduling.
Problems may result when ﬁbers change the kernel state of a thread, such as the priority or impersonation token, or when they start asynchronous I/O.
Each type of thread has its own stack and its own set of saved registers.
The KT and UT appear as a single thread to the programmer because UTs can never block but must always enter the kernel, where an implicit switch to the corresponding KT takes place.
When a UT enters the kernel, an explicit switch is made to the KT that corresponds to the UT identiﬁed by the current TEB.
The reason the kernel does not know which UT is running is that UTs can invoke a user-mode scheduler, as ﬁbers do.
But in UMS, the scheduler switches UTs, including switching the TEBs.
When a UT enters the kernel, its KT may block.
When this happens, the kernel switches to a scheduling thread, which UMS calls a primary, and uses this thread to reenter the user-mode scheduler so that it can pick another UT to run.
Eventually, a blocked KT will complete its operation and be ready to return to user mode.
Since UMS has already reentered the user-mode scheduler to run a different UT, UMS queues the UT corresponding to the completed KT to a completion list in user mode.
When the user-mode scheduler is choosing a new UT to switch to, it can examine the completion list and treat any UT on the list as a candidate for scheduling.
Unlike ﬁbers, UMS is not intended to be used directly by the programmer.
The details of writing user-mode schedulers can be very challenging, and UMS does not include such a scheduler.
Rather, the schedulers come from programming language libraries that build on top of UMS.
Microsoft Visual Studio 2010 shipped with Concurrency Runtime (ConcRT), a concurrent programming framework for C++
ConcRT provides a user-mode scheduler together with facilities for decomposing programs into tasks, which can then be scheduled on the available CPUs.
ConcRT provides support for par for styles of constructs, as well as rudimentary resource management and task synchronization primitives.
The key features of UMS are depicted in Figure 19.11
Winsock is a session-layer interface that is largely compatible with UNIX sockets but has some added Windows extensions.
It provides a standardized interface to many transport protocols that may have different addressing schemes, so that any Winsock application can run on any Winsock-compliant protocol stack.
Winsock underwent a major update in Windows Vista to add tracing, IPv6 support, impersonation, new security APIs and many other features.
Winsock follows the Windows Open System Architecture (WOSA) model, which provides a standard service provider interface (SPI) between applications and networking protocols.
Applications can load and unload layered protocols that build additional functionality, such as additional security, on top of the transport protocol layers.
Winsock supports asynchronous operations and notiﬁcations, reliable multicasting, secure sockets, and kernel mode sockets.
There is also support for simpler usage models, like the WSAConnectByName() function, which accepts the target as strings specifying the name or IP address of the server and the service or port number of the destination port.
Another is by using the Windows messaging facility, an approach that is particularly popular for Win32 GUI applications.
Posting a message and sending a message differ in this way: the post routines are asynchronous; they return immediately, and the calling thread does not know when the message is actually delivered.
The send routines are synchronous: they block the caller until the message has been delivered and processed.
In addition to sending a message, a thread can send data with the message.
Since processes have separate address spaces, the data must be copied.
The system copies data by calling SendMessage() to send a message of type WM COPYDATA with a COPYDATASTRUCT data structure that contains the length and address of the data to be transferred.
When the message is sent, Windows copies the data to a new block of memory and gives the virtual address of the new block to the receiving process.
Every Win32 thread has its own input queue from which it receives messages.
If a Win32 application does not call GetMessage() to handle events on its input queue, the queue ﬁlls up; and after about ﬁve seconds, the system marks the application as “Not Responding”
The Win32 API provides several ways for an application to use memory: virtual memory, memory-mapped ﬁles, heaps, and thread-local storage.
An application calls VirtualAlloc() to reserve or commit virtual memory and VirtualFree() to decommit or release the memory.
These functions enable the application to specify the virtual address at which the memory is allocated.
A process may lock some of its committed pages into physical memory by calling VirtualLock()
Another way for an application to use memory is by memory-mapping a ﬁle into its address space.
Memory mapping is also a convenient way for two processes to share memory: both processes map the same ﬁle into their virtual memory.
Memory mapping is a multistage process, as you can see in the example in Figure 19.13
Figure 19.13 Code fragments for memory mapping of a ﬁle.
If a process wants to map some address space just to share a memory region with another process, no ﬁle is needed.
The resulting ﬁle-mapping object can be shared by inheritance, by name lookup, or by handle duplication.
Heaps provide a third way for applications to use memory, just as with malloc() and free() in standard C.
A heap in the Win32 environment is a region of reserved address space.
When a Win32 process is initialized, it is created with a default heap.
Since most Win32 applications are multithreaded, access to the heap is synchronized to protect the heap’s space-allocation data structures from being damaged by concurrent updates by multiple threads.
Win32 provides several heap-management functions so that a process can allocate and manage a private heap.
The Win32 API also provides the HeapLock() and HeapUnlock() functions to enable a thread to gain exclusive access to a heap.
Unlike VirtualLock(), these functions perform only synchronization; they do not lock pages into physical memory.
The original Win32 heap was optimized for efﬁcient use of space.
This led to signiﬁcant problems with fragmentation of the address space for larger server programs that ran for long periods of time.
A new low-fragmentation heap (LFH) design introduced in Windows XP greatly reduced the fragmentation problem.
The Windows 7 heap manager automatically turns on LFH as appropriate.
A fourth way for applications to use memory is through a thread-local storage (TLS) mechanism.
Functions that rely on global or static data typically fail.
For instance, the C runtime function strtok() uses a static variable to keep track of its current position while parsing a string.
For two concurrent threads to executestrtok() correctly, they need separate current position variables.
The TLS mechanism allocates global heap storage and attaches it to the thread environment block that Windows allocates to every user-mode thread.
The TEB is readily accessible by each thread and is used not just for TLS but for all the per-thread state information in user mode.
To use a thread-local static variable, the application declares the variable as follows to ensure that every thread has its own private copy:
Microsoft designed Windows to be an extensible, portable operating system —one able to take advantage of new techniques and hardware.
The use of kernel objects to provide basic services, along with support for clientserver computing, enables Windows to support a wide variety of application environments.
Windows runs on a wide variety of computers, so users can choose and upgrade hardware to match their budgets and performance requirements without needing to alter the applications they run.
What settings are most conducive to the application of the different message-passing techniques?
How might it be useful to allow UTs to continue executing in parallel with their KTs?
Suppose you changed the CPU or the amount of RAM on a hibernating system.
Do you think that would work? Why or why not?
Now that you understand the fundamental concepts of operating systems (CPU scheduling, memory management, processes, and so on), we are in a position to examine how these concepts have been applied in several older and highly inﬂuential operating systems.
The order of presentation highlights the similarities and differences of the systems; it is not strictly chronological or ordered by importance.
The serious student of operating systems should be familiar with all these systems.
In the bibliographical notes at the end of the chapter, we include references to further reading about these early systems.
The papers, written by the designers of the systems, are important both for their technical content and for their style and ﬂavor.
To explain how operating-system features migrate over time from large computer systems to smaller ones.
To discuss the features of several historically important operating systems.
One reason to study early architectures and operating systems is that a feature that once ran only on huge systems may eventually make its way into very small systems.
Indeed, an examination of operating systems for mainframes and microcomputers shows that many features once available only on mainframes have been adopted for microcomputers.
The same operating-system concepts are thus appropriate for various classes of computers: mainframes, minicomputers, microcomputers, and handhelds.
To understand modern operating systems, then, you need to recognize the theme of feature migration and the long history of many operating-system features, as shown in Figure 20.1
A good example of feature migration started with the Multiplexed Information and Computing Services (MULTICS) operating system.
It ran on a large, complex mainframe computer (the GE 645)
Many of the ideas that were developed for MULTICS were subsequently used at Bell Laboratories (one of the original partners in the development of MULTICS) in the design of UNIX.
Around 1980, the features of UNIX became the basis for UNIX-like operating systems on microcomputers; and these features are included in several more recent operating systems for microcomputers, such as Microsoft Windows, Windows XP, and the Mac OS X operating system.
Linux includes some of these same features, and they can now be found on PDAs.
We turn our attention now to a historical overview of early computer systems.
We should note that the history of computing starts far before “computers”with looms and calculators.
We begin our discussion, however, with the computers of the twentieth century.
Before the 1940s, computing devices were designed and implemented to perform speciﬁc, ﬁxed tasks.
Modifying one of those tasks required a great deal of effort and manual labor.
All that changed in the 1940s when Alan Turing and John von Neumann (and colleagues), both separately and together, worked on the idea of a more general-purpose stored program computer.
This fundamental computer concept quickly generated a number of general-purpose computers, but much of the history of these machines is blurred by time and the secrecy of their development during World War II.
The programmer, who was also the operator of the computer system, would write a program and then would operate the program directly from the operator’s console.
First, the program would be loaded manually into memory from the front panel switches (one instruction at a time), from paper tape, or from punched cards.
Then the appropriate buttons would be pushed to set the starting address and to start the execution of the program.
If errors were discovered, the programmer could halt the program, examine the contents of memory and registers, and debug the program directly from the console.
Output was printed or was punched onto paper tape or cards for later printing.
As time went on, additional software and hardware were developed.
Assemblers, loaders, and linkers were designed to ease the programming task.
Common functions could then be copied into a new program without having to be written again, providing software reusability.
Each new I/O device had its own characteristics, requiring careful programming.
A special subroutine—called a device driver—was written for each I/O device.
A device driver knows how the buffers, ﬂags, registers, control bits, and status bits for a particular device should be used.
A simple task, such as reading a character from a paper-tape reader, might involve complex sequences of device-speciﬁc operations.
Rather than writing the necessary code every time, the device driver was simply used from the library.
Later, compilers for FORTRAN, COBOL, and other languages appeared, making the programming task much easier but the operation of the computer more complex.
To prepare a FORTRAN program for execution, for example, the programmer would ﬁrst need to load the FORTRAN compiler into the computer.
The compiler was normally kept on magnetic tape, so the proper tape would need to be mounted on a tape drive.
The program would be read through the card reader and written onto another tape.
The FORTRAN compiler produced assembly-language output, which then had to be assembled.
The output of the assembler would need to be linked to supporting library routines.
Finally, the binary object form of the program would be ready to execute.
It could be loaded into memory and debugged from the console, as before.
A signiﬁcant amount of setup time could be involved in the running of a job.
Each job step might involve the loading and unloading of magnetic tapes, paper tapes, and punch cards.
While tapes were being mounted or the programmer was operating the console, the CPU sat idle.
Remember that, in the early days, few computers were available, and they were expensive.
A computer might have cost millions of dollars, not including the operational costs of power, cooling, programmers, and so on.
Thus, computer time was extremely valuable, and owners wanted their computers to be used as much as possible.
They needed high utilization to get as much as they could from their investments.
As soon as one job was ﬁnished, the operator could start the next.
Since the operator had more experience with mounting tapes than a programmer, setup time was reduced.
The programmer provided whatever cards or tapes were needed, as well as a short description of how the job was to be run.
Of course, the operator could not debug an incorrect program at the console, since the operator would not understand the program.
Therefore, in the case of program error, a dump of memory and registers was taken, and the programmer had to debug from the dump.
Dumping the memory and registers allowed the operator to continue immediately with the next job but left the programmer with the more difﬁcult debugging problem.
Second, jobs with similar needs were batched together and run through the computer as a group to reduce setup time.
For instance, suppose the operator received one FORTRAN job, one COBOL job, and another FORTRAN job.
If she ran them in that order, she would have to set up for FORTRAN (load the compiler tapes and so on), then set up for COBOL, and then set up for FORTRAN again.
If she ran the two FORTRAN programs as a batch, however, she could setup only once for FORTRAN, saving operator time.
For example, when a job stopped, the operator would have to notice that it had stopped (by observing the console), determine why it stopped (normal or abnormal termination), dump memory and register (if necessary), load the appropriate device with the next job, and restart the computer.
During this transition from one job to the next, the CPU sat idle.
To overcome this idle time, people developed automatic job sequencing.
With this technique, the ﬁrst rudimentary operating systems were created.
A small program, called a resident monitor, was created to transfer control automatically from one job to the next (Figure 20.2)
When the computer was turned on, the resident monitor was invoked, and it would transfer control to a program.
When the program terminated, it would return control to the resident monitor, which would then go on to the next program.
Thus, the resident monitor would automatically sequence from one program to another and from one job to another.
But how would the resident monitor know which program to execute? Previously, the operator had been given a short description of what programs were to be run on what data.
Control cards were introduced to provide this information directly to the monitor.
In addition to the program or data for a job, the programmer supplied control cards, which contained directives to the resident monitor indicating what program to run.
For example, a normal user program might require one of three programs to run: the FORTRAN compiler (FTN), the assembler (ASM), or the user’s program (RUN)
We could use a separate control card for each of these:
These cards tell the resident monitor which program to run.
We can use two additional control cards to deﬁne the boundaries of each job:
JOB—First card of a job $END—Final card of a job.
These two cards might be useful in accounting for the machine resources used by the programmer.
Parameters can be used to deﬁne the job name, account number to be charged, and so on.
Other control cards can be deﬁned for other functions, such as asking the operator to load or unload a tape.
One problem with control cards is how to distinguish them from data or program cards.
The usual solution is to identify them by a special character or pattern on the card.
Several systems used the dollar-sign character ($) in the ﬁrst column to identify a control card.
IBM’s Job Control Language (JCL) used slash marks (//) in the ﬁrst two columns.
Figure 20.3 shows a sample card-deck setup for a simple batch system.
The control-card interpreter is responsible for reading and carrying out the instructions on the cards at the point of execution.
The loader is invoked by the control-card interpreter to load system programs and application programs into memory at intervals.
The device drivers are used by both the control-card interpreter and the loader for the system’s I/O devices.
Often, the system and application programs are linked to these same device drivers, providing continuity in their operation, as well as saving memory space and programming time.
The resident monitor provides automatic job sequencing as indicated by the control cards.
When a control card indicates that a program is to be run, the monitor loads the program into memory and transfers control to it.
This cycle is repeated until all control cards are interpreted for the job.
The switch to batch systems with automatic job sequencing was made to improve performance.
The problem, quite simply, is that humans are considerably slower than computers.
Consequently, it is desirable to replace human operation with operating-system software.
Automatic job sequencing eliminates the need for human setup time and job sequencing.
Even with this arrangement, however, the CPU is often idle.
The problem is the speed of the mechanical I/O devices, which are intrinsically slower than electronic devices.
Even a slow CPU works in the microsecond range, with thousands of instructions executed per second.
Thus, the difference in speed between the CPU and its I/O devices may be three orders of magnitude or more.
Over time, of course, improvements in technology resulted in faster I/O devices.
Unfortunately, CPU speeds increased even faster, so that the problem was not only unresolved but also exacerbated.
One common solution to the I/O problem was to replace slow card readers (input devices) and line printers (output devices) with magnetic-tape units.
The CPU did not read directly from cards, however; instead, the cards were ﬁrst copied onto a magnetic tape via a separate device.
When the tape was sufﬁciently full, it was taken down and carried over to the computer.
When a card was needed for input to a program, the equivalent record was read from the tape.
Similarly, output was written to the tape, and the contents of the tape were printed later.
The card readers and line printers were operated off-line, rather than by the main computer (Figure 20.4)
An obvious advantage of off-line operation was that the main computer was no longer constrained by the speed of the card readers and line printers but was limited only by the speed of the much faster magnetic tape units.
Figure 20.4 Operation of I/O devices (a) on-line and (b) off-line.
The technique of using magnetic tape for all I/O could be applied with any similar equipment (such as card readers, card punches, plotters, paper tape, and printers)
The real gain in off-line operation comes from the possibility of using multiple reader-to-tape and tape-to-printer systems for one CPU.
If the CPU can process input twice as fast as the reader can read cards, then two readers working simultaneously can produce enough tape to keep the CPU busy.
There is a disadvantage, too, however—a longer delay in getting a particular job run.
Then it must wait until enough additional jobs are read onto the tape to “ﬁll” it.
The tape must then be rewound, unloaded, hand-carried to the CPU, and mounted on a free tape drive.
This process is not unreasonable for batch systems, of course.
Many similar jobs can be batched onto a tape before it is taken to the computer.
Although off-line preparation of jobs continued for some time, it was quickly replaced in most systems.
Disk systems became widely available and greatly improved on off-line operation.
One problem with tape systems was that the card reader could not write onto one end of the tape while the CPU read from the other.
The entire tape had to be written before it was rewound and read, because tapes are by nature sequential-access devices.
Because the head is moved from one area of the disk to another, it can switch rapidly from the area on the disk being used by the card reader to store new cards to the position needed by the CPU to read the “next” card.
Spooling is also used for processing data at remote sites.
The CPU sends the data via communication paths to a remote printer (or accepts an entire input job from a remote card reader)
The remote processing is done at its own speed, with no CPU intervention.
The CPU just needs to be notiﬁed when the processing is completed, so that it can spool the next batch of data.
Spooling overlaps the I/O of one job with the computation of other jobs.
Even in a simple system, the spooler may be reading the input of one job while printing the output of a different job.
During this time, still another job (or other jobs) may be executed, reading its “cards” from disk and “printing” its output lines onto the disk.
Spooling has a direct beneﬁcial effect on the performance of the system.
For the cost of some disk space and a few tables, the computation of one job and the I/O of other jobs can take place at the same time.
Thus, spooling can keep both the CPU and the I/O devices working at much higher rates.
Spooling leads naturally to multiprogramming, which is the foundation of all modern operating systems.
Many of its basic features that were novel at the time have become standard parts of modern operating systems.
In addition, system calls were added by a set of special instructions called extra codes.
Spooling allowed the system to schedule jobs according to the availability of peripheral devices, such as magnetic tape units, paper tape readers, paper tape punches, line printers, card readers, and card punches.
The most remarkable feature of Atlas, however, was its memory management.
Many computers, like the IBM 650, used a drum for primary memory.
The Atlas system used a drum for its main memory, but it had a small amount of core memory that was used as a cache for the drum.
Demand paging was used to transfer information between core memory and the drum automatically.
The Atlas system used a British computer with 48-bit words.
At that time, this was an extremely large address space.
An associative memory of 32 registers implemented the mapping from a virtual address to a physical address.
If a page fault occurred, a page-replacement algorithm was invoked.
One memory frame was always kept empty, so that a drum transfer could start immediately.
The page-replacement algorithm attempted to predict future memory-accessing behavior based on past behavior.
A reference bit for each frame was set whenever the frame was accessed.
Like the Atlas system, it used paging for memory management.
The paging was used only for relocation; it was not used for demand paging.
Since physical memory was larger than virtual memory, several user processes could be in memory at the same time.
The number of users could be increased by page sharing when the pages contained read-only reentrant code.
Processes were kept on a drum and were swapped in and out of memory as necessary.
The modiﬁcations were typical of the changes made to a basic computer to allow an operating system to be written properly.
Certain instructions, such as I/O and halt, were deﬁned to be privileged.
An attempt to execute a privileged instruction in user mode would trap to the operating system.
A system-call instruction was added to the user-mode instruction set.
This instruction was used to create new resources, such as ﬁles, allowing the operating system to manage the physical resources.
Files, for example, were allocated in 256-word blocks on the drum.
A bit map was used to manage free drum blocks.
Each ﬁle had an index block with pointers to the actual data blocks.
The XDS-940 system also provided system calls to allow processes to create, start, suspend, and destroy subprocesses.
Process creation deﬁned a tree structure, where a process is the root and its subprocesses are nodes below it in the tree.
Each of the subprocesses could, in turn, create more subprocesses.
The THE operating system was designed at the Technische Hogeschool in Eindhoven in the Netherlands in the mid-1960’s.
The system was mainly noted for its clean design, particularly its layer structure, and its use of a set of concurrent processes employing semaphores for synchronization.
Unlike the processes in the XDS-940 system, the set of processes in the THE system was static.
The operating system itself was designed as a set of cooperating processes.
In addition, ﬁve user processes were created that served as the active agents to compile, execute, and print user programs.
When one job was ﬁnished, the process would return to the input queue to select another job.
This scheme gave higher priority to I/O-bound processes and to new processes.
Memory management was limited by the lack of hardware support.
However, since the system was limited and user programs could be written only in Algol, a software paging scheme was used.
The Algol compiler automatically generated calls to system routines, which made sure the requested information was in memory, swapping if necessary.
A 512-word page was used, with an LRU page-replacement strategy.
Another major concern of the THE system was deadlock control.
Closely related to the THE system is the Venus system.
The Venus system was also a layer-structured design, using semaphores to synchronize processes.
The lower levels of the design were implemented in microcode, however, providing a much faster system.
In addition, the system was designed as a time-sharing system, rather than a batch system.
The RC 4000 system, like the THE system, was notable primarily for its design concepts.
The objective was not to design a batch system, or a time-sharing system, or any other speciﬁc system.
Rather, the goal was to create an operating-system nucleus, or kernel, on which a complete operating system could be built.
Thus, the system structure was layered, and only the lower levels—comprising the kernel—were provided.
Although processes could share memory, the primary communication and synchronization mechanism was the message system provided by the kernel.
Processes could communicate with each other by exchanging ﬁxed-sized messages of eight words in length.
All messages were stored in buffers from a common buffer pool.
When a message buffer was no longer required, it was returned to the common pool.
It contained all the messages that had been sent to that process but had not yet been received.
The system supported four primitive operations, which were executed atomically:
The last two operations allowed processes to exchange several messages at a time.
These primitives required that a process service its message queue in FIFO order and that it block itself while other processes were handling its messages.
To remove these restrictions, the developers provided two additional communication primitives that allowed a process to wait for the arrival of the next message or to answer and service its queue in any order:
The device drivers were code that converted the device interrupts and registers into messages.
Thus, a process would write to a terminal by sending that terminal a message.
The device driver would receive the message and output the character to the terminal.
An input character would interrupt the system and transfer to a device driver.
The device driver would create a message from the input character and send it to a waiting process.
The users were provided with a set of interactive commands that allowed them to manipulate ﬁles and to compile and run programs through a terminal.
One result of CTSS was increased development of time-sharing systems.
As larger computers became available, the designers of CTSS set out to create a time-sharing utility.
Large computer systems would be connected by telephone wires to terminals in ofﬁces and homes throughout a city.
The operating system would be a time-shared system running continuously with a vast ﬁle system of shared programs and data.
The segmented virtual address space was merged into the ﬁle system; each segment was a ﬁle.
The ﬁle system itself was a multilevel tree structure, allowing users to create their own subdirectory structures.
Like CTSS, MULTICS used a multilevel feedback queue for CPU scheduling.
Protection was accomplished through an access list associated with each ﬁle and a set of protection rings for executing processes.
It was extended to a multiprocessor system, allowing a CPU to be taken out of service for maintenance while the system continued running.
The longest line of operating-system development is undoubtedly that of IBM computers.
As a result, IBM was faced with many different computers, with different languages and different system software.
Only one set of software would be needed for these systems, which all used the same operating system: OS/360
Unfortunately, OS/360 tried to be all things to all people.
As a result, it did none of its tasks especially well.
The ﬁle system included a type ﬁeld that deﬁned the type of each ﬁle, and different ﬁle types were deﬁned for ﬁxed-length and variable-length records and for blocked and unblocked ﬁles.
Contiguous allocation was used, so the user had to guess the size of each output ﬁle.
The Job Control Language (JCL) added parameters for every possible option, making it incomprehensible to the average user.
Although a base-register addressing mode was used, the program could access and modify the base register, so that absolute addresses were generated by the CPU.
This arrangement prevented dynamic relocation; the program was bound to physical memory at load time.
Two separate versions of the operating system were produced: OS/MFT used ﬁxed regions and OS/MVT used variable regions.
The system was written in assembly language by thousands of programmers, resulting in millions of lines of code.
The operating system itself required large amounts of memory for its code and tables.
Operating-system overhead often consumed one-half of the total CPU cycles.
Over the years, new versions were released to add new features and to ﬁx errors.
However, ﬁxing one error often caused another in some remote part of the system, so that the number of known errors in the system remained fairly constant.
New versions of OS used this hardware in different ways.
OS/VS1 created one large virtual address space and ran OS/MFT in that virtual memory.
Thus, the operating system itself was paged, as well as user programs.
Like MULTICS, TSS/360 was supposed to be a large, time-shared utility.
IBM’s Cambridge Scientiﬁc Center developed CMS as a single-user system and CP/67 to provide a virtual machine to run it on.
As a result, no site would switch from its temporary system to TSS/360
Today, time sharing on IBM systems is largely provided either by TSO under MVS or by CMS under CP/67 (renamed VM)
What went wrong? Part of the problem was that these advanced systems were too large and too complex to be understood.
Another problem was the assumption that computing power would be available from a large, remote source.
Minicomputers came along and decreased the need for large monolithic systems.
They were followed by workstations and then personal computers, which put computing power closer and closer to the end users.
Probably the most famous operating system associated with DEC is VMS, a popular business-oriented system that is still in use today as OpenVMS, a product of Hewlett-Packard.
But perhaps the most inﬂuential of DEC’s operating systems was TOPS-20
TOPS-20 had an advanced command-line interpreter that provided help as needed to users.
That, in combination with the power of the computer and its reasonable price, made the DECSYSTEM-20 the most popular time-sharing system of its time.
Early hobbyist computers were typically built from kits and ran a single program at a time.
The systems evolved into more advanced systems as computer components improved.
An early “standard” operating system for these computers of the 1970s was CP/M, short for Control Program/Monitor, written by Gary Kindall of Digital Research, Inc.
CP/M originally supported only 64 KB of memory and ran only one program at a time.
The command interpreter resembled those in other operating systems of the time, such as the TOPS-10 from DEC.
This operating system, MS-DOS, was similar to CP/M but had a richer set of built-in commands, again mostly modeled after TOPS-10
It supported 640 KB of memory, with the ability to address “extended” and “expanded” memory to get somewhat beyond that limit.
It lacked fundamental current operating-system features, however, especially protected memory.
With the advent of 16-bit CPUs, operating systems for personal computers could become more advanced, feature rich, and usable.
The Apple Macintosh computer was arguably the ﬁrst computer with a GUI designed for home users.
It used a mouse for screen pointing and selecting and came with many utility programs that took advantage of the new user interface.
As microprocessor CPUs evolved to 32-bit chips with advanced features, such as protected memory and context switching, these operating systems added features that had previously been found only on mainframes and minicomputers.
Over time, personal computers became as powerful as those systems and more useful for many purposes.
Minicomputers died out, replaced by general and special-purpose “servers.” Although personal computers continue to increase in capacity and performance, servers tend to stay ahead of them in amount of memory, disk space, and number and speed of available CPUs.
Today, servers typically run in data centers or machine rooms, while personal computers sit on or next to desks and talk to each other and servers across a network.
The desktop rivalry between Apple and Microsoft continues today, with new versions of Windows and Mac OS trying to outdo each other in features, usability, and application functionality.
Other operating systems, such as AmigaOS and OS/2, have appeared over time but have not been long-term competitors to the two leading desktop operating systems.
Meanwhile, Linux in its many forms continues to gain in popularity among more technical users —and even with nontechnical users on systems like the One Laptop per Child (OLPC) children’s connected computer network (http://laptop.org/)
The Mach operating system traces its ancestry to the Accent operating system developed at Carnegie Mellon University (CMU)
Mach’s communication system and philosophy are derived from Accent, but many other signiﬁcant portions of the system (for example, the virtual memory system and task and thread management) were developed from scratch.
Work on Mach began in the mid 1980’s and the operating system was designed with the following three critical goals in mind:
Emulate 4.3 BSD UNIX so that the executable ﬁles from a UNIX system can run correctly under Mach.
Be a modern operating system that supports many memory models, as well as parallel and distributed computing.
Have a kernel that is simpler and easier to modify than 4.3 BSD.
Mach’s development followed an evolutionary path from BSD UNIX systems.
Mach code was initially developed inside the 4.2BSD kernel, with BSD kernel components replaced by Mach components as the Mach components were completed.
The BSD components were updated to 4.3BSD when that became available.
By 1986, the virtual memory and communication subsystems were running on the DEC VAX computer family, including multiprocessor versions of the VAX.
Versions for the IBM RT/PC and for SUN 3 workstations followed shortly.
Through Release 2, Mach provided compatibility with the corresponding BSD systems by including much of BSD’s code in the kernel.
The new features and capabilities of Mach made the kernels in these releases larger than the corresponding BSD kernels.
Mach 3 moved the BSD code outside the kernel, leaving a much smaller microkernel.
This system implements only basic Mach features in the kernel; all UNIX-speciﬁc code has been evicted to run in user-mode servers.
Excluding UNIX-speciﬁc code from the kernel allows the replacement of BSD with another operating system or the simultaneous execution of multiple operating-system interfaces on top of the microkernel.
In addition to BSD, user-mode implementations have been developed for DOS, the Macintosh operating system, and OSF/1
This approach has similarities to the virtual machine concept, but here the virtual machine is deﬁned by software (the Mach kernel interface), rather than by hardware.
With Release 3.0, Mach became available on a wide variety of systems, including single-processor SUN, Intel, IBM, and DEC machines and multiprocessor DEC, Sequent, and Encore systems.
Mach 2.5 was also the basis for the operating system on the NeXT workstation, the brainchild of Steve Jobs of Apple Computer fame.
Unlike UNIX, which was developed without regard for multiprocessing, Mach incorporates multiprocessing support throughout.
This support is also exceedingly ﬂexible, ranging from shared-memory systems to systems with no memory shared between processors.
Mach uses lightweight processes, in the form of multiple threads of execution within one task (or address space), to support multiprocessing and parallel computation.
Its extensive use of messages as the only communication method ensures that protection mechanisms are complete and efﬁcient.
By integrating messages with the virtual memory system, Mach also ensures that messages can be handled efﬁciently.
By providing low-level, or primitive, system calls from which more complex functions can be built, Mach reduces the size of the kernel.
Some previous editions of Operating System Concepts included an entire chapter on Mach.
This chapter, as it appeared in the fourth edition, is available on the Web (http://www.os-book.com)
There are, of course, other operating systems, and most of them have interesting properties.
The MCP operating system for the Burroughs computer family was the ﬁrst to be written in a system programming language.
The SCOPE operating system for the CDC 6600 was also a multi-CPU system.
The coordination and synchronization of the multiple processes were surprisingly well designed.
History is littered with operating systems that suited a purpose for a time (be it a long or a short time) and then, when faded, were replaced by operating systems that had more features, supported newer hardware, were easier to use, or were better marketed.
We are sure this trend will continue in the future.
In what ways is it different from the clock algorithm discussed in Section 9.4.5.2?
Suppose a program consistently uses seven time units every time it is scheduled before it performs an I/O operation and blocks.
How many time units are allocated to this program when it is scheduled for execution at different points in time?
A description of the Apple Macintosh appears in [Apple (1987)]
The Mach operating system and its ancestor, the Accent operating system, are described by [Rashid and Robertson (1981)]
