O’Reilly books may be purchased for educational, business, or sales promotional use.
ZeroMQ, the image of a fourhorn sculpin, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
From the start, I wanted the guide to be a community project, so I put it onto GitHub and let others contribute with pull requests.
This was considered a radical, even vulgar approach by some.
We came to a division of labor: I’d do the writing and make the original C examples, and others would help fix the text and translate the examples into other languages.
You can now find all the examples in several languages, and many in a dozen languages.
It’s a kind of programming language Rosetta Stone, and a valuable outcome in itself.
We set up a high score: reach 80% translation and your language gets its own guide.
About a hundred people have contributed to the guide to date.
This is the magic and power of communities: be the first community in a space, stay healthy, and you own that space for ever.
Conventions Used in This Book We used the following typographical conventions in this book: Italic.
Indicates new terms, commands and command-line options, URLs, email addresses, filenames, and file extensions.
Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, data types, and environment variables.
Constant width italic Shows placeholder user input that you should replace with something that makes sense for you.
You’ll find each example translated into several—often a dozen —other languages.
The examples are licensed under MIT/X11; see the LICENSE file in that directory.
The text of the book explains in each case how to run each example.
An attribution usually includes the title, author, publisher, and ISBN.
Technology professionals, software developers, web designers, and business and creative professionals use Safari Books Online as their primary resource for research, problem solving, learning, and certification training.
Safari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals.
For more information about Safari Books Online, please visit us online.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
For more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.
Acknowledgments Thanks to Andy Oram for making this happen at O’Reilly and editing the book.
Thanks to Martin Sustrik for his years of incredible work on ZeroMQ.
Programming is a science dressed up as art, because most of us don’t understand the physics of software and it’s rarely, if ever, taught.
The physics of software is not algorithms, data structures, languages, and abstractions.
These are just tools we make, use, and throw away.
The real physics of software is the physics of people.
Specifically, it’s about our limitations when it comes to complexity and our desire to work together to solve large problems in pieces.
This is the science of programming: make building blocks that people can understand and use easily, and people will work together to solve the very largest problems.
We live in a connected world, and modern software has to navigate this world.
So, the building blocks for tomorrow’s very largest solutions are connected and massively parallel.
It’s not enough for code to be “strong and silent” any more.
Code has to run like the human brain; trillions of individual neurons firing off messages to each other, a massively parallel network with no central control, no single point of failure, yet able to solve immensely difficult problems.
And it’s no accident that the future of code looks like the human brain, because the endpoints of every network are, at some level, human brains.
If you’ve done any work with threads, protocols, or networks, you’ll realize this is pretty much impossible.
Even connecting a few programs across a few sockets is plain nasty when you start to handle real-life situations.
Connecting computers is so difficult that creating software and services to do this is a multi-billion dollar business.
So we live in a world where the wiring is years ahead of our ability to use it.
We had a software crisis in the 1980s, when leading software engineers like Fred Brooks believed there was no “silver bullet” to “promise even one order of magnitude of improvement in productivity, reliability, or simplicity.”
Brooks missed free and open source software, which solved that crisis, enabling us to share knowledge efficiently.
Today we face another software crisis, but it’s one we don’t talk about much.
Only the largest, richest firms can afford to create connected applications.
Our data and our knowledge are disappearing from our personal computers into clouds that we cannot access and with which we cannot compete.
Who owns our social networks? It is like the mainframe-PC revolution in reverse.
The point is that while the Internet offers the potential of massively connected code, the reality is that this is out of reach for most of us, and so large, interesting problems (in health, education, economics, transport, and so on) remain unsolved because there is no way to connect the code, and thus no way to connect the brains that could work together to solve these problems.
There have been many attempts to solve the challenge of connected software.
There are thousands of IETF specifications, each solving part of the puzzle.
For application developers, HTTP is perhaps the one solution to have been simple enough to work, but it arguably makes the problem worse by encouraging developers and architects to think in terms of big servers and thin, stupid clients.
So today people are still connecting applications using raw UDP and TCP, proprietary protocols, HTTP, and WebSockets.
It remains painful, slow, hard to scale, and essentially centralized.
How many applications use Skype or BitTorrent to exchange data?
To fix the world, we needed to do two things.
One, to solve the general problem of “how to connect any code to any code, anywhere.” Two, to wrap that up in the simplest possible building blocks that people could understand and use easily.
Getting the Examples This book’s examples live in the book’s Git repository.
The simplest way to get all the examples is to clone this repository:
If there are examples missing in a language you use, you’re encouraged to submit a translation.
This is how this book became so useful, thanks to the work of many people.
Any other sequence (e.g., sending two messages in a row) will result in a return code of -1 from the send or recv call.
If you’re reading this online, the link below the example takes you to translations into other programming languages.
For print readers, Example 1-2 shows what the same server looks like in C++
You could throw thousands of clients at this server, all at once, and it would continue to work happily and quickly.
For fun, try starting the client and then starting the server, see how it all still works, and then think for a second what this means.
If you kill the server (Ctrl-C) and restart it, the client won’t recover properly.
In C and some other languages, strings are terminated with a null byte.
We could send a string like “HELLO” with that extra null byte:
However, if you send a string from another language, it probably will not include that null byte.
For example, when we send that same string in Python, we do this:
Then what goes onto the wire is a length (one byte for shorter strings) and the string contents as individual characters (Figure 1-3)
And if you read this from a C program, you will get something that looks like a string, and might by accident act like a string (if by luck the five bytes find themselves followed by an innocently lurking null), but isn’t a proper string.
When your client and server don’t agree on the string format, you will get weird results.
Getting the Message Out The second classic pattern is one-way data distribution, in which a server pushes updates to a set of clients.
Let’s look at an example that pushes out weather updates consisting of a zip code, temperature, and relative humidity.
We’ll generate random values, just like the real weather stations do.
There’s no start and no end to this stream of updates; it’s like a never-ending broadcast (Figure 1-4)
Example 1-7 shows the client application, which listens to the stream of updates and grabs anything to do with a specified zip code (by default, New York City, because that’s a great place to start any adventure)
Note that when you use a SUB socket you must set a subscription using zmq_setsock opt() and SUBSCRIBE, as in this code.
If you don’t set any subscription, you won’t get any messages.
The subscriber can set many subscriptions, which are added together.
That is, if an update matches any subscription, the subscriber receives it.
A subscription is often but not necessarily a printable string.
The client does zmq_msg_recv(), in a loop (or once if that’s all it needs)
Trying to send a message to a SUB socket will cause an error.
There is one more important thing to know about PUB-SUB sockets: you do not know precisely when a subscriber starts to get messages.
Even if you start a subscriber, wait a while, and then start the publisher, the subscriber will always miss the first messages that the publisher sends.
This is because as the subscriber connects to the publisher (something that takes a small but nonzero amount of time), the publisher may already be sending messages out.
Subscriber connects to an endpoint and receives and counts messages.
Publisher binds to an endpoint and immediately sends 1,000 messages.
You’ll blink, check that you set a correct filter, and try again, and the subscriber will still not receive anything.
In Chapter 2, we’ll explain how to synchronize a publisher and subscribers so that you don’t start to publish data until the subscribers really are connected and ready.
There is a simple (and stupid) way to delay the publisher, which is to sleep.
Don’t do this in a real application, though, because it is extremely fragile as well as inelegant and slow.
Use sleeps to prove to yourself what’s happening, and then read Chapter 2 to see how to do this right.
The alternative to synchronization is to simply assume that the published data stream is infinite and has no start and no end.
One also assumes that the subscriber doesn’t care what transpired before it started up.
So, the client subscribes to its chosen zip code and collects a thousand updates for that zip code.
That means about 10 million updates from the server, if zip codes are randomly distributed.
You can start the client, and then the server, and the client will keep working.
You can stop and restart the server as often as you like, and the client will keep working.
When the client has collected its thousand updates, it calculates the average, prints it, and exits.
A subscriber can connect to more than one publisher, using one connect call each time.
Data will then arrive and be interleaved (“fair queued”) so that no single publisher drowns out the others.
If a publisher has no connected subscribers, then it will simply drop all messages.
If you’re using TCP and a subscriber is slow, messages will queue up on the publisher.
We’ll look at how to protect publishers against this by using the “high-water mark” in the next chapter.
Divide and Conquer As a final example (you are surely getting tired of juicy code and want to delve back into philological discussions about comparative abstractive norms), let’s do a little supercomputing.
Our supercomputing application is a fairly typical parallel processing model (Figure 1-5)
A ventilator that produces tasks that can be done in parallel.
A sink that collects results back from the worker processes.
In reality, workers run on superfast boxes, perhaps using GPUs (graphic processing units) to do the hard math.
It generates 100 tasks, each one a message telling the worker to sleep for some number of milliseconds.
The code for the worker application is in Example 1-9
It receives a message, sleeps for that number of seconds, and then signals that it’s finished.
It collects the 100 messages and then calculates how long the overall processing took, so we can confirm that the workers really were running in parallel if there are more than one of them.
When we start one, two, and four workers, we get results like this from the sink:
Let’s look at some aspects of this code in more detail:
The workers connect upstream to the ventilator, and downstream to the sink.
If the workers bound to their endpoints, you would need (a) more endpoints and (b) to modify the ventilator and/or the sink each time you added a worker.
We say that the ventilator and sink are stable parts of our architecture and the workers are dynamic parts of it.
The ventilator’s PUSH socket distributes tasks to workers (assuming they are all connected before the batch starts going out) evenly.
This is called load balancing, and it’s something we’ll look at again in more detail.
The pipeline pattern also exhibits the “slow joiner” syndrome, leading to accusations that PUSH sockets don’t load-balance properly.
If you are using PUSH and PULL and one of your workers gets way more messages than the others, it’s because that PULL socket has joined faster than the others, and grabs a lot of messages before the others manage to connect.
Ugly code hides problems and makes it hard for others to help you.
You might get used to meaningless variable names, but people reading your code won’t.
Use names that are real words, that say something other than “I’m too careless to tell you what this variable is really for.” Use consistent indentation and clean layout.
Write nice code, and your world will be more comfortable.
If you copy/paste a lot of code, you’re going to copy/paste errors, too.
If you’re using the fork() system call, each process needs its own context.
If you do zmq_ctx_new() in the main process before calling fork(), the child processes get their own contexts.
In general, you want to do the interesting stuff in the child processes and just manage these from the parent process.
Always close a message the moment you are done with it, using zmq_msg_close()
If you are opening and closing a lot of sockets, that’s probably a sign that you need.
When you exit the program, close your sockets and then call zmq_ctx_destroy()
In a language with automatic object destruction, sockets and contexts will be destroyed as you leave the scope.
If you use exceptions you’ll have to do the cleanup in something like a “final” block, the same as for any resource.
First, do not try to use the same socket from multiple threads.
Please don’t explain why you think this would be excellent fun; just don’t do it.
Next, you need to shut down each socket that has ongoing requests.
The proper way is to set a low LINGER value (one second), and then close the socket.
If your language binding doesn’t do this for you automatically when you destroy a context, I’d suggest sending a patch.
This will cause any blocking receives or polls or sends in attached threads (i.e., which share the same context) to return with an error.
Catch that error, and then set LINGER on and close sockets in that thread, and exit.
The zmq_ctx_destroy() call in the main thread will block until all sockets it knows about are safely closed.
Many applications these days consist of components that stretch across some kind of network, either a LAN or the Internet.
So, many application developers end up doing some kind of messaging.
Some developers use message queuing products, but most of the time they do it themselves, using TCP or UDP.
These protocols are not hard to use, but there is a great difference between sending a few bytes from A to B and doing messaging in any kind of reliable way.
Let’s look at the typical questions we face when we start to connect pieces using raw TCP.
Any reusable messaging layer would need to address all or most of these:
How do we handle I/O? Does our application block, or do we handle I/O in the background? This is a key design decision.
Blocking I/O creates architectures that do not scale well, but background I/O can be very hard to do right.
How do we handle dynamic components (i.e., pieces that go away temporarily)? Do we formally split components into “clients” and “servers” and mandate that servers cannot disappear? What, then, if we want to connect servers to servers? Do we try to reconnect every few seconds?
How do we represent a message on the wire? How do we frame data so it’s easy to write and read, safe from buffer overflows, and efficient for small messages, yet adequate for the very largest videos of dancing cats wearing party hats?
How do we handle messages that we can’t deliver immediately? Particularly if we’re waiting for a component to come back online? Do we discard messages, put them into a database, or put them into a memory queue?
Where do we store message queues? What happens if the component reading from a queue is very slow and causes our queues to build up? What’s our strategy then?
How do we handle lost messages? Do we wait for fresh data, request a resend, or do we build some kind of reliability layer that ensures messages cannot be lost? What if that layer itself crashes?
What if we need to use a different network transport? Say, multicast instead of TCP unicast? Or IPv6? Do we need to rewrite the applications, or is the transport abstracted in some layer?
How do we route messages? Can we send the same message to multiple peers? Can we send replies back to an original requester?
How do we write an API for another language? Do we reimplement a wire-level protocol, or do we repackage a library? If the former, how can we guarantee efficient and stable stacks? If the latter, how can we guarantee interoperability?
How do we represent data so that it can be read between different architectures? Do we enforce a particular encoding for data types? To what extent is this the job of the messaging system rather than a higher layer?
How do we handle network errors? Do we wait and retry, ignore them silently, or abort?
I see it’s efficient because it uses poll() instead of select()
But really, ZooKeeper should be using a generic messaging layer and an explicitly documented wire-level protocol.
It is incredibly wasteful for teams to be building this particular wheel over and over.
But how do we make a reusable messaging layer? Why, when so many projects need this technology, are people still doing it the hard way by driving TCP sockets in their code, and solving the problems in that long list over and over (Figure 1-7)?
It turns out that building reusable messaging systems is really difficult, which is why few free and open source (FOSS) projects ever tried, and why commercial messaging products are complex, expensive, inflexible, and brittle.
In 2006, iMatix designed the Advanced Message Queuing Protocol, or AMQP, which started to give FOSS developers perhaps the first reusable recipe for a messaging system.
It takes weeks to learn to use it, and months to create stable architectures that don’t crash when things get hairy.
Most messaging projects (like AMQP) that try to solve this long list of problems in a reusable way do so by inventing a new concept, the “broker,” that does addressing, routing, and queuing.
This results in a client/server protocol or a set of APIs on top of some undocumented protocol that allows applications to speak to this broker.
Brokers are an excellent thing in reducing the complexity of large networks.
But adding brokerbased messaging to a product like ZooKeeper would make it worse, not better.
A broker rapidly becomes a bottleneck and a new risk to manage.
If the software supports it, we can add a second, third, and fourth broker and make some failover scheme.
However, it creates more moving pieces, more complexity, more things to break.
You literally need to watch the brokers day and night, and beat them with a stick when they start misbehaving.
You need boxes, and you need backup boxes, and you need people to manage those boxes.
It is only worth doing for large applications with many moving pieces, built by several teams of people over several years.
Either they avoid network programming and make monolithic applications that do not scale, or they jump into network programming and make brittle, complex applications that are hard to maintain.
Or they bet on a messaging product, and end up with scalable applications that depend on expensive, easily broken technology.
There has been no really good choice, which may be why messaging is largely stuck in the last century and stirs strong emotions—negative ones for users, gleeful joy for those selling support and licenses (Figure 1-8)
What we need is something that does the job of messaging, but does it in such a simple and cheap way that it can work in any application, with close to zero cost.
It should be a library with which you link without any other dependencies.
It should run on any OS and work with any programming language.
It does this intelligently, pushing messages as close as possible to the receiver before queuing them.
It lets your applications talk to each other over arbitrary transports: TCP, multicast, in-process, inter-process.
You don’t need to change your code to use a different transport.
It handles slow/blocked readers safely, using different strategies that depend on the messaging pattern.
It lets you route messages using a variety of patterns, such as request-reply and publish-subscribe.
These patterns are how you create the topology, the structure of your network.
It lets you create proxies to queue, forward, or capture messages with a single call.
It delivers whole messages exactly as they were sent, using a simple framing on the wire.
When you want to represent data you choose some other product on top, such as Google’s protocol buffers, XDR, and others.
Sometimes it retries, sometimes it tells you an operation failed.
And it scales: each of these tasks maps to a node, and the nodes talk to each other across arbitrary transports.
Two nodes in one process (node is a thread), two nodes on one box (node is a process), or two boxes on one network (node is a box)—it’s all the same, with no application code changes.
As the clients run, we take a look at the active processes using top, and we see something like (on a four-core box):
Pub-sub filtering is now done on the publisher side instead of the subscriber side.
Most of the API is backward compatible, except a few changes that went into v3.0 with little regard to the cost of breaking existing code.
So although I’d love to say, “You just recompile your code with the latest libzmq and everything will work,” that’s not how it is.
For what it’s worth, we banned such API breakage afterwards.
In other languages, your binding author may have done the work already.
Note that these two functions now return -1 in case of error, and zero or more according to how many bytes were sent or received.
We added zmq_ctx_set() to let you configure a context before starting to work with it.
Warning: Unstable Paradigms! Traditional network programming is built on the general assumption that one socket talks to one connection, one peer.
When we assume “one socket = one connection,” we scale our architectures in certain ways.
We create threads of logic where each thread works with one socket, one peer.
As you read the code examples, your brain will try to map them to what you know.
You will read “socket” and think “Ah, that represents a connection to another node.” That is wrong.
You will read “thread” and your brain will again think, “Ah, a thread represents a connection to another node,” and again your brain will be wrong.
One socket may have many outgoing and many incoming connections.
Your application code cannot work with these connections directly; they are encapsulated under the socket.
Many architectures follow some kind of client/server model, where the server is the component that is most static, and the clients are the components that are most dynamic—i.e., they come and go the most.
There are sometimes issues of addressing: servers will be visible to clients, but not necessarily vice versa.
It also depends on the kind of sockets you’re using, with some exceptions for unusual network architectures.
A server node can bind to many endpoints (that is, a combination of protocol and address), and it can do this using a single socket.
With most transports, you cannot bind to the same endpoint twice (unlike, for example, in UDP)
The ipc transport does, however, let one process bind to an endpoint already used by another process.
It’s meant to allow a process to recover after a crash.
The socket type defines the semantics of the socket: its policies for routing messages inwards and outwards, queuing, etc.
You can connect certain types of socket together, such as a publisher socket and a subscriber socket.
Sockets work together in “messaging patterns.” We’ll look at this in more detail later.
The zmq_msg_send() method does not actually send the message to the socket connection(s)
It queues the message so that the I/O thread can send it asynchronously.
So, the message is not necessarily sent when zmq_msg_send() returns to your application.
It has one limitation: it does not yet work on Windows.
By convention we use endpoint names with an “.ipc” extension to avoid potential conflict with other filenames.
On Unix systems, if you use ipc endpoints you need to create these with appropriate permissions; otherwise, they may not be shareable between processes running under different user IDs.
You must also make sure all processes can access the files, e.g., by running in the same working directory.
We’ve seen that one socket can handle dozens, even thousands of connections at once.
This has a fundamental impact on how you write applications.
A traditional networked application has one process or one thread per remote connection, and that process or.
Request-reply, which connects a set of clients to a set of services.
This is a remote procedure call and task distribution pattern.
Publish-subscribe, which connects a set of publishers to a set of subscribers.
Pipeline, which connects nodes in a fan-out/fan-in pattern that can have multiple steps and loops.
This is a pattern you should use only to connect two threads in a process.
We’ll see an example at the end of this chapter.
The zmq_socket() man page is fairly clear about the patterns; it’s worth reading it several times until it starts to make sense.
These are the socket combinations that are valid for a connect-bind pair (either side can bind):
One of the things we aim to provide you with in this book is a set of such high-level patterns, both small (how to handle messages sanely) and large (how to make a reliable publish-subscribe architecture)
You create and pass around zmq_msg_t objects, not blocks of data.
To read a message, you use zmq_msg_init() to create an empty message, and then.
You then fill that data using memcpy(), and pass the message to zmq_msg_send()
To know how much data the message contains, use zmq_msg_size()
Here is a typical chunk of code working with messages that should be familiar if you have been paying attention.
This is from the zhelpers.h file we use in all the examples:
You can easily extend this code to send and receive blobs of arbitrary length.
You send and receive each part separately, in the low-level API.
You may send zero-length messages, e.g., for sending a signal from one thread to another.
If you want to send files of arbitrary sizes, you should break them into pieces and send each piece as separate single-part messages.
You must call zmq_msg_close() when finished with a message, in languages that don’t automatically destroy objects when a scope closes.
Handling Multiple Sockets The main loop of most examples so far has been:
To actually read from multiple sockets all at once, use zmq_poll()
An even better way might be to wrap zmq_poll() in a framework that turns it into a nice event-driven reactor, but that involves significantly more work than we want to cover here.
Let’s start with a dirty hack, partly for the fun of not doing it right, but mainly because it lets me show you how to do non-blocking socket reads.
This rather confused program acts both as a subscriber to weather updates, and a worker for parallel tasks.
Multiple socket reader (msreader.c) // //  Reading from multiple sockets //  This version uses a simple recv loop // #include "zhelpers.h"
The cost of this approach is some additional latency on the first message (the sleep at the end of the loop, when there are no waiting messages to process)
This would be a problem in applications where submillisecond latency was vital.
Also, you need to check the documentation for nanosleep() (or whatever function you use) to make sure it does not busy-loop.
You can treat the sockets fairly by reading first from one, then the second, rather than prioritizing them as we did in this example.
When you work with multipart messages, each part is a zmq_msg item.
For example, if you are sending a message with five parts, you must construct, send, and destroy five zmq_msg items.
You can do this in advance (and store the zmq_msg items in an array or structure), or as you send them, one by one.
Here is how we send the frames in a multipart message (we receive each frame into a message object):
Here is how we receive and process all the parts in a message, be it single part or multipart:
When you send a multipart message, the first part and all following parts are only actually sent on the wire when you send the final part.
If you are using zmq_poll(), when you receive the first part of a message, all the rest has also arrived.
There is no way to cancel a partially sent message, except by closing the socket.
This pattern is extremely common in the real world and is why our societies and economies are filled with intermediaries who have no other real function than to reduce the complexity and scaling costs of larger networks.
Real-world intermediaries are typically called wholesalers, distributors, managers, and so on.
The Dynamic Discovery Problem One of the problems you will hit as you design larger distributed architectures is discovery.
That is, how do pieces know about each other? It’s especially difficult if pieces come and go; thus, we can call this the “dynamic discovery problem.”
The simplest is to entirely avoid it by hard-coding (or configuring) the network architecture so discovery is done by hand.
That is, when you add a new piece, you reconfigure the network to know about it.
In practice, this leads to increasingly fragile and unwieldy architectures.
Let’s say you have one publisher and a hundred subscribers.
You connect each subscriber to the publisher by configuring a publisher endpoint in each subscriber.
If you continue to connect each subscriber to each publisher, the cost of avoiding dynamic discovery gets higher and higher.
You might wonder, if all networks eventually get large enough to need intermediaries, why don’t we simply have a message broker in place for all applications? For beginners, it’s a fair compromise.
Just always use a star topology, forget about performance, and things will usually work.
However, message brokers are greedy things; in their role as central intermediaries, they become too complex, too stateful, and eventually a problem.
It’s better to think of intermediaries as simple stateless message switches.
The best analogy is an HTTP proxy; it’s there, but doesn’t have any special role.
Adding a pub-sub proxy solves the dynamic discovery problem in our example.
We set the proxy in the “middle” of the network (Figure 2-5)
The proxy opens an XSUB socket and an XPUB socket, and binds each to well-known IP addresses and ports.
Shared Queue (DEALER and ROUTER Sockets) In the “Hello World” client/server application, we have one client that talks to one service.
However, in real cases we usually need to allow multiple services as well as multiple clients.
This lets us scale up the power of the service (many threads or processes or nodes rather than just one)
The only constraint is that services must be stateless, with all state being in the request or in some shared storage, such as a database.
There are two ways to connect multiple clients to multiple servers.
The brute-force way is to connect each client socket to multiple service endpoints.
One client socket can connect to multiple service sockets, and the REQ socket will then distribute requests among these services.
Let’s say you connect a client socket to three service endpoints: A, B, and C.
That’s clearly not the kind of thing we want to be doing at 3 a.m.
Too many static pieces are like liquid concrete: knowledge is distributed, and the more static pieces you have, the more effort it is to change the topology.
What we want is something sitting in between clients and services that centralizes all knowledge of the topology.
Ideally, we should be able to add and remove services or clients at any time without touching any other part of the topology.
When you use REQ to talk to REP, you get a strictly synchronous request-reply dialog.
If either the client or the service tries to do anything else (e.g., sending two requests in a row without waiting for a response), it will get an error.
Obviously, we can use zmq_poll() to wait for activity on either socket, but we can’t use REP and REQ.
Luckily, there are two sockets called DEALER and ROUTER that let you do nonblocking request-response.
You’ll see in Chapter 3 how DEALER and ROUTER sockets let you build all kinds of asynchronous request-reply flows.
For now, we’re just going to see how DEALER and ROUTER let us extend REQ-REP across an intermediarythat is, our little broker.
In this simple extended request-reply pattern, REQ talks to ROUTER and DEALER talks to REP.
In between the DEALER and ROUTER, we have to have code (like our broker) that pulls messages off one socket and shoves them onto the other (Figure 2-8)
The request-reply broker binds to two endpoints, one for clients to connect to (the frontend socket) and one for workers to connect to (the backend)
To test this broker, you will want to change your workers so they connect to the backend socket.
Example 2-3 is a client that shows what I mean.
And finally, the code for the broker, which properly handles multipart messages, is in Example 2-5
Request-reply broker (rrbroker.c) // //  Simple request-reply broker // #include "zhelpers.h"
Using a request-reply broker makes your client/server architectures easier to scale, because clients don’t see workers and workers don’t see clients.
The only static node is the broker in the middle (Figure 2-9)
The two sockets (or three if we want to capture data) must be properly connected, bound, and configured.
When we call the zmq_proxy() method, it’s exactly like starting the main loop of rrbroker.
Let’s rewrite the request-reply broker to call zmq_proxy(), and re-badge this as an expensive-sounding “message queue” (people have charged houses for code that did less)
Message queue broker (msgqueue.c) // //  Simple message queuing broker //  Same as request-reply broker but using QUEUE device // #include "zhelpers.h"
As an example, we’re going to write a little proxy (Example 2-7) that sits in between a publisher and a set of subscribers, bridging two networks.
The frontend socket (SUB) faces the internal network where the weather server is sitting, and the backend (PUB) faces subscribers on the external network.
It subscribes to the weather service on the frontend socket, and republishes its data on the backend socket (Figure 2-10)
Weather update proxy (wuproxy.c) // //  Weather proxy device // #include "zhelpers.h"
It looks very similar to the earlier proxy example, but the key part is that the frontend and backend sockets are on two different networks.
We can use this model, for example, to connect a multicast network (pgm transport) to a TCP publisher.
Methods that process data may return the number of bytes processed, or -1 on an.
There are two main exceptional conditions that you may want to handle as nonfatal:
We’ll take the parallel pipeline example from the previous section.
If we’ve started a whole lot of workers in the background, we now want to kill them when the batch is finished.
Let’s do this by sending a kill message to the workers.
The best place to do this is the sink, because it really knows when the batch is done.
The sink creates a PUB socket on a new endpoint.
When the sink detects the end of the batch, it sends a kill to its PUB socket.
When it’s finished collecting results, it broadcasts a kill message to all workers.
Handling Interrupt Signals Realistic applications need to shut down cleanly when interrupted with Ctrl-C or another signal, such as SIGTERM.
By default, these simply kill the process, meaning messages won’t be flushed, files won’t be closed cleanly, and so on.
Example 2-10 shows how we handle signals in various languages.
So, check for an EINTR return code, a NULL return, and/or s_interrupted.
If you call s_catch_signals() and don’t test for interrupts, your application will become immune to Ctrl-C and SIGTERM, which may be useful but is usually not.
Detecting Memory Leaks Any long-running application has to manage memory correctly, or eventually it’ll use up all available memory and crash.
If you use a language that handles this automatically for you, congratulations.
If you program in C or C++ or any other language where you’re responsible for memory management, here’s a short tutorial on using valgrind, which, among other things, will report on any leaks your programs have:
To install valgrind, such as on Ubuntu or Debian, issue:
For any application that exits by itself, that’s not needed, but for long-running applications, this is essential.
Build your application with -DDEBUG, if it’s not your default setting.
That ensures valgrind can tell you exactly where memory is being leaked.
By “perfect” MT programs, I mean code that’s easy to write and understand, that works with the same design approach in any programming language and on any operating system, and that scales across any number of CPUs with zero wait states and no point of diminishing returns.
If you’ve spent years learning tricks to make your MT code work at all, let alone rapidly, with locks and semaphores and critical sections, you will be disgusted when you realize it was all for nothing.
If there’s one lesson we’ve learned from 30+ years of concurrent programming, it is: just don’t share state.
Sooner or later, they’re going to get into a fight.
And the more drunkards you add to the table, the more they fight each other over the beer.
The tragic majority of MT applications look like drunken bar fights.
The list of weird problems that you need to fight as you write classic shared-state MT code would be hilarious if it didn’t translate directly into stress and risk, as code that seems to work suddenly fails under pressure.
A few years ago, a large firm with worldbeating experience in buggy code released its list of “11 Likely Problems in Your Multithreaded Code,” which covers forgotten synchronization, incorrect granularity, read and write tearing, lock-free reordering, lock convoys, the two-step dance, and priority inversion.
The point is, do you really want code running the power grid or stock market to start getting twostep lock convoys at 3 p.m.
Some widely used models, despite being the basis for entire industries, are fundamentally broken, and shared state concurrency is one of them.
Code that wants to scale without limit does it like the Internet does, by sending messages and sharing nothing except a common contempt for broken programming models.
You may treat threads as separate tasks with their own context, but these threads cannot communicate over inproc.
However, they will be easier to break into standalone processes afterwards.
The only place where it’s remotely sane to share sockets between threads is in language bindings that need to do magic like garbage collection on sockets.
If you need to start more than one proxy in an application, for example, you will want to run each in its own thread.
It is easy to make the error of creating the proxy frontend and backend sockets in one thread, and then passing the sockets to the proxy in another thread.
Remember: do not use or close sockets except in the thread that created them.
If you follow these rules, you can quite easily split threads into separate processes when you need to.
Application logic can sit in threads, processes or nodes: whatever your scale needs.
Further, running workers as threads will cut out a network hop, latency, and network traffic.
The MT version of the Hello World service in Example 2-11 basically collapses the broker and workers into a single process.
We use pthreads because it’s the most widespread standard for multithreading.
All the code should be recognizable to you by now.
Each worker thread creates a REP socket and then processes requests on this socket.
The only differences are the transport (inproc instead of tcp) and the bind-connect direction.
The server creates a ROUTER socket to talk to clients and binds this to its external interface (over tcp)
The server creates a DEALER socket to talk to the workers and binds this to its internal interface (over inproc)
The server starts a proxy that connects the two sockets.
The proxy pulls incoming requests fairly from all clients and distributes those out to workers.
Note that creating threads is not portable in most programming languages.
The POSIX library is pthreads, but on Windows you have to use a different API.
We’ll see in Chapter 3 how to wrap this in a portable API.
Let’s make three threads that signal each other when they are ready (Figure 2-13)
In Example 2-12, we use PAIR sockets over the inproc transport.
The child thread creates the second socket, connects it to that inproc endpoint, and then signals to the parent thread that it’s ready.
Note that multithreading code using this pattern is not scalable out to processes.
If you use inproc and socket pairs, you are building a tightly bound application; i.e., one where your threads are structurally interdependent.
The other design pattern is a loosely bound application, where threads have their own context and communicate over ipc or tcp.
You can easily break loosely bound threads into separate processes.
This is the first time we’ve shown an example using PAIR sockets.
Why use PAIR? Other socket combinations might seem to work, but they all have side effects that could interfere with signaling:
You can use PUSH for the sender and PULL for the receiver.
This looks simple and will work, but remember that PUSH will distribute messages to all available receivers.
If you by accident start two receivers (e.g., you already have one running and you start a second), you’ll “lose” half of your signals.
You can use DEALER for the sender and ROUTER for the receiver.
ROUTER, however, wraps your message in an “envelope,” meaning your zero-size signal turns into a multipart message.
If you don’t care about the data and treat anything as a valid signal, and if you don’t read more than once from the socket, that won’t matter.
If, however, you decide to send real data, you will suddenly find ROUTER providing you with “wrong” messages.
You can use PUB for the sender and SUB for the receiver.
This will correctly deliver your messages exactly as you sent them, and PUB does not distribute as PUSH or DEALER do.
However, you need to configure the subscriber with an empty subscription, which is annoying.
Worse, the reliability of the PUB-SUB link is timingdependent, and messages can get lost if the SUB socket is connecting while the PUB socket is sending its messages.
For these reasons, PAIR makes the best choice for coordination between pairs of threads.
Node Coordination When you want to coordinate nodes, PAIR sockets won’t work well any more.
This is one of the few areas where the strategies for threads and nodes are different.
The second significant difference between threads and nodes is that you typically have a fixed number of threads but a more variable number of nodes.
Let’s take one of our earlier scenarios (the weather server and clients) and use node coordination to ensure that subscribers don’t lose data when starting up.
The publisher knows in advance how many subscribers it expects.
This is just a magic number it gets from somewhere.
The publisher starts up and waits for all subscribers to connect.
Each subscriber subscribes and then tells the publisher it’s ready via another socket.
When the publisher has all subscribers connected, it starts to publish data.
In this case, we’ll use a REQ-REP socket flow to synchronize the subscribers and the publisher (Figure 2-14)
Synchronized publisher (syncpub.c) // //  Synchronized publisher // #include "zhelpers.h"
Synchronized subscriber (syncsub.c) // //  Synchronized subscriber // #include "zhelpers.h"
We can’t assume that the SUB connect will be finished by the time the REQ/REP dialog is complete.
There are no guarantees that outbound connects will finish in any order.
So, the example does a bruteforce sleep of one second between subscribing and sending the REQ/REP synchronization.
Publisher opens PUB socket and starts sending “Hello” messages (not data)
Subscribers connect to SUB socket, and when they receive a “Hello” message, they.
When the publisher has had all the necessary confirmations, it starts to send real.
Pub-Sub Message Envelopes In the pub-sub pattern, we can split the key into a separate message frame that we call an envelope.
If you want to use pub-sub envelopes, make them yourself.
It’s optional, and in previous pub-sub examples, we didn’t do this.
Using a pub-sub envelope is a little more work for simple cases, but it’s cleaner, especially for real cases, where the key and the data are naturally separate things.
Figure 2-15 shows is what a publish-subscribe message with an envelope looks like.
That is, they look for “all messages starting with XYZ.” The obvious question is: how to delimit keys from data so that the prefix match doesn’t accidentally match data.
The best answer is to use an envelope, because the match won’t cross a frame boundary.
Here is a minimalist example of how pub-sub envelopes look in code.
This publisher (Example 2-15) sends messages of two types, A and B.
The subscriber, shown in Example 2-16, wants only messages of type B.
Pub-sub envelope subscriber (psenvsub.c) // //  Pub-sub envelope subscriber // #include "zhelpers.h"
When you run the two programs, the subscriber should show you this: [B] We would like to see this [B] We would like to see this [B] We would like to see this [B] We would like to see this ...
This example shows that the subscription filter rejects or accepts the entire multipart message (key plus data)
If you subscribe to multiple publishers and you want to know their addresses so that you can send them data via another socket (and this is a typical use case), create a threepart message, as illustrated in Figure 2-16
High-Water Marks When you can send messages rapidly from process to process, you soon discover that memory is a precious resource, and one that can be trivially filled up.
A few seconds of delay somewhere in a process can turn into a backlog that blows up a server unless you understand the problem and take precautions.
The problem is this: if you have process A sending messages to process B, which suddenly gets very busy (garbage collection, CPU overload, whatever), then what happens to the messages that process A wants to send? Some will sit in B’s network buffers.
If you don’t take some precaution, A can easily run out of memory and crash.
What are the answers? One is to pass the problem upstream.
A is getting the messages from somewhere else, so tell that process to stop, and so on up the line.
It sounds great, but what if you’re sending out a Twitter feed? Do you tell the whole world to stop tweeting while B gets its act together?
Flow control works in some cases, but not in others.
The answer for messaging is to set limits on the size of buffers, and then, when we reach those limits, to take some sensible action.
In some cases (not for a subway system, though), the answer is to throw away messages.
Each connection out of a socket or into a socket has its own pipe, and HWM, for sending and/or receiving, depending on the socket type.
Some (DEALER, ROUTER, PAIR) have both send and receive buffers.
When your socket reaches its HWM, it will either block or drop data, depending on the socket type.
Over the inproc transport, the sender and receiver share the same buffers, so the real HWM is the sum of the HWMs set by both sides.
Lastly, the high-water marks are counted in message parts, not whole messages.
When you use the ROUTER socket type (discussed in detail in the next chapter), every message is at least two parts.
In short, if you have not proven that an architecture works in realistic conditions, it will most likely break at the worst possible moment.
The Request-Reply Mechanisms We already looked briefly at multipart messages.
Let’s now look at a major use case, which is reply message envelopes.
An envelope is a way of safely packaging up data with an address, without touching the data itself.
By separating reply addresses into an envelope, we make it possible to write general-purpose intermediaries such as APIs and proxies that create, read, and remove addresses no matter what the message payload or structure is.
When you use REQ and REP sockets, you don’t even see envelopes; these sockets deal with them automatically.
But for most of the interesting request-reply patterns, you’ll want to understand envelopes and ROUTER sockets.
The Simple Reply Envelope A request-reply exchange consists of a request message, and an eventual reply message.
In the simple request-reply pattern there’s one reply for each request.
In more advanced patterns, requests and replies can flow asynchronously.
The REQ socket creates the simplest possible reply envelope, which has no addresses, just an empty delimiter frame and the message frame containing the “Hello” string.
The REP socket does the matching work: it strips off the envelope, up to and including the delimiter frame, saves the whole envelope, and passes the “Hello” string up the application.
Thus, our original “Hello World” example used request-reply envelopes internally, but the application never saw them.
If you spy on the network data flowing between hwclient and hwserver, this is what you’ll see: every request and every reply is in fact two frames, an empty frame and then the body.
This may not seem to make much sense for a simple REQ-REP dialog, but you’ll see the reason when we explore how ROUTER and DEALER handle envelopes.
The Extended Reply Envelope Now let’s extend the REQ-REP pair with a ROUTER-DEALER proxy in the middle and see how this affects the reply envelope.
We can, in fact, insert any number of proxy steps (Figure 3-2)
The proxy does this, in pseudo-code: prepare context, frontend and backend sockets while true: poll on both sockets if frontend had input: read all frames from frontend send to backend if backend had input: read all frames from backend send to frontend.
The ROUTER socket, unlike other sockets, tracks every connection it has, and tells the caller about these.
The way it tells the caller is by sticking the connection identity in front of each message received.
An identity, sometimes called an address, is just a binary string with no meaning except “this is a unique handle to the connection.” Then, when you send a message via a ROUTER socket, you first send an identity frame.
When receiving messages, a ZMQ_ROUTER socket shall prepend a message part containing the identity of the originating peer to the message before passing it to the application.
When sending messages a ZMQ_ROUTER socket shall remove the first part of the message and use it to determine the identity of the peer the message shall be routed to.
If there are three REQ sockets connected to a ROUTER socket, it will invent three random identities, one for each REQ socket.
Internally, this means the ROUTER socket keeps a hash table where it can search for 02 and find the TCP connection for the REQ socket.
When we receive the message off the ROUTER socket, we get three frames (Figure 3-3)
The core of the proxy loop is “read from one socket, write to the other,” so we literally send these three frames out on the DEALER socket.
If you now sniffed the network traffic, you would see these three frames flying from the DEALER socket to the REP socket.
The REP socket does as before: strips off the whole envelope, including the new reply address, and once again delivers the “Hello” to the caller.
Incidentally, the REP socket can only deal with one request-reply exchange at a time, which is why if you try to read multiple requests or send multiple replies without sticking to a strict recv-send cycle, it gives an error.
You should now be able to visualize the return path.
When hwserver sends “World” back, the REP socket wraps that with the envelope it saved and sends a three-frame reply message across the wire to the DEALER socket (Figure 3-4)
Now the DEALER reads these three frames and sends all three out via the ROUTER socket.
The ROUTER takes the first frame of the message, which is the 02 identity, and looks up the connection for this.
If it finds that, it then pumps the next two frames out onto the wire (Figure 3-5)
What’s This Good For? To be honest, the use cases for strict request-reply or extended request-reply are somewhat limited.
For one thing, there’s no easy way to recover from common failures like the server crashing due to buggy application code (we’ll see more about this in Chapter 4)
However, once you grasp the way these four sockets deal with envelopes, and how they talk to each other, you can do very useful things.
We saw how ROUTER uses the reply envelope to decide which client REQ socket to route a reply back to.
Each time ROUTER gives you a message, it tells you what peer that message came from, as an identity.
You can use this with a hash table (with the identity as the key) to track new peers as they arrive.
All they care about is that one identity frame that lets them figure out which connection to send a message to.
The REQ socket sends to the network an empty delimiter frame in front of the message data.
If you connect a REQ socket to multiple peers, requests are distributed to and replies expected from each peer in turn, one at a time.
The REP socket reads and saves all identity frames up to and including the empty delimiter, then passes the following frame or frames to the caller.
If you connect a REP socket to multiple peers, requests are read from peers in fair fashion, and replies are always sent to the same peer that made the last request.
The DEALER socket is oblivious to the reply envelope and handles this like any multipart message.
They distribute sent messages among all connections, and fair-queue received messages from all connections.
The ROUTER socket is oblivious to the reply envelope, like DEALER.
It creates identities for its connections and passes these identities to the caller as a first frame in any received message.
Conversely, when the caller sends a message, it uses the first message frame as an identity to look up the connection to send to.
Request-Reply Combinations We have four request-reply sockets, each with a certain behavior.
We’ve seen how they connect in simple and extended request-reply patterns.
But these sockets are building blocks that you can use to solve many problems.
Where we use a REQ socket, we can use a DEALER; we just have to read and write the envelope ourselves.
Where we use a REP socket, we can stick a ROUTER; we just need to manage the identities ourselves.
Think of REQ and DEALER sockets as “clients” and REP and ROUTER sockets as “servers.” Mostly, you’ll want to bind REP and ROUTER sockets, and connect REQ and DEALER sockets to them.
It’s not always going to be this simple, but it is a clean and memorable place to start.
The REQ to REP Combination We’ve already covered a REQ client talking to a REP server, but there’s one important aspect to mention here: the REQ client must initiate the message flow.
A REP server cannot talk to a REQ client that hasn’t first sent it a request.
Technically, it’s not even possible, and the API also returns an EFSM error if you try it.
This gives us an asynchronous client that can talk to multiple REP servers.
If we rewrote our “Hello World” client using DEALER, we’d be able to send off any number of “Hello” requests without waiting for replies.
When we use a DEALER to talk to a REP socket, we must accurately emulate the envelope that the REQ socket would have sent, or the REP socket will discard the message as invalid.
Send an empty message frame with the MORE flag set.
Receive the first frame and, if it’s not empty, discard the whole message.
Receive the next frame and pass that to the application.
This gives us an asynchronous server that can talk to multiple REQ clients at the same time.
If we rewrote our “Hello World” server using ROUTER, we’d be able to process any number of “Hello” requests in parallel.
As a proxy that switches messages between frontend and backend sockets.
As an application that reads the message and acts on it.
In the first case, the ROUTER simply reads all frames, including the artificial identity frame, and passes them on blindly.
In the second case, the ROUTER must know the format of the reply envelope it’s being sent.
As the other peer is a REQ socket, the ROUTER gets the identity frame, an empty frame, and then the data frame.
The DEALER to ROUTER Combination Now we can switch out both REQ and REP with DEALER and ROUTER to get the most powerful socket combination, which is DEALER talking to ROUTER.
It gives us asynchronous clients talking to asynchronous servers, where both sides have full control over the message formats.
Because both DEALER and ROUTER can work with arbitrary message formats, if you hope to use these safely, you have to become a little bit of a protocol designer.
At the very least, you must decide whether you wish to emulate the REQ/REP reply envelope.
It depends on whether you actually need to send replies or not.
The DEALER to DEALER Combination You can swap a REP with a ROUTER, but you can also swap a REP with a DEALER, if the DEALER is talking to one and only one peer.
When you replace a REP with a DEALER, your worker can suddenly go fully asynchronous, sending any number of replies back.
The cost is that you have to manage the reply envelopes yourself, and get them right, or nothing at all will work.
Let’s just say for now that DEALER to DEALER is one of the trickier patterns to get right, and happily it’s rare that we need it.
Invalid Combinations Mostly, trying to connect clients to clients, or servers to servers, is a bad idea and won’t work.
Rather than give general vague warnings, I’ll explain in detail what’s wrong with each of the following combinations: REQ to REQ.
Both sides want to start by sending messages to each other, and this could only work if you timed things so that both peers exchanged messages at exactly the same time.
Thus, the REQ socket would get confused, and/or return messages meant for another client.
It’s messy, though, and adds nothing over DEALER to ROUTER.
We’ve already seen how they work by routing individual messages to specific connections.
I’ll explain in more detail how we identify those connections, and what a ROUTER socket does when it can’t send a message.
An application that uses a ROUTER socket to talk to specific peers can convert a logical address to an identity if it has built the necessary hash table.
Because ROUTER sockets only announce the identity of a connection (to a specific peer) when that peer sends a message, you can only really reply to a message, not spontaneously talk to a peer.
This is true even if you flip the rules and make the ROUTER connect to the peer rather than wait for the peer to connect to the ROUTER.
However, you can force the ROUTER socket to use a logical address in place of its identity.
The zmq_setsockopt() reference page calls this setting the socket identity.
The peer application sets the ZMQ_IDENTITY option of its peer socket (DEALER or REQ) before binding or connecting.
Usually the peer then connects to the already bound ROUTER socket, but the ROUTER can also connect to the peer.
At connection time, the peer socket tells the router socket, “Please use this identity for this connection.”
If the peer socket doesn’t say that, the router generates its usual arbitrary random identity for the connection.
The ROUTER socket now provides this logical address to the application as a prefix identity frame for any messages coming in from that peer.
The ROUTER also expects the logical address as the prefix identity frame for any outgoing messages to that peer.
Identity check (identity.c) // //  Demonstrate identities as used by the request-reply pattern.
Note that the utility functions s_ are provided by //  zhelpers.h.
It gets boring for everyone to keep repeating this code.
It’s an attitude that makes sense in working code, but it makes debugging hard.
The “send identity as first frame” approach is tricky enough that we often get this wrong when we’re learning, and the ROUTER’s stony silence when we mess up isn’t very constructive.
We’ll see how to connect a ROUTER socket to a REQ socket, and then to a DEALER socket.
These two examples follow the same logic, which is a load-balancing pattern.
This pattern is our first exposure to using the ROUTER socket for deliberate routing, rather than it simply acting as a reply channel.
The load-balancing pattern is very common and we’ll see it several times in this book.
It solves the main problem with simple round-robin routing (as PUSH and DEALER offer), which is that round robin becomes inefficient if tasks do not all take roughly the same time.
It’s the post office analogy: if you have one queue per counter, and you have some people buying stamps (a fast, simple transaction), and some people opening new accounts (a very slow transaction), you will find stamp buyers getting unfairly stuck in queues.
And just as in a post office, if your messaging architecture is unfair, people will get annoyed.
The solution in the post office is to create a single queue so that even if one or two counters get stuck with slow work, other counters will continue to serve clients on a first-come, first-serve basis.
One reason PUSH and DEALER use this simplistic approach is sheer performance.
If you arrive in any major US airport, you’ll find long queues of people waiting at immigration.
The border patrol officials will send people in advance to queue up at each counter, rather than using a single queue.
Having people walk 50 yards in advance saves a minute or two per passenger.
And since every passport check takes roughly the same time, it’s more or less fair.
This is the strategy for PUSH and DEALER: send workloads ahead of time so that there is less travel distance.
Let’s return to the scenario of a worker (DEALER or REQ) connected to a broker (ROUTER)
The broker has to know when the worker is ready, and keep a list of workers so that it can take the least recently used worker each time.
The solution is really simple, in fact: workers send a “ready” message when they start, and after they finish each task.
Each time it reads a message, it is from the last-used worker.
And since we’re using a ROUTER socket, we get an identity that we can then use to send a task back to the worker.
It’s a twist on request-reply because the task is sent with the reply, and any response for the task is sent as a new request.
While this example runs in a single process, that is only to make it easier to start and stop.
Each thread has its own context and conceptually acts as a separate process.
The example runs for five seconds, and then each worker prints how many tasks it handled.
If the routing worked, we’d expect a fair distribution of work:
To talk to the workers in this example, we have to create a REQ-friendly envelope consisting of an identity plus an empty envelope delimiter frame (Figure 3-6)
The REQ socket always sends an empty delimiter frame before any data frames; the DEALER does not.
The REQ socket will send only one message before it receives a reply; the DEALER is fully asynchronous.
The synchronous versus asynchronous behavior has no effect on our example since we’re doing strict request-reply.
Now let’s look at exactly the same example, but with the REQ socket replaced by a DEALER socket (Example 3-4)
The code is almost identical, except that the worker uses a DEALER socket and reads and writes that empty frame before the data frame.
This is the approach I use when I want to maintain compatibility with REQ workers.
However, remember the reason for that empty delimiter frame: it’s to allow multihop extended requests that terminate in a REP socket, which uses that delimiter to split off the reply envelope so it can hand the data frames to its application.
If we never need to pass the message along to a REP socket, we can simply drop the empty delimiter frame at both sides, which makes things simpler.
This is usually the design I use for pure DEALER to ROUTER protocols.
It can manage a set of workers with dummy requests and replies, but it has no way to talk to clients.
If we add a second frontend ROUTER socket that accepts client requests, and turn our example into a proxy that can switch messages from frontend to backend, we get a useful and reusable tiny load-balancing message broker (Figure 3-7)
Accepts requests from clients and holds these in a single queue.
The broker code (listed in Example 3-5) is fairly long, but worth understanding.
Load-balancing broker (lbbroker.c) // //  Load-balancing broker //  Clients and workers are shown here in-process //
The main task starts the clients and workers, and then routes requests between the two layers (Example 3-7)
Workers signal “ready” when they start; after that, we treat them as ready when they reply with a response back to a client.
The load-balancing data structure is just a queue of next available workers.
It has two sockets: a frontend for clients and a backend for workers.
It polls the backend in all cases, and polls the frontend only when there are one or more workers ready.
The difficult parts of this program are the envelopes that each socket reads and writes, and the load-balancing algorithm.
We’ll take these in turn, starting with the message envelope formats.
Let’s walk through a full request-reply chain from client to worker and back.
In this code we set the identity of the client and worker sockets to make it easier to trace the message frames.
In reality we’d allow the ROUTER sockets to invent identities for connections.
Let’s assume the client’s identity is “CLIENT” and the worker’s identity is “WORKER.” The client application sends a single frame containing “HELLO” (Figure 3-8)
Since the REQ socket adds its empty delimiter frame and the ROUTER socket adds its connection identity, the proxy reads three frames off the frontend ROUTER socket: the client address, the empty delimiter frame, and the data part (Figure 3-9)
The broker sends this to the worker, prefixed by the address of the chosen worker, plus an additional empty part to keep the REQ at the other end happy (Figure 3-10)
This complex envelope stack gets chewed up first by the backend ROUTER socket, which removes the first frame.
Then the REQ socket in the worker removes the empty part, and provides the rest to the worker application (Figure 3-11)
The worker has to save the envelope (which is all the parts up to and including the empty message frame), and then it can do what’s needed with the data part.
Note that a REP socket would do this automatically, but we’re using the REQ-ROUTER pattern so that we can get proper load balancing.
On the return path, the messages are the same as when they come in; i.e., the backend socket gives the broker a message in five parts, the broker sends the frontend socket a message in three parts, and the client gets a message in one part.
It requires that both clients and workers use REQ sockets, and that workers correctly store and replay the envelopes on messages they get.
Create a poll set that always polls the backend, and polls the frontend only if there are one or more workers available.
If there is activity on the backend, we either have a “ready” message or a reply for.
In either case, we store the worker address (the first part) on our worker queue, and if the rest is a client reply, we send it back to that client via the frontend.
If there is activity on the frontend, we take the client request, pop the next worker (which is the last used), and send the request to the backend.
This means sending the worker address, the empty part, and then the three parts of the client request.
You should now see that you can reuse and extend the load-balancing algorithm with variations based on the information the worker provides in its initial “ready” message.
For example, workers might start up and do a performance self-test, then tell the broker how fast they are.
The broker can then choose the fastest available worker rather than the oldest.
What we want is an API that lets us receive and send an entire message in one shot, including the reply envelope with any number of reply addresses—one that lets us do what we want with the absolute fewest lines of code.
The challenge of making a good API affects all languages, though my specific use case is C.
Whatever language you use, think about how you could contribute to your language binding to make it as good as (or better than) the C binding I’m going to describe.
Here is the worker code, rewritten onto an API using these concepts:
I find it cumbersome to have to close sockets manually, and to have to explicitly define the linger timeout in some (but not all) cases.
It’d be great to have a way to close sockets automatically when I close the context.
Writing a lot of these, we end up doing the same work over and over: calculating timers, and calling code when sockets are ready.
A simple reactor with socket readers and timers would save a lot of repeated work.
It would be useful if this happened in all applications.
Example 3-10 shows the load-balancing broker rewritten to use a higher-level API (CZMQ for the C case)
This has identical functionality to the previous lbbroker example, but it uses CZMQ to start child threads, to hold the list of workers, and to read and send messages.
The main loop for the load balancer is shown in Example 3-12
It works the same way as the previous example, but is a lot shorter because CZMQ gives us an API that does more with fewer calls.
So how about reactors? The CZMQ zloop reactor is simple but functional.
Set a reader on any socket (i.e., code that is called whenever the socket has input)
Set a timer that goes off once or multiple times at specific intervals.
It rebuilds its poll set each time you add or remove readers, and it calculates the poll timeout to match the next timer.
Then it calls the reader and timer handlers for each socket and timer that need attention.
The actual handling of messages sits inside dedicated functions or methods.
You may not like the style—it’s a matter of taste.
What it does help with is mixing timers and socket activity.
In the rest of this text, we’ll use zmq_poll() in simpler cases, and zloop in more complex examples.
Example 3-13 shows the load-balancing broker rewritten once again, this time to use zloop.
Load-balancing broker using zloop (lbbroker3.c) // //  Load-balancing broker //  Demonstrates use of the CZMQ API and reactor style // //  The client and worker tasks are identical to the previous example ...
In the reactor design, each time a message arrives on a socket, the reactor passes it to a handler function.
We have two handlers, one for the frontend and one for the backend, and as seen in Example 3-14
The main task (Example 3-15) now sets up child tasks and then starts its reactor.
If you press Ctrl-C, the reactor exits and the main task shuts down.
Because the reactor is a CZMQ class, this example may not translate into all languages equally well.
Getting applications to shut down properly when you send them Ctrl-C can be tricky.
If you use the zctx class it’ll automatically set up signal handling, but your code still has to cooperate.
If you have nested loops, it can be useful to make the outer ones conditional on !zctx_interrupted.
The Asynchronous Client/Server Pattern In the ROUTER to DEALER example, we saw a 1-to-N use case where one server talks asynchronously to multiple workers.
For each request, the server sends 0 or more replies.
Clients can send multiple requests without waiting for a reply.
Servers can send multiple replies without waiting for new requests.
Asynchronous client-to-server (DEALER to ROUTER) // //  While this example runs in a single process, that is only to make //  it easier to start and stop the example.
Each task has its own //  context and conceptually acts as a separate process.
It connects to the server, and then sends a request once per second.
It collects responses as they arrive, and it prints them out.
We will //  run several client tasks in parallel, each with a different random ID.
It uses the multithreaded server model to deal requests out to a pool of workers and route replies back to clients.
One worker can handle one request at a time, but one client can talk to multiple workers at once.
Each worker task works on one request at a time and sends back a random number of replies, with random delays between replies, as illustrated in Example 3-18
The main thread simply starts several clients and a server, and then //  waits for the server to finish.
The example runs in one process, with multiple threads simulating a real multiprocess architecture.
When you run the example, you’ll see three clients (each with a random ID), printing out the replies they get from the server.
Look carefully and you’ll see each client task gets zero or more replies per request.
The clients send a request once per second, and get zero or more replies back.
The server uses a pool of worker threads, each processing one request synchronously.
It connects these to its frontend socket using an internal queue.
It connects the frontend and backend sockets using a zmq_proxy() call.
Figure 3-13 shows a detailed view of the architecture of this example.
Note that we’re doing DEALER to ROUTER dialog between the clients and the server, but internally between the server’s main thread and workers, we’re doing DEALER to DEALER.
If the workers were strictly synchronous, we’d use REP, but since we want to send multiple replies we need an async socket.
We do not want to route replies; they always go to the single server thread that sent us the request.
The server thread receives a two-part message (real message prefixed by client identity)
We send this on to the worker, which treats it as a normal reply envelope and returns that to us.
We can then use it to route the reply back to the right client:
Now for the sockets: we could use the load-balancing ROUTER to DEALER pattern to talk to workers, but it’s extra work.
In this case, a DEALER to DEALER pattern is probably fine: the trade-off is lower latency for each request, but higher risk of unbalanced work distribution.
When you build servers that maintain stateful conversations with clients, you will run into a classic problem.
If the server keeps some state per client, and clients keep coming and going, eventually the server will run out of resources.
Even if the same clients keep connecting, if you’re using default identities, each connection will look like a new one.
We cheat in this example by keeping state only for a very short time (the time it takes a worker to process a request) and then throwing away the state.
To properly manage client state in a stateful asynchronous server, you have to:
In our example, we send a request once per second, which can reliably be used as a heartbeat.
Store state using the client identity (whether generated or explicit) as the key.
If there’s no request from a client within, say, two seconds, the server can detect this and destroy any state it’s holding for that client.
Worked Example: Inter-Broker Routing Let’s take everything we’ve seen so far and scale things up to a real application.
Suppose our best client calls us urgently and asks for a design of a large cloud computing facility.
He has this vision of a cloud that spans many data centers, each a cluster of clients and workers, and that works together as a whole.
Establishing the Details Several espressos later, we want to jump into writing code, but a little voice tells us to get more details before making a sensational solution to entirely the wrong problem.
Workers run on various kinds of hardware, but they are all able to handle any task.
There are several hundred workers per cluster, and as many as a dozen clusters in total.
Each task is an independent unit of work, and all the client wants is to find an available worker and send it the task as soon as possible.
There will be a lot of clients and they’ll come and go arbitrarily.
The real difficulty is to be able to add and remove clusters at any time.
A cluster can leave or join the cloud instantly, bringing all its workers and clients with it.
If there are no workers in its own cluster, a client’s tasks will go off to other available workers in the cloud.
Clients send out one task at a time, waiting for a reply.
If they don’t get an answer within X seconds, they’ll just send out the task again.
This isn’t our concern; the client API does it already.
Workers process one task at a time; they are very simple beasts.
If they crash, they get restarted by whatever script started them.
So we double-check to make sure that we understood this correctly:
So we do a little calculation and see that this will work nicely over plain TCP.
It’s a straightforward problem that requires no exotic hardware or protocols, just some clever routing algorithms and careful design.
We start by designing one cluster (one data center) and then we figure out how to connect clusters together.
Architecture of a Single Cluster Workers and clients are synchronous.
We want to use the load-balancing pattern to route tasks to workers.
Workers are all identical; our facility has no notion of different services.
We make no attempt here to provide guaranteed delivery, retries, and so on.
For reasons we already examined, clients and workers won’t speak to each other directly.
This makes it impossible to add or remove nodes dynamically.
So, our basic model consists of the request-reply message broker we saw earlier (Figure 3-14)
Scaling to Multiple Clusters Now we scale this out to more than one cluster.
Each cluster has a set of clients and workers, and a broker that joins these together, as illustrated in Figure 3-15
The question is, how do we get the clients of each cluster talking to the workers of the other cluster? There are a few possibilities, each with pros and cons:
The advantage here is that we don’t need to modify the brokers or workers.
However, the clients get more complex and become aware of the overall topology.
In effect, we have to move routing and failover logic into the clients, and that’s not nice.
But REQ workers can’t do that; they can only reply to one broker.
We might use REPs, but REPs don’t give us customizable broker-to-worker routing like load balancing does, only the built-in load balancing.
That’s a fail; if we want to distribute work to idle workers, we precisely need load balancing.
One solution would be to use ROUTER sockets for the worker nodes.
This looks the neatest because it creates the fewest additional connections.
We can’t add clusters on the fly, but that is probably out of our scope anyway.
With this solution, clients and workers remain ignorant of the real network topology, and brokers tell each other when they have spare capacity.
In this model, we have workers connecting to both brokers and accepting jobs from either one (Figure 3-16)
However, it doesn’t provide what we wanted, which was that clients get local workers if possible and remote workers only if it’s better than waiting.
Also, workers will signal “ready” to both brokers and so can get two jobs at once, while other workers remain idle.
It seems this design fails because again we’re putting routing logic at the edges.
We interconnect the brokers and don’t touch the clients or workers, which are REQs like we’re used to (Figure 3-17)
This design is appealing because the problem is solved in one place, invisible to the rest of the world.
In effect, it is just a more sophisticated routing algorithm: brokers become subcontractors for each other.
There are other things to like about this design, even before we play with real code:
It treats the common case (clients and workers on the same cluster) as the default and does extra work for the exceptional case (shuffling jobs between clusters)
It lets us use different message flows for the different types of work.
That means we can handle them differently, for example using different types of network connection.
If we find this to be a problem, it’s easy to solve by adding a super-broker.
The REQ client (REQ) threads create workloads and pass them to the broker (ROUTER)
The REQ worker (REQ) threads process workloads and return the results to the broker (ROUTER)
The broker queues and distributes workloads using the load-balancing pattern.
Federation Versus Peering There are several possible ways to interconnect brokers.
What we want is to be able to tell other brokers, “We have capacity,” and then receive multiple tasks.
We also need to be able to tell other brokers, “Stop, we’re full.” It doesn’t need to be perfect; sometimes we may accept jobs we can’t process immediately, but we’ll do them as soon as possible.
The simplest interconnect is federation, in which brokers simulate clients and workers for each other.
We would do this by connecting our frontend to the other broker’s backend socket (Figure 3-18)
Note that it is legal to both bind a socket to an endpoint and connect it to other endpoints.
This would give us simple logic in both brokers and a reasonably good mechanism: when there are no clients, tell the other broker “ready,” and accept one job from it.
The problem is that it is too simple for this problem.
A federated broker would be able to handle only one task at a time.
If the broker emulates a lock-step client and worker, it will by definition also be lock-step, and if it has lots of available workers, they won’t be used.
Our brokers need to be connected in a fully asynchronous fashion.
The federation model is perfect for other kinds of routing, especially service-oriented architectures (SOAs), which route by service name and proximity rather than load balancing or round robin.
So don’t dismiss it as useless; it’s just not right for all use cases.
Instead of federation, let’s look at a peering approach in which brokers are explicitly aware of each other and talk over privileged channels.
Let’s break this down, assuming we want to interconnect N brokers.
Each broker has (N – 1) peers, and all brokers are using exactly the same code and logic.
Each broker needs to tell its peers how many workers it has available at any time.
This can be fairly simple information—just a quantity that is updated regularly.
The obvious (and correct) socket pattern for this is publish-subscribe: every broker opens a PUB socket and publishes state information on that, and every broker also opens a SUB socket and connects that to the PUB socket of every other broker, to get state information from its peers.
Each broker needs a way to delegate tasks to a peer and get replies back, asynchronously.
We’ll do this using ROUTER sockets; no other combination works.
Each broker has two such sockets: one for tasks it receives and one for tasks it delegates.
If we didn’t use two sockets, it would be more work to know whether we were reading a request or a reply each time.
That would mean adding more information to the message envelope.
And there is also the flow of information between a broker and its local clients and workers.
The Naming Ceremony Three flows * two sockets for each flow = six sockets that we have to manage in the broker.
Choosing good names is vital to keeping a multisocket juggling act reasonably coherent in our minds.
Sockets do something, and what they do should form the basis for their names.
It’s about being able to read the code several weeks later on a cold Monday morning before coffee, and not feel any pain.
A local request-reply flow between the broker and its clients and workers.
A cloud request-reply flow between the broker and its peer brokers.
A state flow between the broker and its peer brokers.
Finding meaningful names that are all the same length means our code will align nicely.
It’s not a big thing, but attention to detail helps.
For each flow, the broker has two sockets that we can orthogonally call the frontend and backend.
The conceptual flow is from front to back (with replies going in the opposite direction, from back to front)
So, in all the code we write for this tutorial, we will use these socket names:
For our transport, because we’re simulating the whole thing on one box, we’ll use ipc for everything.
This has the advantage of working like tcp in terms of connectivity (i.e., it’s a disconnected transport, unlike inproc), yet we don’t need IP addresses or DNS names, which would be a pain here.
Instead, we will use ipc endpoints called somethinglocal, something-cloud, and something-state, where something is the name of our simulated cluster.
You might be thinking that this is a lot of work for some names.
It’s easier to remember “three flows, two directions” than “six different sockets” (Figure 3-19)
Note that we connect the cloudbe in each broker to the cloudfe in every other broker, and likewise we connect the statebe in each broker to the statefe in every other broker.
Prototyping the State Flow Because each socket flow has its own little traps for the unwary, we will test them in real code one by one, rather than trying to throw the whole lot into code in one go.
The main loop (Example 3-20) sends out status messages to peers and collects status messages back from peers.
Each broker has an identity that we use to construct ipc endpoint names.
A real broker would need to work with TCP and a more sophisticated configuration scheme.
We’ll look at such schemes later in this book, but for now, using generated ipc names lets us ignore the problem of where to get TCP/IP addresses or names.
We use a zmq_poll() loop as the core of the program.
We send a state message only if we did not get any incoming messages and we waited for a second.
If we send out a state message each time we get one in, we’ll get message storms.
We use a two-part pub-sub message consisting of sender address and data.
Note that we will need to know the address of the publisher in order to send it tasks, and the only way to send this explicitly is as a part of the message.
We don’t set identities on subscribers, because if we did we’d get outdated state information when connecting to running brokers.
We can build this little program and run it three times to simulate three clusters.
You’ll see each cluster report the state of its peers, and after a few seconds they will all happily be printing random numbers once per second.
Try this and satisfy yourself that the three brokers all match up and synchronize to per-second state updates.
In real life, we would not send out state messages at regular intervals, but rather whenever we had a state change—i.e., whenever a worker became available or unavailable.
That may seem like a lot of traffic, but state messages are small and we’ve established that the inter-cluster connections are superfast.
If we wanted to send state messages at precise intervals, we’d create a child thread and open the statebe socket in that thread.
We’d then send irregular state updates to that child thread from our main thread and allow the child thread to conflate them into regular outgoing messages.
Prototyping the Local and Cloud Flows Let’s now prototype the flow of tasks via the local and cloud sockets (Figure 3-21)
This code pulls requests from clients and then distributes them to local workers and cloud peers on a random basis.
Before we jump into the code, which is getting a little complex, let’s sketch the core routing logic and break it down into a simple but robust design.
This was the technique we used in the load-balancing broker earlier in this chapter, and it worked nicely.
We only read from the two frontends when there is somewhere to send the requests.
We can always read from the backends, as they give us replies to route back.
As long as the backends aren’t talking to us, there’s no point in even looking at the frontends.
When we get a message, it may be “ready” from a worker or it may be a reply.
If it’s a reply, we route it back via the local or cloud frontend.
If a worker has replied, it has become available, so we queue it and count it.
While there are workers available, we take a request (if there are any) from either frontend and route it either to a local worker or randomly to a cloud peer.
Randomly sending tasks to a peer broker rather than a worker simulates work distribution across the cluster.
Our own name; in practice this would be configured per node static char *self;
The client task, shown in Example 3-22, implements a request-reply dialog using a standard synchronous REQ socket.
The worker task, shown in Example 3-23, plugs into the load balancer using a REQ socket.
The main task begins by setting up its frontend and backend sockets and then starting its client and worker tasks (Example 3-24)
We’re using load balancing to poll workers at all times, and clients only when there are one or more workers available.
Now we route as many client requests as we have worker capacity for, as illustrated in Example 3-26
We may reroute requests from our local frontend, but not from the cloud frontend.
We’ll reroute randomly for now, just to test things out.
In the next version, we’ll do this properly by calculating cloud capacity.
Because we’re not getting any state information from peers, we naively assume they are running.
The code prompts us to confirm when we’ve started all the brokers.
In the real case, we wouldn’t send anything to brokers who had not told us they exist.
You can satisfy yourself that the code works by watching it run forever.
If there were any misrouted messages, clients would end up blocking, and the brokers would stop printing trace information.
You can prove that by killing either of the brokers.
The other broker tries to send requests to the cloud, and one by one its clients block, waiting for an answer.
As before, we’ll run an entire cluster as one process.
We’re going to take the two previous examples and merge them into one properly working design that lets us simulate any number of clusters.
This code is the size of both previous prototypes together, at 270 lines of code.
That’s pretty good for a simulation of a cluster that includes clients and workers and cloud workload distribution.
The code is presented in the following series of examples, beginning with Example 3-27
Our own name; in practice this would be configured per node static char *self;
It issues a burst of requests and then sleeps for a few seconds.
This simulates sporadic activity; when a number of clients are active at once, the local workers should be overloaded.
The client uses a REQ socket for requests and also pushes statistics to the monitor socket.
The worker task, which uses a REQ socket to plug into the load balancer, is shown in Example 3-29
It’s the same stub worker task that you’ve seen in other examples.
The main task begins by setting up all its sockets (Example 3-30)
The local frontend talks to clients, and our local backend talks to workers.
The cloud frontend talks to peer brokers as if they were clients, and the cloud backend talks to peer brokers as if they were workers.
The state backend publishes regular state messages, and the state frontend subscribes to all state backends to collect these messages.
Finally, we use a PULL monitor socket to collect printable messages from tasks.
After binding and connecting all our sockets, we start our child tasks—workers and clients—as shown in Example 3-31
Track if capacity changes during this iteration int previous = local_capacity;
If we have input messages on our statefe or monitor sockets, we can process these immediately, as shown in Example 3-33
Now we route as many clients requests as we can handle, as illustrated in Example 3-34
If we have local capacity, we poll both localfe and cloudfe.
If we have cloud capacity only, we poll just localfe.
We route any request locally if we can, or else we route it to the cloud.
We broadcast capacity messages to other peers, as shown in Example 3-35; to reduce chatter, we do this only if our capacity has changed.
It’s a nontrivial program and took about a day to get working.
They do this by polling for a response and, if none arrives after a while (10 seconds), printing an error message.
Clients simulate varying loads to get the cluster to 100% at random moments, so that tasks are shifted over to the cloud.
Feel free to play with them to see if you can make a more realistic simulation.
It could in fact use three: information, backends, and frontends.
As in the earlier prototype, there is no point in taking a frontend message if there is no backend capacity.
These are some of the problems that arose during development of this program:
Clients would freeze, due to requests or replies getting lost somewhere.
Recall that the ROUTER socket drops messages it can’t route.
The first tactic here was to modify the client thread to detect and report such problems.
Secondly, I put zmsg_dump() calls after every receive and before every send in the main loop, until the origins of the problems were clear.
The main loop was mistakenly reading from more than one ready socket.
I fixed that by reading only from the first ready socket.
The zmsg class was not properly encoding UUIDs as C strings.
This caused UUIDs that contain 0 bytes to be corrupted.
I fixed this by modifying zmsg to encode UUIDs as printable hex strings.
This simulation does not detect the disappearance of a cloud peer.
If you start several peers and stop one, and that peer was broadcasting capacity to the others, they will continue to send it work even after it’s gone.
You can try this, and you will get clients that complain of lost requests.
First, only keep the capacity information for a short time, so that if a peer does disappear its capacity is quickly set to zero.
The Lazy Pirate pattern: reliable request-reply from the client side.
It can crash and exit, freeze and stop responding to input, run too slowly for its input, exhaust all memory, and so on.
Message queues can overflow, typically in system code that has learned to deal brutally with slow clients.
When a queue overflows, it starts to discard messages, so we get “lost” messages.
Hardware can fail and take with it all the processes running on that box.
Making a software system fully reliable against all of these possible failures is an enormously difficult and expensive job and goes beyond the scope of this modest tome.
If you’re a large company with money to spend on the last two cases, contact my company immediately! There’s a large hole behind my beach house waiting to be converted into an executive swimming pool.
If the server dies while processing a request, the client can figure that out because it won’t get an answer back.
Then it can give up in a huff, wait and try again later, find another server, etc.
As for the client dying, we can brush that off as “someone else’s problem” for now.
Publish-subscribe If the client dies (having gotten some data), the server won’t know about it.
Pubsub doesn’t send any information back from the client to the server.
Subscribers can also self-verify that they’re not running too slowly, and take action (e.g., warn the operator and die) if they are.
Pipeline If a worker dies (while working), the ventilator doesn’t know about it.
Pipelines, like pub-sub and the grinding gears of time, only work in one direction.
But the downstream collector can detect that one task didn’t get done, and send a message back to the ventilator saying, “Hey, resend task 324!” If the ventilator or collector dies, whatever upstream client originally sent the work batch can get tired of waiting and resend the whole lot.
It’s not elegant, but system code should really not die often enough for this to matter.
In this chapter we’ll focus just on request-reply, which is the low-hanging fruit of reliable messaging.
The basic request-reply pattern (a REQ client socket doing a blocking send/receive to a REP server socket) scores low on handling the most common types of failure.
If the server crashes while processing the request, the client just hangs forever.
Similarly, if the network loses the request or the reply, the client hangs forever.
However, with a little extra work, this humble pattern becomes a good basis for real work across a distributed network, and we get a set of reliable request-reply (RRR) patterns that I like to call the Pirate patterns (you’ll eventually get the joke, I hope)
There are, in my experience, roughly three ways to connect clients to servers.
Use case: a single well-known server to which clients need to talk.
Types of failure we aim to handle: server crashes and restarts, and network disconnects.
Multiple clients talking to a broker proxy that distributes work to multiple workers.
Types of failure we aim to handle: worker crashes and restarts, worker busy looping, worker overload, queue crashes and restarts, and network disconnects.
Multiple clients talking to multiple servers with no intermediary proxies.
Types of failure we aim to handle: service crashes and restarts, service busy looping, service overload, and network disconnects.
Each of these approaches has its trade-offs, and often you’ll mix them.
Client-Side Reliability (Lazy Pirate Pattern) We can get very simple, reliable request-reply with some changes to the client.
Poll the REQ socket and receive from it only when it’s sure a reply has arrived.
Resend a request, if no reply has arrived within a timeout period.
Abandon the transaction if there is still no reply after several requests.
If we try to use a REQ socket in anything other than a strict send/receive fashion, we’ll get an error (technically, the REQ socket implements a small finite-state machine to enforce the send/receive ping-pong, so the error code is called “EFSM”)
This is slightly annoying when we want to use REQ in a Pirate pattern, because we may send several requests before getting a reply, as you can see in Example 4-1
The pretty good bruteforce solution is to close and reopen the REQ socket after an error.
Lazy Pirate client (lpclient.c) // //  Lazy Pirate client //  Use zmq_poll to do a safe request-reply.
To run, start lpserver and then randomly kill/restart it // #include "czmq.h"
Example 4-2 shows how we process a server reply and exit our loop if the reply is valid.
If we didn’t receive a reply, we close the client socket and resend the request.
We run this together with the matching server, shown in Example 4-3
Lazy Pirate server (lpserver.c) // //  Lazy Pirate server //  Binds REQ socket to tcp://*:5555 //  Like hwserver except: //   - echoes request as-is //   - randomly runs slowly, or exits to simulate a crash.
To run this test case, start the client and the server in two console windows.
And here is the client’s response: I: connecting to server...
The client sequences each message and checks that replies come back exactly in order: that no requests or replies are lost, and no replies come back more than once or out of order.
Run the test a few times until you’re convinced that this mechanism actually works.
You don’t need sequence numbers in a production application; they just help us trust our design.
The client uses a REQ socket, and it does the brute-force close/reopen because REQ sockets impose that strict send/receive cycle.
You might be tempted to use a DEALER instead, but it would not be a good decision.
First, it would mean emulating the secret sauce that REQ does with envelopes (if you’ve forgotten what that is, it’s a good sign you don’t want to have to do it)
Second, it would mean potentially getting back replies that you didn’t expect.
Handling failures only at the client works when we have a set of clients talking to a single server.
This design can handle a server crash, but only if recovery means restarting that same server.
If there’s a permanent error, such as a dead power supply on the server hardware, this approach won’t work.
Because the application code in servers is usually the biggest source of failures in any architecture, depending on a single server is not a great idea.
Basic Reliable Queuing (Simple Pirate Pattern) Our second approach extends the Lazy Pirate pattern with a queue proxy that lets us talk, transparently, to multiple servers, which we can more accurately call “workers.” We’ll develop this in stages, starting with a minimal working model, the Simple Pirate pattern.
If the application requires some shared state, such as a shared database, we don’t know about it as we design our messaging framework.
Having a queue proxy means workers can come and go without clients knowing anything about it.
This is a nice, simple topology with only one real weakness: the central queue itself, which can become a problem to manage and is a single point of failure.
We don’t need a special client; we’re still using the Lazy Pirate client.
Example 4-4 presents is the queue, which is identical to the main task of the load-balancing broker.
Simple Pirate queue (spqueue.c) // //  Simple Pirate broker //  This is identical to load-balancing pattern, with no reliability //  mechanisms.
The body of this example is exactly the same as lbbroker2
Example 4-5 shows the worker, which takes the Lazy Pirate server and adapts it for the load-balancing pattern (using the REQ “ready” signaling)
To test this, start a handful of workers, a Lazy Pirate client, and the queue, in any order.
You’ll see that the workers eventually all crash and burn, and the client retries and then gives up.
The queue never stops, and you can restart workers and clients ad nauseum.
This model works with any number of clients and workers.
Robust Reliable Queuing (Paranoid Pirate Pattern) The Simple Pirate queue pattern works pretty well, especially because it’s just a combination of two existing patterns.
The queue does not detect worker failure, so if a worker dies while idle, the queue can’t remove it from its worker queue until the queue sends it a request.
To make this work properly, we need to do heartbeating from worker to queue, so that the queue can detect a lost worker at any stage.
We’ll fix these issues in a properly pedantic Paranoid Pirate pattern.
For the Paranoid Pirate worker, we’ll switch to a DEALER socket (Figure 4-3)
This has the advantage of letting us send and receive messages at any time, rather than the lock-step send/receive that REQ imposes.
The downside of DEALER is that we have to do our own envelope management (reread Chapter 3 for background on this concept)
The Paranoid Pirate queue proxy is shown in Example 4-6
Paranoid Pirate queue (ppqueue.c) // //  Paranoid Pirate queue // #include "czmq.h"
Example 4-7 defines the worker class: a structure and a set of functions that act as constructor, destructor, and methods on worker objects.
The ready method (Example 4-8) puts a worker at the end of the ready list.
The next method, shown in Example 4-9, returns the next available worker’s identity.
The purge method (Example 4-10) looks for and kills expired workers.
We hold workers from oldest to most recent, so we stop at the first alive worker.
The main task is a load balancer with heartbeating on workers so we can detect crashed or blocked worker tasks, as shown in Example 4-11
As shown in Example 4-12, first, we send heartbeats to any idle workers if it’s time, then we purge any dead workers.
The queue extends the load-balancing pattern with heartbeating of workers.
Heartbeating is one of those “simple” things that can be difficult to get right.
I’ll explain more about that in the next section; for now, back to the code.
Take a look at the Paranoid Pirate worker in Example 4-13
Paranoid Pirate worker (ppworker.c) // //  Paranoid Pirate worker // #include "czmq.h"
Helper function that returns a new configured socket //  connected to the Paranoid Pirate queue.
We have a single task that implements the worker side of the Paranoid Pirate Protocol (PPP)
The heartbeating code in Example 4-14 lets the worker detect if the queue has died, and vice versa.
To test the robustness of the queue implementation, we simulate various typical problems, such as the worker crashing or running very slowly.
We do this after a few cycles so that the architecture can get up and running first.
When we get a heartbeat message from the queue, it means the queue is (or rather, was recently) alive, so we must reset our liveness indicator.
If the queue hasn’t sent us heartbeats in a while, we destroy the socket and reconnect, as shown in Example 4-17
This is the simplest and most brutal way of discarding any messages we might have sent in the meantime.
This makes it (a) very hard to debug, and (b) dangerous to reuse.
When you want to debug this code, disable the failure simulation.
The worker uses a reconnect strategy similar to the one we designed for the Lazy Pirate client, with two major differences: it does an exponential backoff, and it retries indefinitely (whereas the client retries a few times before reporting a failure)
You should see the workers die one by one as they simulate a crash, and the client eventually give up.
You can stop and restart the queue, and both the client and the workers will reconnect and carry on.
And no matter what you do to queues and workers, the client will never get an out-of-order reply: either the whole chain works, or the client abandons.
When writing the Paranoid Pirate examples, it took me about five hours to get the heartbeating working properly.
The rest of the request-reply chain took perhaps 10 minutes.
It is especially easy to create “false failures”; i.e., when peers decide that they are disconnected because the heartbeats aren’t sent properly.
When we use a ROUTER socket in an application that tracks peers, as peers disconnect and reconnect, the application will leak memory (resources that the application holds for each peer) and get slower and slower.
When we use SUB- or DEALER-based data recipients, we can’t tell the difference between good silence (there’s no data) and bad silence (the other end has died)
When a recipient knows the other side has died, it can for example switch over to a backup route.
If we use a TCP connection that stays silent for a long while, it will, in some networks, just die.
Sending something (technically, a “keep alive” more than a heartbeat) will keep the network alive.
One-Way Heartbeats A second option is to send a heartbeat message from each node to its peers every second or so.
When one node hears nothing from another within some timeout (several seconds, typically), it will treat that peer as dead.
This works in some cases but has nasty edge cases in others.
For pub-sub, this approach does work, and it’s the only model you can use.
As an optimization, you can send heartbeats only when there is no real data to send.
Furthermore, you can send heartbeats at progressively longer intervals, if network activity is an issue (e.g., on mobile networks where activity drains the battery)
As long as the recipient can detect a failure (a sharp stop in activity), that’s fine.
It can be inaccurate when we send large amounts of data, as heartbeats will be delayed behind that data.
If heartbeats are delayed, you can get false timeouts and disconnections due to network congestion.
Thus, always treat any incoming data as a heartbeat, whether or not the sender optimizes out heartbeats.
While the pub-sub pattern will drop messages for disappeared recipients, PUSH and DEALER sockets will queue them.
So if you’ve send heartbeats to a dead peer and it comes back to life, it will get all the heartbeats you’ve sent, which can be thousands.
This design assumes that heartbeat timeouts are the same across the whole network.
Some peers will want very aggressive heartbeating in order to detect faults rapidly.
And some will want very relaxed heartbeating in order to let sleeping networks lie and save power.
Ping-Pong Heartbeats The third option is to use a ping-pong dialog.
One peer sends a ping command to the other, which replies with a pong command.
Because the roles of “client” and “server” are arbitrary in some networks, we usually specify that either peer can in fact send a ping and expect a pong in response.
However, as the timeouts depend on network topologies known best to dynamic clients, it is usually the client that pings the server.
The same optimizations we used in the second model make this work even better: treat any incoming data as a pong, and only send a ping when not otherwise sending data.
It might not have been the simplest option: if designing this today, I’d probably try a ping-pong approach instead.
The heartbeat messages flow asynchronously in both directions, and either peer can decide the other is “dead” and stop talking to it.
In the worker, this is how we handle heartbeats from the queue:
We calculate a liveness, which is how many heartbeats we can still miss before deciding the queue is dead.
It starts at three and we decrement it each time we miss a heartbeat.
We wait in the zmq_poll() loop for one second each time, which is our heartbeat interval.
If there’s any message from the queue during that time, we reset our liveness to three.
If there’s no message during that time, we count down our liveness.
If the liveness reaches zero, we consider the queue dead.
If the queue is dead, we destroy our socket, create a new one, and reconnect.
To avoid opening and closing too many sockets, we wait for a certain interval before reconnecting, and we double the interval each time until it reaches 32 seconds.
And this is how we handle heartbeats to the queue:
We calculate when to send the next heartbeat; this is a single variable because we’re talking to one peer, the queue.
In the zmq_poll() loop, whenever we pass this time, we send a heartbeat to the queue.
The queue does the same, but manages an expiration time for each worker.
Use zmq_poll() or a reactor as the core of your application’s main task.
Start by building the heartbeating between peers, test it by simulating failures, and.
Use simple tracing (i.e., print to console) to get this working.
To help you trace the flow of messages between peers, use a dump method such as the one zmsg offers, and number your messages incrementally so you can see if there are gaps.
In a real application, heartbeating must be configurable and usually negotiated with the peer.
Some peers will want aggressive heartbeating, as low as 10 msec.
Other peers will be far away and want heartbeating as high as 30 seconds.
If you have different heartbeat intervals for different peers, your poll timeout should be the lowest (shortest time) of these.
Do heartbeating on the same socket you use for messages, so your heartbeats also act as a keep alive to stop the network connection from going stale (some firewalls can be unkind to silent connections)
Contracts and Protocols If you’re paying attention, you’ll realize that Paranoid Pirate is not interoperable with Simple Pirate, because of the heartbeats.
But how do we define “interoperable”? To guarantee interoperability, we need a kind of contract, an agreement that lets different teams in different times and places write code that is guaranteed to work together.
It’s fun to experiment without specifications, but that’s not a sensible basis for real applications.
What happens if we want to write a worker in another language? Do we have to read code to see how things work? What if we want to change the protocol for some reason? Even a simple protocol will, if it’s successful, evolve and become more complex.
Lack of contracts is a sure sign of a disposable application.
To create a new specification, register and follow the instructions.
It’s fairly straightforward, though writing technical texts is not everyone’s cup of tea.
It took me about 15 minutes to draft the new Pirate Pattern Protocol.
It’s not a big specification, but it does capture enough to act as the basis for arguments (“Your queue isn’t PPP compatible; please fix it!”)
Turning PPP into a real protocol would take more work:
There should be a protocol version number in the READY command so that it’s possible to distinguish between different versions of PPP.
Right now, READY and HEARTBEAT are not entirely distinct from requests and replies.
To make them distinct, we would need a message structure that includes a “message type” part.
Service-Oriented Reliable Queuing (Majordomo Pattern) The nice thing about progress is how fast it happens when lawyers and committees aren’t involved.
Just a few sentences ago, we were dreaming of a better protocol that would fix the world.
This one-page specification turns PPP into something more solid (Figure 4-4)
This is how we should design complex architectures: start by writing down the contracts, and only then write software to implement them.
The Majordomo Protocol (MDP) extends and improves on PPP in one interesting way: it adds a “service name” to requests that the client sends, and asks workers to register for specific services.
Adding service names turns our Paranoid Pirate queue into a service-oriented broker.
The nice thing about MDP is that it came out of working code, a simpler ancestor protocol (PPP), and a precise set of improvements.
To implement Majordomo, we need to write a framework for clients and workers.
It’s really not sane to ask every application developer to read the spec and make it work, when they could be using a simpler API built and tested just once.
So while our first contract (MDP itself) defines how the pieces of our distributed architecture talk to each other, our second contract defines how user applications talk to the technical framework we’re going to design.
Majordomo has two halves, a client side and a worker side.
Since we’ll write both client and worker applications, we will need two APIs.
Here is a sketch for the client API, using a simple object-oriented approach:
We open a session to the broker, send a request message, get a reply message back, and eventually close the connection.
It’s more or less symmetrical, but the worker dialog is a little different.
The first time a worker does a recv(), it passes a null reply.
Thereafter, it passes the current reply and gets a new request.
The client and worker APIs were fairly simple to construct because they’re heavily based on the Paranoid Pirate code we already developed.
Structure of our class //  We access these properties only via class methods.
Example 4-19 presents the constructor and destructor for our mdcli class.
We can set the request timeout and number of retry attempts before sending requests, as shown in Example 4-20
It sends a request to the broker and gets a reply even if it has to retry several times.
It takes ownership of the request message, and destroys it when sent.
It returns the reply message, or NULL if there was no reply after multiple attempts.
On any blocking call, libzmq will return -1 if there was an error.
We could in theory check for different error codes, but in practice it’s okay to assume it was EINTR (Ctrl-C)
The body of our send method is shown in Example 4-22
Majordomo client application (mdclient.c) // //  Majordomo Protocol client example //  Uses the mdcli API to hide all MDP aspects //
Lets us build this source without creating a library #include "mdcliapi.c"
Example 4-25 shows is the structure of a worker API instance.
We use a pseudo objectoriented approach in a lot of the C examples, as well as the CZMQ binding.
Majordomo worker API (mdwrkapi.c): worker class structure //  Structure of our class //  We access these properties only via class methods.
We have two utility functions, to send a message to the broker and to (re)connect to the broker, as you can see in Example 4-26
Example 4-27 presents the constructor and destructor for our mdwrk class.
You can set the heartbeat interval and retries to match the expected network performance (Example 4-28)
Example 4-29 shows the recv method; it’s a little misnamed since it first sends any reply and then waits for a new request.
If you have a better name for this, let me know!
Finally, here is where we actually have a message to process; as shown in Example 4-30, we return it to the caller application.
Let’s see how the worker API looks in action with an example test program (Example 4-31) that implements an echo service.
Majordomo worker application (mdworker.c) // //  Majordomo Protocol worker example //  Uses the mdwrk API to hide all MDP aspects //
Lets us build this source without creating a library #include "mdwrkapi.c"
Here are some things to note about the worker API code:
This means, for example, that the worker won’t send heartbeats in the background.
Happily, this is exactly what we want: if the worker application gets stuck, heartbeats will stop and the broker will stop sending requests to the worker.
The worker API doesn’t do an exponential backoff; it’s not worth the extra complexity.
If something isn’t as expected, they raise an assertion (or exception, depending on the language)
This is ideal for a reference implementation, so any protocol errors show immediately.
For real applications, the API should be robust against invalid messages.
Its core structure is a set of queues, one per service.
We will create these queues as workers appear (we could delete them as workers disappear, but forget that for now because it gets complex)
Additionally, we will keep a queue of workers per service.
The code for the broker is shown in Example 4-32
The broker class (Example 4-33) defines a single broker instance.
The service class (Example 4-34) defines a single service instance.
The worker class (Example 4-35) defines a single worker, idle or active.
The constructor and destructor for the broker are shown in Example 4-36
The bind method, shown in Example 4-37, binds the broker instance to an endpoint.
Note that MDP uses a single socket for both clients and workers.
Example 4-39 shows how we process a request coming from a client.
We implement Majordomo Management Interface (MMI) requests directly here (at present, only the mmi.service request)
The purge method, shown in Example 4-40, deletes any idle workers that haven’t pinged us in a while.
We hold workers in order from oldest to most recent, so we can stop scanning whenever we find a live worker.
This means we’ll mainly stop at the first worker, which is essential when we have large numbers of workers (because we call this method in our critical path)
Example 4-41 shows the implementation of the methods that work on a service.
Majordomo broker (mdbroker.c): service methods //  Lazy constructor that locates a service by name, or creates a new //  service if there is no service already with that name.
Service destructor is called automatically whenever the service is //  removed from broker->services.
The dispatch method, shown in Example 4-42, sends requests to waiting workers.
Example 4-43 shows the implementation of the methods that work on a worker.
Majordomo broker (mdbroker.c): worker methods //  Lazy constructor that locates a worker by identity, or creates a new //  worker if there is no worker already with that identity.
Worker destructor is called automatically whenever the worker is //  removed from broker->workers.
The send method (Example 4-44) formats and sends a command to a worker.
The caller may also provide a command option and a message payload.
In Example 4-45, we create a new broker instance and then process messages on the broker socket.
This is by far the most complex example we’ve seen.
It’s almost 500 lines of code; writing this and making it somewhat robust took two days.
However, this is still a relatively short piece of code for a full service-oriented broker.
The broker implements all of MDP/0.1 properly (as far as I know), including disconnection if the broker sends invalid commands, heartbeating, and the rest.
It can be extended to run multiple threads, each managing one socket and one set of clients and workers.
The C code is already organized around a broker class to make this trivial.
A primary/failover or live/live broker reliability model is easy, as the broker essentially has no state except service presence.
It’s up to clients and workers to choose another broker if their first choice isn’t up and running.
The examples use five-second heartbeats, mainly to reduce the amount of output when you enable tracing.
However, any retry has to be slow enough to allow for a service to restart, say 10 seconds at least.
We later improved and extended the protocol and the Majordomo implementation, which now sits in its own GitHub project.
If you want a properly usable Majordomo stack, use the GitHub project.
Theory is great in theory, but in practice, practice is better.
Let’s measure the actual cost of round-tripping with a simple test program.
This sends a bunch of messages, first waiting for a reply to each message, and second as a batch, reading all the replies back as a batch.
Both approaches do the same work, but they give very different results.
Round-trip demonstrator (tripping.c) // //  Round-trip demonstrator // //  While this example runs in a single process, that is just to make //  it easier to start and stop the example.
The client task signals to //  main when it's ready.
All it does is receive a message, and bounce it back the way it came.
It uses the zmq_proxy() function to switch messages between the frontend and backend.
Finally, Example 4-49 presents the main task, which starts the client, worker, and broker, and then runs until the client signals it to stop.
On my development box, running this program results in: Setting up test...
Note that the client thread does a small pause before starting.
This is to get around one of the “features” of the router socket: if you send a message with the address of a peer that’s not yet connected, the message gets discarded.
In this example we don’t use the load-balancing mechanism, so without the sleep, if the worker thread is too slow to connect it will lose messages, making a mess of our test.
As we see, round-tripping in the simplest case is 20 times slower than the asynchronous, “shove it down the pipe as fast as it’ll go” approach.
Let’s see if we can apply this to Majordomo to make it faster.
It’s literally a few minutes’ work to refactor the synchronous client API to become asynchronous, as shown in Example 4-50
Structure of our class //  We access these properties only via class methods.
In this asynchronous class, we use a //  DEALER socket instead of a REQ socket; this lets us send any number //  of requests without waiting for a reply.
The constructor and destructor are the same as in mdcliapi, except //  we don't do retries, so there's no retries property.
We use a DEALER socket instead of REQ, so we emulate REQ with an empty delimiter frame before each request and each response.
We don’t retry requests; if the application needs to retry, it can do this itself.
We break the synchronous send method into separate send and recv methods.
The send method is asynchronous and returns immediately after sending.
The recv method waits for (with a timeout) one response and returns that to the caller.
The broker and worker are unchanged because we haven’t modified the protocol at all.
It isn’t fully asynchronous because workers get their messages on a strict last-used basis, but it will scale better with more workers.
On my PC, after eight or so workers, it doesn’t get any faster.
But we got a 4x improvement in throughput with just a few minutes’ work.
It spends most of its time copying message frames around, instead of doing zero-copy, which it could.
But we’re getting 25K reliable request-reply calls a second, with pretty low effort.
It has a fundamental weakness, namely that it cannot survive a broker crash without more work.
If you look at the mdcliapi2 code you’ll see it does not attempt to reconnect after a failure.
A number on every request and a matching number on every reply, which would ideally require a change to the protocol to enforce.
Tracking and holding onto all outstanding requests in the client API (i.e., those for which no reply has yet been received)
In case of fail over, for the client API to resend all outstanding requests to the broker.
It’s not a deal breaker, but it does show that performance often means complexity.
Is this worth doing for Majordomo? It depends on your use case.
For a name lookup service you call once per session, no.
For a web frontend serving thousands of clients, probably yes.
Service Discovery So, we have a nice service-oriented broker, but we have no way of knowing whether a particular service is available or not.
We know when a request fails, but we don’t know why.
It would be useful to be able to ask the broker questions like, “Is the echo service running?” The most obvious way to implement this would be to modify our MDP/Client protocol to add commands to ask such questions.
Adding service discovery to it would make it as complex as the MDP/ Worker protocol.
Another option is to do what email servers do, and ask that undeliverable requests be returned.
This can work well in an asynchronous world, but it also adds complexity.
We need ways to distinguish returned requests from replies and to handle these properly.
Let’s try to use what we’ve already built, building on top of MDP instead of modifying it.
It might indeed be one of several management services, such as “disable service X,” “provide statistics,” and so on.
What we want is a general, extensible solution that doesn’t affect the protocol or existing applications.
There’s a small RFC that layers this on top of MDP: the Majordomo Management Interface (MMI)
We already implemented it in the broker, though unless you read the whole thing you probably missed that.
When a client requests a service that starts with mmi., instead of routing this request to a worker, we handle it internally.
We handle just one service in our broker, which is mmi.service, the service discovery service.
The payload for the request is the name of an external service (a real one, provided by a worker)
Example 4-52 shows how we use the service discovery in an application.
Service discovery over Majordomo (mmiecho.c) // //  MMI echo query example //
Lets us build this source without creating a library #include "mdcliapi.c"
The implementation of MMI in our example broker is flimsy.
For example, if a worker disappears, services remain “present.” In practice, a broker should remove services that have no workers after some configurable timeout.
Idempotent Services Idempotency is not something you take a pill for.
What it means is that it’s safe to repeat an operation.
While many client-to-server use cases are idempotent, some are not.
In such a case, it’s safe (though inefficient) to execute the same request many times.
A name service that translates logical addresses into endpoints to bind or connect to.
In such a case it’s safe to make the same lookup request many times.
One does not want the same log information recorded more than once.
Any service that has an impact on downstream nodes (e.g., sends information on to other nodes)
If that service gets the same request more than once, downstream nodes will get duplicate information.
Any service that modifies shared data in some non-idempotent way; e.g., a service that debits a bank account is definitely not idempotent.
When our server applications are not idempotent, we have to think more carefully about when exactly they might crash.
We can use database transactions to make sure a debit and a credit are always done together, if at all.
If the server dies while sending its reply, however, that’s a problem, because as far as it’s concerned, it has done its work.
If the network dies just as the reply is making its way back to the client, the same problem arises.
The client will think the server died and will resend the request, and the server will do the same work twice, which is not what we want.
To handle non-idempotent operations, we use the fairly standard solution of detecting and rejecting duplicate requests.
The client must stamp every request with a unique client identifier and a unique message number.
The server, before sending back a reply, stores it using the combination of client ID and message number as a key.
The server, when getting a request from a given client, first checks whether it has a reply for that client ID and message number.
If so, it does not process the request, but just resends the reply.
Disconnected Reliability (Titanic Pattern) Once you realize that Majordomo is a “reliable” message broker, you might be tempted to add some spinning rust (that is, ferrous-based hard disk platters)
After all, this works for all the enterprise messaging systems.
It’s such a tempting idea that it’s a little sad to have to be negative toward it.
So, some reasons you don’t want rust-based brokers sitting in the center of your architecture are:
As you’ve seen, the Lazy Pirate client performs surprisingly well.
It works across a whole range of architectures, from direct client-to-server to distributed queue proxies.
It does tend to assume that workers are stateless and idempotent, but we can work around that limitation without resorting to rust.
Rust brings a whole set of problems, from slow performance to additional pieces that you have to manage, repair, and handle 6 a.m.
The beauty of the Pirate patterns in general is their simplicity.
And if you’re still worried about the hardware, you can move to a peer-to-peer pattern that has no broker at all (I’ll explain that later in this chapter)
Having said this, however, there is one sane use case for rust-based reliability, which is an asynchronous disconnected network.
It solves a major problem with Pirate, namely that a client has to wait for an answer in real time.
So, here’s the Titanic pattern (Figure 4-5), in which we write messages to disk to ensure they never get lost, no matter how sporadically clients and workers are connected.
As we did for service discovery, we’re going to layer Titanic on top of MDP rather than extend it.
It’s wonderfully lazy because it means we can implement our fire-and-forget reliability in a specialized worker, rather than in the broker.
It is much easier because we divide and conquer: the broker handles message routing and the worker handles reliability.
It lets us mix brokers written in one language with workers written in another.
The only downside is that there’s an extra network hop between broker and hard disk.
There are many ways to make a persistent request-reply architecture.
The simplest design I could come up with, after playing with this for a few hours, was a “proxy service.” That is, Titanic doesn’t affect workers at all.
If a client wants a reply immediately, it talks directly to a service and hopes the service is available.
If a client is happy to wait a while, it talks to Titanic instead and asks, “Hey, buddy, would you take care of this for me while I go buy my groceries?”
The dialog between the client and Titanic goes along these lines:
Client: “Please accept this request for me.” Titanic: “OK, done.”
Client: “OK, you can wipe that request now, I’m happy.” Titanic: “OK, done.”
Whereas the dialog between Titanic and the broker and worker goes like this:
You can work through these and the possible failure scenarios.
If a worker crashes while processing a request, Titanic retries, indefinitely.
If the request gets processed but the client doesn’t get the reply, it will ask again.
If Titanic crashes while processing a request or a reply, the client will try again.
As long as requests are fully committed to safe storage, work can’t get lost.
The handshaking is pedantic, but can be pipelined; i.e., clients can use the asynchronous Majordomo pattern to do a lot of work and then get the responses later.
We need some way for a client to request its replies.
We’ll have many clients asking for the same services, and clients may disappear and reappear with different identities.
Every request generates a universally unique ID (UUID), which Titanic returns to the client after it has queued the request.
When a client asks for a reply, it must specify the UUID for the original request.
In a realistic case, the client would want to store its request UUIDs safely, such as in a local database.
One way is to use a single service and send it three different request types.
Another way, which seems simpler, is to use three services: titanic.request.
Stores a request message, and return a UUID for the request.
Fetches a reply, if available, for a given request UUID.
Using TSP is clearly more work for client applications than accessing a service directly via MDP.
The shortest robust “echo” client example is presented in Example 4-53
Titanic client example (ticlient.c) // //  Titanic client example //  Implements client side of http://rfc.zeromq.org/spec:9
Lets us build this source without creating a library #include "mdcliapi.c"
The main task (Example 4-54) tests our service call by sending an echo request.
Of course, this can be, and should be, wrapped up in some kind of framework or API.
It’s not healthy to ask average application developers to learn the full details of messaging: it hurts their brains, costs time, and offers too many ways to introduce buggy complexity.
For example, this client blocks on each request, whereas in a real application we’d want to be doing useful work while tasks are executed.
It requires some nontrivial plumbing to build a background thread and talk to that cleanly.
This is the kind of thing you want to wrap in a nice simple API that the average developer cannot misuse.
This server handles the three services using three threads, as proposed.
It does full persistence to disk using the most brutal approach possible: one file per message.
The only complex part is that it keeps a separate queue of all requests in order to avoid reading the directory over and over.
Titanic broker example (titanic.c) // //  Titanic service // //  Implements server side of http://rfc.zeromq.org/spec:9
Return a new UUID as a printable character string //  Caller must free returned string when finished with it.
The titanic.request task (Example 4-56) waits for requests to this service.
It writes each request to disk and returns a UUID to the client.
The client picks up the reply asynchronously using the titanic.reply service.
The titanic.close task, shown in Example 4-58, removes any waiting replies for the request (specified by UUID)
It’s idempotent, so it is safe to call it more than once in a row.
Example 4-59 shows the main thread for the Titanic worker.
It starts three child threads, for the request, reply, and close services.
It then dispatches requests to workers using a simple brute-force disk queue.
It receives request UUIDs from the titanic.request service, saves these to a disk file, and then throws each request at MDP workers until it gets a response.
In the final part of the broker code (Example 4-60), we first check if the requested MDP service is defined or not, using an MMI lookup to the Majordomo broker.
If the service exists, we send a request and wait for a reply using the conventional MDP client API.
This is not meant to be fast, just very simple.
If client already closed request, treat as successful if (!file) return 1;
To test this, start mdbroker and titanic, and then run ticlient.
Now start mdworker arbitrarily, and you should see the client getting a response and exiting happily.
Note that some loops start by sending, and others by receiving messages.
This is because Titanic acts both as a client and a worker in different roles.
The Titanic broker uses the MMI service discovery protocol to send requests only to services that appear to be running.
Since the MMI implementation in our little Majordomo broker is quite poor, this won’t work all the time.
We use an inproc connection to send new request data from the titanic.re quest service through to the main dispatcher.
This saves the dispatcher from having to scan the disk directory, load all request files, and sort them by date/time.
The important thing about this example is not its performance (which, although I haven’t tested it, is surely terrible), but how well it implements the reliability contract.
Then start the ticlient, and then start the mdworker echo service.
You can run all four of these using the -v option to do verbose activity tracing.
You can stop and restart any piece except the client, and nothing will get lost.
If you want to use Titanic in real cases, you’ll rapidly be asking, “How do we make this faster?” Here’s what I’d do, starting with the example implementation:
Use a single disk file for all data, rather than multiple files.
Operating systems are usually better at handling a few large files than many smaller ones.
Organize that disk file as a circular buffer so that new requests can be written contiguously (with very occasional wraparound)
One thread, writing full speed to a disk file, can work rapidly.
Keep the index in memory and rebuild the index at startup time, from the disk buffer.
This saves the extra disk head flutter needed to keep the index fully safe on.
You would want an fsync after every message, or every N milliseconds if you were prepared to lose the last M messages in case of a system failure.
Use a solid-state drive rather than spinning iron oxide platters.
Preallocate the entire file, or allocate it in large chunks, which allows the circular.
This avoids fragmentation and ensures that most reads and writes are contiguous.
What I’d not recommend is storing messages in a database, not even a “fast” key/value store, unless you really like a specific database and don’t have performance worries.
If you want to make Titanic even more reliable, duplicate the requests to a second server, and place it in a second location just far away enough to survive a nuclear attack on your primary location, yet not so far that you get too much latency.
If you want to make Titanic much faster but less reliable, store requests and replies purely in memory.
This will give you the functionality of a disconnected network, but requests won’t survive a crash of the Titanic server itself.
High-Availability Pair (Binary Star Pattern) The Binary Star pattern configures two servers as a primary/backup high-availability pair (Figure 4-6)
At any given time, one of these (the active server) accepts connections from client applications.
The other (the passive server) does nothing, but the two servers monitor each other.
If the active one disappears from the network, after a certain time the passive one takes over as active.
We developed the Binary Star pattern at iMatix for our OpenAMQ server.
To fail over reliably when needed, and only when needed.
Assuming we have a Binary Star pair running, here are the different scenarios that will result in a failover (Figure 4-7):
The hardware running the primary server has a fatal problem (power supply explodes, machine catches fire, or someone simply unplugs it by mistake), and disappears.
The network segment on which the primary server sits crashes—perhaps because a router gets hit by a power spike—and applications start to reconnect to the backup server.
The primary server crashes or is killed by the operator and does not restart automatically.
The operators restart the primary server and fix whatever problems were causing it to disappear from the network.
The operators stop the backup server at a moment when it will cause minimal disruption to applications.
When applications have reconnected to the primary server, the operators restart the backup server.
Recovery (to using the primary server as the active one) is a manual operation.
Painful experience has taught us that automatic recovery is undesirable.
Failover creates an interruption of service to applications, possibly lasting 10–30 seconds.
If there is a real emergency, this is much better than total outage.
But if recovery creates a further such outage, it is better that this happens off-peak, when users have gone off the network.
When there is an emergency, the absolute first priority is certainty for those trying to fix things.
Automatic recovery creates uncertainty for system administrators, who can no longer be sure which server is in charge without double-checking.
Automatic recovery can create situations where networks fail over and then recover, placing operators in the difficult position of analyzing what happened.
There was an interruption of service, but the cause isn’t clear.
Having said this, the Binary Star pattern will automatically fail back to the primary server if this is running (again) and the backup server fails.
The shutdown process for a Binary Star pair is to do one of the following:
Stop the passive server and then stop the active server at any later time.
Stop both servers in any order, but within a few seconds of each other.
Stopping the active and then the passive server with any delay longer than the failover timeout will cause applications to disconnect, then reconnect, and then disconnect again, which may disturb users.
Detailed Requirements The Binary Star pattern is as simple as it can be, while still working accurately.
In fact, the current design is the third complete redesign.
Each of the previous designs we found to be too complex, trying to do too much, and we stripped out functionality until we came to a design that was understandable, easy to use, and reliable enough to be worth using.
The failover is meant to provide insurance against catastrophic system failures, such as hardware breakdown, fire, accident, and so on.
There are simpler ways to recover from ordinary server crashes, and we already covered these.
Failover has to happen automatically, whereas recovery must happen manually.
The semantics for client applications should be simple and easy for developers to understand.
There should be clear instructions for network architects on how to avoid designs that could lead to “split-brain syndrome,” in which both servers in a Binary Star pair think they are the active server.
There should be no dependencies on the order in which the two servers are started.
It must be possible to make planned stops and restarts of either server without.
Operators must be able to monitor both servers at all times.
It must be possible to connect the two servers using a high-speed dedicated network.
That is, failover synchronization must be able to use a specific IP route.
A single backup server provides enough insurance; we don’t need multiple levels of backup.
The primary and backup servers are equally capable of carrying the application load.
We do not attempt to balance load across the servers.
There is sufficient budget to cover a fully redundant backup server that does nothing almost all the time.
The use of an active backup server or load balancing.
In a Binary Star pair, the backup server is inactive and does no useful work until the primary server goes offline.
The handling of persistent messages or transactions in any way.
We assume the existence of a network of unreliable (and probably untrusted) servers or Binary Star pairs.
The Binary Star pair is manually and explicitly defined in the network and is known to applications (at least in their configuration data)
All server-side state must be recreated by applications when they fail over.
Here is the key terminology that we use in Binary Star: Primary.
It will become active if and when the primary server disappears from the network, and when client applications ask the backup server to connect.
Passive The server that takes over if the active server disappears.
Note that when a Binary Star pair is running normally, the primary server is active, and the backup is passive.
Tell the primary server where the backup server is located.
Tell the backup server where the primary server is located.
The main tuning concern is how frequently you want the servers to check their peering status, and how quickly you want to activate failover.
In our example, the failover timeout value defaults to 2,000 msec.
If you reduce this, the backup server will take over as active more rapidly, but may take over in cases where the primary server could recover.
For example, you may have wrapped the primary server in a shell script that restarts it if it crashes.
In that case, the timeout should be higher than the time needed to restart the primary server.
For client applications to work properly with a Binary Star pair, they must:
Try to connect to the primary server and if, that fails, to the backup server.
Retransmit messages lost during a failover, if messages need to be reliable.
It’s not trivial work, and we’d usually wrap this in an API that hides it from real enduser applications.
These are the main limitations of the Binary Star pattern:
A server process cannot be part of more than one Binary Star pair.
A primary server can have a single backup server, and no more.
The passive server does no useful work, and is thus “wasted.”
The backup server must be capable of handling full application loads.
Client applications must do some work to benefit from failover.
Preventing Split-Brain Syndrome Split-brain syndrome occurs when different parts of a cluster think they are active at the same time.
Binary Star has an algorithm for detecting and eliminating split brain, which is based on a three-way decision mechanism (a server will not decide to become active until it gets application connection requests and it cannot see its peer server)
However, it is still possible to (mis)design a network to fool this algorithm.
A typical scenario would be a Binary Star pair that is distributed between two buildings, where each building also had a set of applications and where there was a single network link between both buildings.
Breaking this link would create two sets of client applications, each with half of the Binary Star pair, and each failover server would become active.
To prevent split-brain situations, we must connect a Binary Star pair using a dedicated network link, which can be as simple as plugging them both into the same switch or, better, using a crossover cable directly between two machines.
We must not split a Binary Star architecture into two islands, each with a set of applications.
While this may be a common type of network architecture, we should use federation, not high-availability failover, in such cases.
A suitably paranoid network configuration would use two private cluster interconnects, rather than a single one.
Further, the network cards used for the cluster would be different from those used for message traffic, and possibly even on different PCI paths on the server hardware.
The goal is to separate possible failures in the network from possible failures in the cluster.
Binary Star Implementation Without further ado, here is a proof-of-concept implementation of the Binary Star server, beginning with Example 4-61
The primary and backup servers run the same code, and their roles are chosen by the invoker.
Binary Star server (bstarsrv.c) // //  Binary Star server proof-of-concept implementation.
This server does no //  real work; it just demonstrates the Binary Star failover model.
States in which we can be at any point in time.
The heart of the Binary Star design is its finite-state machine (FSM)
We apply an event to the current state, which checks if the event is accepted, and if so sets a new state (Example 4-62)
The ACTIVE and PASSIVE states are laid out in Example 4-63
First we bind/connect our sockets with our peer and make sure we will get state messages correctly.
We use three sockets: one to publish state, one to subscribe to state, and one for client requests/replies.
We now process events on our two input sockets, and process these events one at a time via our finite-state machine (Example 4-65)
Our “work” for a client request is simply to echo it back.
Now let’s look at the code for the client, beginning with Example 4-66
Binary Star client (bstarcli.c) // //  Binary Star client proof-of-concept implementation.
This client does no //  real work; it just demonstrates the Binary Star failover model.
If there’s no reply within our timeout, we close the socket and try again, as seen in Example 4-67
In Binary Star, it’s the client vote that decides which server is primary; the client must therefore try to connect to each server in turn.
You can then provoke failover by killing the primary server, and recovery by restarting the primary and killing the backup.
Note how it’s the client vote that triggers failover and recovery.
Binary Star is driven by a finite-state machine (Figure 4-8)
States in white accept client requests, and states in gray refuse them.
Events are the peer state, so “Peer Active” means the other server has told us it’s active.
Note that the servers use PUB-SUB sockets for state exchange.
Binary Star Reactor Binary Star is useful and generic enough to package up as a reusable reactor class.
The reactor then runs and calls our code whenever it has a message to process.
This is much nicer than copying/pasting the Binary Star code into each server where we want that capability.
In C, we wrap the CZMQ zloop class that we saw before.
In the Binary Star reactor, we provide handlers for voters and for state changes (active to passive, and vice versa)
The finite-state machine is the same as in the proof-of-concept server.
To understand this reactor in detail, first read the CZMQ zloop class.
We have to tell it whether we’re a primary or backup server, as well as providing our local and remote endpoints to bind and connect to.
The zloop method (Example 4-71) returns the underlying zloop reactor, so we can add additional timers and readers.
The voter method, shown in Example 4-72, registers a client voter socket.
Messages received on this socket provide the CLIENT_REQUEST events for the Binary Star FSM and are passed to the provided application handler.
Next, we register handlers to be called each time there’s a state change.
It will end if any handler returns -1 to the reactor, or if the process receives a SIGINT or SIGTERM.
This gives us the short main program for the server shown in Example 4-76
Binary Star server, using core class (bstarsrv2.c) // //  Binary Star server, using bstar reactor //
Lets us build this source without creating a library #include "bstar.c"
Resolve a logical name into at least a bind endpoint and a connect endpoint.
A realistic name service would provide multiple bind endpoints, and possibly multiple connect endpoints as well.
Be reliable, because if it is unavailable, applications won’t be able to connect to the network.
Putting a name service behind a service-oriented Majordomo broker is clever from some points of view.
However, it’s simpler and much less surprising to just expose the name service as a server to which clients can connect directly.
If we do this right, the name service becomes the only global network endpoint we need to hard-code in our code or configuration files.
The types of failure we aim to handle are server crashes and restarts, server busy looping, server overload, and network issues.
To get reliability, we’ll create a pool of name servers so if one crashes or goes away, clients can connect to another, and so on.
In practice, two would be enough, but for this example we’ll assume the pool can be any size (Figure 4-9)
In this architecture, a large set of clients connect to a small set of servers directly.
It’s fundamentally different from a brokerbased approach like Majordomo, where workers connect to the broker.
Easy, but would need some additional intelligence so clients don’t stupidly try to reconnect to dead servers over and over.
Use DEALER sockets and blast out requests (which will be load-balanced to all connected servers) until they get a reply.
But how does the client know the identity of the server sockets? Either the server has to ping the client first (complex), or the server has to use a hard-coded, fixed identity known to the client (nasty)
Model One: Simple Retry and Failover So, our menu appears to offer the following choices: simple, brutal, complex, or nasty.
Let’s start with simple and then work out the kinks.
We’ll take Lazy Pirate and rewrite it to work with multiple server endpoints.
We’ll start one or several servers first, specifying a bind endpoint as the argument (Example 4-77)
Then we’ll start the client (Example 4-78), specifying one or more connect endpoints as arguments.
The client uses a Lazy Pirate strategy if it only has one server to talk to.
If it has two or more servers to talk to, it will try each server just once.
Although the basic approach is Lazy Pirate, the client aims to just get one successful reply.
It has two techniques, depending on whether we are running a single server or multiple servers:
With a single server, the client will retry several times, exactly as for Lazy Pirate.
With multiple servers, the client will try each server at most once until it’s received.
This solves the main weakness of Lazy Pirate, namely that it cannot fail over to backup or alternate servers.
However, this design won’t work well in a real application.
If we’re connecting many sockets and our primary name server is down, we’re going to experience this painful timeout each time.
Our goal here is to make sure we get a reply back within the shortest possible time, no matter whether a particular server is up or down.
When we have a request, we blast it out as many times as we have servers.
What’s more annoying for the client is that we’ll get multiple replies back, but there’s no guarantee we’ll get a precise number of replies.
Requests and replies can get lost (e.g., if the server crashes while processing a request)
So, we have to number requests and ignore any replies that don’t match the request number.
Our Model One server will work because it’s an echo server, but coincidence is not a great basis for understanding, so we’ll make a Model Two server here that chews up the message and returns a correctly numbered reply with the content “OK.” We’ll use messages consisting of two parts: a sequence number and a body.
We’ll begin by starting one or more servers, specifying a bind endpoint each time, as in Example 4-80
Then we’ll start the client, specifying the connect endpoints as arguments, as in Example 4-81
Each instance has a context, a DEALER socket it uses to talk to the servers, a counter of how many servers it’s connected to, and a request sequence number.
The request method in Example 4-83 does the hard work.
It sends a request to all connected servers in parallel (for this to work, all connections must be successful and completed by this time)
It then waits for a single successful reply, and returns that to the caller.
Here are some things to note about the client implementation:
The client will abandon the chase if it can’t find any responsive server within a few seconds.
The client has to create a valid REP envelope (i.e., add an empty message frame to the front of the message)
The client performs 10,000 name resolution requests (fake ones, as our server does essentially nothing) and measures the average cost.
On my test box, talking to one server, this requires about 60 microseconds.
Pro: it is simple, easy to make and easy to understand.
Pro: it does the job of failover, and it works rapidly, as long as there is at least one.
Con: we can’t prioritize our servers (i.e., primary, then secondary)
Con: the server can do at most one request at a time, period.
Model Three: Complex and Nasty The shotgun approach seems too good to be true.
We’re going to explore the complex/nasty option, even if it’s only to finally realize that we preferred the brutal approach.
We can solve the main problems of the client by switching to a ROUTER socket.
That lets us send requests to specific servers, avoid servers we know are dead, and in general be as smart as we want to be.
But doing ROUTER to ROUTER between two anonymous sockets (which haven’t set an identity) is not possible.
Both sides generate an identity for the other peer only when they receive a first message, and thus neither can talk to the other until it has first received a message.
The only way out of this conundrum is to cheat and use hard-coded identities in one direction.
The proper way to cheat, in a client/server case, is to let the client “know” the identity of the server.
Doing it the other way around would be insane, on top of complex and nasty, because any number of clients should be able to arise independently.
Insane, complex, and nasty are great attributes for a genocidal dictator, but terrible ones for software.
Rather than invent yet another concept to manage, we’ll use the connection endpoint as the identity.
This is a unique string on which both sides can agree without more prior knowledge than they already have for the shotgun model.
It’s a sneaky and effective way to connect two ROUTER sockets.
This means that our client can route a message to the server (i.e., send on its ROUTER socket, specifying the server endpoint as the identity) as soon as the connection is established.
That’s not immediately after doing a zmq_connect(), but at some random.
Herein lies one problem: we don’t know when the server will actually be available and complete its connection handshake.
If the server is online, it could be after a few milliseconds.
If the server is down, and the sysadmin is out to lunch, it could be an hour from now.
We need to know when servers become connected and available for work.
In the Freelance pattern, unlike the broker-based patterns we saw earlier in this chapter, servers are silent until spoken to.
Thus, we can’t talk to a server until it’s told us it’s online, which it can’t do until we’ve asked it.
My solution is to mix in a little of the shotgun approach from Model Two, meaning we’ll fire (harmless) shots at anything we can, and if anything moves, we know it’s alive.
We’re not going to fire real requests, but rather a kind of ping-pong heartbeat.
This is short and sweet to implement as a server.
Example 4-84 presents the code for Model Three of our echo server, now speaking the Freelance Protocol (FLP)
For clarity, it’s split into an example application and a class that does the hard work.
Example 4-86 presents the client API class, which is almost as complex and large as the Majordomo broker.
This API works in two halves—a common pattern for APIs that need to run in the background.
One half is a frontend object that our application creates and works with; the other half is a backend “agent” that runs in a background thread.
The frontend talks to the backend over an inproc pipe socket.
To implement the connect method (Example 4-88), the frontend object sends a multipart message to the backend agent.
The first part is a string “CONNECT”, and the second part is the endpoint.
It waits 100 msec for the connection to come up, which isn’t pretty but saves us from sending all requests to a single server at startup time.
To implement the request method, the frontend object sends a message to the backend, specifying a command “REQUEST” and the request message (Example 4-89)
It runs as an attached thread, talking to its parent over a pipe socket.
It is a fairly complex piece of work, so we’ll break it down into pieces.
First, the agent manages a set of servers, using our familiar class approach (Example 4-90)
We build the agent as a class that’s capable of processing messages coming in from its various sockets, as shown in Example 4-91
Freelance client API (flcliapi.c): control messages //  Callback when we remove server from agent "servers" hash table.
Finally, Example 4-94 shows the agent task itself, which polls its two sockets and processes incoming messages.
This API implementation is fairly sophisticated and uses a couple of techniques that we have not seen before: Multithreaded API.
Tickless poll timer In previous poll loops, we always used a fixed tick interval, such as 1 second.
This is simple enough but not excellent on power-sensitive clients (such as notebooks and mobile phones), where waking the CPU costs power.
For fun, and to help save the planet, the agent uses a “tickless timer,” which calculates the poll delay based on the next timeout we’re expecting.
A proper implementation would keep an ordered list of timeouts.
We just check all timeouts and calculate the poll delay until the next one.
Conclusion In this chapter, we’ve seen a variety of reliable request-reply mechanisms, each with certain costs and benefits.
The example code is largely ready for real use, though it is not optimized.
Of all the different patterns, the two that stand out for production use are the Majordomo pattern, for broker-based reliability, and the Freelance pattern, for brokerless reliability.
How to build a shared key-value store (the Clone pattern)
How to use the Binary Star pattern to add failover to a server.
First, PUB sends each message to “all of many,” whereas PUSH and DEALER rotate messages to “one of many.” You cannot simply replace PUSH with PUB or vice versa and hope that things will work.
This bears repeating, because people seem to quite often suggest doing this.
This means large volumes of data, sent rapidly to many recipients.
If you need millions of messages per second sent to thousands of points, you’ll appreciate pub-sub a lot more than if you need a few messages a second sent to a handful of recipients.
To get scalability, pub-sub uses the same trick as push-pull, which is to get rid of backchatter.
With pub-sub, it’s how the pattern can map cleanly to the Pragmatic General Multicast (PGM) protocol, which is handled by the network switch.
In other words, subscribers don’t connect to the publisher at all; they connect to a multicast group on the switch, to which the publisher sends its messages.
When we remove back-chatter, our overall message flow becomes much simpler, which lets us make simpler APIs, simpler protocols, and in general reach many more people.
But we also remove any possibility to coordinate senders and receivers.
Publishers can’t tell when subscribers are successfully connected, both on initial connections and on reconnections after network failures.
Subscribers can’t tell publishers anything that would allow publishers to control the rate of messages they send.
Publishers only have one setting, which is full speed, and subscribers must either keep up or lose messages.
Publishers can’t tell when subscribers have disappeared due to processes crashing, networks breaking, and so on.
The upside is that there are many use cases where almost reliable multicast is just fine.
When we need this back-chatter, we can either switch to using ROUTER-DEALER (which I tend to do for most normal-volume cases), or we can add a separate channel for synchronization (we’ll see an example of this later in this chapter)
Pub-sub is like a radio broadcast: you miss everything before you join, and then how much information you get depends on the quality of your reception.
Surprisingly, this model is useful and widespread because it maps perfectly to real-world distribution of information.
As we did for request-reply, let’s define reliability in terms of what can go wrong.
Subscribers join late, so they miss messages the server already sent.
Subscribers can fetch messages too slowly, so queues build up and then overflow.
Subscribers can drop off and lose messages while they are away.
Subscribers can crash and restart, and lose whatever data they’ve already received.
Networks can become overloaded and drop data (specifically, for PGM)
Networks can become too slow, so publisher-side queues overflow and publishers.
Pub-Sub Tracing (Espresso Pattern) Let’s start this chapter by looking at a way to trace pub-sub networks.
In Chapter 2 we saw a simple proxy that used these to do transport bridging.
The zmq_proxy() method has three arguments: a frontend and backend socket that it bridges together, and a capture socket to which it will send all messages.
The code is deceptively simple, as you can see in Example 5-1
Espresso pattern (espresso.c) // //  Espresso pattern //  This shows how to capture data using a pub-sub proxy // #include "czmq.h"
The subscriber thread requests messages starting with //  A and B, then reads and counts incoming messages.
The publisher sends random messages starting with A–J, as seen in Example 5-2
The listener, shown in Example 5-3, receives all messages flowing through the proxy on its pipe.
In CZMQ, the pipe is a pair of ZMQ_PAIR sockets that connect attached child threads.
The main task (Example 5-4) starts the subscriber and publisher, and then sets itself up as a listening proxy.
Espresso works by creating a listener thread that reads a PAIR socket and prints anything it gets.
That PAIR socket is one end of a pipe; the other end (another PAIR) is the socket we pass to zmq_proxy()
In practice, you’d filter interesting messages to get the essence of what you want to track (hence the name of the pattern)
The subscriber thread subscribes to “A” and “B,” receives five messages, and then destroys its socket.
When you run an example, the listener prints two subscription messages, five data messages, two unsubscribe messages, and then silence:
That shows neatly how the publisher socket stops sending data when there are no subscribers for it.
I’ve already explained why publishers don’t get notified when there are new subscribers: in large pub-sub systems the volumes of data make it pretty much impossible.
To make really large-scale pub-sub networks work, you need a protocol like PGM that exploits an upscale Ethernet switch’s ability to multicast data to thousands of subscribers.
Trying to do a TCP unicast from the publisher to each of thousands of subscribers just doesn’t scale.
You get weird spikes, unfair distribution (some subscribers getting the message before others), network congestion, and general unhappiness.
The publisher never sees when subscribers join or leave: this all happens in the switch, which we don’t really want to start reprogramming.
However, in a lower-volume network with a few dozen subscribers and a limited number of topics we can use TCP, and then the XSUB and XPUB sockets do talk to each other, as we just saw in the Espresso pattern.
We’ll start by making a publisher and subscriber that highlight the worst-case scenario.
It starts by immediately sending messages to each of a thousand topics, and then it sends one update a second to a random topic.
Without LVC, a subscriber would have to wait an average of 500 seconds to get any data.
To add some drama, let’s pretend there’s an escaped convict called Gregor threatening to rip the head off Roger the toy bunny if we can’t fix that 8.3-minutes delay.
Note that it has a command-line option to connect to some address, but otherwise binds to an endpoint.
We’ll use this later to connect to our last value cache.
Pathological publisher (pathopub.c) // //  Pathological publisher //  Sends out 1,000 topics and then one random update per second.
Pathological subscriber (pathosub.c) // //  Pathological subscriber //  Subscribes to one random topic and prints received messages // #include "czmq.h"
Try building and running these: first the subscriber, then the publisher.
You’ll see that the subscriber reports getting “Save Roger,” as you’d expect:
It’s when you run a second subscriber that you understand Roger’s predicament: you have to leave it an awfully long time before it reports getting any data.
As I promised, it’s a proxy that binds to two sockets and then handles messages on both.
Last value caching proxy (lvcache.c) // //  Last value cache //  Uses XPUB subscription messages to resend data // #include "czmq.h"
We route topic updates from frontend to backend, and we handle subscriptions by sending whatever we cached, if anything, as illustrated in Example 5-8
When we get a new subscription, we pull data from the cache, as shown in Example 5-9
Then run as many instances of the subscriber as you want to try, each time connecting to the proxy on port 5558:
Each subscriber happily reports “Save Roger,” and Gregor the Escaped Convict slinks back to his seat for dinner and a nice cup of hot milk, which is all he really wanted in the first place.
Slow Subscriber Detection (Suicidal Snail Pattern) A common problem you will hit when using the pub-sub pattern in real life is the slow subscriber.
In an ideal world, we stream data at full speed from publishers to subscribers.
In reality, subscriber applications are often written in interpreted languages, or do a lot of work, or are just badly written, to the extent that they can’t keep up with publishers.
How do we handle a slow subscriber? The ideal fix is to make the subscriber faster, but that might take a significant amount of work and time.
Some of the classic strategies for handling a slow subscriber are:
This is what Gmail does when I don’t read my email for a couple of hours.
But in high-volume messaging, pushing queues upstream has the thrilling but unprofitable result of making publishers run out of memory and then crash—especially if there are lots of subscribers and it’s not possible to flush to disk for performance reasons.
None of these classic strategies fit, so we need to get creative.
Rather than disconnecting the publisher, let’s convince the subscriber to kill itself.
When a subscriber detects that it’s running too slowly (where “too slowly” is presumably a configured option that really means “so slowly that if you ever get here, shout really loudly because I need to know, so I can fix this!”), it croaks and dies.
How can a subscriber detect this? One way would be to sequence messages (number them in order) and use an HWM at the publisher.
Now, if the subscriber detects a gap (i.e., the numbering isn’t consecutive), it knows something is wrong.
We then tune the HWM to the “croak and die if you hit this” level.
First, if we have many publishers, how do we sequence messages? The solution is to give each publisher a unique ID and add that to the sequencing.
Second, if subscribers use ZMQ_SUBSCRIBE filters, they will get gaps by definition.
Some use cases won’t use filters, and sequencing will work for them.
But a more general solution is that the publisher timestamps each message.
When a subscriber gets a message, it checks the time, and if the difference is more than, say, one second, it does the “croak and die” thing, possibly firing off a squawk to some operator console first.
The Suicidal Snail pattern works especially well when subscribers have their own clients and service-level agreements and need to guarantee certain maximum latencies.
Aborting a subscriber may not seem like a constructive way to guarantee a maximum latency, but it’s the assertion model.
Allow late data to flow downstream, and the problem may cause wider damage and take longer to appear on the radar.
Example 5-10 shows a minimal example of a Suicidal Snail.
Suicidal Snail (suisnail.c) // //  Suicidal Snail // #include "czmq.h"
It connects to the publisher and subscribes to //  everything.
It sleeps for a short time between messages to simulate doing //  too much work.
If a message is more than one second late, it croaks.
It publishes a time-stamped message to its PUB socket every millisecond.
The main task (Example 5-12) simply starts a client and a server, and then waits for the client to signal that it has died.
Here are some things to note about the Suicidal Snail example:
The message here consists simply of the current system clock as a number of milliseconds.
In a realistic application, you’d have at least a message header with the timestamp and a message body with data.
The example has subscriber and publisher in a single process as two threads.
High-Speed Subscribers (Black Box Pattern) Now let’s look at one way to make our subscribers faster.
A common use case for pubsub is distributing large data streams, like market data coming from stock exchanges.
A typical setup would have a publisher connected to a stock exchange, taking price quotes and sending them out to a number of subscribers.
If there were only a handful of subscribers, we could use TCP.
With a larger number of subscribers, we’d probably use reliable multicast, i.e., PGM.
So we set up our architecture with a bunch of boxes—one for the publisher and one for each subscriber.
And as we pump data into our subscribers, we notice two things:
When we do even the slightest amount of work with a message, it slows down our subscribers to the point where they can’t catch up with the publisher again.
We’re hitting a ceiling, at both the publisher and the subscribers, of around 6M messages a second, even after careful optimization and TCP tuning.
The subscriber talks to the publisher over TCP or PGM.
The subscriber talks to its workers, which are all in the same process, over inproc.
The subscriber thread hits 100% of CPU, and because it is one thread, it cannot use more than one core.
We want to split the work across multiple threads that can run in parallel.
The approach used by many high-performance products, which works here, is sharding.
Using sharding, we split the work into parallel and independent streams.
Half of the topic keys are in one stream, half in another (Figure 5-2)
We could use many streams, but performance won’t scale unless we have free cores.
Ideally, we want to match the number of fully loaded threads in our architecture with the number of cores.
When threads start to fight for cores and CPU cycles, the cost of adding more threads outweighs the benefits.
Reliable Publish-Subscribe (Clone Pattern) As a larger worked example, we’ll take the problem of making a reliable publish-subscribe architecture.
The goal is to allow a set of applications to share some common state.
We have a large set of client applications—say, thousands or tens of thousands.
Any application can update the state at any point in time.
Let’s say that updates are reasonably low-volume, we don’t have real time goals, and the whole state can fit into memory.
A configuration that is shared by a group of cloud servers.
Exchange rate data that is updated in real time and available to applications.
Centralized Versus Decentralized A first decision we have to make is whether to work with a central server or not.
Conceptually, a central server is simpler to understand because networks are not naturally symmetrical.
With a central server we avoid all questions of discovery, bind versus connect, and so on.
Generally, a fully distributed architecture is technically more challenging but ends up with simpler protocols.
That is, each node must act as server and client in the right way, which is delicate.
When done right, the results are simpler than using a central server.
A central server will become a bottleneck in high-volume use cases.
If handling scale on the order of millions of messages a second is required, we should aim for decentralization right away.
A centralized architecture will scale to more nodes more easily than a decentralized one.
That is, it’s easier to connect 10,000 nodes to one server than to each other.
So, for the Clone pattern we’ll work with a server that publishes state updates and a set of clients that represent applications.
Representing State as Key-Value Pairs We’ll develop the Clone pattern in stages, solving one problem at a time.
First, let’s look at how to update a shared state across a set of clients.
We need to decide how to represent our state, as well as the updates.
The simplest plausible format is a key-value store, where one key-value pair represents an atomic unit of change in the shared state.
We looked at a simple pub-sub example in Chapter 2, the weather server and client.
Let’s change the server to send key-value pairs, and the client to store these in a hash table.
This lets us send updates from one server to a set of clients using the classic pub-sub model (Figure 5-3)
An update is either a new key-value pair, a modified value for an existing key, or a deleted key.
We can assume for now that the whole store fits in memory and that applications access it by key, such as by a hash table or dictionary.
For larger stores and some kind of persistence we’d probably store the state in a database, but that’s not relevant here.
Our first attempt at the server is shown in Example 5-13
Lets us build this source without creating a library #include "kvsimple.c"
And our first attempt at the client is shown in Example 5-14
Lets us build this source without creating a library #include "kvsimple.c"
Here are some things to note about this first model:
The server generates messages with a randomized four-digit key, which lets us simulate a large but not enormous hash table (10K entries)
We don’t implement deletions in this version: all messages are inserts or updates.
The server does a 200 msec pause after binding its socket.
We’ll remove that in later versions of the Clone code.
We’ll use the terms publisher and subscriber in the code to refer to sockets.
This will help later when we have multiple sockets doing different things.
Example 5-15 shows the kvmsg class, in the simplest form that works for now.
The kvmsg class holds a single key-value message consisting of a //  list of 0 or more frames.
Example 5-16 contains the code for the constructor and destructor for the class.
The recv method, shown in Example 5-17, reads a key-value message from the socket and returns a new kvmsg instance.
The send method (Example 5-18) sends a multiframe key-value message to a socket.
The key methods in Example 5-19 let the caller get and set the message key as a fixed string and as a printf-formatted string.
The two methods in Example 5-20 let the caller get and set the message sequence number.
The two methods in Example 5-21 let the caller get and set the message body, as a fixed string and as a printf-formatted string.
The size method (Example 5-22) returns the body size of the most recently read message, if any exists.
The store method (Example 5-23) stores the key-value message into a hashmap, unless the key and value are both null.
It nullifies the kvmsg reference so that the object is owned by the hashmap, not the caller.
The dump method, shown in Example 5-24, prints the key-value message to stderr for debugging and tracing:
It’s good practice to have a self-test method that tests the class; this also shows how it’s used in applications.
Later, we’ll make a more sophisticated kvmsg class that will work in real applications.
Both the server and the clients maintain hash tables, but this first model only works properly if we start all clients before the server and the clients never crash.
Getting an Out-of-Band Snapshot So now we have our second problem: how to deal with late-joining clients or clients that crash and then restart.
For a late (or recovering) client to catch up with a server, it has to get a snapshot of the server’s state.
Just as we’ve reduced “message” to mean “a sequenced key-value pair,” we can reduce “state” to mean “a hash table.” To get the server state, a client opens a DEALER socket and asks for it explicitly (Figure 5-4)
To make this work, we have to solve a problem of timing.
Getting a state snapshot will take a certain amount of time, possibly fairly long if the snapshot is large.
We need to correctly apply updates to the snapshot, but the server won’t know when to start sending us updates.
One approach would be to start subscribing, get a first update, and then ask for “state for update N.” This would require the server to store one snapshot for each update, though, which isn’t practical.
Instead, we will do the synchronization in the client, as follows:
The client first subscribes to updates and then makes a state request.
This guarantees that the state is going to be newer than the oldest update it has.
When the client receives its state update, it begins once again to read updates.
The client then applies updates to its own state snapshot.
Lets us build this source without creating a library #include "kvsimple.c"
The state manager task, shown in Example 5-27, maintains the state and handles requests from clients for snapshots.
Model Two of our Clone client is shown in Example 5-28
Lets us build this source without creating a library #include "kvsimple.c"
Here are some things to note about these two programs:
One thread produces the updates (randomly) and sends these to the main PUB socket, while the other thread handles state requests on the ROUTER socket.
The two communicate across PAIR sockets over an inproc connection.
In C it consists of about 50 lines of code.
A lot of the heavy lifting is done in the kvmsg class.
Even so, the basic Clone pattern is easier to implement than it seemed at first.
The hash table holds a set of kvmsg objects, and the server sends these, as a batch of messages, to the client requesting state.
If multiple clients request state at once, each will get a different snapshot.
We assume that the client has exactly one server to talk to.
The server must be running; we do not try to solve the question of what happens if the server crashes.
Right now, these two programs don’t do anything real, but they correctly synchronize state.
It’s a neat example of how to mix different patterns: PAIR-PAIR, PUB-SUB, and ROUTER-DEALER.
Republishing Updates from Clients In our second model, changes to the key-value store came from the server itself.
This is a centralized model that is useful, for example, if we have a central configuration file we want to distribute, with local caching on each node.
A more interesting model takes updates from clients, not the server.
If it crashes, we can start a new instance and feed it new values.
We can use the key-value store to share knowledge between active peers.
To send updates from clients back to the server, we could use a variety of socket patterns.
The simplest plausible solution is a PUSH-PULL combination (Figure 5-5)
Why don’t we allow clients to publish updates directly to each other? While this would reduce latency, it would remove the guarantee of consistency.
You can’t get consistent shared state if you allow the order of updates to change depending on who receives them.
If two clients make changes at the same time, but to different keys, there will be no confusion.
But if the two clients try to change the same key at roughly the same time, they’ll end up with different notions of its value.
There are a few strategies for obtaining consistency when changes happen in multiple places at once.
No matter the precise timing of the changes that clients make, they are all pushed through the server, which enforces a single sequence according to the order in which it gets updates.
By mediating all changes, the server can also add a unique sequence number to all updates.
With unique sequencing, clients can detect the nastier failures—network congestion and queue overflow.
If a client discovers that its incoming message stream has a hole, it can take action.
It seems sensible for the client to contact the server and ask for the missing messages, but in practice that isn’t useful.
If there are holes, they’re caused by network stress, and adding more stress to the network will make things worse.
All the client can do is warn its users that it is “unable to continue,” stop, and not restart until someone has manually checked the cause of the problem.
In our third model, we’ll generate state updates in the client.
Lets us build this source without creating a library #include "kvsimple.c"
The body of the main task, shown in Example 5-30, collects updates from clients and publishes them back out to clients.
Lets us build this source without creating a library #include "kvsimple.c"
We first request a state snapshot, as shown in Example 5-32
Then we wait for updates from the server and, every so often, send a random key-value update to the server, as shown in Example 5-33
Here are some things to note about this third design:
It manages a PULL socket for incoming updates, a ROUTER socket for state requests, and a PUB socket for outgoing updates.
The client uses a simple tickless timer to send a random update to the server once a second.
In a real implementation, we would drive updates from application code.
Working with Subtrees As we grow the number of clients, the size of our shared store will also grow.
Eventually, it stops being reasonable to send everything to every client.
This is the classic story with publish-subscribe: when you have a very small number of clients, you can send every.
So, even when working with a shared store, some clients will want to work only with a part of that store, which we call a subtree.
The client has to request the subtree when it makes a state request, and it must specify the same subtree when it subscribes to updates.
One is the path hierarchy, and another is the topic tree.
We’ll use the path hierarchy and extend our client and server so that a client can work with a single subtree.
Once you see how to work with a single subtree, you’ll be able to extend this yourself to handle multiple subtrees if your use case demands it.
Example 5-34 shows the server implementing subtrees, a small variation on Model Three.
Lets us build this source without creating a library #include "kvsimple.c"
Lets us build this source without creating a library #include "kvsimple.c"
Ephemeral Values An ephemeral value is one that expires automatically unless regularly refreshed.
If you think of Clone being used for a registration service, then ephemeral values would let you use dynamic values.
A node joins the network, publishes its address, and refreshes this regularly.
The usual abstraction for ephemeral values is to attach them to a session and delete them when the session ends.
In Clone, sessions would be defined by clients and would end if the client died.
A simpler alternative is to attach a time to live (TTL) to ephemeral values, which the server uses to expire values that haven’t been refreshed in time.
A good design principle that I use whenever possible is to not invent concepts that are not absolutely essential.
If we have a large quantity of ephemeral values, sessions offer better performance.
If we use a handful of ephemeral values, it’s fine to set a TTL on each one.
If we use masses of ephemeral values, it’s more efficient to attach them to sessions and expire them in bulk.
This isn’t a problem we face at this stage, and we may never face it, so sessions go out the window here.
Next we need a way to say, “delete this value.” Up until now, servers and clients have always blindly inserted or updated new values into their hash tables.
We’ll say that if the value is empty, that means “delete this key.”
Example 5-36 shows a more complete version of the kvmsg class, which implements a properties frame (and adds a UUID frame, which we’ll need later)
It also handles empty values by deleting the key from the hash, if necessary.
The two helpers in Example 5-37 serialize a list of properties to and from a message frame.
The constructor and destructor for the class are shown in Example 5-38
The recv method in Example 5-39 reads a key-value message from the socket and returns a new kvmsg instance.
Send key-value message to socket; any empty frames are sent as such.
The rest of the method is unchanged from kvsimple ...
The dup method (Example 5-40) duplicates a kvmsg instance and returns the new instance.
The key, sequence, body, and size methods are the same as in kvsimple ...
The methods in Example 5-41 get and set the UUID for the key-value message.
The methods in Example 5-42 get and set a specified message property.
The store method (Example 5-43) stores the key-value message into a hashmap, unless the key and value are both null.
It nullifies the kvmsg reference so that the object is owned by the hashmap, not the caller.
The dump method (Example 5-44) extends the kvsimple implementation with support for message properties.
The selftest method, shown in Example 5-45, is the same as in kvsimple, with added support for the UUID and property features of kvmsg.
It uses the full kvmsg class now, and sets a randomized ttl property (measured in seconds) on each message:
Using a Reactor Up until now, we have used a poll loop in the server.
In this next model of the server, we switch to using a reactor.
Using a reactor makes the code more verbose but easier to understand and build out, because each piece of the server is handled by a separate reactor handler.
We use a single thread and pass a server object around to the reactor handlers.
We could have organized the server as multiple threads, each handling one socket or timer, but that works better when threads don’t have to share data.
In this case, all work is centered around the server’s hashmap, so one thread is simpler.
One to handle snapshot requests coming on the ROUTER socket.
One to handle incoming updates from clients, coming on the PULL socket.
One to expire ephemeral values that have passed their TTL.
The code for Model Five of the Clone server is shown in Example 5-46
Lets us build this source without creating a library #include "kvmsg.c"
We handle ICANHAZ? requests by sending snapshot data to the client that requested it, as shown in Example 5-47
Example 5-48 shows is the reactor handler for the snapshot socket; it accepts just the ICANHAZ? request and replies with a state snapshot ending with a KTHXBAI message.
We store each update with a new sequence number and, if necessary, a time to live, as shown in Example 5-49
At regular intervals, we flush ephemeral values that have expired (Example 5-50)
Adding the Binary Star Pattern for Reliability The Clone models we’ve explored up until now have been relatively simple.
However, we’re now going to get into unpleasantly complex territory, which has me getting up for another espresso.
You should appreciate the fact that implementing “reliable” messaging is complex enough that you always need to ask, “Do we actually need this?” before jumping into it.
If you can get away with being unreliable, or with “good enough” reliability, you can make a huge win in terms of cost and complexity.
As you play with the last model, you’ll stop and restart the server.
It might look like it recovers, but of course it’s applying updates to an empty state instead of the proper current state.
Any new client joining the network will only get the latest updates instead of the full historical record.
What we want is a way for the server to recover from being killed or crashing.
We also need to provide backup in case the server is out of commission for any length of time.
When people ask for “reliability,” ask them to list the failures they want to handle.
The server process crashes and is automatically or manually restarted.
The process loses its state and has to get it back from somewhere.
The server machine dies and is offline for a significant time.
The server process or machine gets disconnected from the network, such as when a switch dies or a data center gets knocked out.
It may come back at some point, but in the meantime clients need an alternate server.
We can use the Binary Star pattern from Chapter 6 to organize these into a primary and a backup.
Binary Star is a reactor pattern, so it’s useful that we’ve already refactored the last server model into a reactor style.
We need to ensure that updates are not lost if the primary server crashes.
The simplest technique is to send them to both servers.
The backup server can then act as a client and keep its state synchronized by receiving updates, as all clients do.
It can’t yet store these in its hash table, but it can hold onto them for a while.
We use a pub-sub flow instead of a push-pull flow for client updates sent to the servers.
This takes care of fanning out the updates to both servers.
We add heartbeats to server updates (to clients), so that a client can detect when the primary server has died.
We connect the two servers using the Binary Star bstar reactor class.
Binary Star relies on the clients to “vote” by making an explicit request to the server they consider “active.” We’ll use snapshot requests as the voting mechanism.
We make all update messages uniquely identifiable by adding a UUID field.
The client generates this, and the server propagates it back on republished updates.
The passive server keeps a “pending list” of updates that it has received from clients but not yet from the active server, and updates it’s received from the active server but not yet from the clients.
The list is ordered from oldest to newest, so that it is easy to remove updates off the head.
It’s useful to design the client logic as a finite-state machine.
The client opens and connects its sockets, and then requests a snapshot from the first server.
To avoid request storms, it will ask any given server only twice.
One request might get lost, which would be bad luck.
The client waits for a reply (snapshot data) from the current server and, if it gets it, stores it.
If there is no reply within some timeout, it fails over to the next server.
When the client has gotten its snapshot, it waits for and processes updates.
Again, if it doesn’t hear anything from the server within some timeout, it fails over to the next server.
It’s quite likely during startup or failover that some clients may be trying to talk to the primary server while others are trying to talk to the backup server.
The Binary Star state machine handles this (Figure 5-6), hopefully accurately.
It’s hard to prove software correct; instead, we hammer it until we can’t prove it wrong.
The client detects that primary server is no longer sending heartbeats, and concludes it has died.
The client connects to the backup server and requests a new state snapshot.
The backup server starts to receive snapshot requests from clients, and detects that primary server has gone, so it takes over as primary.
The backup server applies its pending list to its own hash table, and then starts to process state snapshot requests.
Start up as passive server, and connect to the backup server as a Clone client.
Start to receive updates from clients, via its SUB socket.
If both servers crash, we lose all server state and there’s no way to recover it.
Multiple clients do not update the same hash keys at the same time.
Client updates will arrive at the two servers in a different order.
Therefore, the backup server may apply updates from its pending list in a different order than the primary server.
Updates from one client will always arrive in the same order on both servers, so that is safe.
Thus the architecture for our high-availability server pair using the Binary Star pattern has two servers and a set of clients that talk to both servers (Figure 5-7)
In Example 5-52, we define a set of reactor handlers and our server object structure.
The main task parses the command line to decide whether to start as a primary or backup server.
This interconnects the two servers so they can agree on which one is the primary and which one is the backup.
To allow the two servers to run on the same box, we use different ports for the primary and backup, as shown in Example 5-53
Ports 5556/5566 are used to receive voting events (snapshot requests in the clone pattern)
After we’ve set up our sockets, we register our Binary Star event handlers and then start the bstar reactor.
This finishes when the user presses Ctrl-C or when the process receives a SIGINT interrupt.
The active server applies them immediately to its kvmap, whereas the passive one queues them as pending.
We purge ephemeral values using exactly the same code as in //  the previous clonesrv5 example ...
We send a HUGZ message once a second to all subscribers so that they can detect if our server dies (Example 5-56)
They’ll then switch over to the backup server, which will become active.
When we switch from passive to active, we apply our pending list so that our kvmap is up-to-date.
When we switch to passive, we wipe our kvmap and grab a new snapshot from the active process.
When we get an update, we create a new kvmap if necessary, and then add our update to our kvmap (Example 5-58)
This model is only a few hundred lines of code, but it took quite a while to get working.
To be accurate, building Model Six took about a full week of “Sweet god, this is just too complex for the book” hacking.
We’ve assembled pretty much everything and the kitchen sink into this small application.
What surprised me was that the up-front design was pretty accurate.
Still, the details of writing and debugging so many socket flows are quite challenging.
The reactor-based design removes a lot of the grunt work from the code, and what remains is simpler and easier to understand.
The whole server runs as one thread, so there’s no inter-thread weirdness going on—just a structure pointer (self) passed around to all handlers, which can do their thing happily.
One nice side effect of using reactors is that the code, being less tightly integrated into a poll loop, is much easier to reuse.
I built it piece by piece, and got each piece working properly before going onto the next one.
Because there are four or five main socket flows, that meant quite a lot of debugging and testing.
For testing, I always try to use valgrind, which catches memory leaks and invalid memory accesses.
In C, this is a major concern, as you can’t delegate to a garbage collector.
Using proper and consistent abstractions like kvmsg and CZMQ helps enormously.
The Clustered Hashmap Protocol While the Model Six server is pretty much a mashup of the previous model plus the Binary Star pattern, the client is quite a lot more complex.
But before we get to that, let’s look at the final protocol.
I’ve written this up as a specification on the ZeroMQ RFC website as the Clustered Hashmap Protocol (CHP)
Roughly, there are two ways to design a complex protocol such as this one.
One way is to separate each flow into its own set of sockets.
The advantage is that each flow is simple and clean.
The disadvantage is that managing multiple socket flows at once can be quite complex.
Using a reactor makes it simpler, but still, it makes a lot of moving pieces that have to fit together correctly.
The second way to make such a protocol is to use a single socket pair for everything.
In this case I’d have used ROUTER for the server and DEALER for the clients, and then done everything over that connection.
It makes for a more complex protocol, but at least the complexity is all in one place.
In Chapter 7 we’ll look at an example of a protocol done over a ROUTER-DEALER combination.
Note that “SHOULD” and “MUST” are keywords that we use in protocol specifications to indicate requirement levels.
The client MAY open a third connection, if it wants to update the hashmap:
This extra frame is not shown in the commands explained below.
The client MUST start by sending an ICANHAZ command to its snapshot connection.
The sequence number has no significance and may be zero.
The sequence number MUST be the highest sequence number of the KVSYNC commands previously sent.
When the client has received a KTHXBAI command it SHOULD start to receive messages from its subscriber connection, and apply them.
When the server has an update for its hashmap it MUST broadcast this on its publisher socket as a KVPUB command.
The client MUST discard any KVPUB command whose sequence numbers are not strictly greater than the last KTHXBAI or KVPUB command received.
The UUID is optional and frame 2 MAY be empty (size zero)
The properties field is formatted as zero or more instances of name=value followed by a newline character.
If the key-value pair has no properties, the properties field is empty.
If the value is empty, the client SHOULD delete its key-value entry with the specified key.
In the absence of other updates the server SHOULD send a HUGZ command at regular intervals, e.g., once per second.
The client MAY treat the absence of HUGZ as an indicator that the server has crashed; see “Reliability” below.
When the client has an update for its hashmap, it MAY send this to the server via its publisher connection as a KVSET command.
The sequence number has no significance and may be zero.
The UUID SHOULD be a universally unique identifier, if a reliable server architecture is used.
If the value is empty, the server MUST delete its key-value entry with the specified key.
If the KVSET command has a ttl property, the server SHOULD delete the key-value pair and broadcast a KVPUB with an empty value in order to delete this from all clients when the TTL has expired.
Detect the lack of HUGZ over a time period and use this as an indicator that the.
Connect to a backup server and re-request a state synchronization.
Since all updates pass through a single server the overall throughput will be limited to some millions of updates per second, at peak, and probably less.
If you make a nontrivial protocol and you expect applications to implement it properly, most developers will get it wrong most of the time.
You’re going to be left with a lot of unhappy people complaining that your protocol is too complex, too fragile, and too hard to use.
Whereas if you give them a simple API to call, you have some chance of them buying in.
Our multithreaded API consists of a frontend object and a background agent, connected by two PAIR sockets (Figure 5-8)
Connecting two PAIR sockets like this is so useful that your high-level binding should probably do what CZMQ does, which is package a “create new thread with a pipe that I can use to send messages to it” method.
The multithreaded APIs that we see in this book all take the same form:
The constructor for the object (clone_new()) creates a context and starts a background thread connected with a pipe.
It holds onto one end of the pipe so it can send commands to the background thread.
The background thread starts an agent that is essentially a zmq_poll() loop reading from the pipe socket, and any other sockets (here, the DEALER and SUB sockets)
If the method needs a return code, it can wait for a reply message from the agent.
If the agent needs to send asynchronous events back to the frontend, we add a recv.
We may want to expose the frontend pipe socket handle to allow the class to be.
Example 5-59 presents the Clone client, which has now become just a thin shell using the clone cl.
Under the hood, we’re in fact talking to three ports.
However, as the CHP specifies, the three ports are on consecutive port numbers:
Let’s end with the source code for the clone stack.
This is a complex piece of code, but it’s easier to understand when you break it into the frontend object class and the backend agent.
Start up by getting a snapshot from the first server.
When we get a snapshot, switch to reading from the subscriber socket.
If we don’t get a snapshot, then fail over to the second server.
If we got input on the pipe, handle the control message from the frontend object.
If we got input on the subscriber, store or apply the update.
If we didn’t get anything from the server within a certain time, fail over.
The structure of the class is given in Example 5-60
Example 5-61 presents the constructor and destructor for the clone class.
Note that we create a context specifically for the pipe that connects our frontend to the backend agent.
The subtree method in Example 5-62 specifies a subtree for snapshots and updates, which we must do before connecting to a server as the subtree specification is sent as the first command to the server.
The connect method (Example 5-63) connects to a new server endpoint.
The set method (Example 5-64) sets a new value in the shared hashmap.
The get method (Example 5-65) looks up a value in the distributed hash table.
It sends a [GET][key] command to the agent and waits for a value response.
If there is no value available, this method will eventually return NULL.
The backend agent manages a set of servers, which we implement using our simple class model, as shown in Example 5-66
Example 5-67 shows the implementation of the backend agent itself.
The code in Example 5-68 handles the different control messages from the frontendSUBTREE, CONNECT, SET, and GET:
When we set a property, we push the new key-value pair onto all our connected servers, as illustrated in Example 5-69
The asynchronous agent (Example 5-70) manages a server pool and handles the requestreply dialog when the application asks for it.
We’re now ready to process incoming messages, as shown in Example 5-71
If nothing at all comes from our server within the timeout, that means the server is dead.
It’s hard to overemphasize the power and persistence of a working open source community.
There really does not seem to be a better way of making software for the long term.
Not only does the community choose the best problems to solve, but it solves them minimally, carefully, and then looks after these answers for years—perhaps decades—until they’re no longer relevant.
The code is nasty, mainly because it’s highly optimized but also because it’s written in C++, a language that lends itself to subtle and deep nastiness.
A lot of languages have multiple bindings (Erlang, Ruby, C#, at least), written by different people over time or taking varying approaches.
You vote by using one or the other, contributing to it, or ignoring it.
There are a series of reimplementations of libzmq, starting with JeroMQ, a full Java translation of the library, which is now the basis for NetMQ, a C# stack.
How to Make Really Large Architectures There are, it has been said (at least by people reading this sentence out loud), two ways to make really large-scale software.
Option One is to throw massive amounts of money and problems at empires of smart people, and hope that what emerges is not yet another career killer.
If you are building on a lot of experience, have kept your teams solid and are not aiming for technical brilliance, and are furthermore incredibly lucky, it works.
But gambling with hundreds of millions of others’ money isn’t for everyone.
For the rest of us who want to build large-scale software, there’s Option Two, which is open source, and more specifically, free software.
If you’re asking how the choice of software license is relevant to the scale of the software you build, that’s the right question.
The brilliant and visionary Eben Moglen once said, roughly, that a free software license is the contract on which a community builds.
When I heard this, about 10 years ago, the following idea came too: Can we deliberately grow free software communities?
Psychology of Software Architecture Dirkjan Ochtman pointed me to Wikipedia’s definition of software architecture as “the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both.” For me, this vapid and circular jargon is a good example of how miserably little we understand what actually makes a successful large-scale software architecture.
Architecture is the art and science of making large artificial structures for human use.
If there is one thing I’ve learned and applied successfully in 30 years of making larger and larger software systems, it is this: software is about people.
And in software, human use starts with the programmers who make the software itself.
The two most important psychological elements are that we’re really bad at understanding complexity and that we are really good at working together to divide and conquer large problems.
We’re highly social apes, and kind of smart, but only in the right kind of crowd.
Our mental bandwidth is limited, so we’re all stupid at some point.
This is the number one rule: simplicity beats functionality, every single time.
If you can’t understand an architecture on a cold gray Monday morning before coffee, it is too complex.
Selfishness We act only out of self-interest, so the architecture must create space and opportunity for selfish acts that benefit the whole.
For example, I’ll spend hours helping someone else understand something because that could be worth days to me later.
Laziness We make lots of assumptions, many of which are wrong.
We are happiest when we can spend the least effort to get a result or test an assumption quickly, so the architecture has to make this possible.
Jealousy We’re jealous of others, which means we’ll overcome our stupidity and laziness to prove others wrong and beat them in competition.
The architecture thus has to create space for public competition based on fair rules that anyone can understand.
Fear We’re unwilling to take risks, especially if they might make us look stupid.
Fear of failure is a major reason people conform and follow the group in mass stupidity.
The architecture should make silent experimentation easy and cheap, giving people opportunities for success without punishing failure.
Reciprocity We’ll pay extra in terms of hard work, even money, to punish cheats and enforce fair rules.
The architecture should be heavily rule-based, telling people how to work together, but not what to work on.
Conformity We’re happiest to conform, out of fear and laziness, which means if the patterns are good, clearly explained and documented, and fairly enforced, we’ll naturally choose the right path every time.
Pride We’re intensely aware of our social status, and we’ll work hard to avoid looking stupid or incompetent in public.
The architecture has to make sure every piece we make has our name on it, so we’ll have sleepless nights stressing about what others will say about our work.
Greed We’re ultimately economic animals (see selfishness), so the architecture has to give us economic incentive to invest in making it happen.
Maybe it’s polishing our reputation as experts, maybe it’s literally making money from contributing some skill or component.
It doesn’t matter what it is, but there must be economic incentive.
Think of architecture as a marketplace, not an engineering design.
These strategies work not only on a large scale but also on a small scale, within an organization or team.
It happened to the eldest brother-in-law of the cousin of a friend of mine’s colleague at work.
Patrick was a computer scientist with a PhD in advanced network topologies.
He spent two years and his savings building a new product, and chose the BSD license because he believed that would get him more adoption.
He worked in his attic, at great personal cost, and proudly published his work.
People applauded, for it was truly fantastic, and his mailing lists were soon abuzz with activity and patches and happy chatter.
Many companies told him how they were saving millions using his work.
Some of them even paid him for consultancy and training.
He was invited to speak at conferences and started collecting badges with his name on them.
He started a small business, hired a friend to work with him, and dreamed of making it big.
Then one day, someone pointed him to a new project, GPL-licensed, which had forked his work and was improving on it.
He was irritated and upset, and asked how peoplefellow open sourcers, no less!—would so shamelessly steal his code.
There were long arguments on the list about whether it was even legal to relicense the BSD code as GPL.
He tried to ignore the new project, but then he soon realized that new patches coming from that project couldn’t even be merged back into his work!
Worse, the GPL project got popular and some of his core contributors made first small, and then larger patches to it.
Again, he couldn’t use those changes, and he felt abandoned.
Patrick went into a depression, his girlfriend left him for an international currency dealer called, weirdly, Patrice, and he stopped all work on the project.
He fired his friend, who took it rather badly and told everyone that Patrick was a closet banjo player.
Finally, Patrick took a job as a project manager for a cloud company, and by the age of 40, he had stopped programming even for fun.
Then I asked him, “Why didn’t you choose the GPL?” “Because it’s a restrictive viral license,” he replied.
You published your work inviting people to please steal your code as long as they kept this ‘please steal my code’ statement in the resulting work, and when people did exactly that, you got upset.
Seeing your hard work captured by a smarter team and then used against you is enormously painful, so why even make that possible? Every proprietary project that uses BSD code is capturing it.
A public GPL fork is perhaps more humiliating, but it’s fully self-inflicted.
It literally (and I mean that metaphorically) whispers “eat me” in the little voice one imagines a cube of cheese might use when it’s sitting next to an empty bottle of the best beer in the world, which is of course Orval, brewed by an ancient and almost extinct order of silent Belgian monks called Les Gars La-Bas Qui Brassents l’Orval.
The BSD license, like its near clone MIT/X11, was designed specifically by a university (Berkeley) with no profit motive, to leak work and effort.
It is a way to push subsidized technology at below its cost price, a dumping of underpriced code in the hope that it will break the market for others.
For us small businesses who aim our investments like precious bullets, leaking work and effort is unacceptable.
Breaking the market is great, but we cannot afford to subsidize our competitors.
The BSD networking stack ended up putting Windows on the Internet.
We cannot afford battles with those with whom we should naturally be allies.
We cannot afford to make fundamental business errors because in the end, that means we have to fire people.
Your goal as leader of a community is to motivate people to get out there and explore; to ensure they can do so safely and without disturbing others; to reward them when they make successful discoveries; and to ensure they share their knowledge with everyone else (and not because we ask them, not because they feel generous, but because it’s The Law)
You make a small product, at your own cost, but in public view.
If you have a small but real hit, the community then helps design and build the next version, and grows larger.
And then that community builds the next version, and so on.
It’s evident that you remain part of the community, maybe even a majority contributor, but the more control you try to assert over the material results, the less people will want to participate.
Plan your own retirement well before someone decides you are their next problem.
Then your work must be beautiful, immediately useful, and attractive.
Your contributors are users who want to explore just a little beyond where they are now.
The experience when people run or use your work should be an emotional one.
They should feel something, and if you’ve accurately solved even just one big problem that until then they didn’t quite realize they faced, you’ll have a small part of their soul.
It must also be easy to understand, use, and join.
Too many projects have barriers to access: put yourself in the other person’s mind and see all the reasons they come to your site, thinking “Um, interesting project, but...” and then leave.
You want them to stay, and try it, just once.
If you do these things well, your community will be smart, but more importantly, it will be intellectually and geographically diverse.
A group of likeminded experts cannot explore the problem landscape well.
Stranger, Meet Stranger How much up-front agreement do two people need to work together on something? In most organizations, a lot.
But you can bring this cost down to near zero, and then people can collaborate without having ever met, done a phone conference, or had a meeting or business trip to discuss Roles and Responsibilities over way too many bottles of cheap Korean rice wine.
You need well-written rules that are designed by cynical people like me to force strangers into mutually beneficial collaboration instead of conflict.
And then you want something like our C4 rulebook to control how work actually happens.
C4 (which I now use for every new open source project) has detailed and tested answers to a lot of common mistakes people make, such as the sin of working offline in a corner with others “because it’s faster.” Transparency is essential to get trust, which is essential to get scale.
By forcing every single change through a single transparent process, you build real trust in the results.
Another cardinal sin that many open source developers make is to place themselves above others.
Your job, as founder of a project, is not to impose your vision of the product on others, but to make sure the rules are good, honest, and enforced.
Infinite Property One of the saddest myths of the knowledge business is that ideas are a sensible form of property.
It’s medieval nonsense that should have been junked along with slavery, but sadly it’s still making too many powerful people too much money.
What does work sensibly as property is the hard work we do in building a market.
Whether it’s moral authority over a project, money from consulting, or the sale.
But what you really own is “footfall,” participants in your project, which ultimately defines your power.
Thankfully, GitHub solved this problem for us, for which I will die a grateful person (there are many reasons to be grateful in life, which I won’t list here because we only have a hundred or so pages left, but this is one of them)
You cannot scale a single project with many owners like you can scale a collection of many small projects, each with fewer owners.
When we embrace forks, a person can become an “owner” with a single click.
Now they just have to convince others to join by demonstrating their unique value.
First, simply because most people are too nice, we need some kind of symbolic leadership or owners who provide ultimate authority in case of conflict.
I’ve seen it work with self-elected groups of “elders,” but old men like to talk a lot.
One of the real benefits of free software is that it’s always remixable, so instead of fighting over a pie, one simply forks the pie.
I’ve seen communities of brilliant people with noble goals die because the founders got some or all of these four things wrong.
The core problem is that you can’t expect consistently great leadership from any one company, person, or group.
What works today often won’t work tomorrow, yet structures become more solid, not more flexible, over time.
The best answer I can find is a mix of two things.
The first is the GPL with its guarantee of remixability.
No matter how bad the authority is, no matter how much it tries to privatize and capture the community’s work, if it’s GPL licensed, that work can walk away and find a better authority.
Before you say, “all open source offers this,” think it through.
I can kill a BSD-licensed project by hiring the core contributors and not releasing any new patches.
But even with a billion dollars to spend, I cannot kill a GPLlicensed project.
The second is the philosophical anarchist model of authority, which is that we choose it, it does not own us.
You may get the feeling I’m a fan of Git and GitHub.
This would be accurate: these two tools have made such a positive impact on our work over the last years, especially when it comes to building community.
These can be peers in a network, or they can be strangers working in the same project.
I think C4 is the first time anyone has attempted to codify a community’s rulebook as a formal and reusable protocol spec.
Previously, our rules were spread out over several wiki pages, and they were quite specific to libzmq in many ways.
But experience teaches us that the more formal, accurate, and reusable the rules are, the easier it is for strangers to collaborate up-front.
At the time of C4, we also had some disagreement in the libzmq project over precisely what process we were using.
Let’s just say some people felt they had a special status, which created friction with the rest of the community.
It’s easy to use C4: just host your project on GitHub, get one other person to join, and open the floor to pull requests.
We’ve done this in quite a few projects and it does seem to work.
I’ve been pleasantly surprised a few times just applying these rules to my own work, like CZMQ.
None of us are so amazing that we can work without others.
Goals C4 is meant to provide a reusable optimal collaboration model for open source software projects.
To learn more about the best possible process, we need to get results from the widest possible range of projects.
It has these specific goals: To maximize the scale of the community around a project, by reducing the friction for new Contributors and creating a scaled participation model with strong positive feedbacks;
The number one goal is maximizing the size and health of the community—not technical quality, not profits, not performance, not market share.
The goal is simply increasing the number of people who contribute to the project.
The science here is simple: the larger the community, the more accurate the results.
To relieve dependencies on key individuals by separating different skill sets so that there is a larger pool of competence in any required domain;
Perhaps the worst problem we faced in libzmq was dependence on people who could understand the code, manage GitHub branches, and make clean releases—all at the same time.
It’s like looking for athletes who can run marathons and sprint, swim, and also lift weights.
Asking us to be really good at two contradictory things reduces the number of candidates sharply, which is a Bad Thing for any project.
We had this problem severely in libzmq in 2009 or so, and we fixed it by splitting the role of maintainer into two: one person makes patches and another makes releases.
To allow the project to develop faster and more accurately, by increasing the diversity of the decision making process;
The greater the diversity of the community and the number of people who can weigh in on discussions without fear of being criticized or dismissed, the faster and more accurately the software develops.
Going very fast in the wrong direction is not just useless, it’s actively damaging (and we suffered a lot of that in libzmq before we switched to C4)
To support the natural life-cycle of project versions from experimental through to stable, by allowing safe experimentation, rapid failure, and isolation of stable code;
To be honest, this goal seems to be fading into irrelevance.
It’s quite an interesting effect of the process: the Git master is almost always perfectly stable.
This has to do with the size of changes and their latency—i.e., the time between someone writing the code and someone actually using it fully.
However, people still expect “stable” releases, so we’ll keep this goal there for a while.
To reduce the internal complexity of project repositories, thus making it easier for Contributors to participate and reducing the scope for error;
Curious observation: people who thrive in complex situations like to create complexity because it keeps their value high.
Git made branches easy and left us with the all-too-common statement, “Git is easy once you understand that a Git branch is just a folded five-dimensional lepton space that has a detached history with no intervening cache.” Developers should not be made to feel stupid by their tools.
I’ve seen too many top-class developers confused by repository structures.
We’ll come back to dispose of Git branches shortly, dear reader.
To enforce collective ownership of the project, which increases economic incentive to Contributors and reduces the risk of hijack by hostile entities.
Preliminaries The project SHALL use the Git distributed revision control system.
Its command-line API is horribly inconsistent, and it has a complex, messy internal model that it shoves in your face at the slightest provocation.
But despite doing its best to make its users feel stupid, Git does its job really, really well.
The project SHALL be hosted on github.com or equivalent, herein called the “Platform”
I’m sure one day some large firm will buy GitHub and break it, and another platform will rise in its place.
GitHub serves up a near-perfect set of minimal, fast, simple tools.
I’ve thrown hundreds of people at it, and they all stick like flies in a dish of honey.
We made the mistake in libzmq of switching to Jira because we hadn’t learned yet how to properly use the GitHub issue tracker.
Jira is a great example of how to turn something useful into a complex mess because the business depends on selling more “features.” But even without criticizing Jira, keeping the issue tracker on the same platform means one less UI to learn, one less login, and smooth integration between issues and patches.
The project SHOULD have clearly documented guidelines for code style.
This is a protocol plug-in: insert code style guidelines here.
If you don’t document the code style you use, you have no basis except prejudice to reject patches.
A “Contributor” is a person who wishes to provide a patch, being a set of commits that solve some clearly identified problem.
A “Maintainer” is a person who merges patches to the project.
Maintainers are not developers; their job is to enforce process.
Now we move on to definitions of the parties, and the splitting of roles that saved us from the sin of structural dependency on rare individuals.
This worked well in libzmq, but as you will see it depends on the rest of the process.
C4 isn’t a buffet; you will need the whole process (or something very like it), or it won’t hold together.
Contributors SHALL NOT have commit access to the repository unless they are also Maintainers.
Everyone, without distinction or discrimination, SHALL have an equal right to become a Contributor under the terms of this contract.
It used to be that the libzmq maintainers would reject patches simply because they didn’t like them.
Now, that may sound reasonable to the author of a library (though libzmq was not written by any one person), but let’s remember our goal of creating a work that is owned by as many people as possible.
Saying “I don’t like your patch so I’m going to reject it” is equivalent to saying, “I claim to own this and I think I’m better than you, and I don’t trust you.” Those are toxic messages to give to others who are thinking of becoming your co-investors.
I think this fight between individual expertise and collective intelligence plays out in other areas.
It defined Wikipedia, and still does, a decade after that work surpassed anything built by small groups of experts.
For me, we make software by synthesizing knowledge, much as we make Wikipedia articles.
Licensing and Ownership The project SHALL use the GPLv3 or a variant thereof (LGPL, AGPL)
I’ve already explained how full remixability creates better scale and why the GPL and its variants seem the optimal contract for remixable software.
If you’re a large business aiming to dump code on the market, you won’t want C4, but then you won’t really care about community either.
All contributions to the project source code (“patches”) SHALL use the same license as the project.
This removes the need for any specific license or contribution agreement for patches.
You fork the GPL code, you publish your remixed version on GitHub, and you or anyone else can then submit that as a patch to the original code.
Any work that contains BSD code may also contain unlicensed proprietary code, so you need explicit action from the author of the code before you can remix it.
The project SHALL be owned collectively by all its Contributors.
This is perhaps redundant, but worth saying: if everyone owns their patches, then the resulting whole is also owned by every contributor.
There’s no legal concept of owning lines of code: the “work” is at least a source file.
Each Contributor SHALL be responsible for identifying themselves in the project Contributor list.
Patch Requirements In this section, we define the obligations of the contributor: specifically, what constitutes a “valid” patch, so that maintainers have rules they can use to accept or reject patches.
Maintainers and Contributors MUST have a Platform account and SHOULD use their real names or a well-known alias.
In the worst-case scenario, where someone has submitted toxic code (patented, or owned by someone else), we need to be able to trace who and when, so we can remove the code.
Asking for real names or a well-known alias is a theoretical strategy for reducing the risk of bogus patches.
We don’t know if this actually works because we haven’t had the problem yet.
A patch SHOULD be a minimal and accurate answer to exactly one identified and agreed problem.
This implements the Simplicity Oriented Design process that I’ll come to later in this chapter.
One clear problem, one minimal solution: apply, test, and repeat.
A patch MUST adhere to the code style guidelines of the project if these are defined.
I’ve spent time cleaning up other peoples’ patches because they insisted on putting the “else” beside the “if ” instead of just below, as Nature intended.
A patch MUST adhere to the “Evolution of Public Contracts” guidelines defined below.
One of the big changes we made with C4 was simply to ban, outright, this kind of sanctioned sabotage.
We just don’t allow the breaking of existing public contracts, period, unless everyone agrees, in which case no period.
A patch SHALL NOT include non-trivial code from other projects unless the Contributor is the original author of that code.
The first is that it forces people to make minimal solutions because they cannot simply import swathes of existing code.
In the cases where I’ve seen this happen to projects, it’s always bad unless the imported code is very cleanly separated.
You write the patch, you are allowed to publish it as LGPL, and we can merge it back in.
But you find a 200-line code fragment on the Web and try to paste that, and we’ll refuse.
A patch MUST compile cleanly on at least the most important target platforms.
This is probably asking a lot because most contributors have only one platform on which to work.
A “Correct Patch” is one that satisfies the above requirements.
Just in case it wasn’t clear, we’re back to legalese and definitions.
Development Process In this section, we aim to describe the actual development process, step-by-step.
Change on the project SHALL be governed by the pattern of accurately identifying problems and applying minimal, accurate solutions to these problems.
To initiate changes, a user SHALL log an issue on the project Platform issue tracker.
This is meant to stop us from going offline and working in a ghetto, either by ourselves or with others.
Although we tend to accept pull requests that have clear argumentation, this rule lets us say “stop” to confused or too-large patches.
The user SHOULD write the issue by describing the problem they face or observe.
Problem: user cannot do common tasks A or B except by using a complex workaround.
Because everyone I’ve ever worked with has needed to learn this, it seems worth restating: document the real problem first, and the solution second.
The user SHOULD seek consensus on the accuracy of their observation, and the value of solving the problem.
Because many apparent problems are illusionary, by stating the problem explicitly we give others a chance to correct our logic.
You’re only using A and B a lot because function C is unreliable.
Users SHALL NOT log feature requests, ideas, suggestions, or any solutions to problems that are not explicitly documented and provable.
There are several reasons for not logging ideas, suggestions, or feature requests.
In our experience, these just accumulate in the issue tracker until someone deletes them.
But more profoundly, when we treat all changes as problem solutions, we can prioritize trivially.
Either the problem is real and someone wants to solve it now, or it’s not on the table.
Thus, the release history of the project SHALL be a list of meaningful issues logged and solved.
To work on an issue, a Contributor SHALL fork the project repository and then work on their forked repository.
Here we explain the GitHub fork + pull request model so that newcomers only have to learn one process (C4) in order to contribute.
To submit a patch, a Contributor SHALL create a Platform pull request back to the project.
GitHub has made this so simple that we don’t need to learn Git commands to do it, for which I’m deeply grateful.
Sometimes, I’ll tell people whom I don’t particularly like that command-line Git is awesome and all they need to do is learn Git’s internal model in detail before trying to use it on real work.
A Contributor SHALL NOT commit changes directly to the project.
Anyone who submits a patch is a contributor, and all contributors follow the same rules.
No special privileges to the original authors, because otherwise we’re not building a community, only boosting our egos.
To discuss a patch, people MAY comment on the Platform pull request, on the commit, or elsewhere.
Randomly distributed discussions may be confusing if you’re walking up for the first time, but GitHub solves this for all current participants by sending emails to those who need to follow what’s going on.
We had the same experience and the same solution in Wikidot, and it works.
There’s no evidence that discussing in different places has any negative effect.
To accept or reject a patch, a Maintainer SHALL use the Platform interface.
Working via the GitHub web user interface means pull requests are logged as issues, with workflow and discussion.
There was a rule we defined in the FFII years ago to stop people burning out: no less than two people on any project.
One-person projects tend to end in tears, or at least bitter silence.
We have quite a lot of data on burnout, why it happens, and how to prevent it (even cure it)
I’ll explore this later in the chapter, because if you work with or on open source projects you need to be aware of the risks.
The “no merging your own patch” rule has two goals.
First, if you want your project to be C4-certified, you have to get at least one other person to help.
If no one wants to help you, perhaps you need to rethink your project.
Second, having a control for every patch makes it much more satisfying, keeps us more focused, and stops us breaking the rules because we’re in a hurry, or just feeling lazy.
We already said this, but it’s worth repeating: the role of maintainer is not to judge a patch’s substance, only its technical quality.
The substantive worth of a patch only emerges over time: people use it and like it, or they do not.
And if no one is using a patch, eventually it’ll annoy someone else, who will remove it, and no one will complain.
There is a criterion I call change latency, which is the round-trip time from identifying a problem to testing a solution.
If maintainers cannot respond to pull requests as rapidly as people expect, they’re not doing their jobs (or they need more hands)
The Contributor MAY tag an issue as “Ready” after making a pull request for the issue.
By default, GitHub offers the usual variety of issues, but with C4 we don’t use them.
Instead, we need just two labels, “Urgent” and “Ready.” A contributor who wants another user to test an issue can label it as “Ready.”
The user who created an issue SHOULD close the issue after checking the patch is successful.
When one person opens an issue and another works on it, it’s best to allow the original person to close the issue.
That acts as a double-check that the issue was properly resolved.
Maintainers SHOULD ask for improvements to incorrect patches and SHOULD reject incorrect patches if the Contributor does not respond constructively.
Initially, I felt it was worth merging all patches, no matter how poor.
There’s an element of trolling involved: accepting even obviously bogus patches could, I felt, pull in more contributors.
But people were uncomfortable with this, so we defined the “correct patch” rules, and the maintainer’s role in checking for quality.
Any Contributor who has value judgments on a correct patch SHOULD express these via their own patches.
In essence, the goal here is to allow users to try patches rather than to spend time arguing pros and cons.
As easy as it is to make a patch, it’s as easy to revert it with another patch.
You might think this would lead to “patch wars,” but that hasn’t happened.
We’ve had only a handful of cases in libzmq where patches by one contributor were killed by another person who felt the experimentation wasn’t going in the right direction.
Maintainers MAY commit changes to non-source documentation directly to the project.
This exit allows maintainers who are making release notes to push those without having to create an issue, which would then affect the release notes, leading to stress on the space-time fabric and possibly involuntary rerouting backwards in the fourth dimension to before the invention of cold beer.
It is simpler to agree that release notes aren’t changes to the software.
Creating Stable Releases We want some guarantee of stability for a production system.
In the past, this meant taking unstable code and then over months hammering out the bugs and faults until it was safe to trust.
Since we went full speed with C4, we’ve found that the Git master of libzmq is mostly perfect, most of the time.
This frees our time to do more interesting things, such as building new open source layers on top of libzmq.
However, people still want that guarantee: many users will simply not install except from an “official” release.
So, a stable release today involves two things: first, a snapshot of the master taken at a time when there have been no new changes for a while and there are no dramatic open bugs; and second, a way to fine-tune that snapshot to fix the critical issues remaining in it.
The project SHALL have one branch (“master”) that always holds the latest in-progress version and SHOULD always build.
This is redundant because every patch always builds, but it’s worth restating.
If the master doesn’t build (and pass its tests), someone needs waking up.
The project SHALL NOT use topic branches for any reason.
In short (or “tl;dr,” as they say on the webs), branches make the repository too complex and fragile, and they require up-front agreement, all of which are expensive and avoidable.
To make a stable release someone SHALL fork the repository by copying it and thus become maintainer of this repository.
Forking a project for stabilization MAY be done unilaterally and without agreement of project maintainers.
If you think the maintainers aren’t producing stable releases right, fork the repository and do it yourself.
Forking isn’t a failure, it’s an essential tool for competition.
You can’t do this with branches, which means a branch-based release policy gives the project maintainers a monopoly.
And that’s bad because they’ll become lazier and more arrogant than if real competition is nipping at their heels.
Maintainers of the stabilization project SHALL maintain it through pull requests which MAY cherry-pick patches from the forked project.
Perhaps the C4 process should just say that stabilization projects have maintainers and contributors, like any project.
A patch to a repository declared “stable” SHALL be accompanied by a reproducible test case.
New code does not require the same paranoia as code that people are trusting for production use.
In the normal development process, we did not mention test cases.
While I love testable patches, many changes aren’t easily (or at all) testable.
However, to stabilize a code base you want to fix serious bugs, and you want to be 100% sure every change is accurate.
A stabilization repository SHOULD progress through these phases: “unstable”, “candidate”, “stable”, and then “legacy”
That is, the default behavior of stabilization repositories is to die.
The key point here is that these forked stabilization repositories all die in the end, as the master continues to evolve and be forked off for production releases.
Up until the end of 2011, libzmq’s naturally happy state was marred by broken promises and broken contracts.
We stopped making promises (aka “road maps”) for libzmq completely, and our dominant theory of change is now that it emerges carefully and accurately over time.
At a 2012 Chicago meetup, Garrett Smith and Chuck Remes called this the “drunken stumble to greatness,” which is how I think of it now.
You’d think this was a given for professional software engineers, but no, it’s not.
You want C4 certification for your project, you make sure your public contracts are documented.
Yes, I intend at some point to create a C4 certification process to act as a quality indicator for open source projects.
All Public Contracts SHOULD have space for extensibility and experimentation.
Now, the real thing is that public contracts do change.
It’s not about not changing them; it’s about changing them safely.
This means educating (especially protocol) designers to create that space up-front.
A patch that modifies a Public Contract SHOULD not break existing applications unless there is prior consensus on the value of doing this.
A patch that introduces new features to a Public Contract SHOULD do so using new names.
Old names SHOULD be deprecated in a systematic fashion by marking new names as “experimental” until they are stable, then marking the old names as “deprecated”
This life-cycle notation has the great benefit of actually telling users what is going on, with a consistent direction.
When sufficient time has passed, old deprecated names SHOULD be marked “legacy” and eventually removed.
In theory, this gives applications time to move on to stable new contracts without risk.
You can upgrade first, make sure things work, and then, over time, fix things up to remove dependencies on deprecated and legacy APIs and protocols.
When old names are removed, their implementations MUST provoke an exception (assertion) if used by applications.
I’ve not tested this rule to be certain it makes sense.
Perhaps what it means is “if you can’t provoke a compile error because the API is dynamic, provoke an assertion.”
The process for changing it (Digistan’s COSS) is a little outdated now: it relies on a single-editor workflow with the ability to fork, but not merge.
A Real-Life Example In the “XPUB subscription notifications” email thread, Dan Goes asks how to make a publisher that knows when a new client subscribes and sends out previous matching messages.
It’s a standard pub-sub technique called “last value caching.” Over a one-way transport like pgm (where subscribers literally send no packets back to publishers), this can’t be done.
But over TCP, it can, if we use an XPUB socket and if that socket didn’t cleverly filter out duplicate subscriptions to reduce upstream traffic.
Though I’m not an expert contributor to libzmq, this seemed like a fun problem to solve.
How hard could it be? I started by forking the libzmq repository to my own GitHub account, and then cloned it to my laptop, where I built it:
Because the libzmq code is neat and well organized, it was quite easy to find the main files to change (xpub.cpp and xpub.hpp)
Each socket type has its own source file and class.
They inherit from socket_base.cpp, which has this hook for socket-specific options:
Then I checked where the XPUB socket filters out duplicate subscriptions, in its xread_activated() method:
The code seems obvious, except that “subscription” also includes unsubscription, which confused me for a few seconds.
If there’s anything else weird in the rm and add methods, that’s a separate issue to fix later.
Now it was time to make an issue for this change.
Jira kindly offered me the traditional choice between “bug” and “new feature,” and I spent 30 seconds wondering where this counterproductive historical distinction came from.
Putting such thoughts aside, I created an issue, #443, and described the problem and a plausible solution:
The API sits in include/zmq.h, so this is where I added the option name.
When you invent a concept in an API or anywhere, please take a moment to choose a name that is explicit and short and obvious.
Don’t fall back on generic names that require additional context to understand.
You have one chance to tell the reader what your concept is and does.
It technically kind of aims in the right direction, but it’s miserably long and obscure.
I chose ZMQ_XPUB_VERBOSE: short and explicit, and clearly an on/off switch, with “off ” being the default setting.
Next, it was time to add a private property to the xpub class definition in xpub.hpp: // If true, send all subscription messages upstream, not just // unique ones bool verbose;
Finally, I changed the xread_activated() method to use this new option, and while I was at it, I made that test on socket type more explicit too:
This made me a little suspicious, but being lazy and jetlagged, I didn’t immediately make a test case to actually try out the change.
In the worst case, I added a patch that wasn’t really useful.
I didn’t implement a matching zmq_getsockopt() method, because “minimal” means what it says.
There’s no obvious use case for getting the value of an option that you presumably just set in code.
Symmetry isn’t a valid reason to double the size of a patch.
I did have to document the new option, because the process says, “All Public Contracts SHOULD be documented.”
Committing the code, I pushed the patch to my forked repository (the “origin”): Git commit -a -m "Fixed issue #443" Git push origin master.
Switching to the GitHub web interface, I went to my libzmq fork and pressed the big “Pull Request” button at the top.
GitHub asked me for a title, so I entered “Added ZMQ_XPUB_VERBOSE option.” I’m not sure why it asks this as I made a neat commit message, but hey, let’s go with the flow here.
GitHub lets you continue to make commits after you’ve kicked off a pull request.
That simplifies things, but the maintainer may refuse the whole bundle based on one patch that doesn’t look valid.
Because Dan was waiting (at least in my highly optimistic imagination) for this fix, I then went back to the zeromq-dev list and told him I’d made the patch, with a link to the commit.
You learn to count time zones when you work with people across the world.
Ian was in a conference, Mikko was getting on a plane, and Chuck was probably in the office, but three hours later, Ian merged the pull request.
After Ian merged the pull request, I resynchronized my fork with the upstream libzmq repository.
First, I added a “remote” that tells Git where this repository sits (just once, in the directory where I’m working):
Then I pulled changes back from the upstream master and checked the Git log to verify: Git pull --rebase upstream master Git log.
And that is pretty much it, in terms of how much Git one needs to learn and use to contribute patches to libzmq.
Almost all projects that use Git use branches, and the selection of the “best” branching strategy is like a rite of passage for an open source project.
It has base branches (master, develop), feature branches, release branches, hotfix branches, and support branches.
Many teams have adopted Git-flow, which even has Git extensions to support it.
I’m a great believer in popular wisdom, but sometimes you have to recognize mass delusion for what it is.
Here is a section of C4 that might have shocked you when you first read it:
The project SHALL NOT use topic branches for any reason.
To be clear, it’s public branches in shared repositories that I’m talking about.
Using branches for private work, such as to work on different issues, appears to work well enough, though it’s more complexity than I personally enjoy.
To channel Stallman again: “Your freedom to create complexity ends one inch from our shared workspace.”
More profoundly, and perhaps this is why the majority seems to be “wrong”: I think the “branches versus forks” argument is really a deeper “design versus evolve” argument about how to make software optimally.
For now, I’ll try to be scientific about my irrational hatred of branches by looking at a number of criteria and comparing branches and forks in each one.
There is no inherent reason why branches are more complex than forks.
However, Gitflow uses five types of branch, whereas C4 uses two types of fork (development and stable) and one branch (master)
Circumstantial evidence thus indicates that branches lead to more complexity than forks.
For new users, it is definitely—and we’ve measured this in practice—easier to learn to work with many repositories and no branches except master.
Change Latency The smaller and more rapid the delivery, the better.
Development branches seem to correlate strongly with large, slow, risky deliveries.
It’s certainly not how C4 works, which is by focusing tightly on individual problems and their minimal solutions.
Forks have a different outcome: it’s up to the forker to ensure that his changes merge cleanly, and to keep them simple so they won’t be rejected.
Evidence definitely shows that learning to use Git branches is complex.
For most developers, every cycle spent learning Git is a cycle lost on more productive things.
I’ve been told several times, by different people that I do not like branches because I “never properly learned Git.” That is fair, but it is a criticism of the tool, not the human.
Cost of Failure The lower the cost of failure, the better.
Branches demand more perfection from developers, since mistakes potentially affect others.
Forks make failure extremely cheap because literally nothing that happens in a fork can affect others not using that fork.
Up-Front Coordination The less need for up-front coordination, the better.
Scalability The more you can scale a project, the better.
People expect branches and find forks to be uncommon and thus confusing.
If you use branches, a single patch will have the same commit hash tag, whereas across forks the patch will have different hash tags.
That makes it harder to track patches as they cross forks, true.
But seriously, having to track hexadecimal hash tags is not a feature.
Sometimes better ways of working are just surprising at first.
Economics of Participation The more tangible the rewards, the better.
People like to own their work and get credit for it.
Forks create more competition, in a healthy way, while branches suppress competition and force people to collaborate and share credit.
This may sound positive, but in my experience it demotivates people.
A branch isn’t a product you can “own,” whereas a fork can be.
Robustness in Conflict The more a model can survive conflict, the better.
Like it or not, people fight over ego, status, beliefs, and theories of the world.
If your organizational model depends on agreement, you won’t survive the first real fight.
Branches do not survive real arguments and fights, whereas forks can be hostile and still benefit all parties.
Guarantees of Isolation The stronger the isolation between production code and experiment, the better.
I’ve seen experimental code pushed to mainline production by error.
But the real fault is in allowing two entirely separate generations of product to exist in the same protected space.
If you can push to random-branch-x, you can push to master.
Branches sit there until someone remembers to work on them.
When we look for self-organization, the more visible and declarative the problems, the faster and more accurately we can work.
Conclusions In this section, I’ve listed a series of arguments, most of which came from fellow team members.
Here’s how it seems to break down: Git veterans insist that branches are the way to work, whereas newcomers tend to feel intimidated when asked to navigate Git branches.
What we’ve discovered, accidentally, is that when you stop using branches at all, Git becomes trivial to use.
It literally comes down to six commands (clone, remote, commit, log, push, and pull)
Furthermore, a branchfree process actually works; we’ve used it for a couple of years now with no visible downside except surprise to the veterans, and growth of “single” projects over multiple repositories.
If you can’t use forks, perhaps because your firm doesn’t trust GitHub’s private repositories, then you can perhaps use topic branches, one per issue.
However, you’ll still suffer the costs of getting up-front consensus, low competitiveness, and risk of human error.
Designing for Innovation Let’s look at innovation, which Wikipedia defines as, “the development of new values through solutions that meet new requirements, inarticulate needs, or old customer and market needs in value adding new ways.” This really just means solving problems more cheaply.
It sounds straight-forward, but the history of collapsed tech giants proves that it’s not.
I’ll try to explain how teams so often get it wrong, and suggest a way for doing innovation right.
The Tale of Two Bridges Two old engineers were talking of their lives and boasting of their greatest projects.
One of the engineers explained how he had designed one of the greatest bridges ever made.
We spent two years studying the land and choosing designs and materials.
We hired the best engineers and spent another five years designing the bridge.
We contracted the largest engineering firms to build the structures, the towers, the tollbooths, and the roads that would connect the bridge to the main highways.
Under the road level we had trains, and a special path for cyclists.
At first, people pulled packages across that rope with a pulley and string.
Then someone threw a second rope, and built a foot walk.
A group of men then rebuilt that, made it solid, and women started to cross, every day, with their produce.
A market grew up on one side of the bridge, and slowly that became a large town, since there was a lot of space for houses.
The rope bridge got replaced with a wooden bridge, to allow horses and carts to cross.
Then the town built a real stone bridge, with metal beams.
Funny thing,” he said, “my bridge was demolished about 10 years after we built it.
Turns out it was built in the wrong place and no one wanted to use it.
As we rolled out releases, we hit the problem that it’s very easy to promise stuff, and rather harder to make it as planned.
For one thing, much of the work was voluntary, and it’s not clear how you force volunteers to commit to a road map.
So we were making promises we could not keep, and the real deliveries didn’t match the road maps.
The second problem was that by defining the road map, we in effect claimed territory, making it harder for others to participate.
People do prefer to contribute to changes they believe were their idea.
Writing down a list of things to do turns contribution into a chore rather than an opportunity.
In the dominant theory of innovation, brilliant individuals reflect on large problem sets and then carefully and precisely create a solution.
Sometimes they have “eureka” moments where they “get” brilliantly simple answers to whole large problem sets.
The inventor and the process of invention are rare, precious, and can command a monopoly.
Looking more closely, however, you will see that the facts don’t match.
History doesn’t show lone inventors: it shows lucky people who steal or claim ownership of ideas that are being worked on by many.
It shows brilliant people striking lucky once, and then spending decades on fruitless and pointless quests.
The best known large-scale inventors, like Thomas Edison, were in fact just very good at managing systematic broad research done by large teams.
It’s like claiming that Steve Jobs invented every device made by Apple.
It is a nice myth, good for marketing, but utterly useless as practical science.
Recent history, much better documented and less easy to manipulate, shows this well.
The Internet is surely one of the most innovative and fast-moving areas of technology, and one of the best documented.
Instead, it has a massive economy of people who have carefully and progressively solved a long series of immediate problems, documented their answers, and made those available to all.
The innovative nature of the Internet comes not from a small, select band of Einsteins.
It comes from RFCs anyone can use and improve, made by hundreds or thousands of smart, but not uniquely smart, individuals.
It comes from open source software anyone can use and improve.
It comes from sharing, scale of community, and the continuous accretion of good solutions and disposal of bad ones.
We can only accurately perceive problems to which we are close.
We can rank the cost/benefit economics of problems using a market for solutions.
Our intelligence can make this process faster, but does not replace it.
Smarter people may work faster, but they may work in the wrong direction.
It’s the collective vision of reality that keeps us honest and relevant.
We don’t need road maps if we have a good process.
Functionality will emerge and evolve over time as solutions compete for market share.
It’s just an information processing machine that likes to polish its own ego and collect karma.
We can neither collect problems nor measure solutions without other people.
The size and diversity of the community is a key factor.
Larger, more diverse communities collect more relevant problems, solve them more accurately, and do this faster than a small expert group.
So, when we trust the solitary experts, we see classic errors: focus on ideas, not problems; focus on the wrong problems; misjudgments about the value of solving problems; not using one’s own work; and many other misjudgments of the real market.
To best understand how we ended up with SOD, let’s look at the alternatives.
Trash-Oriented Design The most popular design process in large businesses seems to be Trash-Oriented Design, or TOD.
It’s tenacious nonsense, but a powerful crutch for people who lack imagination.
The theory goes that ideas are rare, so the trick is to capture them.
It’s like nonmusicians being awed by a guitar player, not realizing that great talent is so cheap it literally plays on the streets for coins.
The main output of TOD is expensive “ideation”: concepts, design documents, and products that go straight into the trash can.
The Creative People come up with long lists of “we could do X and Y.” I’ve seen endlessly detailed lists of all the amazing things a product could do.
Once the creative work of idea generation has happened, it’s just a matter of execution, of course.
So, the managers and their consultants pass their brilliant ideas to designers, who create acres of preciously refined design documents.
The designers take the tens of ideas the managers came up with, and turn them into hundreds of world-changing designs.
These designs get given to engineers, who scratch their heads and wonder who the heck came up with such nonsense.
They start to argue back, but the designs come from up high, and really, it’s not up to engineers to argue with creative people and expensive consultants.
So the engineers creep back to their cubicles, humiliated and threatened into building the gigantic but oh-so-elegant junk heap.
It is bone-breaking work because the designs take no account of practical costs.
As the project gets delayed, the managers bully the engineers into giving up their evenings and weekends.
Eventually, something resembling a working product makes it out of the door.
The designers curse the engineers for their incompetence and pay more consultants to put lipstick onto the pig, and slowly the product starts to look a little nicer.
By this time, the managers have started to try to sell the product and they find, shockingly, that no one wants it.
Undaunted, they courageously build million-dollar websites and ad campaigns to explain to the public why they absolutely need this product.
They do deals with other businesses to force the product on the lazy, stupid, and ungrateful market.
After 12 months of intense marketing, the product still isn’t making profits.
Worse, it suffers dramatic failures and gets branded in the press as a disaster.
Hundreds of millions of dollars end up in the trash.
Meanwhile, another visionary manager somewhere in the organization drinks a little too much tequila with some marketing people and has a Brilliant Idea.
Trash-Oriented Design would be a caricature if it wasn’t so common.
The main lessons of TOD are quite straightforward, but hard to swallow.
Anyone who tries to start a discussion with “Oooh, we can do this too!” should be beaten down with all the passion one reserves for traveling evangelists.
It is like sitting in a cafe at the foot of a mountain, drinking a hot chocolate and telling others, “Hey, I have a great idea, we can climb that mountain! And build a chalet on top! With two saunas! And a garden! Hey, and we can make it solar powered! Dude, that’s awesome! What color should we paint it? Green! No, blue! OK, go and make it, I’ll stay here and make spreadsheets and graphics!”
The starting point for a good design process is to collect real problems that confront real people.
The second step is to evaluate these problems with the basic question, “How much is it worth to solve this problem?” Having done that, we can collect the set of problems that are worth solving.
Their success will depend on how good and cheap the solution is, and how important the problem is (and sadly, how big the marketing budgets are)
But their success will also depend on how much they demand in effort to use—in other words, how simple they are.
Now, having slain the dragon of utter irrelevance, we attack the demon of complexity.
But the vast majority of products still end up being too complex and less successful than they might be.
Management correctly identifies some interesting and difficult problem with economic value.
In doing so, they already leapfrog over any TOD team.
The team, with enthusiasm, starts to build prototypes and core layers.
These work as designed, and thus encouraged, the team goes off into intense design and architecture discussions, coming up with elegant schemas that look beautiful and solid.
Management comes back and challenges the team with yet more difficult problems.
We tend to equate cost with value, so the harder and more expensive to solve the problem is, the more the solution should be worth, in their minds.
The team, being engineers and thus loving to build stuff, build stuff.
They build and build and build and end up with massive, perfectly designed complexity.
The products go to market, and the market scratches its head and asks, “Seriously, is this the best you can do?” People do use the products, especially if they aren’t spending their own money in climbing the learning curve.
Management gets positive feedback from its larger customers, who share the same idea that high cost (in training and use) means high value, and so continues to push the process.
Meanwhile somewhere across the world, a small team is solving the same problem using a better process, and a year later smashes the market to little pieces.
Much open source software is the output of COD processes.
It is insanely hard for engineers to stop extending a design to cover more potential problems.
They argue, “What if someone wants to do X?” but never ask themselves, “What is the real value of solving X?”
A good example of COD in practice is Bluetooth, a complex, over-designed set of protocols that users hate.
It continues to exist only because in a massively patented industry there are no real alternatives.
Bluetooth is perfectly secure, which is close to pointless for a proximity protocol.
At the same time, it lacks a standard API for developers, meaning it’s really costly to use Bluetooth in applications.
They add more and more features, utterly misreading the economics of their work.
The main lessons of COD are also simple, but hard for experts to swallow.
Making stuff that you don’t immediately have a need for is pointless.
It doesn’t matter how talented or brilliant you are, if you just sit down and make stuff people are not actually asking for, you are most likely wasting your time.
Ironically, solving the simpler problems often has more value to more people than solving the really hard ones.
If you allow engineers to just work on random things, they’ll mostly focus on the most interesting but least worthwhile things.
Engineers and designers love making stuff and decoration, and this inevitably leads to complexity.
It is crucial to have a “stop mechanism”: a way to set short, hard deadlines that force people to make smaller, simpler answers to just the most crucial problems.
This process starts with a realization: we do not know what we have to make until after we start making it.
Coming up with ideas or large-scale designs isn’t just wasteful, it’s a direct hindrance to designing the truly accurate solutions.
The really juicy problems are hidden like far valleys, and any activity except active scouting creates a fog that hides those distant valleys.
You need to keep mobile, pack light, and move fast.
We collect a set of interesting problems (by looking at how people use technology or other products) and we line these up from simple to complex, looking for and identifying patterns of use.
We take the simplest, most dramatic problem and we solve this with a minimal plausible solution, or “patch.” Each patch solves exactly a genuine and agreed-upon problem in a brutally minimal fashion.
We apply one measure of quality to patches, namely, “Can this be done any more simply while still solving the stated problem?” We can measure complexity in terms of concepts and models that the user has to learn or guess in order to use the patch.
A perfect patch solves a problem with zero learning required by the user.
Our product development consists of a patch that solves the problem “we need a proof of concept” and then evolves in an unbroken line to a mature series of products, through hundreds or thousands of patches piled on top of each other.
We do not do anything that is not a patch.
We enforce this rule with formal processes that demand that every activity or task is tied to a genuine and agreed-upon problem, explicitly enunciated and documented.
We build our projects into a supply chain where each project can provide problems to its “suppliers” and receive patches in return.
The supply chain creates the “stop mechanism” because when people are impatiently waiting for an answer, we necessarily cut our work short.
Individuals are free to work on any projects and provide patches at any place they feel it’s worthwhile.
No individuals “own” any project, except to enforce the formal processes.
A single project can have many variations, each a collection of different, competing patches.
Projects export formal and documented interfaces so that upstream (client) projects are unaware of changes happening in supplier projects.
Thus, multiple supplier projects can compete for client projects, in effect creating a free and competitive market.
We tie our supply chain to real users and external clients, and we drive the whole process by rapid cycles so that a problem received from outside users can be analyzed, evaluated, and solved with a patch in a few hours.
At every moment, from the very first patch, our product is shippable.
This is essential, because a large proportion of patches will be wrong (10–30%), and only by giving the product to users can we know which patches have become problems that need solving.
You don’t need to be a genius to use SOD successfully, you just need to be able to see the difference between the fog of activity and the progress toward new real problems.
People have pointed out that hill-climbing algorithms have known limitations.
But this is nonetheless how life itself works: collecting tiny incremental improvements over long periods of time.
We reduce the risk of local peaks by spreading out widely across the landscape, but it is somewhat moot.
The theory says, this is how innovation really works, so it’s better to embrace it and work with it than to try to work on the basis of magical thinking.
And in fact, once you see all innovation as more or less successful hill-climbing, you realize why some teams and companies and products get stuck in a never-never land of diminishing prospects.
They simply don’t have the diversity and collective intelligence to find better hills to climb.
When Nokia killed its open source projects, it cut its own throat.
A really good designer with a good team can use SOD to build world-class products, rapidly and accurately.
To get the most out of SOD, the designer has to use the product continuously from day one, and develop his or her ability to smell out problems such as inconsistency, surprising behavior, and other forms of friction.
We naturally overlook many annoyances, but a good designer picks these up and thinks about how to patch them.
Design is about removing friction in the use of a product.
In an open source setting, we do this work in public.
Projects that do this are in my view missing the point of open source, which is to engage your users in your exploration, and to build community around the seed of the architecture.
However, not all projects will be so lucky, and if you work with or in open source, you should understand the risk of burnout that volunteers face.
In this section, I’ll explain what causes burnout, how to recognize it, how to prevent it, and (if it happens) how to try to treat it.
Disclaimer: I’m not a psychiatrist and this section is based on my own experiences of working in pro bono contexts for the last 20 years, including on free software projects and NGOs such as the FFII.
In a pro bono context, we’re expected to work without direct or obvious economic incentive.
That is, we sacrifice family life, professional advancement, free time, and health in order to accomplish some goal we have decided to accomplish.
In any project, we need some kind of reward to make it worth continuing each day.
Mostly, we do things because people say, “Hey, great!” Karma is a powerful motivator.
At a certain stage, it seems our subconscious simply gets disgusted and says, “Enough is enough!” and refuses to go any further.
If we try to force ourselves, we can literally get sick.
This is what I call “burnout,” though the term is also used for other kinds of exhaustion.
Too much investment in a project with too little economic reward, for too long.
We are great at manipulating ourselves and others, and this is often part of the process that leads to burnout.
We tell ourselves that it’s for a good cause and that the other guy is doing OK, so we should be able to as well.
When I got burned out on open source projects like Xitami, I remember clearly how I felt.
I simply stopped working on the project, refused to answer any more emails, and told people to forget about it.
Have the victims worked a lot on a project that was not paying back in any way? Did they make exceptional sacrifices? Did they lose or abandon their jobs or studies to work on the project? If you’re answering “yes,” it’s burnout.
There are three simple tenets I’ve developed over the years to reduce the risk of burnout in the teams I work with:
Working solo on a critical or popular project—the concentration of responsibility on one person who cannot set her own limits—is probably the main factor in burnout.
It’s a management truism: if someone in your organization is irreplaceable, get rid of him or her.
Getting money from somewhere else makes it much easier to sustain a sacrificial project.
This should be a basic course in colleges and universities, as pro bono work becomes a more common way for young people to experiment professionally.
When a person is working alone on a critical project, you know he is going blow his fuse sooner or later.
It’s actually fairly predictable: it will happen in something like 18–36 months, depending on the individuals and how much economic stress they face in their private lives.
I’ve not seen anyone burn out after half a year, nor last five years working on an unrewarding project.
There is a simple cure for burnout that works in at least some cases: get paid decently for your work.
However, this pretty much destroys the freedom of movement (across that infinite problem landscape) that the volunteer enjoys.
Patterns for Success I’ll end this code-free chapter with a series of patterns for success in software engineering.
They aim to capture the essence of what divides glorious success from tragic failure.
They were described as “religious maniacal dogma” by a manager, and “anything else would be effing insane” by a colleague, in a single day.
But treat the Lazy Perfectionist and others as tools to use, sharpen, and throw away if something better comes along.
The Lazy Perfectionist Never design anything that’s not a precise, minimal answer to a problem we can identify and have to solve.
The Lazy Perfectionist spends his idle time observing others and identifying problems that are worth solving.
He looks for agreement on those problems, always asking, “What is the real problem?” Then he moves, precisely and minimally, to build (or get others to build) a usable answer to one problem.
And he repeats this until there are no problems left to solve, or time or money runs out.
The Benevolent Tyrant The control of a large force is the same principle as the control of a few men: it is merely a question of dividing up their numbers.
The Benevolent Tyrant divides large problems into smaller ones and throws them at groups to focus on.
He brokers contracts between these groups, in the form of APIs and the “unprotocols” we’ll read about in the next chapter.
The Benevolent Tyrant constructs a supply chain that starts with problems and results in usable solutions.
He is ruthless about how the supply chain works, but does not tell people what to work on, or how to do their work.
The Earth and Sky The ideal team consists of two sides: one writing code, and one providing feedback.
The Earth and Sky work together as a whole, in close proximity, but they communicate formally through issue tracking.
Sky seeks out problems from others and from personal use of the product and feeds these to Earth.
Earth and Sky can work through dozens of issues in a day.
Sky talks to other users, and Earth talks to other developers.
Earth and Sky may be two people, or two small groups.
The Open Door The accuracy of knowledge comes from diversity.
She does not argue quality or direction, instead allowing others to argue that and to get more engaged.
She calculates that even a troll will bring more diverse opinions to the group.
She lets the group form its opinion about what goes into stable code, and she enforces this opinion with the help of a Benevolent Tyrant.
The Laughing Clown, often acting as the Happy Failure, makes no claim to high competence.
Instead, his antics and bumbling attempts provoke others into rescuing him from his own tragedy.
Somehow, however, he always identifies the right problems to solve.
People are so busy proving him wrong, they don’t realize they’re doing valuable work.
The Mindful General operates in unknown territory, solving problems that are hidden until they are nearby.
Thus he makes no plans, but seeks opportunities, then exploits them rapidly and accurately.
He develops tactics and strategies in the field, and teaches these to his men so they can move independently, and together.
The Social Engineer If you know the enemy and know yourself, you need not fear the result of a hundred battles.
The Social Engineer reads the hearts and minds of those she works with and for.
She asks of everyone, “What makes this person angry, insecure, argumentative, calm, happy?” She studies their moods and dispositions.
With this knowledge she can encourage those who are useful, and discourage those who are not.
The Constant Gardener He will win whose army is animated by the same spirit throughout all its ranks.
The Constant Gardener grows a process from a small seed, step-by-step, as more people come into the project.
He makes every change for a precise reason, with agreement from.
He never imposes a process from above, but lets others come to consensus, and then he enforces that consensus.
In this way, everyone owns the process together, and by owning it, they are attached to it.
The Rolling Stone After crossing a river, you should get far away from it.
She accepts that all that we make is destined for the trash can; it is just a matter of time.
With precise, minimal investments, she can move rapidly away from the past and stay focused on the present and near future.
Above all, she has no ego and no pride to be hurt by the actions of others.
The Pirate Gang Code, like all knowledge, works best as collective—not private—property.
It accepts authority insofar as authority provides goals and resources.
The Pirate Gang owns and shares all it makes: every work is fully remixable by others in the Pirate Gang.
The gang moves rapidly as new problems emerge, and it is quick to abandon old solutions if those stop being relevant.
No persons or groups can monopolize any part of the supply chain.
The Flash Mob Water shapes its course according to the nature of the ground over which it flows.
The Flash Mob comes together in space and time as needed, then disperses as soon as it can.
Physical closeness is essential for high-bandwidth communications, but over time it creates technical ghettos, where Earth gets separated from Sky.
The Flash Mob tends to collect a lot of frequent flier miles.
The Canary Watcher measures the quality of an organization by his own pain level, and the observed pain levels of those with whom he works.
He brings new participants into existing organizations so they can express the raw pain of the innocent.
He may use alcohol to get others to verbalize their pain points.
He asks others, and himself, “Are you happy in this process, and if not, why not?” When an organization causes pain in himself or others, he treats that as a problem to be fixed.
The Hangman Never interrupt others when they are making mistakes.
The Hangman knows that we learn only by making mistakes, and he gives others copious rope with which to learn.
A little tug to remind the other of her precarious position.
Allowing others to learn by failure gives the good reason to stay, and the bad an excuse to leave.
The Hangman is endlessly patient, because there is no shortcut to the learning process.
The Historian Keeping the public record may be tedious, but it’s the only way to prevent collusion.
The Historian forces discussion into the public view, to prevent collusion to own areas of work.
The Pirate Gang depends on full and equal communications that do not depend on momentary presence.
No one really reads the archives, but the simple possibility stops most abuses.
The Historian encourages the right tool for the job: email for transient discussions, IRC for chatter, wikis for knowledge, issue tracking for recording opportunities.
The Provocateur When a man knows he is to be hanged in a fortnight, it concentrates his mind wonderfully.
Teams work best when they don’t have time for the crap.
An external enemy can move a passive team into action.
But she gently reminds the team of the stakes: fail, and we all look for other jobs.
The Mystic When people argue or complain, just write them a Sun Tzu quotation.
He knows that to argue with an emotional person only creates more emotion.
It’s hard to be angry at a Chinese general, especially when he has been dead for 2,400 years.
The Mystic plays Hangman when people insist on the right to get it wrong.
Partially it’s about slowing down, and partially it’s about ensuring that when you move fast, you go—and this is essential, dear reader—in the right direction.
It’s my standard interview riddle: what’s the rarest property of any software system, the absolute hardest thing to get right, the lack of which causes the slow or fast death of the vast majority of projects? The answer is not code quality, funding, performance, or even (though it’s a close answer) popularity.
Accuracy is half the challenge, and that applies to any engineering work.
The other half is specific distributed computing itself, which sets up a whole range of problems that we need to solve if we are going to create large architectures.
We need to encode and decode data, and we need to define protocols to connect clients and servers; we need to secure these protocols against attackers, and we need to make stacks that are robust.
This chapter will tackle these challenges, starting with a basic reappraisal of how to design and build software and ending with a fully formed example of a distributed application for large-scale file distribution.
If you’ve been reading the book carefully, you’ll have seen MOPED in action already.
The development of the Majordomo pattern in Chapter 4 is a near-perfect case.
The goal of MOPED is to define a process by which we can take a rough use case for a new distributed application, and go from “Hello World” to fully working prototype in any language in under a week.
Step 2: Draw a Rough Architecture From my experience, it’s essential to be able to draw the core of your architecture.
This helps others understand what you are thinking, but it also helps you think through your ideas.
There is really no better way to explain your ideas to your colleagues than using a whiteboard.
You don’t need to get it right, and you don’t need to make it complete.
What you do need to do is break your architecture into pieces that make sense.
The nice thing about software architecture (as compared to constructing bridges) is that you really can replace entire layers cheaply, if you’ve isolated them.
Start by choosing the core problem that you are going to solve.
Ignore anything that’s not essential to that problem: you will add it in later.
The problem should be an end-toend problem: the rope across the gorge.
The rope across the gorge is one client talking to a broker talking to one worker.
It’s just like the many diagrams we saw in earlier chapters.
Our goal is not to define a real architecture, but to throw a rope across the gorge to bootstrap our process.
We’ll make the architecture progressively more complete and realistic over time: e.g., adding multiple workers, adding client and worker APIs, handling failures, and so on.
Step 3: Decide on the Contracts A good software architecture depends on contracts, and the more explicit they are, the better things scale.
You don’t care how things happen; you only care about the results.
If I send an email, I don’t care how it arrives at its destination, as long as the contract is respected (it arrives within a few minutes, it’s not modified, and it doesn’t get lost)
So what is a contract in a distributed system? There are, in my experience, two types of contract:
The APIs need to be as absolutely simple, consistent, and familiar as possible.
Yes, you can generate API documentation from code, but you must first design it, and designing an API is often hard.
You’ll write minimal contracts that are mostly just place markers.
Most messages and most API methods will be missing or empty.
You’ll also want to write down any known technical requirements in terms of throughput, latency, reliability, etc.
These are the criteria on which you will accept, or reject, any particular piece of work.
Step 4: Write a Minimal End-to-End Solution The goal is to test out the overall architecture as rapidly as possible.
Make skeleton applications that call the APIs, and skeleton stacks that implement both sides of every protocol.
You want to get a working end-to-end “Hello World” as soon as you can.
You want to be able to test code as you write it and to weed out the broken assumptions and inevitable errors you make.
Do not go off and spend six months writing a test suite! Instead, make a minimal bare-bones application that uses your still-hypothetical API.
If you design an API wearing the hat of the person who implements it, you’ll start to think of performance, features, options, and so on.
You’ll make it more complex, more irregular, and more surprising than it should be.
But—and here’s the trick (it’s a cheap one, was big in Japan)—if you design an API while wearing the hat of the person who has to actually write apps that use it, you’ll use all that laziness and fear to your advantage.
Our goal is to get the simplest test case working, without any avoidable functionality.
Everything you can chop off the list of things to do, you chop.
I’ll repeat this once again: you can always add functionality, that’s relatively easy.
But aim to keep the overall weight to a minimum.
Step 5: Solve One Problem and Repeat You’re now in the happy cycle of issue-driven development, where you can start to solve tangible problems instead of adding features.
Write issues that state a clear problem, and propose a solution for each.
As you design the API, keep in mind your standards for names, consistency, and behavior.
Writing these down in prose often helps keep them sane.
From here, every single change you make to the architecture and code can be proven by running the test case, watching it not work, making the change, and then watching it work.
Now you can go through the whole cycle (extending the test case, fixing the API, updating the protocol, extending the code, as needed), taking problems one at a time and testing the solutions individually.
It should take about 10–30 minutes for each cycle, with the occasional spike due to random confusion.
Unprotocols When this man thinks of protocols, this man thinks of massive documents written by committees, over years.
This man thinks of the IETF, W3C, ISO, Oasis, regulatory capture, FRAND patent license disputes...
The useless folk need a place to sit out their lives with minimal risk of reproducing; after all, that only seems fair.
More money means more people who want their particular prejudices and assumptions expressed in prose.
But the second reason is the lack of good abstractions on which to build.
People have tried to build reusable protocol abstractions, like BEEP.
Most did not stick, and those that did, like SOAP and XMPP, are on the complex side of things.
It used to be, decades ago, when the Internet was a young and modest thing, that protocols were short and sweet.
They weren’t even “standards,” but “requests for comments,” which is as modest as you can get.
It’s been one of my goals since we started iMatix in 1995 to find a way for ordinary people like me to write small, accurate protocols without the overhead of the committees.
Somewhere around mid-2007, I kicked off the Digital Standards Organization to define new, simpler ways of producing little standards, protocols, and specifications.
At the time, I wrote that a new specification should take “minutes to explain, hours to design, days to write, weeks to prove, months to become mature, and years to replace.”
In 2010, we started calling such little specifications “unprotocols,” which some people might mistake for a dastardly plan for world domination by a shadowy international organization, but which really just means “protocols without the goats.”
Contracts Are Hard Writing contracts is perhaps the most difficult part of large-scale architecture.
With unprotocols, we remove as much of the unnecessary friction as possible.
What remains is still a hard set of problems to solve.
A good contract (be it an API, a protocol, or a rental agreement) has to be simple, unambiguous, technically sound, and easy to enforce.
I’ll try to summarize what I’ve learned from my experience as a protocol writer:
Don’t solve problems you don’t have in front of you.
A protocol may often break down into commands and fields; use clear, short names for these entities.
Use terminology that is obvious and clear to your audience.
Make nothing for which you cannot demonstrate an immediate need.
Make the simplest plausible solution for each problem that you identify.
Implement your protocol as you build it, so that you are aware of the technical consequences of each choice.
Use a language that makes it hard (like C) and not one that makes it easy (like Python)
Test your specification on other people as you build it.
Your best feedback on a specification is when someone else tries to implement it without the assumptions and knowledge that you have in your head.
Cross-test rapidly and consistently, throwing others’ clients against your servers and vice versa.
Be prepared to throw it out and start again as often as needed.
Plan for this, by layering your architecture so that, e.g., you can keep an API but change the underlying protocols.
Only use constructs that are independent of programming language and operating system.
Solve a large problem in layers, making each layer an independent specification.
Think about how different teams could build competing specifications at each layer.
The point about a written specification is that no matter how weak it is, it can be systematically improved.
By writing down a specification you will be able to spot inconsistencies and gray areas that are impossible to see in code.
How to Write Unprotocols When you start to write an unprotocol specification document, stick to a consistent structure so that your readers know what to expect.
Cover section: with a one-line summary, URL to the spec, formal name, version, who to blame.
The change process: i.e., how can I as a reader fix problems in the specification?
Maturity indicator: is this an experimental, draft, stable, legacy, or retired version?
Goals of the protocol: what problems is it trying to solve?
Formal grammar: prevents arguments due to different interpretations of the text.
Describe in clear language the obligations and expectations of each party, the level of obligation, and the penalties for breaking the rules.
Do not try to define how each party honors its part of the deal.
As long as your process is open, you don’t need a committee: just make clean, minimal designs and make sure anyone is free to improve them.
If you use an existing license, you won’t have legal worries afterwards.
I use GPLv3 for my public specifications and advise you to do the same.
That is, learn to write a formal grammar such as ABNF (Augmented Backus-Naur Form) and use this to fully document your messages.
Use a market-driven life-cycle process like Digistan’s COSS so that people place the right weight on your specs as they mature (or don’t)
Why Use the GPLv3 for Public Specifications? The license you choose is particularly crucial for public specifications.
Traditionally, protocols are published under custom licenses, where the authors own the text and derived works are forbidden.
This sounds great (after all, who wants to see a protocol forked?), but it is in fact highly risky.
A protocol committee is vulnerable to capture, and if the protocol is important and valuable, the incentive for capture grows.
Once captured, like some wild animals, an important protocol will often die.
The real problem is that there’s no way to free a captive protocol published under a conventional license.
The word “free” isn’t just an adjective to describe speech or air, it’s also a verb, and the right to fork a work against the wishes of the owner is essential to avoiding capture.
Imagine that iMatix writes a protocol today, that’s really amazing and popular.
Those implementations are fast and awesome, and free as in beer.
They start to threaten an existing business, whose expensive commercial product is slower and can’t compete.
So one day some representatives of that business come to our iMatix office in MaetangDong, South Korea, and offer to buy our firm.
Because we’re spending vast amounts on sushi and beer, we accept gratefully.
With evil laughter, the new owners of the protocol stop improving the public version, close the specification, and add patented extensions.
Their new products support this, and they take over the whole market.
When you contribute to an open source project, you really want to know your hard work won’t be used against you by a closed-source competitor.
This is why the GPL beats the “more permissive” BSD/MIT/X11 licenses.
This applies just as much to protocols as to source code.
When you implement a GPLv3 specification, your applications are, of course, yours, and they can be licensed any way you like.
First, that specification will never be embraced and extended into proprietary forms.
Any derived forms of the specification must also be GPLv3
And second, no one who ever implements or uses the protocol will ever launch a patent attack on anything it covers.
Using ABNF My advice when writing protocol specs is to learn, and use, a formal grammar.
It’s just less hassle than allowing others to interpret what you mean, and then recover from the inevitable false assumptions.
The target of your grammar is other people: engineers, not compilers.
My favorite grammar is ABNF, as defined by RFC 2234, because it is probably the simplest and most widely used formal language for defining bidirectional communications protocols.
Most IETF (Internet Engineering Task Force) specifications use ABNF, which is good company to be in.
I’ll give a 30-second crash course in writing ABNF here.
An element can be another rule (which you define below as another rule), or a predefined “terminal” (like CRLF, OCTET), or a number.
To define repetition, use “*” (read the RFC, because it’s not intuitive)
I’m not sure if this extension is proper, but I then prefix elements with “C:” and “S:” to indicate whether they come from the client or server.
Here’s a piece of ABNF for an unprotocol called NOM that we’ll come back to later in this chapter:
I’ve actually used these keywords (OHAI, WTF) in commercial projects.
They’re good in first drafts that you want to throw away later.
The Cheap or Nasty Pattern There is a general lesson I’ve learned over a couple of decades of writing protocols small and large.
I call this the “Cheap or Nasty” pattern: you can often split your work into two aspects or layers, and solve these separately—one using a “cheap” approach, the other using a “nasty” approach.
The key insight to making Cheap or Nasty work is to realize that many protocols mix a low-volume chatty part for control, and a high-volume asynchronous part for data.
For instance, HTTP has a chatty dialog to authenticate and get pages, and an asynchronous dialog to stream data.
Protocol designers who don’t separate control from data tend to make horrid protocols, because the trade-offs in the two cases are almost totally opposed.
What is perfect for control is bad for data, and what’s ideal for data just doesn’t work for control.
This is especially true when we want high performance at the same time as extensibility and good error checking.
Let’s break this down using a classic client/server use case.
The server chats back, then starts to send data back to the client.
Eventually, the client disconnects or the server finishes, and the conversation is over.
Now, before starting to design these messages, stop and think, and let’s compare the control dialog and the data flow:
The control dialog lasts a short time and involves very few messages.
The data flow could last for hours or days and involve billions of messages.
The control dialog is where all the “normal” errors happen, e.g., not authenticated, not found, payment required, censored, and so on.
Any errors that happen during the data flow are exceptional (disk full, server crashed)
The control dialog is where things will change over time as we add more options, parameters, and so on.
The data flow should barely change over time because the semantics of a resource are fairly constant over time.
Thus, when we talk about performance, it applies only to data flows.
It’s pathological to design a one-time control dialog to be fast.
When we talk about the cost of serialization, this only applies to the data flow.
The cost of encoding/ decoding the control flow could be huge, and for many cases it would not change a thing.
So, we encode control using Cheap, and we encode data flows using Nasty.
A Cheap message is full of rich information that can change for each application.
Your goal as designer is to make this information easy to encode and parse, trivial to extend for experimentation or growth, and highly robust against change, both forwards and backwards.
It uses a simple self-describing structured encoding for data, be it XML, JSON, HTTP-style headers, or some other.
Any encoding is fine, so as long as there are standard simple parsers for it in your target languages.
It uses a straight request-reply model where each request has a success/failure reply.
This makes it trivial to write correct clients and servers for a Cheap dialog.
Performance doesn’t matter when you do something only once or a few times per session.
A Cheap parser is something you take off the shelf and throw data at.
It shouldn’t crash, shouldn’t leak memory, should be highly tolerant, and should be relatively simple to work with.
A Nasty message carries minimal information that practically never changes.
Your goal as designer is to make this information ultra-fast to parse, and possibly even impossible to extend and experiment with.
It uses a hand-optimized binary layout for data, where every bit is precisely crafted.
It uses a pure asynchronous model, where one or both peers send data without.
Performance is all that matters when you are doing something several million times per second.
A Nasty parser is something you write by hand, which writes or reads bits, bytes, words, and integers individually and precisely.
It rejects anything it doesn’t like, does no memory allocations at all, and never crashes.
Cheap or Nasty isn’t a universal pattern; not all protocols have this dichotomy.
Also, how you use Cheap or Nasty will depend on your circumstances.
In some cases, it can be two parts of a single protocol.
In other cases it can be two protocols, one layered on top of the other.
You have two kinds of commands and two ways to signal errors: Synchronous control commands.
Errors are normal: every request has a response that is either OK or an error response.
Asynchronous data commands Errors are exceptional: bad commands either are discarded silently or cause the whole connection to be closed.
It’s usually good to distinguish a few kinds of errors, but as always keep it minimal and add only what you need.
Serializing Your Data When we start to design a protocol, one of the first questions we face is how to encode data on the wire.
There are a half-dozen different ways to serialize data, each with pros and cons.
But this is a classic example of a control flow (the whole of MDP is a classic example, really, as it’s a chatty request-reply protocol)
When we came to improve MDP for the second version, we had to change this framing.
Where we’d expect to parse the command header in the various intermediaries (client API, broker, and worker API), and pass the command body untouched from application to application.
Today’s XML is the epitome of “somewhere in that mess is a small, elegant language trying to escape.”
Still, XML was way, way better than its predecessors, which included such monsters as the Standard Generalized Markup Language (SGML), which in turn were a cool breeze compared to mind-torturing beasts like EDIFACT.
So, the history of serialization languages seems to be one of gradually emerging sanity, hidden by waves of revolting EIAs doing their best to hold onto their jobs.
These are all pretty equivalent, so as long as you don’t go overboard with validating parsers, schemas, and other “trust us, this is all for your own good” nonsense.
While modern scripting languages support JSON and XML easily enough, older languages do not.
It’s also somewhat of a pain to work with tree-structured data in a language like C.
So, you can drive your choice according to the languages you’re aiming for.
If your universe is a scripting language, then go for JSON.
If you are aiming to build protocols for wider system use, keep things simple for C developers and stick to HTTP-style headers.
Serialization Libraries The msgpack.org site says this about the MessagePack serialization library:
It lets you exchange data among multiple languages like JSON, but it’s faster and smaller.
For example, small integers (like flags or error code) are encoded into a single byte, and typical short strings only require an extra byte in addition to the strings themselves.
I’m going to make the perhaps unpopular claim that “fast and small” are features that solve non-problems.
The only real problem that serialization libraries solve is, as far as I can tell, the need to document the message contracts and actually serialize data to and from the wire.
It’s based on a two-part argument: first, that making your messages smaller and reducing CPU cost for encoding and decoding will make a significant difference to your application’s performance; and second, that this will be equally valid across the board, for all messages.
But most real applications tend to fall into one of two categories: either the speed of serialization and size of encoding are marginal compared to other costs, such as database access or application code performance, or network performance really is critical, and then all significant costs occur in a few specific message types.
Thus, aiming for “fast and small” across the board is a false optimization.
You get neither the easy flexibility of Cheap for your infrequent control flows, nor the brutal efficiency of Nasty for your high-volume data flows.
Worse, the assumption that all messages are equal in some way can corrupt your protocol design.
Cheap or Nasty isn’t only about serialization strategies; it’s also about synchronous versus asynchronous, error handling, and the cost of change.
My experience is that most performance problems in message-based applications can be solved by (a) improving the application itself and (b) hand-optimizing the highvolume data flows.
And to hand-optimize your most critical data flows, you need to cheat and to know and exploit facts about your data, which is something general- purpose serializers cannot do.
Now let us address documentation: the need to write our contracts explicitly and formally, not only in code.
This is a valid problem to solve: indeed, one of the main ones if we’re to build a long-lasting, large-scale, message-based architecture.
Here is how we describe a typical message using the MessagePack interface definition language (IDL):
It works, but in most practical cases wins you little over a serialization language backed by decent specifications written by hand or produced mechanically (we’ll come to this)
The price you’ll pay is an extra dependency and, quite probably, worse overall performance than if you used Cheap or Nasty.
There are two reasons I like this modernized C language.
First, I’m too weak-minded to learn a big language like C++
Life just seems filled with more interesting things to understand.
Second, I find that this specific level of manual control lets me produce better results, faster.
The point here isn’t C versus C++, but the value of manual control for high-end professional users.
It’s no accident that the best cars, cameras, and espresso machines in the world have manual controls.
That level of on-the-spot fine-tuning often makes the difference between world-class success and being second best.
When you are really, truly concerned about the speed of serialization and/or the size of the result (often these contradict each other), you need handwritten binary serialization.
Your basic process for writing an efficient Nasty encoder/decoder (codec) is:
Build representative data sets and test applications that can stress-test your codec.
Test, measure, improve, and repeat until you run out of time and/or money.
Here are some of the techniques we use to make our codecs better:
There’s simply no way to know what your code is doing until you’ve profiled it for function counts and for CPU cost per function.
The heap is very fast on a modern Linux kernel, but it’s still the bottleneck in most naive codecs.
Use local variables (the stack) instead of the heap where you can.
Test on different platforms and with different compilers and compiler options.
You need to learn the main ones, and allow for them.
If you are concerned about codec performance, you are almost definitely sending the same kinds of data many times.
You can detect these and use that to compress (for example, a short value that means “same as last time”)
The best compression techniques (in terms of CPU cost for compactness) require knowing about the data.
For example, the techniques used to compress a word list, a video, and a stream of stock market data are all different.
Do you really need to encode integers in big-endian network byte order? x86 and ARM account for almost all modern CPUs, yet they use little-endian byte order (ARM is actually bi-endian, but Android, like Windows and iOS, is little-endian)
Code Generation Reading the previous two sections, you might have wondered, “Could I write my own IDL generator that’s better than a general-purpose one?” If this thought wandered into your mind, it probably left pretty soon after, chased by dark calculations about how much work that would actually involve.
What if I told you of a way to build custom IDL generators cheaply and quickly? You can have a way to get perfectly documented contracts, code that is as evil and domainspecific as you need, and all you need to do is sign away your soul (who ever really used that, amirite?) right here....
At iMatix, until a few years ago, we used code generation to build ever larger and more ambitious systems; then we decided the technology (the Generator Script Language, or GSL) was too dangerous for common use, and we sealed the archive and locked it away, with heavy chains, in a deep dungeon.
If you want to try the examples that are coming up, grab the repository and build yourself a gsl command.
Typing “make” in the src subdirectory should do it (and if you’re that guy who loves Windows, I’m sure you’ll send a patch with project files)
This section isn’t really about GSL at all, but about a useful and little-known trick that’s handy for ambitious architects who want to scale themselves, as well as their work.
Once you learn the trick, you can whip up your own code generators in a short time.
The code generators most software engineers know about come with a single hard-coded model.
For instance, Ragel “compiles executable finite state machines from regular languages” (i.e., Ragel’s model is a regular language)
This certainly works for a good set of problems, but it’s far from universal.
How do you describe an API in Ragel? Or a project makefile? Or even a finite-state machine like the one we used to design the Binary Star pattern in Chapter 4?
All of these would benefit from code generation, but there’s no universal model.
So, the trick is to design your own models as you need them, and then make code generators as cheap compilers for those models.
You need some experience in how to make good models, and you need a technology that makes it cheap to build custom code generators.
Scripting languages like Perl and Python are a good option.
However, we actually built GSL specifically for this, and that’s what I prefer.
Let’s take a simple example that ties into what we already know.
We’ll see more extensive examples later, because I really do believe that code generation is crucial knowledge for large-scale work.
In Chapter 4, we developed the Majordomo Protocol (MDP) and wrote clients, brokers, and workers for that.
Now, could we generate those pieces mechanically, by building our own interface description language and code generators?
When we write a GSL model, we can use any semantics we like.
In other words, we can invent domain-specific languages on the spot.
I’ll invent a couple—see if you can guess what they represent:
Now, how about this one? table name = person column name = firstname type = string column name = lastname type = string column name = rating type = integer.
The second, we could compile into SQL to create and work with a database table.
So, for this exercise our domain languageour model—consists of “classes” that contain “messages” that contain “fields” of various types.
Any way to write a self-describing hierarchy of items and attributes would work.
Now, here is a short IDL generator written in GSL that turns our protocol models into documentation:
The XML models and this script are in the subdirectory examples/models.
Here is the Markdown text we get for the worker protocol: ## The MDP/Worker Protocol.
A READY command consists of a multipart message of 4 frames:
A REQUEST command consists of a multipart message of 5 frames:
A REPLY command consists of a multipart message of 5 frames:
A HEARBEAT command consists of a multipart message of 3 frames:
A DISCONNECT command consists of a multipart message of 3 frames:
Which, as you can see, is close to what I wrote by hand in the original spec.
Now, if you have cloned the book repository and you are looking at the code in examples/models, you can generate the MDP client and worker codecs.
We pass the same two models to a different code generator:
Actually, MDP is so simple that it’s barely worth the effort of writing the code generator.
The profit comes when we want to change the protocol (which we did for the standalone Majordomo project)
We modify the protocol, run the command, and out pops more perfect code.
The codec_c.gsl code generator is not short, but the resulting codecs are much better than the handwritten code I originally put together for Majordomo.
For instance, the handwritten code had no error checking, and would die if you passed it bogus messages.
I’m now going to explain the pros and cons of GSL-powered model-oriented code generation.
Power does not come for free, and one of the greatest traps in our business is the ability to invent concepts out of thin air.
The job of a designer is to remove problems, not add features.
First, I will lay out the advantages of model-oriented code generation:
You can create “perfect” abstractions that map to your real world.
So, our protocol model maps 100% to the “real world” of Majordomo.
This would be impossible without the freedom to tune and change the model in any way.
From a single model, you can create documentation, code in any language, test tools—literally any output you can think of.
You can generate (and I mean this literally) perfect output, since it’s cheap to improve.
You get a single source that combines specifications and semantics.
You can leverage a small team to a massive size.
At iMatix, we produced the millionline OpenAMQ messaging product out of perhaps 85K lines of input models, including the code generation scripts themselves.
You may get carried away and create models for the pure joy of creating them.
You may alienate newcomers, who will see “strange stuff,” from your work.
You may give people a strong excuse to not invest in your project.
Cynically, model-oriented abuse works great in environments where you want to produce huge amounts of perfect code that you can maintain with little effort, and which no one can ever take away from you.
Personally, I like to cross my rivers and move on.
But if long-term job security is your thing, this is almost perfect.
So, if you do use GSL and want to create open communities around your work, here is my advice:
Use it only where you would otherwise be writing tiresome code by hand.
Design natural models that are what people would expect to see.
Write the code by hand first so you know what to generate.
Transferring Files Let’s take a break from the lecturing and get back to our first love and the reason for doing all of this: code.
I’ve promised, for a year or two, to write a proper explanation.
Here’s a gratuitous piece of information to brighten your morning: the word “proper” comes from the archaic French propre, which means “clean.” English common folk in the Dark Ages, not being familiar with hot water and soap, changed the word to mean “foreign” or “upper-class,” as in “that’s proper food!”; later it came to mean just “real,” as in “that’s a proper mess you’ve gotten us into!”
There are several reasons why you can’t just pick up a random file, blindfold it, and shove it whole into a message.
Even if we could send a file with one instruction (say, using a system call like sendfile), we’d hit the reality that networks are neither infinitely fast, nor perfectly reliable.
After trying to upload a large file several times on a slow, flaky network (WiFi, anyone?), you’ll realize that a proper file transfer protocol needs a way to recover from failures.
That is, it needs a way to send only the part of a file that hasn’t yet been received.
Finally, after all this, if you build a proper file server you’ll notice that simply sending massive amounts of data to lots of clients creates that situation we like to call, in the technical parlance, “server went belly-up due to all available heap memory being eaten by a poorly designed application.” A proper file transfer protocol needs to pay attention to memory use.
This is large enough to be troublesome when we have lots of clients asking for the same file at once, and on many machines, 1 GB is going to be too large to allocate in memory anyhow.
As a base reference, let’s measure how long it takes to copy this file from disk back to disk.
This will tell us how much our file transfer protocol adds on top (including network costs):
The four-figure precision is misleading; expect variations of 25% either way.
Example 7-1 shows our first cut at the code, where the client asks for the test data and the server just sends it, without stopping for breath, as a series of messages, where each message holds one “chunk.”
The server thread, shown in Example 7-2, reads the file from the disk in chunks and sends each chunk to the client as a separate message.
We only have one test file, so we’ll open that once and then serve it out as needed.
The main task, shown in Example 7-3, starts the client and server threads; it’s easier to test this as a single process with threads than as multiple processes.
It’s pretty simple, but we already run into a problem: if we send too much data to the ROUTER socket, we can easily overflow it.
The simple but stupid solution is to put an infinite high-water mark on the socket.
It’s stupid because we now have no protection against exhausting the server’s memory.
Yet without an infinite HWM, we risk losing chunks of large files.
We have to control the amount of data the server sends up-front.
There’s no point in it sending more than the network can handle.
In this version of the protocol, the client will explicitly say, “Give me chunk N,” and the server will fetch that specific chunk from disk and send it.
Example 7-4 presents the improved second model, where the client asks for one chunk at a time, and the server only sends one chunk for each request it gets from the client.
The server thread (Example 7-5) waits for a chunk request from a client, reads that chunk, and sends it back to the client.
The main task is just the same as in the first model ...
It is much slower now, because of the to-and-fro chatting between client and server.
We pay about 300 microseconds for each request-reply round-trip, on a local loop connection (client and server on the same box)
It doesn’t sound like much, but it adds up quickly:
First, while request-reply is easy, it’s also too slow for high-volume data flows.
Paying it for every single chunk isn’t acceptable, particularly on real networks with latencies perhaps 1,000 times higher.
Our model 2 file transfer protocol isn’t so bad, apart from performance:
To prove that, we set the high-water mark to 1 in both the sender and the receiver.
It lets the client choose the chunk size, which is useful because if there’s any tuning of the chunk size to be done (for network conditions, for file types, or to reduce memory consumption further), it’s the client that should be doing this.
It allows the client to cancel the file transfer at any point in time.
If we just didn’t have to do a request for each chunk, it’d be a usable protocol.
What we need is a way for the server to send multiple chunks without waiting for the client to request or acknowledge each one.
The server could send 10 chunks at once, then wait for a single acknowledgment.
That’s exactly like multiplying the chunk size by 10, though, so it’s pointless.
The server could send chunks without any chatter from the client but with a slight delay between each send, so that it would send chunks only as fast as the network could handle them.
This would require the server to know what’s happening at the network layer, though, which sounds like hard work.
And what happens if the network is really fast, but the client itself is slow? Where are chunks queued then?
We could modify libzmq to take some other action on reaching the HWM.
Perhaps it could block? That would mean that a single slow client would block the whole server, so no thank you.
Maybe it could return an error to the caller? Then the server could do something smart like...
Apart from being complex and variously unpleasant, none of these options would even work.
What we need is a way for the client to tell the server, asynchronously and in the background, that it’s ready for more.
If we do this right, data should flow without interruption from the server to the client, but only as long as the client is reading it.
In fact, it’s identical, except that we send multiple requests without waiting for a reply for each one.
This is a technique called “pipelining,” and it works because our DEALER and ROUTER sockets are fully asynchronous.
Example 7-6 presents the third model of our file transfer test-bench, with pipelining.
The client sends a number of requests ahead (the “credit”), and then each time it processes an incoming chunk, it sends one more credit.
The server will never send more chunks than the client has asked for.
Up to this many chunks in transit size_t credit = PIPELINE;
The rest of the code is exactly the same as in model 2, except //  that we set the HWM on the server’s ROUTER socket to PIPELINE //  to act as a sanity check ...
In the third model, I chose a pipeline size of 10 messages (each message is a chunk)
In practice, I still got performance spikes with a pipeline of 5, probably because the credit messages sometimes get delayed by outgoing data.
Your calculations may be good, but the real world tends to have its own opinions.
What we’ve made is clearly not yet a real file transfer protocol, but it proves the pattern, and I think it is the simplest plausible design.
For a real working protocol, we might want to add some or all of the following:
Authentication and access controls, even without encryption: the point isn’t to protect sensitive data, but to catch errors like sending test data to production servers.
A Cheap-style request including the file path, optional compression, and other stuff we’ve learned is useful from HTTP (such as If-Modified-Since)
A Cheap-style response, at least for the first chunk, that provides meta-data such as file size (so the client can preallocate and avoid unpleasant disk-full situations)
The ability to fetch a set of files in one go; otherwise the protocol becomes inefficient for large sets of small files.
Confirmation from the client when it’s fully received a file, to recover from chunks that might be lost if the client disconnects unexpectedly.
So far, our semantic has been “fetch”; that is, the recipient knows (somehow) that it needs a specific file, so it asks for it.
The knowledge of which files exist, and where they are, is then passed out-of-band (e.g., in HTTP, by links in the HTML page)
How about a “push” semantic? There are two plausible use cases for this.
First, if we adopt a centralized architecture with files on a main “server” (not something I’m advocating, but people do sometimes like this), then it’s very useful to allow clients to upload files to the server.
Second, it lets us do a kind of pub-sub for files, where the client asks for all new files of some type; as the server gets these, it forwards them to the client.
A fetch semantic is synchronous, while a push semantic is asynchronous.
Also, you can do cute things like “subscribe to this path,” thus creating a publish-subscribe file transfer architecture.
That is so obviously awesome that I shouldn’t need to explain what problem it solves.
Still, here is the problem with the fetch semantic: that out-of-band route to tell clients what files exist.
No matter how you do this, it ends up being complex.
Either clients have to poll, or you need a separate pub-sub channel to keep clients up-to-date, or you need user interaction.
Thinking this through a little more, though, we can see that fetch is just a special case of publish-subscribe.
To make this work (and we will, my dear readers), we need to be a little more explicit about how we send credit to the server.
The cute trick of treating a pipelined “fetch chunk” request as credit won’t fly because the client doesn’t know any longer what files actually exist, how large they are, or anything.
And this gives us “credit-based flow control,” which effectively removes the need for high-water marks and any risk of memory overflow.
State Machines Software engineers tend to think of (finite) state machines as a kind of intermediary interpreter.
That is, you take a regular language and compile that into a state machine, then execute the state machine.
How messages flow between peers, and the significance of each message.
We’ve seen in this chapter how to produce codecs that handle serialization.
But if we leave the second job to developers, that gives them a lot of room to interpret.
As we make more ambitious protocols (file transfer + heartbeating + credit + authentication), it becomes less and less sane to try to implement clients and servers by hand.
In this section I’ll explain how to model protocols using state machines, and how to generate neat and solid code from those models.
My initial experience with using state machines as a software construction tool dates back to 1985 and my first real job making tools for application developers.
In 1991 I turned that knowledge into a free software tool called Libero, which spat out executable state machines from a simple text model.
The thing about Libero’s model was that it was readable.
That is, you described your program logic as named states, each accepting a set of events, each doing some real.
The resulting state machine hooked into your application code, driving it like a boss.
Libero was charmingly good at its job, fluent in many languages, and modestly popular, given the enigmatic nature of state machines.
As a worked example, let’s see how to carry on a stateful dialog with a peer on a ROUTER socket.
We’ll develop the server using a state machine (and the client by hand)
We have a simple protocol that I’ll call “NOM.” I’m using the oh-so-very-serious “Keywords for Unprotocols” proposal:
I’ve not found a quick way to explain the true nature of state machine programming.
In my experience, it invariably takes a few days of practice.
After three or four days’ exposure to the idea there is a near-audible “click!” as something in the brain connects all the pieces together.
We’ll make it concrete by looking at the state machine for our NOM server.
A useful thing about state machines is that you can read them state by state.
Each state has a unique descriptive name and one or more events, which we list in any order.
For each event we perform zero or more actions, and we then move to a next state (or stay in the same state)
The Check Credentials action produces either an “OK” or an “error” event.
It’s in the Authenticated state that we handle these two possible events, by sending an appropriate reply back to the client (Figure 7-2)
If authentication failed, we return to the Start state where the client can try again.
When authentication has succeeded, we arrive in the Ready state.
Here we have three possible events: an ICANHAZ or HUGZ message from the client, or a heartbeat timer event (Figure 7-3)
There are a few more things about this state machine model that are worth knowing:
Events in upper-case (like “HUGZ”) are “external events” that come from the client as messages.
Events in lower-case (like “heartbeat”) are “internal events,” produced by code in the server.
The “Send SOMETHING” actions are shorthand for sending a specific reply back to the client.
Events that aren’t defined in a particular state are silently ignored.
It is a fairly complete tool that I’ll use and expand for more serious work later.
A selftest method that runs the self-test steps listed in the XML file.
It accepts client messages on a ROUTER socket, so the first frame of every request is the client’s connection identity.
The server manages a set of clients, each with state.
As messages arrive, it feeds these as “events” to the state machine.
Here’s the core of the state machine, as a mix of GSL commands and the C code we intend to generate:
Each client is held as an object with various properties, including the variables we need to represent a state machine instance:
You will see by now that we are generating technically perfect code that has the precise design and shape we want.
The only clue that the nom_server class isn’t handwritten is that the code is too good.
People who complain that code generators produce poor code are used to poor code generators.
It is trivial to extend our model as we need it.
First, we add a “selftest” item to the state machine and write our tests.
We’re not using any XML grammar or validation, so it really is just a matter of opening the editor and adding a half-dozen lines of text:
Designing on the fly, I decided that “send” and “recv” were a nice way to express “send this request, then expect this reply.” Here’s the GSL code that turns this model into real code:
After some thought I decided to grab code directly from the state machine model—i.e., embed action bodies in the XML file.
And the code generator grabs that C code and inserts it into the generated nom_server.c file:
Now we have something quite elegant: a single source file that describes my server state machine, and which also contains the native implementations for my actions.
A nice mix of high-level and low-level that is about 90% smaller than the C code.
Beware, as your head spins with notions of all the amazing things you could produce with such leverage: while this approach gives you real power, it also moves you away from your peers, and if you go too far, you’ll find yourself working alone.
By the way, this simple little state machine design exposes just three variables to our custom code:
In the Libero state machine model there are a few more concepts that we haven’t used here but that we will need when we write larger state machines:
When an action raises an exception, further processing on the event stops.
The state machine can then define how to handle exception events.
Default state, where we can define default handling for events (especially useful for exception events)
I’m not going to give a deep explanation now, since we’ll see SASL in action a bit later.
But I’ll explain the principle so you’re already somewhat prepared.
In the NOM protocol the client started with an OHAI command, which the server either accepted (“Hi Joe!”) or rejected.
This is simple but not scalable, since the server and client have to agree up-front on what kind of authentication they’re going to do.
What SASL introduces, which is genius, is a fully abstracted and negotiable security layer that’s still easy to implement at the protocol level.
The server challenges the client, passing a list of security “mechanisms” that it knows about.
The client chooses a security mechanism that it knows about, and answers the server’s challenge with a blob of opaque data that (and here’s the neat trick) some generic security library calculates and gives to the client.
The server takes the security mechanism the client chose, and that blob of data, and passes it to its own security library.
Either the library accepts the client’s answer, or the server challenges again.
When we come to real code, we’ll implement just two mechanisms, ANONYMOUS and PLAIN, which don’t need any special libraries.
To support SASL we have to add an optional challenge/response step to our “openpeering” flow.
Here is what the resulting protocol grammar looks like (I’m modifying NOM to do this):
Depending on the mechanism, the initial challenge from the server may be empty.
We don’t care a jot: we just pass this to the security library to deal with.
The SASL RFC goes into detail about other features (that we don’t need), the kinds of ways SASL could be attacked, and so on.
Large-Scale File Publishing: FileMQ Let’s put all these techniques together into a file distribution system that I’ll call FileMQ.
This is going to be a real product, living on GitHub.
What we’ll make here is a first version of FileMQ, as a training tool.
If the concept works, the real thing may eventually get its own book.
It’s the DropBox pattern: chuck your files somewhere, and they get magically copied somewhere else, when the network connects again.
However, what I’m aiming for is a fully decentralized architecture that looks more like Git, that doesn’t need any cloud services (though we could put FileMQ in the cloud), and that does multicast (i.e., can send files to many places at once)
FileMQ has to be secure(able), has to be easily hooked into random scripting languages, and has to be as fast as possible across our domestic and office networks.
I want to use it to back up photos from my mobile phone to my laptop, over WiFi.
To share presentation slides in real time across 50 laptops in a conference.
To back up video from my phone as I take it, during protests or riots.
To synchronize configuration files across a cloud of Linux servers.
The hard part is making this, and making it simple.
FileMQ has to be distributed, so every node can be a server and a client at the same time.
But I don’t want the protocol to be symmetrical, because that seems forced.
We have a natural flow of files from point A to point B, where A is the “server” and B is the “client.” If files flow back the other way, we have two flows.
FileMQ is not yet a directory synchronization protocol, but we’ll bring it quite close.
Thus, I’m going to build FileMQ as two pieces: a client and a server.
Then, I’ll put these together in a main application (the “filemq” tool) that can act both as client and server.
The two pieces will look quite similar to the nom_server, with the same kind of API:
If we wrap this C API in other languages, we can easily script FileMQ, embed it applications, port it to smartphones, and so on.
To start with, we write down the protocol as an ABNF grammar.
Our grammar starts with the flow of commands between the client and server.
You should recognize these as a combination of the various techniques we’ve seen already:
Client or server sends a heartbeat HUGZ            = signature %x09
Client or server responds to a heartbeat HUGZ-OK         = signature %x0A.
And here are the different ways the server can tell the client things went wrong: ;   Server error reply - refused due to access rights S:SRSLY         = signature %x80 reason.
Server error reply - client sent an invalid command S:RTFM          = signature %x81 reason.
You want to be using the latest CZMQ master for this.
Now try running the track command, which is a simple tool that uses FileMQ to track changes in one directory in another:
Open two file navigator windows, one into src/fmqroot/send and one into src/fmqroot/ recv.
Drop files into the send folder, and you’ll see them arrive in the recv folder.
Delete files in the send folder, and they’re deleted in the recv folder simultaneously.
I use track for things like updating my MP3 player, mounted as a USB drive.
As I add or remove files in my laptop’s Music folder, the same changes happen on the MP3 player.
Internal Architecture To build FileMQ I used a lot of code generation, possibly too much for a tutorial.
They are an evolution of the set we saw earlier:
The best way to learn to use GSL code generation is to translate these into a language of your choice and make your own demo protocols and stacks.
There’s a generic set of classes to handle chunks, directories, files, patches, SASL security, and configuration files.
If I was creating a new project I’d fork the whole FileMQ project, and go and modify the three models:
Why didn’t I make the reusable classes into a separate library? The answer is twofold.
Second, it’d make things more complex for you as you build and play with FileMQ.
It’s never worth adding complexity to solve a theoretical problem.
Although I wrote FileMQ in C, it’s easy to map to other languages.
It is quite amazing how nice C becomes when you add CZMQ’s generic zlist and zhash containers, and class style.
I only implemented the PLAIN mechanism, which is enough to prove the concept.
It gives you information about a file (like size) and lets you read from and write to files, remove files, check if a file exists, and check if a file is “stable” (more on that later)
When there are changes, it returns a list of “patches.”
Every class has a test method, and the main development cycle is “edit, test.” These are mostly simple self-tests, but they make the difference between code I can trust and code I know will still break.
It’s a safe bet that any code that isn’t covered by a test case will have undiscovered errors.
You should, really, be able to read the source code and rapidly understand what these classes are doing.
If you want to port the FileMQ implementation into other languages, start by forking the whole repository, and later we’ll see if it’s possible to do this in one overall repo.
Public API The public API consists of two classes (as we sketched earlier):
If I was a keen young developer eager to use FileMQ in another language, I’d probably spend a happy weekend writing a binding for this public API, then stick it in a subdirectory of the filemq project called, say, “bindings/,” and make a pull request.
The actual API methods come from the state machine description, like this (for the server):
Design Notes The hardest part of making FileMQ wasn’t implementing the protocol, but maintaining accurate state internally.
An FTP or HTTP server is essentially stateless, but a publishsubscribe server has to maintain subscriptions, at least.
The server detects if a client has died by its lack of response (HUGZ-OK) to a heartbeat.
In that case, it deletes all state for the client, including its subscriptions.
The client API holds subscriptions in memory and replays them when it has connected successfully.
This means the caller can subscribe at any time (and doesn’t care when connections and authentication actually happen)
The server and client use virtual paths, much like an HTTP or FTP server.
You publish one or more “mount points,” each corresponding to a directory on the server.
Each of these maps to some virtual path; for instance, “/” if you have only one mount point.
Clients then subscribe to virtual paths, and files arrive in an inbox directory.
There are some timing issues: if the server is creating its mount points while clients are connected and subscribing, the subscriptions won’t attach to the right mount points.
So, we bind the server port as the last thing.
Clients can reconnect at any point; if the client sends OHAI, that signals the end of any previous conversation and the start of a new one.
I might one day make subscriptions durable so that they survive a disconnection.
The client stack, after reconnecting, replays any subscriptions the caller application has already made.
Configuration I’ve built several large server products, like the Xitami web server that was popular in the late ’90s, and the OpenAMQ messaging server.
Getting configuration easy and obvious was a large part of making these servers fun to use.
Allow users to add custom configuration files that are never overwritten.
And then layer these one on top of the other, so command-line settings override custom settings, which override default settings.
It can be a lot of work to do this right.
For FileMQ, I’ve taken a somewhat simpler tack: all configuration is done from the API.
File Stability It is quite common to poll a directory for changes and then do something “interesting” with new files.
But as one process is writing to a file, other processes have no idea when the file has been fully written.
One solution is to add a second “indicator” file that we create after creating the first file.
There is a neater way, which is to detect when a file is “stable” (i.e., no one is writing to it any longer)
FileMQ does this by checking the modification time of the file.
If it’s more than a second old, then the file is considered stable—at least, stable enough to be shipped off to clients.
If a process comes along after five minutes and appends to the file, it’ll be shipped off again.
For this to work, and this is a requirement for any application hoping to use FileMQ successfully, do not buffer more than a second’s worth of data in memory before writing.
If you use very large block sizes, the file may look stable when it’s not.
Delivery Notifications One of the nice things about the multithreaded API model we’re using is that it’s essentially message-based.
This makes it ideal for returning events back to the caller.
A more conventional API approach would be to use callbacks, but callbacks that cross thread boundaries are somewhat delicate.
Here’s how the client sends a message back when it has received a complete file:
We can now add a _recv() method to the API that waits for events back from the client.
It makes a clean style for the caller: create the client object, configure it, and then receive and process any events it returns.
Symbolic Links While using a staging area is a nice, simple API, it also creates costs for senders.
If I already have a 2 GB video file on a camera and I want to send it via FileMQ, the current implementation asks that I copy it to a staging area before it will be sent to subscribers.
One option is to mount the whole content directory (e.g., /home/me/Movies), but this is fragile since it means the application can’t decide to send individual files.
A symbolic link contains a text string that is automatically interpreted and followed by the operating system as a path to another file or directory.
The symbolic link is a second file that exists independently of its target.
If a symbolic link is deleted, its target remains unaffected.
This doesn’t affect the protocol in any way; it’s an optimization in the server implementation.
A symbolic link consists of a filename with the extension .ln.
The link file contains one line, which is the real path to the file.
Since we’ve collected all operations on files in a single class (fmq_file), it’s a clean change.
When we create a new file object we check if it’s a symbolic link; if so, all readonly actions (get file size, read file) operate on the target file, not the link.
Recovery and Late Joiners As it stands now, FileMQ has one major remaining problem: it provides no way for clients to recover from failures.
The scenario is that a client, connected to a server, starts to receive files, and then disconnects for some reason.
The client may be on a laptop that is shut down, then resumed.
As we move to a more mobile world (see Chapter 8), this use case becomes more and more frequent.
Take any FileMQ use case, and you’ll see that if the client disconnects and reconnects, it should get anything it missed.
A further improvement would be to recover from partial failures, like HTTP and FTP do.
One answer to recovery is “durable subscriptions.” The first drafts of the FILEMQ protocol aimed to support this, with client identifiers that the server could hold onto and store so that if a client reappeared after a failure, the server would know what files it had not received.
Stateful servers are, however, nasty to make and difficult to scale.
How do we, for example, fail over to a secondary server? Where does it get its subscriptions from? It’s far nicer if each client connection works independently and carries all necessary state with it.
Another nail in the coffin of durable subscriptions is that this approach requires upfront coordination.
Up-front coordination is always a red flag, whether it’s in a team of people working together or a bunch of processes talking to each other.
What about late joiners? In the real world, clients do not neatly line up and then all say “Ready!” at the same time.
In the real world they come and go arbitrarily, and it’s valuable if we can treat a brand new client in the same way as a client that has gone away and come back.
To deal with this I will add two concepts to the protocol: a resync option and a cache field (a dictionary)
If the client wants recovery, it sets the resync option and tells the server what files it already has via the cache field.
We need both, because there’s no way in the protocol to distinguish between an empty field and a null field.
When the client specifies the RESYNC option, the ‘cache’ dictionary field tells the server which files the client already has.
If the filename starts with ‘/’ then it SHOULD start with the path, otherwise the server MUST ignore it.
If the filename does not start with ‘/’ then the server SHALL treat it as relative to the path.
Clients that know they are in the classic pub-sub use case just don’t provide any cache data, and clients that want recovery provide their cache data.
It requires no state in the server, no up-front coordination, and works equally well for brand new clients (which may have received files via some out-of-band means) and clients that have received some files and were then disconnected for a while.
Second, it’s reliable: the chance of getting the same hash for different versions of one file is close enough to zero.
A cyclic-redundancy check (such as CRC-32) is faster but not reliable.
Here is what a typical ICANHAZ message looks like when we use both caching and resyncing (this is output from the dump method of the generated codec class):
Although we don’t do this in FileMQ, the server can use the cache information to help the client catch up with deletions that it has missed.
To do this it would have to log deletions, and then compare this log with the client cache when a client subscribes.
Test Use Case: The Track Tool To properly test something like FileMQ we need a test case that plays with live data.
One of my sysadmin tasks is to manage the MP3 tracks on my music player.
That is, by the way, a Sansa Clip reflashed with Rock Box, which I highly recommend.
As I download tracks into my Music folder, I want to copy these to my player, and as I find tracks that annoy me, I delete them in the Music folder and want those gone from my player, too.
I could write this using a bash or Perl script—a powerful file distribution protocol is kind of overkill—but to be honest the hardest work in FileMQ was the directory comparison code, and I want to benefit from that.
So I put together a simple tool called “track” that calls the FileMQ API.
From the command line this runs with two arguments, the sending and receiving directories:
The code is a neat example of how to use the FileMQ API to do local file distribution.
Here is the full program, minus the license text (it’s MIT/X11 licensed):
Note how we work with physical paths in this tool.
The server publishes the physical path “/home/ph/Music” and maps this to the virtual path “/”
I could use any structure within the server directory, and it would be copied faithfully to the client’s inbox.
Getting an Official Port Number We’ve been using port 5670 in the examples for FILEMQ.
Unlike all the previous examples in this book, this port isn’t arbitrary but was assigned by the Internet Assigned Numbers Authority, which “is responsible for the global coordination of the DNS Root, IP addressing, and other Internet protocol resources.”
I’ll explain very briefly when and how to request registered port numbers for your application protocols.
The main reason is to ensure that your applications can run in the public domain without conflict with other protocols.
Many products don’t bother with this, however, and tend instead to use the IANA list as “ports to avoid.”
If you aim to make a public protocol of any importance, such as FILEMQ, you’re going to want an IANA-registered port.
Document your protocol clearly, as IANA will want a specification of how you intend to use the port.
It’s not a formal agreement but must be solid enough to pass expert review.
Fill in the application on iana.org, providing all the necessary information.
Note that you don’t request a specific port number; IANA will assign you one.
It’s therefore wise to start this process before you ship software, not afterwards.
But what is that “real world”? I’ll argue that it is becoming a world of ever-increasing numbers of moving pieces.
Some people use the phrase “the Internet of Things,” suggesting that we’ll soon see a new category of devices that are more numerous, but also more stupid than our current smartphones, tablets, laptops, and servers.
However, I don’t think the data points this way at all.
Yes, there are more and more devices, but they’re not stupid at all.
They’re smart and powerful, and getting more so all the time.
The mechanism at work is something I call “cost gravity,” and it has the effect of reducing the cost of technology by half every 18–24 months.
Put another way, our global computing capacity doubles every two years, over and over and over.
The future is filled with trillions of devices that are fully powerful multicore computers: they don’t run a cut-down “operating system for things,” but full operating systems and full applications.
Now, to make even a thin imitation of this vision come true today, we need to solve a set of technical problems.
These include: How do peers discover each other? How do they talk to existing networks like the Web? How do they protect the information they carry? How do we track and monitor them, to get some idea of what they’re doing? Then we need to do what most engineers forget about: package this solution into a framework that is dead easy for ordinary developers to use.
Design for the Real World Whether we’re connecting a roomful of mobile devices over WiFi or a cluster of virtual boxes over simulated Ethernet, we will hit the same kinds of problems.
How do we learn about other nodes on the network? Do we use a discovery service, centralized mediation, or some kind of broadcast beacon?
Presence How do we track when other nodes come and go? Do we use some kind of central registration service, or heartbeating or beacons?
Connectivity How do we actually connect one node to another? Do we use local networking, wide-area networking, or do we use a central message broker to do the forwarding?
Point-to-point messaging How do we send a message from one node to another? Do we send this to the node’s network address, or do we use some indirect addressing via a centralized message broker?
Testing and simulation How do we simulate large numbers of nodes so we can test performance properly? Do we have to buy two dozen Android tablets, or can we use pure software simulation?
Distributed logging How do we track what this cloud of nodes is doing so we can detect performance problems and failures? Do we create a main logging service, or do we allow every device to log the world around it?
Content distribution How do we send content from one node to another? Do we use server-centric protocols like FTP or HTTP, or do we use decentralized protocols like FileMQ?
If we can solve these problems reasonably well, and the further problems that will emerge (like security and wide-area bridging), we’ll get something like a framework for what I might call “Really Cool Distributed Applications” and my grandkids might call “the software our world runs on.”
One, the number of moving pieces increases exponentially over time (it doubles every 24 months)
Two, these pieces stop using wires because dragging cables everywhere gets really boring.
Four, today it’s really difficult—nay, still rather impossible—to build such applications.
Five, let’s make it cheap and easy using all the techniques and tools we’ve built up.
The Secret Life of WiFi The future is clearly wireless, and while many big businesses live by concentrating data in their clouds, the future doesn’t look quite so centralized.
The devices at the edges of our networks get smarter every year, not dumber.
They’re hungry for work and information to digest and profit from.
And they don’t drag cables around, except once a night.
It’s all wireless and more and more, it’s 802.11-branded WiFi of different alphabetical flavors.
Why Mesh Isn’t Here Yet As such a vital part of our future, WiFi has a big problem that’s not often discussed, but that anyone betting on it needs to be aware of.
The phone companies of the world have built themselves nice, profitable mobile phone cartels in nearly every country with a functioning government, based on convincing governments that without monopoly rights to airwaves and ideas, the world would fall apart.
Technically, we call this “regulatory capture” and “patents,” but in fact it’s just a form of blackmail and corruption.
If you, the state, give me, a business, the right to overcharge, tax the market, and ban all real competitors, I’ll give you 5%
But WiFi snuck past this, borrowing unlicensed airspace and riding on the back of the open and unpatented and remarkably innovative Internet Protocol stack.
So today, we have the curious situation where it costs me several euros a minute to call from Seoul to Brussels if I use the state-backed infrastructure that we’ve subsidized over decades, but nothing at all if I can find an unregulated WiFi access point.
Oh, and I can do video, send files and photos, and download entire home movies all for the same amazing price point of precisely zero point zero zero (in any currency you like)
God help me if I try to send just one photo to my home using the service for which I actually pay.
That would cost me more than the camera I took it on.
This is the price we pay for having tolerated the “trust us, we’re the experts” patent system for so long.
But more than that, it’s a massive economic incentive to chunks of the technology sector—and especially chipset makers who own patents on the anti-Internet GSM, GPRS, 3G, and LTE stacks, and who treat the telcos as prime clients—to actively throttle WiFi development.
And of course, it’s these firms that bulk out the IEEE committees that define WiFi.
The reason for this rant against lawyer-driven “innovation” is to steer your thinking toward the question, “What if WiFi were really free?” This will happen one day, not too far off, and it’s worth betting on.
We’ll see several things happen: first, much more aggressive use of airspace, especially for near-distance communications where there is no risk of interference; second, big capacity improvements as we learn to use more airspace in parallel; third, acceleration of the standardization process; and last, broader support in devices for really interesting connectivity.
Right now, streaming a movie from your phone to your TV is considered “leading edge.” This is ridiculous.
How about a stadium of people watching a game, sharing photos and HD video with each other in real time, creating an ad hoc event that literally saturates the airspace with a digital frenzy? I should be able to collect terabytes of imagery from those around me, in an hour.
To get more serious, why is our digital society in the hands of central points that are monitored, censored, logged, used to track who we talk to and to collect evidence against us, and then shut down when the authorities decide we have too much free speech? The loss of privacy we’re living through is only a problem when it’s one-sided, but then the problem is calamitous.
It’s how the Internet was designed, and it’s quite feasible, technically.
Some Physics Naive developers of distributed software treat the network as infinitely fast and perfectly reliable.
While this is approximately true for simple applications running over Ethernet, WiFi rapidly proves the difference between magical thinking and science.
That is, WiFi breaks so easily and dramatically under stress that I sometimes wonder how anyone would dare use it for real work.
The ceiling moves up as WiFi gets better, but never fast enough to stop us hitting it.
To understand how WiFi performs technically, you need to understand a basic law of physics: the power required to connect two points increases according to the square of the distance.
People who grow up in larger houses have exponentially louder voices, as I learned in Dallas.
For a WiFi network this means that as two radios get further apart, they have to either use more power or lower their signal rate.
There’s only so much power you can pull out of a battery before users treat the device as hopelessly broken.
Thus, even though a WiFi network may be rated at a certain speed, the real bit rate between the access point (AP) and a client depends on how far apart the two are.
As you move your WiFi-enabled phone away from the AP, the two radios trying to talk to each other will first increase their power and then reduce their bit rate.
This effect has some consequences of which we should be aware if we want to build robust distributed applications that don’t dangle wires behind them like puppets:
If you have a group of devices talking to an AP, when the AP is talking to the slowest device, the whole network has to wait.
It’s like having to repeat a joke at a party to the designated driver who has no sense of humor, is still fully and tragically sober, and has a poor grasp of language.
If you use unicast TCP and send a message to multiple devices, the AP must send the packets to each device separately, Yes, you knew this; it’s also how Ethernet works.
But now understand that one distant (or low-powered) device will force everything to wait for that slowest device to catch up.
As you try to put more devices onto an AP, performance rapidly gets worse, to the point where adding one more device can break the whole network for everyone.
What’s the Current Status? Despite its uncomfortable role as enterprise technology that somehow escaped into the wild, WiFi is already useful for more than making a free Skype call.
It’s not ideal, but it works well enough to let us solve some interesting problems.
Every packet has to go from client A to AP, then to client B.
You cut your bandwidth by 50%—but that’s only half the problem.
If A and B are very close together but both are far from the AP, they’ll both be using a low bit rate.
Imagine your AP is in the garage, and you’re in the living room trying to stream video from your phone to your TV.
There is an old “ad hoc” mode that lets A and B talk to each other, but it’s way too slow for anything fun, and of course, it’s disabled on all mobile chipsets.
Actually, it’s disabled in the top-secret drivers that the chipset makers kindly provide to hardware makers.
There is also a new Tunneled Direct Link Setup (TDLS) protocol that lets two devices create a direct link, using an AP for discovery but not for traffic.
And there’s a “5G” WiFi standard (it’s a marketing term, so it goes in quotes) that boosts link speeds to a gigabit.
I assume TDLS will be restricted in various ways so as to placate the telcos.
Mesh removes the access point completely, at least in the imaginary future where it exists and is widely used.
Devices talk to each other directly and maintain little routing tables of neighbors that let them forward.
Imagine the AP software embedded into every device, but smart enough (it’s not as impressive as it sounds) to do multiple hops.
No one who is making money from the mobile data extortion racket wants to see 802.11s available, because citywide mesh is such a nightmare for the bottom line, so it’s happening as slowly as possible.
The only large organization with the power (and, I assume the surface-to-surface missiles) to get mesh technology into wide use is the US Army.
The AP is clearly a bottleneck; you cannot get better than half of its advertised speed even if you put A and B literally beside the AP.
Worse, if there are other APs in the same airspace, they’ll shout each other out.
In my home, WiFi barely works at all because the neighbors two houses down have an AP that they’ve amplified.
Even on a different channel, it interferes with our home WiFi.
In the cafe where I’m sitting now there are over a dozen networks.
Realistically, as long as we’re dependent on AP-based WiFi, we’re subject to random interference and unpredictable performance.
There’s no inherent reason that WiFi, when idle, is hungrier than Bluetooth, for example.
The main difference is in tuning and in the protocols.
For wireless power-saving to work well, devices have to mostly sleep, and beacon out to other devices only once every so often.
For this to work, they need to synchronize their clocks.
This happens properly for the mobile phone part, which is why my old flip phone can run five days on a charge.
Current power amplifier technology is also inefficient, meaning you draw a lot more energy from your battery than you pump into the air (the waste turns into a hot phone)
Power amplifiers are improving as people focus more on mobile WiFi.
If we can’t trust centralized APs, and if our devices are smart enough to run full operating systems, can’t we make them work as APs? I’m so glad you asked that question.
Especially since we can switch this on and off in software, on a modern OS like Android.
Again, the villains of the piece are the US telcos, who mostly detest this feature and kill it or cripple it on the phones they control.
Smarter telcos realize that it’s a way to amplify their “last mile” and bring higher-value products to more users, but crooks don’t compete on smarts.
Discovery One of the great things about short-range wireless is the proximity.
WiFi maps closely to the physical space, which maps closely to how we naturally organize.
In fact, the Internet is quite abstract, and this confuses a lot of people who “kind of get it” but in fact don’t really.
With WiFi, we have technical connectivity that is potentially supertangible.
You see what you get and you get what you see.
Tangible means easy to understand, and that should mean love from users instead of the typical frustration and quiet seething hatred.
Say we have a bunch of WiFi radios in a room, happily beaconing to each other.
For lots of applications it makes sense that they can find each other and start chatting, without any user input.
After all, most real-world data isn’t private, it’s just highly localized.
Preemptive Discovery over Raw Sockets I’m in a hotel room in Gangnam, Seoul, with a 4G wireless hotspot, a Linux laptop, and a couple of Android phones.
Dynamic Host Control Protocol (DHCP) servers tend to dish out addresses in sequence, so my phones are probably close by, numerically speaking:
Now I ping myself, just to try to double-check things:
The response time is a bit faster now, which is what we’d expect.
That’s the second phone, with the same kind of latency as the first one.
Let’s continue, and see if there are any other devices connected to the hotspot:
Now, ping uses raw IP sockets to send ICMP_ECHO messages.
The useful thing about ICMP_ECHO is that it gets a response from any IP stack that has not deliberately had echo switched off.
That’s still a common practice on corporate websites who fear the old “ping of death” exploit, where malformed messages could crash the machine.
I call this pre-emptive discovery since it doesn’t take any cooperation from the device.
We don’t rely on any cooperation from the phones to see them sitting there; as long as they’re not actively ignoring us, we can see them.
However, sadly, there’s a fatal flaw in my idea of using ICMP_ECHO to discover devices.
Opening a raw IP socket requires root privileges on a POSIX box.
We can get the power to open raw sockets on Linux by giving sudo privileges to our command (ping has the so-called sticky bit set)
But on a mobile OS like Android, it requires root access (i.e., rooting the phone or tablet)
That’s out of the question for most people, so ICMP_ECHO is out of reach for most devices.
The next step most people take is UDP multicast or broadcast.
Cooperative Discovery Using UDP Broadcasts Multicast tends to be seen as more modern and “better” than broadcast.
In IPv6, broadcast doesn’t work at all: you always have to use multicast.
Nonetheless, all IPv4 local network discovery protocols end up using UDP broadcast anyhow.
The reason: broadcast and multicast end up working much the same, except broadcast is simpler and less risky.
Multicast is seen by network admins as kind of dangerous, as it can leak over network segments.
This code uses a single socket to broadcast 1-byte messages and receive anything that other nodes are broadcasting.
When I run it, it shows just one node, which is itself:
If I switch off all networking and try again, sending a message fails, as I’d expect: Pinging peers...
Working on the basis of solve the problems currently aiming at your throat, let’s fix the most urgent issues in this first model.
On the one hand, this broadcast address means precisely “send to all nodes on the local network, and don’t forward.” However, if you have several interfaces (wired Ethernet, WiFi), broadcasts will go out on your default route only, and via just one interface.
What we want to do is either send our broadcast on each interface’s broadcast address, or find the WiFi interface and its broadcast address.
Like many aspects of socket programming, getting information on network interfaces is not portable.
Do we want to write nonportable code in our applications? No, this is better hidden in a library.
There’s no handling for errors except “abort,” which is too brutal for transient problems like “your WiFi is switched off.” The code should distinguish between soft errors (ignore and retry) and hard errors (assert)
The code needs to know its own IP address and ignore beacons that it sent out.
Like finding the broadcast address, this requires inspecting the available interfaces.
The simplest answer to these issues is to push the UDP code into a separate library that provides a clean API, like this:
Example 8-3 shows the refactored UDP ping program that calls this library, which is much cleaner and nicer.
The library, udplib, hides a lot of the unpleasant code (which will become uglier as we make this work on more systems)
I’m not going to print that code here, but you can read it in the repository.
Now there are more problems sizing us up and wondering if they can make lunch out of us.
In IPv6, broadcast doesn’t exist at all; one uses multicast.
From my experience with WiFi, IPv4 multicast and broadcast work identically except that multicast breaks in some situations where broadcast works fine.
The problem is that some access points do not forward multicast packets.
When you have a device (e.g., a tablet) that acts as a mobile AP, it’s possible it won’t get multicast packets, meaning it won’t see other peers on the network.
The simplest plausible solution is simply to ignore IPv6 for now, and use broadcast.
A perhaps smarter solution would be to use multicast, and deal with asymmetric beacons if they happen.
Multiple Nodes on One Device So, we can discover nodes on the WiFi network, as long as they’re sending out beacons as we expect.
This is correct; the semantics of two readers on one socket would be weird, to say the least.
If I use that, I can bind several processes to the same UDP port, and they will all receive any message arriving on that port.
This is great because the next thing I want to do is design an API and then start dozens of nodes to see them discovering each other.
It would be really cumbersome to have to test each node on a separate device.
And when we get to testing how real traffic behaves on a large, flaky network, the two alternatives are simulation or temporary insanity.
And I speak from experience: we were, this summer, testing on dozens of devices at once.
It takes about an hour to set up a full test run, and you need a space shielded from WiFi interference if you want any kind of reproducibility (unless your test case is “prove that interference kills WiFi networks faster than Orval can kill a thirst”)
If I was a whizz Android developer with a free weekend I’d immediately (as in, it would take me two days) port this code to my phone and get it sending beacons to my PC.
I like being able to start a dozen threads from one process, and have each thread act like an independent node.
I like not having to work in a real Faraday cage when I can simulate one on my laptop.
If I have multiple nodes on one device, I clearly can’t use the IP address and port number as the node address.
Arguably, the node identifier only has to be unique within the context of the device.
My mind fills with complex stuff I could make, like supernodes that sit on real UDP ports and forward messages to internal nodes.
I hit my head on the table until the idea of inventing new concepts leaves it.
Experience tells us that WiFi does things like disappear and reappear while applications are running.
Users click on things, with interesting results like changing the IP address halfway through a session.
We cannot depend on IP addresses, nor on established connections (in the TCP fashion)
We need some long-lasting addressing mechanism that survives interfaces and connections being torn down and then recreated.
Here’s the simplest solution I can see: we give every node a UUID, and we specify that nodes, represented by their UUIDs, can appear or reappear at certain IP address:port endpoints, and then disappear again.
At the least, we need a way to distinguish between the node object that is “us,” and node objects that are our peers.
We’ll be doing things like creating an “us” and then asking it how many peers it knows about, and who they are.
From the developer’s point of view, a node (the application) needs a way to talk to the outside world.
Let’s borrow a term from networking and call this an “interface.” The interface represents us to the rest of the world and presents the rest of the world to us, as a set of other peers.
When we want to talk to a peer, we get the interface to do that for us.
And when a peer talks to us, it’s the interface that delivers us the message.
The interface has to be multithreaded, so that one thread can do I/O in the background, while the foreground API talks to the application.
We used this design in the Clone and Freelance client APIs.
The interface background thread does the discovery business: bind to the UDP port, send out UDP beacons, and receive beacons.
We need to at least send UUIDs in the beacon message so that we can distinguish our own beacons from those of our peers.
We need to track peers that appear and that disappear.
For this I’ll use a hash table that stores all known peers, and expire peers after some timeout.
The third iteration of the UDP ping program, shown in Example 8-4, is even simpler and more beautiful than the second.
The main body, in C, is just 10 lines of code.
The interface code (Example 8-5) should be familiar if you’ve studied how we make multithreaded API classes.
Example 8-6 presents the constructor and destructor for the interface class.
Note that the class has barely any properties; it is just an excuse to start the background thread, and a wrapper around zmsg_recv()
In Example 8-7, we wait for a message from the interface.
This returns us a zmsg_t object, or NULL if interrupted.
The structure in Example 8-8 defines each peer that we discover and track.
The constructor and destructor for the peer class are shown in Example 8-9
The methods in Example 8-10 return the peer’s UUID in binary format or as a printable string.
Just resets the peer's expiration time; we call this method //  whenever we get any activity from a peer.
Peer hash calls this handler automatically whenever we delete //  peer from agent peers, or destroy that hash table.
The structure in Example 8-11 holds the context for our agent, so we can pass that around cleanly to methods that need it.
The constructor and destructor for our agent are presented in Example 8-12
Each interface has one agent object, which implements its background thread.
Example 8-13 shows how we handle a beacon coming into our UDP socket; this may be from other peers or an echo of our own broadcast beacon.
The method in Example 8-14 checks one peer item for expiry; if the peer hasn’t sent us anything by now, it’s “dead” and we can delete it.
The main loop for the background agent is shown in Example 8-15
It uses zmq_poll() to monitor the frontend pipe (commands from the API) and the backend UDP handle (beacons)
When I run this in two windows, it reports one peer joining the network.
If kill that peer, a few seconds later it tells me the peer has left:
On Ethernet, five seconds (the expiry time I used in this code) seems like a lot.
On a badly stressed WiFi network you can get ping latencies of 30 seconds or more.
If you use a too-aggressive value for the expiry, you’ll disconnect nodes that are still there.
On the other side, end user applications expect a certain liveliness.
If it takes 30 seconds to report that a node has gone, users will get annoyed.
A decent strategy is to detect and report disappeared nodes rapidly, but only delete them after a longer interval.
Visually, a node would be green when it’s alive, then gray for a while as it went out of reach, then finally disappear.
We’re not doing this now, but we will do it in the real implementation of the as-yet-unnamed framework we’re making.
As we will also see later, we have to treat any input from a node, not just UDP beacons, as a sign of life.
It’s not ideal, but it works for the local networks we have today.
However, we can’t use UDP for real work, not without additional work to make it reliable.
There’s a joke about UDP, but sometimes you’ll get it, and sometimes you won’t.
There is one more use case for UDP after discovery, which is multicast file distribution.
I’ll explain why and how, then shelve that for another day.
The why is simple: what we call “social networks” are just augmented culture.
We create culture by sharing, and this means more and more sharing works that we make or remix—photos, documents, contracts, tweets.
The clouds of devices we’re aiming toward do more of this, not less.
One is the pub-sub pattern, where one node sends out content to a set of other nodes, all at the same time.
The second is the late joiner pattern, where a node arrives somewhat later and wants to catch up to the conversation.
We can deal with the late joiner using TCP unicast, but doing TCP unicast to a group of clients at the same time has some disadvantages.
Second, it’s unfair since some will get the content before others.
Before you jump off to design a UDP multicast protocol, realize that it’s not a simple calculation.
When you send a multicast packet, the WiFi access point uses a low bit rate, to ensure that even the furthest devices will get it safely.
Most normal APs don’t do the obvious optimization, which is to measure the distance to the furthest device and use that bit rate.
So, if you have a few devices close to the AP, multicast will be insanely slow.
But if you have a roomful of devices that all want to get the next chapter of the book, multicast can be insanely effective.
The curves cross at around 6–12 devices, depending on the network.
You could in theory measure the curves in real time and create an adaptive protocol.
That would be cool, but probably too hard for even the smartest of us.
Spinning Off a Library Project At this stage the code is growing larger than an example should be, so it’s time to create a proper GitHub project.
It’s a rule: build your projects in public view and tell people.
I explained in Chapter 6 about growing communities around projects.
The trademarks of the 21st century are domain names, so the first thing I do when spinning off a project is to look for a domain name that might work.
Quite randomly, one of our old mobile projects was called “Zyre,” and I have the domain names for it.
So, we take the FileMQ project we built in Chapter 7 as a template for a new GitHub project.
The GNU autoconf tools are quite decent but have a painful syntax.
It’s easiest to copy existing project files, and modify them.
The FileMQ project builds a library and has test tools, license files, man pages, and so on.
It’s not too large, so it’s a good starting point.
I put together a README to summarize the goals of the project and point to C4
The issue tracker is enabled by default on new GitHub projects, so once we’ve pushed the.
However, it’s always good to recruit more maintainers, so I create an issue, “Call for maintainers,” that says:
By default, GitHub offers the usual variety of issue types, but with C4 we don’t use them.
Instead, we need just two labels (“Urgent,” in red, and “Ready,” in black)
Point-to-Point Messaging We’re going to take our last UDP ping program and build a point-to-point messaging layer on top of that.
Our goal is to be able to detect peers as they join and leave the network, send messages to them, and get replies.
It is a nontrivial problem to solve, and it takes Min and me two days to get a “Hello World” version working.
I’ll explain these in enough detail that you understand why we made each choice we did, with some code fragments to illustrate.
However, we have a few more aspects to get working before this will work in real use:
We need some protocol identification so that we can check for, and reject, invalid packets.
We need some version information so that we can change this protocol over time.
We probably want a fixed protocol header that will never change in future versions, and a body that depends on the version (Figure 8-1)
The C language (and a few others, like Erlang) makes it simple to read and write binary structures.
Here is how we send a beacon, using the zre_udp class to do the non-portable network calls:
When we receive a beacon, we need to guard against bogus data.
We’re not going to be paranoid against, for example, denial-of-service attacks.
We just want to make sure we’re not going to crash when a bad ZRE implementation sends us erroneous frames.
To validate a frame we check its size and header.
If those are OK, we assume the body is usable.
When we get a UUID that isn’t ourselves (recall, we’ll get our own UDP broadcasts back), we can treat this as a peer:
Experience teaches us that ROUTER-to-ROUTER is particularly difficult to use successfully.
At a minimum, one peer must bind and one must connect, meaning the architecture is not symmetrical.
But also, you simply can’t tell when you are allowed to safely send a message to a peer.
It’s a Catch-22: you can talk to a peer after it’s talked to.
One side or the other will be losing messages and thus has to retry, which means the peers cannot be equal.
I’m going to explain the Harmony pattern, which solves this problem, and which we use in Zyre.
But if we have one DEALER per peer, and we connect that DEALER to the peer, we can safely send messages to a peer as soon as we’ve connected to it.
Now, the next problem is to know who sent us a particular message.
We need a reply address, which is the UUID of the node that sent any given message.
One ROUTER socket that we bind to an ephemeral port, which we broadcast in our beacons.
One DEALER socket per peer that we connect to the peer’s ROUTER socket.
We can get the first beacon from a peer after we start to receive messages from it.
A message comes in on the ROUTER socket and has a nice UUID attached to it, but no physical IP address and port.
To do this, our first command to any new peer we connect to is an OHAI command with our IP address and port.
This ensures that the receiver connects back to us before trying to send us any command.
If we receive a UDP beacon, we connect to the peer.
We read messages from our ROUTER socket, and each message comes with the.
We send messages to each peer using a dedicated per-peer DEALER socket, which must be connected.
When we connect to a peer, we also tell our application that the peer exists.
Every time we get a message from a peer, we treat that as a heartbeat (it’s alive)
If we were not using UDP but some other discovery mechanism, I’d still use the Harmony pattern for a true peer network: one ROUTER for input from all peers, and one DEALER per peer for output.
Bind the ROUTER, connect the DEALER, and start each conversation with an OHAI equivalent that provides the return IP address and port.
We would need some external mechanism to bootstrap each connection.
So if we kill peers when they go quiet, we’ll have false disconnections.
Since UDP beacons aren’t reliable, it’s tempting to add in TCP beacons.
Imagine you have 100 nodes on a network, and each node sends a TCP beacon once a second.
Each beacon is 22 bytes, not counting TCP’s framing overhead.
That’s about 1–2% of a typical WiFi network’s ideal capacity, which sounds OK.
But when a network is stressed, or fighting other networks for airspace, that extra 200K a second will break what’s left.
So what we do is switch to TCP heartbeats only when a specific peer hasn’t sent us any UDP beacons in a while.
And then, we send TCP heartbeats only to that one peer.
If the peer continues to be silent, we conclude it’s gone away.
If the peer comes back, with a different IP address and/or port, we have to disconnect our DEALER socket and reconnect to the new port.
This gives us a set of states for each peer, though at this stage the code doesn’t use a formal state machine:
Peer visible thanks to UDP beacon (we connect using IP address and port from beacon)
Peer visible thanks to OHAI command (we connect using IP address and port from command)
Peer seems alive (we got a UDP beacon or command over TCP recently)
Peer seems quiet (no activity in some time, so we send a HUGZ command)
Peer has disappeared (no reply to our HUGZ commands, so we destroy peer)
There’s one remaining scenario we haven’t addressed in the code at this stage.
It’s possible for a peer to change IP addresses and ports without actually triggering a disappearance event.
For example, if the user switches off WiFi and then switches it back on, then the the access point can assign the peer a new IP address.
We’ll need to handle a disappeared WiFi interface on our node by unbinding the ROUTER socket and rebinding it when we can.
Since this is not central to the design now, I decided to log an issue on the GitHub tracker and leave it for a rainy day.
Group Messaging Group messaging is a common and very useful pattern.
The concept is simple: instead of talking to a single node, you talk to a “group” of nodes.
The group is just a name, a string that you agree on in the application.
It’s precisely like using the publish-subscribe prefixes in PUB and SUB sockets.
In fact, the only reason I say “group messaging” and not “pub-sub” is to prevent confusion, since we’re not going to use PUB-SUB sockets for this.
But we’ve just done such a lot of work to solve the late joiner problem.
Applications are inevitably going to wait for peers to arrive before sending messages to groups, so we have to build on the Harmony pattern rather than start again beside it.
Let’s look at the operations we want to do on groups:
We want to know what other nodes are in any given group.
We want to send a message to (all nodes in) a group.
These will look familiar to anyone who’s used Internet Relay Chat (IRC), except we have no server.
Every node will need to keep track of what each group represents.
This information will not always be fully consistent across the network, but it will be close enough.
Our interface will track a set of groups (each an object)
These are all the known groups with one or more member node, excluding ourselves.
Since nodes can join the network at any time, we have to tell new peers what groups we’re in.
When a peer disappears, we’ll remove it from all groups we know about.
We send this to all peers when we join a group.
Plus, we add a groups field to the first command we send (renamed from OHAI to HELLO at this point because I need a larger lexicon of command verbs)
Lastly, let’s add a way for peers to double-check the accuracy of their group data.
The risk is that we miss one of the above messages.
Though we are using Harmony to avoid the typical message loss at startup, it’s worth being paranoid.
For now, all we need is a way to detect such a failure.
We’ll deal with recovery later, if the problem actually happens.
What we want is a rolling counter that simply tells us how many join and leave operations (“transitions”) there have been for a node.
It starts at 0 and increments for each group we join or leave.
We will also put the transitions counter into the JOIN, LEAVE, and HELLO commands.
And to try to provoke the problem, we’ll test by joining/leaving several hundred groups, with a high-water mark set to 10 or so.
We need a command that means “talk to one peer” and one that means “talk to many peers.” After some attempts, my best choices are WHISPER and SHOUT, and this is what the code uses.
The SHOUT command needs to tell the user the group name, as well as the sender peer.
Since groups are like publish-subscribe, you might be tempted to use this to broadcast the JOIN and LEAVE commands as well, perhaps by creating a “global” group that all nodes join.
My advice is to keep groups purely as user-space concepts, for two reasons.
First, how do you join the global group if you need the global group to send out a JOIN command? Second, it creates special cases (reserved names) that are messy.
It’s simpler just to send JOINs and LEAVEs explicitly to all connected peers, period.
I’m not going to work through the implementation of group messaging in detail, since it’s fairly pedantic and not exciting.
The data structures for group and peer management aren’t optimal, but they’re workable.
A list of groups for our interface, which we can send to new peers in a HELLO command.
A hash of groups for other peers, which we update with information from HELLO, JOIN, and LEAVE commands.
A hash of peers for each group, which we update with the same three commands.
At this stage I’m starting to get pretty happy with the binary serialization (our codec generator from Chapter 7), which handles lists and dictionaries as well as strings and integers.
This version is tagged in the repository as v0.2.0, and you can download the tarball if you want to check what the code looked like at this stage.
Testing and Simulation When you build a product out of pieces, and this includes a distributed framework like Zyre, the only way to know that it will work properly in real life is to simulate real activity on each piece.
On Assertions The proper use of assertions is one of the hallmarks of a professional programmer.
Our confirmation bias as creators makes it hard to test our work properly.
We tend to write tests to prove the code works, rather than trying to prove it doesn’t.
We pretend to ourselves and others that we can be (could be) perfect, when in fact we consistently make mistakes.
Bugs in code are seen as “bad” rather than “inevitable,” so psychologically we want to see fewer of them, not uncover more of them.
Some cultures teach us to aspire to perfection, and punish mistakes, in education and work, which makes this attitude worse.
To accept that we’re fallible, and then to learn how to turn that into profit rather than shame, is one of the hardest intellectual exercises in any profession.
We leverage our fallibilities by working with others, and by challenging our own work sooner, not later.
One trick that makes it easier is to use assertions.
The code asserts, “At this point, such and such must be true,” and if the assertion fails, the code kills itself.
The faster you can prove code incorrect, the faster and more accurately you can fix it.
Believing that code works and proving that it behaves as expected is less science, and more magical thinking.
It’s far better to be able to say, “libzmq has 500 assertions and despite all my efforts, not one of them fails.”
So, the Zyre code base is scattered with assertions, and particularly a couple on the code that deals with the state of peers.
This is the hardest aspect to get right: peers need to track each other and exchange state accurately, or things stop working.
The algorithms depend on asynchronous messages flying around, and I’m pretty sure the initial design has flaws.
And as I test the original Zyre code by starting and stopping instances of zre_ping by hand, every so often I get an assertion failure.
Running by hand doesn’t reproduce these often enough, so let’s make a proper tester tool.
That confirmation bias engineers have to their own work makes up-front testing incredibly profitable, and latestage testing incredibly expensive.
I’ll tell you a short story about a project we worked on in the late 1990s.
We provided the software, and other teams the hardware, for a factory automation project.
Three or four teams brought their experts on-site, which was a remote factory (funny how the polluting factories are always in a remote border country)
One of these teams, a firm specializing in industrial automation, built ticket machines: kiosks, and software to run on them.
Nothing unusual: swipe a badge, choose an option, receive a ticket.
They assembled two of these kiosks on-site, each week bringing some more bits and pieces.
The stuff had to be resistant against dust since the kiosks sat outside.
The internals of the kiosk were just sat on wooden shelving.
A year later, a second factory, and the same story.
So when it came to the third and largest factory, a year later, we jumped up and said, “Please let us make the kiosks and the software and everything.”
We made a detailed design for the software and hardware and found suppliers for all the pieces.
It took us three months to search the Internet for each component, and another two months to get them assembled into stainless-steel bricks, each weighing about 20 kilos.
You loaded up the paper bin with enough for six months, then screwed the brick into a housing, and it automatically booted, found its DNS server, and loaded its Linux OS and then the application software.
It connected to the real server and showed the main menu.
You got access to the configuration screens by swiping a special badge and then entering a code.
The software was portable, so we could test that as we wrote it, and as we collected the pieces from our suppliers we kept one of each so we had a disassembled kiosk to play with.
When we got our finished kiosks, they all worked immediately.
We spent a week or so on-site, and in 10 years, only one kiosk broke (the screen died and was replaced)
The lesson is, test up-front so that when you plug the thing in, you know precisely how it’s going to behave.
If you haven’t tested it up-front, you’re going to be spending weeks or months in the field, ironing out problems that should never have been there.
Since I don’t believe in magic, that meant the code was still wrong somewhere.
So, the next step was heavy-duty testing of the Zyre v0.2.0 code to try to break its assertions, and get a good idea of how it would behave in the field.
The first version of the tester consists of a main thread that starts and stops a set of child threads, each running one interface, each with a ROUTER, DEALER, and UDP socket (R, D, and U in Figure 8-2)
The nice thing is that when I am connected to a WiFi access point, all Zyre traffic (even between two interfaces in the same process) goes across the AP.
This means I can fully stress-test any WiFi infrastructure with just a couple of PCs running in a room.
It’s hard to emphasize how valuable this is: if we had built Zyre as, say, a dedicated service for Android, we’d literally need dozens of Android tablets or phones to do any large-scale testing.
The focus is now on breaking the current code, trying to prove it wrong.
There’s no point at this stage in testing how well it runs, how fast it is, how much memory it uses, or anything else.
We’ll work up to trying (and failing) to break each individual functionality, but first we’ll try to break some of the core assertions I’ve put into the code.
The first command that any node receives from a peer MUST be HELLO.
In other words, messages cannot be lost during the peer-to-peer connection process.
The state each node calculates for its peers matches the state each peer calculates for itself.
In other words, again, no messages are lost in the network.
Now, I think Harmony gets around all these potential cases.
So, the first version of the tester simulates an unstable and dynamic network, where nodes come and go randomly.
Here is the main thread of the tester, which manages a pool of 100 threads, starting and stopping each one randomly.
Every ~750 msec it either starts or stops one random thread.
We randomize the timing so that threads aren’t all synchronized.
After a few minutes we have an average of 50 threads happily chatting to each other like Korean teenagers in Gangnam subway station:
Note that we maintain a pipe to each child thread (CZMQ creates the pipe automatically when we use the zthread_fork() method)
It’s via this pipe that we tell child threads to stop, when it’s time for them to leave.
The child threads do the following (I’m switching to pseudo-code for clarity):
Getting nodes to agree on consistent group status was the most difficult.
Every node needs to track the group membership of the whole network, as I already explained in the section “Group Messaging.” Group messaging is a publish-subscribe pattern.
It’s essential that none of these ever get lost, or we’ll find nodes dropping randomly off groups.
So, each node counts the total number of JOINs and LEAVEs it’s ever done, and broadcasts this status (as a 1-byte rolling counter) in its UDP beacon.
Other nodes pick up the status and compare it to their own calculations, and if there’s a difference, the code asserts.
The first problem was that UDP beacons get delayed randomly, so they’re useless for carrying the status.
When a beacon arrives late, the status is inaccurate and we get a false negative.
To fix this we moved the status information into the JOIN and LEAVE commands.
Get initial status for a peer from its HELLO command.
When getting a JOIN or LEAVE from a peer, increment the status counter.
Check that the new status counter matches the value in the JOIN or LEAVE command.
The next problem we got was that messages were arriving unexpectedly on new connections.
The Harmony pattern connects, then sends HELLO as the first command.
This means the receiving peer should always get a HELLO as the first command from a new peer.
Instead, we were seeing PING, JOIN, and other commands arriving first.
This turned out to be due to CZMQ’s ephemeral port logic.
An ephemeral port is just a dynamically assigned port that a service can get rather than asking for a fixed port number.
CZMQ’s logic is to look for a free port in this range, bind to that, and return the port number to the caller.
We have a stateful protocol that always starts with a HELLO command.
We know that it’s possible for peers to connect to us, thinking we’re an existing node that went away and came back, and send us other commands.
Step one is, when we discover a new peer, to destroy any existing peer connected to the same endpoint.
It’s not a full answer, but it’s polite, at least.
Step two is to ignore anything coming in from a new peer until that peer says HELLO.
In fact, if you draw this on a piece of paper and think it through, you’ll see that you never get a HELLO from such a connection.
The peer will send PINGs and JOINs and LEAVEs and then eventually time out and close, as it fails to get any heartbeats back from us.
You’ll also see that there’s no risk of confusion, no way for commands from two peers to get mixed into a single stream on our DEALER socket.
When you are satisfied this works, we’re ready to move on.
This version is tagged in the repository as v0.3.0 and you can download the tarball if you want to check what the code looked like at this stage.
Note that doing heavy simulation of lots of nodes will probably cause your process to run out of file handles, giving an assertion failure in libzmq.
I raised the per-process limit to 30,000 by running (on my Linux box):
Tracing Activity To debug the kinds of problems we saw here, we need extensive logging.
There’s a lot happening in parallel, but every problem can be traced down to a specific exchange between two nodes, consisting of a set of events that happen in strict sequence.
We know how to make very sophisticated logging, but as usual it’s wiser to make just what we need, no more.
The very simplest technique is to print the necessary information to the console, with a timestamp.
Then it’s simple to find the nodes affected by a failure, filter the log file for only messages referring to them, and see exactly what happened.
With Zyre we want to distribute messages to a set of peers, and we want to do this fairly.
Using a single ROUTER socket for output would be problematic, since any one blocked peer would block outgoing traffic to all peers.
And we’re using a separate DEALER socket to talk to each peer, so in theory each DEALER socket will send its queued messages in the background reasonably fairly.
The normal behavior of a DEALER socket that hits its high-water mark is to block.
This is usually ideal, but it’s a problem for us here.
Our current interface design uses one thread that distributes messages to all peers.
If one of those send calls were to block, all output would block.
One is to use zmq_poll() on the whole set of DEALER sockets, and only write to sockets that are ready.
First, the DEALER socket is hidden inside the peer class, and it is cleaner to allow each class to handle this opaquely.
Second, what do we do with messages we can’t yet deliver to a DEALER socket? Where do we queue them? Third, it seems to be sidestepping the issue.
If a peer is really so busy it can’t read its messages, something is wrong.
We can then provide each peer with a reasonable outgoing queue (the HWM) and, if that gets full, treat it as a fatal error on that peer.
If we’re sending large chunks—e.g., for content distribution—we’ll need a credit-based flow control mechanism on top.
Therefore, the first step is to prove to ourselves that we can turn the normal blocking DEALER socket into a non-blocking socket.
When we run this, we send four messages successfully (they go nowhere, the socket just queues them), and then we get a nice EAGAIN error:
The next step is to decide what a reasonable high-water mark would be for a peer.
Zyre is meant for human interactions; that is, applications that chat at a low frequency, such as two games or a shared drawing program.
I’d expect a hundred messages per second to be quite a lot.
Here’s how we configure a new DEALER socket for a peer:
And finally, what do we do when we get an EAGAIN on a peer? We don’t need to go through all the work of destroying the peer since the interface will do this automatically if it doesn’t get any message from the peer within the expiry timeout.
Just dropping the last message seems very weak, though: it will give the receiving peer gaps.
Brutal is good because it forces the design to a “good” or “bad” decision rather than a fuzzy “should work, but to be honest there are a lot of edge cases so let’s worry about it later.” Destroy the socket, disconnect the peer, and stop sending anything to it.
The peer will eventually have to reconnect and reinitialize any state.
It’s kind of an assertion that 100 messages a second is enough for anyone.
Distributed Logging and Monitoring Let’s look at logging and monitoring.
If you’ve ever managed a real server (like a web server) you know how vital it is to have a capture of what is going on.
To see what kinds of work are done the most, to optimize performance.
Let’s scope this in terms of the problems we think we’ll have to solve:
We want to track key events (such as nodes leaving and rejoining the network)
For each event, we want to track a consistent set of data: the date/time, node that.
We want to be able to switch logging on and off at any time.
We want to be able to process log data mechanically, since it will be sizable.
We want to be able to monitor a running system; that is, collect logs and analyze.
We want log traffic to have minimal effect on the network.
We want to be able to collect log data at a single point on the network.
As in any design, some of these requirements are hostile to each other.
For example, collecting log data in real time means sending it over the network, which will affect network traffic to some extent.
However, as in any design these requirements are also hypothetical until we have running code, so we can’t take them too seriously.
We’ll aim for plausibly good enough and improve over time.
A Plausible Minimal Implementation Arguably, just dumping log data to disk is one solution, and it’s what most mobile applications do (using “debug logs”)
But most failures require correlation of events from two nodes.
This means searching lots of debug logs by hand to find the ones that matter.
We want to send log data somewhere central, either immediately or opportunistically (i.e., store and forward)
My first idea, when it comes to sending data, is to use Zyre for this: just send log data to a group called “LOG,” and hope someone collects it.
But using Zyre to log Zyre itself is a Catch-22
Who logs the logger? What if we want a verbose log of every message sent? Do we include logging messages in that, or not? It quickly gets messy.
We want a logging protocol that’s independent of Zyre’s main ZRE protocol.
The simplest approach is a PUB-SUB protocol, where all nodes publish log data on a PUB socket and a collector picks that up via a SUB socket (Figure 8-3)
A passive log collector that stores log data on disk for eventual statistical analysis.
This would be a PC with sufficient hard disk space for weeks or months of log data.
A collector that stores log data into a database where it can be used in real time by other applications.
This might be overkill for a small workgroup but would be snazzy for tracking the performance of larger groups.
The collector could collect log data over WiFi and then forward it over Ethernet to a database somewhere.
A live meter application that joins the Zyre network and then collects log data from nodes, showing events and statistics in real time.
That means as soon as a node connects to the collector it can start sending log data without loss.
How do we tell nodes what endpoint to connect to? We may have any number of collectors on the network, and they’ll be using arbitrary network addresses and ports.
We need some kind of service announcement mechanism, and here we can use Zyre to do the work for us.
We could use group messaging, but it seems neater to build service discovery into the ZRE protocol itself.
It’s nothing complex: if a node provides a service X, it can tell other nodes about that when it sends them a HELLO command.
We’ll extend the HELLO command with a headers field that holds a set of name=value pairs.
Let’s define that the header X-ZRELOG specifies the collector endpoint (the SUB socket)
A node that acts as a collector can add a header like this (for example):
When another node sees this header, it simply connects its PUB socket to that endpoint.
Log data now gets distributed to all collectors (zero or more) on the network.
Making this first version was fairly simple and took half a day.
Here are the pieces we had to make or change:
We made a new class, zre_log, that accepts log data and manages the connection to the collector, if any.
We added some basic management for peer headers, taken from the HELLO command.
When a peer has the X-ZRELOG header, we connect to the endpoint it specifies.
Where we were logging to stdout, we switched to logging via the zre_log class.
We extended the interface API with a method that lets the application set headers.
We wrote a simple logger application that manages the SUB socket and sets the XZRELOG header.
We send our own headers when we send a HELLO command.
This version is tagged in the Zyre repository as v0.4.0, and you can download the tarball if you want to check what the code looked like at this stage.
At this point the log message is just a string.
We’ll make more professionally structured log data in a little while.
Here’s the scenario I had, which caused a few minutes’ confusion.
As the tester created a new interface, that reused the dynamic port freed by the (just stopped) logger, and suddenly the interface began to receive log data from nodes on its mailbox.
We saw a similar situation before, where a new interface could reuse the port freed by an old interface and start getting old data.
The lesson is, if you use dynamic ports, be prepared to receive random data from illinformed applications that are reconnecting to you.
As I write this, libzmq doesn’t check socket types when connecting.
The ZMTP/2.0 protocol does announce each peer’s socket type, so this check is doable.
The ZRE protocol has no fail-fast (assertion) mechanism; we need to read and parse a whole message before realizing that it’s invalid.
Protocol Assertions As Wikipedia puts it, “Fail-fast systems are usually designed to stop normal operation rather than attempt to continue a possibly flawed process.” A protocol like HTTP has a fail-fast mechanism in that the first four bytes that a client sends to an HTTP server must be “HTTP”
If they’re not, the server can close the connection without reading anything more.
However, we can throw out the entire message if it’s not valid.
The problem is going to be worse when we use ephemeral ports, but it applies broadly to all protocols.
So, let’s define a protocol assertion as being a unique signature that we place at the start of each message, which identities the intended protocol.
When we read a message, we check the signature, and if it’s not what we expect we discard the message silently.
A good signature should be hard to confuse with regular data and give us enough space for a number of protocols.
The pattern %xAAA is meant to stay away from values we might otherwise expect to see at the start of a message: %x00, %xFF, and printable characters.
As our protocol codec is generated, it’s relatively easy to add this assertion.
Check if first two bytes are %xAAA with expected 4-bit signature.
If not, skip all “more” frames, get first frame, and repeat.
To test, I switched the logger back to using an ephemeral port.
The interface now properly detects and discards any messages that don’t have a valid signature.
If the message has a valid signature and is still wrong, that’s a proper bug.
Binary Logging Protocol Now that we have the logging framework working properly, let’s look at the protocol itself.
Sending strings around the network is simple, but when it comes to WiFi we really cannot afford to waste bandwidth.
We have the tools to work with efficient binary protocols, so let’s design one for logging.
In the first version we send UUID strings to identify each node.
We can send binary UUIDs, but it’s still verbose and wasteful.
In the log files we don’t care about the node identifiers.
So what’s the shortest identifier we can use that’s going to be unique enough for logging? I say “unique enough” because while we really want zero chance of duplicate UUIDs in the live code, log files are not so critical.
The simplest plausible answer is to hash the IP address and port into a 2-byte value.
To be sure, I generated 10,000 addresses across a small number of IP addresses (matching a simulation setup), and then across a large number of addresses (matching a real-life setup)
Over several runs I didn’t get any collisions, so this will work as an identifier for the log data.
This adds four bytes (two for the node recording the event, and two for its peer in events that come from a peer)
Next, we want to store the date and time of the event.
We’ll use this, as there’s no need for millisecond resolution in a log file: events are sequential, clocks are unlikely to be that tightly synchronized, and network latencies mean that precise times aren’t that meaningful.
Finally, we want to allow some additional data, formatted as text and depending on the type of event.
The codec does protocol assertions just like the main ZRE protocol does.
Code generation has a fairly steep starting curve, but it makes it so much easier to push your designs past “amateur” into “professional.”
Content Distribution We now have a robust framework for creating groups of nodes, letting them chat to each other, and monitoring the resulting network.
The next step is to allow them to distribute content as files.
As usual, we’ll aim for the very simplest plausible solution and then improve that stepby-step.
An application can tell the Zyre API, “Publish this file,” and provide the path to a file that exists somewhere in the filesystem.
Zyre will distribute that file to all peers—both those that are on the network at that time, and those that arrive later.
Each time an interface receives a file, it tells its application, “Here is this file.”
Each node is going to be a file publisher, and a file subscriber.
We bind the publisher to an ephemeral port (if we use the standard FileMQ port 5670, we can’t run multiple interfaces on one box), and we broadcast the publisher’s endpoint in the HELLO message, as we did for the log collector.
This lets us interconnect all nodes so that all subscribers talk to all publishers.
We need to ensure that each node has its own directory for sending and receiving files (the outbox and the inbox)
Again, this is so we can run multiple nodes on one box.
Since we already have a unique ID per node, we just use that in the directory name.
The last thing is to expose content distribution in the Zyre API.
A way for the application to say, “Publish this file.”
A way for the interface to tell the application, “We received this file.”
In theory the application can publish a file just by creating a symbolic link in the outbox directory, but as we’re using a hidden outbox, this is a little difficult.
We could literally copy file data into this directory, but since FileMQ supports symbolic links, we use that instead.
The file has an “.ln” extension and contains one line, the actual pathname.
This is complex code that does a lot at once.
But we’re only at around 10K lines of code for FileMQ and Zyre together.
Message-based applications do keep their shape, if you’re careful to organize them properly.
Writing the Unprotocol We have all the pieces for a formal protocol specification, and it’s time to put the protocol on paper.
There are two reasons for this: first, to make sure that any other implementations talk to each other properly; and second, because I want to get an official port for the UDP discovery protocol, and that means doing the paperwork.
Join a group S:JOIN          = header %x04 group status status          = OCTET         ; Sender group status sequence.
Ping a peer that has gone silent S:PING          = header %06
Like many of the projects in this book, it’s an icebreaker for others.
There are some major areas that are unfinished, which we may address in later editions of this book or versions of the software: High-level APIs.
The message-based API that Zyre offers now is usable but still rather more complex than I’d like for average developers.
If there’s one target we absolutely cannot miss, it’s raw simplicity.
This means we should build high-level APIs, in lots of languages, that hide all the messaging and come down to simple methods like start, join/leave group, get message, publish file, and stop.
Security How do we build a fully decentralized security system? We might be able to leverage public key infrastructure for some work, but that requires that nodes have their own Internet access, which isn’t guaranteed.
The answer is, as far as we can tell, to use any existing secure peer-to-peer link (TLS, Bluetooth, perhaps NFC) to exchange a session key, a symmetric cipher.
Nomadic content How do I, as a user, manage my content across multiple devices? The Zyre + FileMQ combination might help for local network use, but I’d like to be able to do this across.
Federation How do we scale a local-area distributed application across the globe? One plausible answer is federation, which means creating clusters of clusters.
The challenges are then quite similar: discovery, presence, group messaging.
I’ve implemented all kinds of workers/clients and the broker in Ruby, because that is the main language we use for development, but also some PHP clients to connect to the bus from existing PHP webapps.
We use this service bus for cloud services connecting all kinds of platform devices to a service bus exposing functionality for automation.
Due to the nature of our domain, we need to process large volumes of prices quickly.
In addition, it’s extremely critical to minimize latency in processing orders and prices.
Everything must be handled in a soft real time with a predictable ultra-low latency per price.
Each price can take a lot of processing stages, each of which increases total latency.
As a consequence, low and predictable latency of messaging between components becomes a key factor of our architecture.
It saved us from a bottleneck in the processing of messages and made processing time very stable and predictable.
The reason I was confident this would work was that our work on the guide had, for a year or more, shown the way.
True, the text is my own work, which is perhaps as it should be.
When we write, we tell a story, and one doesn’t want different voices telling one tale; it feels strange.
This code base, all licensed as open source under the MIT/X11 license, may form the basis for other books or projects.
Removing Friction I’ll explain the technical toolchain we used in terms of the friction we removed.
With this book we’re telling a story, and the goal is to reach as many people as possible, as cheaply and smoothly as we can.
The core idea was to host this book on GitHub and make it easy for anyone to contribute.
It turned out to be more complex than that, however.
At the same time I noticed a few people quite determinedly translating every single example.
This was mainly binding authors who’d realized that the examples were a great way to encourage people to use their bindings.
For their efforts, I extended the scripts to produce language-specific versions of the book online.
Instead of including the C code, we’d include the Python, or PHP code.
Once we have an idea of who works on what, we know how to structure the work itself.
It’s clear that to write and test an example, what you want to work on is source code.
It’s fast and works well with source control systems like Git.
Since the main platform for our websites is Wikidot, I write using Wikidot’s very readable markup format.
At least in the first chapters, it was important to draw pictures to explain the flow of messages between peers.
I found Ditaa, a lovely tool that chews up line drawings and spits out elegant graphics.
Having the graphics in the text, as text, makes it remarkably easy to work.
By now you’ll realize that the toolchain we use is highly customized, though it uses a lot of external tools.
All are available on Ubuntu, which is a mercy, and the whole toolchain is in the zguide repository in the bin subdirectory.
The original text sits in a series of text files (one per chapter)
The examples sit in the examples subdirectory, classified per language.
We take the text and process this into a set of Wikidot-ready files, for each of the.
We extract the graphics and call Ditaa on each one to produce image files, which.
We extract inline listings (which are not translated) and store these in the listings.
We use pygmentize on each example and listing to create a marked-up page in.
We upload all changed files to the book wiki using the Wikidot API.
So, we store the SHA-1 signatures of every image, listing, example, and text file and only process and upload changes, and that makes it easy to publish a new version of the book when people make new contributions.
To produce the PDF and Epub formats, we do this: bin/mkpdfs.
We use the mkbook script on all the input files to produce a DocBook output.
We push the DocBook format though db2epub to create Epub books, in each language.
When creating a community project, it’s important to lower the “change latency,” which is the time it takes for people to see their work live or, at least, to see that you’ve accepted their pull requests.
If that is more than a day or two, you’ve often lost your contributor’s interest.
Licensing I want people to reuse this text in their own work: in presentations, articles, and even other books.
However, the deal is that if they remix my work, others can remix theirs.
I’d like credit, and have no argument against others making money from their remixes.
However, when we started turning the examples into standalone projects (as with Majordomo), we used LGPL.
We’d like to hear your suggestions for improving our indexes.
Constant Gardner role, 367 contact information for this book, xvi context.
For two years Pieter was president of the FFII, a large NGO fighting software patents.
Pieter speaks English, French, Dutch, and bits and pieces of a dozen other languages.
He lives with his beautiful wife and three lovely children in Brussels, Belgium, where he plays with a West African drumming group; he also travels extensively.
The fourhorn sculpin is mostly found in arctic coastal waters around North America and northern Eurasia, but it can also be found in some freshwater lakes in Europe.
This fish is named for its four bony protuberances on its head.
The fourhorn sculpin has a dark and slightly flattened body with eyes close to the top of its heads, a large pelvis, and a distinctive large mouth.
One way to distinguish between males and females of this species is that males have a yellowish brown belly and females have a white belly.
This fish mostly feeds on organisms at the bottom of the sea, crustaceans, and fish eggs.
During this time males will typically dig a pit that females will put all their eggs into.
Once eggs are laid into a pit, males will guard the eggs during the three month incubation period.
