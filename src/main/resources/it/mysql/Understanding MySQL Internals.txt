You’ll also find links to news, events, articles, weblogs, sample chapters, and code examples.
Conferences O’Reilly brings diverse innovators together to nurture the ideas that spark revolutionary industries.
We specialize in documenting the latest tools and systems, translating the innovator’s knowledge into useful skills for those in the trenches.
Subscribers can zero in on answers to time-critical questions in a matter of seconds.
Read the books on your Bookshelf from cover to cover or simply flip to the page you need.
O’Reilly books may be purchased for educational, business, or sales promotional use.
Understanding MySQL Internals, the image of a banded broadbill, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc.
While every precaution has been taken in the preparation of this book, the publisher and author assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
This book is dedicated to my wife, Sarah, and my.
In the summer of 2003, somebody on the MySQL mailing list proposed a book about MySQL internals.
As I read the email, I realized that I had the background to write such a book, but I had just finished writing my first book and was not looking forward to writing another.
I tried to talk myself out of the responsibility, saying to myself nobody would ever publish a book so technical and specialized.
There simply would not be enough of an audience for it.
I realized the door was open and I was standing in the doorway, but my inertia was keeping something good from happening.
I thought about a passage in the Book of Mormon that says “a natural man is an enemy to God,” and the principle behind it.
If you drift along, seeking only the pleasure of the moment and staying safely within your natural comfort zone, you do not accomplish much.
Good things happen when you push yourself outside of your comfort zone, doing what is difficult but what you know deep inside is the right thing to do.
Interestingly enough, my editor happened to be Andy Oram, who also participated in the publication of Understanding the Linux Kernel and Linux Device Drivers.
He and I worked together on this book, and I appreciate his help very much.
I felt that his strengths very well compensated for my weaknesses.
Writing about the internals of an application means approaching it as a developer rather than just a user or an administrator.
Although I had worked on the MySQL source code extensively, I found myself doing a lot of research to figure out the gory details of algorithms, the purposes of functions and classes, the reasons for certain decisions, and other matters relevant to this book.
In addition, as I was writing the book, MySQL developers were writing new code.
And while the book was being written, I had to do other work to feed my growing family.
Fortunately, a good portion of that work involved projects that dealt with MySQL internals, allowing me to stay on top of the game.
Growth comes through challenges, and I feel it did for me in this process.
Now that I have finished the book, I have a better view of the design of MySQL as a whole, and a better knowledge of its dark and not so dark parts.
It is my hope that the reader will experience a similar growth.
Introduces the major modules in the source code and their purpose.
Chapter 2, Nuts and Bolts of Working with the MySQL Source Code Tells you how to download the source code and build a server from scratch.
Chapter 3, Core Classes, Structures, Variables, and APIs Lists the basic data structures, functions, and macros you need for later reference.
Chapter 4, Client/Server Communication Lays out the formats of the data sent between client and server, and the main functions that perform the communication.
Chapter 5, Configuration Variables Discusses how MySQL handles configuration in general, as well as the effects of many particular configuration variables, and shows you a framework for adding a new configuration variable.
Chapter 6, Thread-Based Request Handling Explains MySQL’s reasons for using threads and the main variables, such as locks, related to threads.
Chapter 7, The Storage Engine Interface Describes the relation of individual storage engines (formerly known as table types) to the MySQL core, and shows you a framework for adding a new storage engine.
Chapter 8, Concurrent Access and Locking Explains the different types of locks available in MySQL, and how each storage engine uses locks.
Chapter 9, Parser and Optimizer Explains the major activities that go into optimizing queries.
Chapter 10, Storage Engines Briefly describes the most important MySQL storage engines and some of the tree structures and other data structures they employ.
Chapter 11, Transactions Lists the main issues required to support transactions, and uses InnoDB to illustrate the typical architecture used to provide that support.
Chapter 12, Replication Gives on overview of replication with an emphasis on issues of implementation.
Who This Book Is For This book can be useful for a number of readers: a developer trying to extend MySQL in some way; a DBA or database application programmer interested in how exactly MySQL runs his queries; a computer science student learning about database kernel development; a developer looking for ideas while working on a product that requires extensive database functionality that he must implement himself; a closed-source database developer wondering how in the world MySQL runs its queries so fast; a random, curious computer geek who has used MySQL some and wonders what is inside; and, of course, anybody who wants to look smart by having a book on MySQL internals displayed on his shelf.
Although MySQL source is open in the sense of being publicly available, it is in essence closed to you if you do not understand it.
It may be intimidating to look at several hundred thousand lines of code written by gifted programmers that elegantly and efficiently solves difficult problems one line at a time.
To understand the code, you will need a measure of the inspiration and perspiration of those who created it.
Hopefully, this book can provide enough guidance to remove those barriers and to open the source of MySQL for you.
I do not believe it is possible to understand and appreciate MySQL strictly through a conceptual discussion.
Why is it so popular then? Why do we know enough about it for O’Reilly to be willing to publish a book on its internals?
The reason, in my opinion, is that what makes a good database is not so much the concepts behind it, but how well they are implemented.
It is important to be conceptually sound on a basic level, but a good portion of the genius is in implementing those concepts in a way that provides a reasonable combination of good performance and the ease of maintenance.
In other words, the devil is in the details, and MySQL developers have done a great job of taking that devil by the horns and twisting his head off.
Thus, in order to appreciate the inner workings of MySQL, you need to get close to the places where that devil is being subdued.
Somewhere in the dark depths of the optimizer or inside the B-tree, there is music to be heard as you study the code.
It will take some work to hear that music, but once you do, you can feel its beauty.
And to hear the music you must not be afraid to compile the code, add a few debugging messages to help you understand the flow, and perhaps even change a few things to appreciate what will make the server crash (and how) if you fail to handle something that turns out to be important after all.
The first chapter provides a brief introduction of how different components of MySQL work together.
Immediately afterward you will find a chapter about downloading and building MySQL from the source.
When approaching a new code base, I find it very useful to look at class/structure definitions and API call prototypes.
I have a confession to make: I first look at the code, then read the comments, and I never look at block diagrams unless somebody asks me to.
Chapter 3 is for the developers whose heads are wired like mine; it talks about the core server classes, structures, and API.
In Chapter 4 I talk about the communication protocol between the client and the server.
Afterward, I hope you will say: “I am thankful for the MySQL API, and I even have a clue of how to fix it up if I had to!”
Every one of them tells you about some special server capability or perhaps a problem some DBA had to solve at some point.
It would not be too much of an exaggeration to say that if you understand the variables, you understand the server.
Toward the end you will find a tutorial on how to add your own configuration variables.
Every server has to deal with the issue of how to handle multiple clients concurrently.
Understanding threads and how they are used in MySQL is critical to being effective in working with MySQL source.
One of the distinct features of the MySQL architecture is its ability to integrate thirdparty storage engines.
Chapter 7 focuses on the storage engine interface and provides a functional example of a simple storage engine.
Although at the moment MySQL supports a number of page and row-level locking storage engines, the core architecture has a strong MyISAM heritage.
Part of that heritage is the mechanism to acquire a table lock.
The table lock awareness, even when it is in essence a token lock, is important for an aspiring MySQL developer.
This is the chapter I would recommend to a DBA trying to improve the performance of MySQL.
The key to optimizing MySQL queries and tables is to learn to think like the optimizer.
This chapter also provides an overview of the source code for the brave developers preparing to immerse themselves into the optimizer’s dark depths.
Chapter 10 is a cursory overview of MySQL storage engines.
It may be helpful to a developer trying to create or integrate her own.
A curious reader looking for what is out there may also find it of interest.
By no means is this book a comprehensive guide to MySQL internals.
The subject is so deep that I do not believe it is humanly possible to scratch the surface even if you had 10,000 pages and the time to create them.
To make matters more complicated, MySQL developers are adding new code daily.
Fortunately, most of the core code tends to remain intact, so the book has a shot at not becoming obsolete before it is published.
Nevertheless, do not be surprised when you look at the current MySQL code and find that some things are not quite like what you see in the book.
You are likely to see new classes and calls in the API.
On occasion, you may find that an old API call has a new argument.
But hopefully the book can always serve as a guide to teach you enough basics about the code to bring you to a level of proficiency that will enable you to accomplish your goals.
Conventions Used in This Book The following typographical conventions are used in this book:
Italic Indicates filenames, directories, and file extensions, new terms, URLs, commands and command-line options, usernames, hostnames, email addresses, and emphasized text.
Constant width Indicates parts of code (such as variables, class names, methods, and macros), elements of SQL statements, contents of files, and output from commands.
Constant width italic Indicates text that should be replaced with user-supplied values.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
The longer examples can be downloaded from the book’s web site at http://www.oreilly.com/catalog/9780596009571
You do not need to contact us for permission unless you’re reproducing a significant.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Comments and Questions Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
To comment or ask technical questions about this book, send email to:
For more information about our books, conferences, Resource Centers, and the O’Reilly Network, see our web site at:
It’s a virtual library that lets you easily search thousands of top tech books, cut and paste code samples, download chapters, and find quick answers when you need the most accurate, current information.
Acknowledgments I would like to express special thanks to Andy Oram for his continual guidance and encouragement as we worked together on this book.
I am particularly grateful to the MySQL development team for their cooperation and active participation in the review.
His knowledge and vision of the MySQL code and architecture is amazing, as well as his ability to pay attention to detail.
At times, as I read his reviews, I would wonder if he possibly got tired of the loads of technical detail and would speed-read past some inaccuracies or errors.
The next moment I saw a note in red about some little but nevertheless important detail.
Special thanks also go to Brian Aker, Martin “MC” Brown, and Paul Kinzelman for their reviews and suggestions.
And last, but not least—special thanks to my wife Sarah and my children Benjamin, Jennifer, Julia, Joseph, and Jacob for their patience and support, as I spent many Saturdays in the office working on the book.
MySQL architecture is best understood in the context of its history.
Over time, the tool was rewritten in C and ported to run on Unix.
It was still just a low-level storage engine with a reporting front end.
Working under the adverse conditions of little computational resources, and perhaps building on his God-given talent, Monty developed a habit and ability to write very efficient code naturally.
In addition to the above, with TcX being a very small company and Monty being one of the owners, he had a lot of say in what happened to his code.
While there are perhaps a good number of programmers out there with Monty’s talent and ability, for a number of reasons, few get to carry their code around for more than 20 years.
Monty’s work, talents, and ownership of the code provided a foundation upon which the Miracle of MySQL could be built.
Some time in the 1990s, TcX customers began to push for an SQL interface to their data.
He tried borrowing mSQL code for the SQL part and integrating it with his low-level storage engine.
Then came the classic move of a talented, driven programmer: “I’ve had enough of those tools that somebody else wrote that don’t work! I’m writing my own!”
The initial public release provided only a binary distribution for Solaris.
A month later, the source and the Linux binary were released.
In the next two years, MySQL was ported to a number of other operating systems as the feature set gradually increased.
MySQL was originally released under a special license that allowed commercial use to those who were not redistributing it with their software.
Special licenses were available for sale to those who wanted to bundle it with their product.
This provided TcX with some revenue to justify the further development of MySQL, although the purpose of its original creation had already been fulfilled.
It supported a decent subset of the SQL language, had an optimizer a lot more sophisticated than one would expect could possibly be written by one person, was extremely fast, and was very stable.
Numerous APIs were contributed, so one could write a client in pretty much any existing programming language.
However, it still lacked support for transactions, subqueries, foreign keys, stored procedures, and views.
The locking happened only at a table level, which in some cases could slow it down to a grinding halt.
Some programmers unable to get around its limitations still considered it a toy, while others were more than happy to dump their Oracle or SQL Server in favor of MySQL, and deal with the limitations in their code in exchange for improvement in performance and licensing cost savings.
Around 1999–2000 a separate company named MySQL AB was established.
It hired several developers and established a partnership with Sleepycat to provide an SQL interface for the Berkeley DB data files.
Since Berkeley DB had transaction capabilities, this would give MySQL support for transactions, which it previously lacked.
After some changes in the code in preparation for integrating Berkeley DB, version 3.23 was released.
Although the MySQL developers could never work out all the quirks of the Berkeley DB interface and the Berkeley DB tables were never stable, the effort was not wasted.
As a result, MySQL source became equipped with hooks to add any type of storage engine, including a transactional one.
By April of 2000, with some encouragement and sponsorship from Slashdot, masterslave replication capability was added.
The old nontransactional storage engine, ISAM, was reworked and released as MyISAM.
Among a number of improvements, full-text search capabilities were now supported.
However, around the same time, Heikki Tuuri approached MySQL AB with a proposal to integrate his own storage engine, InnoDB, which was also capable of transactions and row-level locking.
Heikki’s contribution integrated much more smoothly with the new table handler interface already polished off by the Berkeley DB integration efforts.
The MySQL/ InnoDB combination became version 4.0, and was released as alpha in October of MySQL to another level.
It might be worthy of mention that the version number change was not caused by the addition of InnoDB.
MySQL developers have always viewed InnoDB as an important addition, but by no means something that they completely depend on for success.
Back then, and even now, the addition of a new storage engine is not likely to be celebrated with a version number change.
In fact, compared to previous versions, not much was added in version 4.0
Perhaps the most significant addition was the query cache, which greatly improved performance of a large number of applications.
Replication code on the slave was rewritten to use two threads: one for network I/O from the master, and the other to process the updates.
Unlike version 4.0, it added a number of significant improvements.
Perhaps the most significant was subqueries, a feature long-awaited by many users.
Spatial indexing support was added to the MyISAM storage engine.
It was made more secure against attacks, and supported prepared statements.
The decision to create a separate development branch was made because MySQL developers felt that it would take a long time to stabilize 4.1 if, on top of all the new features that they were adding to it, they had to deal with the stored procedures.
For a while this created quite a bit of confusion—there were two branches in the alpha stage.
MySQL Architecture For the large part, MySQL architecture defies a formal definition or specification.
When most of the code was originally written, it was not done to be a part of some great system in the future, but rather to solve some very specific problems.
However, it was written so well and with enough insight that it reached the point where there were enough quality pieces to assemble a database server.
Core Modules I make an attempt in this section to identify the core modules in the system.
However, let me add a disclaimer that this is only an attempt to formalize what exists.
Rather, they tend to think of files, directories, classes, structures, and functions.
It is much more common to hear “This happens in mi_open( )” than to hear “This happens on the MyISAM storage engine level.” MySQL developers know the code so well that they are able to think conceptually on the level of functions, structures, and classes.
They will probably find the abstractions in this section rather useless.
However, it would be helpful to a person used to thinking in terms of modules and managers.
With regard to MySQL, I use the term “module” rather loosely.
Unlike what one would typically call a module, in many cases it is not something you can easily pull out and replace with another implementation.
The code from one module might be spread across several files, and you often find the code from several different modules in the same file.
The newer code tends to fit into the pattern of modules better.
So in our definition, a module is a piece of code that logically belongs together in some way, and performs a certain critical function in the server.
Interaction of the Core Modules When the server is started on the command line, the Initialization Module takes control.
It parses the configuration file and the command-line arguments, allocates global memory buffers, initializes global variables and structures, loads the access control tables, and performs a number of other initialization tasks.
Once the initialization job is complete, the Initialization Module passes control to the Connection Manager, which starts listening for connections from clients in a loop.
When a client connects to the database server, the Connection Manager performs a number of low-level network protocol tasks and then passes control to the Thread Manager, which in turn supplies a thread to handle the connection (which from now on will be referred to as the Connection Thread)
The Connection Thread might be created anew, or retrieved from the thread cache and called to active duty.
The credentials of the connecting user are verified, and the client may now issue requests.
Some requests, known in the MySQL code terminology as commands, can be accommodated by the Command Dispatcher directly, while more complex ones need to be redirected to another module.
A typical command may request the server to run a query, change the active database, report the status, send a continuous dump of the replication updates, close the connection, or perform some other operation.
In MySQL server terminology, there are two types of client requests: a query and a command.
A query is anything that has to go through the parser.
A command is a request that can be executed without the need to invoke the parser.
We will use the term query in the context of MySQL internals.
Thus, not only a SELECT but also a DELETE or INSERT in our terminology would be called a query.
What we would call a query is sometimes called an SQL statement.
If full query logging is enabled, the Command Dispatcher will ask the Logging Module to log the query or the command to the plain-text log prior to the dispatch.
Thus in the full logging configuration all queries will be logged, even the ones that are not syntactically correct and will never be executed, immediately returning an error.
The Query Cache Module checks whether the query is of the type that can be cached, and if there exists a previously computed cached result that is still valid.
In the case of a hit, the execution is short-circuited at this point, the cached result is returned to the user, and the Connection Thread receives control and is now ready to process another command.
If the Query Cache Module reports a miss, the query goes to the Parser, which will make a decision on how to transfer control based on the query type.
Select queries are forwarded to the Optimizer; updates, inserts, deletes, and table-creation and schema-altering queries go to the respective Table Modification Modules; queries that check, repair, update key statistics, or defragment the table go to the Table Maintenance module; queries related to replication go to the Replication Module; and status requests go to the Status Reporting Module.
At this point, each of the modules that will receive control from the Parser passes the list of tables involved in the query to the Access Control Module and then, upon success, to the Table Manager, which opens the tables and acquires the necessary locks.
Now the table operation module is ready to proceed with its specific task and will issue a number of requests to the Abstracted Storage Engine Module for low-level operations such as inserting or updating a record, retrieving the records based on a key value, or performing an operation on the table level, such as repairing it or updating the index statistics.
The Abstracted Storage Engine Module will automatically translate the calls to the corresponding methods of the specific Storage Engine Module via object polymorphism.
In other words, when dealing with a Storage Engine object, the caller thinks it is dealing with an abstracted one, when in fact the object is of a more specific type: it is the Storage Engine object corresponding to the given table type.
The interface methods are virtual, which creates the effect of transparency.
The correct method will be called, and the caller does not need to be aware of the exact object type of the Storage Engine object.
As the query or command is being processed, the corresponding module may send parts of the result set to the client as they become available.
If an error message is issued, both the client and the server will understand that the query or command has failed and take the appropriate measures.
The client will not accept any more result set, warning, or error message data for the given query, while the server will always transfer control to the Connection Thread after issuing an error.
Note that since MySQL does not use exceptions for reasons of implementation stability and portability, all calls on all levels must be checked for errors with the appropriate transfer of control in the case of failure.
If the low-level module has made a modification to the data in some way and if the binary update logging is enabled, the module will be responsible for asking the Logging Module to log the update event to the binary update log, sometimes known as the replication log, or, among MySQL developers and power users, the binlog.
Once the task is completed, the execution flow returns to the Connection Thread, which performs the necessary clean-up and waits for another query or command from the client.
The session continues until the client issues the Quit command.
In addition to interacting with regular clients, a server may receive a command from a replication slave to continuously read its binary update log.
This command will be handled by the Replication Master Module.
If the server is configured as a replication slave, the Initialization Module will call the Replication Slave Module, which in turn will start two threads, called the SQL Thread and the I/O thread.
They take care of propagating updates that happened on the master to the slave.
It is possible for the same server to be configured as both a master and a slave.
Network communication with a client goes through the Client/Server Protocol Module, which is responsible for packaging the data in the proper format, and depending on the connection settings, compressing it.
The Client/Server Protocol Module in turn uses the Low-Level Network I/O module, which is responsible for sending and receiving the data on the socket level in a cross-platform portable way.
It is also responsible for encrypting the data using the OpenSSL library calls if the connection options are set appropriately.
As they perform their respective tasks, the core components of the server heavily rely on the Core API.
The Core API provides a rich functionality set, which includes file I/O, memory management, string manipulation, implementations of various data structures and algorithms, and many other useful capabilities.
MySQL developers are encouraged to avoid direct libc calls, and use the Core API to facilitate ports to new platforms and code optimization in the future.
Detailed Look at the Core Modules We will now take a closer look at each of the components.
One purpose of the discussion is to connect the conceptual language used earlier with the actual source.
In addition, we will cover the some of the history of each component and try to estimate its future development path.
Frequent references to the source will be made, and you may find it helpful to open the mentioned files in a text editor and locate the function references.
That chapter will also tell you how to get the source code.
The Server Initialization Module is responsible for the server initialization on startup.
Most of the code is found in the file sql/mysqld.cc.
The entry point is what a C/ C++ programmer would expect: main( )
If the file is not mentioned, the location is sql/mysqld.cc:
Although the code found in version 3.22 was never rewritten from scratch, it has been significantly refactored as new features were added to MySQL.
One big chunk of initialization code that used to be under main( ) got reorganized gradually into a number of helper functions over the lifetime of the code.
Additionally, the command line and configuration file option parsing got switched from the GNU getopt( ) to the MySQL Core API option parser once it became available in version 4.0
Based on the past history, we should anticipate possible incremental additions in the future as new features that require special initialization on startup are added.
The Connection Manager listens for incoming connections from clients, and dispatches the requests to the Thread Manager.
However, it deserves to be classified as a separate module due to its critical role in the operation of the server.
The abundance of #ifdef directives speaks to the challenge of porting networking code to a variety of operating systems.
Over time, the code evolved somewhat to accommodate quirks in the network system calls of different operating systems.
Further changes might be necessary in the future as new ports are attempted, or as the different operating system vendors introduce new quirks into new versions of their products.
The Thread Manager is responsible for keeping track of threads and for making sure a thread is allocated to handle the connection from a client.
Objects of the THD type are thread descriptors, and are critical in the operation of most of the server modules.
Many functions take a THD pointer as their first argument.
The thread management code was significantly reworked in version 3.23 when the thread cache was added.
It is reasonable to expect that it will not receive any significant changes in the future.
However, if we, in our abstraction, consider the THD class itself as part of this module, we have a different story as far as changes are concerned.
The Connection Thread is the heart of the work of processing client requests on an established connection.
However, despite its size, it deserves to be classified as a module due to its role in the server.
The code evolved over time, gradually becoming more compact and readable as various initializations involving THD variables were moved under the THD class.
It is reasonable to expect that the code will not change much in the future.
The User Authentication Module authenticates the connecting user and initializes the structures and variables containing the information on his level of privileges.
However, the rest of the functionality is found in sql/sql_acl.cc and sql/password.cc.
The code has been significantly reworked only once, in version 4.1
Due to the possible impact of the changes, MySQL developers waited a while before they attempted the updates in the protocol needed to implement a more secure authentication.
Since then, there have not been many changes to this code.
However, with the addition of plug-in capability in 5.1, MySQL developers are planning to add pluggable authentication and roles capabilities, which will require changes in this code.
The Access Control Module verifies that the client user has sufficient privileges to perform the requested operation.
Some other functions of interest follow, all located in sql/sql_acl.cc unless otherwise indicated:
The code itself has not changed very much since version 3.22
However, new privilege types were added in version 4.0, which somewhat changed the way this module was used by the rest of the code.
MySQL developers are planning to add support for roles, which will require significant changes to this module.
The Parser is responsible for parsing queries and generating a parse tree.
Note that unlike many open source projects, MySQL has its own generated lexical scanner instead of using lex.
Some files of interest, in addition to the ones just mentioned, include:
The group of files under sql/ with names starting in item_ and extensions of .h or .cc.
As the new SQL features are added, the parser keeps changing to accommodate them.
However, the core structure of the parser is fairly stable, and so far has been able to accommodate the growth.
It is reasonable to expect that while some elements will be added on, the core will not be changed very much for some time.
MySQL developers have been, and sometimes still are, talking about a core rewrite of the parser and moving it away from yacc/Bison to make it faster.
However, they have been talking about it for at least seven years already, and this has not yet become a priority.
The Command Dispatcher is responsible for directing requests to the lower-level modules that will know how to resolve them.
The module kept growing over time as the set of supported commands increased.
Small growth is expected in the future, but the core structure is unlikely to change.
The Query Cache Module caches query results, and tries to short-circuit the execution of queries by delivering the cached result whenever possible.
Few changes aside from bug fixes are expected in the future.
The Optimizer is responsible for creating the best strategy to answer the query, and executing it to deliver the result to the client.
It is perhaps the most complex module in the MySQL code.
As you descend into the depths of the optimizer, there is a cave worth visiting.
It is the range optimizer, which was separate enough from the optimizer core and complex enough to be isolated into a separate file, sql/opt_range.cc.
The range optimizer is responsible for optimizing queries that use a key with a given value range or set of ranges.
The optimizer has always been in a state of change.
The addition of subqueries in 4.1 has added another layer of complexity.
Version 5.0 added a greedy search for the optimal table join order, and the ability to use several keys per table (index merge)
It is reasonable to expect that many more changes will be made in the future.
One long-awaited change is improvements in the optimization of sub-queries.
The Table Manager is responsible for creating, reading, and modifying the table definition files (.frm extension), maintaining a cache of table descriptors called table cache, and managing table-level locks.
Most of the code is found in sql/sql_base.cc, sql/table.cc, sql/unireg.cc, and sql/lock.cc.
In the past, Monty has expressed some dissatisfaction with the inefficiencies in the table cache code, and wanted to rewrite it.
However, some progress has finally been made in version 5.1
This collection of modules is responsible for operations such as creating, deleting, renaming, dropping, updating, or inserting into a table.
Unfortunately, due to the space constraints, this book will not cover it in detail.
However, once you become familiar with the rest of the code, you should be able to figure out the details by reading the source and using the debugger without too much trouble by starting from the following entry points:
The Update and Delete modules have been changed significantly in version 4.0 with the addition of multi-table updates and deletes.
Otherwise, aside from fairly minor improvements from time to time, they have not changed much.
It is reasonable to expect that for the large part the code will remain as it is in the future.
The Table Maintenance Module is responsible for table maintenance operations such as check, repair, back up, restore, optimize (defragment), and analyze (update key distribution statistics)
The core function is mysql_admin_table( ), with the following convenience wrappers:
The bulk of the work happens on the storage engine level.
The module was introduced in version 3.23 to provide an SQL interface for table maintenance.
Prior to that table maintenance had to be performed offline.
In version 4.1, significant changes were made to the Network Protocol Module to support prepared statements.
This affected all the modules that talk back to the client, including the Table Maintenance Module.
Otherwise, not much has changed since its introduction, and it is reasonable to expect that not much will in the future.
The Status Reporting Module is responsible for answering queries about server configuration settings, performance tracking variables, table structure information, replication progress, condition of the table cache, and other things.
Some functions of interest, all in sql/sql_show.cc unless indicated otherwise, are:
The addition of new functionality has created the need for additional status reporting.
It is reasonable to expect that this pattern will continue in the future.
This module is actually an abstract class named handler and a structure called a handlerton.
The handlerton structure was added in version 5.1 for plug-in integration.
It provides a standardized interface to perform low-level storage and retrieval operations.
The table hander is defined in sql/handler.h and partially implemented in sql/handler.cc.
The derived specific storage engine classes will have to implement all the pure virtual methods of this class.
This module was introduced in version 3.23 to facilitate the integration of Berkeley DB tables.
This move had far-reaching consequences: now a variety of low-level storage engines could be put underneath MySQL with a fair amount of ease.
The code was further refined during the integration of InnoDB.
The future of the module will largely depend on what new storage engines will be integrated into MySQL, and on the way the existing ones will change.
For example, sometimes a new feature in some underlying storage engine may require an addition to the abstracted interface to make it available to the higher-level modules.
Each of the storage engines provides a standard interface for its operations by extending the handler class mentioned earlier.
The methods of the derived class define the standard interface operations in terms of the low-level calls of the specific storage engine.
Meanwhile, for a quick introduction, you may want to take a look at a few files and directories of interest:
When the storage engine interface was first abstracted (version 3.23), there were only three fully functional storage engines: MyISAM, ISAM (older version of MyISAM), and MEMORY.
Note that the MEMORY storage engine used to be called HEAP, and some of the file and directory names in the source tree still reflect the earlier name.
However, the list grew rapidly with the addition of BerkeleyDB, MERGE, InnoDB, and more recently, NDB for the MySQL Cluster.
Most storage engines are still in fairly active development, and we may see some new ones added in the future.
The Logging Module is responsible for maintaining higher-level (logical) logs.
A storage engine may additionally maintain its own lower-level (physical or logical) logs for its own purposes, but the Logging Module would not be concerned with those; the storage engine itself takes charge.
The logical logs at this point include the binary update log (used mostly for replication, otherwise), command log (used mostly for server monitoring and application debugging), and slow query log (used for tracking down poorly optimized queries)
However, most of the work in logging happens in the binary replication log.
Significant changes were made to this module with the introduction of replication.
Version 5.0 brought on some changes required for XA transactions.
Version 5.1 added the capability to search logs as if they were an SQL table, which required a significant refactoring of this code.
The binary logging part required significant changes to accommodate row-based replication.
At this point it is hard to anticipate where this code is going in the future.
The Replication Master Module is responsible for the replication functionality on the master.
The most common operation for this module is to deliver a continuous feed of replication log events to the slave upon request.
The module was added in version 3.23, and it has not experienced any major changes other than a thorough cleanup to isolate chunks of code into functions.
In the beginning, the code had very ambitions development plans for fail-safe replication.
However, before those plans could be realized, MySQL acquired NDB Cluster code from Ericsson, and began pursuing another route to the eventual goal of automatic failover.
In light of those developments, it is not clear at this point how the native MySQL replication will progress.
The Replication Slave Module is responsible for the replication functionality of the slave.
The role of the slave is to retrieve updates from the master, and apply them on the slave.
The network I/O thread requests and receives a continuous feed of updates from the master, and logs them in a local relay log.
The SQL thread applies them as it reads them from the relay logs.
The module was added in 3.23 along with the Replication Master module.
It went through a substantial change in version 4.0 when the monolithic slave thread was broken down into the SQL thread and the I/O thread.
The MySQL client/server communication protocol sits on top of the operating system protocol (TCP/IP or local socket) in the protocol stack.
This module implements the API used across the server to create, read, interpret, and send the protocol packets.
The files sql/protocol.h and sql/protocol.cc define and implement a hierarchy of classes.
The Protocol class hierarchy was added in version 4.1 to deal with prepared statements.
It appears that at this point most of the problematic areas in the protocol at this level have been addressed, and it is reasonable to.
However, MySQL developers are thinking about adding support for notifications.
All functions in this module have names starting with vio_
This module was introduced in 3.23, spurred by the need to support SSL connections.
Abstracting the low-level network I/O also facilitated porting to new platforms and maintaining the old ports.
It provides functionality for portable file I/O, memory management, string manipulation, filesystem navigation, formatted printing, a rich collection of data structures and algorithms, and a number of other things.
If a problem ever arises, there is usually a solution for it in the Core API Module.
This module is to a great extent an expression of Monty’s ability and determination to never solve just one problem.
It is perhaps the core component of the Miracle of MySQL.
The code is found in the mysys/ and strings/ directories.
Many of the core API functions have names starting with my_
The module has always been in a state of growth and improvement.
As the new functionality is added, great care is put into preserving its stability and high level of performance.
It is reasonable to expect that this pattern will continue in the future.
Much can be learned about MySQL by studying its source.
Monty Widenius, the creator of MySQL, once half-jokingly remarked that the source is the ultimate documentation.
Indeed, assuming that the hardware and the compiler are functioning properly, the software will do exactly what the source tells it to.
However, understanding the source of a complex program such as MySQL can be a challenge.
The purpose of this chapter is to give you a head start in your study of the source.
Unix Shell Although MySQL runs on a number of different platforms, you will find it easier to study the source if you get an account on some Unix-like system, such as Linux, FreeBSD, Mac OS X, or Solaris.
If you do not have a preference to start with, I recommend Linux.
It could be either a remote server, or running on your desktop.
The examples in this chapter assume you are logged in to a Unix command shell, and that your shell is Bourne-compatible to some degree.
One way to get such a shell is to execute:
BitKeeper MySQL developers use BitKeeper (http://www.bitmover.com) for source revision control.
A BitKeeper repository containing MySQL source code is publicly available with read-only access.
Although MySQL source code can also be obtained by downloading a compressed archive, using BitKeeper offers a number of advantages:
You get the most recent source version and can stay up to date with all the developments on a daily basis.
BitKeeper tools allow you to easily keep track of changes.
You can easily keep track of your own changes and submit patches to MySQL developers.
The initial setup requires a download of over 30 MB of data if you are downloading the revision history.
Special tools such as autoconf, automake, and bison have to be installed in order to build MySQL.
Since BitMover decided to discontinue the Open Logging License, it is not possible to automatically integrate your changes, submit patches, and do other tasks without buying a commercial license.
If the disadvantages of using BitKeeper in your situation outweigh the advantages, please refer to the “Building from Source Distribution” section in this chapter.
Otherwise, the first step is to make sure that BitKeeper is installed on your system.
Without a commercial license, the only advantage of using BitKeeper is being able to get the most recent development source.
If you are not planning to use the commercial version of BitKeeper, follow these instructions to download the free BitKeeper client:
If you want the entire revision history, which will make the download take longer, enter:
You can also browse the change sets online at http://mysql.bkbits.net for different MySQL version trees.
Once you have BitKeeper installed, I would recommend running bk helptool to familiarize yourself with the following basic commands:
Once you get comfortable with BitKeeper, the next step is to clone the repository of the MySQL version you would like to study.
As of this writing, there are six version repositories available.
The following directions and discussion apply mostly to the commercial version of BitKeeper.
To create a local copy of the repository, execute the clone command:
For example, if you want to get a copy of the 5.1 version repository, enter:
If you are behind a restrictive firewall, it might block an outgoing connection on that port.
Fortunately, in the commercial version there’s a workaround if you happen to have a local HTTP proxy (substitute the proper host name and port in the first command):
If you do not have the commercial version, you may still be able to overcome the restriction with creative tunneling and port-forwarding.
The initial clone operation will transfer over 30 MB of data, so it could take a while depending on the speed of your connection and the overall network traffic congestion.
Once the clone has been completed, you will see a subdirectory in your current directory corresponding to the name of the repository.
If you prefer a different name, add an argument to the original command, for example:
Preparing the System to Build MySQL from BitKeeper Tree Once you have cloned the BitKeeper repository, the following tools must be installed for the build scripts to work:
The required version’s utilities will be present on most Linux distributions that were put together in the second half of 2003 or later.
If you have an older or a very customized Linux installation, or if you are using a different Unix variant, refer to Table 2-2 to verify that you have the required version of each tool.
Note that the executable binary for each tool must be in your PATH.
The following Bourne shell script will provide a helpful summary that you can use to evaluate whether your system is ready:
If the tools are installed and in your path, the script produces output similar to this:
This is free software; see the source for copying conditions.
This is free software; see the source for copying conditions.
This is free software; see the source for copying conditions.
This is free software; see the source for copying conditions.
This is free software; see the source for copying conditions.
Determine the versions of each utility from the output of the script, and compare them against the table.
If the numbers are less than the version numbers required, perform the necessary upgrades.
Building MySQL from BitKeeper Tree MySQL developers have created a number of scripts to build different types of MySQL binaries on different platforms.
They are found in the BUILD directory of the BitKeeper tree.
Unfortunately, at the time of this writing, I could not find the one that would build a debugging-enabled binary on any architecture.
To solve the problem, I will provide instructions on how to create one:
At the shell prompt from the root of the cloned repository, execute the following:
The script will generate the make files, create the necessary headers with definitions, and then compile the MySQL server, client, and miscellaneous utilities.
During the build, you may see a number of warnings from different tools engaged in the process.
Those are usually safe to ignore if the build does not abort.
To verify that the build was successful, type at the shell prompt:
Ideally, you will see all of the tests either pass or be skipped.
Some tests, especially the ones that test replication, might fail on some systems due to the difficulties with server shutdown and restart.
Since the BitKeeper repository may contain source in between releases, on occasion a developer may check in a test case for the bug that has not yet been fixed.
So if a couple of the tests fail, this is not something to worry about for somebody studying how MySQL works.
If the majority of the tests pass, consider the built binary fit for at least your educational use.
If you do not want to wait that long, you may simply check to see whether the mysqld binary was created:
If the build was successful, the command will produce output similar to:
If the build process succeeds in creating a binary out of the unmodified clone of the public BitKeeper tree, the test suite also succeeds 95 percent of the time.
Building from Source Distribution Although it is preferred that you use the BitKeeper repository, in some cases it might be desirable for you to use another method to build MySQL.
Although in most of the situations you will need only gcc, gdb, and GNU make, there are times when other tools mentioned in the section “Preparing the System to Build MySQL from BitKeeper Tree” are necessary.
For example, you will need Bison to change the parser, and adding another file to the source will require the use of autoconf, automake, and m4
Therefore, it is still recommended that you follow the same procedures outlined in that section to prepare your system to the fullest extent possible.
If you have a non-GNU tar already installed, it is recommended that you still install GNU tar.
MySQL is archived using GNU tar, and some variants of tar are not compatible with it.
The following instructions explain how to download and compile MySQL using the source distribution:
Refer to the table listing MySQL versions in the “BitKeeper” section of this chapter, and decide which version of MySQL you would like to work with.
If you have chosen a different version, you will need to make the appropriate modifications to the following procedures, replacing 4.1 with the version you have chosen.
Visit http://dev.mysql.com/downloads/mysql/4.1.html (note the version number in the URL), scroll down to the bottom of the page where it says “Source downloads,” and click on the link that says “Pick a mirror” on the “Tarball” line.
Optionally fill out the form at the top of the next page and submit it, or just scroll down to the bottom of the page, pick the mirror closest to you, click on the link, and proceed with the download.
In the Unix shell, change to the directory where you plan to keep your MySQL sources, and execute the following commands:
Follow the instructions in the section “Building MySQL from BitKeeper Tree.” If you have not installed all of the required tools for a BitKeeper Tree build, additionally edit BUILD/FINISH.sh to comment out the following lines by adding # to the beginning:
Installing MySQL into a System Directory If desired, you may install MySQL into a system directory by executing:
A full listing of build configuration options can be obtained by executing:
If you do not plan to deploy the MySQL server binary you have built on this system, installing it into a system directory is not necessary.
The mysql-test-run script permits you to start up the binary you have built and test it while it is located in the directory where it was created.
Source Code Directory Layout Table 2-3 lists the top-level subdirectories in the MySQL source tree, with a brief explanation of each.
Note that some reorganization is possible in the future versions, but most of the structure should be fairly stable.
NEW-RPMS Used by the distribution build scripts to hold new RPMs.
However, the interface between Berkeley DB and core MySQL is not very well developed, and InnoDB storage engine is a better choice when transactions are needed.
I personally do not like using it because it alters execution and obscures the timesensitive bugs, but some developers, including Monty, love it for its ability to print out the execution trace.
To enable it, add –with-debug to extra_configs in your build script.
MyISAM is the latest version of the original MySQL storage engine.
It does not support transactions and requires table locks, but it uses less disk space and is faster on a number of queries than InnoDB, the transactional storage engine.
Preparing the System to Run MySQL in a Debugger To fully enjoy the study of MySQL internals, and to be able to execute the examples in the subsequent sections of this chapter, you must have gdb (http://www.gnu.org/ software/gdb/) installed on your system, and be present in your PATH.
You also need to have the X Window System, including a terminal program such as xterm.
There are a number of X standard implementations, perhaps the most popular of them being X.org (http://www.x.org)
When a developer writes a script and does not know where to put it, it ends up in this directory.
Note, however, that this is the home of mysqld_safe, the king of all scripts.
It is used to start the server from the command line.
This includes the parser and optimizer, the abstracted table handler (storage engine) interface, replication, query cache, table lock manager, the code to read and write table definitions, the logging code, and a number of other pieces.
It does have a few stray C files that could not find a home in other directories.
This is also the home of mysqld.cc, the file that defines main( ), where the execution starts when the server is launched.
Also contains configuration files for package builds, such as the RPM spec files.
The tools just mentioned will be preinstalled by default on most Linux distributions.
As you can see in the output, both libraries are not stripped.
If you happen to have the misfortune of having them stripped by default, and you are not able to find a package with unstripped versions for your distribution, you can fix the problem by recompiling glibc.
Debugger-Guided Source Tour Now with the tedious but necessary preparation behind your back, you can actually start exploring the source code.
I find it particularly helpful, when faced with large quantities of unfamiliar code, to start by running a very simple test case in a debugger.
MySQL, being a threaded server, presents a number of difficulties in this respect.
Fortunately, MySQL developers have created a set of tools to facilitate the process for their own use, which they make available to the public.
In this section, you will learn how to use them.
It is important that the file be under the t subdirectory, have the extension .test, and be different from the names of the already existing test files in the t subdirectory.
Outside of those restrictions, the name of the file can be anything you want.
If you choose a different name, however, you must also change references to it accordingly in the rest of this example.
Put the following line in the edited file: select 1;
Execute the following command to create the master result file: $ ./mysql-test-run --local --record example.
An xterm window will open with a gdb prompt inside.
Refer to the sections “Basics of Working with gdb,” and “Interesting Breakpoints and Variables,” later in this chapter, to set breakpoints of interest, then enter c at the gdb prompt to continue execution.
You may connect using a MySQL command client to port 9306 and manually issue various queries, set breakpoints in the debugger, and examine their execution.
You will enter the MySQL command-line client shell, from which you continue with:
The debugger shows you the stack trace at the current breakpoint.
When finished with this debugger-guided source tour, press Ctrl-C if you do not have the gdb prompt, execute the quit command in the debugger, confirm that you want to stop the program being run when prompted, and return to the shell prompt from which you have executed mysql-test-run.
To speed up the execution of your next debugger-guided source tour, execute the following command at the shell prompt to clean up:
Basics of Working with gdb gdb has a command-line interface similar to a Unix shell.
You type a command and then press the Enter key to execute it.
If you have never worked with gdb before, begin by executing the help command, which produces the following output:
Type "help" followed by a class name for a list of commands in that class.
The instructions in the preceding output give you a starting point from which you can continue a more in-depth study of gdb.
For example, if you want to learn about how to run a program being debugged, execute help running, which in turn gives you the following:
Now you have a list of commands, related to running a program, to learn.
Args may include "*", or "[...]"; they are expanded using "sh"
With no arguments, uses arguments last specified (with "run" or "set args")
To cancel previous arguments and run with no arguments, use "set args" without arguments.
Using the preceding method, you can get a full listing of gdb commands and learn about each one.
Table 2-4 is a list of commands that you may find particularly useful when studying the MySQL source.
Using abbreviated versions significantly speeds up the tasks even if you are a fast typist.
If an argument n is given and the execution continues from a breakpoint, stop on that breakpoint only on the nth pass.
If an argument n is given, do this n times or until the program stops for another reason.
If an argument n is given, do this n times or until the program stops for another reason.
With the full argument, also prints the values of local variables in each stack frame.
Finding Things in the Source A typical question programmers ask when working with a large unfamiliar code base is “Where in the world is the function get_and_lock_X( ) defined?” There are many techniques to find the answer, and many programmers have their own favorites.
For those who do not, or who are having a hard time adapting them to the MySQL source, I will share mine.
Suppose you need to find the definition of mysql_lock_tables( )
Start an instance of MySQL server in a debugger as outlined in the section “Debugger-Guided Source Tour.”
Once the debugger window opens, type in the debugger window: i li mysql_lock_tables.
If no argument is given, disassembles the executable code surrounding the pc register of the current stack frame.
Unfortunately, it doesn’t say which directory the file is in, so one more step is necessary.
Execute the following command from the root of the source tree: $ find.
Sometimes things do not go as smoothly as in the previous example.
What appears to be a function could in reality be a preprocessor macro.
If that happens, the debugger will tell you it knows nothing about the symbol.
Suppose you need to find the definition of ha_commit( ), and the debugger has already told you that there is no such symbol.
Execute the following command from the root of the source tree:
Interesting Breakpoints and Variables If you have ever worked with an unfamiliar code base of significant size, you have most certainly been confronted with the challenge of mentally penetrating the execution flow.
Yes, I understand this init_X( ) function, but where in the world does the meat really begin when I do operation Y?
Table 2-5, along with debugger-guided source tours inspired by its contents, will hopefully help you answer a number of such questions.
It is based on the 4.1 source, but for the most part should be applicable to other versions.
The asterisks in the final column do not indicate C++ pointers; they mean “all variables whose names match.”
Operation Good place for an entry breakpoint Interesting variables to examine.
Checking whether the query can be answered from a query cache.
Execution of replication thread on the slave that handles network I/O.
Execution of replication thread on the slave that handles SQL commands.
Making a Source Modification If you have not added any extra files to the source, after making the change, simply execute:
If the only files you have changed are in the sql directory, it is sufficient to run make only in that directory, which will reduce the time of the process because make would doesn’t check other directories to see whether anything needs to be done there.
For each directory where you added files, edit Makefile.am in that directory.
In each Makefile.am, find the appropriate variable that ends in SOURCES.
Add the names of your C/C++ files that you have added to the SOURCES variable.
In each Makefile.am file, find the variables INCLUDES and noinst_HEADERS.
Add the names of the new headers that you want to be installed by a make install command to INCLUDES, and the ones that do not need to be installed to noinst_ HEADERS.
Key functions and variables involved in common MySQL operations (continued)
Operation Good place for an entry breakpoint Interesting variables to examine.
Coding Guidelines If you are making changes to MySQL, it is recommended that you follow the same coding guidelines as MySQL developers.
This will make it easier for your code to work with what’s already written, help you avoid bugs, and increase the chances of your patches being accepted by MySQL developers without serious modifications.
MySQL developers make their coding guidelines publicly available at http://dev.
In addition, I will provide a reorganized summary with some extra tips and comments from my own experience.
Stability Here are some guidelines for preserving the code’s stability while making changes:
Always remember that, more often than not, you are in a thread and must follow the rules of thread-safe programming.
Most global variables have an associated mutex that other threads will lock before accessing it.
Make sure to learn which mutex is associated with each global variable, and lock it when accessing that variable.
Be aware that you have very little stack space available.
To verify the stack position in question, set a breakpoint there in the debugger, and when it is reached, run the bt command.
Note that the memory allocated with sql_alloc( ) lasts until the end of the query execution.
If you want your allocation to persist past that, use my_malloc( )
The memory pool will be freed at once with a call to free_root( ) at the end of the query.
Do not use STL, iostream, or any other C++ extension that would require linking against libstdc++
Do not introduce dependencies on additional external libraries whenever possible.
Try to reuse the existing MySQL code as much as possible.
Portability Following these suggestions increases the likelihood that your code will work on systems other than the one you write and test on:
Do not assign integer values to a pointer that could be unaligned.
When introducing a system or compiler-specific optimization that might not be supported on another system, make sure to enclose it in an #ifdef block, and provide an alternative if that optimization is not available.
Do not put C++ style comments into C files, even though some C compilers support it.
Performance The following suggestions can help you maximize your code’s memory and processor use:
Develop a habit of thinking about performance naturally and coding that way.
Learn the basics of how the CPU works and visualize what happens to it on the assembly level when your C/C++ code is executed.
Be aware of what is going on inside the calls you are making.
Style and Ease of Integration Following are some conventions established by the MySQL developers for general consistency.
Follow the indentation, variable naming, and commenting guidelines in internals.html.
Make sure your editor is not configured to do that automatically.
In your functions, return 0 on success, and a non-zero value on failure.
When possible, use the following syntax to call several functions short-circuiting out if one fails:
Use my_bool in C and bool in C++ source files.
Pass by pointer instead of by reference in the C++ code.
Write optimized code even in the sections where performance is not critical.
Keeping Your BitKeeper Repository Up to Date MySQL source keeps changing with time—a lot during the alpha stage, less during beta, and only very little once it reaches the stable status.
It is recommended that you stay current with the recent developments whether you are just studying or are making modifications to the MySQL source.
Instructions follow on how to update your local BitKeeper repository.
This script executes every time you commit a change, and in its original version it will notify MySQL developers and the public about the details of your change.
This notification is desirable when a MySQL developer makes a commit, but may not be so desirable for you.
If you do want the world to be notified every time you commit, you may keep this script the way it is.
BitKeeper will take a couple of minutes to examine the changed files, and then it will present a GUI dialog asking you to comment on each change you have made.
After you have commented your changes to each individual file, as well as to the entire change set, press the Commit button twice and wait for the BitKeeper window to disappear.
Execute the following command at the Unix shell prompt from anywhere inside the repository tree:
In some rather rare cases, the pull may result in conflicts.
This usually happens when you change the same lines at the same time that some MySQL developer does.
In this event, BitKeeper will print a message on the standard output about failed conflicts and instruct you to run bk resolve.
Do so, and follow the prompts that BitKeeper gives you.
If needed, refer to the BitKeeper documentation by running bk helptool.
Submitting a Patch If you have added a feature or fixed a bug, and would like MySQL developers to consider it for submission, follow the steps in this section.
The instructions assume you have used the commercial version of BitKeeper.
If the patch is reasonably small (under a few kilobytes), include it in the body of the message.
Otherwise, post it at a URL and include the URL in the body of the message.
Make sure to include a brief description of the patch.
The MySQL source code contains several hundred thousand lines and continues to grow.
However, the task is not as difficult and daunting as it appears initially, if you are familiar with the core elements of the code and their respective roles.
The purpose of this chapter is to give you a foundation that will enable you to read most sections of the code with some degree of ease.
This chapter is meant to be a literacy crash course.
We will focus on the core elements of the code that are critical to understanding what is generally going on.
The more specific details of various modules will be discussed in later chapters dedicated to them.
In the discussion of the various code elements, I will inevitably have to leave out a number of less significant class members, API calls, and global variables due to space constraints.
Additionally, by the time this book is printed, a number of new code elements might appear, and a few might change names or functions to some extent.
However, it is reasonable to expect that such cases will be minimal.
The major part of the code we will discuss in this chapter has already stabilized and will not change significantly.
I must also note that again for reasons of space constraints we will have to leave out a large number of vital classes, structures, and macros.
However, I hope that once you become familiar with what we have covered, you will have sufficient background to elicit the additional information through your own study of the code.
It contains the information pertinent to the thread that is handling the given request.
Handling client requests is not the only time a thread is created.
MySQL has a number of system threads, such as the replication slave threads and delayed insert threads.
Additionally, there exists a special case when a thread descriptor object is created without a thread—the server running in bootstrap mode to create the necessary system tables during installation.
Due to the close relationship between client connections and threads, the terms thread and connection are often used synonymously by MySQL developers.
I will do so in the discussion of this class.
It is passed as the first argument in most of the higher-level calls.
With the exception of low-level storage and retrieval operations, few things of significance happen inside the server without some involvement of this class.
Familiarity with its members will give you a good idea of the overall architecture and capabilities of the server.
Most commonly, an instance of THD is pointed to by a variable thd of type THD*
Therefore, if you are trying to find places in the code where a particular member of this class is used, the following command will work almost without fail:
The class consists mostly of data members, which will be the primary focus of this discussion.
There are a few fairly simple and infrequently used methods, which we will not discuss due to the space constraints.
However, once you understand the role of the data members we will discuss, the function of the methods will become apparent to you from their source.
Table 3-1 lists the most prominent members of the THD class.
Note that in version 4.1, part of THD was moved into the newly created class Statement, and THD was made a subclass of Statement.
The table also lists the public members of Statement that were in THD and are still frequently referenced as members of THD.
It is a member of Statement in version 4.1 and later.
Will point to a different object type based on whether the current query is a prepared statement or not.
System variables local to this connection that can be changed by the client.
In other words, the first matching host column value from the mysql.user table.
Note that in MySQL, a user entity consists of two parts: a username and a host name.
Will usually be set before an operation that could take a long time.
As far as the client-server protocol is concerned, aside from some minor limitations, all MySQL versions are forward and backward compatible.
Any client can talk to any server, regardless of the version.
This variable helps the server know how to not confuse clients of older versions.
It also keeps track of whether the client is prepared to use SSL or compression.
This variable is a bit mask of the global privileges for the current connection.
One would think that just like the other two, it would be a bit mask for some kind of column access.
However, this happens to be just a temporary variable used to determine whether the user has some privilege on a table when processing SHOW TABLES.
If she does not have any, the table will be excluded from the output.
A regular table is a non-temporary table that was referenced with a higher-level query such as SELECT, UPDATE, DELETE, INSERT, REPLACE, or ALTER.
Tables listed here are automatically closed at the end of the query.
A temporary table exists in the scope of the given connection, and can be created either manually with CREATE TEMPORARY TABLE, or inside the optimizer when it is unable to retrieve the results of a SELECT by merely examining the query tables.
A derived table is a table resulting from a sub-query in the FROM clause of a SELECT statement.
Note that the term derived table is not precisely defined in the SQL standard, and therefore MySQL documentation refers more precisely to a “subquery in the FROM clause.” However, for historical reasons, “derived” is being used throughout the code.
MYSQL_LOCK* lock A descriptor structure containing the list of all the tables automatically locked by this thread without the use of the LOCK TABLES command.
This type of locking occurs when the server processes regular queries such as SELECT, INSERT, or UPDATE and the need for a lock is discovered.
Only one group of tables may be locked at any given time.
The lock must be acquired and released for the entire group at once.
A descriptor structure containing a list of all the tables locked with LOCK TABLES.
A structure used to store the information about the current POSIX Threads condition that this thread might be waiting for.
Used to wake sleeping threads during shutdown or when they are being killed with the KILL command.
The condition gets artificially broadcast, and the threads awaken, check their killed flag, and realize that they are supposed to exit.
Each server participating in replication must be assigned a unique ID in its configuration file.
When a master performs an update, it logs the originating server ID to the binary update log.
When updates are performed by a regular client, the originating ID is the same as the server ID.
However, the slave thread must preserve the originating ID of the master to avoid infinite update loops.
If this is done, the slave can break the potentially infinite replication loop by skipping update events that have the same ID as the server ID.
By setting the value of this variable during query initialization, the caller is able to control which server ID gets logged with the update.
Used for processing INSERT DELAYED queries, which permit the client to request that the rows be inserted some time in the future when the table becomes available.
Used to manage logical update logging, keeping track of changed tables (for the query cache)
Examples of when status messages might occur: if the server is in the middle of a transaction, if there exists some additional query result data to retrieve, or if the query did not use a key efficiently.
The client, however, is allowed to set the value of the next generated key by executing SET INSERT_ID=value.
When this command is executed, next_insert_id is set to the specified value.
Set to the value of the last generated automatically incrementing unique key.
Available to the client through the SQL function LAST_INSERT_ID( )
MySQL has the capability to restrict the number of connections and queries per hour that a particular user is allowed to perform.
They will be stored in this variable and can be viewed later with SHOW WARNINGS.
Every new query will have a value one higher than the previous query.
This value shows up in the Id column of SHOW PROCESSLIST output and is used as an argument to the KILL command.
Examples of such threads include delayed insert threads, replication slave threads, event scheduler and worker threads, and the NDB cluster binlog thread.
When mysqld is started with the –bootstrap option, it merely executes queries read from the standard input, and exits once the standard input is closed.
However, an instance of THD is created and marked as a special case by setting this variable.
The bootstrap execution mode is used during installation to create the system tables necessary for server operation.
Each thread is responsible for checking this variable frequently during time-consuming operations.
If set, the thread must perform the cleanup as quickly as possible and exit.
MySQL uses a fairly complex protocol on top of the one already provided by the operating system for client/ server communication.
This structure lies at the core of the protocol’s implementation.
A packet can send a command, a message, or a block of data.
Packets can be compressed, or transmitted over the SSL layer.
All network communication functions use NET one way or the other, usually by accepting it as an argument.
Becoming familiar with the members of NET is a major step toward understanding the client/server communication protocol.
The same definition is also used by the client library, which is written in C.
This would exclude any possibility for NET to have any methods.
Now it is also used to support Windows shared memory and named pipe connections.
Initially set to the value of net-buffer-length configuration variable, but may be increased to accommodate a larger packet up to the value of max-allowed-packet configuration variable.
Packet sequence numbers are mainly used for sanity checks in the protocol.
An out-of-order packet sequence number can be caused only by a bug, barring hardware and operating system problems.
Many platforms interrupt potentially successful network I/O with an error for a number of peculiar reasons.
Failing once, therefore, is not a good reason to give up.
Thus, it will read a portion of the next packet.
This variable keeps track of how many extra bytes were read.
When compression is not enabled, the reading peer will try only the exact length of the packet after reading the header first.
No extra bytes will be read, so this variable is not used if there is no compression.
However, this algorithm is not very efficient for small packets.
It is reasonable to suppose that it will be optimized in the future to attempt a larger read from the start.
If this happens, it is very likely that this variable will be used to keep track of how many bytes past the first packet boundary have been read.
Not to be confused with the length of the packet itself, which for instance does not include the header.
Additionally, the buffer may contain a carryover from the next packet if compression is used.
When compression is used, that byte is not safe to overwrite with a zero without saving it first.
This variable is used to save that byte and restore it later.
If this variable is set to 1, no OK packet will be sent.
A table can exist in an open or closed state.
In order to be used in the server, it has to be opened.
Whenever this happens, a table descriptor is created, and placed in the table cache to be reused later when another request is made that references the same table.
Instances of TABLE are frequently referenced in the parser, optimizer, access control, and query cache code.
Studying its members is a good way to become acquainted to a degree with the low-level details of the server implementation.
This structure is defined in sql/table.h as struct st_table, but then is aliased to TABLE with a typedef in sql/handler.h.
A buffer containing the text of the last error message sent to the client.
In earlier versions, the drivers had to figure out the state themselves from the value of the error code.
In version 4.1, this would also include the reporting of SQL state.
The object is used for all low-level data storage and retrieval operations.
Field **field Array of field descriptors for each field in this table.
Note that this refers to the length of the record when it is processed in memory on the optimizer level, not when it is stored by the storage engine.
A column is counted once for each key it is a part of.
This would include all the keys present in the table that have not been taken offline with ALTER TABLE...
Note that in 4.0 and earlier, the map is implemented through a simple bit mask.
This is basically the value of keys_in_use after some filtering that comes from taking into account the FORCE KEY and IGNORE KEY directives in the query.
The length of the array is stored in the variable keys.
This is not necessarily a hard limit on how many rows the table can have, but more of a hint to help the storage engine figure out the best record storage format.
Currently stored in the table definition file (.frm), but otherwise not used.
This is a hint to the storage engine as to what the average length of a record in a variable length record table is expected to be.
Provides an idea of what is found in the record variable.
Field *next_number_field If the field has an auto-increment field (there can be only one), points to its descriptor.
If the field has a timestamp field (there can be only one), this points to its descriptor.
So table_cache_key is often used in the code to reference the name of the database.
It is actually a base abstract class for a number of subclasses defined for each specific field type, such as integer, string, or timestamp.
This class naturally plays a critical role in the parser and optimizer, because most of the operations in processing a query involve table fields.
Field is defined in sql/field.h, and partially implemented in sql/field.cc.
The implementation is partial because it is an abstract class.
Its subclasses, which all have names beginning with Field_, complete the implementation.
This variable is a bit mask with the one bit corresponding to the table number set, and all others cleared.
If another thread performs FLUSH TABLES, the table descriptor is no longer valid.
This is detected by comparing the value of this variable against the global refresh_version.
If this technique is used, the appropriate key is created in the temporary table.
In that case this field is set to the descriptor of the GROUP BY expression.
Used by the range optimizer to store the estimate of how many records the key range will match for each key in the table.
This class has only a few data members, but on the other hand does contain many methods.
We will, therefore, cover all of the data members, and only the most important methods.
LEX_STRING comment The contents of the comment on this field.
The comment can be entered in the CREATE TABLE statement when the field is defined; for example, CREATE TABLE t1 (n INT COMMENT 'some integer value')
This would be the case for a B-tree key, but not for a hash or full-text key, for example.
Stores the string pointed to by from in the in-memory copy of the record associated with this field descriptor.
Stores the time value specified by ltime in the in-memory copy of the record associated with this field descriptor.
Note that the caller must pass a pointer to a preallocated String object.
Currently used by the range optimizer to decide whether the range optimization would be appropriate.
Because a is stored as a string, the key order will be lexicographic, and '10' would make it into the range.
However, the syntax of the query asks for a numeric comparison.
So the record with a equal to '10' should be excluded.
This is so that they could be compared with integers as integers rather than converting the integer to a string and performing string comparisons.
A char(N) field with default attributes, for example, would not be such a field: the comparison is case-insensitive, and the trailing spaces are ignored.
Returns the result of comparison of str against the value in the inmemory copy of the record associated with this field descriptor in the context of keys.
Utility API Calls A number of core jobs, such as memory allocation, string operations, or file management, are performed by a group of internal API calls.
Due to portability requirements, the standard C library is used very sparingly, and the C++ libraries are not used at all.
I cannot cover them all in this book, but I provide a representative sample in Table 3-5
Used for allocating memory blocks for global buffers and other objects that have a lifetime of more than one query, as well as large memory blocks.
Nevertheless, my_free( ) is still what is used throughout the code to free memory blocks.
When the work is done, block should be freed using my_free( )
Returns a pointer to the allocated block, or 0 on failure.
Among other capabilities, this will keep reading until all of the Count bytes have been read successfully if MY_FULL_IO is set in MyFlags.
Will keep writing until all the Count bytes have been written.
If MY_WAIT_IF_FULL is present in MyFlags, will wait for the disk space to become available instead of just failing when the disk is full.
An I/O cache is somewhat similar to the standard C library structure FILE.
Returns 0 on success, and a nonzero value of failure.
Technically speaking, this is actually a preprocessor macro that is aliased to the _my_b_read( ) function when there is not enough data in the cache buffer to satisfy the request.
Returns 0 on success, and a nonzero value of failure.
Technically speaking, this is actually a preprocessor macro which is aliased to the _my_b_write( ) function when there is not enough space in the buffer, and a physical write is required to satisfy the request.
Returns 0 on success, and a nonzero value of failure.
Returns 0 on success, and a nonzero value of failure.
The arguments should be formatted in the Unix style with forward slashes.
On Windows, the slashes will be reversed in the result.
The flags in the last argument allow a number of file path operations.
For example, if MY_RESOLVE_ SYMLINKS is set, the resulting path written to the to argument will have the symbolic links followed and resolved.
Returns 0 on success, and a nonzero value on failure.
Returns a pointer to the record on success, and 0 on failure.
Should be called repeatedly after the initial call to hash_ search( ) for the retrieval of subsequent records associated with the key.
Returns 0 on success, and a nonzero value of failure.
Note that the actual pointer value—not the key— is compared, and only one record is deleted.
Note that unlike in the hash, the operations are conducted on the element data, as opposed to pointer references.
So, it becomes necessary to know the size of the element.
Note that the second argument is a pointer from which the data will be copied into the array upon insertion.
Note that element must point to a location with sufficient memory to cover the size of array element.
Used for cleanup after the caller is done using the array.
Preprocessor Macros MySQL makes heavy use of the C preprocessor.
A number of tasks are complex enough to justify some form of an alias rather than being spelled out in the code, but are still too simple to justify a function.
Other tasks are performed differently—or in some cases, not at all—depending on the compilation options.
Returns a pointer to the terminating null character of the result.
Should be used for all small memory allocations while processing a query.
Note that all blocks allocated with sql_alloc( ) are freed when the query is finished.
It is neither necessary nor possible to free individual blocks allocated with sql_alloc( )
On a little-endian architecture, the macro is a mere pointer dereference.
However, on a big-endian system it has to perform a computation to return the correct value.
Rather than waste the CPU by initializing them all the time, this macro exists to initialize them when the use of one of those tools is detected, or when FORCE_INIT_OF_VARS is defined.
Global Variables MySQL code uses a large number of global variables for various purposes: configuration settings, server status information, various data structures shared among threads, and other things.
Studying the global variables provides numerous insights on the server architecture.
Often the very existence of the variable concisely tells a story about how and why different components work together.
The value is displayed in the output of SHOW STATUS under Opened_tables.
Not to be confused with the value of Open_tables, which is the number of tables currently in the table cache.
However, it is not possible in some situations due either to the table size or the limitations of the in-memory storage engine.
Then the table is created on disk, and this counter is incremented.
The value is displayed in the output of SHOW STATUS under Aborted_clients.
The value is displayed in the output of SHOW STATUS under Aborted_ connects.
This is currently an internal variable that cannot be set by the user.
Can be viewed by the user with SHOW STATUS LIKE 'Com_%'
Primarily used to avoid initiating the shutdown process more than once.
Note that some of the existing threads might be just waiting for a request instead of processing one.
I_List<THD> threads sql/mysqld.cc A list of all threads that currently exist in the server.
Can be viewed by the user with SHOW PROCESSLIST or SHOW FULL PROCESSLIST for more detail.
A list of MyISAM key caches that exist in the server.
A descriptor of the collection of server configuration variables that can be modified by a client.
Contains the limits on the values of the server configuration variables that can be modified by a client.
When a table is opened, the descriptor is placed into the table cache.
Subsequent requests to open the same table will be satisfied from the cache.
The contents of the cache can be viewed with SHOW OPEN TABLES.
In this chapter we will discuss the details of the client/server communication in MySQL.
The goal is to give you the ability to look at a binary dump of the client/ server communication and be able to understand what happened.
This chapter can also be helpful if you are trying to write a MySQL proxy server, a security application to audit MySQL traffic on your network, or some other program that for some reason needs to understand the low-level details of the MySQL client/server protocol.
Protocol Overview The server listens for connections on a TCP/IP port or a local socket.
When a client connects, a handshake and authentication are performed.
The client sends a command, and the server responds with a data set or a message appropriate for the type of command that was sent.
When the client is finished, it sends a special command telling the server it is done, and the session is terminated.
Packet Format There are two types of packets: compressed and noncompressed.
The decision on which one will be used for the session is made during the handshake stage, and depends on the capabilities and settings of both the client and the server.
Additionally, regardless of the compression option, the packets are divided into two categories: commands sent by the client, and responses returned by the server.
Server response packets are divided into four categories: data packets, end-of-datastream packets, success report (OK) packets, and error message packets.
A compressed packet will have an additional 3-byte field, low byte first, containing the length of the compressed packet body part that follows.
An uncompressed packet will have the body immediately after the header.
The compression is done with the use of ZLIB (see http://www.zlib.net)
The body of the compressed packet is exactly what a call to compress( ) with the uncompressed body as argument would return.
It is, however, possible for the body to be stored without compression when the compressed body would turn out no smaller than the uncompressed one, or when compress( ) fails for some reason—e.g., due to the lack of available memory.
It is important to remember, though, that even in that case, the compressed format is still used, which unfortunately results in the waste of 3 bytes per packet.
Therefore, a session that predominately uses small or poorly compressible packets goes faster if the compression is turned off.
What if you need to send a bigger packet? In version 3.23 and earlier, it is not possible.
Version 4.0 added a compatible improvement to the protocol that overcame this limitation.
The last short packet will always be present even if it must have a zero-length body.
It serves as an indicator that there are no more packet parts left in the stream for this large packet.
Relationship Between MySQL Protocol and OS Layer If you try to run a network sniffer on the MySQL port, you will notice that sometimes several MySQL protocol packets are contained in one TCP/IP packet, and sometimes a MySQL packet spans several TCP/IP layer packets, while some fit into exactly one TCP/IP packet.
If you somehow manage to intercept the local socket traffic, you will observe a similar effect.
Some buffer writes will have exactly one packet, while others may contain several.
If the lower-level socket-buffer write operation has a limit on the maximum number of bytes it can handle in one chunk, you may also see one MySQL packet being transferred in several buffer writes.
While the correct packet sequencing is ensured by the underlying transmission protocol, this field is used for the sanity checks of the application logic.
To understand the mechanics of this phenomenon, let’s examine the API the server or the client uses to send packets.
When the buffer has reached capacity, its contents will be flushed, which results in an operating system write( ) call on the socket—or possibly a sequence of them if the contents of the buffer cannot be written into the socket in one operation.
On the operating system level this may result in sending one or more packets, depending on how much it takes to accommodate the data volume under the operating system protocol constraints.
In some cases, the data in the network buffer needs to be sent to the client immediately.
Authenticating Handshake The session between a client and a server begins with an authenticating handshake.
Before it can begin, the server checks whether the host that the client is connecting from is even allowed to connect to this server.
If it is not, an error message packet is sent to the client notifying it that the host is not allowed to connect.
The length is variable, and is calculated according to the formula in the Length column.
The subsequent offsets are a function of the length of this field.
With the terminating zero byte, the length of the string is one greater than the value of the macro.
See Table 4-5 later for the meaning of different bits.
A character set collation is a set of rules that defines a sequential order among characters.
A list of available collations and their codes can be obtained by executing SHOW COLLATION LIKE '%' in version 4.1
Reports whether the server is in transaction or autocommit mode, if there are additional results from a multistatement query, or if a good index (or some index) was used for query optimization.
The rest of the random seed string terminated with a zero byte.
Fields of the client’s credentials packet, up to MySQL version 4.0
Protocol capabilities bit mask of the client, low byte first.
Maximum packet length that the client is willing to send or receive.
Zero values means the client imposes no restrictions of its own in addition to what is already there in the protocol.
Varies; see description Credentials string in following format: zero-terminated MySQL username, then if the password is not empty, scrambled password (8 bytes)
This can be optionally followed by the initial database name, in which case a zero byte terminator is added immediately after the XOR encrypted password, followed by the database name string without a terminating zero byte.
Fields of the client’s credentials packet, MySQL version 4.1 and later.
Maximum packet length that the client is willing to send or receive.
Zero values means the client imposes no restrictions of its own in addition to what is already there in the protocol.
Default character set (or more precisely, collation) code of the client.
If the SSL capability option is enabled both on the client and on the server, the client will first send the initial part of the response packet without the credentials string.
When the server receives it, it will see the SSL capability bit enabled in the capabilities mask, and know that it should expect the rest of the communication in SSL.
The client switches to the SSL layer, and resends the entire response packet securely this time.
It would be more efficient, of course, to not resend the initial part of the response, but, for historical reasons, this small overhead allowed the code on the server to stay fairly clean without thorough rework.
Once the server receives the credentials packet, it verifies the information.
From this point, it can respond in three different ways:
If the check succeeds, the standard OK response packet is sent (for details, see the “Server Responses” section, later in the chapter)
If the credentials did not meet the expectations of the server, the standard error message response is sent.
If the entry for that user has the old-style password hash, it is impossible to authenticate with the new authentication protocol.
The server responds with either OK or a standard error message.
At this point the handshake is complete, and the client begins to issue commands.
Authentication Protocol Security Neither the old nor the new protocol ever sends the user password across the connection in plain text.
However, there are a number of weaknesses in the old protocol.
First, knowing the value of the password hash allows the attacker to perform authentication without actually knowing the password.
Therefore, if the attacker can get read access to the user table in the mysql database, or obtain the value of the stored password hash some other way, she will be able to authenticate with a specially modified version of the MySQL client library.
Second, even without having access to the hash, the correct password can be guessed in a small number of attempts if the attacker can intercept the authentication traffic.
This is possible due to the weakness in the encryption method of the old protocol.
The encryption is done using a home-cooked XOR procedure (see the scramble_323( ) function mentioned earlier), which lacks true cryptographic strength.
The authentication method now uses SHA1 hashes for encryption, which are much more resistant to cracking.
Despite the added improvements, do not feel complacent about the security of the new protocol.
It is still recommended to block access to the MySQL port on the firewall, and if this is not possible, require the clients to use SSL.
Protocol Capabilities Bit Mask During the authentication handshake, the client and the server exchange information on what the other is able or willing to do.
This enables them to adjust their expectations of their peer and not send the data in some unsupported format.
The exchange of information is accomplished through fields containing the bit mask of protocol capabilities.
The server, regardless of the version, always announces its capabilities with a 2-byte bit mask.
Although both newer clients and servers understand the 4-byte mask, the first packet in the dialog must be understood by any client regardless of the version.
For this reason, even the newer clients expect the greeting packet to contain a 2-byte mask.
Once the client knows that it is talking to a newer server, it can announce its capabilities with a 4-byte mask.
However, if the newer client detects that it is talking to an older server, it will announce the capabilities with only a 2-byte mask.
Table 4-5 explains the meaning of the bits used in the capabilities’ bit mask.
If this flag is set, the server is being asked to report the number of records that were matched by the WHERE clause.
Not all of those will necessarily be updated, as some may already contain the desired values.
If this flag is cleared, the client is old and wants only 1 byte for field flags.
This flag will also be set by the modern server to indicate that it is capable of sending the field definition in the new format with 2 bytes for field flags.
It indicates that the initial default database can be specified during authentication.
At this point, it does not appear to be used.
For the server, this means that a different inactivity timeout value should be applied.
The former is for regular clients, while the latter is for the interactive ones.
This distinction was created to deal with applications using buggy persistent connection pools that would lose track of established connections without closing them first, keep creating new ones, and eventually overflow the server max_connections limit.
The workaround was to set wait_timeout to a low value that would disconnect the lost connections sooner.
This, unfortunately, had a side effect of disconnecting interactive clients too soon, which was solved by giving them a separate timeout.
Command Packet Once the authentication is complete, the client begins sending commands to the server using command packets.
The body of a command packet is documented in Table 4-6
However, a thread in a threaded application on some platforms may get a SIGPIPE signal spuriously under some circumstances.
Version 4.1 just blocks it during the client initialization and does not worry about the issue from that point on.
When present in the client packet, indicates that the client is aware of servers that support transactions.
Table 4-7 documents different types of commands with their codes and arguments.
Command code enum value Code numeric value Argument description Command description.
Tells the server to change the default database for the session to the one specified by the argument.
Tells the server to return a list of fields for the specified table.
This is an obsolete command still supported on the server for compatibility with old clients.
Tells the server to create a database with the specified name.
This is an obsolete command still supported on the server for compatibility with old clients.
Tells the server to drop the database with the specified name.
This is an obsolete command still supported on the server for compatibility with old clients.
Tells the server to refresh the table cache, rotate the logs, reread the access control tables, clear the host name lookup cache, reset the status variables to 0, clear the replication master logs, or reset the replication slave depending on the options in the bit mask.
Tells the server to send back a string containing a brief status report.
Tells the server to send back a report on the status of all running threads.
This is an obsolete command still supported on the server for compatibility with old clients.
A 4-byte integer with the low byte first containing the MySQL ID of the thread to be terminated.
Tells the server to terminate the thread identified by the argument.
This is an obsolete command still supported on the server for compatibility with old clients.
Tells the server to dump some debugging information into its error log.
Command code enum value Code numeric value Argument description Command description.
Tells the server the client wants to change the user associated with this session.
Tells the server to send a continuous feed of the replication master log events starting at the specified offset in the specified log.
Used by the replication slave, and in the mysqlbinlog command-line utility.
Tells the server to send the table definition and data to the client in raw format.
Used when a replication slave receives a LOAD DATAFROMMASTER query.
Tells the replication master server to register the slave using the information supplied in the argument.
This command is a remnant of the started fail-safe replication project.
It was introduced in the early version 4.0, but not much has changed since.
It is possible that this command might get removed in the future versions.
Tells the server to prepare the statement specified by the argument.
Tells the server to execute the statement referenced by the statement ID.
Command code enum value Code numeric value Argument description Command description.
Adding it anywhere else would alter the numeric codes of the commands and thus break all of the commands after the point of the insertion in older clients.
This requirement allows us to easily track the history of features to a certain extent.
Tells the server the packet contains the data for one bound parameter in a prepared statement.
Used to avoid unnecessary copying of a large amount of data when the value of the bound parameter is very long.
Tells the server to close the prepared statement specified by the statement ID.
Tells the server to enable or disable the option specified by the code.
At this point, seems to be used only to enable or disable the support of multiple statements in one query string.
Command code enum value Code numeric value Argument description Command description.
Server Responses Once the server receives a command, it processes it and sends one or more response packets.
Data Field Data fields are critical components in many of the server response packets.
A data field consists of a length specifier sequence followed by the actual data value.
The length specifier sequence can be understood by studying the definition of net_store_ length( ) from sql/pack.c:
It must be noted that all length values following the code are stored with the low byte first.
It indicates that there is no length value or data following the code, and the value of the field is the SQL NULL.
Why such a complexity? Most of the time the data field is fairly short, and, especially if a query returns a lot of records and/or selects a lot of columns, there could be many of them in the response.
Wasting even a byte per field in this situation would add up to a large overhead.
Immediately after the length sequences is the actual data value, which is converted to a string representation.
Note that in version 4.1, when returning the data for prepared statements fields and when the data value is not a string, the data is sent in the raw binary format with the low byte first without a length specifier.
COM_QUERY if the query does not need to return a result set; for example, INSERT, UPDATE, or ALTER TABLE.
This type of packet is appropriate for commands that do not return a result set.
Its format, however, permits sending some extra status information, such as the number of modified records, the value of the automatically generated primary key, or a custom status message in a string format.
The structure of the packet body is documented in Table 4-8
A byte with the value of 0, indicating that the packet has no fields.
I will refer to its length as rows_len to express the subsequent offsets.
To send an OK packet from inside the server, you must call send_ok( )
In version 4.1 and later, the function is declared in sql/protocol.h, and defined in sql/protocol.cc.
Error Packet When something goes wrong with the processing of a command, the server responds with an error packet.
The value is stored in the field length format of a data field.
I will refer to the length of this value as id_len.
In the protocol of version 4.0 and earlier, the status field is present only if it is a nonzero value.
In the protocol of version 4.1 and later, it is reported unconditionally.
Contains the number of warnings the last command has generated.
For example, if the command was COM_QUERY with LOAD DATA INFILE, and some of the fields or lines could not be properly imported, a number of warnings will be generated.
The client will always treat a response packet starting with a byte containing 255 as an error message.
The field will not be included if the server is talking to a very ancient pre-3.23 client, and the subsequent offsets should be adjusted accordingly in that case.
In the pre-4.1 era, there was nothing else in the body in addition to this byte.
Result Set Packets A large number of queries produce a result set.
Any time the expected information from a query is more than a simple status report, a result set is returned.
A packet with the body consisting of the standard field-length specifier sequence.
However, this time, the meaning of the number is different.
It indicates the number of fields in the result set.
A group of field description packets (see the upcoming explanation for the format description), one for each field, in the field order of the result set.
Because most of the packet elements have variable lengths, the offsets are dependent on the content of the previous fields.
I will, therefore, omit the offset column in the format descriptions.
Format of server’s result set sequence, versions 4.0 and earlier.
Varies Table name of the field in the data field format.
If the table was aliased in the query, contains the name of the alias.
Varies Column name of the field in the data field format.
If the column was aliased in the query, contains the name of the alias.
The idea is to make the sequence look like a standard data field.
If present, contains the default value of the field in the standard field data format.
Format of server’s result set sequence, versions 4.1 and later.
Data field (see the section “Data Field,” earlier in this chapter) containing the ASCII string def.
Varies Database name of the field in the data field format.
Varies Table name of the field in the data field format.
If the table was aliased in the query, contains the name of the alias.
Varies Table name of the field in the data field format.
If the table was aliased in the query, contains the original name of the table.
Varies Column name of the field in the data field format.
If the column was aliased in the query, contains the name of the alias.
Varies Column name of the field in the data field format.
If the column was aliased in the query, contains the original name of the table.
The idea is to make the sequence look like a standard data field.
Following the field definition sequence of packets, the server sends the actual rows of data, one packet per row.
Each row data packet consists of a sequence of values stored in the standard field data format.
When reporting the result of a regular query (sent with COM_QUERY), the field data is converted to the string format.
When using a prepared statement (COM_PREPARE), the field data is sent in its native format with the low byte first.
After all of the data rows have been sent, the packet sequence is terminated with an EOF packet.
If present, contains the default value of the field in the standard field data format.
Format of server’s result set sequence, versions 4.1 and later (continued)
Much can be learned about the internal workings of MySQL server by studying its configuration variables.
In some cases, the very existence of a variable with a certain name tells a story.
For example, key_buffer_size reveals that MySQL uses a key cache.
Other option names are perhaps not as self-explanatory, but you will learn a lot by asking yourself why that option exists, and studying the source to find out how the different settings affect the behavior.
Some are there because some platform-specific bug needed to be tracked down or worked around at some point.
Others exist just to allow the user to choose a file or a directory used for some internal operation, but their very existence permits us to take a peek at what MySQL is doing behind the scenes.
Due to the space constraints, we cannot hear all of those stories, so we’ll focus on the most interesting ones.
Configuration Variables Tutorial To compensate for the lack of coverage for many configuration variables in this chapter, this section contains a brief “nuts-and-bolts” tutorial on how option parsing works.
This will enable you to track down unfamiliar options in the source, as well as add your own.
Configuration File and Command-Line Options mysqld can receive configuration variable settings on the command line, or it can read them from configuration files.
There can be multiple configuration files, and their contents can be merged with the command-line configuration options.
By default, other locations are also searched after a successful or unsuccessful attempt to load /etc/my.cnf in the given order: first my.cnf in the directory specified by a compiled-in macro DATADIR, and then .my.cnf (note the initial period) in the home directory of the real (rather than effective) user that started mysqld.
Note that the loading attempts in this sequence continue regardless of whether the previous attempt succeeded or failed.
To find out which configuration files mysqld is loading on Linux, you can run the following shell command:
The error message from strace about ptrace( ) on the last line can be safely ignored.
The important parts of the output are the traced calls to stat64( )
The first argument in each of those calls is the name of the configuration file that mysqld is trying to load.
You can see that the first file exists, while the other two do not.
Another way to get the same information is to run:
This is free software, and you are welcome to modify and redistribute it under the GPL license.
The message, among other things, tells us that it is going to check /etc/my.cnf, ~/.my.cnf, and /usr/etc/my.cnf in that order.
The loading of the configuration file can be disabled with the command-line argument --no-defaults.
When --defaults-file is given, the compiled-in configuration files sequence is skipped, and the specified file is loaded instead.
As a consequence, the two options cannot be used together.
What happens when several conflicting configuration option sources are combined? A good way to understand what happens and why is to examine what mysqld does behind the scenes to process the configuration options.
It does this so that it can insert the configuration file options into the list and make it look to the command-line argument processing code as if those options have been specified on the command line.
It is important to note that the arguments from the configuration files get inserted into the list before the regular command-line arguments in the order that the files are processed.
What happens if the same variable in that list is set more than once? From the logic of the argument processing, we can see that the setting that appears last in the list will be the one that will actually take effect.
For security purposes, the --user option is not allowed to be reset with a subsequent option value in the processing chain.
Therefore, we can observe that in the case of conflicting settings with some exceptions, the command-line arguments have the highest precedence.
After that, it is .my.cnf in the home directory of the real user, my.cnf in the compiled-in DATADIR, and last of all /etc/ my.cnf.
Although mysqld provides many options to load the configuration, the recommended way is to use only /etc/my.cnf and make sure that other configuration files mysqld is interested in do not exist, as well as that no command-line arguments are given.
The configuration file follows the format informally defined as follows:
There can be several sections, each often used by a program with the same name.
Comments can be put on their own lines or at the end of an option line, and start with #
For the numeric option values, K, M, or G suffixes can be used to indicate kilobytes, megabytes, or gigabytes.
Historically, the server configuration parameters have been divided into two groups: options and variables.
In version 3.23, numeric variables had to be set with the setvariable option.
True to its commitment to backward compatibility, later versions of MySQL still support the 3.23 style syntax for setting variables.
However, a rewrite of the configuration parameter processing code in 4.0 has eliminated the distinction.
Table 5-1 lists its members in the order they are defined in the structure.
On the command line, the option name is prefixed with a double hyphen: --
If the code fits within the printable ACSII character range, it is also used for the short (prefixed by a single hyphen) form of the commandline option.
For example, if the value is the ASCII code for b, this option can be specified with -b on the command line.
The type of the variable pointed at should be specified by the appropriate value of the var_type member.
Apparently was intended to point to an array of possible string values for the option.
Set it to 0 if you are adding your own option.
If a lower value is specified, the actual value of the option is set to the minimum value.
If a higher value is specified, the actual value of the option is set to the maximum value.
If processing an option is as simple as just initializing a variable, it is sufficient to provide an appropriate address and a variable type in the definition of the corresponding member of the my_long_options array.
This function is called for each option by handle_options( ) after the value of the option has been initialized.
When the corresponding option is parsed, the variable will be pointed to the location containing the option value.
In other words, it points to somewhere in the middle of one of the members of the argv array.
Thus, the value of the option can end up either in a predefined location allocated by the caller, or in a location allocated by the options parser.
If used, the parsing is aborted and an error code is returned.
Currently, this syntax is used to support configuration of multiple key caches in MyISAM tables, but could be used for other things in the future.
OPT_ARG The option may accept an argument, but it is not an error to not provide one.
In that case the value of the variable will be set to its default value.
This type is usually used for options that tell MySQL to log something, and may optionally specify the location of the log, or for options that enable a feature that has several different modes of operations, with one being a very reasonable default, and others being somewhat obscure.
REQUIRED_ARG The option requires the user to provide an argument.
This type is usually used for numeric variables, or for other options where it does not make sense to just name the option and expect the server to supply a reasonable default.
Example of Adding a New Configuration Option Let us consider an example of adding a simple new configuration option.
On occasion, when trying to start mysqld, a problem may arise.
A stale instance of mysqld might be running and using the resources that the new instance will try to acquire.
We will add an option, kill-old-mysqld, that kills an old instance of mysqld if such is present.
The code shown in this section is available from this book’s web site, as listed in the preface.
For this example, I assume that you have gone through the steps in Chapter 2, and have a source tree where you have previously had a successful build.
Then we add a global variable with the following definition:
The location of it is not relevant in terms of code functionality, but it is a good idea to follow established conventions.
Now the option is recognized by the parser, and the variable gets initialized.
Our next step is to actually do something when that option is present.
For the sake of simplicity we do not deal with the case of a partial read, and leave it as an exercise for the meticulous reader.
Because we have modified only sql/mysqld.cc, it is sufficient to run make only in the sql directory.
If you made the modifications with no typos, it will produce a new mysqld binary with the support for the new option.
You may have noticed from the comments in the source that, to keep this example simple, I have left a lot of dirty work as an exercise for the reader.
Hopefully, this will help you appreciate the challenges of the MySQL development team.
Due to its requirements for portability and robustness, even simple additions to the code base involve a lot of error checking and handling, and a lot of portability workarounds.
There is a long road from “it works for me” to “it is ready for production release.”
Interesting Aspects of Specific Configuration Variables Now that you understand the general handling of configuration variables, this section presents the stories of particular variables that affect mysqld significantly.
However, in some cases this grim task just has to be done.
Then, if at all possible, it will try to use an in-memory temporary table.
Unfortunately, the size of the table cannot always be estimated in advance.
Sometimes in the process of populating the table, the maximum in-memory table size limit is reached (the limit is controlled by the tmp_table_size setting)
When this happens, the temporary table needs to be converted to a disk type (i.e., MyISAM)
This means re-creating the table and repopulating it with the rows collected in the in-memory table up to this point.
For a typical MySQL usage, the need to convert an in-memory table to disk is a rare occurrence.
However, there are applications that run into this situation a lot.
If you know in advance that the temporary result is going to be more than can be stored in memory, the big-tables option comes in handy.
It tells the server to not even bother creating an in-memory table, and to start with a disk-based table right away.
When enabled, big-tables can still be overridden with the SQL_SMALL_RESULT query option for one particular query.
Just like many others in this file, this is a very large function.
Once you have found the start of the definition, you will probably need to use the search function of your editor again.
This issue gained a good amount of publicity in the open source community when the popular development site SourceForge (http://sourceforge.net) discovered with its own benchmark that MySQL did not scale very well with its application, and migrated to PostgreSQL.
The degradation in performance under high load was believed to be caused by the lock contention.
While read locks are shared, the write lock is exclusive.
If one thread is updating just one record, every other thread that wants to read or write to some other record must enter the queue to wait for the lock.
This means you have to suspend it, and there is a context switch.
If this happens enough, pretty soon all your CPU is doing is switching between threads instead of doing the work.
The assumption turned out to be incorrect, at least to a large extent.
The problem was attributed to the inability of LinuxThreads to deal efficiently with frequently acquired and released mutexes, something MySQL server had to do a lot of.
After a patch was applied to LinuxThreads, the benchmarks that performed a heavy mix of reads and writes scaled just fine as long as both types of queries were properly optimized.
While the general case for a minimal conflict type of lock would have been fairly difficult, in one special case the lock contention could be minimized with only a few changes to the code.
When a record is inserted into the table, the MyISAM storage engine first tries to find a previously deleted record whose space is large enough for the new record, and overwrites that space with the new record.
However, if there are no records marked as deleted, the record is written at the end of the datafile.
In the latter case, it turned out not to be so difficult to allow the INSERT and the SELECT operations to proceed concurrently.
When this option is enabled, the MyISAM storage engine attempts to use this optimization whenever possible.
To learn more about the concurrent insert, study these files in the myisam directory:
It is even more challenging when a crash happens, but no core file is produced.
Sometimes you need that core file badly, as the crash cannot be duplicated in a debugger.
And some platforms are not particularly anxious to generate a core file when threads are used.
In the unfortunate event of a crash, this option will engage the full power of the voodoo black magic known as MySQL in order to coax the uncooperative kernel to write out a core file.
As MySQL AB made the transition from being a small company just trying to make a good database to a bigger entity trying to make an impression in the corporate world, it was discovered that IT managers respond to the term “storage engine” much better than “table type,” which is perhaps a more intuitive term for a MySQL hacker.
Due to its development history and Monty’s insight, MySQL ended up with a very powerful architecture that abstracts the storage engine from the parser and optimizer enough to allow multiple storage engines.
One type of storage engine, MEMORY, stores tables only in memory.
MyISAM provides persistent storage and a number of fancy features such as full-text search and spatial indexing, but does not have transactions or row-level locks.
InnoDB provides transactions and row-level locks, but it is slower on some operations than MyISAM and requires more disk space.
Depending on the need of your application, you can pick the right type of storage engine on a per-table basis.
The storage engine can be specified when creating a table.
It can also be changed for existing tables with the ALTER TABLE command.
Normally, the server flushes the changed key blocks out of the MyISAM key cache at the end of every query.
One approach to this performance problem is to delay the key block flushing.
When the key writes are delayed, the changed blocks are not flushed out at the end of a query.
All of the tables are removed from the table cache with FLUSH TABLES.
The table is removed from the table cache with FLUSH TABLE.
The table is displaced from the table cache with a new table.
The changed key blocks are displaced from the key cache with new blocks.
If delay-key-write is set to ON, only the tables with the DELAY_KEY_WRITE=1 setting are handled this way.
When the setting is ALL, all of the MyISAM key writes are delayed regardless of the table options.
When set to OFF, delayed key writes do not happen regardless of the table options.
The advantage of using this option is the performance gain.
The disadvantage is a higher risk of table corruption should a crash happen.
In contrast, a regular Btree index can only be used to look up records based on the entire value or at least a prefix of the key.
This option represents one of the many full-text search configuration options.
During full-text indexing, in order to improve the quality of the index, some frequently used words are ignored.
For example, if the text column contains regular English sentences, there is little value to indexing the word the, as it will appear in an overwhelming majority of the records.
It works very well if the text you are indexing is a collection of standard English sentences, but it will likely be unsuitable if used for other languages or if the collection does not contain regular text.
Note that if you change the stop word list, it is necessary to reindex the existing tables, which can be done with REPAIR TABLE tbl_name QUICK.
To learn more about how full-text indexing works, take a look at:
It controls how much memory is used to cache both InnoDB table data and indices.
Note that InnoDB differs from MyISAM in the way the table data is cached.
MyISAM caches only the keys, and simply hopes the OS will do a good job caching the data.
InnoDB does not put any faith in the OS and takes the matter of caching the data into its own hands.
The same file also contains an extensive explanation of the buffering internals.
It tries very hard to make sure that the data is still consistent with the absolute minimum loss even if you turn the power off in the middle of a transaction.
However, a fine balance must be achieved between performance and data safety, and each application has its own standards.
InnoDB maintains a transaction log that is used for recovery during server startup.
The recovery is attempted regardless of whether there was a crash or not.
In the case of a crash, the log has pending transactions to redo.
If there was no crash, no pending transactions are found in the log, so there is nothing to be done.
We can see, therefore, that the integrity of the transaction log is crucial in the recovery process.
One way to ensure its integrity is to flush it to disk every time a transaction is committed.
This allows us to recover every transaction that gets committed in the case of a crash.
Each log flush implies at least one disk write, and even with modern disks you can only do so many of them per second, although InnoDB can group commits to overcome this limitation to a certain extent.
This problem can be addressed by slightly reducing the stringency of the data safety requirements, and flushing the log to disk only once per second.
With this approach, under the assumption of intact disk I/O (something we can expect from properly functioning hardware and operating system), our data is still consistent but could be up to one second old after the recovery.
For many applications this is a negligible risk and is worth the hundred-fold or so improvement in performance that comes from a dramatic reduction in disk writes.
When this value is 1, during each transaction commit the log buffer is written out to the logfile, and the flush-to-disk operation is performed on the file descriptor.
When set to 2, during each commit the log buffer is written out to the file descriptor, but the flush-to-disk operation is not performed on it.
However, the flushing on the file descriptor takes place once per second.
I must note that the once-per-second flushing is not 100 percent guaranteed due to the process scheduling issues (for example, we might not get the CPU right at the time when we would like to flush), but an attempt is made, and except for the cases of extreme CPU overload, the actual intervals are very close to one second.
When the server gets the CPU, it will check whether it has been a second since the last flush, and do another if it is time.
When InnoDB was introduced into MySQL, many users missed the convenience of table manipulation on the file system level.
Initially InnoDB tables could reside only in the tablespace file or raw device.
However, version 4.1.1 added the ability to place each table in its own file.
Nevertheless, this does not give the user the freedom to manipulate those files like MyISAM.
The backup must be taken either when the server is down, or after all transactions have been committed and no new ones have started.
This means that it does not take a regular non-Boolean argument.
If present, it is on; if absent, it is off.
This can bring great performance benefits for a wide variety of applications, but it unfortunately also introduces a problem: potential deadlocks.
Let’s say, for example, that thread 1 acquires an exclusive lock on record A.
In the meantime, thread 2 acquires an exclusive lock on record B.
In the meantime, thread 2 is trying to lock record A while still holding the lock on B.
Thus neither one can progress, and we have a deadlock condition.
While it is possible to use an algorithm that avoids potential deadlocks, such a strategy can very easily cause severe performance degradation.
Normally deadlocks are very rare, especially if the application was written with some awareness of the problem.
Therefore, instead of preventing them, InnoDB just lets them happen, but it periodically runs a lock detection monitor that frees the deadlock “prisoner” threads and allows them to return and report to the client that they have been aborted because they’ve been waiting for their lock longer than the value of this option.
Note that the deadlock monitoring thread does not actually examine the sequence of the locks each thread is holding to figure out the existence of a logical database deadlock.
Rather, it assumes that if a certain thread has exceeded the time limit in waiting for a lock that it requested, it is probably logically deadlocked.
Even if it is not, it doesn’t matter—the user would appreciate getting an error message rather than waiting indefinitely while nothing productive is being accomplished.
Even a perfectly implemented transactional system with a perfect logic still depends on the assumption that it will read back from the disk exactly what it wrote to it last time.
Any computer professional who has been around knows that this assumption is sometimes not true for a number of reasons: hardware failure, operating system bugs, user errors, etc.
Additionally, InnoDB itself, although exceptionally robust, might still have a bug.
The bottom line: there are times when the tablespace gets corrupted so badly that the standard recovery algorithm fails.
Again, from the purely theoretical point of view, corruption would force us to say that whatever we have on disk in place of our old tablespace is just a bunch of random data.
Fortunately, in most practical situations there exists a better answer.
Usually, the fatal corruption destroys only a couple of pages, while the rest of the data is intact.
It is therefore possible, perhaps by way of a semi-educated guess at times, to recover most of the lost data.
This option tells InnoDB how hard to try to recover the lost data.
If the value of this option is greater than 0, no queries that update the tables are allowed.
The user is expected to dump the tables salvaging the data, and then re-create a clean tablespace and repopulate it.
To learn more about what happens when you force a recovery, search for the variable srv_force_recovery in the following files:
One of the possible uses is to load the data from disk-based tables into in-memory (i.e., MEMORY) tables for faster access.
This option controls the size of the MyISAM key cache.
Note that there is no option to set the data cache in MyISAM.
Unlike InnoDB, MyISAM hopes that the operating system will do a good job caching, which does happen very often on Linux.
To learn more about how the MyISAM key cache works, take a look at mysys/mf_ keycache.c.
Different directories contain files in different languages, thus the name of the option.
I find this option interesting for a couple of reasons.
Unlike many applications of MySQL’s degree of complexity—which often do not run without a suite of configuration files, shared libraries, and other paraphernalia—the mysqld binary is not nearly as capricious when you copy it to another system and try to run it.
However, there is one external file it will absolutely not run without: errmsg.sys.
When you are trying to run some quick and dirty test on a system with a particular binary without having to install all of the MySQL files, you can copy mysqld and errmsg.sys to some directory, e.g., /tmp/mysql and execute something like this to start the server:
Thus, language has the honor of being one of the options needed to start a lean, inconspicuous “parachuted into the enemy camp” MySQL server.
Another distinction of this option and its associated MySQL functionality is that it serves as a creative language-learning tool.
By starting MySQL with different language options, you can read error messages in different languages, which helps you build some basic technical vocabulary or at least acquire a collection of silly phrases to entertain unsuspecting natives.
As MySQL continues to increase in functionality, the proposed book is becoming less and less marketable due to the increase in the number of error messages, and therefore, the days one would need to learn Swedish.
The build process creates errmsg.sys from errmsg.txt using a utility called comp_err, which is a part of the source tree.
In version 5.0 the error message maintenance was simplified, and all of the errormessage data is now contained in one file, errmsg.txt.
To learn more about how MySQL deals with error messages, take a look at init_ errmessage( ) from sql/derror.cc, and at the ER( ) macro from sql/unireg.h.
It is very helpful for debugging clients, but on the other hand, the log grows very fast on active servers, and therefore the option should be used with care.
The actual logging for this type of log happens in the function:
Note that the logging code was refactored in version 5.1
For versions 5.1 and later, additional log classes were added and moved to sql/log.h.
The method that writes to the general log has also changed its signature to:
It is primarily used for replication on a replication master, but it can also be used for incremental backup.
The logging happens on the logical level; i.e., queries along with some meta information are being logged.
This option was introduced in version 3.23 during the development of replication.
This option has existed since the very early days of MySQL (in pre-3.23 it logged ISAM activity, as the MyISAM storage engine did not yet exist)
It has been helpful in debugging MyISAM problems on numerous occasions, and is a great tool for learning about MyISAM.
There are two criteria: execution time (controlled by the long_query_time option) and key use.
The logging is implemented using the standard MySQL log class in sql/log.cc.
As you can see, this code fragment controls the decision on which queries get logged.
If your system runs a lot of queries that are raising false alarms (or perhaps you have your own definition of slow), you might find it beneficial to play with this area of code.
The server allocates the memory for a temporary buffer to store the packet, and it requests enough to fit it entirely.
This architecture requires a precaution to avoid having the server run out of memory—a cap on the size of the packet, which this option accomplishes.
The code of interest in relation to this option is found in sql/net_serv.cc.
This variable also limits the length of a result of many string functions.
Many operating systems do not fare well when the resources are limited.
This option puts a cap on the number of maximum connections the server is willing to take.
The idea is to have the MySQL server throttle itself down before it hijacks the system in case of some unexpected load spike.
The name comes from the fact that it is allocated from the program’s heap.
However, they require a precaution—they can be quite easily populated to the point of having the system run out of memory.
This option puts a cap on how big each inmemory table can get.
Note that this option will not keep a malicious user from performing a denial-ofservice attack; she can create a large number of in-memory tables of the allowed size, overrunning the memory that way.
It tells the optimizer to abort the queries that it believes would require it to examine more than the given number of record combinations.
This requires memory allocations in proportion to the maximum possible size of a given key.
If sorting were to be done using the full length of a blob or text column, it could require enormous amounts of memory allocation, since those columns could potentially be as big as 4 GB (for a LONGBLOB)
To solve the problem, MySQL puts a limit on the length of the key prefix it will use for sorting.
The trade off is that the sort results are correct only to the prefix values.
This variable imposes a limit on the length of the sort key prefix.
Originally, this cutoff point for the BLOB sort key was 1,024
However, arbitrary magic numbers are bad, so it was made to be a parameter, which is controlled by this option.
On the code level, the turning point is in the following lines inside sortlength( ) in sql/filesort.cc:
However, power can fail, the operating system may crash or have a bug in the I/O code, and MySQL itself may crash or have a bug in the MyISAM storage engine.
While the MyISAM tables lack the robustness of InnoDB for recovery from such crashes, most of the time even the most severe problems can be overcome with a table repair, often losing no more than just one record.
With this option disabled, the repair would have to be done using the REPAIR TABLE command (online), or the myisamchk utility (offline)
Suppose that, in the middle of the night, a small table somehow gets corrupted.
It is very nice not to get awakened by your pager telling you that your web application is down when all you have to do to fix it is a manual repair.
The disadvantage is that this option could potentially trigger a large CPU- and I/O-intensive repair without your knowledge, making things a lot worse for the end user during that time.
The code related to this option, although conceptually very simple, is perhaps a bit difficult to follow.
The storage engine class (a subclass of handler) can optionally define the bool auto_repair( ) method.
One may ask why in the world an application would run the same query over and over on the data that has not changed.
This is one of the many options that control the behavior of the query cache.
To learn about how the query cache works, study sql/sql_cache.cc.
On the code level, two sections are worth studying in connection with this option.
However, it was included because its presence and name illuminated the internal workings of the MySQL replication.
The slave stays connected to the master and continuously reads the contents of the master update log, known in MySQL jargon as the binary log.
The slave then applies the updates it reads from the master to its copy of the data, and thus is able to stay in sync.
In the 3.23 version, there was only one slave thread that applied the updates immediately.
This worked fine when the slave was able to keep up with the master.
However, there were situations when the slave fell behind a lot.
Were the master to experience a fatal unrecoverable crash, the slave would never get the data it had not yet replicated.
To address this problem, the slave algorithm was reworked in version 4.0
The slave now has two threads: one for network I/O, and the other for applying the SQL updates.
The I/O thread reads the updates from the master and appends them to the so-called relay log.
The SQL thread in turn reads the contents of the relay log, and applies them to the slave data.
Learning more about this option really means understanding how the replication is implemented on the slave.
Suppose server A is a slave of server B, which in turn is a slave of server C, which is a slave of server A.
Then A sees it in the binary log of C.
There has to be some way to tell A that the update it sees in the binary log of C originated from A, and therefore should be ignored.
The solution was to assign each server participating in replication a unique 32-bit ID, similar in concept to an IP address.
Each binary log event is tagged with the ID of the server that originated it.
When a slave applies a binary log event received from another server, it logs it with whatever server ID was in the log event record rather than its own.
This breaks potentially infinite update loops in a circular replication topology.
Second, since they are not used, the server will positively authenticate any set of credentials from any host that can establish a connection to the server.
This option is particularly useful when you have lost the MySQL root user password.
You can start the server with skip-grant-tables, connect to it, use SQL statements to manually edit the privilege tables, and then either issue FLUSH PRIVILEGES or just restart the server.
For security reasons, it is recommended that you also use skip-networking in conjunction with skip-grant-tables.
Otherwise, anybody on the network who can get to your MySQL port will have unlimited access to your server.
If you take this security precaution, FLUSH PRIVILEGES is not enough to put the server into its normal mode, and a restart with regular options is required to enable network connections.
This option is also useful when you want to deploy a minimum installation of the server.
By eliminating the need for privilege tables, you are able to run an instance of MySQL server with only two files: mysqld and errmsg.sys.
The handling of this option in the source code is fairly simple, as you would expect.
This enables the client to have unrestricted access to the server functionality.
Having proper debugging information is critical to making sure the same type of crash does not happen again.
MySQL users have had a difficult time collecting such information.
An effort has been made to help them create meaningful bug reports.
On an x86 or Alpha Linux, the MySQL server binary is capable of unwinding its own stack and printing the stack trace when it receives a fatal signal such as SIGSEGV.
In addition, the postmortem diagnostic message includes the query that was executing as well as the settings of the variables that are most likely to cause a crash.
Although reliable in most cases, the reported values should always be taken with at least a small grain of salt.
If the server crashed already, the memory could very well be seriously corrupted, making the reported data absolutely bogus.
This report has been helpful on many occasions in catching a wide range of bugs, including the ones that only happened in some production environments and could not be duplicated otherwise.
By default, the stack tracing takes place when a fatal signal is received; however, sometimes it is not desirable (e.g., if you are trying to debug the crash in a debugger)
Indeed, if it succeeded on the master, and the slave has the same data as the master did when it succeeded, there is no reason for it to fail.
If it does, you would ideally want to stop replicating and have the DBA check things out manually to verify the integrity of the data.
In practice, most applications that use MySQL have a high degree of record isolation.
In other words, although a table may contain millions of records, if one record is incorrect or gone altogether, the problem can be fixed manually or even simply ignored.
In those situations, it is more important for the replication to progress in a timely manner than for the data on the slave to always be a perfect replica of the master.
And if this is a priority, errors such as a duplicate key error can be simply ignored as the replication continues.
This option tells the slave server which error codes it should ignore.
The error codes to ignore can be specified in a comma-delimited list, or one could just use the keyword all to ignore all errors.
When records need to be sorted, MySQL uses an algorithm that is known as filesort in MySQL jargon.
The record set is broken into chunks, and each chunk is sorted with a radix sort.
If there is more than one chunk, each sorted chunk is written out to a temporary file while being merged with the already sorted collection.
This way, we can get the best of both worlds: the speed of a radix sort and the ability to sort large collections of records.
To learn more about the filesort algorithm and its implementation, take a look at sql/ filesort.cc.
By setting it to different values, you can tell MySQL that a REAL is an alias for FLOAT instead of DOUBLE; a space is allowed between the database and table names; || means string concatenation rather logical OR; and other tweaks needed to port an application from some other database to MySQL without changes in its code.
There are a lot of places in the code that are affected by the use of this option.
Some things to do to become familiar with how it works:
It caches table descriptors, which greatly increases the speed of the queries.
Each time a table is referenced in query, the table cache may already have the needed descriptor, and the expensive operation of initializing one does not need to be done.
You can view the contents of the table cache with the SHOW OPEN TABLES command.
When a process repeatedly creates and removes files with unique names, the kernel ends up allocating large amounts of memory that it never releases.
MySQL may on occasion need to create a temporary file to resolve a query.
On a large site with a lot of traffic and a wide diversity of queries, this may take place frequently enough to cause serious problems.
For most users, it did not until MySQL was put to use on one very loaded site with a number of frequently executing, sophisticated queries.
MySQL developers responded with a workaround by adding an option to limit the possibilities for the name of the temporary table to a smaller set of names.
To see how this option works, take a look at the beginning of create_tmp_table( ) in sql/select.cc.
When two or more different transactions occur in parallel, there are several different models or sets of rules for what a read operation should return when some data was written by another transaction but not yet committed.
This set of rules is known by the term of transaction isolation level.
Many transactional engines, including InnoDB, give the user an option to select a desired transaction isolation level for a given transaction.
This option allows you to set a global transaction isolation level for the whole server.
When implementing a server, a programmer is faced with a dilemma as to whether to use threads or processes to handle requests.
In this chapter we discuss the rationale, strengths and weaknesses, and implementation of thread-based request handling in the MySQL server.
Threads Versus Processes Perhaps the most important difference between a process and a thread is that a child thread shares the heap (global program data) with the parent, while a child process does not.
This has a number of implications when you are deciding which model to use.
Advantages of Using Threads Threads have been implemented in programming libraries and operating systems industry-wide for the following reasons:
The memory overhead of creating another thread is limited to the stack plus some bookkeeping memory needed by the thread manager.
If the data could possibly be modified by another concurrently running thread, all that needs to be done is to protect the relevant section with a mutual exclusion lock or mutex (described later in this chapter)
In the absence of such a possibility, the global data is accessed as if there were no threads to worry about.
Creating a thread takes much less time than creating a process because there is no need to copy the heap segment, which could be very large.
The kernel spends less time in the scheduler on context switches between threads than between processes.
This leaves more CPU time for the heavily loaded server to do its job.
Disadvantages of Using Threads Despite the importance of threads in modern computing, they are known to have drawbacks:
If one thread crashes, it brings the whole server down.
One rogue thread can corrupt the global data, causing other threads to malfunction.
A programmer must think constantly about the possibility of some other thread doing things to cause trouble, and how to avoid it.
Threaded servers are notorious for synchronization bugs that are nearly impossible to duplicate in testing but happen at a very wrong time in production.
The high probability of such bugs is a result of having a shared address space, which brings on a much higher degree of thread interaction.
Mutex contention at some point can get out of hand.
If too many threads try to acquire the same mutex at the same time, this may result in excessive context switching, with lots of CPU time spent in the kernel scheduler and very little left to do the job.
Since all threads share the same address space, the whole server is theoretically limited to 4 GB of RAM even when there is a lot more physical RAM available.
When a stack is allocated, even if the thread does not use the majority of the allocated space, the address space of the server has to be reserved for it.
Thus, even though there might be plenty of physical memory, it may not possible to have large buffers, to have a lot of concurrent threads, and to give each thread plenty of room for its stack at the same time.
Advantages of Using Forked Processes The drawbacks of threads correspond to the strengths of using multiple processes instead:
Although a definite possibility, it is not as easy for a rogue forked-server process to disrupt the whole server.
Most of the time, the programmer only needs to think of one thread of execution, undisturbed by possible concurrent intruders.
If a bug happens once, it is usually fairly easy to duplicate it.
With its own address space for each forked process, there is not much interaction between them.
On a 32-bit system, the issue of running out of address space is usually not as acute.
Disadvantages of Using Forked Processes To wrap up our overview, I’ll list the problems with multiple processes, which mirror the advantages of threads:
Possibly large memory segments are copied unnecessarily when a child is forked.
This makes it cumbersome to access the data global to the server.
Creating a process requires more overhead in the kernel than creating a thread.
One big performance hit is the need to copy the data segment of the parent process.
Linux, however, cheats in this area by implementing what is called copy-on-write.
The actual copy of a parent process page does not take place until the child or the parent modifies that page.
Context switches between processes are more time-consuming than between threads because the kernel needs to switch the pages, file descriptor tables, and other extra context info.
Less time is left for the server to do the actual work.
In summary, a threaded server is ideal when a lot of data needs to be shared between the connection handlers, and when the programming skills are not lacking.
When it came down to deciding which model was the right one for MySQL, the choice was clear.
A database server needs to have lots of shared buffers, and other shared data.
As far as the programming skills were concerned, they were not lacking at all.
Just as a good rider becomes one with the horse, Monty had become one with the computer.
He felt confident enough to be able to write virtually bug-free code, deal with the concurrency issues presented by threads, and even work with a small stack.
What an exciting challenge! Needless to say, he chose threads.
Implementation of Request Handling The server listens in the main thread for connections.
For each connection, it allocates a thread to handle it.
Depending on the server configuration settings and current status, the thread may be either created anew or dispatched from the thread cache.
Upon terminating the client session, depending on the server configuration settings and status, the thread may either terminate or enter the thread cache to wait for another request dispatch.
Structures, Variables, Classes, and API Perhaps the most important class for threads is THD, which is a class for thread descriptors.
Nearly every one of the server functions inside the parser and optimizer accepts a THD object as an argument, and it usually comes first in the parameter list.
Whenever a thread is created, its descriptor is put into a global thread list I_List<THD> threads.
To locate the target thread when executing the KILL command.
It is actually used in a rather unexpected manner: as a means of passing a THD object instantiated by the main thread to the thread waiting in the thread cache that is being dispatched to handle the current request.
All operations related to creating, terminating, or keeping track of threads are protected by a mutex LOCK_thread_count.
Three POSIX threads condition variables are used in conjunction with threads.
COND_thread_count helps with synchronization during shutdown to make sure all threads have finished their work and exited before the main thread terminates.
COND_thread_cache is broadcast when the main thread decides to wake up a cached thread and dispatch it to handle the current client session.
In addition, a number of global status variables are used in relation to threads.
The server never forces a thread to exit preemptively because doing so without giving it a chance to clean up could cause serious data corruption.
Rather, each thread is coded to pay attention to the environment and exit when asked.
Can be viewed in the output of SHOW STATUS under Threads_connected.
They exit if they see that this flag is set.
After a fairly sophisticated combination of tests of what could possibly go wrong in accept( ) on a wide variety of platforms, we finally get to the following code segment:
Once this limit is reached, one additional administrative connection is allowed to give the DBA a chance to fix the crisis caused by reaching this limit.
The purpose of this limit is to allow the server to put brakes on itself before it takes the system down by utilizing too many resources.
If set to 0 (the default), the thread caching is disabled.
Can be viewed in the output of SHOW STATUS under Threads_cached.
Can be viewed in the output of SHOW STATUS under Threads_created.
Can be viewed in the output of SHOW STATUS under Connections.
Can be viewed in the output of SHOW STATUS under Threads_running.
After some additional THD object manipulation, the execution descends into create_new_thread( ) in the same file, sql/mysqld.cc.
A few more checks and initializations, and we reach the conditional that determines how the request handling thread is obtained.
There are two possibilities: use a cached thread or create a new one.
With the thread caching enabled, an old thread simply goes to sleep instead of exiting when it is done serving client requests.
When a new client connects, instead of just creating a new thread, the server first checks to see whether it has any sleeping threads in the cache.
If it does, it wakes one of them up, passing the THD instance as an argument.
Although caching threads could give a boost in performance on a heavily loaded system, the original motivation for the feature was to debug a timing problem on Linux on an Alpha system.
Alternatively, if the thread caching is disabled or there are no cached threads available, a new thread has to be created to handle the request.
This also covers the case when the thread cache has been disabled.
If the test for cached thread availability comes out negative, the code turns to the else part, where the job of spawning a new thread gets done in the following line:
Commands are accepted and processed as long as no loop exit condition is encountered.
The thread is killed with the KILL command by the database administrator, or by the server itself during the shutdown.
Currently, the only other possibility is if the replication master decides to abort the feed of updates requested by a slave (or a client pretending to be a slave) through COM_BINLOG_DUMP.
The key element of this code segment is the call to end_thread( ) from sql/mysqld.cc.
If end_thread( ) decides to cache this thread, the following loop is executed:
If the thread does not get the chance of going into the thread cache, its fate is to terminate through pthread_exit( )
Thread Programming Issues MySQL faces many of the same complications as other programs that depend on threads.
Standard C Library Calls When writing code that can be concurrently executed by several threads, functions from external libraries must be called with extra care.
There is always a chance that the called code uses a global variable, writes to a shared file descriptor, or uses some other shared resource without ensuring mutual exclusion.
If this is the case, we must protect the call by a mutex.
While exercising caution, MySQL must also avoid unnecessary protection, or it will experience a decrease in performance.
For example, it is reasonable to expect malloc() to be thread-safe.
Other potentially non-thread-safe functions such as gethostbyname() often have thread-safe counterparts.
If the appropriate threadsafe counterpart is not detected, the protective mutex is enabled as the last resort.
Overall, MySQL saves itself a lot of thread-safety worries by implementing many standard C library equivalents in the portability wrapper in mysys and in the string library under strings.
Even when C library calls are made eventually, they happen through a wrapper in most cases.
If a call on some system unexpectedly turns out to lack thread safety, the problem can be easily fixed by adding a protective mutex to the wrapper.
Mutually Exclusive Locks (Mutexes) In a threaded server, several threads may access shared data.
If they do so, each thread must make sure the access is mutually exclusive.
This is accomplished through mutually exclusive locks, otherwise known as mutexes.
As the application’s degree of complexity increases, you face a dilemma as to how many mutexes to use, and which ones should protect what data.
On one end of the spectrum, you could have a separate mutex for each variable.
This has the advantage of reducing the mutex contention to the minimum, but it has a few problems.
What happens if you need to access a group of variables atomically? You have to acquire a mutex for each individual variable.
If you do so, you must make sure to always acquire them in the same order to avoid deadlocks.
On the other end of the spectrum is having a single mutex for everything.
This makes it very simple for the programmer—get the lock when accessing a global variable, and release it when done.
Unfortunately, this approach has a very negative impact on performance.
Many threads would be unnecessarily made to wait while one was accessing some variable that the others did not need to have protected.
The solution is in some balanced grouping of the global variables and in having a mutex for each group.
This is what is done in MySQL to solve this problem.
Table 6-2 contains a list of global mutexes in MySQL, with descriptions of the respective groups of variables they protect.
At this point, the protection is redundant because the active_mi value never gets changed concurrently.
However, the protection will become necessary when multi-master support is added.
LOCK_crypt Protects the calls to the Unix C library call crypt( ), which is not thread-safe.
Delayed inserts return to the client immediately even if the table is locked, in which case they are processed in the background by a delayed insert thread.
LOCK_mapped_file Protects the data structures and variables used in operations with memorymapped files.
Currently there exists internal support for this functionality, but it does not appear to be used anywhere in the code.
LOCK_open Protects the data structures and variables relevant to the table cache, and opening and closing tables.
LOCK_status Protects the variables displayed in the output of SHOW STATUS.
LOCK_thread_count Protects the variables and data structures involved in the creation or destruction of threads.
THR_LOCK_charset Protects the variables and data structures relevant to character set operations.
THR_LOCK_heap Protects the variables and data structures relevant to the in-memory (MEMORY) storage engine.
THR_LOCK_isam Protects the variables and data structures relevant to the ISAM storage engine.
THR_LOCK_lock Protects the variables and data structures relevant to the table lock manager.
THR_LOCK_malloc Protects the variables and data structures relevant to the malloc( ) family call wrappers.
THR_LOCK_myisam Protects the variables and data structures relevant to the MyISAM storage engine.
In addition to global mutexes, there are a number of class/structure encapsulated mutexes used to protect portions of that particular structure or class.
There are also a couple of file scope global (static) mutexes in the mysys library.
Imagine a situation when a certain variable is modified by only one thread and only infrequently, but it is read by many others often.
If we were to use a mutex, most of the time one reader would end up waiting for the other to finish reading even though it could have just executed concurrently.
There is another type of lock that is more suitable for this situation: a read-write lock.
Read locks can be shared, while write locks are exclusive.
Thus, multiple readers can proceed concurrently as long as there is no writer.
Clearly, a read-write lock is able to do everything a mutex can, and more.
Why not use the read-write locks all the time? As the saying goes, there is no free lunch, and it applies very well in this case.
The extra functionality comes at the cost of greater implementation complexity.
As a result, read-write locks require more CPU cycles even when the lock is obtained immediately.
Thus, in choosing the type of lock to use, one must consider the probability of firsttry failure to acquire it, as well as how it will be reduced by changing from a mutex to a read-write lock.
Even if changing to a read-write lock reduces the probability of failure to a virtual zero, it is still not worth it.
On the other hand, if using the read-write lock in this particular case does not significantly reduce the probability of first-try failure, the CPU overhead might still not be worth it.
Most MySQL critical regions are fairly short, which leads to a low probability of the first-try failure.
Thus, in most cases a mutex is preferred to a read-write lock.
However, there are a few cases where a read-write lock is used.
THR_LOCK_open Protects the variables and data structures that keep track of open files.
Synchronization A threaded application is often faced with the problem of thread synchronization.
One thread needs to know that the other has reached a certain state.
A thread waiting for a condition can call pthread_cond_wait( ), passing it the condition variable and the mutex used in the given context.
The call must also be protected by the same mutex.
The signal or the broadcast must also be protected by the same mutex that the waiting thread uses with pthread_cond_wait( )
A signaled condition wakes up only one thread that is waiting for it, while a broadcast one wakes up all waiting threads.
LOCK_grant Protects variables and data structures dealing with the access control.
The sys_init_slave system variable descriptor stores the commands to be executed on the master every time a slave connects to the master as specified by the init-slave configuration setting.
Currently there are only two possible tasks: clean up Berkeley DB logs and flush the tables.
COND_refresh Signaled when the data in the table cache has been updated.
COND_thread_cache Signaled to wake up a thread waiting in the thread cache.
In addition to these condition variables, a number of structures and classes use local conditions for synchronization of operations on that class or structure.
There also exist a couple of file scope global (static) condition variables inside the mysys library.
Preemption The term preemption means interrupting a thread to give the CPU some other task.
The preempting thread sets the appropriate flags, telling the thread being preempted that it needs to clean up and terminate or yield.
At that point, it becomes the responsibility of the thread being preempted to notice the message and comply.
Most of the time this approach works very well, but there is one exception.
If the thread being preempted is stuck performing a blocking I/O, it will not have a chance to check the preempting message flags.
To address the problem, MySQL uses a technique known in MySQL developer terminology as the thread alarm.
A thread that is about to enter blocking I/O makes a request to receive an alarm signal after a timeout period with a call to thr_alarm( )
If the I/O completes before the timeout, the alarm is canceled with end_thr_alarm( )
The alarm signal on most systems interrupts the blocking I/O, thus allowing the thread that is potentially being preempted to check the flags and the error code from the I/O and to take the appropriate action.
The action is usually to clean up and exit the I/O loop if preempted, or else retry the I/O.
MySQL provides a layer of abstraction that permits different storage engines to access their tables using the same API.
In the past, this interface was called the table handler.
In the current terminology, storage engine refers to the code that actually stores and retrieves the data, while table handler refers to the interface between the storage engine and the MySQL optimizer.
The abstract interface greatly facilitates the task of adding another storage engine to MySQL.
It can be used for integrating custom storage engines, which permits you to quickly develop an SQL interface to just about anything that knows how to read and write records.
The interface is implemented through an abstract class named handler, which provides methods for basic operations such as opening and closing a table, sequentially scanning through the records, retrieving records based on the value of a key, storing a record, and deleting a record.
Each storage engine implements a subclass of handler, implementing the interface methods to translate the handler operations into the low-level storage/retrieval API calls of that particular storage engine.
Starting in version 5.0, the handlerton structure was added to allow storage engines to provide their own hooks for performing operations that do not necessarily involve one-table instances such as initialization, transaction commit, savepoint, and rollback.
In this chapter we will examine the handler class and the handlerton structure, and then provide—as a modestly-sized example you can study—a simple storage engine for reading comma-delimited files.
The handler Class The handler class is defined in sql/handler.h and implemented in sql/handler.cc.
Sql_alloc is a class with no members that merely overrides the new and delete operators so that new allocates memory from the memory pool associated with the connection, while delete does nothing.
An instance of handler is created for each table descriptor.
Note that it is possible to have several table descriptors for the same table, and therefore just as many instances of handler in the same server.
In the past, the multiple handler instances for the same table resulted only from having several copies of the table descriptor in the table cache, and thus one handler instance per descriptor.
Now, with the addition of index_merge, additional handler instances may be created during optimization.
The data members of handler are documented in Table 7-1
The record reference is an internal, unique record identifier for the given table.
For this field, MyISAM uses the offset of the record in the datafile.
InnoDB uses the value of the primary key formatted in a special way.
The length of the value is stored in the ref_length member.
The ones that do not use a datafile “wing it” by storing in this variable the combined length of all of the records plus the holes where newly inserted records could be put.
This value is used in the output of SHOW TABLE STATUS.
The ones that do not use an index file put here the approximate amount of memory or disk space used for storing the indexes for this table.
In MyISAM, the amount of space occupied by records that have been marked as deleted.
This value can be set with an AUTO_INCREMENT clause during table creation or with ALTER TABLE.
InnoDB provides just an estimate, due to the complications caused by multi-versioning.
Contains the result compare_key( ) should return if the actual value of the key turns out to be equal to the one it is being compared against.
Depending on the mode of traversing the key range, the optimizer may find it more convenient to think that an equal value is the same as a lesser or a greater value, and make such a request in read_range_first( )
Set to true if the start and the end of the range have the same value.
Frequently the error is the attempt to create a duplicate key value of a unique key.
This argument is the path to the .frm file containing the definition of the table, with the .frm extension stripped off.
The remaining arguments are passed to open() and are interpreted by the specific storage engine.
Returns 0 on success, or a nonzero error code on failure.
This method has a generic implementation that deals with the most common errors.
If an unknown error code is encountered, the message is looked up via get_error_message( )
The buf argument is the address of the String buffer that stores the resulting message.
Returns true if the error in the storage engine was temporary.
If the argument contains an error code not connected with a duplicate key error, returns (uint)-1
Returns an estimated number of block read operations it would take to read rows number of rows from ranges number of ranges using the key number index.
Normally, the MySQL optimizer scans the table without using keys, as the full scan of a plain datafile is faster than traversing a B-tree index.
However, some storage engines may organize their data in such a way that it is beneficial to traverse a key in the case of a full table scan.
This method returns a key map with bits set for the keys that can be used for scanning the table.
The size of the buffer is the logical length of the record, plus possibly some extra reserved length for the purposes specific to the storage engine.
Returns a pointer to a textual description of the index specified by the argument.
Returns 0 on success, and a nonzero value on failure.
Returns 0 on success, and a nonzero value on failure.
The argument specifies whether a full table scan is going to be performed.
Returns 0 on success, and a nonzero value on failure.
Returns 0 on success, and a nonzero value on failure.
Returns 0 on success, and a nonzero value on failure.
Does the real work to open the table ( as opposed to ha_ open( ), which is just a wrapper)
The name argument is the path to the .frm file, with the extension stripped off.
The remaining arguments contain flags that specify what to initialize and what to do if the table files are locked.
The flags are mostly meaningful to the MyISAM storage engine.
Returns 0 on success, or a nonzero error code on failure.
Note that this method is pure virtual and must be implemented in a subclass.
Returns 0 on success, or a nonzero error code on failure.
Note that this method is pure virtual, and must be implemented in a subclass.
This call is the bottom of the execution stack shared by all storage engines when handling an INSERT query.
Thus, failure to implement it results in all INSERT queries returning an error.
This call is the bottom of the execution stack shared by all storage engines when handling an UPDATE query.
Thus, failure to implement it results in all UPDATE queries returning an error.
This call is the bottom of the execution stack shared by all storage engines when handling a DELETE query.
Positions the key cursor according to the values of key and key_len on the first key, and reads the record into buf if a match exists.
The matching is performed according to the lookup method specified byfind_flag.
Returns 0 on success, or a nonzero error code on failure.
Same as index_read( ) except that the key specified by the index argument is made active first.
Returns 0 on success, and a nonzero error code on failure.
Returns 0 on success, and a nonzero error code on failure.
Returns 0 on success, and a nonzero error code on failure.
Returns 0 on success, and a nonzero error code on failure.
Starting from the currently active record, reads the next record that has the same key value as the previously read record into the buffer pointed to by buf.
Because some storage engines do not store the value of the last read key, the key and keylen arguments are used to remind them.
On success, the active key cursor is advanced, and 0 is returned.
The method has a default implementation that returns HA_ERR_WRONG_ COMMAND.
Reads into buf the record found through the last key value matching the values ofkey andkey_len, and positions the cursor immediately before that record.
The method has a default implementation that returns HA_ERR_WRONG_ COMMAND.
The range boundaries are saved to be used by read_range_next( )
The eq_range argument indicates whether the start and the end of the range have the same value.
The sorted argument tells whether the caller expects to receive the records in the key order.
The method returns 0 on success, and a nonzero error code on failure.
Returns 0 on success, and a non-zero error code on failure.
Can be called when MySQL needs to repeat full-text search many times; e.g., in a join.
Returns 0 on success, and a nonzero error code otherwise.
The flags argument specifies the search mode, the inx argument is the number of the index, and the key and keylen arguments supply the key to search.
Returns a pointer to a full-text search descriptor on success, and NULL on error.
Returns 0 on success, and a nonzero error code on error.
Returns 0 on success, and a nonzero error code on error.
Note that the method is pure virtual and must be implemented in the subclass.
The interpretation of pos is up to the storage engine.
Returns 0 on success, and a nonzero error code on failure.
Note that the method is pure virtual and must be implemented in the subclass.
Retrieves one arbitrarily chosen record from the table and places it into the buffer pointed to by the buf argument.
The primary_key argument affects the method by which the record is chosen.
Currently, the default implementation uses two methods for choosing this record.
The first one scans the table and returns the first record not marked as deleted, whereas the other method picks the first record in the key with the number of the primary_key argument.
Returns 0 on success, and a nonzero error code on failure.
Currently meaningful only in MyISAM, where this method is an alias for rnd_pos( )
At this time, the method is called only once, by the code that removes duplicates from the result set when processing SELECT DISTINCT on a temporary table.
It is possible that this method will be renamed or eliminated in the future.
Returns 0 on success and a nonzero error code on error.
Currently this method is never called, and no storage engine implements it.
The worst thing that can happen if a bogus value is returned is that the optimizer will prefer a less optimal key or choose not to use a key at all.
Stores the unique reference value to the current record in the refmember.
For MyISAM tables, this value is the position of the record in the datafile; thus the name of the method.
Some storage engines may not remember the unique reference value of the last record, and may need to look at the actual record, which is supplied by the argument.
Note that the method is pure virtual and must be implemented in the subclass.
Note that the method is pure virtual and must be implemented in the subclass.
Gives hints to the storage engines to use some special optimizations.
For example, if the argument is HA_EXTRA_ KEYREAD, read operations on a key may retrieve only those parts of the record that are included in the key.
Returns 0 on success and a nonzero error code otherwise.
The default implementation just returns 0, as hints are safe to ignore.
Similar to extra( ) except that it allows the caller to pass an argument to the requested operation (cache_size)
Mainly used for controlling cache sizes for various types of I/O.
Frees the resources allocated by earlier extra( ) calls, and resets the operational modes of the storage engine to the defaults.
MySQL calls this method once at the beginning of every statement for every table used in the statement.
MyISAM just locks the key file via the operating system if the external locking option is enabled, thus the historical name of the option.
Transactional storage engines use it as a hook for starting a transaction and performing other initializations if necessary.
Returns 0 on success, and a nonzero error code on error.
Note that the method is pure virtual and must be implemented in a subclass.
Used by InnoDB to clear the locks on the rows read in the semiconsistent read mode (read last committed version if the current version is locked by another transaction)
Returns 0 on success, and a nonzero error code on error.
If not supported, the table is cleared via multiple calls to delete_row( )
Returns 0 on success, and a nonzero error code on error.
Interestingly enough, although this method has a fairly complex default implementation, most existing storage engines reimplement it.
The check_opt argument points to a structure describing the options for the operation.
Returns 0 on success and a nonzero error code on failure.
Returns 0 on success and a nonzero error code on failure.
Returns 0 on success and a nonzero error code on failure.
Restructures the table to be in the most optimal form for a typical query.
Returns 0 on success and a nonzero error code on failure.
Returns 0 on success and a nonzero error code on failure.
Assigns the keys of this table to the key cache specified inside the check_opt structure.
Returns 0 on success and a nonzero error code on failure.
Loads the keys of this table into the cache specified inside the check_opt structure.
Returns 0 on success and a nonzero error code on failure.
If fd is less than 0, the data is written to the network connection associated with thd.
The dump format must be understood by net_read_ dump( )
Returns 0 on success, and a nonzero error code on error.
Often used before a large sequence of updates while holding a lock on the table.
Instructs the storage engine to enable the bulk insert optimization.
MySQL calls it before inserting a large number of rows into the table.
MyISAM optimizes bulk inserts by caching key values in memory and inserting them into the B-tree index in key order.
Returns 0 on success, and a nonzero error code otherwise.
A method used by InnoDB to perform operations on a table space allocated for this table.
Discarding prepares the table space for an import from the backup.
Importing restores the data from the backup after the table space file to be restored has been copied into its designated location.
Returns 0 on success, and a nonzero error code on error.
Returns 0 on success, and a nonzero error code on error.
Used in SHOW TABLES to display some extra information about the table in theComment column.
Returns a pointer to the string containing the updated comment value.
InnoDB is the only engine that provides its own implementation.
The default implementation just returns the value of the argument.
Appends extra information specific to the storage engine to the String object specified by the argument.
Note that this method is pure virtual and must be implemented in the subclass.
Note that this method is pure virtual and must be implemented in the subclass.
Note that this method is pure virtual and must be implemented in the subclass.
Returns a bit mask of capabilities of the key or its part specified by the arguments.
Note that this method is pure virtual and must be implemented in the subclass.
Returns a bit mask of capabilities for the given key with respect to creating or dropping that key.
The default implementation returns DDL_SUPPORT, which means that a storage engine supports the index of a given definition, but cannot add it to the existing table (MySQL will create a new table with this index and copy the data over)
The second argument is the start of the key definition array, while the third is its size.
Returns 0 on success, and a nonzero error code on error.
Drops the keys from the table specified by the arguments.
Returns 0 on success, and a nonzero error code on error.
The limit is either what the storage engine itself supports, or the limit imposed by the core code, whichever is less.
The limit is either what the storage engine itself supports, or the limit imposed by the core code, whichever is less.
The limit is either what the storage engine itself supports, or the limit imposed by the core code, whichever is less.
The limit is either what the storage engine itself supports, or the limit imposed by the core code, whichever is less.
The limit is either what the storage engine itself supports, or the limit imposed by the core code, whichever is less.
Returns the limit on the length of a record imposed by this storage engine.
Returns the limit on the number of key parts imposed by this storage engine.
Returns the limit on the key length imposed by this storage engine.
Returns the limit on the key part length imposed by this storage engine.
Returns the lower limit on the length of a record imposed by this storage engine.
This can happen if CHECK TABLE or just a regular read/write operation discovers a problem.
The table then effectively gets taken offline by being marked as crashed.
Moves the table specified by from to the path specified by to.
The arguments are paths to the table definition files with the .frm extension removed.
The default implementation iterates through all the possible extensions returned by bas_ext( ) and renames the matching files.
Returns 0 on success, and a nonzero error code on error.
The argument is the path to the table definition file with the .frm extension removed.
The default implementation iterates through all the possible extensions returned by bas_ext( ) and deletes the matching files.
Returns 0 on success, and a nonzero error code on error.
Creates a table specified by name using the table descriptor form and the creation information descriptor info.
Returns 0 on success, and a nonzero error code on error.
Note that this method is pure virtual and must be implemented in the subclass.
In most situations only one lock descriptor block is needed, with MERGE tables being the exception.
Stores the location of the lock descriptor associated with this table at the address indicated by to.
The other arguments supply the values of the current thread descriptor and the type of the lock in case the storage engine wants to know these values for its internal purposes.
The main purpose of this method is to allow the storage engine to modify the lock before it gets stored.
Row-level locking storage engines use it to prevent the table lock manager from putting an excessive lock on the table.
Returns the value of to on success, and 0 on failure.
Note that this method is pure virtual, and must be implemented in the subclass.
If the optimizer needed to do something with the storage engine, it would call a virtual method of handler for the current table.
However, as the process of integrating various storage engines moved forward, it became apparent that interfacing through the handler methods alone was inadequate.
A handlerton is a C structure consisting mostly of callback function pointers.
The callbacks are invoked to handle certain events involving the given storage engine.
For example, when a transaction is committed, a save point takes place, or a connection is closed, some special action may be required, in which case the handlerton will have the pointer to the appropriate callback.
The default implementation returns HA_CACHE_TBL_ NONTRANSACT, which permits caching regardless of whether there is a transaction in progress.
The storage engine may then use its transaction visibility rules to decide.
Used by storage engines capable of filtering out records that do not match a portion of the WHERE clause.
Originally created for the NDB storage engine, which may store the records on remote nodes and can benefit from processing portions of the WHERE clause internally.
The requested part of the WHERE clause represented by the argument is pushed onto the expression stack of this storage engine instance.
Returns a new expression tree that the caller would have to evaluate to decide whether the record indeed matched the WHERE clause.
If the filtering is fully completed inside the storage engine, returns NULL.
The default implementation immediately returns the argument without doing anything else.
Used for communicating to the storage engine that it is acceptable to read the last committed version of a record if the current version is locked by another transaction.
This is used by InnoDB to avoid unnecessary locks during UPDATE and DELETE queries.
Should be set initially to the size of the savepoint structure.
Adding a Custom Storage Engine to MySQL There are a number of reasons for adding a custom storage engine to MySQL:
You have a legacy, proprietary database and want to give it an SQL/ODBC interface.
You have some very specific requirements in the areas of performance or data security that are not being met by any of the existing storage engines.
You have created a low-level data storage and retrieval module that you believe will rule the world, but you do not want to (or are not able to) write an SQL optimizer to go with it.
Your proprietary SQL optimizer does not meet your needs, and you want a better one for your storage engine.
Our storage engine will provide a read-only SQL interface to comma-separated value (CSV) text files.
In version 4.1 and earlier, storage engine integration requires a lot of source modifications.
For the sake of brevity, I will not provide instructions.
The function to be called every time a query is written to the replication log.
Those who need to integrate their storage engine into other versions are advised to search (case-insensitive) for the string “blackhole” in the source tree of the given version, and follow the patterns of the blackhole storage engine.
We assume that you already have downloaded and unpacked the MySQL source distribution.
They provide the definition and the implementation of our storage engine class.
This is necessary to include those files in the compilation framework.
Now you need to make a few changes to the core code to make it aware of the presence of a new storage engine.
Add the following line to other include directives at the top of sql/handler.cc:
Still in sql/handler.cc, extend the array sys_table_types[] with the following member (any position except the very last element in the array is fine):
In sql/handler.cc, extend the switch statement in get_new_handler( ) with the following code:
When the build is finished, you will have a binary in sql/mysqld that has support for your new storage engine.
We will use this to iterate through the array of table field pointers to store the parsed data in the right place and the right format.
The impossible value of 256 indicates that the last character either did not exist (we are on the first one), or its value is irrelevant.
Set to 1 if we are inside a quoted string.
How many bytes we have seen so far in this line.
Special case - a character that was escaped itself should not be regarded as an escape character.
We still have a number of small tasks left to complete the job.
On success, update our estimate for the total number of records in the table.
The rest of the variables merely appear in SHOW TABLE STATUS output and do not affect the optimizer.
Nothing special to do on the storage engine level when the table.
While it appears that the procedure of custom storage-engine integration should have stabilized at that point, it is still possible that some changes might be introduced into later 5.1 versions that would require modifications to these instructions.
When in doubt, search for the string “blackhole” in the source and follow the pattern of the blackhole storage engine.
Copy Makefile.am from the book’s web site (or from Example 8-5) to storage/ oreilly-csv.
Add the following lines right after the blackhole plug-in section (or after some other plug-in section):
Used in the plug-in as well as the handlerton descriptor.
We will use this to iterate through the array of table field pointers to store the parsed data in the right place and the right format.
The impossible value of 256 indicates that the last character either did not exist (we are on the first one), or its value is irrelevant.
Set to 1 if we are inside a quoted string.
How many bytes we have seen so far in this line.
If at the end a field, and a matching field exists in the table (it may not if the CSV file has extra fields), transfer the field value buffer contents into the corresponding Field object.
This actually takes care of initializing the correct parts of the buffer argument passed to us by the caller.
Special case - a character that was escaped itself should not be regarded as an escape character.
We still have a number of small tasks left to complete the job.
The parsed line may not have had the values of all of the fields.
On success, update our estimate for the total number of records in the table.
Stores the "position" reference to the current record in the ref variable.
At this point, this method is called in situations that are impossible for this storage engine, but this could change in the future.
The rest of the variables merely appear in SHOW TABLE STATUS output and do not affect the optimizer.
Returns an array of all possible file extensions used by the storage engine.
There are a number of ways to deploy your new binary.
If you plan to extend it, review the information in Chapter 3 on how to write a test case and execute it in a debugger.
If you just want to run the binary and see what happens, assuming you already have a regular MySQL installation from the same version as your source, you can just back up your regular mysqld binary, replace it with the newly built one, and restart the server.
To see the new storage engine in action, create a comma-delimited file with the base name matching the name of the table and the extension .csv (e.g., t1.csv), and place it in the directory corresponding to the database you plan to work in.
For example, if your datadir is set to /var/lib/mysql and you want the table to be created in the.
After creating the file, create a table of type OREILLY_CSV with the fields corresponding to those in the file in the appropriate database.
You are now ready to run SELECT queries on the table.
The comments in the long examples contain detailed explanations of the finer points of the code.
I will add a few comments here on the issues you would need to deal with were you to extend the engine.
To simplify the example, we have made our storage engine read-only; there is no support for updates or delete operations.
To add the ability to write, we would need to address the issue of having multiple instances of the handler object for the same table.
In our example, this does not present a big problem other than wasting file descriptors.
Write access will require us to keep track of the current write position, which could catch us by surprise if another instance of the handler object was used to perform a write.
Other storage engines solve this problem by maintaining a cache of shared, low-level table descriptor structures.
This also has a nice side effect of using fewer file descriptors.
If you want to add key support, it is highly recommended that you implement the write capability first.
Afterward, you can have a lot of fun creating your own B-trees, hashes, and other forms of indexing.
Our engine reads blocks from a file descriptor using calls to my_pread( ), and then parses them.
This is not as efficient and convenient as using mmap( ), but it is more robust in case of an I/O error.
In version 5.1, MyISAM has the option to use mmap( ) for regular tables as well.
A proper locking mechanism is necessary to ensure data consistency when there is a possibility of multiple clients accessing and possibly modifying the same data at the same time.
There are three main approaches to solving this problem: table-level locks, page-level locks, and row-level locks.
Table-level locks have the simplest logic, which results in fewer bugs and better performance in the area of lock acquisition.
On the other hand, locking the entire table results in poor performance for applications that do a large number of concurrent reads and writes.
Row-level locks allow high performance at a very high level of concurrency at the cost of greater complexity in the implementation.
This results in slower performance for applications that have a low probability of lock contention as well as higher probability of bugs.
It is also very difficult to completely avoid deadlocks, and many implementations do deadlock detection instead.
As the granularity of a lock decreases, the amount of memory required to lock the same amount of data generally increases.
So does the complexity of the algorithm, and the potential for a deadlock.
However, the decrease in the granularity of the lock increases the potential for concurrent access, which can delay the unfortunate application that has to wait for the lock.
The row-level lock has the smallest granularity; the table-level lock has the greatest; and the page-level lock comes in between, offering a compromise.
MyISAM and MEMORY storage engines can only work with table-level locks.
InnoDB supports row-level locks, and Berkeley DB supports page-level locks.
When a parser processes a query, it determines somewhat simplistically what type of table locks need to be acquired based on the query type.
Once the execution reaches the lock manager, it gives each associated storage engine an opportunity to update the type of lock for each table.
The storage engines that support finer granularity locks request a lock that permits concurrent reads and writes, and then handle the locking issues internally.
The architecture of locking in MySQL is largely a result of its development history.
In the early days of 3.23, a feature was introduced that changed this assumption.
It became possible in MyISAM to concurrently read from a table and insert a new row into it as long as that row was being placed at the end of the datafile.
The introduction of concurrent insert required some changes to the table lock manager but did not alter its basic architecture.
Prior to that, the locking algorithm was entirely determined by the query.
The newly introduced complication was solved by adding a callback function pointer to the lock structure to check for the availability of concurrent insert.
If the pointer is set to 0, or if the callback reports that concurrent insert is not available, the lock gets upgraded to a regular write lock.
Things changed with the arrival of BDB, which supports page-level locks.
It introduced the challenge of making sure that the table lock manager would not try to lock the entire table when only one or several rows needed to be locked.
Somewhere down the call hierarchy the storage engine now needed to examine the nature of the query and communicate to the table lock manager which tables it did not want locked.
Table Lock Manager As explained earlier, all queries involving tables of all storage engines go through the table lock manager regardless of the granularity levels of the locks supported by the storage engine.
For example, even if row-level locks are supported, a special table lock is acquired that permits concurrent write access.
TL_IGNORE A special value used in locking requests to communicate that nothing should be done in the lock descriptor structures.
TL_UNLOCK A special value used in locking requests to communicate that a lock should be released.
Table locks are divided into two groups: read locks and write locks.
The table lock manager maintains four queues for each table:
Threads that currently hold a read lock are found in the current read-lock queue in the order of lock acquisition.
Threads currently waiting for a read lock are found in the pending read-lock queue.
The same paradigm applies to the current and pending write-lock queues.
Read Lock Request A read lock is always granted as long as there are no current write locks on the table and no higher-priority write locks in the pending write-lock queue.
If the lock request can be granted immediately, the corresponding lock descriptor is placed in the current read-lock queue.
TL_READ_NO_INSERT A special read lock that does not allow concurrent inserts.
Other threads are allowed to acquire read and write locks while this lock is being held.
Altering a table involves creating a temporary table with the new structure, populating it with new rows, and then renaming it to the original name.
Thus a table can be read while being altered during most of the operation.
TL_WRITE_ONLY An internal value used when aborting old locks during operations that require closing tables.
The requested read and pending write locks are prioritized according to the following rules:
All write locks in the pending write-lock queue that are not a TL_WRITE have a lower priority than a read lock.
The presence of a current write lock causes the requesting thread to suspend itself and wait for the lock to become available except in the cases below:
Write Lock Request When a write lock is requested, the table lock manager first checks whether there are any write locks already in the current write-lock queue.
If there are none, the pending write-lock queue is checked.
If the pending write-lock queue is not empty, the request is placed in the write-lock queue and the thread suspends itself to wait for the lock.
Otherwise, with the empty pending write-lock queue, the current read-lock queue is checked.
The presence of a current read lock causes the write lock request to wait except in the following cases:
If the exceptional requirements are met, the lock request is granted and placed in the current write-lock queue.
If there are locks in the current write queue, the exceptional case of TL_WRITE_ONLY request is handled first.
TL_WRITE_ONLY is granted only if there are no current write locks.
Otherwise, the request is aborted and an error code is returned to the caller.
With the exceptional case out of the way, the table lock manager can now examine the possibility of coexistence for the requested and the current write lock at the head of the current write-lock queue.
The request can be granted without a wait under one of the following circumstances:
The conflicting write lock is being held by the requesting thread.
The locking mechanism provided by the table lock manager is insufficient for a number of storage engines.
MyISAM, InnoDB, NDB, and Berkeley DB storage engines provide some form of an internal locking mechanism.
MyISAM mostly depends on the table lock manager to ensure proper concurrent access.
If the insert operation results in writing the record at the end of the datafile, reading can be done without a lock.
In this case, the table lock manager permits one concurrent insert lock and many read locks.
The storage engine ensures consistency by remembering the old end of file prior to the start of the concurrent insert, and by not permitting the reads to read past the old end of file until the concurrent insert is complete.
Internally, it implements a complex row-level locking system that includes deadlock detection.
It deals with the table locks in a manner similar to InnoDB.
InnoDB Locking Although InnoDB is not the only storage engine that supports some internal locking mechanism, it is perhaps the most interesting.
Being the most stable and mature of all the transactional storage engines in MySQL, it is usually the engine of choice for mission-critical, high-load environments.
There are two types of row-level locks: shared and exclusive.
To support the coexistence of row- and table-level locks, InnoDB also uses so-called intention locks on a table.
There are also two types of intention table locks, shared and exclusive.
As the name intention locks suggests, it is possible for another transaction to acquire another shared lock if one is holding a shared lock already.
However, only one transaction can hold an exclusive lock at any one time.
It is necessary for a transaction to acquire the appropriate intention lock on the table before locking a row in it.
Shared row locks are possible after acquiring an exclusive intention lock.
However, only an exclusive intention lock allows a transaction to acquire an exclusive row lock.
Record or row locking occurs as InnoDB is searching for records requested by the optimizer.
What InnoDB actually locks is the index entry, the space before it, and the space after the last record.
The next-key locking is necessary to avoid the phantom row problem in transactions.
If we did not lock the space before the record, it would be possible for another transaction to insert another record in between.
Thus, if we were to run the same query again, we would see the record that was not there the first time we ran the query.
This would make it impossible to meet the requirement of the serializable read transaction isolation level.
It will usually roll back the last transaction involved in a deadlock.
The deadlock detection algorithm fails in some cases; for example, if tables from other storage engines are used, or if some tables were locked with LOCK TABLES.
Additionally, some transactions may be considered to be in a virtual deadlock.
For example, if a query is written is such a way that it examines several billion records, it may not release its locks for weeks, although from a theoretical point of view it eventually will.
For such situations InnoDB uses a lock timeout, which is controlled by the configuration variable innodb_lock_wait_ timeout.
It is important for the application programmer to write code that deals with this possibility.
It is also possible to minimize the chance of a deadlock by careful programming.
Accessing records always in the same index order, writing properly optimized queries, and committing transactions frequently are some of the techniques that help prevent potential deadlocks.
Once a query is received, it first needs to be parsed, which involves translating it from what is essentially a textual format into a combination of internal binary structures that can be easily manipulated by the optimizer.
In this context, when we say optimizer, we refer to the server module responsible for creating and executing the plan to retrieve the records requested by the query.
The optimizer picks the order in which the tables are joined, the method to read the records (e.g., read from an index or scan the table), as well as which keys to use.
Its goal is to deliver the query result in the least amount of time possible.
In this chapter, we’ll examine the parser and optimizer in detail.
Parser MySQL’s parser, like many others, consists of two parts: the lexical scanner and the grammar rule module.
The lexical scanner breaks the entire query into tokens (elements that are indivisible, such as column names), while the grammar rule module finds a combination of SQL grammar rules that produce this sequence, and executes the code associated with those rules.
In the end, a parse tree is produced, which can now be used by the optimizer.
Unlike some parsers, which translate the textual representation of the query into byte code, MySQL’s parser converts it directly into internal interlinked C/C++ structures in the program memory.
The lexical scanner examines the stream of query characters, breaks it into tokens, and identifies each token.
Each token is given a type—for example, a keyword, a string literal, a number, an operator, or a function name.
The grammar rules module matches the stream of tokens against a set of rules, and finds the correct rule, which in this case is the select rule (see sql/sql_yacc.yy)
The parser has two main objectives, not necessarily listed in the order of importance.
Many installations of MySQL have to support the load of thousands of queries per second.
This would not be possible if parsing alone took even one millisecond.
Second, the generated parse tree must provide the information to the optimizer in a way that permits it to access the data efficiently.
The optimizer needs quick access to various parts of the WHERE clause, table, field, and key lists, ORDER BY and GROUP BY expressions, subquery structuring, and other data.
As difficult as it is to reach those two objectives, the MySQL development team has largely succeeded at the task so far.
Lexical Scanner Many open source projects use the very popular utility GNU Flex to generate lexical scanners.
The programmer only provides a set of guidelines for classifying characters, and Flex produces the C code to do the scanning that can be integrated with the rest of the code.
Unlike them, MySQL has its own lexical scanner to gain both performance and flexibility.
A handwritten token identifier can be fine-tuned with optimizations that are not possible with generated code.
Additionally, it can also be coded to identify the tokens with context sensitivity.
The generated hash is perfect, meaning there are no collisions.
The scanner (see sql/sql_lex.cc) tags each token as a keyword, a function name, a number of a particular type, or some other special symbol that has a meaning in the grammar rules.
The list of keywords is found in the array symbols[] in sql/lex.h.
The list of functions is contained in the array sql_functions[] in the same file.
Note that there was a change in later releases of 5.1
Now the built-in functions are looked up by the grammar rules module instead of the lexical scanner.
The entry point to the Lexical Scanner is yylex( ) in sql/sql_lex.cc.
The name of the function has special significance: it needs to be compatible with GNU Bison, the grammar rules module generator, which expects to retrieve the tokens by calling a function with this name.
Grammar Rules Module This module is often called the parser, but I refer to it as the grammar rules module to separate it from the lexical scanner part of the server.
Just like in many other opensource projects, the grammar rules module is generated using the parser generator utility GNU Bison.
It is recommended that you become familiar with Bison if you plan on modifying MySQL syntax, or just want to understand the parsing process better.
You can learn more about it from the Bison manual (published by the Free Software Foundation), also available online at http://www.gnu.org/software/bison/ manual.
The entry point to the grammar rules module is yyparse( )
Parse Tree The end result of the parser execution is the parse tree.
As you can imagine, the complexity of the SQL syntax requires an equally complex structure that efficiently stores the information needed for executing every possible SQL statement.
While it would not be possible within the scope of this chapter, or perhaps even one book, to comprehensively describe all of the elements of the parse tree, I will attempt to provide a brief overview of the essentials.
The sql_command shows what type of SQL query we are executing, whether it is a select, an update, an insert, a delete, or some other query type.
The class has many members containing the information about various query particulars such as the WHERE clause; the table list; the field list; information about optimizer hints; cross-references to other instances of SELECT_LEX for subqueries; the ORDER BY, GROUP BY, and HAVING expressions; and many other details.
We will focus on the Item* where member, which is the root node of the WHERE clause tree, because most of the information needed by the optimizer is extracted from the WHERE clause.
The Item class defined in sql/item.h is the base class for all other Item_ classes, which represent the nodes of an expression tree.
This family of classes covers arithmetic operations (e.g., addition, subtraction, multiplication, division), various SQL functions, logical operators such as AND and OR, references to table fields, subqueries returning one row, and every other element of an SQL expression found in WHERE, HAVING, GROUP BY, ORDER BY, or the field list of a select query.
The rest of the name depends on the type of the return value.
For example, if the return value is an integer, the method name is val_int( )
The optimizer later uses the Item contained in the where member of LEX_SELECT to build a filter expression for record combinations it examines.
The filter expressions are evaluated via a call to Item::val_int( )
If it returns 1, the record is considered to have met the constraint and is included in the result set; otherwise, it is discarded.
The filter expression is identical to the original WHERE clause if the optimizer is not able to make any improvements to it.
Otherwise, it may be rewritten to eliminate unnecessary computations, and permit better use of keys.
An example of an expression tree for the WHERE clause is shown in Figure 9-1
The expression in the example may have come from the following query:
Optimizer To help you understand the role of the optimizer, consider the following query:
We want to retrieve the first name, the last name, the phone number, and the product name and price for all the orders where payment has failed for one reason or another.
For each of those records we retrieve a matching record out of customer using the key on its id column, and also a matching record out of product using the key on its id column.
We now have to examine as many record combinations as there are records in orders that have the payment_ status value set to 'FAILED'
Thus it becomes apparent that the optimizer must not only figure out a way to deliver the records requested by a query but also do it in a way that is optimal—or at least be able to deliver satisfactory performance.
This is a much bigger challenge than just delivering the results, which therefore justifies the name of optimizer for this module.
Determine which keys can be used to retrieve the records from tables, and choose the best one for each table.
For each table, decide whether a table scan is better than reading on a key.
If there are a lot of records that match the key value, the advantages of the key are reduced and the table scan becomes faster.
Determine the order in which tables should be joined when more than one table is present in the query.
Rewrite the WHERE clause to eliminate dead code, reducing the unnecessary computations and changing the constraints whenever possible to open the way for using keys.
Attempt to replace an outer join with an inner join.
Attempt to simplify subqueries, as well as determine to what extent their results can be cached.
Basics of the Optimizer Algorithm In MySQL optimizer terminology, every query is a set of joins.
The term join is used here more broadly than in SQL commands.
A query on only one table is a degenerate join.
While we normally do not think of reading records from one table as a join, the same structures and algorithms used with conventional joins work perfectly to resolve the query with only one table.
Simple queries without subqueries or UNION consist of only one join.
Queries with subqueries that cannot be optimized, as well as UNION queries, will involve more than one join.
Some subqueries may require what can be called a recursive join: while one join is being performed, the optimizer needs to execute a subquery for each row of the join, which results in its own join.
Nevertheless, a join is the basic unit of the optimizer’s work.
In the source code, a join is connected to the join descriptor class JOIN defined in sql/sql_select.h.
The procedure described in this section thus falls into two parts: first the optimizer determines the best join order, then it does a nested loop to accomplish the join.
A join is essentially a Cartesian product of table subsets.
Each subset is obtained by reading records from the table based on a single key value, a key range (or a set of key ranges), a full index scan, or a full table scan.
The records are then eliminated, if necessary, using the constraints from the WHERE clause.
The optimizer selects the record access methods and puts the tables in an order it believes would minimize the cost, which is more often than not in proportion to the total number of record combinations it would have to examine.
The problem of query optimization can be broken down into two parts: first, for a given join order, find the best access paths for each table, and second, once you have that ability, find the best join order, or at least a reasonably good one, in a short amount of time.
The access path defines whether the optimizer is going to read on a key, scan the table (ALL), or scan the key (index)
If a key read is performed, it defines how that key is going to be used—for example, reading one record based on one value (eq_ref), possibly more than one record based on one value (ref), or a range of values (range)
Therefore, the best access path has already been computed for the old partial plan, and the optimizer only needs to compute it for the newly added table.
The selection and order of the tables in the old partial plan greatly affect the best access path for the new table.
For example, in one case, the old tables may contain a column whose value can be used to perform a key read, while in another case that possibility may not exist, necessitating a full scan for the new table.
The exhaustive search examines all of the possible combinations of tables and finds the best plan.
Take the first table out of the resulting set, and place it first in the partial join order.
For each tested combination, append it to the existing partial plan and evaluate the cost.
Pick the combination with the lowest cost, and place the first table in that combination next in the partial plan.
Both the exhaustive and the greedy search have the optimization to discontinue the pursuit of the path if the current partial combination has a cost that exceeds the best cost found so far.
Thus while the greedy search may not always find the best plan, it has a controlled complexity, and will have the performance advantage over the exhaustive search.
Indeed, it does not matter if the optimizer finds the best plan if the gain in the execution time is offset by the loss in the discovery time.
Prior to version 5.0, only the exhaustive search was available.
After the join order has been determined, the optimizer begins to execute the join.
The join is performed via a sequence of nested loops, starting from the first table.
For each record of the first table, the optimizer loops through the second to create combinations.
For each record in the second table, in turn, the optimizer loops through each record of the third, and so on and so forth, creating a record combination for each iteration of the innermost loop.
The combination is then compared against the WHERE clause of the query—or more precisely, the optimized filter expression generated from the original WHERE clause.
You may wonder why anybody would ever write such a constraint in its unoptimized form.
In many applications queries are frequently generated via complex business logic algorithms, which often produce unoptimized queries a human would never write.
Additionally, query rewriting may produce such a query when a column reference gets replaced with a constant.
Thus, trivial optimizations like the one discussed here often lead to significant speed gains.
The matched records are passed to the send_data( ) method of the result processing object associated with the join.
The resultant processing object may send the records to the client, write them to a file or a temporary table, or pass them on somewhere else for further processing.
Using EXPLAIN to Understand the Optimizer The MySQL EXPLAIN command tells the optimizer to show its query plan.
A query plan describes what the optimizer is going to do to solve the query.
For example, start with the table orders; read records on key payment_status; for each record of.
Much can be learned by studying the output of EXPLAIN on a query.
To understand the query plan, execute the following in the MySQL command-line client:
The purpose of the \G switch at the end of the query is to request that the result set be displayed vertically.
The output of EXPLAIN contains a lot of columns, which often makes the default mode of horizontal output unreadable.
The output on line 4 tells us that the optimizer will first examine the customer table.
The state key will be queried by supplying one key value, but the result may contain more than one record (line 5)
The key value used is a constant supplied directly in the WHERE clause or obtained some other way as opposed to the value of some other column, which may vary (line 9)
The records retrieved from this table will be checked to see whether they match the WHERE clause (line 11)
Line 15 reveals that the second table in the join is orders.
However, the value of the key this time is not a constant anymore.
It is taken from the field id of the currently processed record of customer (line 20)
It will vary as the optimizer retrieves different records of customer.
Note that this optimization strategy is possible only if customer is placed before orders in the join order.
The optimizer estimates that on average for every record combination of the tables preceding the join order (in this case it is just one table, customer), it will have to examine five records in the orders table (line 21)
Why did the optimizer choose to do what it did in this example? To help you understand, we’ll force it to choose a different query plan:
The STRAIGHT_JOIN directive tells the optimizer that the orders table must come before the customer table in all of the possible join orders it may consider.
In this case the STRAIGHT_JOIN instruction leaves only one possible combination: first orders, then customer.
Only one match should indeed be expected since a primary key by definition has to be unique.
What else could the optimizer have done differently? Due to the nature of the WHERE clause, there are two possible keys to use in the customer table: the primary key and the state key.
Let us try to make the optimizer use the original join order but use the primary key instead:
As you have seen in the previous examples, EXPLAIN produces a set of rows.
Each row describes a table participating in a join and shows how the records are going to be retrieved from that table.
The order of rows corresponds to the join order in the algorithm.
It also shows the order of the queries, which is meaningful only if there is more than one query involved (for example, in a query with subqueries)
The output of EXPLAIN is in essence a human-readable dump of the JOIN class (see sql/ sql_select.h), which serves as the query plan descriptor.
Table 9-1 defines the relationships between the EXPLAIN fields and corresponding elements of the source code.
A join not involving subqueries or UNION will have this value set to simple.
If no alias is used, the real name of this table.
This section describes the types of selects that can be indicated by the select_type field in the output of the EXPLAIN command.
In the following example, the select from the orders table is labeled PRIMARY.
In the following example, SELECT id FROM customer WHERE state = 'AZ' is labeled UNION, while SELECT id FROM customer WHERE state = 'NV' is PRIMARY.
When index_merge optimization is used, contains a list of keys.
If the key is used to look up one or more records based on one value of the key or its prefix, the zero-based index number of the key is contained in join_tab[k-1]
The name of the key is stored in the namemember of the KEY structure.
This does not have to be the full length of the key—it is possible to use only a key prefix.
See the explanation for the key field on how to locate the key definition structure.
The length of the key being used is the key_ length member of the KEY structure.
Collected from a number of the join descriptor data members.
Relationship between EXPLAIN and elements of the source code (continued)
A subquery is considered dependent on the outer select if the optimizer thinks it could possibly use the information that will change for each row of the outer select.
This, unfortunately, means that the optimizer will rerun the subquery for each row of the outer select.
The optimizer sees that it only needs to run it once.
The optimizer thinks it needs to run it once for every row of the outer query.
Note that even though it may not be necessary to do so, the optimizer may merely fail to notice the independence of the subquery.
A table is called derived if it is generated from the result set of another query.
In the SQL standard such tables are called “subquery in the FROM clause.” In the following example, the wy table is derived:
This section describes the types of selects that can be indicated by the select_type field in the output of the EXPLAIN command.
This happens when the table has a unique key and the WHERE clause supplies a value for it.
In the following example, we assume that id is a unique key in customer:
However, it is possible to retrieve more than one record.
This happens either when the key is not unique, or when only the prefix of the key is available.
Happens when no key constraint can be used, and the optimizer needs to read columns that are not covered by an index.
This record access method is possible only for range-capable keys.
B-tree keys are range capable, while hash keys are not.
In the following example, we assume customer has a range-capable key on last_name:
This is not an efficient use of the index and means the user did not employ the index well.
Nevertheless, it is the best the optimizer can do with the query the user provided.
There were no constraints on the index values, which would have reduced the number of values to read.
While using the whole index, the scan will access only the parts of the record covered by the index.
This kind of index scan can be more efficient than the full table scan if the index covers only a small part of the entire record.
In the following example, we assume that customer has a key spanning last_name:
This is possible only for full-text capable keys, which are currently implemented only in the MyISAM storage engine.
In the following example, we assume that customer has a full-text key on description:
In the following example,we assume that last_name is a key that can contain NULL values:
In the following example, we assume that id is a unique key in customer:
In the following example, we assume that customer_id is a non-unique key in orders:
In the following example, we assume that the product table has a key on price and another key on name:
This section describes the strings that can appear in the Extra field in the output of the EXPLAIN command.
Using where The WHERE clause was evaluated to eliminate some records.
This is necessary unless the optimizer can detect that all of the records it is going to read on a key will automatically satisfy the WHERE clause.
In the following example, we assume that price is not a key in product and we see Using where:
Note that if we were to add a key on price, Using where disappears.
The optimizer has requested a read on the price key of all the records with the price equal to 1300.00
Using index The optimizer noticed that all of the columns it needed are contained in a key.
Therefore, it decided to scan just the key instead of the entire data.
In the following example, we assume that name is a key in product:
Note that if we replace MAX(price) with COUNT(*), the same index is used, but the Extra column now says Using index.
The query is optimized in a different way because COUNT(*) cannot be done without looking at all of the values in the index.
It needs to know how many there are, and the storage engine interface currently does not provide a way for the optimizer to ask or for the storage engine to communicate even if the value is being stored by the storage engine.
Using filesort The optimizer was asked to retrieve the records in sorted order (ORDER BY), but its record access method does not guarantee it.
The term filesort refers to the MySQL sorting algorithm, which performs a radix or quick sort on small chunks in the memory.
If the entire record set to be sorted does not fit into the sort buffer, the temporary results get stored in a file.
In the following example, we assume that product does not have a key on price:
Note that if we add a key on price, the Using filesort message disappears.
The optimizer is able to use the key, and it will routinely retrieve the records in the key order, thus eliminating the need for post-sorting.
Using temporary The optimizer needs to create a temporary table to store an intermediate result.
For example, if a GROUP BY is done on a nonkey column, the optimizer creates a temporary table with a unique key consisting of the GROUP BY expression.
For each record of the regular result set (omitting GROUP BY), an attempt is made to insert it into the temporary table.
If the insert fails due to the unique constraint violation, the existing record is updated appropriately.
Once the temporary table has been populated, the result set is sorted and returned to the client.
In the following example, we assume that product does not have a key on name:
If we add a key on name, the need for using the temporary table disappears.
Distinct The optimizer is able to eliminate records in a join, which is made possible by the use of the DISTINCT keyword in the query.
In the example, we assume that orders has a key on product_id, that id is a unique key in product, and that the optimizer puts orders first in the join order:
Indeed, while there might be a number of records in orders that match the orders part of the WHERE clause, it is sufficient to check the product part of the WHERE clause only for each distinct value of product_id.
Due to the nature of the query, any two records with the same product_id in orders will have identical product parts.
Therefore, since the query asked only for distinct values of the product_id, once the optimizer finds a unique value of product_id in orders that matches the WHERE clause, it does not have to examine the rest of the records with the similar value of the key, and it can move on to the next unique value in the index instead.
Not exists A special optimization is used during a left join to eliminate record combinations.
If a join is done on a column that is defined with the NOT NULL attribute in the second table, and the WHERE clause requires that the column be NULL, the only way this is possible is if the matching value of the first table column does not exist in the second table.
Thus, even if only one ON clause match is discovered for a record in product, the optimizer can safely move on to the next record in product without examining all the other combinations of that record with the records in orders.
However, as the join progresses, certain record combinations in the preceding tables (in the join order) may permit either a range or index merge optimization on some keys.
Thus, the optimizer for each record combination in the preceding tables checks to determine which index is best to use.
In some cases, the optimizer may choose to use the index on s, while other times it may choose to use the one on s1
If the range is not very restrictive, the optimizer may even choose to scan w2 instead.
The value of N in index map: N is a hexadecimal (in version 5.0) expression of the bitmap of the keys that are being considered in this optimization.
It is more of an attempt to rescue a sinking ship: a query that, without it, would be a complete performance disaster.
If the optimizer chooses it, it should be considered an invitation to write a better query.
Using union( ) This comment appears in the case of the index_merge access method.
Two or more keys are being used to retrieve the records, and the correct result can be obtained via a sorted list merge of the results.
In other words, the constraints for each key are such that there is no need to sort the records from each index by row ID: each key naturally produces a sorted list.
Natural sorted order by row ID is guaranteed when all of the parts of a key are known, or when the key is a clustered primary key (in InnoDB and BDB tables)
In the following example, we assume that customer has a key on state and a key on (lname,fname):
Two or more keys are being used to retrieve the records, but the optimizer is not sure that each key will naturally produce a sorted list.
Thus, to eliminate the duplicate rows, additional processing is required.
In the example, customer table has a key on state and on (lname,fname)
Since there is no key on lname, the (lname,fname) key has to be used.
The optimizer does not have a constraint that covers all of its parts, and the records therefore are not necessarily ordered by row ID.
Using intersect( ) This comment appears in the case of the index_merge access method.
Two or more keys are being used to retrieve the records, and the correct result can be obtained via a sorted list intersection of the results.
This optimization is very similar to Using union( ) except that the result sets are intersected (AND operation) instead of combined (OR operation)
In the following example, we assume that customer has a key on state and a key on (lname,fname):
Using where with pushed condition Prior to the introduction of NDB tables, the optimizer operated under the assumption that reading a record from a table either on a key or via a scan would, in the worst case scenario, have to access a local disk.
Even if this was not the case, there was not much else it could do; none of the existing storage engines had the ability to prefilter the records.
With the introduction of NDB, the ability to prefilter became a necessity.
Thus, the performance could be optimized a great deal if the storage engine was smart enough to communicate  a filtering constraint to a remote node.
If the storage engine supports it (currently only NDB does), the optimizer can push a filtering constraint onto the condition stack of the storage engine instance.
In turn, the storage engine can use this additional information to optimize record retrieval.
In the example, table t is of type NDB and does not have a key on column n:
Range Optimizer MySQL developers have put a lot of effort into optimizing queries with constraints restricting the values of a key to a particular range.
There is a module that is dedicated to this particular purpose, which is called the range optimizer.
Regular range optimization occurs when the range of the key values is known for only one key in the ascending key order.
Regular range optimization can handle various key value combinations used in combination with Boolean operators.
It can also handle a variety of range constraint operators.
The range optimizer will search the following set of intervals for the key (c1,c2):
Note the capability to convert a constant into a degenerate interval.
This type of optimization is done by the class QUICK_RANGE_SELECT.
There is a special case of the range optimization when spatial keys are used.
This is used when range constraints are available for more than one key, but the result does not come in a sorted order, thus requiring additional processing.
See the explanation of Using sort_union( ) in the earlier section, “Extra field,” for more details.
Similar to Range, except the records are read in the descending key order.
Although there are no ranges in the full-text search, the code organization made the range optimizer the most natural fit for the full-text optimization code.
Used when range constraints are available for more than one key, the result set comes naturally in a sorted order in each key, and the final result is obtained via the intersection of the results on each key.
Used when range constraints are available for more than one key, the result set naturally comes in a sorted order in each key, and the final result is obtained via the union of the results on each key.
Handles some special cases of MIN( )/MAX( ) functions with a GROUP BY when several keys have range constraints.
Subquery Optimization Currently, MySQL performs relatively few optimizations of subqueries.
If it notices that a subquery would return only one record for each evaluation, it evaluates the query and replaces the whole subquery with a constant.
It also attempts some rather minor rewriting of subqueries in special cases.
Other important optimizations are still on the to-do list and are so far scheduled for version 5.2:
Ability to cache the results of a subquery returning multiple records, and use them instead of executing the subquery for each record combination.
Ability to create and use appropriate keys in the temporary tables that store the result of the FROM clause subqueries.
Ability to create and use appropriate keys in the temporary tables storing the result of subqueries from the WHERE clause.
The MySQL subquery optimizer at this point is very much a work in progress.
Core Optimizer Classes and Structures The key structures and classes used by the optimizer are defined in sql/sql_select.h.
If you are interested in getting to know the optimizer internals, you should also become familiar with the structures and classes from sql/opt_range.h.
We will discuss the ones of the most critical importance.
As we have mentioned already, every SELECT query in MySQL is considered a join.
If only one table is referenced, it is treated as a special case of one-table join.
Thus, the key class that describes the query plan for a SELECT query is called JOIN.
Table 9-2 describes its most significant data members and methods.
TABLE** table An array of table descriptors for this join.
A table is considered constant if it contains at most one record, or if at most one record match is possible during a key lookup (the key is either primary or unique)
JOIN_TAB contains the information relevant to the optimization about each table instance participating in a join.
Keeps track of the currently known best join order during the computation of best join order.
A select lex unit is either a single SELECT or a union of SELECT statements.
Depending on its type, the result could be sent to a client, to a file, written to another table, or be stored in memory for further processing.
The arguments are the thread descriptor, a SELECT column list, a bit mask of SELECT options, and a result set object that handles the processing of the output rows.
Most of the arguments correspond to members of the JOIN that require initialization.
May create the first temporary table if the query requires the use of temporary tables.
Measured in comparison with the cost of reading a record via table scan.
Table B is dependent on table A if table A must precede table A in the join order of any possible query plan.
TABLE_REF          ref An auxiliary descriptor mostly containing the information about the key being used for this table.
There was no hierarchy to speak of since there could only be one SELECT.
The former serves as a descriptor of a UNION, while the latter describes a single SELECT.
Called by the optimizer when the list of the fields in the result set becomes available.
Called by the optimizer once for each row of data in the result set.
Called by the optimizer when an error occurs during the generation of the result set.
The results of the SELECT are written to a local file.
In order to execute this update a corresponding SELECT is performed with the result being handled in a special way to make it an UPDATE.
The essence of executing a single SELECT consists of the following steps:
If a JOIN has already been executed with JOIN::exec( ), and it needs to be executed again, a call to JOIN::reinit( ) is necessary.
The purpose of JOIN::optimize( ) is to restructure the query in a more optimal way, and determine the execution plan.
A call to optimize_cond( ) to eliminate redundancies from the WHERE clause and rewrite it to be more efficient.
A call to remove_const( ) to remove constants from ORDER BY and GROUP BY expressions.
A call to create_tmp_table( ) to create a temporary table if needed.
Initializes a number of members of the JOIN class, allocating memory as needed.
Detects constant tables (where no more than one record match is possible), and sets up the constant table bitmap.
Initializes statistical information on the number of records in each table, and key cardinalities.
Calls choose_plan( ), unless the join has been reduced to trivial (all constant tables)
Depending on the type of select_result, the results may be sent to a client, a temporary table, a file, or an internal processor (e.g., in the case of optimized subqueries)
A call to select_describe( ) if the query is an EXPLAIN.
Special handling of a result set with no records via a call to return_zero_rows( )
For DISTINCT, GROUP BY, and ORDER BY queries that were not possible to optimize in another way, a call to create_tmp_table( ) to create a post-processing temporary table.
A call to do_select( ) to perform the nested loop logic of a join.
JOIN::cleanup( ) releases all or some of the resources allocated in the initialization or during the execution, depending on the value of the argument.
In this chapter we will discuss the most prominent storage engines within MySQL in more detail.
Unfortunately, due to the large number of different storage engines and the complexity that some possess, we are not able to examine each one in sufficient detail on the code level.
Indeed, storage engines like MyISAM and InnoDB each deserve their own thousand-page book.
However, I will provide pointers to the source for those who would like to learn more.
Shared Aspects of Architecture While there is a great degree of freedom in the implementation of a storage engine, all storage engines must integrate with the main MySQL server code.
As a result they have a few things in common.
Aside from having to support the basic concepts of tables residing in a database, records, columns, keys, read and write operations, and other aspects stipulated by the storage engine interface requirements, each storage engine also inherits the features and properties from the core table manipulation code.
In other words, they get some functionality and architecture regardless of whether they need it.
Regardless of the storage engine, all tables have one .frm file per table containing the table definition with the column names, their types and sizes, key information, and other table properties.
A .frm file in essence gathers and stores the information from CREATE TABLE.
Up until version 5.1 the filename was always the same as the name of the table, and it resided in a directory corresponding to the database name.
Code reads and parses the files using openfrm( ) from sql/table.cc, and writes to them using create_frm( ) from the same source file.
Regardless of the storage engine, the server reads the table definition from the .frm file, and stores it in what is called a table cache.
This way, the next time the table needs to be accessed, the server does not have to reread and reparse the .frm file, but rather can use the cached information.
Thus, each storage engine can either take advantage of this feature, or politely ask the table lock manager to always grant a write lock, which bypasses the core code table locking.
In that case, the storage engine itself becomes responsible for ensuring consistency during concurrent access.
MyISAM The MyISAM storage engine has roots very far back in the history of MySQL.
When MySQL was first released, the original storage engine was ISAM.
However, at that time there was no abstraction of storage engines in the code that would be easily visible to a user or a developer trying to extend MySQL.
When that abstraction was introduced, ISAM was refactored and enhanced to become MyISAM.
MyISAM Architecture MyISAM stores its data on a local disk.
In addition to the .frm file common to all storage engines, it uses two additional files: a datafile (.MYD), and an index file (.MYI)
It is essentially a concatenation of table records with some necessary meta information.
There are two record formats: fixed length and variable length.
The first bit in the record header indicates whether this record is valid or has been deleted.
For a valid record, the subsequent bits indicate whether their corresponding columns that could be NULL are actually NULL.
After that, the remaining bits of the header act merely as padding bits and have no meaning.
Having fields of type BIT (in MySQL 5.0.3 and higher) complicates the situation, because in some cases bit values might be stored in the header as well.
Thus, to compute the length of the header, the n in the formula from the previous paragraph should be increased by the number of bits being stored in the header.
If the first bit of the header indicates the record has been deleted, the subsequent bits serve as a pointer to the next deleted record in the deleted record chain.
The deleted record chain allows inserts to overwrite the old deleted records instead of appending the new record to the end of file.
Immediately after the header, we find a concatenation of column values for the record in the column order of the table.
Integers and floating point numbers in the record are stored in the little-endian (low byte first) order.
You can find the details of the fixed-length record storage in storage/myisam/mi_ statrec.c.
For records with variable length, the format is more complicated.
The first byte contains a special code describing the subtype of the record.
The meaning of the subsequent bytes varies with each subtype, but the common theme is that there is a sequence of bytes that contains the length of the record, the number of unused bytes in the block, NULL value indicator flags, and possibly a pointer to the continuation of the record if the record did not fit into the previously created space and had to be split up.
This can happen when one record gets deleted, and the new one to be inserted in its place exceeds the original one in size.
MyISAM index files are much more complex than the datafiles.
In short, they consist of a detailed header describing the various key and column properties, and containing a large amount of meta information, followed by the actual key pages.
The header consists of the following section types: state, base, keydef, and recinfo.
The state and base sections occur only once, the keydef section is present once for each key, and the recinfo section is present once for each field of each key.
Note that each record in the table starts with a special field that is used to mark deleted records and NULL fields, and this additional field will also have its own recinfo section.
It contains such information as key and datafile length, timestamps, number of times the table was opened, number of keys, number of deleted and actual records, pointers to the root key block for each key, as well as many other parameters.
In many ways it is conceptually similar to the state section.
It stores the number of records in the table, total number of fields (including the extra ones for dealing with NULL values and deleted records), various limit values (such as maximum key length and maximum key block) length, and a number of other items.
The base section is shared among all threads accessing the table, while each thread has its own copy of the state section.
Following the base section, you may find one or more keydef sections—one per key.
Each keydef section begins with a relatively short header containing the number of key parts, the type of key algorithm (B-tree or R-tree), special option flags, the block length used for this key, and key length limits.
Following that there are one or more keyseg sections, one per column in the key.
Each keyseg section contains the information about the corresponding key part (or column)
Each recinfo consists of a field type code, the field length, a flag indicating if the field value can be NULL, and the offset of the NULL marker.
The internal structure storing the data from the base section is MI_COLUMNDEF defined in include/myisam.h.
MyISAM supports two types of storage structures, B-tree and R-tree.
Thus, each block is a leaf of a B-tree or an Rtree containing key values along with pointers to other blocks or offsets into the datafile for the leaf nodes.
The first bit is used to indicate whether this is a leaf node (it is a leaf if the bit is cleared)
The remaining bits contain the size of the used portion of the block.
MyISAM Key Types MyISAM supports three types of keys: regular B-tree, full-text (which uses a B-tree), and spatial (which uses an R-tree)
The B-tree is a very common storage structure, and the subject has been treated in great detail in many other publications; therefore, we will only briefly visit the MyISAM B-tree.
A MyISAM B-tree consists of leaf and nonleaf nodes, or pages.
You can distinguish a nonleaf node from a leaf node by looking at the highest bit of the first byte of the page.
Both leaf and nonleaf nodes contain key values and pointers to the record positions in the datafile.
Key values in a node may be compressed by replacing a common prefix with a referencing pointer.
A full-text key is essentially a B-tree that stores a pointer to the record and the relevancy weight for each word in each indexed column or set of columns.
A full-text key can be created with syntax similar to this:
Once you have created a full-text index, you may use the myisam_ftdump utility to view the details of the index with a command similar to:
The first non-option argument is the full table path (datadir, database name, and table name)
One way to get the key number is to execute SHOW CREATE TABLE.
The ordinal number of the index minus 1 is the key number to be used with the utility.
The first column shows the position (in bytes) of the start of the record containing the search keyword in the datafile.
A full-text lookup essentially consists of performing a B-tree search in the full-text index, finding the appropriate record positions, computing the relevancy rankings of each record for the search with the help of the individual key word relevancy values stored in the tree, and then ordering the records by the computed record relevancy ratings.
On the SQL level, the full-text functionality is available via MATCH() ...
In the following example, we assume that the table documents has a full-text key on (title,body)
We can use a query similar to this to retrieve the results:
Word frequencies are computed, and eventually the relative weight of each keyword in the record is obtained.
Then the keywords with the weights and the record position are inserted into the full-text index.
The idea of a spatial key comes from the following type of problem.
Suppose you have the latitude and the longitude for every point of interest, and you would like to determine which ones lie within a given bounding rectangle.
In a practical application, the points of interest could be restaurants, and the bounding rectangle could be the zip code boundary.
With a traditional B-tree approach you could store the latitude and the longitude in a table, and have a key on one or the other, or even a compound key containing both.
While this is better than a full scan, it is impossible to avoid the problem of not using the ranges for both coordinates efficiently.
You retrieve all the values for a range on one, but the range of the other does not get used very well.
In 1984, Antonin Gutman proposed an extension to a traditional B-tree to address this challenge.
The extended B-tree was given the name of R-tree, with R standing for region.
While a traditional B-tree node contains key values and pointers to child nodes and/or actual records or pointers to actual records, the R-tree replaces the key values with bounding boxes that contain all of the descendant nodes under the given element of the node.
Due to the nature of an R-tree, a key value is a geometric object in an n-dimensional space.
In MySQL, in order to have an R-tree index, the column must be of the type GEOMETRY, or there must be a way for MySQL to convert it to the GEOMETRY type.
Thus, to create an index on a column in table stores named gps_coord of the type GEOMETRY, use the following syntax:
Note that the bounding polygon does not have to be rectangular, nor do the GEOMETRY column values have to be points.
In fact, the same column can contain points, lines, polygons, and other geometric objects.
The measure of extension can be defined in two ways: by area (in the n-dimensional sense) or by perimeter.
Once the right bounding box is found in the node, the child node is examined the same way until a leaf node is reached.
If the node is full, it needs to be split.
First, all pairs of keys (bounding boxes) are examined to find the pair that will waste the most area (in the n-dimensional sense) if joined.
Each key that has not yet been selected is hypothetically added to both groups.
The algorithm then calculates the increase in the area of the minimum bounding box for each group that results from the addition, and compares the increases.
The difference of increases serves as a measure of the preference of this key for one group or the other.
The one with the greatest measure of preference is chosen to join the group for which it produces the smaller increase in the minimum bounding box area.
This split algorithm is called quadratic-cost split because its complexity is 0(N2) with respect to the number of keys in the node.
There exist faster split algorithms (linearcost), and slower (exhaustive split)
The quadratic cost algorithm is a nice trade-off between keeping a balanced tree and maintaining a good insertion speed.
The R-tree search is very similar to the B-tree search.
Although the match is not found, we scan the current node until we find a bounding rectangle of interest, which in the simplest case will contain the search rectangle.
Descend into the matching node unless we are at a leaf.
If at a leaf, follow the record pointer to retrieve the record.
To delete a record, the search key is found and removed from the leaf node.
Then, if that makes the node less than one-third full, the whole node is removed and placed into the reinsert list along with its descendants.
The matching key is also deleted from the parent node, and the deletion is propagated in this manner upward.
When the deletion is complete, the reinsert list is processed to restore the removed key values into the tree.
InnoDB InnoDB is one of the most complex storage engines currently present in MySQL.
It has an extensive system for managing I/O and memory.
It has internal mechanisms for deadlock detection, and performs a quick and reliable crash recovery.
It implements a number of algorithms to overcome the performance limitations of traditional databases that support transactions.
Unlike MyISAM, which always stores its data in files, InnoDB uses tablespaces.
A tablespace can be stored in a file or on a raw partition.
All tables may be stored in one common tablespace, or every table may have its own tablespace.
The data is stored in a special structure called a clustered index, which is a B-tree with the primary key acting as the key value, and the actual record (rather than a pointer) in the data part.
A secondary key will store the value of the primary key that identifies the record.
Both primary and secondary keys are stored in a B-tree on disk.
However, when buffering the index page, InnoDB will build an adaptive hash index in memory to speed up the index lookups for the cached page.
While MyISAM buffers only the key pages, InnoDB buffers both keys and data.
On one hand, the buffering of the data does not have to depend on the operating system’s file cache, and good performance is achieved even when there is something wrong with the operating system’s file caching.
Additionally, in comparison with the operating system’s file cache, accessing the data avoids an extra system call.
On the other hand, with the operating system’s file cache still enabled, double caching (the same data being cached in the operating system’s file cache and in the data buffer) is possible, which only wastes memory.
The InnoDB engine keeps two types of logs: an undo log and a redo log.
The purpose of an undo log is to roll back transactions, as well as to display the older versions of the data for queries running in the transaction isolation level that requires it.
The purpose of a redo log is to store the information to be used in crash recovery.
It permits the recovery process to re-execute the transactions that may or may not have completed before the crash.
After re-executing those transactions, the database is brought to a consistent state.
Both formats store mostly the same information, but the new one uses less space.
The record begins with a list of field data offsets in the record.
The new format has 3 bits with the record type.
The remainder of the record contains the actual field data.
The complexity of the record format is necessary to optimize the insert operation.
A conventional B-tree would require moving half of the records on average when a new record is inserted.
InnoDB tries to avoid that with a very creative approach.
The heap number indicates the sequential order of the record in the page.
The next key pointer indicates the position of the next record in the primary key order.
To locate keys in a page efficiently, InnoDB maintains an additional structure known as the page directory.
It is a sparse sorted array of pointers to keys within the page.
Afterward, since the index is sparse, it may still be necessary to examine a few more keys in the linked list of records.
The number of records owned indicates how much further to go before the next record that has a pointer to it from the page directory is reached, and thus contains the information when the search for a given key stops.
Each InnoDB data row has two additional internal fields storing the information to be used in transactions, recovery, and multi-versioning.
One field is 6 bytes long and contains the ID of the last transaction that modified the record.
It also contains a 7byte field known as a roll pointer.
The roll pointer points to the record in the rollback segment in the undo log.
This pointer can be used to roll back a transaction, or to show the older version of the data if the current transaction isolation level requires it.
Memory (Heap) The MEMORY storage engine, formerly known as HEAP, stores its data in memory.
The original purpose of the code was for the optimizer to be able to create and use temporary tables when performing a SELECT that could be done in one pass.
After the introduction of the storage engine architecture in version 3.23, it became fairly easy to give users access to this in-memory table engine that was being used for temporary tables.
This simple addition has provided numerous benefits to MySQL users.
An in-memory table can be used to store temporary results when executing a complex set of queries; as a fast data accumulator that gets periodically flushed to disk; as a fast cache for a portion of the data from some large disk-based table; and in many other ways.
The MEMORY engine supports two types of keys: hash and B-tree.
The definition of the table is stored if the server is restarted.
However, the data rows are present only for as long as the server is running, and they are lost after a restart.
A hash index lookup is faster than a B-tree one if the exact value of the key is known.
However, if only the prefix of the key value is known, or only the limit values of a range are known, a hash index is of no help.
A B-tree, however, can answer such requests via the index.
A MEMORY table is generally faster than a similar MyISAM table on most operations.
However, if the MyISAM table is small enough to fit into the file cache of the operating system, the difference is not as big as you might expect: perhaps a factor of.
MyISAM Merge The MERGE storage engine combines a group of identically structured MyISAM tables into one logical unit.
Reads and writes can still happen to and from one of the MyISAM tables, or to and from the MERGE table.
The MERGE engine was created to solve a very common problem.
Most of the queries are restricted to a fairly narrow and easily predictable time range.
However, once in a while you need to query the whole table.
If you had all of the data in one MyISAM table, you would not get very good performance for a number of reasons: lock contention, increased unnecessary I/O, or long repair times in case of a crash.
Having all of the data in separate tables based on the time makes those queries that need to see more than one table unnecessarily complex.
You can now query individual tables when the time range is sufficiently narrow, and the MERGE table when it is not.
This storage engine is capable of storing the data on a fail-safe cluster of database servers.
In 2003, MySQL AB acquired the division of Ericsson (a Swedish telecom company) that had developed the NDB code to handle Ericsson’s phone system, and started the work on integrating it into MySQL.
A running MySQL server provides a central point of access to the NDB cluster.
The queries are parsed by the MySQL parser, and passed on to the optimizer.
At that time, the calls to the handler class method are translated into NDB, API calls (see the .hpp and .cpp files in the ndb/src/ndbapi directory), which in turn communicate with the cluster nodes.
The cluster nodes are divided into two types: management nodes (ndb_mgmd), and data nodes (ndbd)
Each data node loads its entire dataset into memory on startup, and writes it to disk on shutdown.
There are periodic asynchronous writes to disk to ensure that not much gets lost in case of a catastrophic failure of the entire cluster (e.g., the power goes down on all nodes at the same time)
The idea is that if you have a large dataset, you can set up enough data nodes to have enough combined memory to operate the cluster this way.
There is some work in progress to support operating the cluster from disk.
As one would expect, the performance of NDB greatly depends on the speed of the network that connects the cluster nodes.
The NDB cluster can use TCP/IP over Ethernet, or be connected via SCI bus and use SCI sockets.
If the nodes are on the same computer, shared memory can also be used.
Although using SCI can provide significant speed gains, TCP/IP over Ethernet is by far the simpler and the better tested method.
It is important to remember that NDB was created for a particular purpose (to meet the needs of a large telephone database application), which it has fulfilled very well.
It is well suited for similar applications that follow a similar design philosophy.
Archive The purpose of the ARCHIVE storage engine is to provide the functionality to store large amounts of data using the minimum amount of space.
The idea is to compress and archive away the data but still be able to query or append to it on occasion with minimum hassle.
This engine was created during several inspired coding sessions by Brian Aker, the Director of Architecture at MySQL AB.
Brian has an amazing ability to code up something very useful in a very short amount of time in between his other responsibilities.
Compared to MyISAM, InnoDB, or NDB, this is a very simple storage engine.
Deleting or updating a record in a compressed datafile is a very costly operation.
Not having to worry about updating and deleting data permits you to keep the records in a compressed format.
Additionally, such a limitation makes tampering with the existing data difficult: the only way to delete or update a record is to change it to another storage engine, run the modification query, and then change it back to the ARCHIVE storage engine.
With no need to worry about updates and deletes, solving the issue of high-performance concurrent access is easy.
Thus, the ARCHIVE engine provides the effect of having row-level locks as far as performance is concerned.
There is some discussion among MySQL developers that they might be supported in the future.
The source code of the ARCHIVE storage engine is found in storage/archive directory.
A reader interested in implementing his own storage engine is well advised to study this code.
It is simple enough to be fairly easily understood, and yet accomplishes enough to be useful as a next step for the examples we’ve covered thus far.
Federated This is another simple storage engine, and again the fruit of Brian Aker’s coding inspiration.
Its purpose is to allow access to tables stored on a remote MySQL server as if they were local.
The FEDERATED storage engine stores the information about how to access the remote server, and which table to map to in the comments field of the CREATE TABLE statement.
There are no other datafiles created or used by this storage engine.
When the optimizer requests the data from the storage engine, the storage engine in turn issues an SQL query to the remote server using the regular MySQL client/server communication protocol, and retrieves the data from the remote table.
When processing queries that update the table, the storage engine translates them into corresponding update queries on the remote server, and also sends them via the standard client/server protocol.
This storage engine also serves as a very good learning example.
In the MySQL architecture, the majority of the burden for implementing transactions is placed on the storage engine.
The details of transaction logging, row or page locks, implementing the isolation levels, commits and rollbacks, and other critical components of transaction implementations vary greatly from storage engine to storage engine.
However, every storage engine has to use the same interface to communicate with the upper SQL layer.
Thus the focus of this chapter will be to outline how to integrate an already existing transactional storage engine into MySQL.
InnoDB is the most robust transactional storage engine in MySQL.
Therefore I will use it as an example and analyze why things are done a certain way.
Overview of Transactional Storage Engine Implementation Chapter 7 discussed the basics of implementing a storage engine.
As you may recall, there are two parts to integrating a custom storage engine into MySQL: defining and implementing the handler subclass, and defining and implementing the handlerton.
This is understandable: the handler subclass methods are conceptually associated with a particular table instance, while the handlerton functions are associated only with the thread or connection.
Thus, operations such as COMMIT, ROLLBACK, and SAVEPOINT naturally fit into the handlerton mode of integration.
It is important to understand that the task of the actual implementation of transactions essentials is left completely to the discretion of the storage engine.
Even if you already have a fully functional transactional storage engine, the process of integration is not trivial.
How do you work with nontransactional or even transactional tables belonging to another storage engine? How do you handle the possible caching of queries? How do you handle replication logging? How do you avoid deadlocks?
You will also see solutions to the various challenges that you can try to understand and apply to your situation.
Implementing the handler Subclass The first simple but very important method to implement is handler::has_ transactions( )
It is used to report to the upper SQL layer that the storage engine has transactional support.
The return value of 1 (TRUE) is interpreted as the positive answer.
They both can be used by a transactional storage engine to start a transaction.
Originally, the purpose of this method was to prevent a table that could have been used by some application outside of MySQL server from being modified.
However, its strategic position in the hierarchy of calls makes it very useful for transactional storage engines to perform per-table-instance initializations to start a transaction.
A transactional storage engine will usually need a data structure to keep track of the state of the current transaction.
The MySQL storage engine architecture meets this need by allocating memory for a pointer to the transaction descriptor in the THD class.
That memory is found in the ha_data array under THD.
Each storage engine gets a location at a fixed offset in that array, specified by an autogenerated value that is placed in the slot member of the handlerton.
When a transaction is started, the storage engine needs to register it in the core SQL layer via a call to trans_register_ha( )
As you can see, the difference is in the value of the second argument.
If it is FALSE, only the current statement of the transaction is registered.
The purpose of registering transactions and statements is to facilitate the COMMIT and ROLLBACK operations.
A transaction may be started in two ways, externally or internally.
A transaction is started externally when the client issues a statement BEGIN or START TRANSACTION.
When a transaction starts externally, the upper SQL layer has control and can record the current state of the storage engine if needed.
However, when the transaction starts internally, the upper SQL layer does not have control.
Thus, the transaction registration process serves to notify the upper SQL layer of times when transactions start internally.
So the handler methods themselves normally would not do much related specifically to transaction support.
Rather, they serve as wrappers for the lower-level engine API calls that take care of transactional integrity as they store and retrieve the records.
As their names suggest, those functions have a common theme: they take a record in MySQL upper SQL layer format, perform the necessary format conversions, and then perform their respective operations such as searching for a record, updating a record, or inserting a record.
This is a pointer to a structure that organizes InnoDB table data in a way to be able to perform operations using records in the MySQL upper SQL layer format most efficiently.
There is one member in this structure that deserves more detailed attention, though: trx of type stuct trx_struct*
This structure may be of particular interest to those trying to integrate their own transactional storage engine.
Transactional issues are dealt with in stride as they arise.
Other operations, such as opening or creating a table, also follow a pattern.
There is usually a lengthy initialization followed by a call to a core InnoDB API function, which in turn is followed by some cleanup.
Overall, studying the implementation of the InnoDB handler reveals the complexity involved in integrating a powerful transactional storage engine into MySQL.
Defining the handlerton As you may recall from Chapter 7, a handlerton is a structure with data members and callback function pointers specific to a storage engine.
Unlike the handler class, a handlerton is not specific to a table instance.
A singleton is a well-known design pattern that applies when a class is created is such a way that only one instance of it can exist through the whole application.
A handlerton is in essence a singleton that is connected to a table handler, hence the name.
The introduction of the handlerton initiated from the need to support XA transactions, which have caused a major refactoring of the transaction handling within MySQL.
Thus, the handlerton became a crucial hub of transaction capability integration for storage engines.
These functions are invoked in direct response to their corresponding SQL commands.
Each handlerton member is prefixed with innobase_ to form the name of the actual InnoDB callback.
They have some initialization, a call to a core InnoDB API function to actually do the job, and then possibly some cleanup afterward.
Other callbacks require several calls to the core InnoDB API.
But in both cases, a handlerton callback serves mainly as glue between the core MySQL code and the core InnoDB API, to allow transactions to happen as they are supposed to.
Note that the complexity of the handlerton callbacks is much lower than that of the handler methods.
The reason for this might be that InnoDB has a streamlined transactional system, and therefore, when asked to do a standard transactional operation such as commit, rollback, or savepoint, it doesn’t require much glue to make it work from inside the core MySQL code.
However, things are different when records are being accessed individually via the handler methods.
The expectations of the MySQL upper SQL layer might not always be in line with the native capabilities of InnoDB.
Thus, a lot of glue work is required, and the code is more complex.
Working with the Query Cache MySQL has a unique feature for a database: a query cache.
The server can be configured to cache the results of every SELECT.
Then, if another SELECT arrives that is identical to one that is cached, and the tables involved in the query have not changed, the cached result is returned immediately instead of MySQL actually going to the tables and pulling out the matching records.
This feature provides a great performance boost for a number of applications, especially web applications that heavily rely on a database and are frequently accessed by a large number of users in a way that makes them send identical queries to the MySQL server.
Since the introduction of the query cache, many MySQL users have reported two- to threefold improvements in performance.
Thus, any storage engine, transactional or not, needs to be able to work correctly with the query cache.
The main issue of working with the query cache is being able to easily tell if the table has changed or not.
While it is not difficult for a nontransactional storage engine to answer that question, things are not as easy for a multiversioned transactional storage engine that supports various isolation levels.
If a handler does not support it, the query cache will use the pessimistic approach: on every commit it will invalidate all queries that refer to tables used in the committed transaction.
It performs a fairly complex analysis to make the decision.
The function handles all the issues and returns TRUE if the query involving the given table is safe to cache, and FALSE otherwise.
Working with the Replication Binary Log MySQL replication works by having the master maintain a binary log of updates (called the binlog), and having the slave read and apply them.
Thus, it becomes critical for a transactional storage engine to make sure that the contents of the binary log are consistent with the state of the database.
The core MySQL code already provides a lot of help.
The SQL statements are not written into the binary log until the transaction commits, and they are not written at all if the transaction gets rolled back.
However, there are a couple of critical issues a transactional storage engine might need to address:
To guarantee consistency of binlog and table data in case of a crash, the storage engine must implement XA transactions.
In a statement-based replication, slaves execute binlog updates sequentially and in one thread.
Thus, all of the updates on the master must happen under the SERIALIZABLE transaction isolation level in order to guarantee the same results on the slave.
Avoiding Deadlocks A transactional storage engine with row-level locking, especially one integrated with an SQL server, is naturally prone to deadlocks.
Thus, it is important to have a plan for avoiding or resolving deadlocks.
When placing a new lock, InnoDB makes sure that it doesn’t cause deadlocks.
It will roll back a problematic transaction when a deadlock is discovered.
However, the deadlock detection algorithm is aware only of InnoDB locks, and cannot detect deadlocks when some of the problem locks do not belong to InnoDB.
To solve this problem, InnoDB has also timeoutbased deadlock detection that rolls back transactions that are taking a long time.
Thus, the application programmer should be prepared to reissue a transaction if it gets rolled back due to a timeout or risk of deadlock.
While this may appear to be a major setback in the area of performance, in practice, properly optimized queries in a well-designed application almost never cause a deadlock.
Another issue that working with the MySQL server adds to the deadlock dilemma is the need to be aware of MySQL table locks.
MySQL allows a user to lock a table directly at the start of a transaction via the LOCK TABLES command.
With the increasing number of storage engines being added to MySQL, a possibility that existed mostly on the theoretical level is becoming more and more of a reality.
The InnoDB method of lock timeout can be used to address this problem.
The master keeps a log of updates, while the slave reads it and executes it in sequence.
A server may act as a master or a slave.
The master maintains a log of updates that is called the binary log for historical reasons.
Each event contains some information that is relevant for the slave to be able to execute the update exactly the same way the master did it.
The majority of events are merely SQL queries that update the database in one way or another.
However, it is also necessary to store some metadata that the slave must use to recreate the context of the update in order for the update query to yield the same results.
The slave connects to the master and starts executing updates as it reads them from the master’s binary log.
There are two threads on the slave to perform this work: the I/O thread and the SQL thread.
The I/O thread downloads the contents of the master binary log and stores them locally in temporary files called relay logs.
The relay logs are processed by the SQL thread, which re-creates the original execution context and executes the updates.
The slave keeps track of where it is in the replication process via two parameters: current log name and current log position.
If the slave ever disconnects from the master and then reconnects, the slave will request a feed of updates starting from its current position in the current log.
The master keeps track of the log sequence order and will automatically switch to the next log once the end of the last one is reached during the binary log feed process.
During the initial connection, the slave requests a read from the first log known to the master.
It is possible to tell the slave to start replication from an arbitrary position using the CHANGE MASTER TO command.
This means that at some point in the future the slave will catch up to the current state of the master, but the master does not normally wait for the slave to catch up.
The lag between the slave and the master depends on a number of factors: the speed of the network connecting the two servers, the types of update queries being run, the processing capabilities of the master and the slave, and the load on both servers.
While this technique may be useful in a number of situations, it is frequently impractical.
Network delays and outages, load spikes on the master or the slave, and possibly other circumstances may create unacceptable delays of the application user.
Statement-Based Versus Row-Based Replication Replicating the data between two SQL databases can take place on the SQL level or on the row level.
In the statement-based approach, every SQL statement that could modify the data gets logged on the master.
Then those statements are re-executed on the slave against the same initial dataset and in the same context.
In the row-based approach, every row modification gets logged on the master and then applied on the slave.
Statement-based replication generally requires less data to be transferred between the master and the slave, as well as taking up less space in the update logs.
It does not have to deal with the format of the row.
The compactness of the data transfer will generally allow it to perform better.
On the other hand, it is necessary to log a lot of execution context information in order for the update to produce the same results on the slave as it did originally on the master.
In some cases it is not possible to provide such a context.
Statement-based replication is also more difficult to maintain, as the addition of new SQL functionality frequently requires extensive code updates for it to replicate properly.
It is only necessary to know which record is being updated, and what is being written to that record.
Given a good code base, the maintenance of a row-based replication is also fairly simple.
Since the logging happens at a lower level, the new code will naturally execute the necessary low-level routines that modify the database, which will do the logging with no additional code changes.
It requires a lot of awareness of the internal physical format of the record, and still has to deal with the schema modifications.
In some situations the performance overhead associated with the increased I/O could become unacceptable.
Up to version 5.0, the developers managed to deal with the drawbacks of this approach.
Creative techniques were invented to properly replicate the execution context in difficult situations.
However, with the introduction of stored procedures it became impossible to keep up.
A stored procedure has the ability to branch on a number of conditions and in a number of different ways.
MySQL replication developers addressed the problem by adding an option to replicate row by row, starting in version 5.1.5
As of version 5.1.8, MySQL can take advantage of three replication modes: row, statement, and mixed.
The mode is controlled by the configuration variable binlog_ format.
In row mode, the replication is physical whenever it can be: when actual rows are updated, the entire updated row is written to the binary log.
However, when a new table is created, dropped, or altered, the actual SQL statement is recorded.
In statement mode, the replication works the same as it did in the earlier versions: SQL statements are logged for every update.
In mixed mode, the master decides on a per-query basis which logging to use, statement-based or row-based.
Two-Threaded Slave The original implementation of the slave (version 3.23) used only one thread, which was responsible for reading the binary log feed from the master and applying it to the slave data.
This approach was fine for reliable masters and slaves that did not lag too far behind.
However, there were situations in which this was not adequate.
Suppose the slave somehow ends up lagging a day behind a master, while the master somehow becomes completely unusable.
This would result in the loss of a day’s worth of data.
At the suggestion of Jeremy Zawodny from Yahoo!, the slave code was rewritten in version 4.0 to use two threads.
The I/O thread is responsible for reading the binary log feed from the master and storing it in temporary relay logs.
The SQL thread then reads the relay logs and applies the updates to the slave data.
To a great extent, this eliminated the risk of losing large amounts of data when the slave lags behind and the master becomes unusable.
The majority of the time, the reason the slave lags behind is not in the I/O but in the slow execution of the updates.
For example, slaves are often used to perform the reads that are not timecritical so that the master can be relieved.
Thus, a slave may experience a spike in the load, which will delay the application of the updates arriving from the master.
Another possibility is that an update query is encountered in the binary log feed that takes a long time to execute—let’s say three hours.
Assuming the master and the slave are equally capable hardware-wise and are equally loaded, we can have the following scenario.
The master executes a three-hour update, and it is written to the binary log once it is completed.
Then the slave reads that update and starts executing it.
In the meantime, the master had three hours to perform possibly a very large.
By the time the slave is done with the long update query, it is three hours’ worth of updates behind the master.
In both scenarios, the two-threaded slave architecture permits the additional updates to be transferred from the master in the binary log feed while the slave is applying the long three-hour update.
If the master happens to become unusable beyond repair during that window, the update feed in the relay logs will contain a very recent update.
It is still possible to miss a couple of updates prior to the crash due to the network delays, but it is much better than losing several hours of data.
Multi-Master MySQL replication was not originally written with multi-master support in mind.
A slave is natively capable of replicating only one master.
A fairly simple patch can be created to allow one slave to collect updates from multiple masters without conflict resolution.
This was done at one time, but for a number of reasons did not make it into the main branch of the source tree.
A more complex patch to allow some conflict resolution was planned at one point, but for a number of reasons did not make it to development.
In the mean time, there exists a very popular configuration that in essence serves as a multi-master.
Specifically, server A is configured with the binlog enabled and as a slave of server B, while server B has its binlog also enabled and acts as a slave of server A.
Thus it becomes possible to write to either of the servers and have the updates appear on both.
This configuration, however, will maintain a consistent data snapshot as long the stream of updates is guaranteed to produce the same results regardless of their order, or as long as the updates are serialized.
It is possible that server B will get the update event from server A before server B executes its update.
However, it is also possible that server B will take a while to get the update from server A, and will execute its update first.
The logic of the application would anticipate it to be 100 since that is the query that was executed last.
However, if all updates are order-independent, then the mutual master-slave relationship configuration will produce consistent results.
It is also possible to use this configuration for a hot failover.
It that server goes down, the application switches to the other server.
When the original server comes back up, it will catch up automatically (barring a drastic crash with the loss of data), and can serve as a standby.
Let us first configure one server as a master by enabling the log-bin option and setting a server ID with the server-id option to some unique number, e.g., the last byte of the IP address.
We use the \G option to enable vertical display of columns, which makes the output more readable.
The File field is the name of the current binary log to which the master is writing.
The Position field shows the offset in the current binary log where the next event is going to be written, or in other words, the size of the current binary log.
Note that those rules apply to the default database of the thread (selected via the USE command, or a call to mysql_select_db( ) in the client API), rather than the actual database where the update occurred.
The combination of the File and the Position fields is sometimes referred to as the replication coordinates.
The replication coordinates allow a slave to tell the master where to start the binary log feed when the slave connects to the master.
They also make it possible to track the progress of a slave as it applies the updates, and can be used to synchronize the master and the slave.
In this example, slave-host should be replaced with the host name or the IP address of the slave.
We choose a unique server ID (the last byte of the IP address works well), load the current dataset from the master, and instruct the slave of the location of the master with the following command:
If we did not encounter any problems, we will see an output similar to this:
Slave_IO_State Textual description of what the I/O thread on the slave is doing.
Master_User The slave I/O thread connects to the master as this user.
Master_Port The TCP/IP port on which the slave IO thread connects to the master.
Connect_Retry If the slave IO thread loses the connection to the master, it will retry after the timeout specified by this parameter.
Master_Log_File The logfile name on the master corresponding to the event that the SQL thread is currently processing.
Relay_Log_File The name of the relay log that the SQL thread is on.
Relay_Log_Pos The position in the relay log that the SQL thread is on.
Replicate_Do_DB Shows which databases are replicated according to the replicate-do-db rules.
Replicate_Do_Table Shows which tables are replicated according to the replicate-do-table rules.
Last_Errno The error code of the last error that caused the replication to stop.
Last_Error The text of the error message of the last error that caused the replication to stop.
Skip_Counter The number of subsequent events that the SQL thread is going to skip.
This is used mostly when replication breaks due to some problem or possibly an oversight on the part of the database administrator, and there is a query that needs to be skipped for the replication to continue as planned.
Exec_Master_Pos The position in the master log corresponding to the current position of the SQL thread.
Relay_Log_Space The amount of disk space (in bytes) occupied by the relay logs.
Until_Condition Sometimes a slave can be instructed to replicate until a certain position in the master or relay log is reached.
This parameter tells whether there is an UNTIL condition, and whether it is in the context of the master or the relay log.
Until_Log_File The name of the logfile in the UNTIL condition.
Until_Log_Pos The position in the logfile in the UNTIL condition.
Master_SSL_Allowed Indicates whether the slave I/O thread should connect to the master via SSL.
Master_SSL_CA_File Pathname to the certificate authority file the slave will use to connect to the master.
Master_SSL_CA_Path Pathname to a directory containing trusted SSL CA certificates in pem format.
To an observant reader, the status variables in Table 12-1 tell a story about how replication works.
For example, the Master_ connect parameters indicate that the slave connects to the master and acts as a regular MySQL client.
The presence of the Connect_retry parameter explains that a slave is capable of dealing with disruptions in the connectivity.
The Relay_ parameters explain how the slave stores data temporarily in relay logs.
The SSL parameters tell us the I/O between the master and the slave can be encrypted via SSL.
The Replicate_ options tell us about the ability of the slave to replicate selectively.
Binary Log Format Learning some details of the binary log format can reveal a lot about the replication internals.
The magic number is used by the code that reads the binary log for a quick sanity check to make sure a valid binary log is being used.
It is also used by the Unix file utility, which identifies file types.
The magic number is followed by a sequence of event entries.
All events have a common header with the fields listed in Table 12-2 in sequential order.
The second field represents the type code of the event, and is explained in Table 12-3
All of the integers in the header are stored in the little-endian format (low byte first)
Master_SSL_Cipher The cipher to be used in the SSL connection.
Otherwise, it will show 0, which may not always be accurate because the I/O thread may take some time to read an event from the master.
Number of seconds since the start of the year 1970 as returned by the libc call time( )
The values and the meaning of the code are explained in Table 12-3
The length of the whole event, including the header, in bytes.
Start Written at the start of the binary log in earlier versions of MySQL.
Intvar Contains the value of the auto-increment field to be used in the next query.
In the newer version of MySQL, the New Load event is used instead.
Create File Tells the slave to create a file with the given ID for the purpose of replicating LOAD DATA INFILE.
Append Block Tells the slave to append a block to the file specified by the ID.
Rand Records the information necessary to reseed the random-number generator used by the RAND( ) function so that it will produce the same results on the slave as it did on the master.
User Var Records the value stored in a user variable that was used in an update.
The header of an event is followed by the body.
The structure of the body greatly varies by the event type.
Again, the study of the binlog format reveals a lot of hidden details and challenges associated with replication.
Although the basic conceptual idea of a master-slave replication with the master keeping an update log and the slave re-executing it is nearly trivial, the devil is in the details, and the complexity of the binary log format and the variety of event types are a witness to it.
Those interested in the specifics should refer to the source code in sql/log_event.cc.
Log_event is the base class for a family of classes responsible for each event.
The names of the classes begin with the name of the event, and end with _log_event.
Format Description Starting in version 5.0.2 of MySQL, written as the first event in the binary log to specify which format version it is using.
Write Rows Contains a list of rows to write to a table.
Update Rows Contains a list of rows to update in a table.
Delete Rows Contains a list of rows to delete from a table.
It accepts the name of the log as an argument and dumps it out in the SQL format with some comments about the log details.
Each event in the output starts with a comment line indicating its offset in the binary log.
The next line, also a comment, shows the timestamp of the event, the server ID originating the event, the position of the next event, and the type of the event.
Following that is a set of SQL queries that will produce the same change in the database on the slave as the original event produced on the master.
It is possible to examine only portions of a binary log using the arguments --start-position, --stop-position, --start-datetime, and --stop-datetime.
Creating a Custom Replication Utility Usually replication can be successfully managed via SQL commands alone.
This is the recommended approach and should be used to its fullest extent whenever possible.
Occasionally a situation arises when SQL commands alone are not enough.
For example, you may want to replicate only a certain subset of events that are not easily defined with the standard replication table matching rules.
Or perhaps you need to rewrite certain queries before replicating them.
In this case, the open source nature of MySQL comes handy.
The approach that yields the best performance and also provides a more robust solution when done right is to modify the loop in the code of the SQL thread in sql/slave.cc or somewhere down the calling hierarchy.
One simple mistake can not only break replication, but crash the entire slave server.
Keeping up with the new releases of MySQL may become a hassle.
And, overall, a deeper understanding of MySQL source code is required to accomplish the task.
On the positive side, this approach requires less knowledge of MySQL source code and is less intrusive.
On the negative side, you do not get to tap into the proven robustness of the native slave event management and processing code, and a fair amount of unnecessary I/O will happen due to the presence of a mediator.
We’d like to hear your suggestions for improving our indexes.
He worked on the MySQL development team from 2000–2002 and was the original developer of replication in MySQL.
In 2003, he wrote his first book, MySQL Enterprise Solutions (Wiley)
He currently lives in Provo, Utah, with his wife, Sarah, and his five children, and works as an independent consultant with an emphasis on MySQL.
In addition to his great interest in computers, Sasha is equally passionate about distance running.
He has won a number of marathons, has a personal best time of 2:24:47, and is attempting to qualify for the U.S.
Colophon The animal on the cover of Understanding MySQL Internals is a banded broadbill (Eurylaimus javanicus)
Broadbills are a family of small- to medium-size passerine (perching) birds marked by their bright colors and a whitish dorsal patch.
They have large heads, rounded wings, and short to long tails.
Their name originates from their large, flattened, hooked bill (often covered by a short crest), which they use to snap up insects in a kingfisher-like fashion.
Because of their dense habitats, they are often incredibly difficult to observe despite their bright coloring.
Broadbills build elaborate, pearshaped nests, which are suspended on tree limbs over quiet forest backwaters and streams.
Biologists believe this may be an adaptive behavior to deter mammalian and reptilian predators.
Adult broadbills will also sometimes feign injury to draw predators away from their nests.
Broadbills are gregarious creatures and are often found in small feeding flocks.
They communicate using a variety of mating and territorial displays.
Male green broadbills, for instance, have a spinning display, while other species of broadbills may employ head bobbing, wing flapping, and feather fluffing.
Some have display flights in which their primary wing feathers “buzz” during times of courtship or territorial defense.
The sound can be heard from as far as 60 meters away.
In addition, broadbills have a variety of calls—often described as a cacophony of whistles, rattles, or screams—which they use during courtship rituals, as an alarm signal, and for contact between mates.
