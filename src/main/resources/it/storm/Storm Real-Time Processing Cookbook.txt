No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Quinton Anderson is a software engineer with a background and focus on real-time computational systems.
His career has been split between building real-time communication systems for defense systems and building enterprise applications within financial services and banking.
Quinton does not align himself with any particular technology or programming language, but rather prefers to focus on sound engineering and polyglot development.
He is passionate about open source, and is an active member of the Storm community; he has also enjoyed delivering various Storm-based solutions.
Quinton's next area of focus is machine learning; specifically, Deep Belief networks, as they pertain to robotics.
Please follow his blog entries on Computational Theory, general IT concepts, and Deep Belief networks for more information.
I would like to thank the Storm community for their efforts in building a truly awesome platform for the open source community; a special mention, of course, to the core author of Storm, Nathan Marz.
I would like to thank my wife and children for putting up with my long working hours spent on this book and other related projects.
Your effort in making up for my absence is greatly appreciated, and I love you all very dearly.
I would also like to thank all those who participated in the review process of this book.
Maarten Ectors is an executive who is an expert in cloud computing, big data, and disruptive innovations.
Maarten's strengths are his combination of deep technical and business skills as well as strategic insights.
Currently, Maarten is responsible for the cloud strategy at Canonical—the company behind Ubuntu—where he is changing the future of cloud, big data, and other disruptive innovations.
Previously, Maarten had his own company and was defining and executing the cloud strategy of a global mobile company.
He was heading cloud and disruptive innovation, founded Startups@NSN, was responsible for implementing offshoring in Europe, and so on.
Earlier, he worked as the Director of professional services for Telcordia (now Ericsson) and as a Senior Project / Product Manager for a dotcom.
Maarten started his career at Accenture, where he was active in Java developments, portals, mobile applications, content management, ecommerce, security, project management, and so on.
I would like to thank my family for always being there for me.
Alexey Kachayev began his development career in a small team creating an open source CMS for social networks.
For over 2 years, he had been working as a Software Engineer at CloudMade, developing geo-relative technology for enterprise clients in Python and Scala.
Currently, Alexey is the CTO at Attendify and is focused on development of a distributed applications platform in Erlang.
He is an active speaker at conferences and an open source contributor (working on projects in Python, Clojure, and Haskell)
His area of professional interests include distributed systems and algorithms, types theory, and functional language compilers.
I would like to thank Nathan Marz and the Storm project contributors team for developing such a great technology and spreading great ideas.
He is a recognized expert in Hadoop, R, Data Science, and Cloud Computing, and has led innovative data teams building large-scale apps for the past decade.
Paco is an evangelist for the Mesos and Cascading open source projects.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Why Subscribe? f Fully searchable across every book published by Packt.
Preface Open source has changed the software landscape in many fundamental ways.
There are many arguments that can be made for and against using open source in any given situation, largely in terms of support, risk, and total cost of ownership.
Open source is more popular in certain settings than others, such as research institutions versus large institutional financial service providers.
Within the emerging areas of web service providers, content provision, and social networking, open source is dominating the landscape.
This is true for many reasons, cost being a large one among them.
These solutions that need to grow to "Web scale" have been classified as "Big Data" solutions, for want of a better term.
These solutions serve millions of requests per second with extreme levels of availability, all the while providing customized experiences for customers across a wide range of services.
Designing systems at this scale requires us to think about problems differently, architect solutions differently, and learn where to accept complexity and where to avoid it.
As an industry, we have come to grips with designing batch systems that scale.
Large-scale computing clusters following MapReduce, Bulk Synchronous Parallel, and other computational paradigms are widely implemented and well understood.
The surge of innovation has been driven and enabled by open source, leaving even the top software vendors struggling to integrate Hadoop into their technology stack, never mind trying to implement some level of competition to it.
More data, more services, more value, more convenience, and they want it now and at lower cost.
As the sheer volume of data increases, the demand for real-time response time increases too.
The next phase of computational platforms has arrived, and it is focused on real time, at scale.
It represents many unique challenges, and is both theoretically and practically challenging.
This cookbook will help you master a platform, the Storm processor.
The Storm processor is an open source, real-time computational platform written by Nathan Marz at Backtype, a social analytics company.
It was later purchased by Twitter and released as open source.
It has since thrived in an ever-expanding open source community of users, contributors, and success stories within production sites.
Moreover, you will find by the end of this book that it is also extremely enjoyable to deliver systems based on Storm, using whichever language is congruent with your way of thinking and delivering solutions.
This book is designed to teach you Storm with a series of practical examples.
These examples are grounded in real-world use cases, and introduce various concepts as the book unfolds.
Furthermore, the book is designed to promote DevOps practice around the Storm technology, enabling the reader to develop Storm solutions and deliver them reliably into production, where they create value.
An introduction to the Storm processor A common criticism of open source projects is their lack of documentation.
Storm does not suffer from this particular issue; the documentation for the project is excellent, well-written, and well-supplemented by the vibrant user community.
This cookbook does not seek to duplicate this documentation but rather supplement it, driven largely by examples with conceptual and theoretical discussion where required.
It is highly recommended that the reader take the time to read the Storm introductory documentation before proceeding to Chapter 1, Setting Up Your Development Environment, specifically the following pages of the Storm wiki:
What this book covers Chapter 1, Setting Up Your Development Environment, will demonstrate the process of setting up a local development environment for Storm; this includes all required tooling and suggested development workflows.
Chapter 2, Log Stream Processing, will lead the reader through the process of creating a log stream processing solution, complete with a base statistics dashboard and log-searching capability.
Chapter 3, Calculating Term Importance with Trident, will introduce the reader to Trident, a data-flow abstraction that works on top of Storm to enable highly productive enterprise data pipelines.
Chapter 4, Distributed Remote Procedure Calls, will teach the user how to implement distributed remote procedure calls.
Chapter 5, Polyglot Topology, will guide the reader to develop a Polyglot technology and add new technologies to the list of already supported technologies.
Chapter 6, Integrating Storm with Hadoop, will guide the user through the process of integrating Storm with Hadoop, thus creating a complete Lambda architecture.
Chapter 7, Real-time Machine Learning, will provide the reader with a very basic introduction to machine learning as a topic, and provides various approaches to implementing it in realtime projects based on Storm.
Chapter 8, Continuous Delivery, will demonstrate how to set up a Continuous Delivery pipeline and deliver a Storm cluster reliably into an environment.
Chapter 9, Storm on AWS, will guide the user through various approaches to automated provisioning of a Storm cluster into the Amazon Computing Cloud.
What you need for this book This book assumes a base environment of Ubuntu or Debian.
The first chapter will guide the reader through the process of setting up the remaining required tooling.
If the reader does not use Ubuntu as a developer operating system, any *Nix-based system is preferred, as all the recipes assume the existence of a bash command interface.
Who this book is for Storm Real-time Processing Cookbook is ideal for developers who would like to learn real-time processing or would like to learn how to use Storm for real-time processing.
Clojure, C++, and Ruby experience would be useful but is not essential.
It would also be useful to have some experience with Hadoop or similar technologies.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "You must then create your first spout by creating a new class named HelloWorldSpout, which extends from BaseRichSpout and is located in the storm.cookbook package."
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "Uncheck the Use default location checkbox."
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/submit-errata, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Introduction This chapter provides a very basic and practical introduction to the Storm processor.
This will cover everything, from setting up your development environment to basic operational concerns in deploying your topologies and basic quality practices such as unit and integration testing of your Storm topology.
Upon completion of this chapter, you will be able to build, test, and deliver basic Storm topologies.
This book does not provide a theoretical introduction to the Storm processor and its primitives and architecture.
The author assumes that the readers have orientated themselves through online resources such as the Storm wiki.
Delivery of systems is only achieved once a system is delivering a business value in a production environment consistently and reliably.
In order to achieve this, quality and operational concerns must always be taken into account while developing your Storm topologies.
Setting up your development environment A development environment consists of all the tools and systems that are required in order to start building Storm topologies.
The focus of this book is on individual delivery of Storm with a focus on the technology; however, it must be noted that the development environment for a software development team, be it centralized or distributed, would require much more tooling and processes to be effective and is considered outside the scope of this book.
The following classes of tools and processes are required in order to effectively set up the development environment, not only from an on-going perspective, but also in terms of implementing the recipes in this book:
The provisioning and installation recipes in this book are based on Ubuntu; they are, however, quite portable to other Linux distributions.
Environmental variables are the enemy of maintainable and available systems.
Developing on one environment type and deploying on another is a very risky example of such a variable.
Developing on your target type should be done whenever possible.
The installation should then be followed by Maven, the build system:
Puppet, Vagrant, and VirtualBox must then be installed in order to provide application and environment provisioning:
There is currently a debate around which fork of the Java SDK is to be used since Sun was acquired by Oracle.
While the author understood the need for OpenJDK, the recipes in this book have been tested using the Oracle JDK.
In general, there is no difference between OpenJDK and Oracle JDK, apart from the Oracle JDK being more stable but lagging behind in terms of features.
How it works… The JDK is obviously required for any Java development to take place.
Maven is a widely used build system that prefers convention over configuration.
Maven includes many useful features including the Project Object Model (POM), which allows us to manage our libraries, dependencies, and versions in an effective manner.
Maven is backed by many binary repositories on the Internet that allow us to transparently maintain binary dependencies correctly and package our topologies for deployment.
Within the growing arena of DevOps and Continuous Delivery, the Puppet system is widely used to provide declarative server provisioning of Linux and other operating systems and applications.
Puppet provides us with the ability to program the state of our servers and deployment environments.
This is important because our server's state can then be maintained within a version control system such as GIT and manual changes to servers can be safely removed.
This provides many advantages, including deterministic Mean Time to Recovery (MTTR) and audit trail, which, in general, means making systems more stable.
This is also an important step on the path towards continuous delivery.
It allows the automation of provisioning of VirtualBox virtual machines.
Within the context of the Storm processor, this is important, given that it is a cluster-based technology.
In order to test a cluster, you must either build an actual cluster of machines or provision many virtual machines.
Vagrant allows us to do this locally in a deterministic and declarative way.
A virtual machine is an extremely useful abstraction within the IT infrastructure, operations, and development.
However, it must be noted that, while reduced performance is expected and acceptable within locally hosted VMs, their usability at all times depends entirely on the availability of RAM.
The processing power is not a key concern, especially with most modern processors being extremely underutilized, although this is not necessarily the case once your topologies are working; it is recommended that you ensure your computer has at least 8 GB of RAM.
Each client contains a checkout of the files at their current version, depending on what branch the client is using.
This has worked well, in such a way that it allows teams to collaborate closely and know to some degree what other members of the team are doing.
Centralized servers have some distinct downfalls that have led to the rise of distributed control systems.
Firstly, the centralized server represents a single point of failure; if the server goes down or becomes unavailable for any reason, it becomes difficult for developers to work using their existing workflows.
Secondly, if the data on the server is corrupt or lost for any reason, the history of the code base is lost.
Open source projects have been a large driver of distributed version controls, for both reasons, but mostly because of the collaboration models that distribution enables.
Developers can follow a disciplined set of workflows on their local environments and then distribute these changes to one or many remote repositories when it is convenient to do so, in both a flat and hierarchical manner.
The obvious additional advantage is that there naturally exist many backups of the repository because each client has a complete mirror of the repository; therefore, if any client or server dies, it can simply be replicated back, once it has been restored.
How to do it… Git is used in this book as the distributed version control system.
In order to create a repository, you need to either clone or initialize a repository.
For a new project that you create, the repository should be initialized.
In order to test if the workflow is working, we need some files in our repository.
Using vim, or any other text editor, simply add some descriptive text and press the Insert key.
Once you have finished typing, simply hit the Esc key and then a colon, followed by wq; hit the Enter key.
This should give you an output that looks similar to the following:
Git requires that you add all files and folders manually; you can do it as follows:
This will open a vim editor and allow you to add your comments.
You can specify the commit message directly while issuing the command, using the –m flag.
Without pushing this repository to a remote host, you will essentially be placing it under the same risk as that of a centralized host.
It is therefore important to push the repository to a remote host.
In order to push your repository to this remote host, simply navigate there in your browser and sign up for an account.
Once the registration process is complete, create a new repository using the menu system.
Enter the following values in order to create the repository:
Once the repository is created, you need to add the remote repository to your local repository and push the changes to the remote repository.
You must replace [user] in the preceding command with your registered username.
Cloning of a repository will be covered in later recipes, as will some standard version control workflows.
Creating a "Hello World" topology The "Hello World" topology, as with all "Hello World" applications, is of no real use to anyone, except to illustrate some really basic concepts.
The "Hello World" topology will show how to create a Storm project including a simple spout and bolt, build it, and execute it in the local cluster mode.
Create a new project folder and initialize your Git repository.
We must then create the Maven project file as follows:
Using vim, or any other text editor, you need to create the basic XML tags and project metadata for the "Hello World" project.
We then need to declare which Maven repositories we need to fetch our dependencies from.
Add the following to the pom.xml file within the project tags:
You can override these repositories using your .m2 and settings.xml files, the details of which are outside the scope of this book; however, this is extremely useful within development teams where dependency management is the key.
We then need to declare our dependencies by adding them within the project tags:
Finally we need to add the build plugin definitions for Maven:
With the POM file complete, save it using the Esc + : + wq + Enter key sequence and complete the required folder structure for the Maven project:
Then return to the project root folder and generate the Eclipse project files using the following:
You must now start your Eclipse environment and import the generated project files into the workspace:
HelloWorldSpout, which extends from BaseRichSpout and is located in the storm.cookbook package.
The spout will simply generate tuples based on random probability.
After construction, the Storm cluster will open the spout; provide the following implementation for the open method:
The Storm cluster will repeatedly call the nextTuple method, which will do all the work of the spout.
This class will consume the produced tuples and implement the required counting logic.
Create the new class within the storm.cookbook package; it should extend the BaseRichBolt class.
Declare a private member variable and provide the following implementation for the execute method, which does the work for this bolt:
Finally, you need to bring the elements together and declare the Storm topology.
Create a main class named HelloWorldTopology within the same package and provide the following main implementation:
This will essentially set up the topology and submit it to either a local or remote Storm cluster, depending on the arguments passed to the main method.
After you have resolved the compiler issues, you can execute the cluster by issuing the following command from the project's root folder:
How it works… The following diagram describes the "Hello World" topology:
The spout essentially emits a stream containing one of the following two sentences:
Based on random probability, it works by generating a random number upon construction and then generates subsequent random numbers to test against the original member's variable value.
When it matches, Hello World is emitted; during the remaining executions, the other random words are emitted.
The bolt simply matches and counts the instances of Hello World.
In the current implementation, you will notice sequential increments being printed from the bolt.
In order to scale this bolt, you simply need to increase the parallelism hint for the topology by updating the following line:
The key parameter here is parallism_hint, which you can adjust upwards.
If you execute the cluster again, you will then notice three separate counts that are printed independently and interweaved with each other.
You can scale a cluster after deployment by updating these hints using the Storm GUI or CLI; however, you can't change the topology structure without recompiling and redeploying the JAR.
For the command-line option, please see the CLI documentation on the wiki available at the following link:
It is important to ensure that your project dependencies are declared correctly within your POM.
The Storm JARs must be declared with the provided scope; if not, they would be packaged into your JAR; this would result in duplicate class files on the classpath within a deployed node of the cluster.
Note that Storm checks for this classpath duplication; it will fail to start if you have included Storm into your distribution.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.
Testing the cluster in the local mode is useful for debugging and verifying the basic functional logic of the cluster.
It doesn't, however, give you a realistic view as to the operation of the cluster.
Moreover, any development effort is only complete once the system is running in a production environment.
This is a key consideration for any developer and is the cornerstone of the entire DevOps movement; regardless of the methodology, however, you must be able to reliably deploy your code into an environment.
This recipe demonstrates how to create and provision an entire cluster directly from version control.
It isn't acceptable that people can log into a server and make changes to its settings or files without strict version control being in place.
In software development and IT operations, this applies heavily to disaster recovery and integration.
Both tasks can only be performed often if they are automated.
The deployment of Storm topologies to an AWS cluster is the subject for a later chapter; however, the fundamentals will be presented in this recipe in a development environment.
Using your favorite editor, create a file in the project root called Vagrantfile.
Inside the file, you must create the file header and the configuration for the virtual machines that we want to create.
We need at least one nimbus node, two supervisor nodes, and a zookeeper node:
Note that the use of a single zookeeper node is only for development environments, as this cluster is not highly available.
The purpose of this cluster is to test your topology logic in a realistic setting and identify stability issues.
You must then create the virtual machine provisioning for each machine, specialized by the previous configuration at execution time.
The first set of properties defines the hardware, networking, and operating system:
The provisioning of the application is then configured using a combination of the bash and Puppet scripts:
The Vagrant file simply defines the hypervisor-level configuration and provisioning; the remaining provisioning is done through Puppet and is defined at two levels.
The first level makes the base Ubuntu installation ready for application provisioning.
In order to create the first level of provisioning, you need to create the JDK provisioning bash script and the provisioning initialization Puppet script.
In the scripts folder of the project, create the installJdk.sh file and populate it with the following code:
This will simply be invoked by the Puppet script in a declarative manner.
For more information on Hiera, please see the Puppet documentation page at http://docs.puppetlabs.com/hiera/1/index.html.
You must then clone the repository, which contains the second level of provisioning:
You must now configure a Puppet plugin called Hiera, which is used to externalize properties from the provisioning scripts in a hierarchical manner:
The final datafile required is the host's file, which act as the DNS in our local cluster:
The host's file is not required in properly configured environments; however, it works nicely in our local "host only" development network.
The project is now complete, in that it will provision the correct virtual machines and install the base required packages; however, we need to create the Application layer provisioning, which is contained in a separate repository.
Initialize your Git repository for this project and push it to bitbucket.org.
This recipe only works in the bottom two layers, with the Application layer presented in the next recipe.
A key reason for the separation is that you will typically create different provisioning at these layers depending on the Hypervisor you are using for deployment.
Once the VMs are provisioned, however, the application stack provisioning should be consistent through all your environments.
This is key, in that it allows us to test our deployments hundreds of times before we get to production, and ensure that they are in a repeatable and version-controlled state.
In the development environment, VirtualBox is the Hypervisor with Vagrant and Puppet providing the provisioning.
Vagrant works by specializing a base image of a VirtualBox.
For each box defined in our Vagrant file, the following parameters are specified:
This base provisioning does not include any of the baseline controls you would expect in a production environment, such as security, access controls, housekeeping, and monitoring.
You must provision these before proceeding beyond your development environment.
You can find these kinds of recipes on Puppet Forge (http://forge.puppetlabs.com/)
Provisioning agents are then invoked to perform the remaining heavy lifting:
The preceding command installs the host's file that gives the resolution of our cluster name:
This updates all the packages in the apt-get cache within the Ubuntu installation.
Vagrant then proceeds to install the JDK and the base provisioning.
The base VM image could contain the entire base provisioning already, thus making this portion of the provisioning unrequired.
However, it is important to understand the process of creating an appropriate base image and also to balance the amount of specialization in the base images you control; otherwise, they will proliferate.
Once you have a base set of virtual machines that are ready for application provisioning, you need to install and configure the appropriate packages on each node.
Create a new project named storm-puppet with the following folder structure:
The entry point into the Puppet execution on the provisioned node is site.pp.
A module exists in the modules folder and has its own manifests and template folder structure, much as with the structure found at the root level of the Puppet project.
The installation of the Storm application is the same on each storm node; only the configurations are adjusted where required, via templating.
Next create the install.pp file, which will download the required binaries and install them:
The install manifest here assumes the existence of package, Debian packages, for Ubuntu.
These were built using scripts and can be tweaked based on your requirements.
The configuration of each node is done through the template-based generation of the configuration files.
All the storm parameters are defined using Hiera, with the Hiera configuration invoked from params.pp in the storm manifests:
Due to the sheer number of properties, the file has been concatenated.
Each class of node is then specified; here we will specify the nimbus class:
And finally, specify the zoo class (for a zookeeper node):
Once all the files have been created, initialize the Git repository and push it to bitbucket.org.
If you would like to ssh into any of the nodes, simply specify the following command:
How it works… There are various patterns that can be applied when using Puppet.
The simplest one is using a distributed model, whereby nodes provision themselves as opposed to a centralized model using Puppet Master.
In the distributed model, updating server configuration simply requires that you update your provisioning manifests and push them to your central Git repository.
The various nodes will then pull and apply this configuration.
This can either be achieved through cron jobs, triggers, or through the use of a Continuous Delivery tool such as Jenkins, Bamboo, or Go.
Provisioning in the development environment is explicitly invoked by Vagrant through the following command:
Puppet is declarative, in that each language element specifies the desired state together with methods for getting there.
This means that, when the system is already in the required state, that particular provisioning step will be skipped, together with the adverse effects of duplicate provisioning.
The storm-puppet project is therefore cloned onto the node and then the manifest is applied locally.
Each node only applies provisioning for itself, based on the hostname specified in the site.pp manifest, for example:
In this case, the nimbus node will include the Hiera configurations for cluster1, and the installation for the nimbus and ui nodes will be performed.
Any combination of classes can be included in the node definition, thus allowing the complete environment to be succinctly defined.
Deriving basic click statistics The click topology is designed to gather basic website-usage statistics, specifically:
The system assumes a limited possible visitor population and prefers server-side client keys as opposed to client-side cookies.
The topology derives the geographic information from the IP address and a public IP resolution service.
The click topology also uses Redis to store click events being sent into the topology, specifically as a persistent queue, and it also leverages Redis in order to persistently recall the previous visitors to the site.
Getting ready Before you proceed, you must install Redis (Version 2.6 or greater):
In the pom.xml file, update the project name and references, and then add the following dependencies to the <dependencies> tag:
Take a special note of the scope definitions of JUnit and JMock so as to not include them in your final deployable JAR.
In the source/main/java folder, create the ClickTopology main class in the package storm.cookbook package.
This class defines the topology and provides the mechanisms to launch the topology into a cluster or in a local mode.
This is followed by the main method, which is guided by the number of arguments passed at runtime:
The topology assumes that the web server pushes messages onto a Redis queue.
You must create a spout to inject these into the Storm cluster as a stream.
In the storm.cookbook package, create the ClickSpout class, which connects to Redis when it is opened by the cluster:
The cluster will then poll the spout for new tuples through the nextTuple method:
Next, we need to create the bolts that will enrich the basic data through the database or remote API lookups.
This bolt will check the client's ID against previous visit records and emit the enriched tuple with a flag set for unique visits.
Create the RepeatVisitBolt class, providing the open and Redis connection logic:
In the execute method, the tuple from the ClickSpout class is provided by the cluster.
The bolt needs to look up the previous visit flags from Redis, based on the fields in the tuple, and emit the enriched tuple:
This bolt will emit an enriched tuple by looking up the country and city of the client's IP address through a remote API call.
The GeographyBolt class delegates the actual call to an injected IP resolver in order to increase the testability of the class.
In the storm.cookbook package, create the GeographyBolt class, extending from the BaseRichBolt interface, and implement the execute method:
Provide a resolver by implementing the resolver, HttpIPResolver, and injecting it into GeographyBolt at design time:
The GeoStatsBolt class simply receives the enriched tuple from GeographicBolt and maintains an in-memory structure of the data.
It also emits the updated counts to any interested party.
The GeoStatsBolt class is designed such that the total population of the countries can be split between many bolts; however, all cities within each country must arrive at the same bolt.
The topology, therefore, splits streams into the bolt on this basis:
Creating the GeoStatsBolt class, provide the implementation for the execute method:
The bulk of logic is contained in the inner-model class that maintains an in-memory model of the city and country:
Finally, the VisitorStatsBolt method provides the final counting functionality for visitors and unique visits, based on the enriched stream from the RepeatVisitBolt class.
This bolt needs to receive all the count information in order to maintain a single in-memory count, which is reflected in the topology definition:
In order to implement the VisitorStatsBolt class, create the class and define two member-level integers, total and uniqueCount; then implement the execute method:
How it works… The following diagram illustrates the click topology:
The spout emits the click events from the web server into the topology, through a shuffle grouping, to both the geography and repeat bolts.
This ensures that the load is evenly distributed around the cluster, especially for these slow or highly latent processes.
It is important to understand the commutative versus associative nature of your data model, together with any other concerns that are in your streams and inherent models before designing your topology.
It is important to understand the parallelism of Storm while setting up the topology structure.
These are the key elements to consider when sizing your cluster.
The cluster will try distributing work to worker processes, each containing many executors that may be executing one or more tasks.
The number of executors per worker is therefore a function of the number of executors over the number of workers.
A good example of this can be seen in the previously mentioned wiki page.
Using these numbers, you can size your cluster in terms of nodes and cores per node, where ideally you should have one core per thread (executor) in the cluster.
Unit testing a bolt Unit testing is an essential part of any delivery; the logic contained in the bolts must also be unit tested.
Getting ready Unit testing often involves a process called mocking that allows you to use dynamically generated fake instances of objects as dependencies in order to ensure that a particular class is tested on a unit basis.
This book illustrates unit testing using JUnit 4 and JMock.
Please take the time to read up on JMock's recipes online at http://jmock.org/cookbook.html.
This class is a simple abstraction of some of the initialization code:
The test case logic of the class is contained in a single execute method:
The base provisioning of the values must be done before the tests using Redis:
It is always useful to leave data in the stack after the test completes in order to review and debug, clearing again only on the next run.
How it works… Firstly, the unit test works by defining a set of test data.
This allows us to test many different cases without unnecessary abstractions or duplication.
Before the tests execute, the static data is populated into the Redis DB, thus allowing the tests to run deterministically.
The test method is then executed once per line of parameterized data; many different cases are verified.
JMock provides mock instances of the collector and the tuples to be emitted by the bolt.
The expected behavior is then defined in terms of these mocked objects and their interactions:
Although these are separate lines of code, within the bounds of the expectations they should be read declaratively.
I expect the getStringField method of the tuple to be called exactly once with the value ip, and the mock object must then return a value to the class being tested.
This mechanism provides a clean way to exercise the bolt.
There are many different kinds of unit tests; often it becomes necessary to test against a DB in such a manner; if you can help it, rather mock out all dependencies of the class and implement a true unit test.
This would be possible with the geography bolt due to the resolver abstraction.
Implementing an integration test Integration testing can mean many different things depending on the situation and audience.
For the purposes of this book, integration testing is a means of testing the topology from end-to-end, with defined input and output points within a local cluster.
This allows for a full-functional verification of the functionality before deploying it to an actual cluster.
Set up a local topology by adding in a testing utility bolt:
Then, define the expected parameters as a set of arrays arranged in pairs:
The test logic can then be based on these parameters:
How it works… The integration test works by creating a local cluster and then injecting input values into the cluster through Redis, in the same way as a real web server would for the given design.
It then adds a specific testing bolt to the end of the topology that receives all the output tuples and tests these against the expected values.
Once the TestBolt value is submitted to the cluster, it is no longer accessible from the test; therefore, the outputs can only be accessed through persistence.
TestBolt persists received tuples to Redis, where the test case can read and validate them.
This is then read by the test and validated against the expected values:
Deploying to the cluster The final step in the development process is to functionally test the topology in a cluster before promoting it to the next environment.
Package your topology using the following command within the project's root:
This will produce a completely packaged JAR in the target folder of the project.
You can deploy this to the cluster using the storm client command:
How it works… The storm command-line client provides you with all the tools you need to control the cluster's functionality.
Part of this is the ability to deploy packaged topologies.
Introduction This chapter will present an implementation recipe for an enterprise log storage and a search and analysis solution based on the Storm processor.
Log data processing isn't necessarily a problem that needs solving again; it is, however, a good analogy.
Stream processing is a key architectural concern in the modern enterprise; however, streams of data are often semi-structured at best.
By presenting an approach to enterprise log processing, this chapter is designed to provide the reader with all the key elements to achieve this level of capability on any kind of data.
Log data is also extremely convenient in an academic setting given its sheer abundance.
A key success factor for any stream processing or analytics effort is a deep understanding of the actual data and sourcing data can often be difficult.
It is, therefore, important that the reader considers how the architectural blueprint could be applied to other forms of data within the enterprise.
The following diagram illustrates all the elements that we will develop in this chapter:
You will learn how to create a log agent that can be distributed across all the nodes in your environment.
You will also learn to collect these log entries centrally using Storm and Redis, and then analyze, index, and count the logs, such that we will be able to search them later and display base statistics for them.
Creating a log agent Modern enterprise architectures consist of a huge number of solutions, each comprising many nodes.
Each node contains an array of applications and services, both at the operating system and Application layers.
These services and applications generate varying volumes of log data.
There is an increasing recognition of the importance of log data within the enterprise community for the following reasons:
In order to leverage valuable log data, it must be sourced from these nodes and delivered securely and easily to a centralized log service for storage, indexing, and analysis.
This recipe demonstrates how to achieve this through an open source log agent called logstash.
There are many good commercial and open source log solutions available.
Then, using your favorite text editor, create a file called shipper.conf containing the following:
After starting a local instance of Redis, you can start this logging agent by issuing the following command:
The preceding configuration file (shipper.conf) configures at least one input and output.
The file input plugin will tail files based on filenames or wildcards in the specified paths.
Many file inputs can be configured, each with a different type.
The log type is important for later processing and categorization.
As we have not configured any filters in this configuration file, the raw log will be passed to the output plugin.
The output plugin is the Redis plugin that will output the log to the Redis instance on localhost to a list called rawLogs.
Creating the log spout The log topology will read all logs through the Redis channel that is fed by logstash; these logs will be emitted into the topology through the spout described in this recipe.
As this is a new topology, we must first create the new topology project.
How to do it… Start by creating the project directory and the standard Maven folder structure (http://maven.apache.org/guides/introduction/introduction-to-thestandard-directory-layout.html)
Import the project into Eclipse after generating the Eclipse project files as follows:
Tuples in the log topology will carry a log domain object that encapsulates the data and parsing logic for a single log record or an entry in a logfile.
The getter, setter, and equals methods have been excluded from this code snippet; however, they must be implemented in order.
Then create the Logspout class that extends the BaseRichSpout interface and implements the same pattern as described in Chapter 1, Setting Up Your Development Environment, declaring a single field as follows:
And then emitting the received log entries into the topology as follows:
Literals should be avoided in the code as far as possible; tuples allow for effective runtime coupling; however, peppering code with field name literals for elements that are effectively coupled prior to runtime doesn't add any value.
How it works… The Redis spout implementation is already familiar; the key logic implemented in this recipe is the parsing logic within the domain object of the LogEntry class.
An array begins with [ (left bracket) and ends with ] (right bracket)
A value can be a string in double quotes; a number; true, false, or null; an object; or an array.
The constructor of the LogEntry object takes JSONObject as the only parameter and initializes its internal values based on the contained values.
The LogEntry object is also able to convert itself into a JSONObject through the toJSON() method, which will become useful later.
Although the structure is well-defined, the date-time format can vary.
The parseDate() method, therefore, provides a best-effort approach to parse the date.
A static list of supported date-time formats is defined as the FORMATS class member variable.
Rule-based analysis of the log stream Any reasonable log management system needs to be able to achieve the following:
These often include log entries at the INFO or DEBUG levels (yes, these exist in production systems)
This recipe integrates the JBoss Library and Drools into a bolt to make these goals easily achievable in a declarative and clear manner.
Drools is an open source implementation of a forward-chaining rules engine that is able to infer new values and execute the logic based on matching logic.
You can find more details on the Drools project at http://www.jboss.org/drools/
As with the LogSpout class, the LogRulesBolt class will emit a single value containing a LogEntry instance.
The initialization of this knowledge session includes only a single set of rules for the syslog logs.
It is recommended that rules management be extracted out into Drools Guvnor, or similar, and rules resources be retrieved via an agent.
This is outside the scope of this book but more details are available at the following link:
In the bolt's execute method, you need to pass the LogEntry object from the tuple into the knowledge session.
The rules resource file should be placed at the root of the classpath; create the file named Syslog.drl in src/main/resources and add this folder to the build path within Eclipse by right-clicking on the folder and going to Build Path | Use as source folder.
How it works… Drools supports two types of knowledge sessions, namely stateful and stateless.
For this use case, a stateless session is all that is required.
Stateful sessions are always to be used with caution as they can lead to performance problems.
There are use cases where this is vital; however, the nature of a forward-chaining rete engine is that it will degrade in performance exponentially as facts are added to its knowledge base.
A knowledge session is used to evaluate facts against a known set of rules.
This is set up within the prepare method of the bolt, with the rules provided at that point.
Within the execution of the bolt, LogEntry is extracted from the tuple and passed into the knowledge session through the following call:
The knowledge session will act as an entry during execution and we can expect it to be potentially different once the call has completed.
Contained within the LogEntry object is a control field called filter.
If a rule sets this to true, the log entry is to be dropped; this is implemented by checking prior to emitting a tuple containing the entry after the rules execution.
Within the rules resource file, there are three rules currently defined.
These rules are implemented for demonstration purposes only and aren't necessarily viable production rules.
The Host Correction rule tries to correct the host name value such that it is fully qualified.
The autonomy of a rule is that, when a matching criterion is met, the result is displayed.
The when clause of this rule will match against the LogEntry instance whose sourceHost field is localhost.
This clause also assigns any matching instance to a local variable l within the scope of this rule.
The functionality specified in the then clause is simply plain old Java, which is added into the current classpath after compilation at runtime.
The Filter By Type rule will set the filter to true for all entries whose type doesn't match syslog.
Firstly, because it includes a salience value, which ensures it is evaluated last.
This ensures that it never extracts fields from filtered logs.
It then uses regular expression matching to extract more fields and structure from the logfile.
While regular expressions are outside the scope of this book, they are widely understood and well documented.
For completeness's sake, here are some more useful examples of expressions for log entries:
Further reading on regular expressions can be found at Wikipedia:
These extra fields can then be added to the fields or tags and used later for analysis or search and grouping.
Drools also includes a module called Drools Fusion that essentially supports Complex Event Processing (CEP)
It is often referred to as an emerging enterprise approach, which may be true, but practically it simply means that the rules engine understands temporal concerns.
Using temporal operators, it can correlate events over time and derive new knowledge or trigger actions.
These temporal operators are supported based on the bolt implementation in this recipe.
Indexing and persisting the log data Log data needs to be stored for some defined period of time in order to be useful; it also needs to be searchable.
In order to achieve this, the recipe integrates with an open source product call Elastic Search, which is a general-use, clustered search engine with a RESTful API (http://www.elasticsearch.org/)
Create a new BaseRichBolt class called IndexerBolt and declare the org.
You must initialize it as follows within the prepare method:
The LogEntry object can then be indexed during the execute method of the bolt:
The unit test of this bolt is not obvious; it is therefore worthwhile to give some explanation here.
How it works… Elastic Search provides a complete client API for Java (given that it is implemented in Java), making integration with it quite trivial.
The prepare method of the bolt will create a cluster node in either the local or clustered mode.
The cluster mode will join an existing cluster based on the name provided with a local storage node being created on the current node; this prevents the double-hop latency of a write over a different transport.
Elastic Search is a large complex system in its own right; it is recommended that you read the provided documentation in order to understand the operational and provisioning concerns.
When Storm is in the debug mode, the Elastic Search node will run an embedded cluster, with many nodes (if requested) being executed within the same JVM.
This is all enabled in the prepare method of the bolt.
When a tuple is received, the LogEntry object is extracted and the JSON contents of LogEntry are sent to Elastic Search.
The ID of the log within the Elastic Search cluster is then extracted from the response and emitted with the LogEntry objects to downstream bolts.
In this particular recipe, we will only use this value for unit testing; however, downstream bolts could easily be added to persist this value against some log statistics that would be extremely useful within a user interface for drilldown purposes.
The unit test for this particular bolt is quite tricky.
This is because, in a typical unit test, we know what the expected outcome is before we run the test.
In this case, we don't know the ID until we have received the response from the Elastic Search cluster.
This makes expressing expectations difficult, especially if we want to validate the log in the search engine.
To achieve this, we make use of a custom matcher for JMock.
The key method in the custom matcher is the matches method.
This method simply ensures that an instance of Values is returned but it also holds onto the value for later evaluation.
This allows us to set the following set of expectations:
And then retrieve the record ID and validate it against the embedded Elastic Search cluster.
If you would like to be able to search the logfiles in the cluster, download and install the excellent log search front engine, Kibana, from kibana.
This recipe has maintained the JSON log structure from logstash and Kibana is designed as the frontend for logstash on Elastic Search; it will work seamlessly with this recipe.
It also uses the Twitter Bootstrap GUI framework, meaning that you can integrate it with the analytics dashboard quite easily.
Create a new BaseRichBolt class called VolumeCountingBolt in the storm.
Then implement a static utility method to derive the minute representation of the log's time:
Finally, create the LogTopology class as per the pattern presented in Chapter 1, Setting Up Your Development Environment, and create the topology as follows:
How it works… This implementation looks surprisingly simple, and it is.
It makes use of the stormcassandra project (a type of storm-contrib) to abstract all the persistence complexity away.
Cassandra's column family data model offers the convenience of column indexes with the performance of log-structured updates, strong support for materialized view, and powerful built-in caching.
A recent addition to the Cassandra functionality is that of counter columns, which are essentially persistent columns, within a given column family, that can be incremented safely from anywhere in the cluster.
The storm-cassandra project provides two styles of persistence, firstly for standard Cassandra column families and secondly for counter-based columns.
We will focus on the second type, as it is appropriate for our use case; you can read about the other style on the project's README file but it is essentially the same.
It expects to be told which column family to use, which tuple field to use for the row key and which tuple field to use for the increment amount.
It will then increment columns by that amount based on the remaining fields in the tuple.
This will increment the SomeCounter counter column by 1L in the columnFamily column family.
A key mind shift for any developer with a relational database background is data modeling in a column family database.
Column family databases, as part of the big data family of databases, promote the use of highly denormalized data models.
This approach removes table relationships and their locking concerns and enables massive-scale parallel read and write processing on the database.
This promotes data duplication; however, given the cost of commodity disks, this is seen as a small sacrifice in order to meet the scaling objectives of today.
The mind shift is to think of data models in terms of the queries that we will perform on the dataset, rather than modeling real-world entities into concise normalized structures.
The query we are trying to answer with this data model is essentially this: select all total count for all logfiles for a given point in time.
This approach allows us to easily derive this data by emitting a tuple to count that total; we can easily emit a tuple to answer any other question, and examples could include the following:
Column families can contain more than counts, design any denormalized structure, and emit a tuple to represent a set of columns for that row; if the row already exists, the columns will simply be added or updated.
Integration testing is obviously a vital task in the delivery process.
Unit integration testing involves integration testing a topology, typically as part of the continuous integration build cycle, and should be seen as complementary to the necessary functional style of integration testing of a deployed cluster.
The integration test presented here is essentially the same as that of the integration test presented in Chapter 1, Setting Up Your Development Environment; however, it is sufficiently complex to warrant an explanation here.
How to do it… Start by creating the unit test.
Add a setup method that should be invoked before the class:
Then create each of the associated setup methods; first set up an embedded version of Cassandra:
Then set up a local, embedded instance of Elastic Search:
This will set up the fixtures we require in order to test our topology; we also need to shut these down gracefully at the end of the test, so add the AfterClass method for the test suite:
How it works… This test case works by creating embedded instances of the required clusters for this topology, namely Cassandra and Elastic Search.
As with the previous integration test, it then injects test data into the input channel and allows the log entry to flow through the topology, after which it validates the entry in the search engine and validates that the counter has incremented appropriately.
This test will take longer to run than a standard unit test and therefore should not be included in your standard Maven build.
The test should, however, be used as part of your local development workflow and validated further on a continuous integration server.
Creating a log analytics dashboard The log analytics dashboard is a web application that presents aggregated data to the user, typically in a graphical manner.
For achieving this, we must take cognizance of the following user interface design principles:
The dashboard in this recipe will present a single dynamic graph of the log volume by minute per logfile.
This will generate a standard project structure and Maven POM file for you.
Open the pom.xml file and remove the default dependencies, replacing them with the following dependencies:
Then add the following build plugins to the build section of the POM:
Then import the project into Eclipse using the mvn eclipse:eclipse command and the Eclipse project import process.
The excellent Twitter Bootstrap GUI library will be used in the creation of the user interface.
Start by downloading this into a separate location on your drive and expanding it.
The bootstrap gives us a rapid start by providing many practical examples; we will simply copy one and adapt it:
The Twitter Bootstrap is really quite an excellent departure point for any web-based GUI; it is highly recommended that you read the self-contained documentation in the downloaded package.
While there is much HTML to update, we will focus on the important elements: the central content and graph.
Next, we include these into the HTML file and write the client-side JavaScript to retrieve the data and update the graph.
Add the following script includes at the bottom of the HTML file, after the other <script> tags:
Then add our custom JavaScript into a <script></script> tag below the other script imports, towards the bottom of the file:
And then add the code to fetch the data from the server:
In order to expose the data to the client layer, we need to expose services to retrieve the data.
Start by creating a utility class called CassandraUtils in the storm.cookbook.
Finally, you expose the service by creating the LogServices class:
You can now run your project using the following command from the root of your weblog project:
How it works… At a high level, the dashboard works by periodically querying the server for counts for a given time.
It maintains an in-memory structure on the client side to hold the results of these queries and then feeds the consolidated two-dimensional array into the graph class.
Take a look at the HTML; the following code defines where the graph will be displayed:
The in-memory structure is essentially simply a two-dimensional array of values and so it is important to map these onto the x and y axes on the graph, which is done through the following:
Data is fetched through the fetch() method, which issues an Ajax asynchronus request to the server.
Finally, the getdata() method maps the log structure into a two-dimension array to be displayed by the graph.
On the server side, the service is exposed via Jersey.
It is the open source, productionquality, JSR 311 Reference Implementation for building RESTful web services.
For this recipe, only the single service is defined by the following annotations to the LogCount class:
The value passed into the timestamp variable will be used in performing the lookup against Cassandra.
The results of the query are then mapped onto a JSON object and returned to the caller:
It is usually quite difficult to bring up the entire topology and set of clusters in order to simply test the web application; a convenient main class is provided in the supporting material that populates the column family with random data, allowing for easy testing of the web application in isolation.
Introduction This chapter will present the implementation of a very well-known data processing algorithm, Term Frequency–Inverse Document Frequency (TF-IDF), using Storm's Trident API.
TF-IDF is a numerical statistic that reflects how important a word is to a document within a collection of documents.
This is often a key concern in search engines but is also an important starting point in sentiment mining, as the trend of the important words within textual content can be an extremely useful predictor or an analytical tool.
If you want the details of how it is used in this context, please read the documentation for the Similarity class in Apache Lucence at http://lucene.
If you're familiar with high-level batch processing tools such as Pig or Cascading, the concepts of Trident will be very familiar: Trident has joins, aggregations, grouping, functions, and filters.
In addition to these, Trident adds primitives for doing stateful, incremental processing on top of any database or persistence store.
Trident has consistent, exactly-once semantics; so it is easy to reason about Trident topologies.
Within the Big Data architecture, such as Lambda, Trident then becomes a key component providing the real-time portion of the data stream, which is then augmented with a historical batch of data to form a complete dataset.
We will also see how DRPC easily enables such architecture in a later chapter.
For some background on the Lambda architecture, please see this blog post at the following link:
Creating a URL stream using a Twitter filter There are many approaches to sourcing input documents for the TF-IDF implementation.
Twitter provides a stream API that allows you to receive a sample of the total tweets within Twitter.
The approach of using a sample is more than sufficient for most applications, as more data may not improve your results, especially in any meaningful way relative to the costs involved.
For this reason, this is the only way Twitter allows you to consume the data without special agreements in place.
Tweet status streams can be filtered using the Twitter streaming API, so that only a subset of the population is sampled and delivered in a stream.
This enables one to listen for tweets for a particular topic.
Furthermore, tweets often have links attached to them, which is where the bulk of the information is held given the small character limit on the tweet itself.
The approach for this recipe is therefore to subscribe to a Twitter stream using a filter and then extract the URLs contained within the tweets and emit them into the topology.
These links will later be used to download the content of the documents to which they refer and calculate the TF-IDF value for each term within the document content.
Import the project into Eclipse after generating the Eclipse project files:
Create a new spout called TwitterSpout that extends from BaseRichSpout, and add the following member-level variables:
In the open method of the spout, initialize the blocking queue and create a Twitter stream listener:
You then need to emit the tweet into the topology.
Next, you must create a bolt to publish the tuple persistently to another topology within the same cluster.
Create a BaseRichBolt class called PublishURLBolt that doesn't declare any fields, and provide the following execute method:
Finally, you will need to read the URL into a stream in the Trident topology.
How it works… The cluster will execute two separate topologies.
The first topology will simply receive data from the Twitter API and publish it to a queue; the other topology will be the actual Trident topology that does all the heavy lifting.
This separation by persistent queue is not only important from a best-practice perspective, but it also ensures that the production and consumption of data are completely decoupled.
These are important reasons from the maintenance and stability perspective; the direct functional reason for the separation is the ability to implement exactly-once semantics that require a transactional spout, which we will cover in a later chapter.
An event listener must be provided to it in order to handle newly received tweets from the API.
Because the listening process is executed in a separate thread, the listener posts them to a thread-safe queue:
The tweet is then removed from the queue as part of the nextTuple() method of the spout and emitted into the topology:
The creation of the Twitter filter is the key for this approach.
We are trying to discover the most important terms for a given topic; therefore, we need to set up an appropriate filter so that we only receive relevant tweets, as far as possible.
The Twitter API documents the filter usage in great detail and it is important to understand this in order to construct an appropriate filter; this is available at the following link:
The Twitter API is exposed via Twitter4J in the following calls within the open method:
There are lock-free mechanisms to achieve the coordination between these threads.
There's more… Testing the topology against an active Twitter stream is quite difficult because of the rate of unknown data being used as part of debugging and testing processes.
It is therefore important to have a testing spout to inject URLs into the Trident topology.
The Trident API ships with a testing utility to achieve this.
Simply provide this spout to the Trident stream as an input parameter as opposed to TweetURLSpout:
This will essentially inject a known set of URLs into the topology in a cycle, giving us a stable, predictable testing environment.
This can be removed when we deploy to an actual cluster.
Finally, while you can use your username and password to access Twitter, this isn't advised.
Once you have done this, create a properties file within your resources folder with the following content:
This recipe consumes the URL stream, downloading the document content and deriving a clean stream of terms that are suitable for later analysis.
A lemma is the canonical form of a word; for example, run, runs, ran, and running are forms of the same lexeme with "run" as the lemma.
Lexeme, in this context, refers to the set of all the forms that have the same meaning, and lemma refers to the particular form that is chosen by convention to represent the lexeme.
The lemma is important for this recipe because it enables us to group terms that have the same meaning.
Where their frequency of occurrence is important, this grouping is important.
A lemma is similar to a stem; however, a stem is often not a valid dictionary word because it is derived algorithmically.
Therefore, a lemma is preferred for this recipe, given that the imperative is "understanding" and not "searching" the order.
How to do it… First we need to fetch the document content based on the URL:
Next we need to tokenize the document, create another class that extends from BaseFunction and call it DocumentTokenizer.
We then need to filter out all the invalid terms that may be emitted by this function.
To do this, we need to implement another class that extends BaseFunction called TermFilter.
The execute method of this function will simply call a checking function to optionally emit the received tuple.
The dictionary needs to be initialized during the prepare method for this function:
Finally, you need to create the actual topology, or at least partially for the moment.
Create a class named TermTopology that provides a main(String[] args) method and creates a local mode cluster:
How it works… Trident's API provides for various types of operations.
Functions are partition-local in nature, meaning that they are applied to each batch independently.
A function takes in a set of input fields and emits zero or more tuples as output.
The fields of the output tuple are appended to the original input tuple in the stream.
If a function emits no tuples, the original input tuple is filtered out.
Otherwise, the input tuple is duplicated for each output tuple.
The document fetch function takes a URL as input, fetches the document, and emits the document content, meaning that the resulting tuple will contain both the document content and the URL.
If there are any problems fetching the document, it won't emit anything, thus acting as a natural filter.
The fetch function makes use of the Apache Tika library (http://tika.apache.org/) to fetch and extract the contents of the document:
And it then filters based on MIME type to ensure we only get the document types that we are looking for:
The tokenize function does a little bit more than purely tokenizing the document.
It makes use of the Apache Lucene's analyzer library (http://lucene.apache.org/) to tokenize and filter out the stop words as it proceeds further:
The document filter doesn't simply emit tuples that don't meet a set of criteria; among them is a dictionary lookup.
Importantly, the Trident API starts by defining a stream on the topology and linking in the document fetch function through the each function:
It then defines a term stream that tokenizes the documents on the document stream, filters the terms, and projects out the required fields:
The projection essentially drops the fields from the tuples that aren't listed.
This is important because we don't want to send the document content redundantly and unnecessarily around the network.
The true power of Trident is demonstrated in this recipe, with many of the abstractions used in order to calculate the TF-IDF value.
Before the recipe is presented, it is important to understand the simple math behind TF-IDF.
We will need the following components to calculate the TF-IDF:
There are many ways to calculate the term frequency; for this recipe, we will use the raw frequency, that is, the number of times a term appears in the document.
Add the following API calls to the topology definition in the TermTopology class:
And then implement the TfidfExpression class by extending the BaseFunction interface and providing the execute method:
How it works… It is important at this point to understand the flow of data across the entire topology, as illustrated by the following diagram:
The initial data flow is that of a single stream; however, after the terms have been filtered, it branches off into three separate flows.
Both D and df(t) are functions that are calculated across batches and over time, they are in a sense long-living concerns, while tf(t,d) is only applicable for the given batch.
The value for D is derived by grouping documents by some static property.
In this case, the concept of source was introduced and, for the purposes of this recipe, will only have the value twitter.
This property is important so that we can use a grouping around the state, which will allow us to easily re-use the map state that ships with Trident without having to define some custom state implementation.
These API calls will effectively group around the static property and count all the documents:
The same logic is applied to df(t), except that we group around the term.
Finally, we group and aggregate around the term and document to get the term frequency, which is a nonpersistent aggregate.
The terms documentID and frequency are then passed to the expression that does a lookup of the other values using a DRPC query:
The queries are defined within the topology and simply executed, as shown in the preceding code snippet:
Each query takes a set of arguments and passes them as a key to the MapGet query function.
You can then apply this schema by executing the following command:
Introduction This chapter builds on the concepts introduced in Chapter 3, Calculating Term Importance with Trident, by providing the next steps towards a fully fledged enterprise-ready TF-IDF implementation.
In this chapter we will investigate the usage of distributed remote procedure calls (DRPC) to complete a portion of the required processing in order to give us a point-intime view of the TF-IDF value for a given term and document, as Chapter 3, Calculating Term Importance with Trident, focused on a purely streamed delivery of the TF-IDF values.
We will then explore the usefulness of DRPC in terms of integration testing of a Trident topology.
Trident has many useful stateful abstractions that we will need to properly exercise in our integration testing.
Finally we will implement a rolling window addition to the TF-IDF algorithm, which will allow us to position our topology as a speed layer for a larger Lambda architecture.
A theoretical basis for DRPC and Trident API is required for this chapter; please take some time to read the Storm project documentation on these topics at the following links:
Lambda architecture is a term coined by Nathan Marz; it was first introduced in his blog entry on How to beat the CAP theorem.
It provides an elegant combination of both batch and real-time data systems that are scalable and truly fault-tolerant.
A full understanding of the theoretical basis and approach is recommended; it is available at the following link:
A classic design consideration within data systems is choosing an appropriate balance between precomputation and on-the-fly computation.
Either because the amount of potential data is far too large in practical terms, or because the final result is dependent on a point-in-time perspective of the data that is not possible to precompute.
In the previous chapter, we emitted a constant stream of TF-IDF values based on the documents received from Twitter and the Internet.
The TF-IDF value is perfectly correct at the time when it is emitted; however, as time passes the value that was emitted is potentially invalidated because it is coupled to a global state that is affected by new tuples that arrive after the value was computed.
In some applications this is the desired result; however, in other applications we need to know what the current value is at this point in time, not at some previous point in time.
In this case, we need to compute as much state as is possible as a part of normal stream processing, and defer the remaining computation until the time of the query.
This is a use case for which DRPC is ideally suited.
Trident provides a rich set of abstractions for querying sources of state and processing the resulting tuples using the same power that is inherent in any stream processing.
Our ability to defer portions of the processing to a later time enables us to deal with use cases where state is only valid in the context of "now"
Create a new branch of your source using the following command:
Once this is complete, edit the TermTopology class, and add the following method:
Then update your buildTopology method by removing the final stream definition and adding the DRPC creation:
How it works… At a high level, all we are doing as part of the stream processing is persisting computed values for d, df(term), and tf(document,term), but we don't calculate the final TF-IDF value.
We defer this calculation until the time the value is requested.
It is important to note the GroupBy definitions for each case.
The d value is grouped by a static value for the source of the stream, which gives us a global count across batch boundaries.
The df value is grouped by the term, which will effectively give us a count of the number of documents that contain the term, again across batch boundaries.
Finally, the tf value is stored by the document and term that gives us a count of the term on a per document basis.
With these elements calculated, we can defer the calculation to a later point in time.
The first function splits out the arguments that will be passed to the DRPC call from a client.
The arguments, doc01 and area, should then be placed into a single tuple in the fields documentId and term.
In order to achieve this, we can't simply apply a split function to the arguments, as this would generate many tuples.
This function splits the input text based on " " (space) and then projects the values out to consecutive fields within the same tuple.
Next we use a state query to add the value for tf to the tuple, based on values that were passed as arguments:
After this query is complete, the tuple would contain three fields: documentId, term, and tf, where tf is the value that we computed earlier.
After this query is complete, the tuple will also contain a field for df.
Finally we look up the value for d and add it to the tuple:
Then we pass all the fields to the expression function that we created in Chapter 3, Calculating Term Importance with Trident, and project just the final fields:
Because we defined the stream using newDRPCStream, the output of the stream will be returned to the calling DRPC client.
If you would like to test this quickly, update your main method to periodically call the DRPC query you have just created.
Integration testing of a Trident topology In previous chapters, we have implemented integration tests by hooking into the defined topology and providing testing bolts that allow us to exercise the topology as a black box.
While this is possible with Trident, it becomes increasingly less elegant, especially in light of the fact that there are rich testing APIs based on Clojure.
In this recipe, we will convert our pure Java project into a Polyglot project in which the Java and Clojure code coexist comfortably.
We will then implement a full integration test of the TFIDF topology using the Clojure testing API.
It is assumed that the reader is familiar with Clojure and functional programming techniques.
If this is not the case, please refer to any of the excellent online resources; however, it must be noted that one doesn't have to be proficient at Clojure in order to implement the integration tests.
Given their simplicity and elegance, the task of integration testing is possible with very little Clojure exposure.
Once this is complete you can delete your Maven POM file.
Next create a new file called project.clj in the root of the project:
Next, you will need to refactor your folder structure to make room for the Clojure source files:
In order to enable your normal development workflow, you need to install lein and the Eclipse plugin.
In order to import the lein project into Eclipse, you need to follow this procedure:
Browse to the location of your project and select Finish.
Right-click on the project in the project explorer and then select Configure | Convert to leiningen.
Your project should now be fully available and working in Eclipse.
If you want to change your project dependencies, simply right-click on your project in Eclipse and go to leiningen | Reset project configuration.
Next you need to add some more DRPC queries to the topology to enable our testing.
And update buildTopology to include these methods at the appropriate time:
How it works… Leiningen is for automating Clojure projects without setting your hair on fire.
In fact, lein (short name) deploys to maven repositories and consumes dependencies from maven repositories.
In designing the integration test, the properties that we would like to assert are the values for d, df, and tf-idf.
We have provided DRPC queries for all of the values and therefore the integration test will treat the topology as a black box, injecting values into a spout and verifying the results using DRPC queries.
Let's explore the integration test in detail to understand its functionality.
Everything in Clojure is a list, typically following a form where the function name is the first entry in the list.
Because functions are first-class citizens in Clojure, this is also true for defining a function (achieved by calling a function)
This function is called before the tests are run in order to clear out the Cassandra database so that we can start the test with a known state.
Next, we need to declare our test, start a cluster, and initialize the DRPC server to test the topology:
As you can see, the macro makes it simple to create a cluster and then execute your functionality using it; your functionality is the body passed as an argument.
Next we need to create the topology and a way to inject tuples into the topology:
Clojure's Java interop defines various syntactical idioms for calling Java constructs.
Next we can feed tuples into the topology and then verify the results:
There's more… To run the tests, you can either use the command-line REPL that can be launched using lein:
Or you can launch the REPL in Eclipse, using the Ctrl + Alt + S shortcut key.
Once the REPL has been launched and is in the correct namespace (this happens automatically when using Eclipse shortcut), simply call the function:
When you change any Java code you will need to restart the REPL to force it to pick them up.
The command-line REPL requires that you first compile the Java source using lein javac.
Implementing a rolling window topology In many temporal applications, it is important to be able answer the question of "What happened in the last X amount of time?" This is commonly referred to as a sliding window:
However, while working with Storm as the Speed layer of the Lambda architecture, it is required that we implement a rolling time window whereby we can segment time in a fixed manner.
These fixed-time boundaries allow us to easily merge the Batch and Speed layers and provide a complete and seamless answer.
In Chapter 6, Integrating Storm and Hadoop, we will explore combining the results of the rolling windows with precomputed data views from the batch layer using DRPC.
It must also be noted that a favorable property of the topology is that rolled windows are not immediately overwritten or discarded.
This allows for a more robust implementation whereby batch processing failures can be tolerated within the speed layer.
The time boundary would not be required if you delayed more processing until a point in time and performed the processing using DRPC only.
This would mean all aggregations are created at that point in time and then discarded, which is typically not practical.
In order to implement the rolling time window, we will need to use a fork of this state implementation.
Start by cloning, building, and installing it into our local Maven repo:
Then update your project dependencies to include this new version by changing the following code line:
Finally, replace the getStateFactory method in TermTopology with the following:
How it works… The standard implementation of the map state through the trident-cassandra state implementation assumes a set rowKey per row in your column family.
There is therefore a relationship between the map state at the Trident level and a single row within the column family at a Cassandra level.
The fork of the state implementation adds a bucket capability.
Essentially this means that you can dynamically specify the row key that will be used for the state at runtime, using a row key strategy that you provide.
Logically this gives us the ability to partition our state without introducing an extra layer of complexity in our topology logic.
The strategy implementation in this case will logically partition our state based on hours, by returning a concatenated combination of both the static row key and a representation of the current hour.
Therefore, at the Trident level, all state updates are partitioned by this hour.
So, as the value for d is incremented, it is only incremented for this hour; the same is true of the values for df.
It can easily be changed to using daily slots by introducing a formatDay method based on the current time.
This functionality enables the goal of keeping the values for previous time windows; however, this also implies that separate a housekeeping functionality must be put in place to clean out windows that are no longer required because they have been taken into account in the Batch layer.
Simulating time in integration testing A vital property of any automated test is that it is consistent and repeatable.
In other words, it must either fail or succeed consistently.
This can make testing temporal applications difficult because of the fact that time is always different between test runs.
In order to create stable tests, it is important to simulate the time within the cluster so that the integration tests can advance the time by specific amounts and explicitly test all the boundary conditions.
The trident testing API fully enables such a test scenario; this recipe will explore the minor changes required to be made to the topology under test and the actual integration tests.
How it works… Storm ships with some utility classes to enable the simulated time within the cluster.
Within your integration test, we are using the local cluster with the simulated time instead of the actual time.
In order to test the time bucket, we will perform the following steps:
Start at time 0 and inject a series of tuples.
Verify that the resulting values for d, df, and tf-idf are not related in any way.
However, with the bucket functionality in place, the value for d should be 5 in both cases due to the state being entirely partitioned at the Cassandra level.
Introduction We break away briefly from the TF-IDF thread to explore the polyglot capabilities of Storm.
Polyglot development is becoming increasingly important, particularly in the open source world where mashups present a rapid path to delivery, regardless of the underlying technology.
There is an increasing number of JVM-based languages that maintain binarylevel compatibility with Java, such as Scala.
In these cases, the Polyglot project is simply a composition of JAR files with appropriate levels of modularity.
In the cases where the underlying execution environment isn't common, other approaches are required.
There are many use cases, over and above convenient re-use, such as high-performance computing, where native implementations provide for greater levels of optimization or leverage of lowlevel hardware capabilities, such as the rich functionality of GPUs, in most modern PCs.
There are many approaches to integrating systems developed in incompatible languages, including messaging, sockets, and Apache Thrift.
As a real-time system, Storm is shipped with a very lean multi-language protocol, with implementations for Ruby and Python.
The example used is the canonical word count topology where the topology will be defined in Clojure and the bolts will be implemented in C++ and Ruby.
Implementing the multilang protocol in Qt The multilang protocol is extremely trivial in nature.
All exchanges are delimited via a single line containing the word end, which isn't JSON-encoded.
In this recipe, we will only implement the bolt adaptor; however, adding the spout functionality is trivial from this base.
The bolt can receive new tuple messages that contain an ID, some component metadata, and the actual tuple as a JSON value containing a JSON array of values.
The bolt then sends ack, fail, emit, or log messages back via STDOUT.
Qt is an open source C++ framework, originally developed by Trolltech as a cross-platform GUI framework.
Qt has enjoyed impressive longevity in the open source communities, providing a wide range of cross-platform C++ capability.
This functionality extends far beyond GUI concerns into base container classes and Threads; it even augments the C++ language with some missing elements, such as reflection and reference counting-based memory management.
It is, therefore, a convenient framework to partner with Java where required as it supports all the operating systems that Java does.
It must be noted that C++ doesn't automatically equate to better-performing code.
The JVM's JIT compiler does an impressive job and as a result Java code will outperform the C++ code that is written in an "average" manner in many cases.
The XML DOM parsers were a good example of this for many years where the C++-implemented code had simply been ported from Java.
Getting ready In order to get ready for this recipe, we simply need to install the Qt SDK; you can easily do this using apt-get.
How to do it… We are going to implement the logic to comply with the Storm multilang protocol using Qt.
In order to do this, we will start by creating a new Qt project.
Create a folder for the C++ project and create a Qt Project file.
The .pro file is the Qt makefile or the project file.
You must then create a tuple header file named qtuple.h.
Note that the code does not use C++ 11 features, such as delegate constructors; this would be an obvious improvement if you have already chosen to upgrade.
Next, you must define the QBasicBolt class starting with the header file definition.
Please note that the readLine and sendMsgToParent methods have both been marked as virtual.
This was done to make this class more testable where a unit test could mock out these functions and perform an in-process black box test of the class.
These methods represent the input and output points of the class.
How it works… This class provides the implementation of the multilang protocol.
It abstracts this complexity away from the implementing subclass, thereby allowing it to simply implement the executable method with the appropriate bolt logic.
A compiled executable will be invoked by the ShellBolt class that ships with Storm.
The ShellBolt class will act as a parent for the duration of the execution, exchanging commands and tuples with the concrete instance of the QBasicBolt class.
The ShellBolt class essentially delegates the logic to this externally provided bolt.
The details of ShellBolt are presented in the next recipe; for the moment, it is sufficient to understand that a Java class will start the process and exchange messages via STDIN and STDOUT with the concrete instance of QBasicBolt.
The bolt starts by initializing and placing a PID file, which will be used for process management.
It then enters into a permanent loop of consuming tuples and generating responses.
Note that the process method is defined as a pure virtual function and must be provided by the concrete instance.
The class also provides convenience methods to be able to emit tuples, logs, and failures.
Care must be taken to generate the appropriate JSON elements of the tuple by the concrete instance in accordance with the specification on wiki; however, if inappropriate messages are generated, the topology will fail rapidly as it should.
Implementing the SplitSentence bolt in Qt The SplitSentence bolt is embarrassingly simple, which is important for canonical examples.
Remember that the point is to focus on the creation of the multilang adaptor.
The bolt simply reads the sentence from the tuple, tokenizes the sentence based on spaces, and emits each word.
This is done by completing the implementation of the main method in the main.cpp file.
Then, update the .pro file to reflect the following changes:
Finally, the build process involves generating a platform-specific makefile and then building the executable.
How it works… The bolt simply extracts the sentence from the tuple, splits it, and then emits those words as separate tuples.
There are a few things to note for those who are familiar with the Java API.
Firstly, the values are passed into the process method as a JSON value within the method.
Secondly, at this level, we don't have any idea of the field names of the fields; therefore, you need to access each value in the input tuple based on the value index within the array.
Finally, the value that you emit must also be a JSON array, otherwise the bolt's parent will fail to process the command.
For those with a Java background, the code will be legible.
A few points to consider while making changes and experimenting are as follows:
This allows the implementation to be included in many places and is still correctly addressed with the help of the details in the header.
You can provide implementations inside the header file; however, you must remember that the implementation will be included in every library that it is included in, resulting in potential conflicts and bloated footprints.
Prefer values on the stack, pass by reference, and, when allocating onto the stack, ensure that you have a delete instruction to free the memory or that it is appropriately parented.
There's more… In order to expose the Qt-based bolt to the topology, we need to provide the Java-based parent bolt.
The parent simply declares the fields and defines the command to be run.
In a production situation, you would need to ensure the Storm nodes are appropriately provisioned with Qt binaries; otherwise, the compiled executable will fail to run when the topology is deployed onto the cluster.
The provisioning scripts provided in Chapter 1, Setting Up your Development Environment, would be a good starting point for this.
Implementing the count bolt in Ruby The Ruby bolt is also very simple given that Storm ships with an implementation of the multilang adaptor for Ruby.
How it works… The Ruby bolt extends the Storm bolt and implements the process method.
A member-level hash map holds the counts for each word.
This map is incremented and the new total is emitted as each word is received.
Take note of the number of parameters passed to the parent constructor in the parent bolt.
As with the Qt example, you must ensure that the appropriate version of Ruby is provisioned on your nodes or this will fail.
Defining the word count topology in Clojure To close off, we need to define the topology using Clojure.
Remember that the point of this topology is to drive home the polyglot nature of Storm.
You can deliver multi-technology realtime topologies and you must select the appropriate method.
The bolts described earlier in this chapter used the minimal multilang protocol.
There are various other ways, including Thrift, the Clojure's Java interop, and, in the case of Qt, you could have easily used the Qt Jambi project.
The selection of the appropriate method depends on many factors within your environment.
How it works… We define a spout for testing purposes that emits the sentences:
The body of the function adds elements to the topology, starting with the spout, and then adds the two bolts, which we have defined earlier in the chapter.
Note that we simply create instances of the bolts that are the Java parents of the underlying bolt implementation, using the Clojure's Java interop notation for creating a new instance of the class RubyCount.
Once the REPL has launched, execute the run-local! function and the topology will launch and execute.
You can use this command to package the deployable JAR file for you.
In this chapter, we will implement the Batch and Service layers to complete the architecture.
There are some key concepts underlying this big data architecture:
Immutable state is the key, in that it provides true fault-tolerance for the architecture.
If a failure is experienced at any level, we can always rebuild the data from the original immutable data.
This is in contrast to many existing data systems, where the paradigm is to act on mutable data.
This approach may seem simple and logical; however, it exposes the system to a particular kind of risk in which the state is lost or corrupted.
It also constrains the system, in that you can only work with the current view of the data; it isn't possible to derive new views of the data.
When the architecture is based on a fundamentally immutable state, it becomes both flexible and fault-tolerant.
Abstractions allow us to remove complexity in some cases, and in others they can introduce complexity.
It is important to achieve an appropriate set of abstractions that increase our productivity and remove complexity, but at an appropriate cost.
It must be noted that all abstractions leak, meaning that when failures occur at a lower abstraction, they will affect the higher-level abstractions.
It is therefore often important to be able to make changes within the various layers and understand more than one layer of abstraction.
The designs we choose to implement our abstractions must therefore not prevent us from reasoning about or working at the lower levels of abstraction when required.
Open source projects are often good at this, because of the obvious access to the code of the lower level abstractions, but even with source code available, it is easy to convolute the abstraction to the extent that it becomes a risk.
In a big data solution, we have to work at higher levels of abstraction in order to be productive and deal with the massive complexity, so we need to choose our abstractions carefully.
In the case of Storm, Trident represents an appropriate abstraction for dealing with the data-processing complexity, but the lower level Storm API on which Trident is based isn't hidden from us.
We are therefore able to easily reason about Trident based on an understanding of lower-level abstractions within Storm.
Another key issue to consider when dealing with complexity and productivity is composition.
Composition within a given layer of abstraction allows us to quickly build out a solution that is well tested and easy to reason about.
Complexity always equates to risk and cost in the long run, both from a development perspective and from an operational perspective.
Real-time solutions will always be more complex than batch-based systems; they also lack some of the qualities we require in terms of performance.
Nathan Marz's Lambda architecture attempts to address this by combining the qualities of each type of system to constrain complexity and deliver a truly fault-tolerant architecture.
We divided this flow into preprocessing and "at time" phases, using streams and DRPC streams respectively.
We also introduced time windows that allowed us to segment the preprocessed data.
In this chapter, we complete the entire architecture by implementing the Batch and Service layers.
The Service layer is simply a store of a view of the data.
In this case, we will store this view in Cassandra, as it is a convenient place to access the state alongside Trident's state.
The preprocessed view is identical to the preprocessed view created by Trident, counted elements of the TF-IDF formula (D, DF, and TF), but in the batch case, the dataset is much larger, as it includes the entire history.
The Batch layer is implemented in Hadoop using MapReduce to calculate the preprocessed view of the data.
MapReduce is extremely powerful, but like the lower-level Storm API, is potentially too low-level for the problem at hand for the following reasons:
We would like to think of a data pipeline in terms of streams of data, tuples within the stream and predicates acting on those tuples.
This allows us to easily describe a solution to a data processing problem, but it also promotes composability, in that predicates are fundamentally composable, but pipelines themselves can also be composed to form larger, more complex pipelines.
Cascading provides such an abstraction for MapReduce in the same way as Trident does for Storm.
With these tools, approaches, and considerations in place, we can now complete our realtime big data architecture.
The following figure illustrates the final architecture, where the elements in light grey will be updated from the existing recipe, and the elements in dark grey will be added in this chapter:
Implementing TF-IDF in Hadoop TF-IDF is a well-known problem in the MapReduce communities; it is well-documented and implemented, and it is interesting in that it is sufficiently complex to be useful and instructive at the same time.
Cascading has a series of tutorials on TF-IDF at http://www.cascading.
For this recipe, we shall use a Clojure Domain Specific Language (DSL) called Cascalog that is implemented on top of Cascading.
Cascalog has been chosen because it provides a set of abstractions that are very semantically similar to the Trident API and are very terse while still remaining very readable and easy to understand.
Getting ready Before you begin, please ensure that you have installed Hadoop by following the instructions at http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntulinux-single-node-cluster/
Next, you need to edit the project.clj file to include the dependencies:
It is always a good idea to validate your dependencies; to do this, execute lein deps and review any errors.
It is also good practice to understand your dependency tree.
This is important to not only prevent duplicate classpath issues, but also to understand what licenses you are subject to.
To do this, simply run lein pom, followed by mvn dependency:tree.
In this particular case, you will notice that there are two conflicting versions of Avro.
We then need to create the Clojure-based Cascade queries that will process the document data.
We first need to create the query that will create the "D" view of the data; that is, the D portion of the TF-IDF function.
This is achieved by defining a Cascalog function that will output a key and a value, which is composed of a set of predicates:
You can define this and any of the following functions in the REPL, or add them to core.clj in your project.
If you want to use the REPL, simply use lein repl from within the project folder.
The required namespace (the use statement), require, and import definitions can be found in the source code bundle.
We then need to add similar functions to calculate the TF and DF values:
This Batch layer is only interested in calculating views for all the data leading up to, but not including, the current hour.
This is because the data for the current hour will be provided by Trident when it merges this batch view with the view it has calculated.
In order to achieve this, we need to filter out all the records that are within the current hour.
Each of the preceding query definitions require a clean stream of words.
In order to filter these and emit a clean set of words for these queries, we can compose a function that splits the text into words and filters them based on a list of stop words and the time function defined previously:
We will be storing the outputs from our queries to Cassandra, which requires us to define a set of taps for these views:
The way this schema is created means that it will use a static row key and persist name-value pairs from the tuples as column:value within that row.
This is congruent with the approach used by the Trident Cassandra adaptor.
This is a convenient approach, as it will make our lives easier later.
We can complete the implementation by a providing a function that ties everything together and executes the queries:
Next, we need to get some data to test with.
Simply download the project and copy the contents of src/data to the data folder in your project structure.
To do this, we need to insert the data into Hadoop:
Before that, however, a revision of Cascading pipelines is required.
Pipe assemblies define what work should be done against tuple streams, which are read from tap sources and written to tap sinks.
The work performed on the data stream may include actions such as filtering, transforming, organizing, and calculating.
Pipe assemblies may use multiple sources and multiple sinks, and may define splits, merges, and joins to manipulate the tuple streams.
This concept is embodied in Cascalog through the definition of queries.
A query takes a set of inputs and applies a list of predicates across the fields in each tuple of the input stream.
Queries can also be composed to form larger, more complex queries.
In either event, these queries are reduced down into a Cascading pipeline.
Cascalog therefore provides an extremely terse and powerful abstraction on top of Cascading; moreover, it enables an excellent development workflow through the REPL.
Queries can be easily composed and executed against smaller representative datasets within the REPL, providing the idiomatic API and development workflow that makes Clojure beautiful.
If we unpack the query we defined for TF, we will find the following code:
The <- macro defines a query, but does not execute it.
The initial vector, [?key ?dfcount-str], defines the output fields, which is followed by a list of predicate functions.
Each predicate can be one of the following three types:
Operations typically act within the scope of a single tuple.
The :> keyword is used to separate input variables from output variables.
If no :> keyword is specified, the variables are considered as input variables for operations and output variables for generators and aggregators.
The (src ?doc-id ?time ?df-word) predicate function names the first three values within the input tuple, whose names are applicable within the query scope.
Each predicate within the scope of the query can use any bound value or add new bound variables to the scope of the query.
The final set of bound values that are emitted is defined by the output vector.
We defined three queries, each calculating a portion of the value required for the TF-IDF algorithm.
These are fed from two single taps, which are files stored in the Hadoop filesystem.
The document file is stored using Apache Avro, which provides a high-performance and dynamic serialization layer.
The record structure, in this case, is for a document and is defined as follows:
Both the stop words and documents are fed through an ETL function that emits a clean set of words that have been filtered.
The words are derived by splitting the line field using a regular expression:
The ETL function is also a query, which serves as a source for our downstream queries, and defines the [?doc-id ?time ?word] output fields.
The output tap, or sink, is based on the Cassandra scheme.
A query defines predicate logic, not the source and destination of data.
The sink ensures that the outputs of our queries are sent to Cassandra.
The ?- macro executes a query, and it is only at execution time that a query is bound to its source and destination, again allowing for extreme levels of composition.
The following, therefore, executes the TF query and outputs to Cassandra:
A new column was created called time that holds the Unix epoc time in milliseconds.
The updated text file was then processed using some basic Java code that leverages Avro:
Persisting documents from Storm In the previous recipe, we looked at deriving precomputed views of our data taking some immutable data as the source.
In an operational system, we need Storm to store the immutable data into Hadoop so that it can be used in any preprocessing that is required.
How to do it… As each tuple is processed in Storm, we must generate an Avro record based on the document record definition and append it to the data file within the Hadoop filesystem.
We must create a Trident function that takes each document tuple and stores the associated Avro record.
Within the prepare function, initialize the Avro schema and document writer:
As each tuple is received, coerce it into an Avro record and add it to the file:
How it works… There are various logical streams within the topology, and certainly the input for the topology is not in the appropriate state for the recipes in this chapter containing only URLs.
We therefore need to select the correct stream from which to consume tuples, coerce these into Avro records, and serialize them into a file.
Within the context of the topology definition, include the following code:
The function should consume tuples from the document stream whose tuples are populated with already fetched documents.
Integrating the batch and real-time views The final step to complete the big data architecture is largely complete already and is surprisingly simple, as is the case with all good functional style designs.
We need three new state sources that represents the D, DF, and TF values computed in the Batch layer.
We will combine the values from these states with the existing state before performing the final TF-IDF calculation.
One version should be from the current hour, and the other from all the data prior to the current hour:
Within a cluster deployment of Cassandra, simply replace the word localhost with a list of seed node IP addresses.
Seed nodes are simply Cassandra nodes, which, when appropriately configured, will know about their peers in the cluster.
For more information on Cassandra, please see the online documentation at http://wiki.
Finally, edit the existing DRPC query to reflect the added state and combiner function:
How it works… We have covered a huge amount of ground to get to this point.
We have implemented an entire real-time, big data architecture that is fault-tolerant, scalable, and reliable using purely open source technologies.
It is therefore useful at this point to recap the journey we have taken to the point, ending back where we are now:
This data pipeline defines predicates that not only act on tuples but also on persistent, mutable states.
We achieved this by implementing a portion of the pipeline in a DRPC stream that is only invoked at "at time"
This allowed us to segment the state into time-window buckets.
With the high-level picture in place, the final DRPC query stream becomes easier to understand.
A key to understanding this is that in each stage in this process, the tuple is simply receiving new values and each function is simply adding new named values to the tuple.
The state queries are doing the same based on existing fields within the tuple.
Finally, we end up with a very "wide" tuple that we trim down before returning the final result.
Introduction We have explored how Storm fits into a Big Data architecture as the real-time layer.
A key use case for Big Data solutions involves the enabling of data science.
We have seen some basic data science already through the implementation of some basic analysis of term frequencies, both in real time and batch.
When analyzing data, there are both operational and insights components to the process.
The operational component is the one we have explored already in great detail using Storm—the ability to process data streams and identify important terms.
Insights involve aggregated views of large datasets such that they provide some higher-level insight, and is typically an offline activity.
Both operationally and in terms of insights, we have only explored data analysis and data science in terms of a single temporal dimension—the past.
Increasingly, business is dependent on knowing what to expect before it happens.
This branch of data science is known by many names: machine learning, predictive analytics, and artificial intelligence.
Within this text, the terms machine learning and predictive analytics will be used interchangeably.
There are many use cases for machine learning, and some of them are as follows:
Within predictive analytics, one still needs to distinguish between operational analytics and insights.
This chapter will largely focus on the operational aspects, given that Storm is a platform ideally suited for this purpose; however, areas where insights are involved will also be identified.
A final classification that is important at this stage is the high-level families of predictive models and algorithms.
At a high level, two families exist, namely supervised and unsupervised.
The distinction between these two refers to the way in which the model is trained or built.
Which can either be done through training data that contains both features and known target values; or the models can be built without targets, leaving the discovery of any targets purely to the algorithm.
This distinction deserves more explanation; however, it must be noted that this book does not aim to present a complete overview of machine learning, rather simply introduce how to implement such techniques using Storm.
With that goal in mind, some concepts will be explained to a reasonable level of detail, some will be grossly over simplified, and some will be ignored entirely.
Machine learning is an extremely large and complex area, and it is highly recommended that the reader explores the topic in greater detail using one of the many online resources available to them.
Returning to supervised learning, we shall start with some terminology.
For a given dataset, specifically, for each datum within a dataset, there exist certain classes of fields:
For a better understanding of these terms, those with any relational database experience, can think about these classes in terms of a single table with rows and columns.
These terms describe the types of columns one would expect in a dataset that are to be used for training a supervised learning algorithm.
Therefore, while building a model, each datum is a row in that table.
By way of an example, suppose that you had an audit dataset with the following columns:
By including the target field, we are asking an algorithm to find some relationships within the features that will result in a given outcome.
When no target is given, the algorithm is said to be unsupervised in nature, in that, it needs to find relationships in the data without this guiding data.
Random Forest is an example of a supervised model, whereas K-Means is an example of an unsupervised model.
The fundamental assumption built into machine learning is that things that occurred in the past will happen in similar ways in the future.
Various machine learning algorithms attempt to extract the underlying concepts inherent in the featured set of data, and then use these concepts in predicting future events of a similar kind.
Of course, this assumption is invalidated entirely through novel data; however, the vast majority of data isn't novel.
This chapter demonstrates the implementation of such techniques on Storm, and the concepts that have been introduced extremely briefly here will be explained further as and when appropriate.
There are three approaches to implementing machine learning within Storm.
We will explore all the three (Storm-Pattern, Storm-R, and Trident-ML) approaches, but first, let's lay a basis for achieving exactly-once semantics.
Implementing a transactional topology In the previous chapters, we have concerned ourselves with data that was not "transactional" in nature, not in the way we typically think of things such as financial transactions.
As a result, one can potentially tolerate system failure because there is no direct monetary implication to each transaction that may be affected by any failure cases, especially given that transactional schematics come with a cost, both in performance and storage.
The recipes in this chapter deal with scoring transactions that require transactional schematics, and it is therefore relevant to understand how to achieve exactly-once schematics with Storm at this stage.
The transactional schematics of Storm, like most aspects of Storm, are excellently documented on the project's wiki.
Trident's support for exactly-once schematics requires specific implementations of spouts and state.
This brings up a problem when doing state updates (or anything with side effects)—you have no idea if you've ever successfully updated the state based on this tuple before.
In order to achieve this, you use the following properties of Storm:
If the batch is replayed, it is given the exact same txid.
Given these properties, and storing a little extra state, it is possible to ensure that state updates are truly idempotent in the face of failures and retries.
Replays of batches for a txid will extract the same set of tuples as the first time that batch was emitted for that txid.
Finally, the transaction ID is stored atomically with the value in the underlying Trident state.
Using this transaction ID, Trident is can detect if a given update is a duplicate of a previous update involving this batch and can decide whether to skip or apply this update.
The logic of including the transaction ID is handled entirely by Trident and its state implementations and works regardless of the underlying persistence.
Getting ready Before we get started, we need to install a message broker that more readily supports the transactional logic inherent in Storm's Trident, specifically Apache Kafka.
According to the Kafka website (http://kafka.apache.org/), it is designed to support the following:
In order to install Kafka, download the source packages from the website for Version 0.7.2
Once the download is complete, install Scala Build Tool (sbt), then unpack and build the Kafka server by executing the following command:
The default installation of Kafka sets the number of partitions to 1, which isn't a practical value.
With Kafka installed, open up three separate terminal instances within the kafka directory.
Then, from within the second terminal, execute the following command:
Note that your Zookeeper and Kafka instance are executing in separate terminals.
Your third terminal will be used to interact with Kafka topics.
Once the initialization sequence is complete, you will be able to type text; each line you type will publish that text to the "test" topic.
Enter a few lines and then escape the application by using Ctrl + C.
Finally, verify that the messages can be read; to do this, type the following command into the terminal:
This application should display all of your previously created messages.
You can use Ctrl + C to quit this application, but note that we will be shortly using this subscriber utility for testing.
How to do it… We will now implement an extremely basic transactional topology.
This will illustrate how transactional semantics are achieved in Storm.
In order to fully illustrate this, we will create the topology and test and understand the resulting state, but we will also create some forced errors in order to check whether the failure and recovery cases are working.
This will give you a clear understanding of how the transactional elements hang together and where to start debugging should you encounter errors.
Start by creating a Storm project that includes the following dependencies in the POM file:
Then, define a really simple topology in the makeTopology() method:
Trident ships with a built-in Debug function, but here we need a specialized equivalent that prints out the message contents and generates periodic errors.
To implement this, simply maintain a member-level count and throw exceptions based on modulus tests within the execute method:
So, we can simply expect the topology to print our messages, group them by content, and store a count against content in a Cassandra column family.
Before we test the topology, you need to create the Cassandra column family; execute the following commands in the Cassandra-cli to do so:
Once the topology is initialized, switch back to your third Kafka terminal and run the following command to start the message producer:
Then, take note of the Storm logs, which should be clear.
Also take note of the value in the Cassandra column family, which you can do by the following Cassandra-cli instructions:
Now, switch back to the terminal and enter a second message.
At this point, your topology should throw an exception and terminate.
To test this, simply run the topology again in local mode from Eclipse.
When you execute this , you will notice that the message does not get replayed into the topology, and the count remains at 1 until you publish a third message.
This means that the message was effectively lost as a result of the failure case.
This is obviously what we are trying to prevent; in order to fix the issue, add the following lines of code to your main method:
You can now re-run the same test, you will find that after the failure, the topology does replay the message and the count is updated as expected.
We saw many elements coming together in order to provide the exactly-once semantics that we finally witnessed during our tests.
Firstly, the Kafka spout, which is a prebuilt module from the storm-contrib project, already implements the required set of logic to be a transactional compliant spout.
We configured the spout using the following lines of code:
This configuration tells Kafka where to find the Kafka nodes.
In this case, we only have a single node on localhost; however, in a larger deployment, we would supply a list of actual Kafka nodes.
We also specify the number of partitions and the name of the topic to subscribe to.
We then create an instance of a transactional spout as the spout for the only stream in our topology.
This is all that is required to implement a transactional Kafka spout.
Many other transactional spouts exist within the storm-contrib project; however, if your broker isn't currently supported, using the existing storm-contrib code as a starting point would be the best approach to implementing a new spout.
Secondly, we have seen the transactional state in the Cassandra-backed Trident state.
Trident adds some transaction IS state to the state element in the underlying data store on our behalf.
It then also uses this state to safely replay the message and update the state.
In order to use the transactional state, ensure that you create your Cassandra state using the transactional utility method.
Finally, we managed to create failure cases, which weren't dealt with correctly without adding some additional configurations for the topology.
The properties in question effectively tell Storm where to find a running Zookeeper instance:
This fixes the problem, because Storm stores an additional bit of state in Zookeeper, which allows Storm to enforce strong ordering between batches.
This is key, Trident allows for parallel processing of batches; however, they always commit in order.
This capability is enabled by maintaining a batch-tracking state inside zookeeper.
Within a standard deployment, there is no need to tell Storm where to find Zookeeper; however, in the local mode, there is no Zookeeper instance running, thus the requirement for this configuration.
As mentioned in the introduction to the chapter, three approaches to machine learning within Storm will be presented.
It is important to understand when to use each approach.
This choice starts by understanding what machine learning approach and algorithm you would like to use: online or batch-based.
Remember that, for machine learning, online and batch refers to the way in which the model is trained.
This distinction does not imply any particular underlying engineering approach to achieve either the batch or online modes.
A batch model can be built in a real-time software platform, and conversely, an online model can be built as part of a batch software process.
It is important to understand this distinction between the engineering aspect and the machine learning aspect.
If you would like to explore this concept in more depth, please read the following overview of neural networks in which the distinction is explored in more detail: http://www.webpages.
Once you have chosen your algorithm approach, you can use the following guidelines to choose the correct approach within Storm:
It aims to support the operational deployment of all of the most common models, imported via PMML.
This is the preferred approach when batch training a model.
The model can then be used to score a stream of data.
The model can then be operationalized into Storm using the Storm-R integration with very little effort.
However, this method isn't preferred because of the performance characteristics of R and the coupling to R as a technology.
This is the approach to use if you are needing to implement an online learning mode.
With this framework in place, we can proceed to the first use case, which is batch-building a model and exporting it to be operationalized later using Storm-Pattern.
Getting ready To get started, you need to download the latest version of R from http://www.r-project.
It is also recommended that you download RStudio, which is an excellent open source Integrated Development Environment (IDE) for R from http://www.rstudio.com/
Once you have installed R and RStudio, open R Studio and you should be presented with a view similar to the following screenshot:
While a comprehensive overview of R is well outside of the scope of this book, a quick overview is relevant and useful.
This dock position also contains all the variables that are currently stored in the workspace and available as variables within the REPL.
You can also search the R archives and packages for particular terms or functions by simply typing ??[Search Criteria]
For this recipe, we are going to require two R packages, namely pmml and randomForest.
In order to install them, enter the following commands in the REPL:
RStudio will prompt you to choose a mirror and then install the required packages.
How to do it… Using the randomForest package, we will now build a Random Forest model and then export it using PMML.
Next, you need to split the data into training and testing datasets as described in the followingcommand line:
You then need to train the Random Forest model as described here:
You can then test the model by entering the following commands and inspecting the resulting confusion matrix:
The confusion matrix allows you to inspect the performance of the model against the Out Of Bag (OOB) data.
The following diagram illustrates the basic process involved in building a model:
The key element to understand is that the process of model building is highly iterative and interactive.
RStudio is a powerful tool to help you with this process.
The code that you entered will build you an excellent model based on a prebuilt and clean dataset that was prepared for training purposes.
When applied to real-world datasets, the process is always far more incremental and difficult, often including many other statistical techniques in order to analyze and manipulate the data.
Furthermore, depending on the size of the dataset, you may need to apply other approaches to test the model effectively.
Again, these topics are all subjects of entire books in their own right, and they are mentioned here in order to draw your attention to them; further reading is required if you would like to build models effectively in the real world.
The Random Forest is an extremely useful tool, especially for those starting out in machine learning.
It is an ensemble method that can be used for either regression or classification tasks.
Random Forest works by growing a multitude of decision trees, and a variation between the trees is introduced by projecting the training data into a randomly chosen subspace before fitting each tree.
Essentially, each tree is trained on a bootstrapped sample of the original dataset, and each time a leaf is split, only a randomly chosen subset of the dimensions are considered for splitting.
This ensures that the key underlying concepts that are inherent in the data are chosen, making for far more accurate OOB test results than a standard decision tree.
A Random Forest actually goes far beyond what I have described here; however, for our purposes, this will suffice, as all the complexity has been abstracted away within the R packages.
Good analysis is underpinned by an excellent understanding of the data; the exploring phase is essential, and you should spend much time getting to know your dataset.
These commands should give you three critical outputs to help you to start understanding the data.
Firstly, you will get a view of the data in a table to the top-left of the display.
Secondly, you will get a summary of the fields in the dataset and their types, and it should look something like the following command line:
The output also tells us that we have 1000 observations or rows.
The contents function also discovers that the order ID is a discrete set of values and tries to list them for us; the terminology within R refers to these discrete values as levels.
In this example, the Levels output isn't particularly useful, but there are many cases where it is.
Finally, you will get a summary of the dataset at a field level that should look similar to the following screenshot:
The importance of these outputs is that you can start to understand the type of data that you have within your feature fields: the range, median, and mean of each.
With this clearer understanding of the data in hand, you may practically need to filter, enrich, or clean the data as required.
In this recipe, a dataset was provided that did not require such cleansing; therefore, you moved directly on to building the model.
The steps between getting the data and building the model involved segmenting the data into training set and testing set are given as follows:
This step is vital in order to evaluate the model.
You can't validate the model using data that it has already "seen", so segmenting the original dataset allows us to keep a portion of the data aside for evaluating the model later, which can be achieved through the following commands:
The Predictive Model Markup Language (PMML) is an XML-based markup language developed by the Data Mining Group (DMG) to provide a way for applications to define models related to predictive analytics and data mining, and to share those models between PMMLcompliant applications.
The PMML standard creates an excellent way to decouple the system that generates the model from the system that provides the operational scoring.
This is architecturally significant because systems such as R, SAS, and Weka are excellent at performing analytics, but are not well-suited to use in operational settings.
Furthermore, scoring is often required within the context of a larger operational system, and therefore coupling analytical functionality will lead to a brittle and unsustainable architecture.
To expand on this, consider that predictive scoring is often performed as part of some larger business process.
Credit scoring involves a base set of rules that have been defined manually and a set of models that are able to predict the likely outcome of the engagement with a given individual.
Not only will the predictive models exist within a larger set of scoring mechanisms, such as rules, but all the scoring mechanisms will exist within the larger operational process of loan origination.
These operational systems can take many forms, from traditional ERP to more modern solutions, such as the ones based on Storm, which are dealing with web-scale operational process.
The key is decoupling the iterative, analytical, and manual task of analytics from the low-latency, high-throughput, and highly available concerns of an operational solution.
In order to understand the model a little deeper, let's explore the content of PMML briefly.
What you will see is a model divided into many segments, and many "sub models"
Each model is a tree structure of nodes, which are able to segment the feature space in a hierarchical manner until a leaf node is able to finally score the output for a given tree.
Let's look at one example as shown in the following code snippet:
You will notice that the model specifies the algorithm and the type in terms of classification or regression.
In this case, we have a Random Forest for classification.
There is then a large set of nested Node elements.
The predicate operators specify the operation to be performed by the scoring algorithm while it traverses the tree with a given set of input fields.
You may appreciate at this point, that this structure is extremely simple.
From this generated model, the scoring implementation should be comparatively simple.
Now that you have a built classification model, we need to implement an operational topology that leverages this model in order to perform classification as part of a larger operational data pipeline.
I would like to draw a distinction between an operational data pipeline and an operational process.
I will talk about operational process as an architecture concern that involves potentially many system layers.
These may include ERP, CRM, core processing engine, and so on.
An operational process is, therefore, positioned at the solution architecture level, and, as stated in the previous recipe, it typically won't include an analytics platform, such as R.
A data pipeline is applicable at Trident's level of abstraction.
Trident, in effect, allows you to define a streaming data pipeline.
It abstracts away that state and planning in order to achieve this pipeline at scale and in parallel.
This recipe will therefore present operational scoring using a classification model as part of a Trident data pipeline that may exist within a larger operational process at the solution level.
The recipe illustrates this by implementing an order management topology.
A stream of order data will flow into the order management topology, be unpacked, scored, and then finally enriched with distribution management information before being published.
The ultimate destination being some kind of logistics management system will handle the logistics of delivering the order.
The purpose is to show how scoring can be linked into a data flow that includes more than simply scoring.
Getting ready To begin with, ensure that you have installed Kafka as per the instructions in the Implementing a transactional topology recipe within this chapter.
If you had any trouble building the Random Forest model, you can use the pre-built instance of the model from the code package associated with this chapter.
Please note that the testing scripts for the topology are contained in the supporting code for the chapter and are written in Python.
The script that generates orders, generates JSON array order objects and places them on the appropriate Kafka topic.
Next, you must create a function that will coerce the data into the correct format from the Kafka spout.
This is done by converting the byte array into a string and parsing into a JSON array.
The fields must also be parsed and typed correctly in the output tuple as follows:
As part of the flow of this topology, you will need to publish the content out to another Kafka topic.
To achieve this, we will use a Trident module that leverages a partition persist.
KafkaStateUpdater expects us to use the contents of a single configurable field to publish to the Kafka topic; therefore, our downstream tuple must be coerced into a single JSON array that can be published to downstream systems, as shown in the following lines of code:
The following diagram clarifies the data pipeline of this topology:
Complete your topology by providing the Kafka configuration as per the Implementing a transactional topology recipe using the orders topic.
You can test it using the sendOrders.py Python script as described here:
The script takes two arguments, the number of orders to generate and the number of features to add to each order.
You can use any value for the first argument, but the topology expects exactly 10 features as it currently stands.
If you want to adjust the number of features, you will need to change your topology.
The function itself is quite simple; it takes a string during construction and uses it to initialize a classifier at preparation time.
When a tuple is received within the function, it simply delegates to the classifier as follows:
The classifier function builds an underlying model by parsing the XML node structure and building an in-memory Directed Acyclic Graph (DAG) of the node and predicate hierarchy.
In the case of a Random Forest, the outputs must be further ranked, because it is a forest of trees and executed as follows:
Another piece of topology worth the explanation is the Kafka Push partition persist, and is executed as follows:
A partition persist operation is typically reserved for database-like backers, not message queues.
In this case, it is important to publish using a partition persist in order to support the transactional semantics.
One can't simply publish from within a Trident function if they intend to support transactional semantics.
KafkaState ensures that messages are only published to Kafka once the transaction commits as described here:
You may ask yourself how the model is able to make sense of randomly generated data? The answer is that the model can't make sense of randomly generated data.
It is therefore important to create some relationship within the numbers that are generated from the testing script.
The provided script is, therefore, an interesting starting point for learning purposes.
The script works by defining a set of distributions that are segmented according to a set of probabilities.
For each row, a random value is used to choose the appropriate segment.
Each segment in turn generates a value based on a Gaussian distribution with set mean and standard deviation values per segment.
This approach leaves something tangible for the model to find and predict against, while still having enough variability to be interesting.
Creating an association rules model in R At present, the Storm Pattern Project supports the following set of models:
While these models cover an extremely wide range of use cases, there are many more algorithms that aren't represented.
It is obviously desirable, for various reasons, to have a native implementation of the model within Storm, and the community is working towards supporting a wider base; however, it must be noted that it is unlikely that the Storm community will build out machine learning support at the same speed as the R community, given the sheer size difference of focus from that community.
As a result, when there is an algorithm that you need to use that isn't support by Storm-Pattern, making use of R becomes a viable option (or you could build it and contribute it back to the community)
I realize that I elaborated somewhat in the previous recipe as to analytical systems not polluting the operational architecture; this approach appears to contradict this, but in fact it doesn't.
As a result, you can choose to use as much or as little as you like from the community of packages and capabilities.
If you think about Ubuntu, you can run it without any X-Windows system, purely at the command line, which makes it highly versatile in many environments.
Mixing a full R environment into an operational system may not be the correct approach; however, allowing Storm Bolts to manage the vanilla and slave R instances across a Storm cluster allows R to simply augment the Storm functionality rather than represent another architecturally significant component or layer from a functional perspective.
A popular use case for machine learning is a recommendation engine.
People who choose these options, often chose this option, so you should too.
There are various ways to achieve this, and a popular approach is to use association rules.
This recipe seeks to demonstrate how to build a set of association rules that we will use later to build a recommendation engine in Storm using the model we have created.
Getting ready In order to implement this recipe, you need to have the arules package installed.
Start by installing it using the following command from within RStudio:
We now need to explore the data and get an idea of what we are dealing with.
Execute the summary (Groceries) command, and you should receive an output similar to the following screenshot:
The data essentially consists of tab-separated lists of items, where each row represents a list of items that were purchased together.
It is easy to imagine how such a dataset could be constructed in something like a grocery store.
It is often useful to see the rules with the highest lift for validation purposes execute; this is shown in the following command:
What this tells us is that when liquor and red are selected together, the customer will likely choose beer, so you should recommend that to him to increase the chances of that sale.
Now that we have a model, we just need to save it as follows:
The algorithm, in this case being Apriori, uses these concepts to derive and rank the rules it finds in the transactional data.
The lift value ranks the results of the rule against a random choice model.
The lift measure gives us confidence that the prediction will give us much better results than simply randomly choosing another product to recommend.
Creating a recommendation engine A recommendation engine makes intelligent guesses as to what a customer may want to buy based on previous lists of products, which has been made famous by leaders such as Amazon.
These lists may be from a current selection within the context of the current session.
The list of products may be from previous purchases by the particular customer, and it may even simply be the products that the customer has viewed within a given session.
Whichever approach you choose, the training data and scoring data during operational phases must follow the same principles.
In this recipe, we will use the association rules model from the previous recipe to create a recommendation engine.
The concept behind the engine is that lists are supplied as asynchronous inputs and recommendations are forwarded as asynchronous outputs where applicable.
There are product combinations that aren't strongly supported by the model; in these cases, no recommendation is emitted.
If you need a recommendation for every single input, you could choose to emit a random recommendation when there is no strongly supported recommendation, or you could choose to improve your model through better and generally larger training datasets.
For this recipe, we will be receiving the product list as a JSON array on a Kafka topic.
We will therefore need to coerce the byte array input into a tuple containing two separate values, one being the transaction ID and the other being the list of products, as shown in the following lines of code:
We will also need to publish the output message using the Kafka partition persist.
The recommendation and transaction ID need to be coerced into a single value consisting of a JSON array as follows:
We then need to define the topology as described here:
The Storm-R project's standard function supports only a known input array size.
This works for most use cases; however, for the association case, the input size will vary for each tuple.
It is therefore necessary to override the execute function to cater for this particular case as shown here:
These elements are all that is required to create the recommendation engine.
You can now start your topology in local mode from Eclipse.
In order to test it, a test script is provided with the chapter code bundle named sendSelection.py.
This takes a single parameter, which is the number of transactions, to publish onto the queue as follows:
You can view the output recommendations by issuing the following command from the Kafka command line:
At the fundamental level, Storm-R works in a very similar way to the multilang approach that we investigated in detail in Chapter 5, Polyglot Topology.
For each instance of the function, it creates an R process that it interacts with via Studio.
However, Storm-R cuts much of the complexity and overhead out because it doesn't make use of a generalized protocol; it is specific to R.
The key assumption behind Storm-R is that a single R function will be called from the Storm function and will pass a vector as input.
This assumption nicely aligns the function interface with the concept of a tuple.
The Storm-R module further simplifies the R implementation by auto-generating some code to take care of the marshaling and unmarshaling of the vector across the interface.
As with a standard Trident function, any lengthy functions should execute during the preparation phase of the Storm function.
The Storm-R function expects to be initialized with a function name and a list of R libraries to load during the preparation phase.
It can also take an R script that should be executed during the preparation phase.
The R function that is to be called needs to be available within the R session.
This means that the R function needs to be supplied in a prebuilt package, or it can be defined in the initialization script.
In this recipe, we are using a function that is defined in the initialization script, but specifically we are using a prepackaged initialization script that ships with Storm-R.
Let's unpack the R function definition within the topology with the following code snippet:
Firstly, the constructor receives a list of libraries to load into R; in this case, just the arules package.
This is the R function that will receive a vector input and must return a vector output.
Secondly, the withNamedInitCode method is called and the name recommend is supplied.
What this function does is look up a script on the classpath by appending .R to the name supplied.
Therefore, in this case, a script named /recommend.R is present at the root of the classpath.
This means that you can supply any script there, but in this case, we will use the one that Storm-R brings to the classpath.
There is also an overloaded method that allows you to supply the script's contents as a string.
Let's take a look at the contents of the recommend script as shown here:
Essentially, this script is loading the dataset and building the rules from it.
It is then defining a function called recommend that takes a vector as input and uses it to search the rules for any found matches where the lift is greater than 1.3 and the LHS is the supplied input list.
It then converts the RHS of the found rules into a vector and returns.
The net effect is that at the time of Trident function preparation, the library will be loaded, and this function will be defined, making it available for all subsequent calls in the session.
All subsequent calls will originate from the execute method and contain the values from tuple.
In this recipe, we overrode the execute method of RFunction.
The reason, as stated, is that the standard implementation expects a fixed set of values in the tuple.
The execute method from RFunction consists of the following code:
As you can see, the difference is subtle but important.
The first instance uses the values of the tuple as the input to the R function.
The second instance uses the content of one of the values of the tuple, which is an array itself.
In this way, we can support any length of products.
The test script is built with a complete list of all possible products.
Real-time online machine learning The process of performing predictive analytics is largely iterative and interactive in nature; however, in all the previous examples, there is a definite distinction between the learning phase and the scoring phase within the life of the model.
In the case of online learning algorithms, this line gets blurred.
An online learning algorithm learns continuously through streams of updated training data.
Algorithms are therefore said to be either batch-based or online.
Note that, in either case, the algorithm can be real-time; however, in the batch-based model, a model is built in some offline batch process and is deployed into Storm for the purposes of real-time scoring.
In the online case, the algorithm both learns and scores as it sees new data and is also deployed into Storm as the real-time processing engine.
What this name simply means is that the algorithm learns in an online manner and the predictions are non-discreet.
For this topology, we will have two input streams, one for the training data and one for the data to be scored.
In order to deal with the training data, we need to coerce the input JSON message into doubles as follows:
The scoring input stream is very similar; however, it also contains a transaction ID, which we need to deal with, and type validity, which is important, as shown here:
As with previous recipes, you will need to coerce the output into a single JSON message so that it can be published to Kafka as follows:
Finally, you will need to define the spouts and topology, as described in the following code snippet:
At this point you can run the topology in local mode; in order to test it, you will need to run the supplied scripts as follows:
Both scripts take the number of records as input arguments.
You can view the scored transactional data by using the following command:
This approach works by maintaining some shared state, specifically the weights within the perceptron.
One stream of data updates these weights, and the second stream reads them as part of the scoring process.
Let's unpack how the model is trained in the online mode, as shown in the following lines of code:
Firstly, the data is coerced into the correct format and converted into an instance.
Take note that the order of the fields differs between the spout and the label instance creator.
The instance creator expects the label in the first value, but the input message contains the label in the last field.
This is dealt with easily, because Trident effectively projects the value.
The instance is then used to update the regression model as part of a partition persist of state where the state is backed into a memory map.
The state could easily have been backed by Memcached or Cassandra or any other persistence mechanism that you choose as is the case with any other state in Trident.
The regression model itself is updated and then saved as part of the update functionality within the partition persist step as shown here:
With an updated model in memory, the model can simply be used for scoring.
In this case, the input is also coerced into an instance, but this time the instance is used to query the state.
The query will retrieve the regression model and then use it to predict new values, as shown in the following code snippet:
In this way, the model is learning in an online mode as it "sees" new data.
This updated view is made available to the scoring stream because of the inherent power in the state management model of Trident.
Introduction Continuous Delivery is a term coined by Martin Fowler in his book by the same name.
I won't try to duplicate his work here, but as an extremely brief introduction, Continuous Delivery is a natural extension of Continuous Integration.
The base concept behind Continuous Delivery is that IT risks are best dealt with by failing small and failing often.
The longer it takes you to integrate a system, the larger the risk will be.
So rather integrate often; this way you experience the "pain" of integration in small bite-size chunks.
Off the back of this concept, Continuous Integration was born, enabled with many excellent tool suites.
Continuous Delivery extends the concept by recognizing that the process of operational deployment is also extremely risky and reaches a point where many IT projects fail.
The longer you take to perform a deployment, the greater the risk.
So rather experience the "pain" of deployment as early as possible in small bite size chunks.
As with Continuous Integration, a key focus then becomes speed and automation.
If we are going to integrate often, it needs to involve as little effort as possible and be regressive in nature, meaning that we don't redo the same tasks repetitively.
The other key recognition from Continuous Delivery is that we as IT professionals have only delivered once we have delivered something into a production environment and it is adding value to the business and users it serves.
With these thoughts in mind, we will explore how to implement Continuous Delivery on Stormbased systems.
In this chapter, we will set up Jenkins as our Continuous Integration server; that along with some configuration, will also act as a Continuous Delivery server.
We will then set up our environment tooling, establish a reliable build, deploy, and accept test cycle.
In the next chapter, we will learn how to deploy Storm into the AWS cloud, at which point we will complete the delivery process.
Setting up a CI server Jenkins is one of the most widely used open CI servers (http://jenkins-ci.org/)
Jenkins is a fork of the Hudson CI, which occurred in 2010 as a result of disputes between the community and Oracle over project control issues.
Since the project split, there has been a growing support base for Jenkins.
Evaluating tool choices is often difficult, and you must obviously evaluate various options before choosing the correct tool for your situation.
Jenkins is chosen here because it is one of the leading open source options and is known to be widely used and supported.
This will allow you to build and test your environment in a portable way, and push it to various cloud providers later.
Create a new project folder and initialize the Vagrant configuration file:
You can review the comments in the file, then update the file to reflect the following properties, which will create a 64-bit Ubuntu precise instance with a Puppet-based provisioning script:
You may need to adjust your network settings depending on your situation.
To do this, create two folders within the project called manifests and data.
Within manifests, add a file called default.pp that adds the Jenkins package repository to the list of sources and installs Jenkins:
We have a base Vagrant configuration that brings up a single node, based on a precise64 base box.
Within this box, we simply install Jenkins using the Ubuntu packages.
Because the packages aren't in the default universe, we have to add them and the associated keys first:
The package is then installed, and the service is brought up using the standard package and service types.
Setting up system environments Automating all your environments is critical, but the first step in this process is to understand how to establish a stable environment from scratch for your particular solution.
Once this process has been validated, it is then easy to automate and add into the pipeline.
In this recipe, we will manually establish a complete environment for Jenkins.
Getting ready We will use the Random Forest topology from Chapter 7, Real-time Machine Learning, as the project that will be managed by our Jenkins CI.
In order to build and deploy this project, there are a number of things that must be installed into our CI:
You must now edit /etc/profile and add the following code to it:
You now need to update all the Java references across the system:
Once the download is complete, install Scala Build Tool (SBT), and then unpack and build the Kafka server: cd ~
Finally, you need to check whether your build is working on the CI server before attempting the build in the Jenkins software:
You can replace the previous clone command with a URL to your repository.
How it works… This is a really simple process, and a familiar one to you.
The key here is that you validate your build and release process within your CI, which is different from your local development environment.
You will notice that I have checked out the storm-contrib module in order to build storm-kafka.
This is because this module isn't deployed into any binary repository at present.
I would suggest that you clone and deploy your own version of the contrib modules if binary version control is important to you or your organization.
Defining a delivery pipeline The delivery pipeline not only ensures what is released is stable and controlled, but also enables this to be done rapidly.
A pipeline can potentially enable every source commit to be a release candidate.
The pipeline needs to be built out in distinct steps, each removing different kinds of risks from the delivery process.
You will need to define a pipeline that is appropriate to your technology and organizational process.
This ensures that the code base is built and is unit tested.
It is important to keep the build and unit test process to a very short time, thus allowing developers to receive rapid feedback on their changes.
It is also necessary to perform a more complete test of the code base in a deployed or semi-deployed form.
This can be divided into integration tests and acceptance tests; whatever approach you take for this, the acceptance tests will generally be far longer running than the unit tests.
For this reason, the acceptance tests should be undertaken out of the developer's workflow and run only after the build and unit tests have passed.
Once the acceptance tests have passed, you can deliver the system into the QA environment for further automated and manual testing in a representative environment.
In this chapter, we will build the pipeline up until the deployment step, which is the subject of Chapter 9, Storm on AWS.
There are many other test types that you should include in your pipeline.
It is important to understand the change in your system's performance over time.
The first step is to install all the required plugins.
Navigate to the Manage Plugins screen through the Manage Jenkins link in the main menu and install the following plugins.
Next, you must configure the first step in the build pipeline.
Create a new Jenkins project called rf-topology-build and capture the following settings for the project:
In Build change Root POM to pom.xml and Goals and options to clean package.
Save the project, and then kick off the first build by manually scheduling the build from the Jenkins home screen.
By clicking on the progress bar, the actual console output of the build will be displayed.
The CI simply polls the Git repo for changes, and when changes are detected, it kicks off a build using the goals specified in the project configuration.
All artifacts and logs are then saved by the CI for later analysis.
Jenkins will also keep track of the build progress for you over time:
If you have a private repository, you will need to configure the deployment keys for your repository.
In order to achieve this, you need to generate an SSH key from the appropriate user on the CI server:
Then, ensure that you have an SSH agent running and add your key:
You must then add your key to the repository; in order to get the key, just issue the following command, and then copy the output:
If your repository is hosted on Bitbucket, you must use the following screen and paste the key:
Once this is complete, you will need to update the URL for Git within your Jenkins project to use the SSH protocol.
The automated acceptance test involves exercising a wide range of use cases across a wide range of test cases to achieve acceptable levels of test coverage at a system level.
There are many technologies that can be used for such a process; however, often you need to implement quite complex logic, so using a traditional programming language and libraries is often a valid way to approach the problem.
In this recipe, we will implement acceptance tests for the Random Forest topology and have them execute as part of the build pipeline.
To start with, create a unit test in the rf-topology project.
The first thing we need to do is define test data.
The following is a sample of a single test case:
The test data essentially consists of an order ID, an expected output value (Hub2 in this case), and an array of inputs.
Obviously, a single set of test data isn't valid for acceptance testing, so you must define a much larger set of test cases.
The test case will essentially undergo two phases: a warm-up phase and the actual test phase.
The warm-up phase simply validates that everything is working correctly before injecting extra data into the scenario.
Within each phase, we will send orders into the deployed topology, and then validate the outputs.
In order to send to a deployed topology, we must publish to the Kafka topic that will enable communications to that topology:
We then need to read all the outputs and validate them:
In order to validate the outputs, we need to remember that we won't get exact results from the predictive model, so we have to accept some level of error in the output.
The final change in the rf-topology project is to add the Maven configurations.
We obviously don't want this long running test to execute with each build, so we need to configure the surefire plugin to exclude the test:
Then, configure the failsafe plugin to execute the test for us:
With the test in place, we need to add it to the build pipeline.
In Build Triggers, set Build after other projects are built to rf-topologybuild.
In Build, set Root POM to pom.xml and Goals and options to clean integration-test.
In Post Steps, set Command to pkill -9 -f storm.cookbook.
You can then save the project and manually schedule a build in order to test your configuration.
The following diagram illustrates the process that is being undertaken within the context of the pipeline:
If there is other data existing in the topics, it will make analysis and debugging difficult.
The preceding command essentially ensures that a bash script is executed in the correct directory.
The content of the script performs the tasks to clear the topics, to start up Kafka and the topology, using Maven:
The tests are then executed via the integration-tests goal from Maven.
You can now create the pipeline view in order to see the pipeline in Jenkins.
To do this, simply click on the + link on the main page and select Build Pipeline View.
You will then have a view similar to the following on your main page under the Views tab:
This view is derived from the relationships between the Jenkins projects, specified by the "Triggered By" relationship between the projects.
One would be to generate data and place it into a file that the test reads; the other would be to embed the data into the test.
The script is very similar to the one that was used to do functional testing in Chapter 7, Real-time Machine Learning, except that the line coercing generates Java code instead of CSV rows:
This script has been tested using Python 2.7; simply execute it and use the output as the test data within your test case.
Introduction Software is ultimately an academic exercise unless it adds value in a production environment.
This isn't the only cloud hosting provider you might want to use, there are many good Platform as a Service (PaaS) providers, and many of them are excellent and also have high levels of automation and availability.
The choice of AWS for the book is simply because it is extremely cheap and easy to get started, which is ideal for learning, but it also allows you to go from the basic deployments all the way through to complete Virtual Private Cloud (VPC), complete with multiple isolated subnets, firewalls, and appropriate IPSEC.
These are all important concepts in an enterprise delivery, which you can learn quickly using AWS EC2 and VPC as presented in this chapter; however, the concepts and the tools are by no means tied to AWS.
You will also learn to extend your original development environment scripts from Chapter 1, Setting Up Your Development Environment, all the way through to production, based on Vagrant and Puppet provisioning.
None of these frameworks are tied in any way to AWS.
Pallet is designed to work with any cloud provider supported by jclouds, which are over 20 at the time of writing this book and counting.
It currently supports VirtualBox, AWS, and VMware, with new providers being released from the community each day.
The point therefore, is to learn the tools and concepts that will equip you to deploy Storm effectively and securely into any production environment you are faced with.
As an added bonus, you will get an introduction to build a secure production environment from scratch, on AWS, which you may simply choose to use for your particular implementation.
Deploying Storm on AWS using Pallet The Storm deploy module is recommended by the community for the deployment of Storm clusters on AWS.
And like the Storm project, it has really excellent documentation.
Because of this fact, this recipe is heavily based on the content of that wiki.
Pallet is a node provisioning, configuration and administration tool written in Clojure.
At this stage, it is useful to quickly introduce and position Pallet quickly within the context of infrastructure deployments, Storm, and the other chapters of this book.
Pallet has the following few properties that are worth mentioning:
This makes it highly portable, supporting just about any image out there.
At this point, it is therefore useful to ask and answer the question: Why present two competing provisioning frameworks? The following elements answer this question:
However, it has matured into a technology that can be leveraged throughout the delivery process and lifecycle.
It is therefore extremely attractive, especially if one intends to implement DevOps or continuous delivery to some extent.
Vagrant allows the deployment process to be established early in your development environments, and then to be repeatedly verified through various environments with little or no change.
It must be noted that this is possible with Pallet too, but Vagrant is far more simple to use.
In the DevOps communities, Puppet and Chef are more widely used than Pallet.
Moreover, your organization may already have a collection of recipes or modules they would prefer to extend instead of introducing another technology within the operations space, given that variables in that space are not a good thing.
In Chapter 1, Setting Up Your Development Environment, you were introduced to the concept of provisioning concerns in three distinct layers.
I will present the concept here again for convenience of being able to contrast the positioning of Pallet and Vagrant/Puppet:
In the interests of clarity, Vagrant provides the functionality to provision at the hypervisor layer and enables the Guest and Application provisioning through Puppet, Chef, and shell scripts.
Over and above these differences, Pallet and the recipes presented here and in Chapter 1, Setting Up Your Development Environment, attempt to achieve the following things:
Getting ready In order to proceed with this recipe, you need a valid AWS account.
Click on the Sign Up button, which will take you to the sign-in page.
Select the I am a new user option and enter your e-mail address.
Complete the registration process by entering your identification verification information.
Once you have signed up, you need to create a set of credentials to be used:
Log in and then select IAM from the main menu.
Click on the Create new user button at the top of the page.
You will then be presented with a dialog box, as shown in the following screenshot:
Ensure that you download the credentials at this point and place them somewhere safe.
Create a file named config.clj under ~/.pallet/ with the following content, specialized for your case:
If you don't have a private key yet, you can generate it using the :ssh-keygen command.
This will generate a private and public key at ~/.ssh/id_rsa.
You can launch spot instances for supervisor nodes by setting a value for supervisor.
Once these configurations are in place, you can simply run the following command in order to launch your cluster:
The --name parameter names your cluster so that you can attach to it or stop it later.
The --release parameter indicates which release of Storm to install.
If you omit --release, it will install Storm from the master branch.
Running this deploy will create multiple EC2 nodes on AWS.
As a result, AWS will start charging your account for the time that these nodes execute.
Make sure that you have estimated and understood the costs involved before you get an unexpected bill.
There's more… Storm deploy automatically attaches your machine to the cluster so that you can manage it through the start and stop commands for the cluster.
Attaching to a cluster configures your Storm client to talk to that particular cluster as well as authorizes your computer to view the Storm UI.
Ganglia is a scalable distributed monitoring system for high-performance computing systems, such as clusters and grids.
In order to get the IPs for your cluster, simply execute the following command:
Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the Amazon Web Services (AWS) Cloud where you can launch AWS resources in a virtual network that you define.
You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways.
This is an exceptionally powerful feature of the AWS cloud service offering.
It essentially allows you to set up an enterprise strength set of environments, from development through to production, which are secure, isolated, and securely connected to your own internal networks.
Network-level isolation is essential for the long-term security of a production system.
A typical enterprise solution will comprise of multiple layers of network, each with varying levels of access, representing layers of added security in order to protect the most vital portions of a system.
There are many possible reference architectures, each appropriate for a given solution.
In order to illustrate the nature of the isolated networks within the context of an enterprise solution, we need to have a solution that consists of a traditional web application, which feeds events through to the Storm cluster asynchronously for processing.
What you will notice in this picture is that the various functions of the solution are isolated at the Network level.
A public-facing subnet makes the web servers available via load balancing and Elastic IP addresses.
The application servers are isolated into their own subnet, with appropriate firewall rules, ensuring that the inward connections are only from specified IPs and ports within the web subnet.
These connections are also only established through appropriate cryptographic relationships between the Web and application servers.
This ensures that no services are incorrectly exposed from the application server.
Moving down the stack, a similar approach is applied to the database, which is typically a cluster too.
Asynchronous events are published from the application server to the analysis stack, via messaging, in this case Kafka.
These events are then consumed and processed by Storm and are saved into HDFS as immutable events, directly from Kafka or from Storm itself, depending on the structure of the topology.
The exact layout of the networks is not the purpose of this recipe or chapter.
One always needs to define an appropriate functional, conceptual, and deployment architecture for their given set of requirements.
The purpose of this recipe is to show how to set up a VPC with a public and private subnet.
In the next chapter, you will learn to deploy a Storm cluster into the private subnet.
Using this experience, you will then be equipped to design and implement a deployment architecture as complex as the one illustrated previously.
This will ensure that you can position Storm as a key element within your production environments, even in environments where regulatory constraints exist.
The environment that we will create in the next two recipes is best described by the following diagram:
You should be presented with a screen similar to the following:
This will create two subnets for you, together with a preconfigured EC2-based NAT instance.
The NAT instance will allow your nodes within the private subnet to access the Internet.
Nodes that aren't associated with an Elastic IP won't be able to access the Internet without a well-configured NAT instance in place.
Next, you will be presented with a screen similar to the following, which allows you to specify the network IP CIDR blocks and other settings for your VPC:
At this point, you need to carefully choose your IP addresses.
You will also need to specify the key pair that will gain you access to administer your NAT instance.
While the defaults are good values, you must be careful not to use the same blocks as the local network.
If you will use the same IP blocks, you are likely to experience route configuration issues and conflicts.
These sorts of issues are notoriously difficult to debug, so rather avoid them.
Congratulations, you have created your first AWS VPC with a pubic and private subnet.
You can now review the subnets and the NAT instance that has been created.
You can verify the NAT instance by identifying it in the EC2 instance list and connecting to it:
In order to access and provision into the private network, we need to set up an IPSEC-based VPN access to the private network.
We do this because we don't want to allocate Elastic IPs to our Storm nodes; there is no good reason for anyone on the Internet to have the ability to connect to our Storm nodes.
From within the EC2 instance list screen, click on the Launch Instance button, select the classic wizard, and then enter OpenVPN Access Server under the Community AMIs tab.
As part of the creation of the instance, create a new security group and open the following ports:
Always assign good descriptive names to your instances after creation; this will make your life much easier in future steps.
Next, you need to provide an Elastic IP to the OpenVPN instance.
Select Elastic IPs from the menu on the left-hand side, and then allocate a new address.
Once it is allocated, associate it with the newly created OpenVPN instance.
Take note of the IP address, because you will need it soon.
Next, verify your security group and change the source/destination check on the node.
And you can disable the source/destination check for the instance from the instance context menu:
Next, you need to set up the OpenVPN software on the instance.
The exact command can be found from the context menu under Connect.
When connected for the first time, the console will prompt you for setup answers for OpenVPN.
Accept all the default values, except the user credential store, which you should not make local unless you have another means of authentication available.
Once the setup process is complete, you will be presented with a standard bash console as the root user.
You must now assign a password to the OpenVPN user:
Log in with the user openvpn and the password that you just chose.
Once you have accepted the terms and conditions, navigate to the Server Network Settings:
Change the value of Hostname or IP address to the Elastic IP address for this instance:
Ensure that you save the settings, select the VPN settings, and add both subnets to the routing section:
Ensure that you save the settings and then select the button to update the running server:
But before you do that, we need to test the access to the private subnet.
In order to do this, we need an instance in the subnet.
Ensure that you don't assign it an Elastic IP, and try to connect to the instance via SSH from your machine.
We can now make the VPN client connection and test again.
Log in using the openvpn user and follow the onscreen prompts to download the client software and connect.
You should now be able to connect to the private IP of the testing node via SSH from your local machine.
Finally, you need to validate that the testing node has Internet access.
You can do this simply by performing a ping operation:
In Chapter 1, Setting Up Your Development Environment, we explored portal development using Vagrant; we even deployed a full Storm cluster into a local set of virtual machines.
This gave us a realistic way of testing the deployment and interactions of a cluster.
It is however very limited, in that all virtual machines are hosted on your local developer system.
In IT operations, it is important to reduce the number of variables and the number of things that need to be changed or configured in a system in order to promote it through different environments.
For that reason, it is wise to standardize an operating system's databases and tool chains across environments, and have the developer's environments mimic all the other environments as closely as possible.
Using Vagrant, you can develop a complete environment on your local machine, as we learned in Chapter 1, Setting Up Your Development Environment, and then use the same scripts to promote the deployment into the next environment, provided it is based on a supported Vagrant provider.
This is an extremely powerful workflow process for a development team.
It enables developers to automate deployments and test them repeatedly throughout the development process, thus greatly reducing the effort and risk of promoting the code through test and into production.
In this final recipe, we will learn how to add the AWS provider configuration to our cluster deployment scripts from Chapter 1, Setting Up Your Development Environment, and then deploy a full Storm cluster into the private subnet we created in the previous recipe.
Getting ready Before you begin this recipe, ensure that you have a valid key pair that you can use for the deployment process.
Ensure that you have the name handy and know the path of the .pem file that you downloaded previously.
If you don't have a key pair, simply select Key Pairs from the EC2 services area in the AWS console, and then click on Create Key Pair.
We do this simply from the command line using the following command:
There are numerous changes that we need to make to the file, so let's go through them one by one.
Right at the top of the file, we specified all the properties for each machine in the virtual cluster.
We need to update this to include some properties we need for AWS, essentially adding the instance type:
Next, we need to define the region and AMI that we are going to use (remember to replace the region as appropriate, together with the related AMI):
The AMI was chosen out of a list of standard Ubuntu AMIs from the Ubuntu AMI Locator at http://cloud-images.ubuntu.com/ locator/ec2/
Replace the following values with the appropriate values for your situation:
KeyPairName: The name of the key pair that you will use in the deployments.
This can be obtained from the VPC console, under Subnets.
Ensure that this is the ID of the private subnet.
This file must correspond to the key pair name provided previously.
Finally, there is a fundamental difference between VirtualBox boxes and Vagrant boxes for the AWS provider.
In the case of the AWS provider, we will use an actual AMI, which is not necessarily geared for Vagrant.
In the case of the previous AMI, it isn't; it is a vanilla Ubuntu AMI.
In the case of the VirtualBox images, they are built with Vagrant in mind, which means that they have certain dependencies already in place.
In this case, the only dependency that the AMI is missing is the installation of Puppet, which we need for the provisioning.
This can be rectified through a simple command-line provisioning step:
With all these additions to the Vagrant file, we can simply update the Vagrant up command slightly in order to provision the cluster onto AWS:
Once this command has completed successfully, you can simply issue Vagrant commands as you normally would.
As an optimization, you could choose to move the data files into an S3 bucket, instead of having them synchronized from the local data folder.
Note that this is only slightly different from the local deployment:
Once the provisioning is complete, you will have a complete Storm cluster, which is accessible only within the private subnet via VPN.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Learn something new in an Instant! A short, fast, focused guide delivering immediate results.
Learn PHPStorm from scratch, from downloading to installation with no prior knowledge required.
Enter, modify, and inspect the source code with as much automation as possible.
Simple, full of easy-to-follow procedures and intuitive illustrations, this book will set you speedily on the right track.
An intuitive guide to building applications for the realtime web with the vert.x platform.
From concept to deployment, learn the full development workflow of a real-time web application.
Utilize the Java skills you already have while stepping up to the next level.
Understand the usage of various socket.io features like rooms, namespaces, and sessions.
Deploy and scale your socket.io and Node.js applications in production.
A practical guide that quickly gets you up and running with socket.io.
Chapter 2: Log Stream Processing Introduction Creating a log agent Creating the log spout Rule-based analysis of the log stream Indexing and persisting the log data Counting and persisting log statistics Creating an integration test for the log stream cluster Creating a log analytics dashboard.
Chapter 3: Calculating Term Importance with Trident Introduction Creating a URL stream using a Twitter filter Deriving a clean stream of terms from the documents Calculating the relative importance of each term.
Chapter 4: Distributed Remote Procedure Calls Introduction Using DRPC to complete the required processing Integration testing of a Trident topology Implementing a rolling window topology Simulating time in integration testing.
