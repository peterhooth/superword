The seek time is the time needed to position the disk head in a new position.
The transfer time per byte is the rate of transfer from disk to memory when the head is in the right position.
The first docID is left unchanged (only shown for arachnocentric)
During query processing, a user’s access postings list is intersected with the results list returned by the text part of the index.
The value of x for which f reaches its minimum.
The collection frequency of term t (the total number of times the term appears in the document collection)
The document frequency of term t (the total number of documents in the collection the term appears in)
Top k items from a set, e.g., k nearest neighbors in kNN, top k retrieved documents, top k selected features from the vocabulary V.
Length of the test document (or application document) in tokens.
Size of the vocabulary of the test document (or application document)
Average size of the vocabulary in a document in the collection.
Number of occurrences of word t in documents of class c.
The term frequency of term t in document d (the total number of occurrences of t in d)
Random variable taking values in V, the vocabulary (e.g., at a given position k in a document)
Set cardinality: the number of members of set A |S| p.
As recently as the 1990s, studies showed that most people preferred getting information from other people rather than from information retrieval systems.
Of course, in that time period, most people also used human travel agents to book their travel.
However, during the last decade, relentless optimization of information retrieval effectiveness has driven web search engines to new quality levels where most people are satisfied most of the time, and web search has become a standard and often preferred source of information finding.
This book presents the scientific underpinnings of this field, at a level accessible to graduate students as well as advanced undergraduates.
In response to various challenges of providing information access, the field of information retrieval evolved to give principled approaches to searching various forms of content.
The field began with scientific publications and library records, but soon spread to other forms of content, particularly those of information professionals, such as journalists, lawyers, and doctors.
Much of the scientific research on information retrieval has occurred in these contexts, and much of the continued practice of information retrieval deals with providing access to unstructured information in various corporate and governmental domains, and this work forms much of the foundation of our book.
Nevertheless, in recent years, a principal driver of innovation has been the World Wide Web, unleashing publication at the scale of tens of millions of content creators.
This explosion of published information would be moot if the information could not be found, annotated and analyzed so that each user can quickly find information that is both relevant and comprehensive for their needs.
By the late 1990s, many people felt that continuing to index.
But major scientific innovations, superb engineering, the rapidly declining price of computer hardware, and the rise of a commercial underpinning for web search have all conspired to power today’s major search engines, which are able to provide high-quality results within subsecond response times for hundreds of millions of searches a day over billions of web pages.
This book is the result of a series of courses we have taught at Stanford University and at the University of Stuttgart, in a range of durations including a single quarter, one semester and two quarters.
These courses were aimed at early-stage graduate students in computer science, but we have also had enrollment from upper-class computer science undergraduates, as well as students from law, medical informatics, statistics, linguistics and various engineering disciplines.
The key design principle for this book, therefore, was to cover what we believe to be important in a one-term graduate course on information retrieval.
The first eight chapters of the book are devoted to the basics of information retrieval, and in particular the heart of search engines; we consider this material to be core to any course on information retrieval.
Chapter 1 introduces inverted indexes, and shows how simple Boolean queries can be processed using such indexes.
Chapter 2 builds on this introduction by detailing the manner in which documents are preprocessed before indexing and by discussing how inverted indexes are augmented in various ways for functionality and speed.
Chapter 3 discusses search structures for dictionaries and how to process queries that have spelling errors and other imprecise matches to the vocabulary in the document collection being searched.
Chapter 4 describes a number of algorithms for constructing the inverted index from a text collection with particular attention to highly scalable and distributed algorithms that can be applied to very large collections.
Chapter 5 covers techniques for compressing dictionaries and inverted indexes.
These techniques are critical for achieving subsecond response times to user queries in large search engines.
The indexes and queries considered in Chapters 1–5 only deal with Boolean retrieval, in which a document either matches a query, or does not.
Chapter 8 focuses on the evaluation of an information retrieval system based on the.
Chapters 9–21 build on the foundation of the first eight chapters to cover a variety of more advanced topics.
Chapter 9 discusses methods by which retrieval can be enhanced through the use of techniques like relevance feedback and query expansion, which aim at increasing the likelihood of retrieving relevant documents.
Chapter 10 considers information retrieval from documents that are structured with markup languages like XML and HTML.
Chapter 11 develops traditional probabilistic information retrieval, which provides a framework for computing the probability of relevance of a document, given a set of query terms.
This probability may then be used as a score in ranking.
Chapter 12 illustrates an alternative, wherein for each document in a collection, we build a language model from which one can estimate a probability that the language model generates a given query.
This probability is another quantity with which we can rank-order documents.
Chapters 13–17 give a treatment of various forms of machine learning and numerical methods in information retrieval.
Chapters 13–15 treat the problem of classifying documents into a set of known categories, given a set of documents along with the classes they belong to.
Chapter 13 motivates statistical classification as one of the key technologies needed for a successful search engine, introduces Naive Bayes, a conceptually simple and efficient text classification method, and outlines the standard methodology for evaluating text classifiers.
It also presents the bias-variance tradeoff as an important characterization of learning problems that provides criteria for selecting an appropriate method for a text classification problem.
Chapter 15 introduces support vector machines, which many researchers currently view as the most effective text classification method.
We also develop connections in this chapter between the problem of classification and seemingly disparate topics such as the induction of scoring functions from a set of training examples.
Chapters 16–18 consider the problem of inducing clusters of related documents from a collection.
In Chapter 16, we first give an overview of a number of important applications of clustering in information retrieval.
The chapter also addresses the difficult problem of automatically computing labels for clusters.
Chapter 18 develops methods from linear algebra that constitute an extension of clustering, and also offer intriguing prospects for algebraic methods in information retrieval, which have been pursued in the approach of latent semantic indexing.
We give in Chapter 19 a summary of the basic challenges in web search, together with a set of techniques that are pervasive in web information retrieval.
Next, Chapter 20 describes the architecture and requirements of a basic web crawler.
Finally, Chapter 21 considers the power of link analysis in web search, using in the process several methods from linear algebra and advanced probability theory.
This book is not comprehensive in covering all topics related to information retrieval.
We have put aside a number of topics, which we deemed outside the scope of what we wished to cover in an introduction to information retrieval class.
Nevertheless, for people interested in these topics, we provide a few pointers to mainly textbook coverage here.
Introductory courses in data structures and algorithms, in linear algebra and in probability theory suffice as prerequisites for all 21 chapters.
We now give more detail for the benefit of readers and instructors who wish to tailor their reading to some of the chapters.
Chapters 1–5 assume as prerequisite a basic course in algorithms and data structures.
Chapter 15 assumes that the reader is familiar with the notion of nonlinear optimization, although the chapter may be read without detailed knowledge of algorithms for nonlinear optimization.
We would like to thank Cambridge University Press for allowing us to make the draft book available online, which facilitated much of the feedback we have received while writing the book.
We also thank Lauren Cowles, who has been an outstanding editor, providing several rounds of comments on each chapter, on matters of style, organization, and coverage, as well as detailed comments on the subject matter of the book.
To the extent that we have achieved our goals in writing this book, she deserves an important part of the credit.
Parts of the initial drafts of Chapters 13–15 were based on slides that were generously provided by Ray Mooney.
While the material has gone through extensive revisions, we gratefully acknowledge Ray’s contribution to the three chapters in general and to the description of the time complexities of text classification algorithms in particular.
The above is unfortunately an incomplete list: we are still in the process of incorporating feedback we have received.
And, like all opinionated authors, we did not always heed the advice that was so freely given.
The published versions of the chapters remain solely the responsibility of the authors.
The authors thank Stanford University and the University of Stuttgart for providing a stimulating academic environment for discussing ideas and the opportunity to teach courses from which this book arose and in which its.
As well as links to some more general resources, it is our intent to maintain on this website a set of slides for each chapter which may be used for the corresponding lecture.
The meaning of the term information retrieval can be very broad.
Just getting a credit card out of your wallet so that you can type in the card number is a form of information retrieval.
However, as an academic field of study, information retrieval might be defined thus:INFORMATION.
Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers)
As defined in this way, information retrieval used to be an activity that only a few people engaged in: reference librarians, paralegals, and similar professional searchers.
Now the world has changed, and hundreds of millions of people engage in information retrieval every day when they use a web search engine or search their email.1 Information retrieval is fast becoming the dominant form of information access, overtaking traditional databasestyle searching (the sort that is going on when a clerk says to you: “I’m sorry, I can only look up your order if you can give me your Order ID”)
It is the opposite of structured data, the canonical example of which is a relational database, of the sort companies usually use to maintain product inventories and personnel records.
This is definitely true of all text data if you count the latent linguistic structure of human languages.
In modern parlance, the word “search” has tended to replace “(information) retrieval”; the term “search” is quite ambiguous, but in context we use the two synonymously.
The field of information retrieval also covers supporting users in browsing or filtering document collections or further processing a set of retrieved documents.
Given a set of documents, clustering is the task of coming up with a good grouping of the documents based on their contents.
It is similar to arranging books on a bookshelf according to their topic.
Given a set of topics, standing information needs, or other categories (such as suitability of texts for different age groups), classification is the task of deciding which class(es), if any, each of a set of documents belongs to.
It is often approached by first manually classifying some documents and then hoping to be able to classify new documents automatically.
Information retrieval systems can also be distinguished by the scale at which they operate, and it is useful to distinguish three prominent scales.
In web search, the system has to provide search over billions of documents stored on millions of computers.
Distinctive issues are needing to gather documents for indexing, being able to build systems that work efficiently at this enormous scale, and handling particular aspects of the web, such as the exploitation of hypertext and not being fooled by site providers manipulating page content in an attempt to boost their search engine rankings, given the commercial importance of the web.
Email programs usually not only provide search but also text classification: they at least provide a spam (junk mail) filter, and commonly also provide either manual or automatic means for classifying mail so that it can be placed directly into particular folders.
Distinctive issues here include handling the broad range of document types on a typical personal computer, and making the search system maintenance free and sufficiently lightweight in terms of startup, processing, and disk space usage that it can run on one machine without annoying its owner.
In between is the space of enterprise, institutional, and domain-specific search, where retrieval might be provided for collections such as a corporation’s internal documents, a database of patents, or research articles on biochemistry.
In this case, the documents will typically be stored on centralized file systems and one or a handful of dedicated machines will provide search over the collection.
This book contains techniques of value over this whole spectrum, but our coverage of some aspects of parallel and distributed search in web-scale search systems is comparatively light owing to the relatively small published literature on the details of such systems.
However, outside of a handful of web search companies, a software developer is most likely to encounter the personal search and enterprise scenarios.
A fat book which many people own is Shakespeare’s Collected Works.
One way to do that is to start at the beginning and to read through all the text, noting for each play whether it contains Brutus and Caesar and excluding it from consideration if it contains Calpurnia.
The simplest form of document retrieval is for a computer to do this sort of linear scan through documents.
This process is commonly referred to as grepping through text, after the Unix command grep, whichGREP performs this process.
Grepping through text can be a very effective process, especially given the speed of modern computers, and often allows useful possibilities for wildcard pattern matching through the use of regular expressions.
With modern computers, for simple querying of modest collections (the size of Shakespeare’s Collected Works is a bit under one million words of text in total), you really need nothing more.
The amount of online data has grown at least as quickly as the speed of computers, and we would now like to be able to search collections that total in the order of billions to trillions of words.
For example, it is impractical to perform the query Romans NEAR countrymen with grep, where NEAR might be defined as “within 5 words” or “within the same sentence”
To allow ranked retrieval: in many cases you want the best answer to an information need among many documents that contain certain words.
Now, depending on whether we look at the matrix rows or columns, we can have a vector for each term, which shows the documents it appears in, or a vector for each document, showing the terms that occur in it.2
To answer the query Brutus AND Caesar AND NOT Calpurnia, we take the vectors for Brutus, Caesar and Calpurnia, complement the last, and then do a bitwise AND:
The answers for this query are thus Antony and Cleopatra and Hamlet (Figure 1.2)
The model views each document as just a set of words.
Let us now consider a more realistic scenario, simultaneously using the opportunity to introduce some terminology and notation.
By documents we mean whatever units we haveDOCUMENT decided to build a retrieval system over.
We will refer to the group of documents over which we perform retrieval as the (document) collection.
Formally, we take the transpose of the matrix to be able to get the terms as column vectors.
When Antony found Julius Caesar dead, He cried almost to roaring; and he wept When at Philippi he found Brutus slain.
Typically, there might be about M = 500,000 distinct terms in these documents.
There is nothing special about the numbers we have chosen, and they might vary by an order of magnitude or more, but they give us some idea of the dimensions of the kinds of problems we need to handle.
Our goal is to develop a system to address the ad hoc retrieval task.
In it, a system aims to provide documents from within the collection that are relevant to an arbitrary user information need, communicated to the system by means of a one-off, user-initiated query.
An information need is the topic about which the user desires to know more, andINFORMATION NEED is differentiated from a query, which is what the user conveys to the com-QUERY puter in an attempt to communicate the information need.
A document is relevant if it is one that the user perceives as containing information of valueRELEVANCE with respect to their personal information need.
Our example above was rather artificial in that the information need was defined in terms of particular words, whereas usually a user is interested in a topic like “pipeline leaks” and would like to find relevant documents regardless of whether they precisely use those words or express the concept with other words such as pipeline rupture.
To assess the effectiveness of an IR system (i.e., the quality ofEFFECTIVENESS its search results), a user will usually want to know two key statistics about the system’s returned results for a query:
Precision: What fraction of the returned results are relevant to the informa-PRECISION tion need?
Recall: What fraction of the relevant documents in the collection were re-RECALL turned by the system?
This idea is central to the first major concept in information retrieval, the inverted index.
The name is actually redundant: an index always maps backINVERTED INDEX from terms to the parts of a document where they occur.
Then for each term, we have a list that records which.
Each item in the list – which records that a term appeared in a document (and, later, often, the positions in the document) – is conventionally called a posting.4 The list is then called a postingsPOSTING.
The dictionary in Figure 1.3 has been sorted alphabetically andPOSTINGS each postings list is sorted by document ID.
To gain the speed benefits of indexing at retrieval time, we have to build the index in advance.
Tokenize the text, turning each document into a list of tokens:
Some information retrieval researchers prefer the term inverted file, but expressions like index construction and index compression are much more common than inverted file construction and inverted file compression.
In a (non-positional) inverted index, a posting is just a document ID, but it is inherently associated with a term, via the postings list it is placed on; sometimes we will also talk of a (term, docID) pair as a posting.
The dictionary is commonly kept in memory, with pointers to each postings list, which is stored on disk.
Do linguistic preprocessing, producing a list of normalized tokens, which are the indexing terms: friend roman countryman so.
Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings.
Until then you can think of tokens and normalized tokens as also loosely equivalent to words.
Here, we assume that the first 3 steps have already been done, and we examine building a basic inverted index by sort-based indexing.
Within a document collection, we assume that each document has a unique serial number, known as the document identifier (docID)
During index con-DOCID struction, we can simply assign successive integers to each new document when it is first encountered.
The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure 1.4
The core indexing step is sorting this listSORTING so that the terms are alphabetical, giving us the representation in the middle column of Figure 1.4
Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index.
This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the.
Unix users can note that these steps are similar to use of the sort and then uniq commands.
The sequence of terms in each document, tagged by their documentID (left) is sorted alphabetically (middle)
Instances of the same term are then grouped by word and then by documentID.
The dictionary stores the terms, and has a pointer to the postings list for each term.
It commonly also stores other summary information such as, here, the document frequency of each term.
We use this information for improving query time efficiency and, later, for weighting in ranked retrieval models.
Each postings list stores the list of documents in which a term occurs, and may store other information such as the term frequency (the frequency of each term in each document) or the position(s) of the term in each document.
This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search.
In the resulting index, we pay for storage of both the dictionary and the postings lists.
The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter 5 we will examine how each can be optimized for storage and access efficiency.
What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few.
For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays.
Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section 2.3), which require additional pointers.
Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory caches.
Extra pointers can in practice be encoded into the lists as offsets.
If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse.
We can also use a hybrid scheme with a linked list of fixed length arrays for each term.
When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure 1.3), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory.
For the document collection shown in Exercise 1.2, what are the returned results for these queries:
How do we process a query using an inverted index and the basic Boolean retrieval model? Consider processing the simple conjunctive query:SIMPLE CONJUNCTIVE.
Intersect the two postings lists, as shown in Figure 1.5
The intersection operation is the crucial one: we need to efficiently intersectPOSTINGS LIST INTERSECTION postings lists so as to be able to quickly find documents that contain both.
This operation is sometimes referred to as merging postings lists:POSTINGS MERGE this slightly counterintuitive name reflects using the term merge algorithm for a general family of algorithms that combine multiple sorted lists by interleaved advancing of pointers through each; here we are merging the lists with a logical AND operation.
There is a simple and effective method of intersecting postings lists using the merge algorithm (see Figure 1.6): we maintain pointers into both lists.
We can extend the intersection operation to process more complicated queries like:
Query optimization is the process of selecting how to organize the work of an-QUERY OPTIMIZATION swering a query so that the least total amount of work needs to be done by the system.
A major element of this for Boolean queries is the order in which postings lists are accessed.
What is the best order for query processing? Consider a query that is an AND of t terms, for instance:
For each of the t terms, we need to get its postings, then AND them together.
The standard heuristic is to process terms in order of increasing document.
Figure 1.7 Algorithm for conjunctive queries that returns the set of documents containing each term in the input list of terms.
This is a first justification for keeping the frequency of terms in the dictionary: it allows us to make this ordering decision based on in-memory data before accessing any postings list.
Consider now the optimization of more general queries, such as:
As before, we will get the frequencies for all terms, and we can then (conservatively) estimate the size of each OR by the sum of the frequencies of its disjuncts.
We can then process the query in increasing order of the size of each disjunctive term.
For arbitrary Boolean queries, we have to evaluate and temporarily store the answers for intermediate expressions in a complex expression.
However, in many circumstances, either because of the nature of the query language, or just because this is the most common type of query that users submit, a query is purely conjunctive.
In this case, rather than viewing merging postings lists as a function with two inputs and a distinct output, it is more efficient to intersect each retrieved postings list with the current intermediate result in memory, where we initialize the intermediate result by loading the postings list of the least frequent term.
The intersection operation is then asymmetric: the intermediate results list is in memory while the list it is being intersected with is being read from disk.
Moreover the intermediate results list is always at least as short as the other list, and in many cases it is orders of magnitude shorter.
The intersection can be calculated in place by destructively modifying or marking invalid items in the intermediate results list.
Or the intersection can be done as a sequence of binary searches in the long postings lists for each posting in the intermediate results list.
Another possibility is to store the long postings list as a hashtable, so that membership of an intermediate result item can be calculated in constant rather than linear or log time.
Moreover, standard postings list intersection operations remain necessary when both terms of a query are very common.
Extend the postings merge algorithm to arbitrary Boolean query formulas.
Can we always merge in linear time? Linear in what? Can we do better than this?
We can use distributive laws for AND and OR to rewrite queries.
Show how to rewrite the query in Exercise 1.5 into disjunctive normal form using the distributive laws.
Would the resulting query be more or less efficiently evaluated than the original form of this query?
Is this result true in general or does it depend on the words and the contents of the document collection?
For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it isn’t.
How should the Boolean query x AND NOT y be handled? Why is naive evaluation of this query normally very expensive? Write out a postings merge algorithm that evaluates this query efficiently.
Despite decades of academic research on the advantages of ranked retrieval, systems implementing the Boolean retrieval model were the main or only search option provided by large commercial information providers for three decades until the early 1990s (approximately the date of arrival of the World Wide Web)
However, these systems did not have just the basic Boolean operations (AND, OR, and NOT) which we have presented so far.
A strict Boolean expression over terms with an unordered results set is too limited for many of the information needs that people have, and these systems implemented extended Boolean retrieval models by incorporating additional operators such as term proximity operators.
A proximity operator is a way of specifying that two terms in a queryPROXIMITY OPERATOR.
Westlaw (http://www.westlaw.com/)is the largest commercial legal search service (in terms of the number of paying subscribers), with over half a million subscribers performing millions of searches a day over tens of terabytes of text data.
Information need: Information on the legal theories involved in preventing the disclosure of trade secrets by employees formerly employed by a competing company.
Information need: Requirements for disabled people to be able to access a workplace.
Query: disab! /p access! /s work-site work-place (employment /3 place)
Information need: Cases about a host’s responsibility for drunk guests.
Boolean queries are precise: a document either matches the query or it does not.
This offers the user greater control and transparency over what is retrieved.
And some domains, such as legal materials, allow an effective means of document ranking within a Boolean model: Westlaw returns documents in reverse chronological order, which is in practice quite effective.
In 2007, the majority of law librarians still seem to recommend terms and connectors for high recall searches, and the majority of legal users think they are getting greater control by using them.
However, this does not mean that Boolean queries are more effective for professional searchers.
Indeed, experimenting on a Westlaw subcollection, Turtle (1994) found that free text queries produced better results than Boolean queries prepared by Westlaw’s own reference librarians for the majority of the information needs in his experiments.
A general problem with Boolean search is that using AND operators tends to produce high precision but low recall searches, while using OR operators gives low precision but high recall searches, and it is difficult or impossible to find a satisfactory middle ground.
In this chapter, we have looked at the structure and construction of a basic.
We introduced the Boolean retrieval model, and examined how to do efficient retrieval via linear time merges and simple query optimization.
In Chapters 2–7 we will consider in detail richer query models and the sort of augmented index structures that are needed to handle them efficiently.
Here we just mention a few of the main additional things we would like to be able to do:
We would like to better determine the set of terms in the dictionary and to provide retrieval that is tolerant to spelling mistakes and inconsistent choice of words.
It is often useful to search for compounds or phrases that denote a concept such as “operating system”
As the Westlaw examples show, we might also wish to do proximity queries such as Gates NEAR Microsoft.
To answer such queries, the index has to be augmented to capture the proximities of terms in documents.
A Boolean model only records term presence or absence, but often we would like to accumulate evidence, giving more weight to documents that have a term several times as opposed to ones that contain it only once.
To be able to do this we need term frequency information (the number of timesTERM FREQUENCY a term occurs in a document) in postings lists.
Boolean queries just retrieve a set of matching documents, but commonly we wish to have an effective method to order (or “rank”) the returned results.
This requires having a mechanism for determining a document score which encapsulates how good a match a document is for a query.
With these additional ideas, we will have seen most of the basic technology that supports ad hoc searching over unstructured information.
Ad hoc searching over documents has recently conquered the world, powering not only web search engines but the kind of unstructured search that lies behind the large eCommerce websites.
Although the main web search engines differ by emphasizing free text querying, most of the basic issues and technologies of indexing and querying remain the same, as we will see in later chapters.
Moreover, over time, web search engines have added at least partial implementations of some of the most popular operators from extended Boolean models: phrase search is especially popular and most have a very partial implementation of Boolean operators.
Nevertheless, while these options are liked by expert searchers, they are little used by most people and are not the main focus in work on trying to improve web search engine performance.
Try using the Boolean search features on a couple of major web search engines.
For instance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar AND burglar, and (iii) burglar OR burglar.
Look at the estimated number of results and top hits.
Do they make sense in terms of Boolean logic? Often they haven’t for major search engines.
Can you make sense of what is going on? What about if you try different words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR conquer.
What bound should the number of results from the first two queries place on the third query? Is this bound observed?
A great increase in the production of scientific literature, much in the form of less formal technical reports rather than traditional journal articles, coupled with the availability of computers, led to interest in automatic document retrieval.
However, in those days, document retrieval was always based on author, title, and keywords; full-text search came much later.
The article of Bush (1945) provided lasting inspiration for the new field:
Consider a future device for individual use, which is a sort of mechanized private file and library.
It needs a name, and, to coin one at random, ‘memex’ will do.
A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility.
Commercial interest quickly gravitated towards Boolean retrieval systems, but the early years saw a heady debate over various disparate technologies for retrieval systems.
It is a common fallacy, underwritten at this date by the investment of several million dollars in a variety of retrieval hardware, that the algebra of George Boole (1847) is the appropriate formalism for retrieval system design.
In this chapter we first briefly mention how the basic unit of a document can be defined and how the character sequence that it comprises is determined (Section 2.1)
We then examine in detail some of the substantive linguistic issues of tokenization and linguistic preprocessing, which determine the vocabulary of terms which a system uses (Section 2.2)
Tokenization is the process of chopping character streams into tokens, while linguistic preprocessing then deals with building equivalence classes of tokens which are the set of terms that are indexed.
Digital documents that are the input to an indexing process are typically bytes in a file or on a web server.
The first step of processing is to convert this byte sequence into a linear sequence of characters.
For the case of plain English text in ASCII encoding, this is trivial.
The sequence of characters may be encoded by one of various single byte or multibyte encoding schemes, such as Unicode UTF-8, or various national or vendor-specific standards.
This can be regarded as a machine learning classification problem, as discussed in Chapter 13,1 but is often handled by heuristic methods, user selection, or by using provided document metadata.
Once the encoding is determined, we decode the byte sequence to a character sequence.
We might save the choice of encoding because it gives some evidence about what language the document is written in.
The characters may have to be decoded out of some binary representation like Microsoft Word DOC files and/or a compressed format such as zip files.
Again, we must determine the document format, and then an appropriate decoder has to be used.
Even for plain text documents, additional decoding may need to be done.
Finally, the textual part of the document may need to be extracted out of other material that will not be processed.
This might be the desired handling for XML files, if the markup is going to be ignored; we would almost certainly want to do this with postscript or PDF files.
We will not deal further with these issues in this book, and will assume henceforth that our documents are a list of characters.
Commercial products usually need to support a broad range of document types and encodings, since users want things to just work with their data as is.
Often, they just think of documents as text inside applications and are not even aware of how it is encoded on disk.
This problem is usually solved by licensing a software library that handles decoding document formats and character encodings.
But, despite some complicated writing system conventions, there is an underlying sequence of sounds being represented and hence an essentially linear structure remains, and this is what is represented in the digital representation of Arabic, as shown in Figure 2.1
The next phase is to determine what the document unit for indexing is.
ThusDOCUMENT UNIT far we have assumed that documents are fixed units for the purposes of indexing.
For example, we take each file in a folder as a document.
A classifier is a function that takes objects of some sort and assigns them to one of a number of distinct classes (see Chapter 13)
Usually classification is done by machine learning methods such as probabilistic models, but it can also be done by hand-written rules.
Figure 2.2 The conceptual linear order of characters is not necessarily the order that you see on the page.
In languages that are written right-to-left, such as Hebrew and Arabic, it is quite common to also have left-to-right text interspersed, such as numbers and dollar amounts.
With modern Unicode representation concepts, the order of characters in files matches the conceptual order, and the reversal of displayed characters is handled by the rendering system, but this may not be true for documents in older encodings.
A traditional Unix (mbox-format) email file stores a sequence of email messages (an email folder) in one file, but you might wish to regard each email message as a separate document.
Many email messages now contain attached documents, and you might then want to regard the email message and each contained attachment as separate documents.
If an email message has an attached zip file, you might want to decode the zip file and regard each file it contains as a separate document.
Going in the opposite direction, various pieces of web software (such as latex2html) take things that you might regard as a single document (e.g., a Powerpoint file or a LATEX document) and split them into separate HTML pages for each slide or subsection, stored as separate files.
In these cases, you might want to combine multiple files into a single document.
For a collection of books, it would usually be a bad idea to index an.
A search for Chinese toys might bring up a book that mentions China in the first chapter and toys in the last chapter, but this does not make it relevant to the query.
Instead, we may well wish to index each chapter or paragraph as a mini-document.
Matches are then more likely to be relevant, and since the documents are smaller it will be much easier for the user to find the relevant passages in the document.
But why stop there? We could treat individual sentences as mini-documents.
It becomes clear that there is a precision/recall tradeoff here.
If the units get too small, we are likely to miss important passages because terms were distributed over several mini-documents, while if units are too large we tend to get spurious matches and the relevant information is hard for the user to find.
An IR system should be designed to offer choices of granularity.
For this choice to be made well, the person who is deploying the system must have a good understanding of the document collection, the users, and their likely information needs and usage patterns.
For now, we will henceforth assume that a suitable size document unit has been chosen, together with an appropriate way of dividing or aggregating files, if needed.
Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation.
These tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction.
A token is an instanceTOKEN of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.
A type is the class of allTYPE tokens containing the same character sequence.
A term is a (perhaps nor-TERM malized) type that is included in the IR system’s dictionary.
The set of index terms could be entirely distinct from the tokens, for instance, they could be.
The major question of the tokenization phase is what are the correct tokens to use? In this example, it looks fairly trivial: you chop on whitespace and throw away punctuation characters.
This is a starting point, but even for English there are a number of tricky cases.
For example, what do you do about the various uses of the apostrophe for possession and contractions?
O’Neill thinks that the boys’ stories about Chile’s capital aren’t amusing.
For O’Neill, which of the following is the desired tokenization?
A simple strategy is to just split on all non-alphanumeric characters, but while o neill looks okay, aren t looks intuitively bad.
For all of them, the choices determine which Boolean queries will match.
A query of neill AND capital will match in three cases but not the other two.
In how many cases would a query of o’neill AND capital match? If no preprocessing of a query is done, then it would match in only one of the five cases.
That is, as defined here, tokens that are not indexed (stop words) are not terms, and if multiple tokens are collapsed together via normalization, they are indexed as one term, under the normalized form.
However, we later relax this definition when discussing classification and clustering in Chapters 13–18, where there is no index.
In these chapters, we drop the requirement of inclusion in the dictionary.
Boolean or free text queries, you always want to do the exact same tokenization of document and query words, generally by processing queries with the same tokenizer.
This guarantees that a sequence of characters in a text will always match the same sequence typed in a query.3
It thus requires the language of the document to be known.
One possible solution is to omit from indexing tokens such as monetary amounts, numbers, and URLs, since their presence greatly expands the size of the vocabulary.
However, this comes at a large cost in restricting what people can search for.
For instance, people might want to search in a bug database for the line number where an error occurs.
It is easy to feel that the first example should be regarded as one token (and is indeed more commonly written as just coeducation), the last should be separated into words, and that the middle case is unclear.
Handling hyphens automatically can thus be complex: it can either be done as a classification problem, or more commonly by some heuristic rules, such as allowing short hyphenated prefixes on words, but not longer hyphenated forms.
Conceptually, splitting on white space can also split what should be regarded as a single token.
This occurs most commonly with names (San Francisco, Los Angeles) but also with borrowed foreign phrases (au fait) and com3
The Boolean case is more complex: this tokenization may produce multiple terms from one query word.
It is harder for a system to handle the opposite case where the user entered as two terms something that was tokenized together in the document processing.
Splitting tokens on spaces can cause bad retrieval results, for example, if a search for York University mainly returns documents containing New York University.
The problems of hyphens and non-separating whitespace can even interact.
Advertisements for air fares frequently contain items like San Francisco-Los Angeles, where simply doing whitespace splitting would give unfortunate results.
The last two can be handled by splitting on hyphens and using a phrase index.
Getting the first case right would depend on knowing that it is sometimes written as two words and also indexing it in this way.
One effective strategy in practice, which is used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis (Example 1.1), is to encourage users to enter hyphens wherever they may be possible, and whenever there is a hyphenated form, the system will generalize the query to cover all three of the one word, hyphenated, and two word forms, so that a query for over-eager will search for over-eager OR “over eager” OR overeager.
However, this strategy depends on user training, since if you query using either of the other two forms, you get no generalization.
Getting the first case correct will affect the correct indexing of a fair percentage of nouns and adjectives: you would want documents mentioning both l’ensemble and un ensemble to be indexed under ensemble.
This phenomenon reaches its limit case with major East Asian Languages (e.g., Chinese, Japanese, Korean, and Thai), where text is written without any spaces between words.
One approach here is to perform word segmentation as prior linguistic processing.
Since there are multiple possible segmentations of character sequences (see Figure 2.4), all such methods make mistakes sometimes, and so you are never guaranteed a consistent unique tokenization.
The other approach is to abandon word-based indexing and to do all indexing via just short subsequences of characters (character k-grams), regardless of whether particular sequences cross word boundaries or not.
Three reasons why this approach is appealing are that an individual Chinese character is more like a syllable than a letter and usually has some semantic content, that most words are short (the commonest length is 2 characters), and that, given the lack of standardization of word breaking in the writing system, it is not always clear where word boundaries should be placed anyway.
Even in English, some cases of where to put word boundaries are just orthographic conventions think of notwithstanding vs.
Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely.
AnSTOP LIST example of a stop list is shown in Figure 2.5
And a lot of the time not indexing stop words does little harm: keyword searches with terms like the and by don’t seem very useful.
The phrase query “President of the United States”, which contains two stop words, is more precise than President AND “United States”
The meaning of flights to London is likely to be lost if the word to is stopped out.
A search for Vannevar Bush’s article As we may think will be difficult if the first three words are stopped out, and the system searches simply for documents containing the word think.
Some of the design of modern IR systems has focused precisely on how we can exploit the statistics of language so as to be able to cope with common words in better ways.
So for most modern IR systems, the additional cost of including stop words is not that big – neither in terms of index size nor in terms of query processing time.
Query term Terms in documents that should be matched Windows Windows windows Windows, windows, window window window, windows.
Figure 2.6 An example of how asymmetric expansion of query terms can usefully model users’ expectations.
Having broken up our documents (and also our query) into tokens, the easy case is if tokens in the query just match tokens in the token list of the document.
However, there are many cases when two character sequences are not quite the same but you would like a match to occur.
For instance, if you search for USA, you might hope to also match documents containing U.S.A.
Token normalization is the process of canonicalizing tokens so that matchesTOKEN.
The advantage of just using mapping rules that remove characters like hyphens is that the equivalence classing to be done is implicit, rather than being fully calculated in advance: the terms that happen to become identical as the result of these rules are the equivalence classes.
It is only easy to write rules of this sort that remove characters.
Since the equivalence classes are implicit, it is not obvious when you might want to add characters.
An alternative to creating equivalence classes is to maintain relations between unnormalized tokens.
The usual way is to index unnormalized tokens and to maintain a query expansion list of multiple vocabulary entries to consider for a certain query term.
A query term is then effectively a disjunction of several postings lists.
The alternative is to perform the expansion during index construction.
When the document contains automobile, we index it under car as well (and, usually, also vice-versa)
Use of either of these methods is considerably less efficient than equivalence classing, as there are more postings to store and merge.
It is also often referred to as term normalization, but we prefer to reserve the name term for the output of the normalization process.
Traditionally, expanding the space required for the postings lists was seen as more disadvantageous, but with modern storage costs, the increased flexibility that comes from distinct postings lists is appealing.
These approaches are more flexible than equivalence classes because the expansion lists can overlap while not being identical.
An example of how such an asymmetry can be exploited is shown in Figure 2.6: if the user enters windows, we wish to allow matches with the capitalized Windows operating system, but this is not plausible if the user enters window, even though it is plausible for this query to also match lowercase windows.
The best amount of equivalence classing or query expansion to do is a fairly open question.
But doing a lot can easily have unexpected consequences of broadening queries in unintended ways.
However, if I put in as my query term C.A.T., I might be rather upset if it matches every appearance of the word cat in documents.5
Below we present some of the forms of normalization that are commonly employed and how they are implemented.
In many cases they seem helpful, but they can also do harm.
In fact, you can worry about many details of equivalence classing, but it often turns out that providing processing is done consistently to the query and to documents, the fine details may not have much aggregate effect on performance.
For English, an alternative to making every token lowercase is to just make some tokens lowercase.
The simplest heuristic is to convert to lowercase words at the beginning of a sentence and all words occurring in a title that is all uppercase or in which most or all words are capitalized.
These words are usually ordinary words that have been capitalized.
Mid-sentence capitalized words are left as capitalized (which is usually correct)
This will mostly avoid case-folding in cases where distinctions should be kept apart.
The same task can be done more accurately by a machine learning sequence model which uses more features to make the decision of when to case-fold.
However, trying to get capitalization right in this way probablyTRUECASING doesn’t help if your users usually use lowercase regardless of the correct case of words.
Other possible normalizations are quite idiosyncratic and particular to English.
For instance, you might wish to equate ne’er and never or the British spelling colour and the American spelling color.
Dates, times and similar items come in multiple formats, presenting additional challenges.
And there are signs of change: Sifry (2007) reports that only about one third of blog posts are in English.
Japanese is a well-known difficult writing system, as illustrated in Figure 2.7
While there are strong conventions and standardization through the education system over the choice of writing system, in many cases the same word can be written with multiple writing systems.
For example, a word may be written in katakana for emphasis (somewhat like italics)
Or a word may sometimes be written in hiragana and sometimes in Chinese characters.
Successful retrieval thus requires complex equivalence classing across the writing systems.
In particular, an end user might commonly present a query entirely in hiragana, because it is easier to type, just as Western end users commonly use all lowercase.
Document collections being indexed can include documents from many different languages.
Or a single document can easily contain text from multiple languages.
For instance, a French email might quote clauses from a contract document written in English.
One option is to run a language identification classifier on documents and then to tag terms in the vocabulary for their language.
Or this tagging can simply be omitted, since it is relatively rare for the exact same character sequence to be a word in different languages.
When dealing with foreign or complex words, particularly foreign names, the spelling may be unclear or there may be variant transliteration standards giving different spellings (for example, Chebyshev and Tchebycheff or Beijing and Peking)
One way of dealing with this is to use heuristics to equivalence class or expand terms with phonetic equivalents.
For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing.
Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.
In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.
The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.
The result of this mapping of text will be something like:
Stemming usually refers toSTEMMING a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.
If confrontedLEMMA with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun.
The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.
Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.
The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is Porter’s algorithmPORTER STEMMER (Porter 1980)
The entire algorithm is too long and intricate to present here, but we will indicate its general nature.
Porter’s algorithm consists of 5 phases of word reductions, applied sequentially.
Within each phase there are various conventions to select rules, such as selecting the rule from each rule group that applies to the longest suffix.
In the first phase, this convention is used with the following rule group:
Many of the later rules use a concept of the measure of a word, which loosely checks the number of syllables to see whether a word is long enough that it is reasonable to regard the matching portion of a rule as a suffix rather than as part of the stem of a word.
Figure 2.8 presents an informal comparison of the different behaviors of these stemmers.
Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words.
However, the exact stemmed form does not matter, only the equivalence classes it forms.
Rather than using a stemmer, you can use a lemmatizer, a tool from Nat-LEMMATIZER ural Language Processing which does full morphological analysis to accurately identify the lemma for each word.
Doing full morphological analysis produces at most very modest benefits for retrieval.
Sample text: Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation.
Porter stemmer: such an analysi can reveal featur that ar not easili visibl from the variat in the individu gene and can lead to a pictur of express that is more biolog transpar and access to interpret.
Paice stemmer: such an analys can rev feat that are not easy vis from the vary in the individ gen and can lead to a pict of express that is mor biolog transp and access to interpret.
Figure 2.8 A comparison of three stemming algorithms on a sample text.
While it helps a lot for some queries, it equally hurts performance a lot for others.
As an example of what can go wrong, note that the Porter stemmer stems all of the following words:
However, since operate in its various forms is a common verb, we would expect to lose considerable precision on queries such as the following with Porter stemming:
For a case like this, moving to using a lemmatizer would not completely fix the problem because particular inflectional forms are used in particular collocations: a sentence with the words operate and system is not a good match for the query operating AND system.
Getting better value from term normalization depends more on pragmatic issues of word use than on formal issues of linguistic morphology.
Stemming should be invoked at indexing time but not while processing a query.
Suggest what normalized form should be used for these words (including the word itself as a possibility):
The following pairs of words are stemmed to the same form by the Porter stemmer.
Does it have a deleterious effect on retrieval? Why or why not?
The postings intersection can use a skip pointer when the end point is still less than the item on the other list.
In the remainder of this chapter, we will discuss extensions to postings list data structures and ways to increase the efficiency of using postings lists.
If the list lengths are m and n, the intersection takes O(m+ n) operations.
Can we do better than this? That is, empirically, can we usually process postings list intersection in sublinear time? We can, if the index isn’t changing too fast.
One way to do this is to use a skip list by augmenting postings lists withSKIP LIST skip pointers (at indexing time), as shown in Figure 2.9
Skip pointers are effectively shortcuts that allow us to avoid processing parts of the postings list that will not figure in the search results.
The two questions are then where to place skip pointers and how to do efficient merging using skip pointers.
Consider first efficient merging, with Figure 2.9 as an example.
Suppose we’ve stepped through the lists in the figure until we have matched 8 on each list and moved it to the results list.
A number of variant versions of postings list intersection with skip pointers is possible depending on when exactly you check the skip pointer.
Skip pointers will only be available for the original postings lists.
For an intermediate result in a complex query, the call hasSkip(p) will always return false.
Finally, note that the presence of skip pointers only helps for AND queries, not for OR queries.
More skips means shorter skip spans, and that we are more likely to skip.
But it also means lots of comparisons to skip pointers, and lots of space storing skip pointers.
Fewer skips means few pointer comparisons, but then long skip spans which means that there will be fewer opportunities to skip.
A simple heuristic for placing skips, which has been found to work well in practice, is that for a postings list of length P, use.
Building effective skip pointers is easy if an index is relatively static; it.
Choosing the optimal encoding for an inverted index is an ever-changing game for the system builder, because it is strongly dependent on underlying computer technologies and their relative speeds and sizes.
Traditionally, CPUs were slow, and so highly compressed techniques were not optimal.
Now CPUs are fast and disk is slow, so reducing disk postings list size dominates.
However, if you’re running a search engine with everything in memOnline edition (c)
For one term the postings list consists of the following 16 entries:
Work out how many comparisons would be done to intersect the two postings lists with the following two strategies.
Consider a postings intersection between this postings list, with skip pointers:
How many postings comparisons will be made by this algorithm while intersecting the two lists?
How many postings comparisons would be made if the postings lists are intersected without the use of skip pointers?
Many complex or technical concepts and many organization and product names are multiword compounds or phrases.
We would like to be able to pose a query such as Stanford University by treating it as a phrase so that a sentence in a document like The inventor Stanford Ovshinsky never went to university.
Most recent search engines support a double quotes syntax (“stanford university”) for phrase queries, which has proven to be veryPHRASE QUERIES easily understood and successfully used by users.
As many as 10% of web queries are phrase queries, and many more are implicit phrase queries (such as person names), entered without use of double quotes.
To be able to support such queries, it is no longer sufficient for postings lists to be simply lists of documents that contain individual terms.
In this section we consider two approaches to supporting phrase queries and their combination.
A search engine should not only support phrase queries, but implement them efficiently.
A related but distinct concept is term proximity weighting, where a document is preferred to the extent that the query terms appear close to each other in the text.
One approach to handling phrases is to consider every pair of consecutive terms in a document as a phrase.
In this model, we treat each of these biwords as a vocabulary term.
The query stanford university palo alto can be broken into the Boolean query on biwords:
This query could be expected to work fairly well in practice, but there can and will be occasional false positives.
Without examining the documents, we cannot verify that the documents matching the above Boolean query do actually contain the original 4 word phrase.
Among possible queries, nouns and noun phrases have a special status in describing the concepts people are interested in searching for.
But related nouns can often be divided from each other by various function words, in phrases such as the abolition of slavery or renegotiation of the constitution.
These needs can be incorporated into the biword indexing model in the following.
Now deem any string of terms of the form NX*N to be an extended biword.
Each such extended biword is made a term in the vocabulary.
To process a query using such an extended biword index, we need to also parse it into N’s and X’s, and then segment the query into extended biwords, which can be looked up in the index.
This algorithm does not always work in an intuitively optimal manner when parsing longer queries into Boolean queries.
Better results can be obtained by using more precise part-of-speech patterns that define which extended biwords should be indexed.
The concept of a biword index can be extended to longer sequences of words, and if the index includes variable length word sequences, it is generally referred to as a phrase index.
Indeed, searches for a single term arePHRASE INDEX not naturally handled in a biword index (you would need to scan the dictionary for all biwords containing the term), and so we also need to have an index of single-word terms.
While there is always a chance of false positive matches, the chance of a false positive match on indexed phrases of length 3 or more becomes very small indeed.
But on the other hand, storing longer phrases has the potential to greatly expand the vocabulary size.
Maintaining exhaustive phrase indexes for phrases of length greater than two is a daunting prospect, and even use of an exhaustive biword dictionary greatly expands the size of the vocabulary.
However, towards the end of this section we discuss the utility of the strategy of using a partial phrase index in a compound indexing scheme.
To process a phrase query, you still need to access the inverted index entries for each distinct term.
As before, you would start with the least frequent term and then work to further restrict the list of possible candidates.
In the merge operation, the same general technique is used as before, but rather than simply checking that both terms are in a document, you also need to check that their positions of appearance in the document are compatible with the phrase query being evaluated.
Suppose the postings lists for to andbe are as in Figure 2.11, and the query is “to be or not to be”
The postings lists to access are: to, be, or, not.
We will examine intersecting the postings lists for to and be.
Then, we look for places in the lists where there is an occurrence of be with a token index one higher than a position of to, and then we look for another occurrence of each word with token index 4 higher than the first occurrence.
In the above lists, the pattern of occurrences that is a possible match is:
Here, /k means “within k words of (on either side)”
Clearly, positional indexes can be used for such queries; biword indexes cannot.
Adopting a positional index expands required postings storage significantly, even if we compress position values/offsets as we.
Let’s examine the space implications of having a positional index.
A posting now needs an entry for each occurrence of a term.
The index size thus depends on the average document size.
The result is that large documents cause an increase of two orders of magnitude in the space required to store the postings list:
The strategies of biword indexes and positional indexes can be fruitfully combined.
If users commonly query on particular phrases, such as Michael Jackson, it is quite inefficient to keep merging positional postings lists.
A combination strategy uses a phrase index, or just a biword index, for certain queries and uses a positional index for other phrase queries.
Good queries to include in the phrase index are ones known to be common based on recent querying behavior.
But this is not the only criterion: the most expensive phrase queries to evaluate are ones where the individual words are common but the desired phrase is comparatively rare.
Hence, having the latter is more desirable, even if it is a relatively less common query.
For each term, a next word index records terms that follow it in a document.
They concludeNEXT WORD INDEX that such a strategy allows a typical mixture of web phrase queries to be completed in one quarter of the time taken by use of a positional index alone, while taking up 26% more space than use of a positional index alone.
Which document(s) if any match each of the following queries, where each expression within quotes is a phrase query?
Consider the following fragment of a positional index with the format:
Describe the set of documents that satisfy the query Gates /2 Microsoft.
Describe each set of values for k for which the query Gates /k Microsoft returns a.
Consider the general procedure for merging two positional postings lists for a given document, to determine the document positions where a document satisfies a /k clause (in general there can be multiple positions at which each term occurs in a single document)
We begin with a pointer to the position of occurrence of each term and move each pointer along the list of occurrences in the document, checking as we do so whether we have a hit for /k.
Let L denote the total number of occurrences of the two terms in the document.
What is the big-O complexity of the merge procedure, if we wish to have postings including positions in the result?
A naive algorithm for this operation could be O(PLmax2), where P is the sum of the lengths of the postings lists (i.e., the sum of document frequencies) and Lmax is the maximum length of a document (in tokens)
Go through this algorithm carefully and explain how it works.
What is the complexity of this algorithm? Justify your answer carefully.
For certain queries and data distributions, would another algorithm be more efficient? What complexity does it have?
The merge can be accomplished in a number of steps linear in L and independent of k, and we can ensure that each pointer moves only to the right.
The merge can be accomplished in a number of steps linear in L and independent of k, but a pointer may be forced to move non-monotonically (i.e., to sometimes back up)
How could an IR system combine use of a positional index and use of stop words? What is the potential problem, and how could it be handled?
Exhaustive discussion of the character-level processing of East Asian lan-EAST ASIAN LANGUAGES guages can be found in Lunde (1998)
For further discussion of Chinese word segmentation, see Sproat et al.
Language identification was perhaps first explored in cryptography; for example, Konheim (1981) presents a character-level k-gram language identification algorithm.
Written language identification is regarded as a fairly easy problem, while spoken language identification remains more difficult; see Hughes et al.
The classic presentation of skip pointers for IR can be found in Moffat andSKIP LIST Zobel (1996)
In practice, the effectiveness of using skip pointers depends on various system parameters.
Moffat and Zobel (1996) report conjunctive queries running about five times faster with the use of skip pointers, but Bahle et al.
In contrast, Strohman and Croft (2007) again show good performance gains from skipping, in a system architecture designed to optimize for the large memory spaces and multiple cores of recent CPUs.
Here, we develop techniques that are robust to typographical errors in the query, as well as alternative spellings.
In Section 3.1 we develop data structures that help the search for terms in the vocabulary in an inverted index.
The * symbol indicates any (possibly empty) string of characters.
Users pose such queries to a search engine when they are uncertain about how to spell a query term, or seek documents containing variants of a query term; for instance, the query automat* would seek documents containing any of the terms automatic, automation and automated.
We then turn to other forms of imprecisely posed queries, focusing on spelling errors in Section 3.3
Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection.
We detail a number of techniques for correcting spelling errors in queries, one term at a time as well as for an entire string of query terms.
Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s)
This can be especially useful in cases like the Herman example, where the user may not know how a proper name is spelled in documents in the collection.
Given an inverted index and a query, our first task is to determine whether each query term exists in the vocabulary and if so, identify the pointer to the.
This vocabulary lookup operation uses a classical data structure called the dictionary and has two broad classes of solutions: hashing, and search trees.
In the literature of data structures, the entries in the vocabulary (in our case, terms) are often referred to as keys.
Hashing has been used for dictionary lookup in some search engines.
Each vocabulary term (key) is hashed into an integer over a large enough space that hash collisions are unlikely; collisions if any are resolved by auxiliary structures that can demand care to maintain.1 At query time, we hash each query term separately and following a pointer to the corresponding postings, taking into account any logic for resolving hash collisions.
There is no easy way to find minor variants of a query term (such as the accented and non-accented versions of a word like resume), since these could be hashed to very different integers.
In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2
Finally, in a setting (such as the Web) where the size of the vocabulary keeps growing, a hash function designed for current needs may not suffice in a few years’ time.
Search trees overcome many of these issues – for instance, they permit us to enumerate all vocabulary terms beginning with automat.
The best-known search tree is the binary tree, in which each internal node has two children.BINARY TREE The search for a term begins at the root of the tree.
Each internal node (including the root) represents a binary test, based on whose outcome the search proceeds to one of the two sub-trees below that node.
Figure 3.1 gives an example of a binary search tree used for a dictionary.
Efficient search (with a number of comparisons that is O(log M)) hinges on the tree being balanced: the numbers of terms under the two sub-trees of any node are either equal or differ by one.
The principal issue here is that of rebalancing: as terms are inserted into or deleted from the binary search tree, it needs to be rebalanced so that the balance property is maintained.
To mitigate rebalancing, one approach is to allow the number of sub-trees under an internal node to vary in a fixed interval.
Each branch under an internal node again represents a test for a range of char1
So-called perfect hash functions are designed to preclude collisions, but are rather more complicated both to implement and to compute.
In this example the branch at the root partitions vocabulary terms into two subtrees, those whose first letter is between a and m, and the rest.
A B-tree may be viewed as “collapsing” multiple levels of the binary tree into one; this is especially advantageous when some of the dictionary is disk-resident, in which case this collapsing serves the function of pre-fetching imminent binary tests.
In such cases, the integers a and b are determined by the sizes of disk blocks.
Section 3.5 contains pointers to further background on search trees and B-trees.
It should be noted that unlike hashing, search trees demand that the characters used in the document collection have a prescribed ordering; for instance, the 26 letters of the English alphabet are always listed in the specific order A through Z.
Some Asian languages such as Chinese do not always have a unique ordering, although by now all languages (including Chinese and Japanese) have adopted a standard ordering system for their character sets.
A search tree on the dictionary is a convenient way of handling trailing wildcard queries: we walk down the tree following the symbols m, o and n in turn, at which point we can enumerate the set W of terms in the dictionary with the prefix mon.
Finally, we use |W| lookups on the standard inverted index to retrieve all documents containing any term in W.
But what about wildcard queries in which the * symbol is not constrained to be at the end of the search string? Before handling this general case, we mention a slight generalization of trailing wildcard queries.
First, consider leading wildcard queries, or queries of the form *mon.
Consider a reverse B-tree on the dictionary – one in which each root-to-leaf path of the B-tree corresponds to a term in the dictionary written backwards: thus, the term lemon would, in the B-tree, be represented by the path root-n-o-m-e-l.
A walk down the reverse B-tree then enumerates all terms R in the vocabulary with a given prefix.
To do this, we use the regular B-tree to enumerate the set W of dictionary terms beginning with the prefix se, then the reverse B-tree to.
We now study two techniques for handling general wildcard queries.
Both techniques share a common strategy: express the given wildcard query qw as a Boolean query Q on a specially constructed index, such that the answer to Q is a superset of the set of vocabulary terms matching qw.
Then, we check each term in the answer to Q against qw, discarding those vocabulary terms that do not match qw.
At this point we have the vocabulary terms matching qw and can resort to the standard inverted index.
Our first special index for general wildcard queries is the permuterm index,PERMUTERM INDEX a form of inverted index.
First, we introduce a special symbol $ into our character set, to mark the end of a term.
Thus, the term hello is shown here as the augmented term hello$
Next, we construct a permuterm index, in which the various rotations of each term (augmented with $) all link to the original vocabulary term.
Figure 3.3 gives an example of such a permuterm index entry for the term hello.
We refer to the set of rotated terms in the permuterm index as the permuterm vocabulary.
How does this index help us with wildcard queries? Consider the wildcard query m*n.
Next, we look up this string in the permuterm index, where seeking n$m* (via a search tree) leads to rotations of (among others) the terms man and moron.
Now that the permuterm index enables us to identify the original vocabulary terms matching a wildcard query, we look up these terms in the standard inverted index to retrieve matching documents.
We can thus handle any wildcard query with a single * symbol.
Not all such dictionary terms will have the string mo in the middle - we filter these out by exhaustive enumeration, checking each candidate to see if it contains mo.
In this example, the term fishmonger would survive this filtering but filibuster would not.
One disadvantage of the permuterm index is that its dictionary becomes quite large, including as it does all rotations of each term.
Notice the close interplay between the B-tree and the permuterm index above.
Indeed, it suggests that the structure should perhaps be viewed as a permuterm B-tree.
However, we follow traditional terminology here in describing the permuterm index as distinct from the B-tree that allows us to select the rotations with a given prefix.
Whereas the permuterm index is simple, it can lead to a considerable blowup from the number of rotations per term; for a dictionary of English terms, this can represent an almost ten-fold space increase.
We now present a second technique, known as the k-gram index, for processing wildcard queries.
Thus cas, ast and stl are all 3-grams occurring in the term castle.
In a k-gram index, the dictionary contains all k-grams that occur in any termk-GRAM INDEX in the vocabulary.
Each postings list points from a k-gram to all vocabulary.
For instance, the 3-gram etr would point to vocabulary terms such as metric and retrieval.
How does such an index help us with wildcard queries? Consider the wildcard query re*ve.
We are seeking documents containing any term that begins with re and ends with ve.
This is looked up in the 3-gram index and yields a list of matching terms such as relive, remove and retrieve.
Each of these matching terms is then looked up in the standard inverted index to yield documents matching the query.
There is however a difficulty with the use of k-gram indexes, that demands one further step of processing.
This is a simple string-matching operation and weeds out terms such as retired that do not match the original query.
Terms that survive are then searched in the standard inverted index as usual.
We have seen that a wildcard query can result in multiple terms being enumerated, each of which becomes a single-term query on the standard inverted index.
Even without Boolean combinations of wildcard queries, the processing of a wildcard query can be quite expensive, because of the added lookup in the special index, filtering and finally the standard inverted index.
A search engine may support such rich functionality, but most commonly, the capability is hidden behind an interface (say an “Advanced Query” interface) that most.
Exposing such functionality in the search interface often encourages users to invoke it even when they do not require it (say, by typing a prefix of their query followed by a *), increasing the processing load on the search engine.
Exercise 3.1In the permuterm index, each permuterm vocabulary term points to the original vocabulary term(s) from which it was derived.
How many original vocabulary terms can there be in the postings list of a permuterm vocabulary term?
Write down the entries in the permuterm index dictionary that are generated by the term mama.
If you wanted to search for s*ng in a permuterm wildcard index, what key(s) would one do the lookup on?
Refer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the postings are lexicographically ordered.
What Boolean query on a bigram index would be generated for this query? Can you think of a term that matches the permuterm query in Section 3.2.1, but does not satisfy this Boolean query?
Give an example of a sentence that falsely matches the wildcard query mon*h if the search were to simply use a conjunction of bigrams.
We next look at the problem of correcting spelling errors in queries.
For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot.
Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears: britian spears, britney’s spears, brandy spears and prittany spears.
We look at two steps to solving this problem: the first based on edit distance and the second based on k-gram overlap.
Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience.
There are two basic principles underlying most spelling correction algorithms.
Of various alternative correct spellings for a mis-spelled query, choose the “nearest” one.
This demands that we have a notion of nearness or proximity between a pair of queries.
When two correctly spelled queries are tied (or nearly tied), select the one that is more common.
For instance, grunt and grant both seem equally plausible as corrections for grnt.
Then, the algorithm should choose the more common of grunt and grant as the correction.
The simplest notion of more common is to consider the number of occurrences of the term in the collection; thus if grunt occurs more often than grant, it would be the chosen correction.
A different notion of more common is employed in many search engines, especially on the web.
The idea is to use the correction that is most common among queries typed in by other users.
The idea here is that if grunt is typed as a query more often than grant, then it is more likely that the user who typed grnt intended to type the query grunt.
Beginning in Section 3.3.3 we describe notions of proximity between queries, as well as their efficient computation.
Spelling correction algorithms build on these computations of proximity; their functionality is then exposed to users in one of several ways:
On the query carot always retrieve documents containing carot as well as any “spell-corrected” version of carot, including carrot and tarot.
As in (1) above, but only when the query term carot is not in the dictionary.
As in (1) above, but only when the original query returned fewer than a preset number of documents (say fewer than five documents)
When the original query returns fewer than a preset number of documents, the search interface presents a spelling suggestion to the end user: this suggestion consists of the spell-corrected query term(s)
Thus, the search engine might respond to the user: “Did you mean carrot?”
We focus on two specific forms of spelling correction that we refer to as isolated-term correction and context-sensitive correction.
In isolated-term correction, we attempt to correct a single query term at a time – even when we.
Such isolated-term correction would fail to detect, for instance, that the query flew form Heathrow contains a mis-spelling of the term from – because each term in the query is correctly spelled in isolation.
We begin by examining two techniques for addressing isolated-term correction: edit distance, and k-gram overlap.
Most commonly, the edit operations allowed for this purpose are: (i) insert a character into a string; (ii) delete a character from a string and (iii) replace a character of a string by another character; for these operations, edit distance is sometimes known as Levenshtein distance.
In fact, the notion of edit distance can be generalized to allowing different weights for different kinds of edit operations, for instance a higher weight may be placed on replacing the character s by the character p, than on replacing it by the character a (the latter being closer to s on the keyboard)
Setting weights in this way depending on the likelihood of letters substituting for each other is very effective in practice (see Section 3.4 for the separate issue of phonetic similarity)
However, the remainder of our treatment here will focus on the case in which all edit operations have the same weight.
The spelling correction problem however demands more than computing edit distance: given a set S of strings (corresponding to terms in the vocabulary) and a query string q, we seek the string(s) in V of least edit distance from q.
We may view this as a decoding problem, in which the codewords (the strings in V) are prescribed in advance.
The obvious way of doing this is to compute the edit distance from q to each string in V, before selecting the.
Accordingly, a number of heuristics are used in practice to efficiently retrieve vocabulary terms likely to have low edit distance to the query term(s)
Once we retrieve such terms, we can then find the ones of least edit distance from q.
In fact, we will use the k-gram index to retrieve vocabulary terms that have many k-grams in common with the query.
We will argue that for reasonable definitions of “many k-grams in common,” the retrieval process is essentially that of a single scan through the postings for the k-grams in the query string q.
Suppose we wanted to retrieve vocabulary terms that contained at least two of these three bigrams.
This straightforward application of the linear scan intersection of postings immediately reveals the shortcoming of simply requiring matched vocabulary terms to contain a fixed number of k-grams from the query q: terms like boardroom, an implausible “correction” of bord, get enumerated.
We could replace the Jaccard coefficient by other measures that allow efficient on the fly computation during postings scans.
We then compute the edit distance from q to each term in this set, selecting terms from the set with small edit distance to q.
Isolated-term correction would fail to correct typographical errors such as flew form Heathrow, where all three query terms are correctly spelled.
When a phrase such as this retrieves few documents, a search engine may like to offer the corrected query flew from Heathrow.
The simplest way to do this is to enumerate corrections of each of the three query terms (using the methods leading up to Section 3.3.4) even though each query term is correctly spelled, then try substitutions of each correction in the phrase.
For the example flew form Heathrow, we enumerate such phrases as fled form Heathrow and flew fore Heathrow.
For each such substitute phrase, the search engine runs the query and determines the number of matching results.
This enumeration can be expensive if we find many corrections of the individual terms, since we could encounter a large number of combinations of alternatives.
In the example above, as we expand the alternatives for flew and form, we retain only the most frequent combinations in the collection or in the query logs, which contain previous queries by users.
For instance, we would retain flew from as an alternative to try and extend to a three-term corrected query, but perhaps not fled fore or flea form.
In this example, the biword fled fore is likely to be rare compared to the biword flew from.
Then, we only attempt to extend the list of top biwords (such as flew from), to corrections of Heathrow.
As an alternative to using the biword statistics in the collection, we may use the logs of queries issued by users; these could of course include queries with spelling errors.
Compute the Jaccard coefficients between the query bord and each of the terms in Figure 3.7 that contain the bigram or.
Consider the four-term query catched in the rye and suppose that each of the query terms has five alternative terms suggested by isolated-term correction.
How many possible corrected phrases must we consider if we do not trim the space of corrected phrases, but instead try all six variants for each of the terms?
For each of the prefixes of the query — catched, catched in and catched in the — we have a number of substitute prefixes arising from each term and its alternatives.
Suppose that we were to retain only the top 10 of these substitute prefixes, as measured by its number of occurrences in the collection.
How many of the possible substitute prefixes are we eliminating at each phase?
Our final technique for tolerant retrieval has to do with phonetic correction: misspellings that arise because the user types a query that sounds like the target term.
Such algorithms are especially applicable to searches on the names of people.
The main idea here is to generate, for each term, a “phonetic hash” so that similar-sounding terms hash to the same value.
The idea owes its origins to work in international police departments from the early 20th century, seeking to match names for wanted criminals despite the names being spelled differently in different countries.
It is mainly used to correct phonetic misspellings in proper nouns.
Algorithms for such phonetic hashing are commonly collectively known as soundex algorithms.
However, there is an original soundex algorithm, withSOUNDEX various variants, built on the following scheme:
Turn every term to be indexed into a 4-character reduced form.
Build an inverted index from these reduced forms to the original terms; call this the soundex index.
When the query calls for a soundex match, search this soundex index.
The variations in different soundex algorithms have to do with the conversion of terms to 4-character forms.
Change all occurrences of the following letters to ’0’ (zero): ’A’, E’, ’I’, ’O’, ’U’, ’H’, ’W’, ’Y’
Repeatedly remove one out of each pair of consecutive identical digits.
Pad the resulting string with trailing zeros and return the first four positions, which will consist of a letter followed by three digits.
For an example of a soundex map, Hermann maps to H655
Given a query (say herman), we compute its soundex code and then retrieve all vocabulary terms matching this soundex code from the soundex index, before running the resulting query on the standard inverted index.
This leads to related names often having the same soundex codes.
While these rules work for many cases, especially European languages, such rules tend to be writing system dependent.
For example, Chinese names can be written in Wade-Giles or Pinyin transcription.
While soundex works for some of the differences in the two transcriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2, it fails in other cases, for example Wade-Giles j and Pinyin r are mapped differently.
Exercise 3.14Find two differently spelled proper nouns whose soundex codes are the same.
Find two phonetically similar proper nouns whose soundex codes are different.
Knuth (1997) is a comprehensive source for information on search trees, including B-trees and their use in searching through dictionaries.
Garfield (1976) gives one of the first complete descriptions of the permuterm index.
Ferragina and Venturini (2007) give an approach to addressing the space blowup in permuterm indexes.
One of the earliest formal treatments of spelling correction was due to Damerau (1964)
Gusfield (1997) is a standard reference on string algorithms such as edit distance.
Probabilistic models (“noisy channel” models) for spelling correction were pioneered by Kernighan et al.
In these models, the misspelled query is viewed as a probabilistic corruption of a correct query.
They have a similar mathematical basis to the language model methods presented in Chapter 12, and also provide ways of incorporating phonetic similarity, closeness on the keyboard, and data from the actual spelling mistakes of users.
Cucerzan and Brill (2004) show how this work can be extended to learning spelling correction models based on query reformulations in search engine logs.
In this chapter, we look at how to construct an inverted index.
We call this process index construction or indexing; the process or machine that performs itINDEXING the indexer.
The design of indexing algorithms is governed by hardware con-INDEXER straints.
We therefore begin this chapter with a review of the basics of computer hardware that are relevant for indexing.
Section 4.3 describes single-pass in-memory indexing, an algorithm that has even better scaling properties because it does not hold the vocabulary in memory.
For very large collections like the web, indexing has to be distributed over computer clusters with hundreds or thousands of machines.
Collections with frequent changes require dynamic indexing introduced in Section 4.5 so that changes in the collection are immediately reflected in the index.
Finally, we cover some complicating issues that can arise in indexing – such as security and indexes for ranked retrieval – in Section 4.6
Index construction interacts with several topics covered in other chapters.
The indexer needs raw text, but documents are encoded in many ways (see Chapter 2)
Indexers compress and decompress intermediate files and the final index (see Chapter 5)
In web search, documents are not on a local file system, but have to be spidered or crawled (see Chapter 20)
In enterprise search, most documents are encapsulated in varied content management systems, email applications, and databases.
Although most of these applications can be accessed via http, native Application Programming Interfaces (APIs) are usually more efficient.
The reader should be aware that building the subsystem that feeds raw text to the indexing process can in itself be a challenging problem.
The seek time is the time needed to position the disk head in a new position.
The transfer time per byte is the rate of transfer from disk to memory when the head is in the right position.
When building an information retrieval (IR) system, many decisions are based on the characteristics of the computer hardware on which the system runs.
We therefore begin this chapter with a brief review of computer hardware.
A list of hardware basics that we need in this book to motivate IR system design follows.
Thus, reading a single byte from disk can take as much time as reading the entire block.
We call the part of main memory where a block being read or written is stored a buffer.BUFFER.
Data transfers from disk to memory are handled by the system bus, not by the processor.
This means that the processor is available to process data during disk I/O.
We can exploit this fact to speed up data transfers by storing compressed data on disk.
Assuming an efficient decompression algorithm, the total time of reading and then decompressing compressed data is usually less than reading uncompressed data.
Servers used in IR systems typically have several gigabytes (GB) of main memory, sometimes tens of GB.
We first make a pass through the collection assembling all term–docID pairs.
We then sort the pairs with the term as the dominant key and docID as the secondary key.
Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency.
For small collections, all this can be done in memory.
In this chapter, we describe methods for large collections that require the use of secondary storage.
To make index construction more efficient, we represent terms as termIDs (instead of strings as we did in Figure 1.4), where each termID is a uniqueTERMID serial number.
We can build the mapping from terms to termIDs on the fly while we are processing the collection; or, in a two-pass approach, we compile the vocabulary in the first pass and construct the inverted index in the second pass.
The index construction algorithms described in this chapter all do a single pass through the data.
Section 4.7 gives references to multipass algorithms that are preferable in certain applications, for example, when disk space is scarce.
A typical document is shown in Figure 4.1, but note that we ignore multimedia information like images in this book and are only concerned with text.
Reuters-RCV1 covers a wide range of international topics, including politics, business, sports, and (as in this example) science.
Some key statistics of the collection are shown in Table 4.2
Typical collections today are often one or two orders of magnitude larger than Reuters-RCV1
You can easily see how such collections overwhelm even large computers if we try to sort their termID–docID pairs in memory.
If the size of the intermediate files during index construction is within a small factor of available memory, then the compression techniques introduced in Chapter 5 can help; however, the postings file of many large collections cannot fit into memory even after compression.
With main memory insufficient, we need to use an external sorting algo-EXTERNAL SORTING ALGORITHM rithm, that is, one that uses disk.
The algorithm parses documents into termID–docID pairs and accumulates the pairs in memory until a block of a fixed size is full (PARSENEXTBLOCK in Figure 4.2)
We choose the block size to fit comfortably into memory to permit a fast in-memory sort.
Next,INVERSION we collect all termID–docID pairs with the same termID into a postings list, where a posting is simply a docID.
The result, an inverted index for the blockPOSTING we have just read, is then written to disk.
In the final step, the algorithm simultaneously merges the ten blocks into one large merged index.
An example with two blocks is shown in Figure 4.3, where we use di to denote the ith document of the collection.
To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing.
In each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure.
All postings lists for this termID are read and merged, and the merged list is written back to disk.
Each read buffer is refilled from its file when necessary.
Two blocks (“postings lists to be merged”) are loaded from disk into memory, merged in memory (“merged postings lists”) and written back to disk.
Notice that Reuters-RCV1 is not particularly large in an age when one or more GB of memory are standard on personal computers.
The techniques we have described are needed, however, for collections that are several orders of magnitude larger.
How would you create the dictionary in blocked sort-based indexing on the fly to avoid an extra pass through the data?
Figure 4.4 Inversion of a block in single-pass in-memory indexing.
Blocked sort-based indexing has excellent scaling properties, but it needs a data structure for mapping terms to termIDs.
For very large collections, this data structure does not fit into memory.
A more scalable alternative is single-pass in-memory indexing or SPIMI.
IN-MEMORY INDEXING writes each block’s dictionary to disk, and then starts a new dictionary for the next block.
The part of the algorithm that parses documents and turns them into a stream of term–docID pairs, which we call tokens here, has been omitted.
SPIMI-INVERT is called repeatedly on the token stream until the entire collection has been processed.
Tokens are processed one by one (line 4) during each successive call of SPIMI-INVERT.
When a term occurs for the first time, it is added to the dictionary (best implemented as a hash), and a new postings list is created (line 6)
The call in line 7 returns this postings list for subsequent occurrences of the term.
A difference between BSBI and SPIMI is that SPIMI adds a posting directly to its postings list (line 10)
Instead of first collecting all termID–docID pairs and then sorting them (as we did in BSBI), each postings list is dynamic (i.e., its size is adjusted as it grows) and it is immediately available to collect postings.
This has two advantages: It is faster because there is no sorting required, and it saves memory because we keep track of the term a postings.
As a result, the blocks that individual calls of SPIMI-INVERT can process are much larger and the index construction process as a whole is more efficient.
Because we do not know how large the postings list of a term will be when we first encounter it, we allocate space for a short postings list initially and double the space each time it is full (lines 8–9)
This means that some memory is wasted, which counteracts the memory savings from the omission of termIDs in intermediate data structures.
However, the overall memory requirements for the dynamically constructed index of a block in SPIMI are still lower than in BSBI.
When memory has been exhausted, we write the index of the block (which consists of the dictionary and the postings lists) to disk (line 12)
We have to sort the terms (line 11) before doing this because we want to write postings lists in lexicographic order to facilitate the final merging step.
If each block’s postings lists were written in unsorted order, merging blocks could not be accomplished by a simple linear scan through each block.
Each call of SPIMI-INVERT writes a block to disk, just as in BSBI.
In addition to constructing a new dictionary structure for each block and eliminating the expensive sorting step, SPIMI has a third important component: compression.
Both the postings and the dictionary terms can be stored compactly on disk if we employ compression.
Compression increases the efficiency of the algorithm further because we can process even larger blocks, and because the individual blocks require less space on disk.
We refer readers to the literature for this aspect of the algorithm (Section 4.7)
Collections are often so large that we cannot perform index construction efficiently on a single machine.
This is particularly true of the World Wide Web for which we need large computer clusters1 to construct any reasonably sized web index.
Web search engines, therefore, use distributed indexing algorithms for index construction.
The result of the construction process is a distributed index that is partitioned across several machines – either according to term or according to document.
In this section, we describe distributed indexing for a term-partitioned index.
A cluster in this chapter is a group of tightly coupled computers that work together closely.
This sense of the word is different from the use of cluster as a group of documents that are semantically similar in Chapters 16–18
The point of a cluster is to solve large computing problems on cheap commodity machines or nodes that are built from standard parts (processor, memory, disk) as opposed to on a supercomputer with specialized hardware.
Although hundreds or thousands of machines are available in such clusters, individual machines can fail at any time.
One requirement for robust distributed indexing is, therefore, that we divide the work up into chunks that we can easily assign and – in case of failure – reassign.
A master node directs the process of assigningMASTER NODE and reassigning tasks to individual worker nodes.
The map phase of MapReduce consists of mapping splits of the input dataMAP PHASE to key-value pairs.
This is the same parsing task we also encountered in BSBI and SPIMI, and we therefore call the machines that execute the map phase parsers.
Each parser writes its output to local intermediate files, the segmentPARSER.
For the reduce phase, we want all values for a given key to be stored closeREDUCE PHASE together, so that they can be read and processed quickly.
In general, key ranges need not correspond to contiguous terms or termIDs.
The term partitions are defined by the person who operates the indexing system (Exercise 4.10)
The parsers then write corresponding segment files, one for each term partition.
Each term partition thus corresponds to r segments files, where r is the number of parsers.
For instance, Figure 4.5 shows three a–f segment files of the a–f partition, corresponding to the three parsers shown in the figure.
Collecting all values (here: docIDs) for a given key (here: termID) into one list is the task of the inverters in the reduce phase.
The master assigns eachINVERTER term partition to a different inverter – and, as in the case of parsers, reassigns term partitions in case of failing or slow inverters.
Each term partition (corresponding to r segment files, one on each parser) is processed by one inverter.
We assume here that segment files are of a size that a single machine can handle (Exercise 4.9)
Finally, the list of values is sorted for each key and written to the final sorted postings list (“postings” in the figure)
Note that postings in Figure 4.6 include term frequencies, whereas each posting in the other sections of this chapter is simply a docID without term frequency information.
The data flow is shown for a–f in Figure 4.5
In general, the map function produces a list of key-value pairs.
All values for a key are collected into one list in the reduce phase.
The instantiations of the two functions and an example are shown for index construction.
Because the map phase processes documents in a distributed fashion, termID–docID pairs need not be ordered correctly initially as in this example.
The example shows terms instead of termIDs for better readability.
The master identifies idle machines and assigns tasks to them.
The same machine can be a parser in the map phase and an inverter in the reduce phase.
And there are often other jobs that run in parallel with index construction, so in between being a parser and an inverter a machine might do some crawling or another unrelated task.
To minimize write times before inverters reduce the data, each parser writes its segment files to its local disk.
In the reduce phase, the master communicates to an inverter the locations of the relevant segment files (e.g., of the r segment files of the a–f partition)
Each segment file only requires one sequential read because all data relevant to a particular inverter were written to a single segment file by the parser.
This setup minimizes the amount of network traffic needed during indexing.
Figure 4.6 shows the general schema of the MapReduce functions.
Input and output are often lists of key-value pairs themselves, so that several MapReduce jobs can run in sequence.
What we describe in this section corresponds to only one of five to ten MapReduce operations in that indexing system.
MapReduce offers a robust and conceptually simple framework for implementing index construction in a distributed environment.
By providing a semiautomatic method for splitting index construction into smaller tasks, it can scale to almost arbitrarily large collections, given computer clusters of.
Thus far, we have assumed that the document collection is static.
This is fine for collections that change infrequently or never (e.g., the Bible or Shakespeare)
But most collections are modified frequently with documents being added, deleted, and updated.
This means that new terms need to be added to the dictionary, and postings lists need to be updated for existing terms.
The simplest way to achieve this is to periodically reconstruct the index from scratch.
This is a good solution if the number of changes over time is small and a delay in making new documents searchable is acceptable – and if enough resources are available to construct a new index while the old one is still available for querying.
If there is a requirement that new documents be included quickly, one solution is to maintain two indexes: a large main index and a small auxiliary indexAUXILIARY INDEX that stores new documents.
We can then filter out deleted documents before returning the search result.
Each time the auxiliary index becomes too large, we merge it into the main index.
The cost of this merging operation depends on how we store the index in the file system.
If we store each postings list as a separate file, then the merge simply consists of extending each postings list of the main index by the corresponding postings list of the auxiliary index.
In this scheme, the reason for keeping the auxiliary index is to reduce the number of disk seeks required over time.
Updating each document separately requires up to Mave disk seeks, where Mave is the average size of the vocabulary of documents in the collection.
With an auxiliary index, we only put additional load on the disk when we merge auxiliary and main indexes.
The simplest alternative is to store the index as one large file, that is, as a concatenation of all postings lists.
In reality, we often choose a compromise between the two extremes (Section 4.7)
To simplify the discussion, we choose the simple option of storing the index as one large file here.
Each token (termID,docID) is initially added to in-memory index Z0 by LMERGEADDTOKEN.
With multiple indexes and an invalidation bit vector, the correct number of hits for a term is no longer a simple lookup.
In fact, all aspects of an IR system – index maintenance, query processing, distribution, and so on – are more complex in logarithmic merging.
Query processing is then switched from the new index and the old index is deleted.
With this change, the algorithms discussed here can all be applied to positional indexes.
In the indexes we have considered so far, postings lists are ordered with respect to docID.
As we see in Chapter 5, this is advantageous for compresOnline edition (c)
During query processing, a user’s access postings list is intersected with the results list returned by the text part of the index.
In ranked retrieval, postings are often ordered ac-RETRIEVAL SYSTEMS cording to weight or impact, with the highest-weighted postings occurring first.
With this organization, scanning of long postings lists during query processing can usually be terminated early when weights have become so small that any further documents can be predicted to be of low similarity to the query (see Chapter 6)
In a docID-sorted index, new documents are always inserted at the end of postings lists.
A low-level employee should not be able to find the salary roster of the corporation, but authorized managers need to be able to search for it.
Users’ results lists must not contain documents they are barred from opening; the very existence of a document can be sensitive information.
User authorization is often mediated through access control lists or ACLs.ACCESS CONTROL LISTS ACLs can be dealt with in an information retrieval system by representing each document as the set of users that can access them (Figure 4.8) and then inverting the resulting user-document matrix.
The inverted ACL index has, for each user, a “postings list” of documents they can access – the user’s access list.
However, such an index is difficult to maintain when access permissions change – we discussed these difficulties in the context of incremental indexing for regular postings lists in Section 4.5
It also requires the processing of very long postings lists for users with access to large document subsets.
User membership is therefore often verified by retrieving access information directly from the file system at query time – even though this slows down retrieval.
Exercise 4.5Can spelling correction compromise document-level security? Consider the case where a spelling correction is based on documents to which the user does not have access.
Choose a block size that is realistic for current technology (remember that a block should easily fit into main memory)
For this collection, compare memory, disk and time requirements of the simple algorithm in Figure 1.4 and blocked sort-based indexing.
Assume that machines in MapReduce have 100 GB of disk space each.
Assume further that the postings list of the term the has a size of 200 GB.
Then the MapReduce algorithm as described cannot be run to construct the index.
How would you modify MapReduce so that it can handle this case?
For optimal load balancing, the inverters in MapReduce must get segmented postings files of similar sizes.
For a new collection, the distribution of key-value pairs may not be known in advance.
Apply MapReduce to the problem of counting how often each term occurs in a set of files.
Write down an example along the lines of Figure 4.6
We claimed (on page 80) that an auxiliary index can impair the quality of collection statistics.
Show that even a small auxiliary index can cause significant error in idf when it is computed on the main index only.
Consider a rare term that suddenly occurs frequently (e.g., Flossie as in Tropical Storm Flossie)
In general, blocked sort-based indexing does well on all three counts.
However, if conserving memory or disk space is the main criterion, then other algorithms may be a better choice.
We have simplified several aspects of the algorithm, including compression and the fact that each term’s data structure also contains, in addition to the postings list, its document frequency and house keeping information.
The MapReduce architecture was introduced by Dean and Ghemawat (2004)
An open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/
An effective indexer for enterprise search needs to be able to communicate efficiently with a number of applications that hold text data in corporations, including Microsoft Outlook, IBM’s Lotus software, databases like Oracle and MySQL, content management systems like Open Text, and enterprise resource planning software like SAP.
Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR)
In this chapter, we employ a number of compression techniques for dictionary and inverted index that are essential for efficient IR systems.
Search systems use some parts of the dictionary and the index much more than others.
For example, if we cache the postings list of a frequently used query term t, then the computations necessary for responding to the one-term query t can be entirely done in memory.
With compression, we can fit a lot more information into main memory.
Instead of having to expend a disk seek when processing a query with t, we instead access its postings list in memory and decompress it.
As we will see below, there are simple and efficient decompression methods, so that the penalty of having to decompress the postings list is small.
As a result, we are able to decrease the response time of the IR system substantially.
Because memory is a more expensive resource than disk space, increased speed owing to caching – rather than decreased space requirements – is often the prime motivator for compression.
The second more subtle advantage of compression is faster transfer of data from disk to memory.
Efficient decompression algorithms run so fast on modern hardware that the total time of transferring a compressed chunk of data from disk and then decompressing it is usually less than transferring the same chunk of data in uncompressed form.
For instance, we can reduce input/output (I/O) time by loading a much smaller compressed postings list, even when you add on the cost of decompression.
So, in most cases, the retrieval system runs faster on compressed postings lists than on uncompressed postings lists.
If the main goal of compression is to conserve disk space, then the speed.
But for improved cache utilization and faster disk-to-memory transfer, decompression speeds must be high.
The compression algorithms we discuss in this chapter are highly efficient and can therefore serve all three purposes of index compression.
In this chapter, we define a posting as a docID in a postings list.
See Section 5.4 for references on compressing frequencies and positions.
The table shows the number of terms for different levels of preprocessing (column 2)
The number of terms is the main factor in determining the size of the dictionary.
The number of nonpositional postings (column 3) is an indicator of the expected size of the nonpositional index of the collection.
The expected size of a positional index is related to the number of positions it must encode (column 4)
In general, the statistics in Table 5.1 show that preprocessing affects the size of the dictionary and the number of nonpositional postings greatly.
The treatment of the most frequent words is also important.
But, although a stop list of 150 words reduces the number of postings by a quarter or more, this size reduction does not carry over to the size of the compressed index.
As we will see later in this chapter, the postings lists of frequent words require only a few bits per posting after compression.
The deltas in the table are in a range typical of large collections.
For example, for a collection of web pages with a high proportion of French text, a lemmatizer for French reduces vocabulary size much more than the Porter stemmer does for an English-only collection because French is a morphologically richer language than English.
The compression techniques we describe in the remainder of this chapter are lossless, that is, all information is preserved.
Lossy compression makes sense when the “lost” information is unlikely ever to be used by the search system.
For example, web search is characterized by a large number of documents, short queries, and users who only look at the first few pages of results.
As a consequence, we can discard postings of documents that would only be used for hits far down the list.
Thus, there are retrieval scenarios where lossy methods can be used for compression without any reduction in effectiveness.
Before introducing techniques for compressing the dictionary, we want to estimate the number of distinct terms M in a collection.
It is sometimes said that languages have a vocabulary of a certain size.
The second edition of the Oxford English Dictionary (OED) defines more than 600,000 words.
But the vocabulary of most large collections is much larger than the OED.
The OED does not include most names of people, locations, products, or scientific.
These names need to be included in the inverted index, so our users can search for them.
A better way of getting a handle on M is Heaps’ law, which estimates vocab-HEAPS’ LAW ulary size as a function of collection size:
The parameter k is quite variable because vocabulary growth depends a lot on the nature of the collection and how it is processed.
Case-folding and stemming reduce the growth rate of the vocabulary, whereas including numbers and spelling errors increase it.
Regardless of the values of the parameters for a particular collection, Heaps’ law suggests that (i) the dictionary size continues to increase with more documents in the collection, rather than a maximum vocabulary size being reached, and (ii) the size of the dictionary is quite large for large collections.
These two hypotheses have been empirically shown to be true of large text collections (Section 5.4)
So dictionary compression is important for an effective information retrieval system.
We also want to understand how terms are distributed across documents.
This helps us to characterize the properties of the algorithms for compressing postings lists in Section 5.3
A commonly used model of the distribution of terms in a collection is Zipf’sZIPF’S LAW law.
So if the most frequent term occurs cf1 times, then the second most frequent term has half as many occurrences, the third most frequent term a third as many occurrences, and so on.
The intuition is that frequency decreases very rapidly with rank.
Equation (5.2) is one of the simplest ways of formalizing such a rapid decrease and it has been found to be a reasonably good model.
Frequency is plotted as a function of frequency rank for the terms in the collection.
The line is the distribution predicted by Zipf’s law (weighted least-squares fit; intercept is 6.95)
This section presents a series of dictionary data structures that achieve increasingly higher compression ratios.
The dictionary is small compared with the postings file as suggested by Table 5.1
So why compress it if it is responsible for only a small percentage of the overall space requirements of the IR system?
One of the primary factors in determining the response time of an IR system is the number of disk seeks necessary to process a query.
If parts of the dictionary are on disk, then many more disk seeks are necessary in query evaluation.
Thus, the main goal of compressing the dictionary is to fit it in main memory, or at least a large portion of it, to support high query throughOnline edition (c)
Figure 5.3 Storing the dictionary as an array of fixed-width entries.
Although dictionaries of very large collections fit into the memory of a standard desktop machine, this is not true of many other application scenarios.
For example, an enterprise search server for a large corporation may have to index a multiterabyte collection with a comparatively large vocabulary because of the presence of documents in many different languages.
We also want to be able to design search systems for limited hardware such as mobile phones and onboard computers.
Other reasons for wanting to conserve memory are fast startup time and having to share resources with other applications.
The search system on your PC must get along with the memory-hogging word processing suite you are using at the same time.
We can overcome these shortcomings by storing the dictionary terms as one long string of characters, as shown in Figure 5.4
The pointer to the next term is also used to demarcate the end of the current term.
As before, we locate terms in the data structure by way of binary search in the (now smaller) table.
Pointers mark the end of the preceding term and the beginning of the next.
For example, the first three terms in this example are systile, syzygetic, and syzygial.
The first block consists of systile, syzygetic, syzygial, and syzygy with lengths of seven, nine, eight, and six characters, respectively.
Each term is preceded by a byte encoding its length that indicates how many bytes to skip to reach subsequent terms.
One source of redundancy in the dictionary we have not exploited yet is the fact that consecutive entries in an alphabetically sorted list share common prefixes.
In the case of Reuters, front coding saves another 1.2 MB, as we found in an experiment.
However, we cannot adapt perfect hashes incrementally because each new term causes a collision and therefore requires the creation of a new perfect hash function.
Even with the best compression scheme, it may not be feasible to store the entire dictionary in main memory for very large text collections and for hardware with limited memory.
If we have to partition the dictionary onto pages that are stored on disk, then we can index the first term of each page using a B-tree.
For processing most queries, the search system has to go to disk anyway to fetch the postings.
One additional seek for retrieving the term’s dictionary page from disk is a significant, but tolerable increase in the time it takes to process a query.
Table 5.2 summarizes the compression achieved by the four dictionary data structures.
The first docID is left unchanged (only shown for arachnocentric)
To devise a more efficient representation of the postings file, one that uses fewer than 20 bits per document, we observe that the postings for frequent terms are close together.
Imagine going through the documents of a collection one by one and looking for a frequent term like computer.
We will find a document containing computer, then we skip a few documents that do not contain it, then there is again a document with the term and so on (see Table 5.3)
The key idea is that the gaps between postings are short, requiring a lot less space than 20 bits to store.
For an economical representation of this distribution of gaps, we need a variable encoding method that uses fewer bits for short gaps.
To encode small numbers in less space than large numbers, we look at two types of methods: bytewise compression and bitwise compression.
As the names suggest, these methods attempt to encode gaps with the minimum number of bytes and bits, respectively.
Variable byte (VB) encoding uses an integral number of bytes to encode a gap.VARIABLE BYTE ENCODING The last 7 bits of a byte are “payload” and encode part of the gap.
Larger wordsNIBBLE further decrease the amount of bit manipulation necessary at the cost of less effective (or no) compression.
Word sizes smaller than bytes get even better compression ratios at the cost of more bit manipulation.
In general, bytes offer a good compromise between compression ratio and speed of decompression.
Obviously, this is not a very efficient code, but it will come in handy in a moment.
How efficient can a code be in principle? Assuming the 2n gaps G with G.
Our goal is to get as close to this lower bound as possible.
The characteristic of a discrete probability distribution3 P that determines its coding properties (including whether a code is optimal) is its entropy H(P),ENTROPY which is defined as follows:
For instance, the parameters need to be stored and retrieved.
And in dynamic indexing, the distribution of gaps can change, so that the original parameters are no longer appropriate.
We can choose a different constant c such that the fractions c/i are relative frequencies and sum to 1 (that is, c/i = cfi/T):
Thus the ith term has a relative frequency of roughly 1/(13i), and the expected average number of occurrences of term i in a document of length L is:
Context should make clear which is meant in this chapter.
Based on these observations: (i) Suggest a modification to variable byte encoding that allows you to encode slightly larger gaps in the same amount of space.
Go through the above calculation of index size and explicitly state all the approximations that were made to arrive at Equation (5.6)
Dictionary compression is covered in detail by Witten et al.
The distribution of gaps in a postings list depends on the assignment of docIDs to documents.
These techniques assign docIDs in a small range to documents in a cluster where a cluster can consist of all documents in a given time period, on a particular web site, or sharing another property.
As a result, when a sequence of documents from a cluster occurs in a postings list, their gaps are small and can be more effectively compressed.
Different considerations apply to the compression of term frequencies and word positions than to the compression of docIDs in postings lists.
Zobel and Moffat (2006) is recommended in general as an in-depth and up-to-date tutorial on inverted.
This chapter only looks at index compression for Boolean retrieval.
During query processing, the scanning of many postings lists can then be terminated early because smaller weights do not change the ranking of the highest ranked k documents found so far.
Document compression can also be important in an efficient information retrieval system.
Unary code is not a universal code in the sense defined above.
However, there exists a distribution over gaps for which unary code is optimal.
Use the numbers in Table 4.2, but do not round Lc, c, and the number of vocabulary blocks.
Thus far we have dealt with indexes that support Boolean queries: a document either matches or does not match a query.
In the case of large document collections, the resulting number of matching documents can far exceed the number a human user could possibly sift through.
Accordingly, it is essential for a search engine to rank-order the documents matching a query.
To do this, the search engine computes, for each matching document, a score with respect to the query at hand.
In this chapter we initiate the study of assigning a score to a (query, document) pair.
We introduce parametric and zone indexes in Section 6.1, which serve two purposes.
First, they allow us to index and retrieve documents by metadata such as the language in which a document is written.
Second, they give us a simple means for scoring (and thereby ranking) documents in response to a query.
Next, in Section 6.2 we develop the idea of weighting the importance of a term in a document, based on the statistics of occurrence of the term.
In Section 6.3 we show that by viewing each document as a vector of such weights, we can compute a score between a query and each document.
Section 6.4 develops several variants of term-weighting for the vector space model.
Chapter 7 develops computational aspects of vector space scoring, and related topics.
As we develop these ideas, the notion of a query will assume multiple nuances.
In Section 6.1 we consider queries in which specific query terms occur in specified regions of a matching document.
Beginning Section 6.2 we will in fact relax the requirement of matching specific regions of a document; instead, we will look at so-called free text queries that simply consist of query terms with no specification on their relative order, importance or where in a document they should be found.
The bulk of our study of scoring will be in this latter notion of a query being such a set of terms.
We have thus far viewed a document as a sequence of terms.
By metadata, we mean specific forms of data about a document, such as its author(s), title and date of publication.
This metadata would generally include fields such as the date of creation and the format of the document, asFIELD well the author and possibly the title of the document.
The possible values of a field should be thought of as finite – for instance, the set of all dates of authorship.
Whereas a field may take on a relatively small set of values, a zone can be thought of as an arbitrary, unbounded amount of text.
For instance, document titles and abstracts are generally treated as zones.
We may build a separate inverted index for each zone of a document, to support queries such as “find documents with merchant in the title and william in the author list and the phrase gentle rain in the body”
This has the effect of building an index that looks like Figure 6.2
Whereas the dictionary for a parametric index comes from a fixed vocabulary (the set of languages, or the set of dates), the dictionary for a zone index must structure whatever vocabulary stems from the text of that zone.
In fact, we can reduce the size of the dictionary by encoding the zone in which a term occurs in the postings.
In Figure 6.3 for instance, we show how occurrences of william in the title and author zones of various documents are encoded.
Such an encoding is useful when the size of the dictionary is a concern (because we require the dictionary to fit in main memory)
But there is another important reason why the encoding of Figure 6.3 is useful: the efficient computation of scores using a technique we will call weighted zoneWEIGHTED ZONE.
In this example we have a collection with fields allowing us to select publications by zones such as Author and fields such as Language.
Thus far in Section 6.1 we have focused on retrieving documents based on Boolean queries on fields and zones.
We now turn to a second application of zones and fields.
Weighted zone scoring is sometimes referred to also as ranked Boolean re-RANKED BOOLEAN RETRIEVAL trieval.
Thus if the term shakespeare were to appear in the title and body zones but not the author zone of a document, the score of this document would be 0.8
How do we implement the computation of weighted zone scores? A simple approach would be to compute the score for each document in turn, adding in all the contributions from the various zones.
However, we now show how we may compute weighted zone scores directly from inverted indexes.
Following the description of the algorithm, we describe the extension to more complex queries and Boolean functions.
The reader may have noticed the close similarity between this algorithm and that in Figure 1.6
Indeed, they represent the same postings traversal, except that instead of merely adding a document to the set of results for.
Figure 6.4 Algorithm for computing the weighted zone score from two postings lists.
Function WEIGHTEDZONE (not shown here) is assumed to compute the inner loop of Equation 6.1
Some literature refers to the array scores[] above as a set of accumulators.
TheACCUMULATOR reason for this will be clear as we consider more complex Boolean functions than the AND; thus we may assign a non-zero score to a document even if it does not contain all query terms.
In the simplest form, each relevance judgments is either Relevant or Non-relevant.
More sophisticated implementations of the methodology make use of more nuanced judgments.
The weights gi are then “learned” from these examples, in order that the learned scores approximate the relevance judgments in the training examples.
For weighted zone scoring, the process may be viewed as learning a linear function of the Boolean match scores contributed by the various zones.
The expensive component of this methodology is the labor-intensive assembly of user-generated relevance judgments from which to learn the weights, especially in a collection that changes frequently (such as the Web)
We now detail a simple example that illustrates how we can reduce the problem of learning the weights gi to a simple optimization problem.
Figure 6.6 The four possible combinations of sT and sB.
Then, the total error of a set of training examples is given by.
The problem of learning the constant g from the given training examples then reduces to picking the value of g that minimizes the total error in (6.4)
By differentiating Equation (6.5) with respect to g and setting the result to zero, it follows that the optimal value of g is.
Exercise 6.1When using weighted zone scoring, is it necessary for all zones to use the same Boolean match function?
Rewrite the algorithm in Figure 6.4 to the case of more than two query terms.
Write pseudocode for the function WeightedZone for the case of two postings lists in Figure 6.4
For the value of g estimated in Exercise 6.5, compute the weighted zone score for each (query, document) example.
Why does the expression for g in (6.6) not involve training examples in which sT(dt, qt) and sB(dt, qt) have the same value?
Thus far, scoring has hinged on whether or not a query term is present in a zone within a document.
We take the next logical step: a document or zone that mentions a query term more often has more to do with that query and therefore should receive a higher score.
To motivate this, we recall the notion of a free text query introduced in Section 1.4: a query in which the terms of the query are typed freeform into the search interface, without any connecting search operators (such as Boolean operators)
This query style, which is extremely popular on the web, views the query as simply a set of words.
A plausible scoring mechanism then is to compute a score that is the sum, over the query terms, of the match scores between each query term and the document.
Towards this end, we assign to each term in a document a weight for that term, that depends on the number of occurrences of the term in the document.
We would like to compute a score between a query term t and a document d, based on the weight of t in d.
The simplest approach is to assign the weight to be equal to the number of occurrences of term t in document d.
This weighting scheme is referred to as term frequency and is denoted tft,d,TERM FREQUENCY with the subscripts denoting the term and the document in order.
For a document d, the set of weights determined by the tf weights above (or indeed any weighting function that maps the number of occurrences of t in d to a positive real value) may be viewed as a quantitative digest of that document.
In this view of a document, known in the literature as the bagBAG OF WORDS of words model, the exact ordering of the terms in a document is ignored but the number of occurrences of each term is material (in contrast to Boolean retrieval)
We only retain information on the number of occurrences of each term.
Thus, the document “Mary is quicker than John” is, in this view, identical to the document “John is quicker than Mary”
Nevertheless, it seems intuitive that two documents with similar bag of words representations are similar in content.
Raw term frequency as above suffers from a critical problem: all terms are considered equally important when it comes to assessing relevancy on a query.
In fact certain terms have little or no discriminating power in determining relevance.
For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document.
Figure 6.7 Collection frequency (cf) and document frequency (df) behave differently, as in this example from the Reuters collection.
An immediate idea is to scale down the term weights of terms with high collection frequency, defined to be the total number of occurrences of a term in the collection.
The idea would be to reduce the tf weight of a term by a factor that grows with its collection frequency.
Instead, it is more commonplace to use for this purpose the document fre-DOCUMENT FREQUENCY quency dft, defined to be the number of documents in the collection that contain a term t.
This is because in trying to discriminate between documents for the purpose of scoring it is better to use a document-level statistic (such as the number of documents containing a term) than to use a collection-wide statistic for the term.
The reason to prefer df to cf is illustrated in Figure 6.7, where a simple example shows that collection frequency (cf) and document frequency (df) can behave rather differently.
In particular, the cf values for both try and insurance are roughly equal, but their df values differ significantly.
Intuitively, we want the few documents that contain insurance to get a higher boost for a query on insurance than the many documents containing try get from a query on try.
How is the document frequency df of a term used to scale its weight? Denoting as usual the total number of documents in a collection by N, we define the inverse document frequency (idf) of a term t as follows:INVERSE DOCUMENT.
Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low.
In fact, as we will see in Exercise 6.12, the precise base of the logarithm is not material to ranking.
We now combine the definitions of term frequency and inverse document frequency, to produce a composite weight for each term in each document.
Here we give the idf’s of terms with various frequencies in the Reuters collection of 806,791 documents.
The tf-idf weighting scheme assigns to term t a weight in document d givenTF-IDF by.
At this point, we may view each document as a vector with one componentDOCUMENT VECTOR corresponding to each term in the dictionary, together with a weight for each component that is given by (6.8)
For dictionary terms that do not occur in a document, this weight is zero.
This vector form will prove to be crucial to scoring and ranking; we will develop these ideas in Section 6.3
As a first step, we introduce the overlap score measure: the score of a document d is the sum, over all query terms, of the number of times each of the query terms occurs in d.
We can refine this idea so that we add up not the number of occurrences of each query term t in d, but instead the tf-idf weight of each term in d.
What is the idf of a term that occurs in every document? Compare this with the use of stop word lists.
Compute the tf-idf weights for the terms car, auto, insurance, best, for each document, using the idf values from Figure 6.8
Can the tf-idf weight of a term in a document exceed 1?
The representation of a set of documents as vectors in a common vector space is known as the vector space model and is fundamental to a host of information retrieval op-VECTOR SPACE MODEL erations ranging from scoring documents on a query, document classification and document clustering.
We first develop the basic ideas underlying vector space scoring; a pivotal step in this development is the view (Section 6.3.2) of queries as vectors in the same vector space as the document collection.
We denote by ~V(d) the vector derived from document d, with one component in the vector for each dictionary term.
Unless otherwise specified, the reader may assume that the components are computed using the tf-idf weighting scheme, although the particular weighting scheme is immaterial to the discussion that follows.
The set of documents in a collection then may be viewed as a set of vectors in a vector space, in which there is one axis for.
How do we quantify the similarity between two documents in this vector space? A first attempt might consider the magnitude of the vector difference between two document vectors.
This measure suffers from a drawback: two documents with very similar content can have a significant vector difference simply because one is much longer than the other.
Thus the relative distributions of terms may be identical in the two documents, but the absolute term frequencies of one may be far larger.
Let ~V(d) denote the document vector for d, with M components.
The resulting Euclidean normalized tf values for these documents are shown in Figure 6.11
These are based on raw term frequency only and are normalized as if these were the only terms in the collection.
Since affection and jealous occur in all three documents, their tf-idf weight would be 0 in most formulations.
As always, the terms being indexed could be stemmed before indexing; for instance, jealous and jealousy would under stemming be considered as a single dimension.
There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector.
The key idea now: to assign to each document d a score equal to the dot product.
This simple example is somewhat misleading: the number of dimenOnline edition (c)
To summarize, by viewing a query as a “bag of words”, we are able to treat it as a very short document.
As a consequence, we can use the cosine similarity between the query vector and a document vector as a measure of the score of the document for that query.
The resulting scores can then be used to select the top-scoring documents for a query.
A document may have a high cosine score for a query even if it does not contain all query terms.
Note that the preceding discussion does not hinge on any specific weighting of terms in the document vector, although for the present we may think of them as either tf or tf-idf weights.
Computing the cosine similarities between the query vector and each document vector in the collection, sorting the resulting scores and selecting the top K documents can be expensive — a single similarity computation can entail a dot product in tens of thousands of dimensions, demanding tens of thousands of arithmetic operations.
In Section 7.1 we study how to use an inverted index for this purpose, followed by a series of heuristics for improving on this.
In a typical setting we have a collection of documents each represented by a vector, a free text query represented by a vector, and a positive integer K.
Figure 6.14 The basic algorithm for computing vector space scores.
We now initiate the study of determining the K documents with the highest vector space scores for a query.
Typically, we seek these K top documents in ordered by decreasing score; for instance many search engines use K = 10 to retrieve and rank-order the first page of the ten best results.
Figure 6.14 gives the basic algorithm for computing vector space scores.
The array Length holds the lengths (normalization factors) for each of the N documents, whereas the array Scores holds the scores for each of the documents.
The outermost loop beginning Step 3 repeats the updating of Scores, iterating over each query term t in turn.
In Step 5 we calculate the weight in the query vector for term t.
Steps 6-8 update the score of each document by adding in the contribution from term t.
For this purpose, it would appear necessary to store, withACCUMULATOR each postings entry, the weight wft,d of term t in document d (we have thus far used either tf or tf-idf for this weight, but leave open the possibility of other functions to be developed in Section 6.4)
In fact this is wasteful, since storing this weight may require a floating point number.
First, if we are using inverse document frequency, we need not precompute idft; it suffices to store N/dft at the head of the postings for t.
Second, we store the term frequency tft,d for each postings entry.
Finally, Step 12 extracts the top K scores – this requires a priority queue.
Such a heap takes no more than 2N comparisons to construct, following which each of the K top scores can be extracted from the heap at a cost of O(log N) comparisons.
In such a concurrent postings traversal we compute the scores of one document at a time, so that it is sometimes called document-at-a-time scoring.
Exercise 6.14If we were to stem jealous and jealousy to a common stem before setting up the vector space, detail how the definitions of tf and idf should be modified.
Compute the Euclidean normalized document vectors for each of the documents, where each vector has four components, one for each of the four terms.
With term weights as computed in Exercise 6.15, rank the three documents by computed score for the query car insurance, for each of the following cases of term weighting in the query:
For assigning a weight for each term in each document, a number of alternatives to tf and tf-idf have been considered.
It seems unlikely that twenty occurrences of a term in a document truly carry twenty times the significance of a single occurrence.
Accordingly, there has been considerable research into variants of term frequency that go beyond counting the number of occurrences of a term.
In this form, we may replace tf by some other function wf as in (6.13), to obtain:
The method is unstable in the following sense: a change in the stop word list can dramatically alter term weightings (and therefore ranking)
A document may contain an outlier term with an unusually large number of occurrences of that term, not representative of the content of that document.
Here CharLength is the number of characters in the document.
More generally, a document in which the most frequent term appears roughly as often as many other terms should be treated differently from one with a more skewed distribution.
Equation (6.12) is fundamental to information retrieval systems that use any form of vector space scoring.
The mnemonic for representing a combination of weights takes the form ddd.qqq where the first triplet gives the term weighting of the document vector, while the second triplet gives the weighting in the query vector.
The first letter in each triplet specifies the term frequency component of the weighting, the second the document frequency component, and the third the form of normalization used.
For example, a very standard weighting scheme is lnc.ltc, where the document vector has log-weighted term frequency, no idf (for both effectiveness and efficiency reasons), and cosine normalization, while the query vector uses log-weighted term frequency, idf weighting, and cosine normalization.
In doing so, we eliminated all information on the length of the original document; this masks some subtleties about longer documents.
First, longer documents will – as a result of containing more terms – have higher tf values.
These factors can conspire to raise the scores of longer documents, which (at least for some information needs) is unnatural.
Longer documents can broadly be lumped into two categories: (1) verbose documents that essentially repeat the same content – in these, the length of the document does not alter the relative weights of different terms; (2) documents covering multiple different topics, in which the search terms probably match small segments of the document but not all of it – in this case, the relative weights of terms are quite different from a single short document that matches the query terms.
Compensating for this phenomenon is a form of document length normalization that is independent of term and document frequencies.
To this end, we introduce a form of normalizing the vector representations of documents in the collection, so that the resulting “normalized” documents are not necessarily of unit length.
Then, when we compute the dot product score between a (unit) query vector and such a normalized document, the score is skewed to account for the effect of document length on relevance.
Consider a document collection together with an ensemble of queries for that collection.
Suppose that we were given, for each query q and for each document d, a Boolean judgment of whether or not d is relevant to the query q; in Chapter 8 we will see how to procure such a set of relevance judgments for a query ensemble and a document collection.
Given this set of relevance judgments, we may compute a probability of relevance as a function of document length, averaged over all queries in the ensemble.
The resulting plot may look like the curve drawn in thick lines in Figure 6.16
To compute this curve, we bucket documents by length and compute the fraction of relevant documents in each bucket, then plot this fraction against the median document length of each bucket.
Thus even though the “curve” in Figure 6.16 appears to be continuous, it is in fact a histogram of discrete buckets of document length.
Figure 6.17 Implementing pivoted document length normalization by linear scaling.
It has been argued that in practice, Equation (6.16) is well approximated by.
Of course, pivoted document length normalization is not appropriate for all applications.
For instance, in a collection of answers to frequently asked questions (say, at a customer service website), relevance may have little to do with document length.
In other cases the dependency may be more complex than can be accounted for by a simple linear pivoted normalization.
In such cases, document length can be used as a feature in the machine learning based scoring approach of Section 6.1.2
Show that if q and the di are all normalized to unit vectors, then the rank ordering produced by Euclidean distance is identical to that produced by cosine similarities.
Compute the vector space similarity between the query “digital cameras” and the document “digital cameras and video cameras” by filling out the empty columns in Table 6.1
Assume N = 10,000,000, logarithmic term weighting (wf columns) for query and document, idf weighting for the query only and cosine normalization for the document only.
Show that for the query affection, the relative ordering of the scores of the three documents in Figure 6.13 is the reverse of the ordering of the scores for the query jealous gossip.
In turning a query into a unit vector in Figure 6.13, we assigned equal weights to each of the query terms.
Consider the case of a query term that is not in the set of M indexed terms; thus our standard construction of the query vector results in ~V(q) not being in the vector space created from the collection.
How would one adapt the vector space representation to handle this case?
Refer to the tf and idf values for four terms and three documents in Exercise 6.10
Compute the two top scoring documents on the query best car insurance for each of the following weighing schemes: (i) nnn.atc; (ii) ntc.atc.
How would one compute ntc.atc scores for the query coyote insurance?
We observed that by assigning a weight for each term in a document, a document may be viewed as a vector of term weights, one for each term in the collection.
The SMART information retrieval system at Cornell (Salton 1971b) due to Salton and colleagues was perhaps the first to view a document as a vector of weights.
The two query evaluation strategies term-at-a-time and document-at-a-time are discussed by Turtle and Flood (1995)
A more detailed and exhaustive notation was developed in Moffat and Zobel (1998), considering a larger palette of schemes for term and document frequency weighting.
Beyond the notation, Moffat and Zobel (1998) sought to set up a space of feasible weighting functions through which hillclimbing approaches could be used to begin with weighting schemes that performed well, then make local improvements to identify the best combinations.
However, they report that such hill-climbing methods failed to lead to any conclusions on the best weighting schemes.
In this chapter we begin in Section 7.1 with heuristics for speeding up this computation; many of these heuristics achieve their speed at the risk of not finding quite the top K documents matching the query.
With Section 7.1 in place, we have essentially all the components needed for a complete search engine.
We therefore take a step back from cosine scoring, to the more general problem of computing scores in a search engine.
In Section 7.2 we outline a complete search engine, including indexes and structures to support not only cosine scoring but also more general ranking factors such as query term proximity.
We describe how all of the various pieces fit together in Section 7.2.4
We conclude this chapter with Section 7.3, where we discuss how the vector space model for free text queries interacts with common query operators.
For a query such as q = jealous gossip, two observations are immediate:
In the absence of any weighting for query terms, these non-zero components are equal – in this case, both equal 0.707
For the purpose of ranking the documents matching this query, we are really interested in the relative (rather than absolute) scores of the documents in the collection.
Given these scores, the final step before presenting results to a user is to pick out the K highest-scoring documents.
While one could sort the complete set of scores, a better approach is to use a heap to retrieve only the top K documents in order.
Where J is the number of documents with non-zero cosine scores, constructing such a heap can be performed in 2J comparison steps, following which each of the K highest scoring documents can be “read off” the heap with log J comparison steps.
Thus far, we have focused on retrieving precisely the K highest-scoring documents for a query.
We now consider schemes by which we produce K documents that are likely to be among the K highest scoring documents for a query.
In doing so, we hope to dramatically lower the cost of computing the K documents we output, without materially altering the user’s perceived relevance of the top K results.
Consequently, in most applications it suffices to retrieve K documents whose scores are very close to those of the K best.
In the sections that follow we detail schemes that retrieve K such documents while potentially avoiding computing scores for most of the N documents in the collection.
Such inexact top-K retrieval is not necessarily, from the user’s perspective, a bad thing.
The top K documents by the cosine measure are in any case not necessarily the K best for the query: cosine similarity is only a proxy for the user’s perceived relevance.
The principal cost in computing the output stems from computing cosine similarities between the query and a large number of documents.
Having a large number of documents in contention also increases the selection cost in the final stage of culling the top K documents from a heap.
We now consider a series of ideas designed to eliminate a large number of documents without computing their cosine scores.
From the descriptions of these ideas it will be clear that many of them require parameters to be tuned to the collection and application at hand; pointers to experience in setting these parameters may be found at the end of this chapter.
It should also be noted that most of these heuristics are well-suited to free text queries, but not for Boolean or phrase queries.
For a multi-term query q, it is clear we only consider documents containing at least one of the query terms.
We can take this a step further using additional heuristics:
We only consider documents containing terms whose idf exceeds a preset threshold.
Thus, in the postings traversal, we only traverse the postings.
This has a fairly significant benefit: the postings lists of low-idf terms are generally long; with these removed from contention, the set of documents for which we compute cosines is greatly reduced.
One way of viewing this heuristic: low-idf terms are treated as stop words and do not contribute to scoring.
For instance, on the query catcher in the rye, we only traverse the postings for catcher and rye.
The cutoff threshold can of course be adapted in a query-dependent manner.
We only consider documents that contain many (and as a special case, all) of the query terms.
This can be accomplished during the postings traversal; we only compute scores for documents containing all (or many) of the query terms.
A danger of this scheme is that by requiring all (or even many) query terms to be present in a document before considering it for cosine computation, we may end up with fewer than K candidate documents in the output.
The idea of champion lists (sometimes also called fancy lists or top docs) is to precompute, for each term t in the dictionary, the set of the r documents with the highest weights for t; the value of r is chosen in advance.
For tfidf weighting, these would be the r documents with the highest tf values for term t.
We call this set of r documents the champion list for term t.
Now, given a query q we create a set A as follows: we take the union of the champion lists for each of the terms comprising q.
We now restrict cosine computation to only the documents in A.
A critical parameter in this scheme is the value r, which is highly application dependent.
Intuitively, r should be large compared with K, especially if we use any form of the index elimination described in Section 7.1.2
One issue here is that the value r is set at the time of index construction, whereas K is application dependent and may not be available until the query is received; as a result we may (as in the case of index elimination) find ourselves with a set A that has fewer than K documents.
There is no reason to have the same value of r for all terms in the dictionary; it could for instance be set to be higher for rarer terms.
We now further develop the idea of champion lists, in the somewhat more general setting of static quality scores.
This quality measure may be viewed as a number between zero and one.
For instance, in the context of news stories on the web, g(d) may be derived from the number of favorable reviews of the story by web.
The net score for a document d is some combination of g(d) together with the query-dependent score induced (say) by (6.12)
Other relative weightings are possible; the effectiveness of our heuristics will depend on the specific relative weighting.
First, consider ordering the documents in the postings list for each term by decreasing value of g(d)
This allows us to perform the postings intersection algorithm of Figure 1.6
In order to perform the intersection by a single pass through the postings of each query term, the algorithm of Figure 1.6 relied on the postings being ordered by document IDs.
But in fact, we only required that all postings be ordered by a single common ordering; here we rely on the g(d) values to provide this common ordering.
This is illustrated in Figure 7.2, where the postings are ordered in decreasing order of g(d)
The first idea is a direct extension of champion lists: for a well-chosen value r, we maintain for each term t a global champion list of the r documents.
The list itself is, like all the postings lists considered so far, sorted by a common order (either by document IDs or by static quality)
Then at query time, we only compute the net scores (7.2) for documents in the union of these global champion lists.
Intuitively, this has the effect of focusing on documents likely to have large net scores.
We conclude the discussion of global champion lists with one further idea.
We maintain for each term t two postings lists consisting of disjoint sets of documents, each sorted by g(d) values.
The first list, which we call high, contains the m documents with the highest tf values for t.
The second list, which we call low, contains all other documents containing t.
When processing a query, we first scan only the high lists of the query terms, computing net scores for any document on the high lists of all (or more than a certain number of) query terms.
If we obtain scores for K documents in the process, we terminate.
If not, we continue the scanning into the low lists, scoring documents in these postings lists.
In all the postings lists described thus far, we order the documents consistently by some common ordering: typically by document ID but in Section 7.1.4 by static quality scores.
As noted at the end of Section 6.3.3, such a common ordering supports the concurrent traversal of all of the query terms’ postings lists, computing the score for each document as we encounter it.
Computing scores in this manner is sometimes referred to as document-at-atime scoring.
We will now introduce a technique for inexact top-K retrieval in which the postings are not all ordered by a common ordering, thereby precluding such a concurrent traversal.
We will therefore require scores to be “accumulated” one term at a time as in the scheme of Figure 6.14, so that we have term-at-a-time scoring.
The idea is to order the documents d in the postings list of term t by decreasing order of tft,d.
Thus, the ordering of documents will vary from one postings list to another, and we cannot compute scores by a concurrent traversal of the postings lists of all query terms.
This latter idea too can be adaptive at the time of processing a query: as we get to query terms with lower idf, we can determine whether to proceed based on the changes in.
If these changes are minimal, we may omit accumulation from the remaining query terms, or alternatively process shorter prefixes of their postings lists.
We may also implement a version of static ordering in which each postings list is ordered by an additive combination of static and query-dependent scores.
We would again lose the consistency of ordering across postings, thereby having to process query terms one at time accumulating scores for all documents as we go along.
Depending on the particular scoring function, the postings list for a document may be ordered by other quantities than term frequency; under this more general setting, this idea is known as impact ordering.
In cluster pruning we have a preprocessing step during which we cluster the document vectors.
Then at query time, we consider only documents in a small number of clusters as candidates for which we compute cosine scores.
For each document that is not a leader, we compute its nearest leader.
We refer to documents that are not leaders as followers.
Intuitively, in the partition of the followers induced by the use of.
The candidate set A consists of L together with its followers.
We compute the cosine scores for all documents in this candidate set.
The use of randomly chosen leaders for clustering is fast and likely to reflect the distribution of the document vectors in the vector space: a region of the vector space that is dense in documents is likely to produce multiple leaders and thus a finer partition into sub-regions.
In the pre-processing step we attach each follower to its b1 closest leaders, rather than a single closest leader.
At query time we consider the b2 leaders closest to the query q.
Why do we use the decreasing rather than the increasing order?
When discussing champion lists, we simply used the r documents with the largest tf values to create the champion list for t.
But when considering global champion lists, we used idf as well, identifying documents with the largest values of g(d) + tf-idft,d.
If we were to only have one-term queries, explain why the use of global champion lists with r = K suffices for identifying the K highest scoring documents.
Explain how the common global ordering by g(d) values in all high and low lists helps make the score computation efficient.
Consider again the data of Exercise 6.23 with nnn.atc for the query-dependent scoring.
Sketch the frequency-ordered postings for the data in Figure 6.9
Sketch the postings for impact ordering when each postings list is ordered by the sum of the static quality score and the Euclidean normalized tf values in Figure 6.11
The nearest-neighbor problem in the plane is the following: given a set of N data points on the plane, we preprocess them into some data structure such that, given a query point Q, we seek the point in N that is closest to Q in Euclidean distance.
Clearly cluster pruning can be used as an approach to the nearest-neighbor problem in the plane, if we wished to avoid computing the distance from Q to every one of the query points.
Devise a simple example on the plane so that with two leaders, the answer returned by cluster pruning is incorrect (it is not the data point closest to Q)
In this section we combine the ideas developed so far to describe a rudimentary search system that retrieves and scores documents.
We first develop further ideas for scoring, beyond vector spaces.
Following this, we will put together all of these elements to outline a complete system.
Because we consider a complete system, we do not restrict ourselves to vector space retrieval in this section.
Indeed, our complete system will have provisions for vector space as well as other query operators and forms of retrieval.
In Section 7.3 we will return to how vector space queries interact with other query operators.
We mentioned in Section 7.1.2 that when using heuristics such as index elimination for inexact top-K retrieval, we may occasionally find ourselves with a set A of contenders that has fewer than K documents.
A common solution to this issue is the user of tiered indexes, which may be viewed as a gener-TIERED INDEXES alization of champion lists.
In this example we have chosen to order the postings entries within a tier by document ID.
Common search interfaces, particularly for consumer-facing search applications on the web, tend to mask query operators from the end user.
The intent is to hide the complexity of these operators from the largely non-technical audience for such applications, inviting free text queries.
Given such interfaces, how should a search equipped with indexes for various retrieval operators treat a query such as rising interest rates? More generally, given the various factors we have studied that could affect the score of a document, how should we combine these features?
The answer of course depends on the user population, the query distribution and the collection of documents.
Typically, a query parser is used to translate the user-specified keywords into a query with various operators that is executed against the underlying indexes.
Sometimes, this execution can entail multiple queries against the underlying indexes; for example, the query parser may issue a stream of queries:
Rank them by vector space scoring using as query the vector consisting of the 3 terms rising interest rates.
If fewer than ten documents contain the phrase rising interest rates, run the two 2-term phrase queries rising interest and interest rates; rank these using vector space scoring, as well.
If we still have fewer than ten results, run the vector space query consisting of the three individual query terms.
Each of these steps (if invoked) may yield a list of scored documents, for each of which we compute a score.
This score must combine contributions from vector space scoring, static quality, proximity weighting and potentially other factors – particularly since a document may appear in the lists from multiple steps.
How do we devise a query parser and how do we devise the aggregate scoring function?
In many enterprise settings we have application builders who make use of a toolkit of available scoring operators, along with a query parsing layer, with which to manually configure the scoring function as well as the query parser.
Such application builders make use of the available zones, metadata and knowledge of typical documents and queries to tune the parsing and scoring.
In collections whose characteristics change infrequently (in an enterprise application, significant changes in collection and query characteristics typically happen with infrequent events such as the introduction of new document formats or document management systems, or a merger with another company)
Web search on the other hand is faced with a constantly changing document collection with new characteristics being introduced all the time.
It is also a setting in which the number of scoring factors can run into the hundreds, making hand-tuned scoring a difficult exercise.
We have now studied all the components necessary for a basic search system that supports free text queries as well as Boolean, zone and field queries.
We briefly review how the various pieces fit together into an overall system; this is depicted in Figure 7.5
In this figure, documents stream in from the left for parsing and linguistic processing (language and format detection, tokenization and stemming)
First, we retain a copy of each parsed document in a document cache.
This will enable us to generate results snippets: snippets of text accompanying each document in the results list for a query.
This snippet tries to give a succinct explanation to the user of why the document matches the query.
The automatic generation of such snippets is the subject of Section 8.7
Data paths are shown primarily for a free text query.
As noted in Chapter 3 the latter may optionally be invoked only when the original query fails to retrieve enough results.
Finally, these ranked documents are rendered as a results page.
Adapt this procedure to work when not all query terms are present in a document.
We introduced the vector space model as a paradigm for free text queries.
We conclude this chapter by discussing how the vector space scoring model.
The relationship should be viewed at two levels: in terms of the expressiveness of queries that a sophisticated user may pose, and in terms of the index that supports the evaluation of the various retrieval methods.
In building a search engine, we may opt to support multiple query operators for an end user.
In doing so we need to understand what components of the index can be shared for executing various query operators, as well as how to handle user queries that mix various query operators.
Vector space scoring supports so-called free text retrieval, in which a query is specified as a set of words without any query operators connecting them.
It allows documents matching the query to be scored and thus ranked, unlike the Boolean, wildcard and phrase queries studied earlier.
Classically, the interpretation of such free text queries was that at least one of the query terms be present in any retrieved document.
However more recently, web search engines such as Google have popularized the notion that a set of terms typed into their query boxes (thus on the face of it, a free text query) carries the semantics of a conjunctive query that only retrieves documents containing all or most query terms.
Clearly a vector space index can be used to answer Boolean queries, as long as the weight of a term t in the document vector for d is non-zero whenever t occurs in d.
The reverse is not true, since a Boolean index does not by default maintain term weight information.
There is no easy way of combining vector space and Boolean queries from a user’s standpoint: vector space queries are fundamentally a form of evidence accumulation, where the presence of more query terms in a document adds to the score of a document.
Boolean retrieval on the other hand, requires a user to specify a formula for selecting documents through the presence (or absence) of specific combinations of keywords, without inducing any relative ordering among them.
Mathematically, it is in fact possible to invoke so-called p-norms to combine Boolean and vector space queries, but we know of no system that makes use of this fact.
Wildcard and vector space queries require different indexes, except at the basic level that both can be implemented using postings and a dictionary (e.g., a dictionary of trigrams for wildcard queries)
The vector space query is then executed as usual, with matching documents being scored and ranked; thus a document containing both rome and roma is likely to be scored higher than another containing only one of them.
The exact score ordering will of course depend on the relative weights of each term in matching documents.
The representation of documents as vectors is fundamentally lossy: the relative order of terms in a document is lost in the encoding of a document as a vector.
Even if we were to try and somehow treat every biword as a term (and thus an axis in the vector space), the weights on different axes not independent: for instance the phrase German shepherd gets encoded in the axis german shepherd, but immediately has a non-zero weight on the axes german and shepherd.
Further, notions such as idf would have to be extended to such biwords.
Thus an index built for vector space retrieval cannot, in general, be used for phrase queries.
Moreover, there is no way of demanding a vector space score for a phrase query — we only know the relative weights of each term in a document.
On the query german shepherd, we could use vector space retrieval to identify documents heavy in these two terms, with no way of prescribing that they occur consecutively.
Phrase retrieval, on the other hand, tells us of the existence of the phrase german shepherd in a document, without any indication of the relative frequency or weight of this phrase.
While these two retrieval paradigms (phrase and vector space) consequently have different implementations in terms of indexes and retrieval algorithms, they can in some cases be combined usefully, as in the three-step example of query parsing in Section 7.2.3
We have seen in the preceding chapters many alternatives in designing an IR system.
How do we know which of these techniques are effective in which applications? Should we use stop lists? Should we stem? Should we use inverse document frequency weighting? Information retrieval has developed as a highly empirical discipline, requiring careful and thorough evaluation to demonstrate the superior performance of novel techniques on representative document collections.
We then present the straightforward notion of relevant and nonrelevant documents and the formal evaluation methodology that has been developed for evaluating unranked retrieval results (Section 8.3)
This includes explaining the kinds of evaluation measures that are standardly used for document retrieval and related tasks like text classification and why they are appropriate.
We then step back to introduce the notion of user utility, and how it is approximated by the use of document relevance (Section 8.6)
Speed of response and the size of the index are factors in user happiness.
It seems reasonable to assume that relevance of results is the most important factor: blindingly fast, useless answers do not make a user happy.
However, user perceptions do not always coincide with system designers’ notions of quality.
For example, user happiness commonly depends very strongly on user interface design issues, including the layout, clarity, and responsiveness of the user interface, which are independent of the quality of the results returned.
We touch on other measures of the quality of a system, in particular the generation of high-quality result summary snippets, which strongly influence user utility, but are not measured in the basic relevance ranking paradigm (Section 8.7)
To measure ad hoc information retrieval effectiveness in the standard way, we need a test collection consisting of three things:
A set of relevance judgments, standardly a binary assessment of either relevant or nonrelevant for each query-document pair.
The standard approach to information retrieval system evaluation revolves around the notion of relevant and nonrelevant documents.
With respect to aRELEVANCE user information need, a document in the test collection is given a binary classification as either relevant or nonrelevant.
This decision is referred to as the gold standard or ground truth judgment of relevance.
As a rule of thumb, 50 information needs has usually been found to be a sufficient minimum.
Relevance is assessed relative to an information need, not a query.
Information on whether drinking red wine is more effective at reducing your risk of heart attacks than white wine.
A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.
This distinction is often misunderstood in practice, because the information need is not overt.
If a user types python into a web search engine, they might be wanting to know where they can purchase a pet python.
Or they might be wanting information on the programming language Python.
From a one word query, it is very difficult for a system to know what the information need is.
But, nevertheless, the user has one, and can judge the returned results on the basis of their relevance to it.
To evaluate a system, we require an overt expression of an information need, which can be used for judging returned documents as relevant or nonrelevant.
At this point, we make a simplification: relevance can reasonably be thought of as a scale, with some documents highly relevant and others marginally so.
But for the moment, we will use just a binary decision of relevance.
Many systems contain various weights (often known as parameters) that can be adjusted to tune system performance.
It is wrong to report results on a test collection which were obtained by tuning these parameters to maximize performance on that collection.
That is because such tuning overstates the expected performance of the system, because the weights will be set to maximize performance on one particular set of queries rather than for a random sample of queries.
In such cases, the correct procedure is to have one or more development test collections, and to tune the parameters on the devel-DEVELOPMENT TEST.
The tester then runs the system with those weights on the test collection and reports the results on that collection as an unbiased estimate of performance.
Here is a list of the most standard test collections and evaluation series.
We focus particularly on test collections for ad hoc information retrieval system evaluation, but also mention a couple of similar test collections for text classification.
This was the pioneering test collection in allowingCRANFIELD precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments.
Individual test collections are defined over different subsets of this data.
The early TRECs each consisted of 50 information needs, evaluated over different but overlapping sets of documents.
This is probably the best subcollection to use in future work, because it is the largest and the topics are more consistent.
Rather, NIST assessors’ relevance judgments are available only for the documents that were among the top k returned for some system which was entered in the TREC evaluation for which the information need was developed.
This evaluation series has con-CLEF centrated on European languages and cross-language information retrieval.
Its scale and rich annotation makes it a better basis for future research.
This is another widely used text classification collection,20 NEWSGROUPS collected by Ken Lang.
After the removal of duplicate articles, as it is usually used, it contains 18941 articles.
Given these ingredients, how is system effectiveness measured? The two most frequent and basic measures for information retrieval effectiveness are precision and recall.
These are first defined for the simple case where an.
We will see later how to extend these notions to ranked retrieval situations.
Precision (P) is the fraction of retrieved documents that are relevantPRECISION.
Recall (R) is the fraction of relevant documents that are retrievedRECALL.
These notions can be made clear by examining the following contingency table:
Retrieved true positives (tp) false positives (fp) Not retrieved false negatives (fn) true negatives (tn)
In terms of the contingency table above, accuracy = (tp+ tn)/(tp + f p + f n + tn)
This seems plausible, since there are two actual classes, relevant and nonrelevant, and an information retrieval system can be thought of as a two-class classifier which attempts to label them as such (it retrieves the subset of documents which it believes to be relevant)
This is precisely the effectiveness measure often used for evaluating machine learning classification problems.
There is a good reason why accuracy is not an appropriate measure for information retrieval problems.
In almost all circumstances, the data is extremely skewed: normally over 99.9% of the documents are in the nonrelevant category.
A system tuned to maximize accuracy can appear to perform well by simply deeming all documents nonrelevant to all queries.
Even if the system is quite good, trying to label some documents as relevant will almost always lead to a high rate of false positives.
However, labeling all documents as nonrelevant is completely unsatisfying to an information retrieval system user.
Users are always going to want to see some documents, and can be.
The measures of precision and recall concentrate the evaluation on the return of true positives, asking what percentage of the relevant documents have been found and how many false positives have also been returned.
The advantage of having the two numbers for precision and recall is that one is more important than the other in many circumstances.
Typical web surfers would like every result on the first page to be relevant (high precision) but have not the slightest interest in knowing let alone looking at every document that is relevant.
In contrast, various professional searchers such as paralegals and intelligence analysts are very concerned with trying to get as high recall as possible, and will tolerate fairly low precision results in order to get it.
Individuals searching their hard disks are also often interested in high recall searches.
Nevertheless, the two quantities clearly trade off against one another: you can always get a recall of 1 (but very low precision) by retrieving all documents for all queries! Recall is a non-decreasing function of the number of documents retrieved.
On the other hand, in a good system, precision usually decreases as the number of documents retrieved is increased.
In general we want to get some amount of recall while tolerating only a certain percentage of false positives.
A single measure that trades off precision versus recall is the F measure,F MEASURE which is the weighted harmonic mean of precision and recall:
A r i t h m e t i c.
Figure 8.1 Graph comparing the harmonic mean to other means.
The graph shows a slice through the calculation of various means of precision and recall for the fixed recall value of 70%
The harmonic mean is always less than either the arithmetic or geometric mean, and often quite close to the minimum of the two numbers.
When the precision is also 70%, all the measures coincide.
This strongly suggests that the arithmetic mean is an unsuitable measure to use.
The harmonic mean is always less than or equal to the arithmetic mean and the geometric mean.
When the values of two numbers differ greatly, the harmonic mean is closer to their minimum than to their arithmetic mean; see Figure 8.1
What is the advantage of using the harmonic mean rather than “averaging” (using the arithmetic mean)?
We need to extend these measures (or to define new measures) if we are to evaluate the ranked retrieval results that are now standard with search engines.
In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top k retrieved documents.
If it is relevant, then both precision and recall increase, and the curve jags up and to the right.
This is for the precision-recall curve shown in Figure 8.2
The justification is that almost anyone would be prepared to look at a few more documents if it would increase the percentage of the viewed set that were relevant (that is, if the precision of the larger set is higher)
Interpolated precision is shown by a thinner line in Figure 8.2
Examining the entire precision-recall curve is very informative, but there is often a desire to boil this information down to a few numbers, or perhaps even a single number.
For each recall level, we then calculate the arithmetic mean of the interpolated precision at that recall level for each information need in the test collection.
A composite precisionrecall curve showing 11 points can then be graphed.
Among evaluation measures, MAP has been shown to have especially good discrimination and stability.
For a single information need, the average precision approximates the area under the uninterpolated precision-recall curve, and so the MAP is roughly the average area under the precision-recall curve for a set of queries.
Using MAP, fixed recall levels are not chosen, and there is no interpolation.
The MAP value for a test collection is the arithmetic mean of average.
A system may not fully order all documents in the collection in response to a query or at any rate an evaluation exercise may be based on submitting only the top k results for each information need.
This has the effect of weighting each information need equally in the final reported number, even if many documents are relevant to some queries whereas very few are relevant to other queries.
Indeed, there is normally more agreement in MAP for an individual information need across systems than for MAP scores for different information needs for the same system.
This means that a set of test information needs must be large and diverse enough to be representative of system effectiveness across different queries.
The above measures factor in precision at all recall levels.
For many promi-PRECISION AT k nent applications, particularly web search, this may not be germane to users.
What matters is rather how many good results there are on the first page or the first three pages.
This is referred to as “Precision at k”, for example “Precision at 10”
It has the advantage of not requiring any estimate of the size of the set of relevant documents but the disadvantages that it is the least stable of the commonly used evaluation measures and that it does not average well, since the total number of relevant documents for a query has a strong influence on precision at k.
The set Rel may be incomplete, such as when Rel is formed by creating relevance judgments for the pooled top k results of particular systems in a set of experiments.
This measure is harder to explain to naive users than Precision at k but easier to explain than MAP.
Thus, R-precision turns out to be identical to the break-even point, anotherBREAK-EVEN POINT measure which is sometimes used, defined in terms of this equality relationship holding.
Like Precision at k, R-precision describes only one point on the precision-recall curve, rather than attempting to summarize effectiveness across the curve, and it is somewhat unclear why you should be interested in the break-even point rather than either the best point on the curve (the point with maximal F-measure) or a retrieval level of interest to a particular application (Precision at k)
Nevertheless, R-precision turns out to be highly correlated with MAP empirically, despite measuring only a single point on.
Another concept sometimes used in evaluation is an ROC curve.
Like precision at k, it is evaluated over some number k of top search results.
For a set of queries Q, let R(j, d) be the relevance score assessors gave to document d for query j.
Must there always be a break-even point between precision and recall? Either show there must be or give a counter-example.
What is the relationship between the value of F1 and the break-even point?
Show that the balanced F-measure (F1) is equal to the Dice coefficient of the retrieved and relevant document sets.
Consider an information need for which there are 4 relevant documents in the collection.
Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result):
What is the MAP of each system? Which has a higher MAP?
Does this result intuitively make sense? What does it say about what is important in getting a good MAP score?
What is the R-precision of each system? (Does it rank the systems the same as MAP?)
What is the precision of the system on the top 20? b.
What is the uninterpolated precision of the system at 25% recall? d.
Assume that these 20 documents are the complete result set of the system.
What is the largest possible MAP that this system could have? g.
What is the smallest possible MAP that this system could have? h.
In a set of experiments, only the top 20 results are evaluated by hand.
For this example, how large (in absolute terms) can the error for the MAP be by calculating (e) instead of (f) and (g) for this query?
To properly evaluate a system, your test information needs must be germane to the documents in the test document collection, and appropriate for predicted usage of the system.
Using random combinations of query terms as an information need is generally not a good idea because typically they will not resemble the actual distribution of information needs.
Given information needs and documents, you need to collect relevance assessments.
This is a time-consuming and expensive process involving human beings.
For tiny collections like Cranfield, exhaustive judgments of relevance for each query and document pair were obtained.
For large modern collections, it is usual for relevance to be assessed only for a subset of the documents for each query.
The most standard approach is pooling, where rel-POOLING evance is assessed over a subset of the collection that is formed from the top k documents returned by a number of different IR systems (usually the ones to be evaluated), and perhaps other sources such as the results of Boolean keyword searches or documents found by expert searchers in an interactive process.
A human is not a device that reliably reports a gold standard judgment of relevance of a document to a query.
Rather, humans and their relevance judgments are quite idiosyncratic and variable.
But this is not a problem to be solved: in the final analysis, the success of an IR system depends on how good it is at satisfying the needs of these idiosyncratic humans, one information need at a time.
Nevertheless, it is interesting to consider and measure how much agreement between judges there is on relevance judgments.
In the social sciences, a common measure for agreement between judges is the kappa statistic.
It isKAPPA STATISTIC designed for categorical judgments and corrects a simple agreement rate for the rate of chance agreement.
There are choices in how the latter is estimated: if we simply say we are making a two-class decision and assume nothing more, then the expected chance agreement rate is 0.5
However, normally the class distribution assigned is skewed, and it is usual to use marginal statistics to calculate expected agree-MARGINAL ment.2 There are still two ways to do it depending on whether one pools.
If there are more than two judges, it is normal to calculate an average pairwise kappa value.
Interjudge agreement of relevance has been measured within the TREC evaluations and for medical IR collections.
Using the above rules of thumb, the level of agreement normally falls in the range of “fair” (0.67–0.8)
The fact that human agreement on a binary relevance judgment is quite modest is one reason for not requiring more fine-grained relevance labeling from the test set creator.
To answer the question of whether IR evaluation results are valid despite the variation of individual assessors’ judgments, people have experimented with evaluations taking one or the other of two judges’ opinions as the gold standard.
The choice can make a considerable absolute difference to reported scores, but has in general been found to have little impact on the relative effectiveness ranking of either different systems or variants of a single system which are being compared for effectiveness.
The advantage of system evaluation, as enabled by the standard model of relevant and nonrelevant documents, is that we have a fixed setting in which we can vary IR systems and system parameters to carry out comparative experiments.
Such formal testing is much less expensive and allows clearer diagnosis of the effect of changing system parameters than doing user studies of retrieval effectiveness.
Indeed, once we have a formal measure that we have confidence in, we can proceed to optimize effectiveness by machine learning methods, rather than tuning parameters by hand.
Of course, if the formal measure poorly describes what users actually want, doing this will not be effective in improving user satisfaction.
Our perspective is that, in practice, the standard formal measures for IR evaluation, although a simplification, are good enough, and recent work in optimizing formal evaluation measures in IR has succeeded brilliantly.
That is not to say that there are not problems latent within the abstractions used.
The relevance of one document is treated as independent of the relevance of other documents in the collection.
This assumption is actually built into most retrieval systems – documents are scored against queries, not against each other – as well as being assumed in the evaluation methods.
Assessments are binary: there aren’t any nuanced assessments of relevance.
Relevance of a document to an information need is treated as an absolute, objective decision.
But judgments of relevance are subjective, varying across people, as we discussed above.
In practice, human assessors are also imperfect measuring instruments, susceptible to failures of understanding and attention.
We also have to assume that users’ information needs do not change as they start looking at retrieval results.
Any results based on one collection are heavily skewed by the choice of collection, queries, and relevance judgment set: the results may not translate from one domain to another or to a different user population.
One clear problem with the relevance-based assessment that we have presented is the distinction between relevance and marginal relevance: whetherMARGINAL RELEVANCE a document still has distinctive usefulness after the user has looked at certain other documents (Carbonell and Goldstein 1998)
Even if a document is highly relevant, its information can be completely redundant with other documents which have already been examined.
The most extreme case of this is documents that are duplicates – a phenomenon that is actually very common on the World Wide Web – but it can also easily occur when several documents provide a similar precis of an event.
In such circumstances, marginal relevance is clearly a better measure of utility to the user.
Maximizing marginal relevance requires returning documents that exhibit diversity and novelty.
One way to approach measuring this is by using distinct facts or entities as evaluation units.
This perhaps more directly measures true utility to the user but doing this makes it harder to create a test collection.
Calculate precision, recall, and F1 of your system if a document is considered relevant only if the two judges agree.
Calculate precision, recall, and F1 of your system if a document is considered relevant if either judge thinks it is relevant.
Formal evaluation measures are at some distance from our ultimate interest in measures of human utility: how satisfied is each user with the results the system gives for each information need that they pose? The standard way to measure human satisfaction is by various kinds of user studies.
These might include quantitative measures, both objective, such as time to complete a task, as well as subjective, such as a score for satisfaction with the search engine, and qualitative measures, such as user comments on the search interface.
In this section we will touch on other system aspects that allow quantitative evaluation and the issue of user utility.
There are many practical benchmarks on which to rate an information retrieval system beyond its retrieval quality.
How fast does it search, that is, what is its latency as a function of index size?
How expressive is its query language? How fast is it on complex queries?
How large is its document collection, in terms of the number of documents or the collection having information distributed across a broad range of topics?
All these criteria apart from query language expressiveness are straightforwardly measurable: we can quantify the speed or size.
Various kinds of feature checklists can make query language expressiveness semi-precise.
What we would really like is a way of quantifying aggregate user happiness, based on the relevance, speed, and user interface of a system.
One part of this is understanding the distribution of people we wish to make happy, and this depends entirely on the setting.
For a web search engine, happy search users are those who find what they want.
One indirect measure of such users is that they tend to return to the same engine.
Measuring the rate of return of users is thus an effective metric, which would of course be more effective if you could also measure how much these users used other search engines.
But advertisers are also users of modern web search engines.
They are happy if customers click through to their sites and then make purchases.
On an eCommerce web site, a user is likely to be wanting to purchase something.
Thus, we can measure the time to purchase, or the fraction of searchers who become buyers.
On a shopfront web site, perhaps both the user’s and the store owner’s needs are satisfied if a purchase is made.
Nevertheless, in general, we need to decide whether it is the end user’s or the eCommerce site owner’s happiness that we are trying to optimize.
Usually, it is the store owner who is paying us.
For an “enterprise” (company, government, or academic) intranet search engine, the relevant metric is more likely to be user productivity: how much time do users spend looking for information that they need.
User happiness is elusive to measure, and this is part of why the standard methodology uses the proxy of relevance of search results.
The standard direct way to get at user satisfaction is to run user studies, where people engage in tasks, and usually various metrics are measured, the participants are observed, and ethnographic interview techniques are used to get qualitative information on satisfaction.
User studies are very useful in system design, but they are time consuming and expensive to do.
They are also difficult to do well, and expertise is required to design the studies and to interpret the results.
We will not discuss the details of human usability testing here.
If an IR system has been built and is being used by a large number of users, the system’s builders can evaluate possible changes by deploying variant versions of the system and recording measures that are indicative of user satisfaction with one variant vs.
The most common version of this is A/B testing, a term borrowed from theA/B TEST advertising industry.
For such a test, precisely one thing is changed between the current system and a proposed system, and a small proportion of traffic (say, 1–10% of users) is randomly directed to the variant system, while most users use the current system.
For example, if we wish to investigate a change to the ranking algorithm, we redirect a random sample of users to a variant system and evaluate measures such as the frequency with which people click on the top result, or any result on the first page.
This particular analysis method is referred to as clickthrough log analysis or clickstream min-CLICKTHROUGH LOG.
The basis of A/B testing is running a bunch of single variable tests (either in sequence or in parallel): for each test only one parameter is varied from the control (the current live system)
It is therefore easy to see whether varying each parameter has a positive or negative effect.
Such testing of a live system can easily and cheaply gauge the effect of a change on users, and, with a large enough user base, it is practical to measure even very small positive and negative effects.
In principle, more analytic power can be achieved by varying multiple things at once in an uncorrelated (random) way, and doing standard multivariate statistical analysis, such as multiple linear regression.
In practice, though, A/B testing is widely used, because A/B tests are easy to deploy, easy to understand, and easy to explain to management.
Having chosen or ranked the documents matching a query, we wish to present a results list that will be informative to the user.
In many cases the user will not want to examine all the returned documents and so we want to make the results list informative enough that the user can do a final ranking of the documents for themselves based on relevance to their information need.3 The standard way of doing this is to provide a snippet, a short sum-SNIPPET mary of the document, which is designed so as to allow the user to decide its relevance.
Typically, the snippet consists of the document title and a short.
For instance, in many legal disclosure cases, a legal associate will review every document that matches a keyword search.
The question is how to design the summary so as to maximize its usefulness to the user.
The two basic kinds of summaries are static, which are always the sameSTATIC SUMMARY regardless of the query, and dynamic (or query-dependent), which are cus-DYNAMIC SUMMARY tomized according to the user’s information need as deduced from a query.
Dynamic summaries attempt to explain why a particular document was retrieved for the query at hand.
A static summary is generally comprised of either or both a subset of the document and metadata associated with the document.
The simplest form of summary takes the first two sentences or 50 words of a document, or extracts particular zones of a document, such as the title and author.
Instead of zones of a document, the summary can instead use metadata associated with the document.
This may be an alternative way to provide an author or date, or may include elements which are designed to give a summary, such as the description metadata which can appear in the meta element of a web HTML page.
This summary is typically extracted and cached at indexing time, in such a way that it can be retrieved and presented quickly when displaying search results, whereas having to access the actual document content might be a relatively expensive operation.
There has been extensive work within natural language processing (NLP) on better ways to do text summarization.
Most such work still aims only toTEXT SUMMARIZATION choose sentences from the original document to present and concentrates on how to select good sentences.
The models typically combine positional factors, favoring the first and last paragraphs of documents and the first and last sentences of paragraphs, with content factors, emphasizing sentences with key terms, which have low document frequency in the collection as a whole, but high frequency and good distribution across the particular document being returned.
In sophisticated NLP approaches, the system synthesizes sentences for a summary, either by doing full text generation or by editing and perhaps combining sentences used in the document.
For example, it might delete a relative clause or replace a pronoun with the noun phrase that it refers to.
This last class of methods remains in the realm of research and is seldom used for search results: it is easier, safer, and often even better to just use sentences from the original document.
Dynamic summaries display one or more “windows” on the document, aiming to present the pieces that have the most utility to the user in evaluating the document with respect to their information need.
If the query is found as a phrase, occurrences of the phrase in the document will be.
In recent years, Papua New Guinea has faced severe economic difficulties and economic growth has slowed, partly as a result of weak governance and civil war, and partly as a result of external factors such as the Bougainville civil war which led to the closure in 1989 of the Panguna mine (at that time the most important foreign exchange earner and contributor to Government finances), the Asian financial crisis, a decline in the prices of gold and copper, and a fall in the production of oil.
PNG’s economic development record over the past few years is evidence that governance issues underly many of the country’s problems.
Good governance, which may be defined as the transparent and accountable management of human, natural, economic and financial resources for the purposes of equitable and sustainable development, flows from proper public sector management, efficient fiscal and accounting mechanisms, and a willingness to make service delivery a priority in practice.
Figure 8.5 An example of selecting text for a dynamic snippet.
This snippet was generated for a document in response to the query new guinea economic development.
The figure shows in bold italic where the selected snippet text occurred in the original document.
If not, windows within the document that contain multiple query terms will be selected.
Commonly these windows may just stretch some number of words to the left and right of the query terms.
This is a place where NLP techniques can usefully be employed: users prefer snippets that read well because they contain complete phrases.
Dynamic summaries are generally regarded as greatly improving the usability of IR systems, but they present a complication for IR system design.
A dynamic summary cannot be precomputed, but, on the other hand, if a system has only a positional index, then it cannot easily reconstruct the context surrounding search engine hits in order to generate such a dynamic summary.
Then, a system can simply scan a document which is about to appear in a displayed results list to find snippets containing the query words.
Beyond simply access to the text, producing a good KWIC snippet requires some care.
Given a variety of keyword occurrences in a document, the goal is to choose fragments which are: (i) maximally informative about the discussion of those terms in the document, (ii) self-contained enough to be easy to read, and (iii) short enough to fit within the normally strict constraints on the space available for summaries.
Generating snippets must be fast since the system is typically generating many snippets for each query that it handles.
Rather than caching an entire document, it is common to cache only a generous but fixed size prefix of the document, such as perhaps 10,000 characters.
For most common, short documents, the entire document is thus cached, but huge amounts of local storage will not be wasted on potentially vast documents.
Summaries of documents whose length exceeds the prefix size will be based on material in the prefix only, which is in general a useful zone in which to look for a document summary anyway.
If a document has been updated since it was last processed by a crawler and indexer, these changes will be neither in the cache nor in the index.
In these circumstances, neither the index nor the summary will accurately reflect the current contents of the document, but it is the differences between the summary and the actual document content that will be more glaringly obvious to the end user.
Rigorous formal testing of IR systems was first completed in the Cranfield experiments, beginning in the late 1950s.
A retrospective discussion of the Cranfield test collection and experimentation with it can be found in (Cleverdon 1991)
The TREC evaluations are described in detail by Voorhees and Harman (2005)
Initially, few researchers computed the statistical significance of their experimental results, but the IR community increasingly demands this (Hull 1993)
Buckley and Voorhees (2000) compare several evaluation measures, including precision at k, MAP, and R-precision, and evaluate the error rate of each measure.
R-precision was adopted as the official evaluation metric inR-PRECISION the TREC HARD track (Allan 2005)
A standard program for evaluating IR systems which computes many measures of ranked retrieval effectiveness is Chris Buckley’s trec_eval program used in the TREC evaluations.
The kappa statistic and its use for language-related purposes is discussedKAPPA STATISTIC by Carletta (1996)
For further discussion of alternative measures of agreement, which may in fact be better, see Lombard et al.
Modern work on sentence selection was initiated by Kupiec et al.
Tombros and Sanderson (1998) demonstrate the advantages of dynamic summaries in the IR context.
User interfaces for IR and human factors such as models of human information seeking and usability testing are outside the scope of what we cover in this book.
In most collections, the same concept may be referred to using different words.
This issue, known as synonymy, has an impact on the recall of mostSYNONYMY information retrieval systems.
For example, you would want a search for aircraft to match plane (but only for references to an airplane, not a woodworking plane), and for a search on thermodynamics to match references to heat in appropriate discussions.
Users often attempt to address this problem themselves by manually refining a query, as was discussed in Section 1.4; in this chapter we discuss ways in which a system can help with query refinement, either fully automatically or with the user in the loop.
The methods for tackling this problem split into two major classes: global methods and local methods.
Global methods are techniques for expanding or reformulating query terms independent of the query and results returned from it, so that changes in the query wording will cause the new query to match other semantically similar terms.
Local methods adjust a query relative to the documents that initially appear to match the query.
Pseudo relevance feedback, also known as Blind relevance feedback (Section 9.1.6)
In this chapter, we will mention all of these approaches, but we will concentrate on relevance feedback, which is one of the most used and most successful approaches.
The idea of relevance feedback (RF) is to involve the user in the retrieval processRELEVANCE FEEDBACK so as to improve the final result set.
In particular, the user gives feedback on the relevance of documents in an initial set of results.
The user marks some returned documents as relevant or nonrelevant.
The system computes a better representation of the information need based on the user feedback.
Relevance feedback can go through one or more iterations of this sort.
The process exploits the idea that it may be difficult to formulate a good query when you don’t know the collection well, but it is easy to judge particular documents, and so it makes sense to engage in iterative query refinement of this sort.
In such a scenario, relevance feedback can also be effective in tracking a user’s evolving information need: seeing some documents may lead users to refine their understanding of the information they are seeking.
Not only is it easy to see the results at work, but this is a domain where a user can easily have difficulty formulating what they want in words, but can easily indicate relevant or nonrelevant images.
After the user enters an initial query for bike on the demonstration system at:
In Figure 9.1 (a), the user has selected some of them as relevant.
These will be used to refine the query, while other displayed results have no effect on the reformulation.
Figure 9.1 (b) then shows the new top-ranked results calculated after this round of relevance feedback.
Figure 9.2 shows a textual IR example where the user wishes to find out about new applications of space satellites.
The Rocchio Algorithm is the classic algorithm for implementing relevance feedback.
It models a way of incorporating relevance feedback information into the vector space model of Section 6.3
Figure 9.2 Example of relevance feedback on a text collection.
A * marks the documents which were judged relevant in the relevance feedback phase.
Figure 9.3 The Rocchio optimal query for separating relevant and nonrelevant documents.
We want to find a query vector, denoted as ~q, that maximizes similarity with relevant documents while minimizing similarity with nonrelevant documents.
If Cr is the set of relevant documents and Cnr is the set of nonrelevant documents, then we wish to find:1
Under cosine similarity, the optimal query vector~qopt for separating the relevant and nonrelevant documents is:
That is, the optimal query is the vector difference between the centroids of the relevant and nonrelevant documents; see Figure 9.3
However, this observation is not terribly useful, precisely because the full set of relevant documents is not known: it is what we want to find.
In the equation, arg maxx f (x) returns a value of x which maximizes the value of the function f (x)
Similarly, arg minx f (x) returns a value of x which minimizes the value of the function f (x)
Some documents have been labeled as relevant and nonrelevant and the initial query vector is moved in response to this feedback.
In a real IR query context, we have a user query and partial knowledge of known relevant and nonrelevant documents.
But, in practice, it has been shown to be most useful for increasing recall in situations.
One way of doing this is with a Naive Bayes probabilistic model.
If R is a Boolean indicator variable expressing the relevance of a document, then we can estimate P(xt = 1|R), the probability of a term t appearing in a document, depending on whether it is relevant or not, as:
Even though the set of known relevant documents is a perhaps small subset of the true set of relevant documents, if we assume that the set of relevant documents is a small subset of the set of all documents then the estimates given above will be reasonable.
This gives a basis for another way of changing the query term weights.
For the moment, observe that using just Equation (9.4) as a basis for term-weighting is likely insufficient.
The equations use only collection statistics and information about the term distribution within the documents judged relevant.
Firstly, the user has to have sufficient knowledge to be able to make an initial query.
This is needed anyhow for successful information retrieval in the basic case, but it is important to see the kinds of problems that relevance feedback cannot solve alone.
If the user spells a term in a different way to the way it is spelled in any document in the collection, then relevance feedback is unlikely to be effective.
Documents in another language are not nearby in a vector space based on term distribution.
Rather, documents in the same language cluster more closely together.
If the user searches for laptop but all the documents use the term notebook computer, then the query will fail, and relevance feedback is again most likely ineffective.
Secondly, the relevance feedback approach requires relevant documents to be similar to each other.
Ideally, the term distribution in all relevant documents will be similar to that in the documents marked by the users, while the term distribution in all nonrelevant documents will be different from those in relevant documents.
Things will work well if all relevant documents are tightly clustered around a single prototype, or, at least, if there are different prototypes, if the relevant documents have significant vocabulary overlap, while similarities between relevant and nonrelevant documents are small.
Implicitly, the Rocchio relevance feedback model treats relevant documents as a single cluster, which it models via the centroid of the cluster.
This approach does not work as well if the relevant documents are a multimodal class, that is, they consist of several clusters of documents within the vector space.
Subsets of the documents using different vocabulary, such as Burma vs.
A query for which the answer set is inherently disjunctive, such as Pop stars who once worked at Burger King.
Instances of a general concept, which often appear as a disjunction of more specific concepts, for example, felines.
Good editorial content in the collection can often provide a solution to this problem.
For example, an article on the attitudes of different groups to the situation in Burma could introduce the terminology used by different parties, thus linking the document clusters.
Users are often reluctant to provide explicit feedback, or in general do not wish to prolong the search interaction.
Furthermore, it is often harder to understand why a particular document was retrieved after relevance feedback is applied.
The long queries that are generated by straightforward application of relevance feedback techniques are inefficient for a typical IR system.
This results in a high computing cost for the retrieval and potentially long response times for the user.
A partial solution to this is to only reweight certain prominent terms in the relevant documents, such as perhaps the top 20 terms by term frequency.
Some web search engines offer a similar/related pages feature: the user indicates a document in the results set as exemplary from the standpoint of meeting his information need and requests more documents like it.
This can be viewed as a particular simple form of relevance feedback.
However, in general relevance feedback has been little used in web search.
One exception was the Excite web search engine, which initially provided full relevance feedback.
However, the feature was in time dropped, due to lack of use.
On the web, few people use advanced search interfaces and most would like to complete their search in a single interaction.
But the lack of uptake also probably reflects two other factors: relevance feedback is hard to explain to the average user, and relevance feedback is mainly a recall enhancing strategy, and web search users are only rarely concerned with getting sufficient recall.
Only about 4% of user query sessions used the relevance feedback option, and these were usually exploiting the “More like this” link next to each result.
About 70% of users only looked at the first page of results and did not pursue things any further.
For people who used relevance feedback, results were improved about two thirds of the time.
An important more recent thread of work is the use of clickstream data (what links a user clicks on) to provide indirect relevance feedback.
The very successful use of web link structure (see Chapter 21) can also be viewed as implicit feedback, but provided by page authors rather than readers (though in practice most authors are also readers)
Give three reasons why relevance feedback has been little used in web search.
Interactive relevance feedback can give very substantial gains in retrieval performance.
Empirically, one round of relevance feedback is often very useful.
Successful use of relevance feedback requires enough judged documents, otherwise the process is unstable in that it may drift away from the user’s information need.
There is some subtlety to evaluating the effectiveness of relevance feedback in a sound and enlightening way.
The obvious first strategy is to start with an initial query q0 and to compute a precision-recall graph.
Following one round of feedback from the user, we compute the modified query qm and again compute a precision-recall graph.
Here, in both rounds we assess performance over all documents in the collection, which makes comparisons straightforward.
If we do this, we find spectacular gains from relevance feedback: gains on the order of 50% in mean average precision.
The gains are partly due to the fact that known relevant documents (judged by the user) are now ranked higher.
Fairness demands that we should only evaluate with respect to documents not seen by the user.
A second idea is to use documents in the residual collection (the set of documents minus those assessed relevant) for the second round of evaluation.
Unfortunately, the measured performance can then often be lower than for the original query.
This is particularly the case if there are few relevant documents, and so a fair proportion of them have been judged by the user in the first round.
The relative performance of variant relevance feedback methods can be validly compared, but it is difficult to validly compare performance with and without relevance feedback because the collection size and the number of relevant documents changes from before the feedback to after it.
A third method is to have two collections, one which is used for the initial query and relevance judgments, and the second that is then used for comparative evaluation.
The performance of both q0 and qm can be validly compared on the second collection.
Perhaps the best evaluation of the utility of relevance feedback is to do user studies of its effectiveness, in particular by doing a time-based comparison:
Such notions of user utility are fairest and closest to real system usage.
Pseudo relevance feedback, also known as blind relevance feedback, provides aPSEUDO RELEVANCE FEEDBACK.
It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction.
The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top k ranked documents are relevant, and finally to do relevance feedback as before under this assumption.
Evidence suggests that it tends to work better than global analysis (Section 9.2)
It has been found to improve performance in the TREC ad hoc task.
But it is not without the dangers of an automatic process.
For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile.
We can also use indirect sources of evidence rather than explicit feedback on relevance as the basis for relevance feedback.
Implicit feedback is less reliable than explicit feedback, but is more useful than pseudo relevance feedback, which contains no evidence of user judgments.
Moreover, while users are often reluctant to provide explicit feedback, it is easy to collect implicit feedback in large quantities for a high volume system, such as a web search engine.
On the web, DirectHit introduced the idea of ranking more highly documents that users chose to look at more often.
In other words, clicks on links were assumed to indicate that the page was likely relevant to the query.
This approach makes various assumptions, such as that the document summaries displayed in results lists (on whose basis users choose which documents to click on) are indicative of the relevance of these documents.
In the original DirectHit search engine, the data about the click rates on pages was gathered globally, rather than being user or query specific.
This is one form of the general area of clickstream mining.
Today, a closely related approach is used inCLICKSTREAM MINING ranking the advertisements that match a web search query (Chapter 19)
Relevance feedback has been shown to be very effective at improving relevance of results.
Its successful use requires queries for which the set of relevant documents is medium to large.
Full relevance feedback is often onerous for the user, and its implementation is not very efficient in most IR systems.
In many cases, other types of interactive retrieval may improve relevance by about as much with less work.
Beyond the core ad hoc retrieval scenario, other uses of relevance feedback include:
Following a changing information need (e.g., names of car models of interest change over time)
Active learning (deciding which examples it is most useful to know the class of to reduce annotation costs)
Why is positive feedback likely to be more useful than negative feedback to an IR system? Why might only using one nonrelevant document be more effective than using several?
Suppose that a user’s initial query is cheap CDs cheap DVDs extremely cheap CDs.
Omar has implemented a relevance feedback web search system, where he is going to do relevance feedback based only on words in the title text returned for a page (for efficiency)
In this section we more briefly discuss three global methods for expanding a query: by simply aiding the user in doing so, by using a manual thesaurus, and through building a thesaurus automatically.
Various user supports in the search process can help the user see how their searches are or are not working.
This includes information about words that were omitted from the query because they were on stop lists, what words were stemmed to, the number of hits on each term or phrase, and whether words were dynamically turned into phrases.
The IR system might also suggest search terms by means of a thesaurus or a controlled vocabulary.
A user can also be allowed to browse lists of the terms that are in the inverted index, and thus find good terms that appear in the collection.
In relevance feedback, users give additional input on documents (by marking documents in the results set as relevant or not), and this input is used to reweight the terms in the query for documents.
In query expansion on theQUERY EXPANSION other hand, users give additional input on query words or phrases, possibly suggesting additional query terms.
I m a g e s V i d e o L o c a l S h o p p i n g m o r e.
A T & T ( C i n g u l a r )
The expanded query suggestions appear just below the “Search Results” bar.
Figure 9.6 shows an example of query suggestion options being presented in the Yahoo! web search engine.
The central question in this form of query expansion is how to generate alternative or expanded queries for the user.
The most common form of query expansion is global analysis, using some form of thesaurus.
For each term t in a query, the query can be automatically expanded with synonyms and related words of t from the thesaurus.
Use of a thesaurus can be combined with ideas of term weighting: for instance, one might weight added terms less than original query terms.
Use of a controlled vocabulary that is maintained by human editors.
The subject headings of traditional library subject indexes, such as the Library of Congress Subject Headings, or the Dewey Decimal system are examples of a controlled vocabulary.
Use of a controlled vocabulary is quite common for wellresourced domains.
A well-known example is the Unified Medical Language System (UMLS) used with MedLine for querying the biomedical research literature.
For example, in Figure 9.7, neoplasms was added to a.
Figure 9.7 Examples of query expansion via the PubMed thesaurus.
When a user issues a query on the PubMed interface to Medline at http://www.ncbi.nlm.nih.gov/entrez/, their query is mapped on to the Medline vocabulary as shown.
This Medline query expansion also contrasts with the Yahoo! example.
The Yahoo! interface is a case of interactive query expansion, whereas PubMed does automatic query expansion.
Unless the user chooses to examine the submitted query, they may not even realize that query expansion has occurred.
Here, human editors have built up sets of synonymous names for concepts, without designating a canonical term.
Statistics Canada maintains a thesaurus of preferred terms, synonyms, broader terms, and narrower terms for matters on which the government collects statistics, such as goods and services.
Here, word co-occurrence statistics over a collection of documents in a domain are used to automatically induce a thesaurus; see Section 9.2.3
Here, we exploit the manual query reformulations of other users to make suggestions to a new user.
This requires a huge query volume, and is thus particularly appropriate to web search.
Thesaurus-based query expansion has the advantage of not requiring any user input.
Use of query expansion generally increases recall and is widely used in many science and engineering fields.
As well as such global analysis techniques, it is also possible to do query expansion by local analysis, for instance, by analyzing the documents in the result set.
Word Nearest neighbors absolutely absurd, whatsoever, totally, exactly, nothing bottomed dip, copper, drops, topped, slide, trimmed captivating shimmer, stunningly, superbly, plucky, witty doghouse dog, porch, crawling, beside, downstairs makeup repellent, lotion, glossy, sunscreen, skin, gel mediating reconciliation, negotiate, case, conciliation keeping hoping, bring, wiping, could, some, would lithographs drawings, Picasso, Dali, sculptures, Gauguin pathogens toxins, bacteria, organisms, bacterial, parasite senses grasp, psyche, truly, clumsy, naive, innate.
As an alternative to the cost of a manual thesaurus, we could attempt to generate a thesaurus automatically by analyzing a collection of documents.
We say that words co-occurring in a document or paragraph are likely to be in some sense similar or related in meaning, and simply count text statistics to find the most similar words.
The other approach is to use a shallow grammatical analysis of the text and to exploit grammatical relations or grammatical dependencies.
For example, we say that entities that are grown, cooked, eaten, and digested, are more likely to be food items.
Simply using word cooccurrence is more robust (it cannot be misled by parser errors), but using grammatical relations is more accurate.
The simplest way to compute a co-occurrence thesaurus is based on termterm similarities.
We begin with a term-document matrix A, where each cell At,d is a weighted count wt,d for term t and document d, with weighting so A has length-normalized rows.
If we then calculate C = AAT , then Cu,v is a similarity score between terms u and v, with a larger number being better.
While some of the thesaurus terms are good or at least suggestive, others are marginal or bad.
For example, a query for Apple computer may expand to Apple red fruit computer.
In general these thesauri suffer from both false positives and false negatives.
Moreover, since the terms in the automatic thesaurus are highly correlated in documents anyway (and often the collection used to derive the thesaurus is the same as the one being indexed), this form of query expansion may not retrieve many additional documents.
However, there is a high cost to manually producing a thesaurus and then updating it for scientific and terminological developments within a field.
In general a domainspecific thesaurus is required: general thesauri and dictionaries give far too little coverage of the rich domain-particular vocabularies of most scientific fields.
However, query expansion may also significantly decrease precision, particularly when the query contains ambiguous terms.
For example, if the user searches for interest rate, expanding the query to interest rate fascinate evaluate is unlikely to be useful.
Overall, query expansion is less successful than relevance feedback, though it may be as good as pseudo relevance feedback.
It does, however, have the advantage of being much more understandable to the system user.
Exercise 9.7If A is simply a Boolean cooccurrence matrix, then what do you get as the entries in C?
Work in information retrieval quickly confronted the problem of variant expression which meant that the words in a query might not appear in a document, despite it being relevant to the query.
There is also the issue of translation, of users knowing what terms a document will use.
Blair and Maron (1985) conclude that “it is impossibly difficult for users to predict the exact words, word combinations, and phrases that are used by all (or most) relevant documents and only (or primarily) by those documents”
Other later work includes Salton and Buckley (1990), Riezler et al.
Koenemann and Belkin (1996) do user studies of the effectiveness of relevance feedback.
Traditionally Roget’s thesaurus has been the best known English language thesaurus (Roget 1946)
In recent computational work, people almost always use WordNet (Fellbaum 1998), not only because it is free, but also because of its rich link structure.
Traditionally, IR systems have retrieved information from unstructured text – by which we mean “raw” text without markup.
Databases are designed for querying relational data: sets of records that have values for predefined attributes such as employee number, title and salary.
There are fundamental differences between information retrieval and database systems in terms of retrieval model, data structures and query language as shown in Table 10.1.1
Some highly structured text search problems are most efficiently handled by a relational database, for example, if the employee table contains an attribute for short textual job descriptions and you want to find all employees who are involved with invoicing.
However, many structured data sources containing text are best modeled as structured documents rather than relational data.
We call the search over such structured documents structured retrieval.
Applications of structured retrieval include digital libraries, patent databases, blogs, text in which entities like persons and locations have been tagged (in a process called named entity tagging) and output from office suites like OpenOffice that save documents as marked up text.
In all of these applications, we want to be able to run queries that combine textual criteria with structural criteria.
Examples of such queries are give me a full-length article on fast fourier transforms (digital libraries), give me patents whose claims mention RSA public key encryption.
In most modern database systems, one can enable full-text search for text columns.
This usually means that an inverted index is created and Boolean or vector space search enabled, effectively combining core database with information retrieval technologies.
Table 10.1 RDB (relational database) search, unstructured information retrieval and structured information retrieval.
There is no consensus yet as to which methods work best for structured retrieval although many researchers believe that XQuery (page 215) will become the standard for structured queries.
These three queries are structured queries that cannot be answered well by an unranked retrieval system.
For instance, an unranked system would return a potentially large number of articles that mention the Vatican, the Coliseum and sightseeing tours without ranking the ones that are most relevant for the query first.
Most users are also notoriously bad at precisely stating structural constraints.
For instance, users may not know for which structured elements the search system supports search.
In our example, the user may be unsure whether to issue the query as sightseeing AND (COUNTRY:Vatican OR LANDMARK:Coliseum) , as sightseeing AND (STATE:Vatican OR BUILDING:Coliseum) or in some other form.
Users may also be completely unfamiliar with structured search and advanced search interfaces or unwilling to use them.
In this chapter, we look at how ranked retrieval methods can be adapted to structured documents to address these problems.
We will only look at one standard for encoding structured documents: Extensible Markup Language or XML, which is currently the most widely usedXML such standard.
We will not cover the specifics that distinguish XML from other types of markup such as HTML and SGML.
But most of what we say in this chapter is applicable to markup languages in general.
In the context of information retrieval, we are only interested in XML as a language for encoding text and documents.
A perhaps more widespread use of XML is to encode non-text data.
For example, we may want to export data in XML format from an enterprise resource planning system and then read them into an analytics program to produce graphs for a presentation.
This type of application of XML is called data-centric because numerical andDATA-CENTRIC XML non-text attribute-value data dominate and text is usually a small fraction of the overall data.
Most data-centric XML is stored in databases – in contrast to the inverted index-based methods for text-centric XML that we present in this chapter.
We have adopted the terminology that is widespread in the XML retrieval community.
For instance, the standard way of referring to XML queries is structured queries, not semistructured queries.
The term structured retrieval is rarely used for database querying and it always refers to XML retrieval in this book.
The data model is flat, that is, there is no nesting of attributes.
In contrast, XML documents have the more complex tree structure that we see in Figure 10.2 in which attributes are nested.
The number of attributes and nodes is greater than in parametric and zone search.
Next we describe a vector space model for XML retrieval (Section 10.3)
Section 10.4 presents INEX, a shared task evaluation that has been held for a number of years and currently is the most important venue for XML retrieval research.
We discuss the differences between data-centric and text-centric approaches to XML in Section 10.5
Each node of the tree is an XML element and is written with an opening and closing tag.
An element canXML ELEMENT have one or more XML attributes.
It has an attribute number with value vii and two child elements, title and verse.
The leaf nodes of the tree consist of text, e.g., Shakespeare, Macbeth, and Macbeth’s castle.
The tree’s internal nodes encode either the structure of the document (title, act, and scene) or metadata functions (author)
The DOM represents elements, attributesXML DOM and text within elements as nodes in a tree.
Figure 10.3 An XML query in NEXI format and its partial representation as a tree.
XPath is a standard for enumerating paths in an XML document collection.XPATH.
We will also refer to paths as XML contexts or simply contexts in this chapter.XML CONTEXT Only a small subset of XPath is needed for our purposes.
The XPath expression node selects all nodes of that name.
Successive elements of a path are separated by slashes, so act/scene selects all scene elements whose parent is an act element.
Double slashes indicate that an arbitrary number of elements can intervene on a path: play//scene selects all scene elements occurring in a play element.
In Figure 10.2 this set consists of a single scene element, which is accessible via the path play, act, scene from the top.
An initial slash starts the path at the root element.
For notational convenience, we allow the final element of a path to be a vocabulary term and separate it from the element path by the symbol #, even though this does not conform to the XPath standard.
For example, title#"Macbeth" selects all titles containing the term Macbeth.
We also need the concept of schema in this chapter.
A schema puts con-SCHEMA straints on the structure of allowable XML documents for a particular application.
A schema for Shakespeare’s plays may stipulate that scenes can only occur as children of acts and that only acts and scenes have the number attribute.
Two standards for schemas for XML documents are XML DTDXML DTD (document type definition) andXML Schema.
Users can only write structuredXML SCHEMA queries for an XML retrieval system if they have some minimal knowledge about the schema of the collection.
We display the query on four lines for typographical convenience, but it is intended to be read as one unit without line breaks.
As in XPath double slashes indicate that an arbitrary number of elements can intervene on a path.
The dot in a clause in square brackets refers to the element the clause modifies.
Similarly, the dot in [about(., summer holidays)] refers to the section that the clause modifies.
The about clause is a ranking constraint: Sections that occur in the right type of article are to be ranked according to how relevant they are to the topic summer holidays.
We usually handle relational attribute constraints by prefiltering or postfiltering: We simply exclude all elements from the result set that do not meet the relational attribute constraints.
In this chapter, we will not address how to do this efficiently and instead focus on the core information retrieval problem in XML retrieval, namely how to rank documents according to the relevance criteria expressed in the about conditions of the NEXI query.
If we discard relational attributes, we can represent documents as trees with only one type of node: element nodes.
In other words, we remove all attribute nodes from the XML document, such as the number attribute in Figure 10.1
We can represent queries as trees in the same way.
This is a query-byexample approach to query language design because users pose queries by creating objects that satisfy the same formal description as documents.
In this section, we discuss a number of challenges that make structured retrieval more difficult than unstructured retrieval.
The first challenge in structured retrieval is that users want us to return parts of documents (i.e., XML elements), not entire documents as IR systems usually do in unstructured retrieval.
If we query Shakespeare’s plays for Macbeth’s castle, should we return the scene, the act or the entire play in Figure 10.2? In this case, the user is probably looking for the scene.
On the other hand, an otherwise unspecified search for Macbeth should return the play of this name, not a subunit.
A system should always retrieve the most specific part of a document answering the query.
This principle motivates a retrieval strategy that returns the smallest unit that contains the information sought, but does not go below this level.
However, it can be hard to implement this principle algorithmically.
The title of the tragedy, Macbeth, and the title of Act I, Scene vii, Macbeth’s castle, are both good hits because they contain the matching term Macbeth.
But in this case, the title of the tragedy, the higher node, is preferred.
Deciding which level of the tree is right for answering a query is difficult.
Parallel to the issue of which parts of a document to return to the user is the issue of which parts of a document to index.
In unstructured retrieval, it is usually clear what the right document.
To represent the semantics of NEXI queries fully we would also need to designate one node in the tree as a “target node”, for example, the section in the tree in Figure 10.3
Without the designation of a target node, the tree in Figure 10.3 is not a search for sections embedded in articles (as specified by NEXI), but a search for articles that contain sections.
Figure 10.5 Partitioning an XML document into non-overlapping indexing units.
In structured retrieval, there are a number of different approaches to defining the indexing unit.
One approach is to group nodes into non-overlapping pseudodocuments as shown in Figure 10.5
In the example, books, chapters and sections have been designated to be indexing units, but without overlap.
For example, the leftmost dashed indexing unit contains only those parts of the tree dominated by book that are not already part of other indexing units.
The disadvantage of this approach is that pseudodocuments may not make sense to the user because they are not coherent units.
For instance, the leftmost indexing unit in Figure 10.5 merges three disparate elements, the class, author and title elements.
We can also use one of the largest elements as the indexing unit, for example, the book element in a collection of books or the play element for Shakespeare’s works.
We can then postprocess search results to find for each book or play the subelement that is the best hit.
For example, the query Macbeth’s castle may return the play Macbeth, which we can then postprocess to identify act I, scene vii as the best-matching subelement.
Unfortunately, this twostage retrieval process fails to return the best subelement for many queries because the relevance of a whole book is often not a good predictor of the relevance of small subelements within it.
Instead of retrieving large units and identifying subelements (top down), we can also search all leaves, select the most relevant ones and then extend them to larger units in postprocessing (bottom up)
For the query Macbeth’s castle in Figure 10.1, we would retrieve the title Macbeth’s castle in the first pass and then decide in a postprocessing step whether to return the title, the scene, the act or the play.
This approach has a similar problem as the last one: The relevance of a leaf element is often not a good predictor of the relevance.
Many XML elements are not meaningful search results, e.g., typographical elements like <b>definitely</b> or an ISBN number which cannot be interpreted without context.
Also, indexing all elements means that search results will be highly redundant.
For the query Macbeth’s castle and the document in Figure 10.1, we would return all of the play, act, scene and title elements on the path between the root node and Macbeth’s castle.
The leaf node would then occur four times in the result set, once directly and three times as part of other elements.
We call elements that are contained within each other nested.
Returning redundant nested elements in a list ofNESTED ELEMENTS returned hits is not very user-friendly.
Because of the redundancy caused by nested elements it is common to restrict the set of elements that are eligible to be returned.
In most of these approaches, result sets will still contain nested elements.
Thus, we may want to remove some elements in a postprocessing step to reduce redundancy.
Alternatively, we can collapse several nested elements in the results list and use highlighting of query terms to draw the user’s attention to the relevant passages.
If query terms are highlighted, then scanning a medium-sized element (e.g., a section) takes little more time than scanning a small subelement (e.g., a paragraph)
Thus, if the section and the paragraph both occur in the results list, it is sufficient to show the section.
An additional advantage of this approach is that the paragraph is presented together with its context (i.e., the embedding section)
This context may be helpful in interpreting the paragraph (e.g., the source of the information reported) even if the paragraph on its own satisfies the query.
If the user knows the schema of the collection and is able to specify the desired type of element, then the problem of redundancy is alleviated as few nested elements have the same type.
But as we discussed in the introduction, users often don’t know what the name of an element in the collection is (Is the Vatican a country or a city?) or they may not know how to compose structured queries at all.
For example, the term Gates under the node author is unrelated to an occurrence under a content node like section if used to refer to the plural of gate.
It makes little sense to compute a single document frequency for Gates in this example.
A compromise is only to consider the parent node x of the term and not the rest of the path from the root to x to distinguish contexts.
There are still conflations of contexts that are harmful in this scheme.
For instance, we do not distinguish names of authors and names of corporations if both have the parent node name.
But most important distinctions, like the example contrast author#"Gates" vs.
In many cases, several different XML schemas occur in a collection since the XML documents in an IR application often come from more than one source.
In other cases, the structural organization of the schemas may be different: Author.
Some form of approximate matching of element names in combination with semi-automatic matching of different document structures can help here.
Human editing of correspondences of elements in different schemas will usually do better than automatic methods.
Another reason is that users often are not familiar with the element names and the structure of the schemas of collections they search as mentioned.
This poses a challenge for interface design in XML retrieval.
Ideally, the user interface should expose the tree structure of the collection and allow users to specify the elements they are querying.
If we take this approach, then designing the query interface in structured retrieval is more complex than a search box for keyword queries in unstructured retrieval.
We can also support the user by interpreting all parent-child relationships in queries as descendant relationships with any number of intervening nodes allowed.
We show edges that are interpreted as descendant relationships as dashed arrows.
It is convenient for users to be able to issue such extended queries without having to specify the exact structural configuration in which a query term should occur – either because they do not care about the exact configuration or because they do not know enough about the schema of the collection to be able to specify it.
Suppose there is no such chapter in the collection, but that there are references to books on FFT (d4)
A reference to a book on FFT is not exactly what the user is looking for, but it is better than returning nothing.
This is a case where we may want to interpret the structural constraints specified in the query as hints as opposed to as strict conditions.
As we will discuss in Section 10.4, users prefer a relaxed interpretation of structural constraints: Elements that do not meet structural constraints perfectly should be ranked lower, but they should not be omitted from search results.
Figure 10.7 A structural mismatch between two queries and a document.
In this section, we present a simple vector space model for XML retrieval.
It is not intended to be a complete description of a state-of-the-art system.
Instead, we want to give the reader a flavor of how documents can be represented and retrieved in XML retrieval.
In unstructured retrieval, there would be a single dimension of the vector space for Caesar.
In XML retrieval, we must separate the title word Caesar from the author name Caesar.
One way of doing this is to have each dimension of the vector space encode a word together with its position within the XML tree.
We first take each text node (which in our setup is always a leaf) and break it into multiple nodes, one for each word.
So the leaf node Bill Gates is split into two leaves Bill and Gates.
Next we define the dimensions of the vector space to be lexicalized subtrees of documents – subtrees that contain at least one vocabulary term.
A subset of these possible lexicalized subtrees is shown in the figure, but there are others – e.g., the subtree corresponding to the whole document with the leaf node Gates removed.
We can now represent queries and documents as vectors in this space of lexicalized subtrees and compute matches between them.
This means that we can use the vector space formalism from Chapter 6 for XML retrieval.
The main difference is that the dimensions of vector space.
Figure 10.8 A mapping of an XML document (left) to a set of lexicalized subtrees (right)
As we discussed in the last section users are bad at remembering details about the schema and at constructing queries that comply with the schema.
We will therefore interpret all queries as extended queries – that is, there can be an arbitrary number of intervening nodes in the document for any parentchild node pair in the query.
But we still prefer documents that match the query structure closely by.
We ensure that retrieval results respect this preference by computing a weight for each match.
A simple measure of the similarity of a path cq in a query and a path cd in a document is the following context resemblance function CR:CONTEXT.
Figure 10.10 Scoring of a query with one structural term in SIMNOMERGE.
The query-document similarity function in Figure 10.9 is called SIMNOMERGE because different XML contexts are kept separate for the purpose of weighting.
An alternative similarity function is SIMMERGE which relaxes the matching conditions of query and document further in the following three ways.
We collect the statistics used for computing weight(q, t, c) and weight(d, t, c) from all contexts that have a non-zero resemblance to c (as opposed to just from c as in SIMNOMERGE)
For instance, for computing the document frequency of the structural term atl#"recognition", we also count occurrences of recognition in XML contexts fm/atl, article//atl etc.
We modify Equation (10.2) by merging all structural terms in the document that have a non-zero context resemblance to a given query structural term.
These three changes alleviate the problem of sparse term statistics discussed in Section 10.2 and increase the robustness of the matching function against poorly posed structural queries.
The evaluation of SIMNOMERGE and SIMMERGE in the next section shows that the relaxed matching conditions of SIMMERGE increase the effectiveness of XML retrieval.
Write down all the structural terms occurring in the XML document in Figure 10.8
How many structural terms does the document in Figure 10.1 yield?
The premier venue for research on XML retrieval is the INEX (INitiative forINEX the Evaluation of XML retrieval) program, a collaborative effort that has produced reference collections, sets of queries, and relevance judgments.
A yearly INEX meeting is held to present and discuss research results.
Figure 10.11 Simplified schema of the documents in the INEX collection.
Since 2006 INEX uses the much larger English Wikipedia as a test collection.
Since CAS queries have both structural and content criteria, relevance assessments are more complicated than in unstructured retrieval.
The information sought is the main topic of the component and the component is a meaningful unit of information.
The information sought is the main topic of the component, but the component is not a meaningful (self-contained) unit of information.
The information sought is present in the component, but is not the main topic.
The information sought is not a topic of the component.
Components are judged on both dimensions and the judgments are then combined into a digit-letter code.
In theory, there are 16 combinations of coverage and relevance, but many cannot occur.
For example, a nonrelevant component cannot have exact coverage, so the combination 3N is not possible.
A 2S component provides incomplete information and may be difficult to interpret without more context, but it does answer the query partially.
The number of relevant components in a retrieved set A of components can then be computed as:
As an approximation, the standard definitions of precision, recall and F from Chapter 8 can be applied to this modified definition of relevant items retrieved, with some subtleties because we sum graded as opposed to binary relevance assessments.
See the references on focused retrieval in Section 10.6 for further discussion.
One flaw of measuring relevance this way is that overlap is not accounted for.
Much of the recent focus at INEX has been on developing algorithms and evaluation measures that return non-redundant results lists and evaluate them properly.
The better run is the SIMMERGE run, which incorporates few structural constraints and mostly relies on keyword matching.
SIMMERGE’s median average precision (where the median is with respect to average precision numbers over topics) is only 0.147
Effectiveness in XML retrieval is often lower than in unstructured retrieval since XML retrieval is harder.
Instead of just finding a document, we have to find the subpart of a document that is most relevant to the query.
Also, XML retrieval effectiveness – when evaluated as described here – can be lower than unstructured retrieval effectiveness on a standard evaluation because graded judgments lower measured performance.
Table 10.3 gives us a sense of the typical performance of XML retrieval, but it does not compare structured with unstructured retrieval.
Table 10.4 directly shows the effect of using structure in retrieval.
The contentonly system treats queries and documents as unstructured bags of words.
The full-structure model ranks elements that satisfy structural constraints higher than elements that do not.
For instance, for the query in Figure 10.3 an element that contains the phrase summer holidays in a section will be rated higher than one that contains it in an abstract.
The table shows that structure helps increase precision at the top of the results list.
Structured retrieval imposes additional constraints on what to return and documents that pass the structural filter are more likely to be relevant.
Recall may suffer because some relevant documents will be filtered out, but for precision-oriented tasks structured retrieval is superior.
In the type of structured retrieval we cover in this chapter, XML structure serves as a framework within which we match the text of the query with the text of the XML documents.
This exemplifies a system that is optimized for text-centric XML.
While both text and structure are important, we give higherTEXT-CENTRIC XML priority to text.
We do this by adapting unstructured retrieval methods to handling additional structural constraints.
The premise of our approach is that XML document retrieval is characterized by (i) long text fields (e.g., sections of a document), (ii) inexact matching, and (iii) relevance-ranked results.
Relational databases do not deal well with this use case.
When querying data-centric XML, we want to impose exact match conditions in most cases.
This puts the emphasis on the structural aspects of XML documents and queries.
Find employees whose salary is the same this month as it was 12 months ago.
It is purely structural and an exact matching of the salaries in the two time periods is probably sufficient to meet the user’s information need.
Text-centric approaches are appropriate for data that are essentially text documents, marked up as XML to capture document structure.
This is becoming a de facto standard for publishing text databases since most text documents have some form of interesting structure – paragraphs, sections, footnotes etc.
Examples include assembly manuals, issues of journals, Shakespeare’s collected works and newswire articles.
Data-centric approaches are commonly used for data collections with complex structures that mainly contain non-text data.
A text-centric retrieval engine will have a hard time with proteomic data in bioinformatics or with the representation of a city map that (together with street names and other textual descriptions) forms a navigational database.
Two other types of queries that are difficult to handle in a text-centric structured retrieval model are joins and ordering constraints.
The query for employees with unchanged salary requires a join.
Retrieve the chapter of the book Introduction to algorithms that follows the chapter Binomial heaps.
This query relies on the ordering of elements in XML – in this case the ordering of chapter elements underneath the book node.
There are powerful query languages for XML that can handle numerical attributes, joins and ordering constraints.
The best known of these is XQuery, a language proposed for standardization by the W3C.
It is designed to be broadly applicable in all areas where XML is used.
Due to its complexity, it is challenging to implement an XQuery-based ranked retrieval system with the performance characteristics that users have come to expect in information retrieval.
This is currently one of the most active areas of research in XML retrieval.
Relational databases are better equipped to handle many structural constraints, particularly joins (but ordering is also difficult in a database framework – the tuples of a relation in the relational calculus are not ordered)
For this reason, most data-centric XML retrieval systems are extensions of relational databases (see the references in Section 10.6)
If text fields are short, exact matching meets user needs and retrieval results in form of unordered sets are acceptable, then using a relational database for XML retrieval is appropriate.
Geva (2006) argue that XML retrieval is a form of passage retrieval.
While element boundaries in XML documents are cues for identifying good segment boundaries between passages, the most relevant passage often does not coincide with an XML element.
Exercise 10.4Find a reasonably sized XML document collection (or a collection using a markup language different from XML like HTML) on the web and download it.
Jon Bosak’s XML edition of Shakespeare and of various religious works at http://www.ibiblio.org/bosak/ or the first 10,000 documents of the Wikipedia are good choices.
Explain why an XML retrieval system would be able to exploit the XML structure of the documents to achieve better retrieval results on the topics than an unstructured retrieval system.
Give an example of an information need that we can answer correctly if we index all lexicalized subtrees, but cannot answer if we only index structural terms.
If we index all structural terms, what is the size of the index as a function of text size?
If we index all lexicalized subtrees, what is the size of the index as a function of text size?
Give an example of a query-document pair for which SIMNOMERGE(q, d) is larger than 1.0
In this chapter, we more systematically introduce this probabilistic approach to IR, which provides a different formal basis for a retrieval model and results in different techniques for setting term weights.
Users start with information needs, which they translate into query representations.
Similarly, there are documents, which are converted into document representations (the latter differing at least by how text is tokenized, but perhaps containing fundamentally less information, as when a non-positional index is used)
Based on these two representations, a system tries to determine how well documents satisfy information needs.
In the Boolean or vector space models of IR, matching is done in a formally defined but semantically imprecise calculus of index terms.
Given only a query, an IR system has an uncertain understanding of the information need.
Given the query and document representations, a system has an uncertain guess of whether a document has content relevant to the information need.
Probability theory provides a principled foundation for such reasoning under uncertainty.
This chapter provides one answer as to how to exploit this foundation to estimate how likely it is that a document is relevant to an information need.
There is more than one possible retrieval model which has a probabilistic basis.
In Chapter 12, we then present the alternative probabilistic language modelOnline edition (c)
Without making any assumptions, the probability of a joint event equals the probability of one of the events multiplied by the probability of the other event conditioned on knowing the first event happened.
Writing P(A) for the complement of an event, we similarly have:
Probability theory also has a partition rule, which says that if an event B canPARTITION RULE be divided into an exhaustive set of disjoint subcases, then the probability of B is the sum of the probabilities of the subcases.
From these we can derive Bayes’ Rule for inverting conditional probabili-BAYES’ RULE ties:
This equation can also be thought of as a way of updating probabilities.
We start off with an initial estimate of how likely the event A is when we do not have any other information; this is the prior probability P(A)
Bayes’ rulePRIOR PROBABILITY lets us derive a posterior probability P(A|B) after having seen the evidence B,POSTERIOR.
Finally, it is often useful to talk about the odds of an event, which provideODDS a kind of multiplier for how probabilities change:
We assume a ranked retrieval setup as in Section 6.3, where there is a collection of documents, the user issues a query, and an ordered list of documents is returned.
For a query q and a document d in the collection, let Rd,q be an indicator random variable that says whether d is relevant with respect to a given query q.
In context we will often write just R for Rd,q.
Using a probabilistic model, the obvious order in which to present documents to the user is to rank documents by their estimated probability of relevance with respect to the information need: P(R = 1|d, q)
In the simplest case of the PRP, there are no retrieval costs or other utility concerns that would differentially weight actions or errors.
You lose a point for either returning a nonrelevant document or failing to return a relevant document (such a binary situation where you are evaluated on your accuracy is called 1/0 loss)
The goal is to return the best possible results as the top k1/0 LOSS documents, for any value of k the user chooses to examine.
The PRP then says to simply rank all documents in decreasing order of P(R = 1|d, q)
If a set of retrieval results is to be returned, rather than an ordering, the BayesBAYES OPTIMAL.
It is the probability of an event or data according to a model.
The term is usually used when people are thinking of holding the data fixed, while varying the model.
Optimal Decision Rule, the decision which minimizes the risk of loss, is to simply return documents that are more likely relevant than nonrelevant:
The PRP is optimal, in the sense that it minimizes the expected loss (also known as the Bayes risk) under 1/0 loss.BAYES RISK.
Nevertheless, the PRP still provides a very useful foundation for developing models of IR.
The Binary Independence Model (BIM) we present in this section is the modelBINARY INDEPENDENCE.
It introduces some simple assumptions, which make estimating the probability function P(R|d, q) practical.
Here, “binary” is equivalent to Boolean: documents and queries are both represented as binary term incidence vectors.
With this representation, many possible documents have the same vector representation.
In a sense this assumption is equivalent to an assumption of the vector space model, where each term is a dimension that is orthogonal to all other terms.
We will first present a model which assumes that the user has a single step information need.
As discussed in Chapter 9, seeing a range of results might let the user refine their information need.
Fortunately, as mentioned there, it is straightforward to extend the Binary Independence Model so as to provide a framework for relevance feedback, and we present this model in Section 11.3.4
To make a probabilistic retrieval strategy precise, we need to estimate how terms in documents contribute to relevance, specifically, we wish to know how term frequency, document frequency, document length, and other statistics that we can compute influence judgments about document relevance, and how they can be reasonably combined to estimate the probability of document relevance.
We then order documents by decreasing estimated probability of relevance.
We assume here that the relevance of each document is independent of the relevance of other documents.
You should think of this quantity as defined with respect to a space of possible documents in a domain.
How do we compute all these probabilities? We never know the exact probabilities, and so we have to use estimates: Statistics about the actual document collection are used to estimate these probabilities.
Since a document is either relevant or nonrelevant to a query, we must have that:
Given a query q, we wish to order returned documents by descending P(R = 1|d, q)
Under the BIM, this is modeled as ordering by P(R = 1|~x,~q)
Rather than estimating this probability directly, because we are interested only in the ranking of documents, we work with some other quantities which are easier to compute and which give the same ordering of documents.
In particular, we can rank documents by their odds of relevance (as the odds of relevance is monotonic with the probability of relevance)
This makes things easier, because we can ignore the common denominator in (11.8), giving:
The left term in the rightmost expression of Equation (11.10) is a constant for a given query.
Since we are only ranking documents, there is thus no need for us to estimate it.
The right-hand term does, however, require estimation, and this initially appears to be difficult: How can we accurately estimate the probability of an entire term incidence vector occurring? It is at this point that we make the Naive Bayes conditional independence assumption that the presenceNAIVE BAYES.
These quantities can be visualized in the following contingency table where the columns add to 1:
Let us make an additional simplifying assumption that terms not occurring in the query are equally likely to occur in relevant and nonrelevant documents: that is, if qt = 0 then pt = ut.
This assumption can be changed, as when doing relevance feedback in Section 11.3.4
The left product is over query terms found in the document and the right product is over query terms not found in the document.
We can manipulate this expression by including the query terms found in the document into the right product, but simultaneously dividing through by them in the left product, so the value is unchanged.
The left product is still over query terms found in the document, but the right product is now over all query terms.
That means that this right product is a constant for a particular query, just like the oddsO(R|~q)
So the only quantity that needs to be estimated to rank documents for relevance to a query is the left product.
We can equally rank documents by the logarithm of this term, since log is a monotonic function.
The resulting quantity used for ranking is called the Retrieval Status Value (RSV) in this model:RETRIEVAL STATUS.
For each term t, what would these ct numbers look like for the whole collection? (11.19) gives a contingency table of counts of documents in the collection, where dft is the number of documents that contain term t:
Adding 12 in this way is a simple form of smoothing.
For trials with categorical outcomes (such as noting the presence or absence of a term), one way to estimate the probability of an event from data is simply to count the number of times an event occurred divided by the total number of trials.
This is referred to as the relative frequency of the event.
Estimating the prob-RELATIVE FREQUENCY ability as the relative frequency is the maximum likelihood estimate (or MLE),MAXIMUM LIKELIHOOD.
This is a form of maximum a posteriori (MAP) estimation, where weMAXIMUM A POSTERIORI.
Under the assumption that relevant documents are a very small percentage of the collection, it is plausible to approximate statistics for nonrelevant documents by statistics from the whole collection.
Under this assumption, ut (the probability of term occurrence in nonrelevant documents for a query) is dft/N and.
The approximation technique in Equation (11.22) cannot easily be extended to relevant documents.
We can use the frequency of term occurrence in known relevant documents (if we know some)
This is the basis of probabilistic approaches to relevance feedback weighting in a feedback loop, discussed in the next subsection.
Based on his data analysis, a plausible proposal would be to use the estimate pt = 13 +
Iterative methods of estimation, which combine some of the above ideas, are discussed in the next subsection.
We can use (pseudo-)relevance feedback, perhaps in an iterative process of estimation, to get a more accurate estimate of pt.
This can be done using the probability estimates of the previous section.
We reestimate pt and ut on the basis of known relevant and nonrelevant documents.
If the sets VR and VNR are large enough, we may be able to estimate these quantities directly from these documents as maximum likelihood estimates:
However, the set of documents judged by the user (V) is usually very small, and so the resulting statistical estimate is quite unreliable (noisy), even if the estimate is smoothed.
So it is often better to combine the new information with the original guess in a process of Bayesian updating.
Here p(k)t is the k th estimate for pt in an iterative updating process and.
Repeat the above process from step 2, generating a succession of approximations to R and hence pt, until the user is satisfied.
It is also straightforward to derive a pseudo-relevance feedback version of this algorithm, where we simply pretend that VR = V.
Determine a guess for the size of the relevant document set.
If unsure, a conservative (too small) guess is likely to be best.
This motivates use of a fixed size set V of highest ranked documents.
If we let Vt be the subset of documents in V containing xt and use add 12 smoothing, we get:
Go to step 2 until the ranking of the returned results converges.
Once we have a real estimate for pt then the ct weights used in the RSV value look almost like a tf-idf value.
What are the differences between standard vector space tf-idf weighting and the BIM probabilistic retrieval model (in the case where no document relevance information is available)?
Describe the differences between vector space relevance feedback and probabilistic relevance feedback.
Probabilistic methods are one of the oldest formal models in IR.
Traditionally, probabilistic IR has had neat ideas but the methods have never won on performance.
It is perhaps the severity of the modeling assumptions that makes achieving good performance difficult.
A general problem seems to be that probabilistic models either require partial relevance information or else only allow for deriving apparently inferior term weighting models.
For a probabilistic IR system, it’s just that, at the end, you score queries not by cosine similarity and tf-idf in a vector space, but by a slightly different formula motivated by probability theory.
Indeed, sometimes people have changed an existing vector-space IR system into an effectively probabilistic system simply by adopted term weighting formulas from probabilistic models.
In this section, we briefly present three extensions of the traditional probabilistic model, and in the next chapter, we look at the somewhat different probabilistic language modeling approach to IR.
Some of the assumptions of the BIM can be removed.
For example, we can remove the assumption that terms are independent.
A case that particularly violates this assumption is term pairs like Hong and Kong, which are strongly dependent.
But dependencies can occur in various complex configurations, such as between the set of terms New, York, England, City, Stock, Exchange, and University.
In this model each term can be directly dependent on only one other term, giving a tree structure of dependencies.
This variant behaves slightly strangely: if a term occurs in over half the documents in the collection then this model gives a negative term weight, which is presumably undesirable.
We can improve on Equation (11.30) by factoring in the frequency of each term and document length:
If the query is long, then we might also use similar weighting for query terms.
This is appropriate if the queries are paragraph long information needs, but unnecessary for short queries.
In the equation presented, there is no length normalization of queries (it is as if b = 0 here)
Length normalization of the query is unnecessary because retrieval is being done with respect to a single fixed query.
The tuning parameters of these formulas should ideally be set to optimize performance on a development test collection (see page 153)
That is, we can search for values of these parameters that maximize performance on a separate development test collection (either manually or with optimization methods such as grid search or something more advanced), and then use these parameters on the actual test collection.
The first part of the expression reflects relevance feedback (or just idf weighting if no relevance information is available), the second implements document term frequency and document length scaling, and the third considers term frequency in the query.
We skip the details because fully introducing the formalism of Bayesian networks would require much too much space, but conceptually, Bayesian networks use directed graphs to show probabilistic dependencies between variables, as in Figure 11.1, and have led to the development of sophisticated algorithms for propagating influence so as to allow learning and inference with arbitrary knowledge within arbitrary directed acyclic graphs.
Turtle and Croft used a sophisticated network to better model the complex dependencies between a document and a user’s information need.
The model decomposes into two parts: a document collection network and a query network.
The document collection network is large, but can be precomputed: it maps from documents to terms to concepts.
The concepts are a thesaurus-based expansion of the terms appearing in the document.
The query network is relatively small but a new network needs to be built each time a query comes in, and then attached to the document network.
The query network maps from query terms, to query subexpressions (built using probabilistic or “noisy” versions of AND and OR operators), to the user’s information need.
The result is a flexible probabilistic network which can generalize various simpler Boolean and probabilistic models.
The system allowed efficient large-scale retrieval, and was the basis of the InQuery text retrieval system, built at the University of Massachusetts.
This system performed very well in TREC evaluations and for a time was sold commercially.
On the other hand, the model still used various approximations and independence assumptions to make parameter estimation and computation possible.
There has not been much follow-on work along these lines, but we would note that this model was actually built very early on in the modern era of using Bayesian networks, and there have been many subsequent developments in the theory, and the time is perhaps right for a new generation of Bayesian network-based information retrieval systems.
An introduction to Bayesian utility theory can be found in (Ripley 1996)
The open-source Indri search engine, which is distributed with the Lemur toolkit (http://www.lemurproject.org/) merges ideas from Bayesian inference networks and statistical language modeling approaches (see Chapter 12), in particular preserving the former’s support for structured query operators.
A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query.
The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often.
What do we mean by a document model generating a query? A traditional generative model of a language, of the kind familiar from formal languageGENERATIVE MODEL theory, can be used either to recognize or to generate strings.
For example, the finite automaton shown in Figure 12.1 can generate strings that include the examples shown.
Figure 12.2 A one-state finite automaton that acts as a unigram language model.
We show a partial specification of the state emission probabilities.
Finite automata can have outputs attached to either their states or their arcs; we use states here, because that maps directly on to the way probabilistic automata are usually formalized.
Example 12.1: To find the probability of a word sequence, we just multiply theprobabilities which the model gives to each word in the sequence, together with the probability of continuing or stopping after producing each word.
In the IR context that we are leading up to, taking the stop probability to be fixed across models seems reasonable.
This is because we are generating queries, and the length distribution of queries is fixed and independent of the document from which we are generating the language model.
The language model that gives the higher probability to the sequence of terms is more likely to have generated the term sequence.
This time, we will omit STOP probabilities from our calculations.
How do we build probabilities over sequences of terms? We can always use the chain rule from Equation (11.1) to decompose the probability of a sequence of events into the probability of each successive event conditioned on earlier events:
Such a model is called a unigram language model:UNIGRAM LANGUAGE.
Such models are vital for tasks like speech recognition, spelling correction, and machine translation, where you need the probability of a term conditioned on surrounding context.
However, most language-modeling work in IR has used unigram language models.
Unigram models are often sufficient to judge the topic of a text.
Moreover, as we shall see, IR language models are frequently estimated from a single document and so it is.
Losses from data sparseness (see the discussion on page 260) tend to outweigh any gains from richer models.
In addition, unigram models are more efficient to estimate and apply than higher-order models.
Even though there is no conditioning on preceding context, this model nevertheless still gives the probability of a particular ordering of terms.
However, any other ordering of this bag of terms will have the same probability.
We could instead just refer to the model as a multinomial model.
From this perspective, the equations presented above do not present the multinomial probability of a bag of words, since they do not sum over all possible orderings of those words, as is done by the multinomial coefficient (the first term on the right-hand side) in the standard presentation of a multinomial model:
The fundamental problem in designing language models is that we do not know what exactly we should use as the model Md.
However, we do generally have a sample of text that is representative of that model.
This problem makes a lot of sense in the original, primary uses of language models.
For example, in speech recognition, we have a training sample of (spoken) text.
But we have to expect that, in the future, users will use different words and in.
This interpretation is not so clear in the IR case, where a document is finite and usually fixed.
We pretend that the document d is only a representative sample of text drawn from a model distribution, treating it like a fine-grained topic.
We then estimate a language model from this sample, and use that model to calculate the probability of observing any word sequence, and, finally, we rank documents according to their probability of generating the query.
Language modeling is a quite general formal approach to IR, with many variant realizations.
The original and basic method for using language models in IR is the query likelihood model.
Our goal is to rank documents by P(d|q), where the probability of a document is interpreted as the likelihood that it is relevant to the query.
P(q) is the same for all documents, and so can be ignored.
The prior probability of a document P(d) is often treated as uniform across all d and so it can also be ignored, but we could implement a genuine prior which could include criteria like authority, length, genre, newness, and number of previous people who have read the document.
But, given these simplifications, we return results ranked by simply P(q|d), the probability of the query q under the language model derived from d.
The Language Modeling approach thus attempts to model the query generation process: Documents are ranked by the probability that a query would be observed as a random sample from the respective document model.
The most common way to do this is using the multinomial unigram language model, which is equivalent to a multinomial Naive Bayes model (page 263), where the documents are the classes, each treated in the estimation as a separate “language”
For retrieval based on a language model (henceforth LM), we treat the generation of queries as a random process.
Estimate P(q|Mdi), the probability of generating the query according to each of these document models.
The intuition of the basic model is that the user has a prototype document in mind, and generates a query based on words that appear in this document.
Often, users have a reasonable idea of terms that are likely to occur in documents of interest and they will choose query terms that distinguish these documents from others in the collection.3 Collection statistics are an integral part of the language model, rather than being used heuristically as in many other approaches.
The probability of producing the query given the LM Md of document d using maximum likelihood.
The answer to this within the language modeling approach is translation language models, as briefly discussed in Section 12.4
That is, we just count up how often each word occurred, and divide through by the total number of words in the document d.
The general approach is that a non-occurring term should be possible in a query, but its probability should be somewhat close to but no more likely than would be expected by chance from the whole collection.
An alternative is to use a language model built from the whole collection.
Both of these smoothing methods have been shown to perform well in IR experiments; we will stick with the linear interpolation smoothing method for the rest of this section.
While different in detail, they are both conceptually similar: in both cases the probability estimate for a word present in the document combines a discounted MLE and a fraction of the estimate of its prevalence in the whole collection, while for words not present in a document, the estimate is just a fraction of the estimate of the prevalence of the word in the whole collection.
The role of smoothing in LMs for IR is not simply or principally to avoid estimation problems.
This was not clear when the models were first proposed, but it is now understood that smoothing is essential to the good properties.
To summarize, the retrieval ranking for a query q under the basic LM for IR we have been considering is given by:
This equation captures the probability that the document that the user had in mind was in fact d.
Ponte and Croft (1998) present the first experiments on the language modeling approach to information retrieval.
Their basic approach is the model that we have presented until now.
The use of multinomials has been standard in most subsequent work in the LM approach and experimental results in IR, as well as evidence from text classification which we consider in Section 13.3
The version of tf-idf from the INQUERY IR system includes length normalization of tf.
The language modeling approach always does better in these experiments, but note that where the approach shows significant gains is at higher levels of recall.
Ponte and Croft argued strongly for the effectiveness of the term weights that come from the language modeling approach over traditional tf-idf weights.
The language modeling approach yields significantly better results than their baseline tf-idf based term weighting approach.
And indeed the gains shown here have been extended in subsequent work.
Under a MLE-estimated unigram probability model, what are P(the) and P(martian)?
Suppose we have a collection that consists of the 4 documents given in the below table.
Build a query likelihood language model for this document collection.
Assume a mixture model between the documents and the collection, with both weighted at 0.5
Maximum likelihood estimation (mle) is used to estimate both as unigram models.
Work out the model probabilities of the queries click, shears, and hence click shears for each document, and use those probabilities to rank the documents returned by each query.
What is the final ranking of the documents for the query click shears?
Include whether it is present in the model or not and whether the effect is raw or scaled.
In the mixture model approach to the query likelihood model (Equation (12.12)), the probability estimate of a term is based on the term frequency of a word in a document, and the collection frequency of the word.
Doing this certainly guarantees that each term of a query (in the vocabulary) has a non-zero chance of being generated by each document.
In particular, include in your answer a concrete numeric example showing this term weighting at work.
The language modeling approach provides a novel way of looking at the problem of text retrieval, which links it with a lot of recent work in speech.
As Ponte and Croft (1998) emphasize, the language modeling approach to IR provides a different approach to scoring matches between queries and documents, and the hope is that the probabilistic language modeling foundation improves the weights that are used, and hence the performance of the model.
The major issue is estimation of the document model, such as choices of how to smooth it effectively.
Compared to other probabilistic approaches, such as the BIM from Chapter 11, the main difference initially appears to be that the LM approach does away with explicitly modeling relevance (whereas this is the central variable evaluated in the BIM approach)
But this may not be the correct way to think about things, as some of the papers in Section 12.5 further discuss.
The LM approach assumes that documents and expressions of information needs are objects of the same type, and assesses their match by importing the tools and methods of language modeling from speech and natural language processing.
The resulting model is mathematically precise, conceptually simple, computationally tractable, and intuitively appealing.
This seems similar to the situation with XML retrieval (Chapter 10): there the approaches that assume queries and documents are objects of the same type are also among the most successful.
On the other hand, like all IR models, you can also raise objections to the model.
The assumption of equivalence between document and information need representation is unrealistic.
Current LM approaches use very simple models of language, usually unigram models.
Without an explicit notion of relevance, relevance feedback is difficult to integrate into the model, as are user preferences.
It also seems necessary to move beyond a unigram model to accommodate notions of phrase or passage matching or Boolean retrieval operators.
Subsequent work in the LM approach has looked at addressing some of these concerns, including putting relevance back into the model and allowing a language mismatch between the query language and the document language.
Term frequency is directly represented in tf-idf models, and much recent work has recognized the importance of document length normalization.
The effect of doing a mixture of document generation probability with collection generation probability is a little like idf: terms rare in the general collection but common in some documents will have a greater influence on the ranking of documents.
In most concrete realizations, the models share treating terms as if they were independent.
On the other hand, the intuitions are probabilistic rather than geometric, the mathematical models are more principled rather than heuristic, and the details of how statistics like term frequency and document length are used differ.
If you are concerned mainly with performance numbers, recent work has shown the LM approach to be very effective in retrieval experiments, beating tf-idf and BM25 weights.
Figure 12.5 Three ways of developing the language modeling approach: (a) query likelihood, (b) document likelihood, and (c) model comparison.
In this section we briefly mention some of the work that extends the basic language modeling approach.
There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work.
Rather than looking at the probability of a document language model Md generating the query, you can look at the probability of a query language model Mq generating the document.
The main reason that doing things in this direction and creating a document likelihood model is less appealing is that there is much lessDOCUMENT.
On the other hand, it is easy to see how to incorporate relevance feedback into such a model: you can expand the query with terms taken from relevant documents in the usual way and hence update the language model Mq (Zhai and Lafferty 2001a)
The relevance model of Lavrenko and Croft (2001) is an instance of a document likelihood model, which incorporates pseudo-relevance feedback into a language modeling approach.
Rather than directly generating in either direction, we can make a language model from both the document and query, and then ask how different these two language models are from each other.
This model is clearly more computationally intensive and we need to build a translation model.
The translation model is usually built using separate resources (such as a traditional thesaurus or bilingual dictionary or a statistical machine translation system’s translation dictionary), but can be built using the document collection if there are pieces of text that naturally paraphrase or summarize other pieces of text.
Candidate examples are documents and their titles or abstracts, or documents and anchor-text pointing to them in a hypertext environment.
Building extended LM approaches remains an active area of research.
In general, translation models, relevance feedback models, and model comparOnline edition (c)
Zhai and Lafferty (2001b) clarify the role of smoothing in LMs for IR and present detailed empirical comparisons of different smoothing methods.
Zhai and Lafferty (2002) argue that a two-stage smoothing model with first Bayesian smoothing followed by linear interpolation gives a good model of the task, and performs better and more stably than a single form of smoothing.
A nice feature of the LM approach is that it provides a convenient and principled way to put various kinds of prior information into the model; Kraaij et al.
Thus far, this book has mainly discussed the process of ad hoc retrieval, where users have transient information needs that they try to address by posing one or more queries to a search engine.
For example, you might need to track developments in multicore computer chips.
One way of doing this is to issue the query multicore AND computer AND chip against an index of recent newswire articles each morning.
In this and the following two chapters we examine the question: How can this repetitive task be automated? To this end, many systems support standing queries.
A standing query is like any other query except that itSTANDING QUERY is periodically executed on a collection to which new documents are incrementally added over time.
If your standing query is just multicore AND computer AND chip, you will tend to miss many relevant new articles which use other terms such as multicore processors.
To achieve good recall, standing queries thus have to be refined over time and can gradually become quite complex.
In this example, using a Boolean search engine with stemming, you might end up with a query like (multicore OR multi-core) AND (chip OR processor OR microprocessor)
Given a set of classes, we seek to determine which class(es) a given object belongs to.
In the example, the standing query serves to divide new newswire articles into the two classes: documents about multicore computer chips and documents not about multicore computer chips.
A class need not be as narrowly focused as the standing query multicore computer chips.
Often, a class is a more general subject area like China or coffee.
Standing queries and topics differ in their degree of specificity, but the methods for.
We therefore include routing and filtering under the rubric of text classification in this and the following chapters.
The notion of classification is very general and has many applications within and beyond information retrieval (IR)
For instance, in computer vision, a classifier may be used to divide images into classes such as landscape, portrait, and neither.
We focus here on examples from information retrieval such as:
The automatic detection of spam pages (which then are not included in the search engine index)
The automatic detection of sexually explicit content (which is included in search results only if the user turns an option such as SafeSearch off)
Sentiment detection or the automatic classification of a movie or productSENTIMENT DETECTION review as positive or negative.
An example application is a user searching for negative reviews before buying a camera to make sure it has no undesirable features or quality problems.
It is easier to find messages in sorted folders than in a very large inbox.
The most common case of this application is a spam folder that holds all suspected spam messages.
Vertical search engines restrict searches toVERTICAL SEARCH ENGINE a particular topic.
This is because the vertical search engine does not include web pages in its index that contain the term china in a different sense (e.g., referring to a hard white ceramic), but does include relevant pages even if they do not explicitly mention the term China.
This list shows the general importance of classification in IR.
Most retrieval systems today contain multiple components that use some form of classifier.
The classification task we will use as an example in this book is text classification.
Books in a library are assigned Library of Congress categories by a librarian.
The multicore computer chips example illustrates one alternative approach: classification by the use of standing queries – which can be thought of as rules – most commonly written by hand.
A rule captures a certain combination of keywords that indicates a class.
Hand-coded rules have good scaling properties, but creating and maintaining them over time is labor intensive.
A technically skilled person (e.g., a domain expert who is good at writing regular expressions) can create rule sets that will rival or exceed the accuracy of the automatically generated classifiers we will discuss shortly; however, it can be hard to find someone with this specialized skill.
Apart from manual classification and hand-crafted rules, there is a third approach to text classification, namely, machine learning-based text classification.
It is the approach that we focus on in the next several chapters.
In machine learning, the set of rules or, more generally, the decision criterion of the text classifier, is learned automatically from training data.
The need for manual classification is not eliminated because the training documents come from a person who has labeled them – where labeling refers to the process of annotatingLABELING each document with its class.
But labeling is arguably an easier task than writing rules.
Almost anybody can look at a document and decide whether or not it is related to China.
Sometimes such labeling is already implicitly part of an existing workflow.
Figure 13.1 Classes, training set, and test set in text classification.
The classes in text classification often have some interesting structure such as the hierarchy in Figure 13.1
There are two instances each of region categories, industry categories, and subject area categories.
A hierarchy can be an important aid in solving a classification problem; see Section 15.3.2 for further discussion.
Until then, we will make the assumption in the text classification chapters that the classes form a set with no subset relationships between them.
Definition (13.1) stipulates that a document is a member of exactly one class.
This is not the most appropriate model for the hierarchy in Figure 13.1
For instance, a document about the 2008 Olympics should be a member of two classes: the China class and the sports class.
For the time being, we only consider one-of problems where a document is a member of exactly one class.
Our goal in text classification is high accuracy on test data or new data – for example, the newswire articles that we will encounter tomorrow morning in the multicore chip example.
It is easy to achieve high accuracy on the training set (e.g., we can simply memorize the labels)
But high accuracy on the training set in general does not mean that the classifier will work well on.
When we use the training set to learn a classifier for test data, we make the assumption that training data and test data are similar or from the same distribution.
The first supervised learning method we introduce is the multinomial NaiveMULTINOMIAL NAIVE BAYES Bayes or multinomial NB model, a probabilistic learning method.
The probability of a document d being in class c is computed as.
In text classification, our goal is to find the best class for the document.
The best class in NB classification is the most likely or maximum a posteriori (MAP)MAXIMUM A.
We will initially work with this intuitive interpretation of the multinomial NB model and defer a formal derivation to Section 13.4
The problem with the MLE estimate is that it is zero for a term–class combination that did not occur in the training data.
If the term WTO in the training data only occurred in China documents, then the MLE estimates for the other classes, for example UK, will be zero:
Figure 13.2 Naive Bayes algorithm (multinomial model): Training and testing.
The problem is that the zero probability for WTO cannot be “conditioned away,” no matter how strong the evidence for the class UK from other features.
The estimate is 0 because of sparseness: The training data are never large enoughSPARSENESS to represent the frequency of rare events adequately, for example, the frequency of WTO occurring in UK documents.
Add-one smoothing can be interpreted as a uniform prior (each term occurs once for each class) that is then updated as evidence from the training data comes in.
Note that this is a prior probability for the occurrence of a term as opposed to the prior probability of a class which we estimate in Equation (13.5) on the document level.
We have now introduced all the elements we need for training and applying an NB classifier.
Thus, the classifier assigns the test document to c = China.
The reason for this classification decision is that the three occurrences of the positive indicator Chinese in d5 outweigh the occurrences of the two negative indicators Japan and Tokyo.
Because we have to look at the data at least once, NB can be said to have optimal time complexity.
Its efficiency is one reason why NB is a popular text classification method.
We used Equation (13.8) to rank documents according to the probability that they are relevant to the query q.
In NB classification, we are usually only interested in the top-ranked class.
Our assumption here is that the length of test documents is bounded.
There are two different ways we can set up an NB classifier.
The model we introduced in the previous section is the multinomial model.
An alternative to the multinomial model is the multivariate Bernoulli model or Bernoulli model.
Figure 13.3 presents training and testing algorithms for the Bernoulli model.
The Bernoulli model has the same time complexity as the multinomial model.
The models also differ in how nonoccurring terms are used in classification.
This is because only the Bernoulli NB model models absence of terms explicitly.
The scores of the test document for the two classes are.
Thus, the classifier assigns the test document to c = not-China.
We can interpret Equation (13.10) as a description of the generative process we assume in Bayesian text classification.
The two models differ in the formalization of the second step, the generation of the document given the class, corresponding to the conditional distribution P(d|c):
For the Bernoulli model, we would have to estimate 2M|C| different parameters, one for each possible combination of M values ei and a class.
The number of parameters in the multinomial case has the same order of magniOnline edition (c)
We assume that attribute values are independent of each other given the class:
The class China generates values for each of the five term attributes (multinomial) or six binary attributes (Bernoulli) with a certain probability, independent of the values of the other attributes.
The fact that a document in the class China contains the term Taipei does not make it more likely or less likely that it also contains Beijing.
In reality, the conditional independence assumption does not hold for text data.
But as we will discuss shortly, NB models perform well despite the conditional independence assumption.
In fact, if the length of documents is not bounded, the number of parameters in the multinomial case is infinite.
Even when assuming conditional independence, we still have too many parameters for the multinomial model if we assume a different probability distribution for each position k in the document.
The position of a term in a document by itself does not carry information about the class.
The conditional independence assumption commits us to this way of processing the evidence.
Also, if we assumed different term distributions for each position k, we would have to estimate a different set of parameters for each k.
The probability of bean appearing as the first term of a coffee document could be different from it appearing as the second term, and so on.
This again causes problems in estimation owing to data sparseness.
For these reasons, we make a second independence assumption for the multinomial model, positional independence: The conditional probabilities forPOSITIONAL.
The independence assumptions reduce the number of parameters to be estimated by several orders of magnitude.
For a completely specified document generation model, we would also have to define a distribution P(nd|c) over lengths.
Without it, the multinomial model is a token generation model rather than a document generation model.
We compare the two models in Table 13.3, including estimation equations and decision rules.
Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language.
The conditional independence assumption states that features are independent of each other given the class.
The pairs hong and kong or london and enOnline edition (c)
Table 13.4 Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation.
In addition, the multinomial model makes an assumption of positional independence.
The Bernoulli model ignores positions in documents altogether because it only cares about absence or presence.
This bag-of-words model discards all information that is communicated by the order of words in natural language sentences.
How can NB be a good text classifier when its model of natural language is so oversimplified?
Even if it is not the method with the highest accuracy for text, NB has many virtues that make it a strong contender for text classification.
It excels if there are many equally important features that jointly contribute to the classification decision.
It is also somewhat robust to noise features (as defined in the next section) and concept drift – the gradual change over time of the con-CONCEPT DRIFT cept underlying a class like US president from Bill Clinton to George W.
This will then hurt them when documents in the following time period have slightly.
Table 13.5 A set of documents for which the NB independence assumptions are problematic.
The Bernoulli model is particularly robust with respect to concept drift.
We will see in Figure 13.8 that it can have decent performance when using fewer than a dozen terms.
The most important indicators for a class are less likely to change.
Thus, a model that only relies on these features is more likely to maintain a certain level of accuracy in concept drift.
NB’s main strength is its efficiency: Training and classification can be accomplished with one pass over the data.
Because it combines efficiency with good accuracy it is often used as a baseline in text classification research.
It is often the method of choice if (i) squeezing out a few extra percentage points of accuracy is not worth the trouble in a text classification application, (ii) a very large amount of training data is available and there is more to be gained from training on a lot of data than using a better classifier on a smaller training set, or (iii) if its robustness to concept drift can be exploited.
In this book, we discuss NB as a classifier for text.
However, it can be shown that NB is an optimal classifier (in the sense of minimal error rate on new data) for dataOPTIMAL CLASSIFIER where the independence assumptions do hold.
Figure 13.6 Basic feature selection algorithm for selecting the k best features.
The rationale for the positional independence assumption is that there is no useful information in the fact that a term occurs in position k of a document.
Table 13.3 gives Bernoulli and multinomial estimates for the word the.
Feature selection is the process of selecting a subset of the terms occurringFEATURE SELECTION in the training set and using only this subset as features in text classification.
First, it makes training and applying a classifier more efficient by decreasing the size of the effective vocabulary.
This is of particular importance for classifiers that, unlike NB, are expensive to train.
Second, feature selection often increases classification accuracy by eliminating noise features.
A noise feature is one that, whenNOISE FEATURE added to the document representation, increases the classification error on new data.
Suppose a rare term, say arachnocentric, has no information about a class, say China, but all instances of arachnocentric happen to occur in China documents in our training set.
Then the learning method might produce a classifier that misassigns test documents containing arachnocentric to China.
We can view feature selection as a method for replacing a complex classifier (using all features) with a simpler one (using a subset of the features)
Of the two NB models, the Bernoulli model is particularly sensitive to noise features.
A Bernoulli NB classifier requires some form of feature selection or else its accuracy will be low.
This section mainly addresses feature selection for two-class classification tasks like China versus not-China.
Section 13.5.5 briefly discusses optimizations for systems with more than two classes.
A common feature selection method is to compute A(t, c) as the expected mutual information (MI) of term t and class c.5 MI measures how much in-MUTUAL INFORMATION formation the presence/absence of a term contributes to making the correct classification decision on c.
We write Ut and Cc if it is not clear from context which term t and class c we are referring to.
At the bottom of the list for UK we find terms like peripherals and tonight (not shown in the figure) that are clearly not helpful in deciding.
We have omitted numbers and other special words from the top ten lists.
As you might expect, keeping the informative terms and eliminating the non-informative ones tends to reduce noise and improve the classifier’s accuracy.
For the Bernoulli model, F1 peaks early, at ten features selected.
At that point, the Bernoulli model is better than the multinomial model.
When basing a classification decision on only a few features, it is more robust to consider binary occurrence only.
For the multinomial model (MI feature selection), the peak occurs later, at 100 features, and its effectiveness recovers somewhat at.
Figure 13.8 Effect of feature set size on accuracy for multinomial and Bernoulli models.
The reason is that the multinomial takes the number of occurrences into account in parameter estimation and classification and therefore better exploits a larger number of features than the Bernoulli model.
Regardless of the differences between the two methods, using a carefully selected subset of the features results in better effectiveness than using all features.
For example, E11 is the expected frequency of t and c occurring together in a document assuming that term and class are independent.
An arithmetically simpler way of computing X2 is the following:
A third feature selection method is frequency-based feature selection, that is, selecting the terms that are most common in the class.
Frequency can be either defined as document frequency (the number of documents in the class c that contain the term t) or as collection frequency (the number of tokens of t that occur in documents in c)
Document frequency is more appropriate for the Bernoulli model, collection frequency for the multinomial model.
When many thousands of features are selected, then frequency-based feature selection often does well.
Thus, if somewhat suboptimal accuracy is acceptable, then frequency-based feature selection can be a good alternative to more complex methods.
However, Figure 13.8 is a case where frequencyOnline edition (c)
More commonly, feature selection statistics are first computed separately for each class on the two-class classification task c versus c and then combined.
One combination method computes a single figure of merit for each feature, for example, by averaging the values A(t, c) for feature t, and then selects the k features with highest figures of merit.
Another frequently used combination method selects the top k/n features for each of n classifiers and then combines these n sets into one global feature set.
Classification accuracy often decreases when selecting k common features for a system with n classifiers as opposed to n different sets of size k.
But even if it does, the gain in efficiency owing to a common document representation may be worth the loss in accuracy.
In Figure 13.7, kong is selected as the seventh term even though it is highly correlated with previously selected hong and therefore redundant.
Although such redundancy can negatively impact accuracy, non-greedy methods (see Section 13.7 for references) are rarely used in text classification due to their computational cost.
Historically, the classic Reuters-21578 collection was the main benchmark for text classification evaluation.
This is a collection of 21,578 newswire articles, originally collected and labeled by Carnegie Group, Inc.
The articles are assigned classes from a set of 118 topic categories.
A document may be assigned several classes or none, but the commonest case is single assignment (documents with at least one class received an average of 1.24 classes)
For each of these classifiers, we can measure recall, precision, and accuracy.
The distribution of documents in classes is very uneven, and some work evaluates systems on only documents in the ten largest classes.
A typical document with topics is shown in Figure 13.9
In Section 13.1, we stated as our goal in text classification the minimization of classification error on test data.
But as we discussed in Section 8.3, accuracy is not a good measure for “small” classes because always saying no, a strategy that defeats the purpose of building a classifier, will achieve high accuracy.
For small classes, precision, recall and F1 are better measures.
We will use effectiveness as a generic term for measures that evaluate theEFFECTIVENESS quality of classification decisions, including precision, recall, F1, and accuracy.
However, many researchers mean effectiveness, not efficiency of text classification when they use the term performance.
Macroaveraging gives equal weight to each class, whereas microaveraging gives equal weight to each per-document classification decision.
Because the F1 measure ignores true negatives and its magnitude is mostly determined by the number of true positives, large classes dominate small classes in microaveraging.
In the example, microaveraged precision (0.83) is much closer to the precision of.
Microaveraged results are therefore really a measure of effectiveness on the large classes in a test collection.
To get a sense of effectiveness on small classes, you should compute macroaveraged results.
To give a sense of the relative effectiveness of NB, we compare it with linear SVMs (rightmost column; see Chapter 15), one of the most effective classifiers, but also one that is more expensive to train than NB.
So there is a surprisingly small effectiveness penalty for its simplicity and efficiency.
However, on small classes, some of which only have on the order of ten positive examples in the training set, NB does much worse.
The table also compares NB with the other classifiers we cover in this book:
In addition, we give numbers for decision trees, an impor-DECISION TREES tant classification method we do not cover.
The bottom part of the table shows that there is considerable variation from class to class.
For instance, NB beats kNN on ship, but is much worse on money-fx.
Comparing parts (a) and (b) of the table, one is struck by the degree to which the cited papers’ results differ.
This is unfortunately typical of what happens when comparing different results in text classification: There are often differences in the experimental setup or the evaluation that complicate the interpretation of the results.
However, these differences may often be invisible or even reverse themselves when working in the real world where, usually, the training sample is drawn from a subset of the data to which the classifier will be applied, the nature of the data drifts over time rather than being stationary (the problem of concept drift we mentioned on page 269), and there may well be errors in the data (among other problems)
Many practitioners have had the experience of being unable to build a fancy classifier for a certain problem that consistently performs better than NB.
Our conclusion from the results in Table 13.9 is that, although most researchers believe that an SVM is better than kNN and kNN better than NB, the ranking of classifiers ultimately depends on the class, the document collection, and the experimental setup.
When performing evaluations like the one in Table 13.9, it is important to maintain a strict separation between the training set and the test set.
We can easily make correct classification decisions on the test set by using information we have gleaned from the test set, such as the fact that a particular term is a good predictor in the test set (even though this is not the case in the training set)
A more subtle example of using knowledge about the test set is to try a large number of values of a parameter (e.g., the number of selected features) and select the value that is best for the test set.
As a rule, accuracy on new data – the type of data we will encounter when we use the classifier in an application – will be much lower than accuracy on a test set that the classifier has been tuned for.
In a clean statistical text classification experiment, you should never run any program on or even look at the test set while developing a text classification system.
Instead, set aside a development set for testing while you developDEVELOPMENT SET your method.
When such a set serves the primary purpose of finding a good value for a parameter, for example, the number of selected features, then it is also called held-out data.
Train the classifier on the rest of the training setHELD-OUT DATA with different parameter values, and then select the value that gives best results on the held-out part of the training set.
Ideally, at the very end, when all parameters have been set and the method is fully specified, you run one final experiment on the test set and publish the results.
This ideal often cannot be met; researchers tend to evaluate several systems on the same test set over a period of several years.
But it is nevertheless highly important to not look at the test data and to run systems on it as sparingly as possible.
Beginners often violate this rule, and their results lose validity because they have implicitly tuned their system to the test data simply by running many variant systems and keeping the tweaks to the system that worked best on the test set.
The class priors in Figure 13.2 are computed as the fraction of documents in the class as opposed to the fraction of tokens in the class.
Based on the data in Table 13.10, (i) estimate a multinomial Naive Bayes classifier, (ii) apply the classifier to the test document, (iii) estimate a Bernoulli NB classifier, (iv) apply the classifier to the test document.
You need not estimate parameters that you don’t need for classifying the test document.
Your task is to classify words as English or not English.
Words are generated by a source with the following distribution:
Assume a training set that reflects the probability distribution of the source perfectly.
Make the same independence assumptions that are usually made for a multinomial classifier that uses terms as features for text classification.
Compute parameters using smoothing, in which computed-zero probabilities are smoothed into probability 0.01, and computed-nonzero probabilities are untouched.
That is, estimate separate parameters for each position in a word.
You only need to compute the parameters you need for classifying zoo.
What are the values of I(Ut;Cc) and X2(D, t, c) if term and class are completely independent? What are the values if they are completely dependent?
The feature selection method in Equation (13.16) is most appropriate for the Bernoulli model.
Why? How could one modify it for the multinomial model?
Features can also be selected according toinformation gain (IG), which is defined as:INFORMATION GAIN.
Because most good text classification features are positively correlated (i.e., they occur more often in c than in c), one may want to explicitly rule out the selection of negative indicators.
Yang and Liu (1999) employ significance tests in the evaluation of text classification methods.
Documents in the same class form a contiguous region and regions of different classes do not overlap.
There are many classification tasks, in particular the type of text classification that we encountered in Chapter 13, where classes can be distinguished by word patterns.
For example, documents in the class China tend to have high values on dimensions like Chinese, Beijing, and Mao whereas documents in the class UK tend to have high values for London, British and Queen.
Documents of the two classes therefore form distinct contiguous regions as shown in Figure 14.1 and we can draw boundaries that separate them and classify new documents.
How exactly this is done is the topic of this chapter.
Whether or not a set of documents is mapped into a contiguous region depends on the particular choices we make for the document representation: type of weighting, stop list etc.
To see that the document representation is crucial, consider the two classes written by a group vs.
Frequent occurrence of the first person pronoun I is evidence for the single-person class.
But that information is likely deleted from the document representation if we use a stop list.
If the document representation chosen is unfavorable, the contiguity hypothesis will not hold and successful vector space classification is not possible.
Unweighted and unnormalized counts should not be used in vector space classification.
We introduce two vector space classification methods in this chapter, Rocchio and kNN.
Rocchio classification (Section 14.2) divides the vector space into regions centered on centroids or prototypes, one for each class, computedPROTOTYPE as the center of mass of all documents in the class.
Rocchio classification is simple and efficient, but inaccurate if classes are not approximately spheres with similar radii.
It is less efficient than other classification methods in classifying documents.
If the training set is large, then kNN can handle non-spherical and other complex classes better than Rocchio.
A large number of text classifiers can be viewed as linear classifiers – classifiers that classify based on a simple linear combination of the features (Section 14.4)
Such classifiers partition the space of features into regions separated by linear decision hyperplanes, in a manner to be detailed below.
Because of the bias-variance tradeoff (Section 14.6) more complex nonlinear models.
Nonlinear models have more parameters to fit on a limited amount of training data and are more likely to make mistakes for small and noisy data sets.
When applying two-class classifiers to problems with more than two classes, there are one-of tasks – a document must be assigned to exactly one of several mutually exclusive classes – and any-of tasks – a document can be assigned to any number of classes as we will explain in Section 14.5
Two-class classifiers solve any-of problems and can be combined to solve one-of problems.
To illustrate properties of document vectors in vector classification, we will render these vectors as points in a plane as in the example in Figure 14.1
In reality, document vectors are length-normalized unit vectors that point to the surface of a hypersphere.
Distances on the surface of the sphere and on the projection plane are approximately the same as long as we restrict ourselves to small areas of the surface and choose an appropriate projection (Exercise 14.1)
Decisions of many vector space classifiers are based on a notion of distance, e.g., when computing the nearest neighbors in kNN classification.
We will use Euclidean distance in this chapter as the underlying distance measure.
In vector space classification, it rarely matters whether the relatedness of two documents is expressed in terms of similarity or distance.
However, in addition to documents, centroids or averages of vectors also play an important role in vector space classification.
For unnormalized vectors, dot product, cosine similarity and Euclidean distance all have different behavior in general (Exercise 14.6)
We will be mostly concerned with small local regions when computing the similarity between a document and a centroid, and the smaller the region the more similar the behavior of the three measures is.
The boundaries in the figure, which we call decision boundaries, are chosen to separateDECISION BOUNDARY the three classes, but are otherwise arbitrary.
To classify a new document, depicted as a star in the figure, we determine the region it occurs in and assign it the class of that region – China in this case.
Our task in vector space classification is to devise algorithms that compute good boundaries where “good” means high classification accuracy on data unseen during training.
Perhaps the best-known way of computing good class boundaries is Roc-ROCCHIO CLASSIFICATION chio classification, which uses centroids to define the boundaries.
The boundary between two classes in Rocchio classification is the set of points with equal distance from the two centroids.
The generalization of a line in M-dimensional space is a hyperplane, which we define as the set of points ~x that satisfy:
A line divides a plane in two, a plane divides 3-dimensional space in two, and hyperplanes divide higherdimensional spaces in two.
The separating hyperplane in this case has the following parameters:
As discussed in Section 14.1, the two assignment criteria will sometimes make different classification decisions.
We omit the query component of the Rocchio formula in Rocchio classification since there is no query in text classification.
Figure 14.5 The multimodal class “a” consists of two different clusters (small upper circles centered on X’s)
Rocchio classification will misclassify “o” as “a” because it is closer to the centroid A of the “a” class than to the centroid B of the “b” class.
In addition to respecting contiguity, the classes in Rocchio classification must be approximate spheres with similar radii.
In Figure 14.3, the solid square just below the boundary between UK and Kenya is a better fit for the class UK since UK is more scattered than Kenya.
But Rocchio assigns it to Kenya because it ignores details of the distribution of points in a class and only uses distance from the centroid for classification.
The assumption of sphericity also does not hold in Figure 14.5
The two clusters before and after the name change need not be close to each other in space.
Two-class classification is another case where classes are rarely distributed like spheres with similar radii.
Most two-class classifiers distinguish between a class like China that occupies a small region of the space and its widely scattered complement.
Assuming equal radii will result in a large number of false positives.
Most two-class classification problems therefore require a modified decision rule of the form:
In the next section, we will introduce another vector space classification method, kNN, that deals better with classes that have non-spherical, disconnected or other irregular shapes.
Unlike Rocchio, k nearest neighbor or kNN classification determines the deci-k NEAREST NEIGHBOR CLASSIFICATION sion boundary locally.
For 1NN we assign each document to the class of its.
For kNN we assign each document to the majority class of its k closest neighbors where k is a parameter.
The rationale of kNN classification is that, based on the contiguity hypothesis, we expect a test document d to have the same label as the training documents located in the local region surrounding d.
The classification decision of each test document relies on the class of a single training document, which may be incorrectly labeled or atypical.
It assigns documents to the majority class of their k closest neighbors, with ties broken randomly.
The parameter k in kNN is often chosen based on experience or knowledge about the classification problem at hand.
It is desirable for k to be odd to make ties less likely.
An alternative way of setting the parameter is to select the k that gives best results on a held-out portion of the training set.
We can also weight the “votes” of the k nearest neighbors by their cosine.
Mave is the average size of the vocabulary of documents in the collection.
Training a kNN classifier simply consists of determining k and preprocessing documents.
In fact, if we preselect a value for k and do not preprocess, then kNN requires no training at all.
In practice, we have to perform preprocessing steps like tokenization.
It makes more sense to preprocess training documents once as part of the training phase rather than repeatedly every time we classify a new test document.
In kNN classification, we do not perform any estimation of parameters as we do in Rocchio classification (centroids) or in Naive Bayes (priors and conditional probabilities)
For this reason, kNN is also called memory-based learning or instance-based learning.
But in kNN large training sets come with a severe efficiency penalty in classification.
Is the inverted index also the solution for efficient kNN?
An inverted index restricts a search to those documents that have at least one term in common with the query.
Thus in the context of kNN, the inverted index will be efficient if the test document has no term overlap with a large number of training documents.
Whether this is the case depends on the classification problem.
If documents are long and no stop list is used, then less time will be saved.
But with short documents and a large stop list, an inverted index may well cut the average test time by a factor of 10 or more.
A measure of the quality of a learning method is its Bayes error rate, the averageBAYES ERROR RATE error rate of classifiers learned by it for a particular problem.
The error of 1NN is asymptotically (as the training set increases) bounded by.
Figure 14.8 There are an infinite number of hyperplanes that separate two linearly separable classes.
Noise affects two components of kNN: the test document and the closest training document.
The two sources of noise are additive, so the overall error of 1NN is twice the optimal error rate.
Exercise 14.3Explain why kNN handles multimodal classes better than Rocchio.
In this section, we show that the two learning methods Naive Bayes and Rocchio are instances of linear classifiers, the perhaps most important group of text classifiers, and contrast them with nonlinear classifiers.
To simplify the discussion, we will only consider two-class classifiers in this section and define a linear classifier as a two-class classifier that decides class membershipLINEAR CLASSIFIER by comparing a linear combination of the features to a threshold.
The corresponding algorithm for linear classification in M dimensions is shown in Figure 14.9
Linear classification at first seems trivial given the simplicity of this algorithm.
However, the difficulty is in training the linear classifier, that is, in determining the parameters ~w and b based on the training set.
In general, some learning methods compute much better parameters than others where our criterion for evaluating the quality of a learning method is the effectiveness of the learned linear classifier on new data.
We now show that Rocchio and Naive Bayes are linear classifiers.
To see this for Rocchio, observe that a vector ~x is on the decision boundary if it has equal distance to the two class centroids:
The dimensions ti and parameters wi of a linear classifier for the class interest (as in interest rate) in Reuters-21578
Terms like dlr and world have negative weights because they are indicators for the competing class currency.
It isCLASS BOUNDARY the “true” boundary of the two classes and we distinguish it from the decision boundary that the learning method computes to approximate the class boundary.
As is typical in text classification, there are some noise documents in Fig-NOISE DOCUMENT ure 14.10 (marked with arrows) that do not fit well into the overall distribution of the classes.
Analogously, a noise document is a document that, when included in the training set, misleads the learning method and increases classification error.
Intuitively, the underlying distribution partitions the representation space into areas with mostly hoOnline edition (c)
In this hypothetical web page classification scenario, Chinese-only web pages are solid circles and mixed Chinese-English web pages are squares.
The two classes are separated by a linear class boundary (dashed line, short dashes), except for three noise documents (marked with arrows)
A document that does not conform with the dominant class in its area is a noise document.
Noise documents are one reason why training a linear classifier is hard.
If we pay too much attention to noise documents when choosing the decision hyperplane of the classifier, then it will be inaccurate on new data.
More fundamentally, it is usually difficult to determine which documents are noise documents and therefore potentially misleading.
If there exists a hyperplane that perfectly separates the two classes, then we call the two classes linearly separable.
Figure 14.8 illustrates another challenge in training a linear classifier.
If we are dealing with a linearly separable problem, then we need a criterion for selecting among all decision hyperplanes that perfectly separate the training data.
In general, some of these hyperplanes will do well on new data, some.
Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough.
If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers.
If a problem is linear, it is best to use a simpler linear classifier.
Exercise 14.4Prove that the number of linear separators of two classes is either infinite or zero.
The method to use depends on whether the classes are mutually exclusive or not.
Classification for classes that are not mutually exclusive is called any-of ,ANY-OF CLASSIFICATION multilabel, or multivalue classification.
Solving an any-of classification task with linear classifiers is straightforward:
Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels)
The decision of one classifier has no influence on the decisions of the other classifiers.
The second type of classification with more than two classes is one-of clas-ONE-OF CLASSIFICATION sification.
True one-of problems are less common in text classification than any-of problems.
With classes like UK, China, poultry, or coffee, a document can be relevant to many topics simultaneously – as when the prime minister of the UK visits China to talk about the coffee and poultry trade.
Nevertheless, we will often make a one-of assumption, as we did in Figure 14.1, even if classes are not really mutually exclusive.
For the classification problem of identifying the language of a document, the one-of assumption is a good approximation as most text is written in only one language.
In such cases, imposing a one-of constraint can increase the classifier’s effectiveness because errors that are due to the fact that the any-of classifiers assigned a document to either no class or more than one class are eliminated.
Thus, we must use a combination method when using twoclass linear classifiers for one-of classification.
Figure 14.12 J hyperplanes do not divide space into J disjoint regions.
Geometrically, the ranking can be with respect to the distances from the J linear separators.
Documents close to a class’s separator are more likely to be misclassified, so the greater the distance from the separator, the more plausible it is that a positive classification decision is correct.
Alternatively, we can use a direct measure of confidence to rank classes, e.g., probability of class membership.
We can state this algorithm for one-of classification with linear classifiers as follows:
Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels)
For example, 14 documents from grain were incorrectly assigned to wheat.
Create a test set by the same procedure, but also add 100 documents from a fourth language.
Train (i) a one-of classifier (ii) an any-of classifier on this training set and evaluate it on the test set.
For some problems, there exists a nonlinear classifier with zero classification error, but no such linear classifier.
Does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification?
To answer this question, we introduce the bias-variance tradeoff in this section, one of the most important concepts in machine learning.
The tradeoff helps explain why there is no universally optimal learning method.
Selecting an appropriate learning method is therefore an unavoidable part of solving a text classification problem.
Throughout this section, we use linear and nonlinear classifiers as prototypical examples of “less powerful” and “more powerful” learning, respectively.
First, many nonlinear models subsume linear models as a special case.
For instance, a nonlinear learning method like kNN will in some cases produce a linear classifier.
Second, there are nonlinear models that are less complex than linear models.
For instance, a quadratic polynomial with two parameters is less powerful than a 10,000-dimensional linear classifier.
Third, the complexity of learning is not really a property of the classifier because there are many aspects.
We refer the reader to the publications listed in Section 14.7 for a treatment of the bias-variance tradeoff that takes into account these complexities.
In this section, linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning methods in text classification.
Note that d and D are independent of each other.
In general, for a random document d and a random training set D, D does not contain a labeled instance of d.
Bias is the squared difference between P(c|d), the true conditional prob-BIAS.
We can think of bias as resulting from our domain knowledge (or lack thereof) that we build into the classifier.
If we know that the true boundary between the two classes is linear, then a learning method that produces linear classifiers is more likely to succeed than a nonlinear method.
But if the true class boundary is not linear and we incorrectly bias the classifier to be linear, then classification accuracy will be low on average.
Linear learning methods have low variance because most randomly drawn training sets produce similar decision hyperplanes.
The circular enclave in Figure 14.11 will be consistently misclassified.
It is apparent from Figure 14.6 that kNN can model very complex boundaries between two classes.
It is therefore sensitive to noise documents of the sort depicted in Figure 14.10
As a result the variance term in Equation (14.11) is large for kNN: Test documents are sometimes misclassified – if they happen to be close to a noise document in the training set – and sometimes correctly classified – if there are no noise documents in the training set near them.
This results in high variation from training set to training set.
This classifier uses only one feature, the number of Roman alphabet characters.
Assuming a learning method that minimizes the number of misclassifications in the training set, the position of the horizontal decision boundary is not greatly affected by differences in the training set (e.g., noise documents)
So a learning method producing this type of classifier has low variance.
But its bias is high since it will consistently misclassify squares in the lower left corner and “solid circle” documents with more than 50 Roman characters.
Learning linear classifiers has less bias since only noise documents and possibly a few documents close to the boundary between the two classes are misclassified.
The variance is higher than for the one-feature classifiers, but still small: The dashed line with long dashes deviates only slightly from the true boundary between the two classes, and so will almost all linear decision boundaries learned from training sets.
Thus, very few documents (documents close to the class boundary) will be inconsistently classified.
Here, the learning method constructs a decision boundary that perfectly separates the classes in the training set.
This method has the lowest bias because there is no document that is consistently misclassified – the classifiers sometimes even get noise documents in the test set right.
Because noise documents can move the decision boundary arbitrarily, test documents close to noise documents in the training set will be misclassified – something that a linear learning method is unlikely to do.
It is perhaps surprising that so many of the best-known text classification algorithms are linear.
Some of these methods, in particular linear SVMs, regOnline edition (c)
Typical classes in text classification are complex and seem unlikely to be modeled well linearly.
However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications.
With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise 14.17)
Thus, linear models in high-dimensional spaces are quite powerful despite their linearity.
Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane, but they are also more sensitive to noise in the training data.
Nonlinear learning methods sometimes perform better if the training set is large, but by no means in all cases.
Routing merely ranks documents according to rel-ROUTING evance to a class without assigning them.
Early work on filtering, a true clas-FILTERING sification approach that makes an assignment decision on each document, was published by Ittner et al.
The definition of routing we use here should not be confused with another sense.
Routing can also refer to the electronic distribution of documents to subscribers, the so-called push model of document distribution.
We have only presented the simplest method for combining two-class classifiers into a one-of classifier.
Another important method is the use of errorcorrecting codes, where a vector of decisions of different two-class classifiers is constructed for each document.
A test document’s decision vector is then “corrected” based on the distribution of decision vectors in the training set, a procedure that incorporates information from all two-class classifiers and their correlations into the final classification decision (Dietterich and Bakiri 1995)
Ghamrawi and McCallum (2005) also exploit dependencies between classes in any-of classification.
Download Reuters-21578 and train and test Rocchio and kNN classifiers for the classes acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat.
You may want to use one of a number of software packages that imOnline edition (c)
Figure 14.14 Example for differences between Euclidean distance, dot product similarity and cosine similarity.
Show that the decision boundaries in Rocchio classification are, as in kNN, given by the Voronoi tessellation.
Prove that the region of the plane consisting of all points with the same k nearest neighbors is a convex polygon.
Can one design an exact efficient algorithm for 1NN for very large M along the ideas you used to solve the last exercise?
Although we point out the similarities of Naive Bayes with linear vector space classifiers, it does not make sense to represent count vectors (the document representations in NB) in a continuous vector space.
There is however a formalization of NB that is analogous to Rocchio.
Improving classifier effectiveness has been an area of intensive machinelearning research over the last two decades, and this work has led to a new generation of state-of-the-art classifiers, such as support vector machines, boosted decision trees, regularized logistic regression, neural networks, and random forests.
Many of these methods, including support vector machines (SVMs), the main topic of this chapter, have been applied with success to information retrieval problems, particularly text classification.
An SVM is a kind of large-margin classifier: it is a vector space based machine learning method where the goal is to find a decision boundary between two classes that is maximally far from any point in the training data (possibly discounting some points as outliers or noise)
The chapter then moves to consider the practical deployment of text classifiers in Section 15.3: what sorts of classifiers are appropriate when, and how can you exploit domain-specific text features in classification? Finally, we will consider how the machine learning technology that we have been building for text classification can be applied back to the problem of learning how to rank documents in ad hoc retrieval (Section 15.4)
While several machine learning methods have been applied to this task, use of SVMs has been prominent.
Support vector machines are not necessarily better than other machine learning methods (except perhaps in situations with little training data), but they perform at the state-of-the-art level and have much current theoretical and empirical appeal.
Intuitively, a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes.
The SVM in particular defines the criterion to be looking for a decision surface that is maximally far away from any data point.
This distance from the decision surface to the closest data point determines the margin of the classifier.
This method of construction necessarily means thatMARGIN the decision function for an SVM is fully specified by a (usually small) subset of the data which defines the position of the separator.
These points are referred to as the support vectors (in a vector space, a point can be thought ofSUPPORT VECTOR as a vector between the origin and that point)
Figure 15.1 shows the margin and support vectors for a sample problem.
Other data points play no part in determining the decision surface that is chosen.
Maximizing the margin seems good because points near the decision surface represent very uncertain classification decisions: there is almost a 50% chance of the classifier deciding either way.
A classifier with a large margin makes no low certainty classification decisions.
This gives you a classification safety margin: a slight error in measurement or a slight document variation will not cause a misclassification.
By construction, an SVM classifier insists on a large margin around the decision boundary.
Compared to a decision hyperplane, if you have to place a fat separator between classes, you have fewer choices of where it can be put.
Again, the points closest to the separating hyperplane are support vectors.
The geometric margin of the classifier is the maximum width of the band thatGEOMETRIC MARGIN can be drawn separating the support vectors of the two classes.
This means that we can impose any scaling constraint we wish on ~w without affecting the geometric margin.
Among other choices, we could use unit vectors, as in Chapter 6, by.
This would have the effect of making the geometric margin the same as the functional margin.
Quadratic optimization problems are a standard, well-known class of mathe-QUADRATIC PROGRAMMING matical optimization problems, and many algorithms exist for solving them.
We could in principle build our SVM using standard quadratic programming (QP) libraries, but there has been much recent research in this area aiming to exploit the structure of the kind of QP that emerges from an SVM.
As a result, there are more intricate but much faster and more scalable libraries available especially for building SVMs, which almost everyone uses to build models.
We will not present the details of such algorithms here.
The data set uniquely defines the best separating hyperplane, and we feed the data through a quadratic optimization procedure to find this plane.
The sign of this function determines the class to assign to the point.
If the point is within the margin of the classifier (or another confidence threshold t that we might have determined to minimize classification mistakes) then the classifier can return “don’t know” rather than one of the two classes.
Also, since the margin is constant, if the model includes dimensions from various sources, careful rescaling of some dimensions may be required.
However, this is not a problem if our documents (points) are on the unit hypersphere.
The optimal decision surface is orthogonal to that line and intersects it at the halfway point.
Install an SVM package such as SVMlight (http://svmlight.joachims.org/), and build an SVM for the data set discussed in Example 15.1
Confirm that the program gives the same solution as the text.
For SVMlight, or another package that accepts the same training data format, the training file would be:
For the very high dimensional problems common in text classification, sometimes the data are linearly separable.
But in the general case they are not, and even if they are, we might prefer a solution that better separates the bulk of the data while ignoring a few weird noise documents.
The formulation of the SVM optimization problem with slack variables is:
The complexity of training and testing with linear SVMs is shown in Table 15.1.3 The time for training an SVM is dominated by the time for solving the underlying QP, and so the theoretical and empirical complexity varies depending on the method used to solve it.
All the recent work on SVM training has worked to reduce that complexity, often by.
Nevertheless, the super-linear training time of traditional SVM algorithms makes them difficult or impossible to use on very large training data sets.
Alternative traditional SVM solution algorithms which are linear in the number of training examples scale badly with a large number of features, which is another standard attribute of text problems.
However, a new training algorithm based on cutting plane techniques gives a promising answer to this issue by having running time linear in the number of training examples and the number of non-zero features in examples (Joachims 2006a)
Nevertheless, the actual speed of doing quadratic optimization remains much slower than simply counting terms as is done in a Naive Bayes model.
Extending SVM algorithms to nonlinear SVMs, as in the next section, standardly increases training complexity by a factor of |D| (since dot products between examples need to be calculated), making them impractical.
With what we have presented so far, data sets that are linearly separable (perhaps with a few exceptions or some noise) are well-handled.
But what are we going to do if the data set just doesn’t allow classification by a linear classifier? Let us look at a one-dimensional case.
The top data set in Figure 15.6 is straightforwardly classified by a linear classifier but the middle data set is not.
We instead need to be able to pick out an interval.
One way to solve this problem is to map the data on to a higher dimensional space and then to use a linear classifier in the higher dimensional space.
For example, the bottom part of the figure shows that a linear separator can easily classify the data.
Materializing the features refers to directly calculating higher order and interaction terms and then putting them into a linear model.
Figure 15.6 Projecting data that is not linearly separable into a higher dimensional space can make it linearly separable.
The general idea is to map the original feature space to some higher-dimensional feature space where the training set is separable.
Of course, we would want to do so in ways that preserve relevant dimensions of relatedness between data points, so that the resultant classifier should still generalize well.
SVMs, and also a number of other linear classifiers, provide an easy and efficient way of doing this mapping to a higher dimensional space, which is referred to as “the kernel trick”
It’s not really a trick: it just exploits the mathKERNEL TRICK that we have seen.
The SVM linear classifier relies on a dot product between data point vectors.
In the language of functional analysis, what kinds of functions are valid kernel functions? Kernel functions are sometimes more precisely referred toKERNEL as Mercer kernels, because they must satisfy Mercer’s condition: for any g(~x)MERCER KERNEL such that.
A kernel function K must be continuous, symmetric, and have a positive definite gram matrix.
Such a K means that there exists a mapping to a reproducing kernel Hilbert space (a Hilbert space is a vector space closed under dot products) such that the dot product there gives the same value as the function K.
If a kernel does not satisfy Mercer’s condition, then the corresponding QP may have no solution.
If you would like to better understand these issues, you should consult the books on SVMs mentioned in Section 15.5
Otherwise, you can content yourself with knowing that 90% of work with kernels uses one of two straightforward families of functions of two vectors, which we define below, and which define valid kernels.
The two commonly used families of kernels are polynomial kernels and radial basis functions.
The case of d = 2 gives a quadratic kernel, and is very commonly used.
The most common form of radial basis function is a Gaussian distribution, calculated as:
A radial basis function (rbf) is equivalent to mapping the data into an infinite dimensional Hilbert space, and so we cannot illustrate the radial basis function concretely, as we did a quadratic kernel.
Beyond these two families, there has been interesting work developing other kernels, some of which is promising for text applications.
In particular, there has been investigation of string kernels (see Section 15.5)
The world of SVMs comes with its own language, which is rather different from the language otherwise used in machine learning.
The terminology does have deep roots in mathematics, but it’s important not to be too awed by that terminology.
A polynomial kernel allows us to model feature conjunctions (up to the order of the polynomial)
That is, if we want to be able to model occurrences of pairs of words, which give distinctive information about topic classification, not given by the individual words alone, like perhaps operating AND system or ethnic AND cleansing, then we need to use a quadratic kernel.
If occurrences of triples of words give distinctive information, then we need to use a cubic kernel.
Simultaneously you also get the powers of the basic features – for most text applications, that probably isn’t useful, but just comes along with the math and hopefully doesn’t do harm.
A radial basis function allows you to have features that pick out circles (hyperspheres) – although the decision boundaries become much more complex as multiple such features interact.
A string kernel lets you have features that are character subsequences of terms.
All of these are straightforward notions which have also been used in many other places under different names.
We presented results in Section 13.6 showing that an SVM is a very effective text classifier.
This was one of several pieces of work from this time that established the strong reputation of SVMs for text classification.
Another pioneering work on scaling and evaluating SVMs for text classification was (Joachims 1998)
It seems that working with simple term features can get one a long way.
It is again noticeable the extent to which different papers’ results for the same machine learning methods differ.
There are lots of applications of text classification in the commercial world; email spam filtering is perhaps now the most ubiquitous.
Many researchers disprefer this measure for text classification evaluation, since its calculation may involve interpolation rather than an actual parameter setting of the system and it is not clear why this value should be reported rather than maximal F1 or another point on the precision/recall curve motivated by the task at hand.
While earlier results in (Joachims 1998) suggested notable gains on this task from the use of higher order polynomial or rbf kernels, this was with hard-margin SVMs.
With soft-margin SVMs, a simple linear SVM with the default C = 1 performs best.
Most of our discussion of classification has focused on introducing various machine learning methods rather than discussing particular features of text documents relevant to classification.
This bias is appropriate for a textbook, but is misplaced for an application developer.
It is frequently the case that greater performance gains can be achieved from exploiting domain-specific text features than from changing from one machine learning method to another.
When confronted with a need to build a text classifier, the first question to ask is how much training data is there currently available? None? Very little? Quite a lot? Or a huge amount, growing every day? Often one of the biggest practical challenges in fielding a machine learning classifier in real applications is creating or obtaining enough training data.
For many problems and algorithms, hundreds or thousands of examples from each class are required to produce a high performance classifier and many real world contexts involve large sets of categories.
We will initially assume that the classifier is needed as soon as possible; if a lot of time is available for implementation, much of it might be spent on assembling data resources.
If you have no labeled training data, and especially if there are existing staff knowledgeable about the domain of the data, then you should never forget the solution of using hand-written rules.
With careful crafting (that is, by humans tuning the rules on development data), the accuracy of such rules can become very high.
Nevertheless the amount of work to create such well-tuned rules is very large.
A reasonable estimate is 2 days per class, and extra time has to go.
At any rate, a very low bias model like a nearest neighbor model is probably counterindicated.
Regardless, the quality of the model will be adversely affected by the limited training data.
In these methods, the system gets some labeled documents, and a further large supply of unlabeled documents over which it can attempt to learn.
Often, the practical answer is to work out how to get more labeled data as quickly as you can.
The best way to do this is to insert yourself into a process where humans will be willing to label data for you as part of their natural tasks.
For example, in many cases humans will sort or route email for their own purposes, and these actions give information about classes.
The alternative of getting human labelers expressly for the task of training classifiers is often difficult to organize, and the labeling is often of lower quality, because the labels are not embedded in a realistic task context.
Rather than getting people to label all or a random sample of documents, there has also been considerable research on active learning, where a system is built whichACTIVE LEARNING decides which documents a human should label.
Usually these are the ones on which a classifier is uncertain of the correct classification.
This can be effective in reducing annotation costs by a factor of 2–4, but has the problem that the good documents to label to train one type of classifier often are not the good documents to label to train a different type of classifier.
If there is a reasonable amount of labeled data, then you are in the perfect position to use everything that we have presented about text classification.
However, if you are deploying a linear classifier such as an SVM, you should probably design an application that overlays a Boolean rule-based classifier over the machine learning classifier.
Users frequently like to adjust things that do not come out quite right, and if management gets on the phone and wants the classification of a particular document fixed right now, then this is much easier to.
This is one reason why machine learning models like decision trees which produce userinterpretable Boolean-like models retain considerable popularity.
It may be best to choose a classifier based on the scalability of training or even runtime efficiency.
To get to this point, you need to have huge amounts of data.
The general rule of thumb is that each doubling of the training data size produces a linear increase in classifier performance, but with very large amounts of data, the improvement becomes sub-linear.
For any particular application, there is usually significant room for improving classifier effectiveness through exploiting features specific to the domain or document collection.
Often documents will contain zones which are especially useful for classification.
Often there will be particular subvocabularies which demand special treatment for optimal classification effectiveness.
If a text classification problem consists of a small number of well-separated categories, then many classification algorithms are likely to work well.
But many real classification problems consist of a very large number of often very similar categories.
The reader might think of examples like web directories (the Yahoo! Directory or the Open Directory Project), library classification schemes (Dewey Decimal or Library of Congress) or the classification schemes used in legal or medical applications.
For instance, the Yahoo! Directory consists of over 200,000 categories in a deep hierarchy.
Accurate classification over large sets of closely related classes is inherently difficult.
Most large sets of categories have a hierarchical structure, and attempting to exploit the hierarchy by doing hierarchical classification is a promising ap-HIERARCHICAL.
However, at present the effectiveness gains from doing this rather than just working with the classes that are the leaves of the hierarchy remain modest.6 But the technique can be very useful simply to improve the scalability of building classifiers over large hierarchies.
Another simple way to improve the scalability of classifiers over large hierarchies is the use of aggressive feature selection.
We provide references to some work on hierarchical classification in Section 15.5
A general result in machine learning is that you can always get a small boost in classification accuracy by combining multiple classifiers, provided only that the mistakes that they make are at least somewhat independent.
There is now a large literature on techniques such as voting, bagging, and boosting multiple classifiers.
Nevertheless, ultimately a hybrid automatic/manual solution may be needed to achieve sufficient classification accuracy.
A common approach in such situations is to run a classifier first, and to accept all its high confidence decisions, but to put low confidence decisions in a queue for manual review.
Such a process also automatically leads to the production of new training data which can be used in future versions of the machine learning classifier.
However, note that this is a case in point where the resulting training data is clearly not randomly sampled from the space of documents.
The default in both ad hoc retrieval and text classification is to use terms as features.
However, for text classification, a great deal of mileage can be achieved by designing additional features which are suited to a specific problem.
Unlike the case of IR query languages, since these features are internal to the classifier, there is no problem of communicating these features to an end user.
At pre-FEATURE ENGINEERING sent, feature engineering remains a human craft, rather than something done by machine learning.
Good feature engineering can often markedly improve the performance of a text classifier.
It is especially beneficial in some of the most important applications of text classification, like spam and porn filtering.
Classification problems will often contain large numbers of terms which can be conveniently grouped, and which have a similar vote in text classification problems.
Typical examples might be year mentions or strings of exclamation marks.
Or they may be more specialized tokens like ISBNs or chemical formulas.
Often, using them directly in a classifier would greatly increase the vocabulary without providing classificatory power beyond knowing that, say, a chemical formula is present.
In such cases, the number of features and feature sparseness can be reduced by matching such items with regular expressions and converting them into distinguished tokens.
Sometimes all numbers are converted into a single feature, but often some value can be had by distinguishing different kinds of numbers, such as four digit numbers (which are usually years) versus other cardinal numbers versus real numbers with a decimal point.
Similar techniques can be applied to dates, ISBN numbers, sports game scores, and so on.
Going in the other direction, it is often useful to increase the number of feaOnline edition (c)
Parts of words are often matched by character k-gram features.
Such features can be particularly good at providing classification clues for otherwise unknown words when the classifier is deployed.
For instance, an unknown word ending in -rase is likely to be an enzyme, even if it wasn’t seen in the training data.
They are useful when the components of a compound would themselves be misleading as classification cues.
For instance, this would be the case if the keyword ethnic was most indicative of the categories food and arts, the keyword cleansing was most indicative of the category home, but the collocation ethnic cleansing instead indicates the category world news.
But it is nevertheless useful to note that such techniques have a more restricted chance of being useful for classification.
For IR, you often need to collapse forms of a word like oxygenate and oxygenation, because the appearance of either in a document is a good clue that the document will be relevant to a query about oxygenation.
Given copious training data, stemming necessarily delivers no value for text classification.
If several forms that stem together have a similar signal, the parameters estimated for all of them will have similar weights.
Techniques like stemming help only in compensating for data sparseness.
This can be a useful role (as noted at the start of this section), but often different forms of a word can convey significantly different cues about the correct document classification.
As already discussed in Section 6.1, documents usually have zones, such as mail message headers like the subject and author, or the title and keywords of a research article.
Text classifiers can usually gain from making use of these zones during training and classification.
In text classification problems, you can frequently get a nice boost to effectiveness by differentially weighting contributions from different document zones.
You can also get value from upweighting words from pieces of text that are not so much clearly defined zones, but where nevertheless evidence from document structure or content suggests that they are important.
There are two strategies that can be used for document zones.
This means that we are using the same features (that is, parameters are “tied” across different zones), but we pay more attention to thePARAMETER TYING occurrence of terms in particular zones.
An alternative strategy is to have a completely separate set of features and corresponding parameters for words occurring in different zones.
This is in principle more powerful: a word could usually indicate the topic Middle East when in the title but Commodities when in the body of a document.
Having separate feature sets means having two or more times as many parameters, many of which will be much more sparsely seen in the training data, and hence with worse estimates, whereas upweighting has no bad effects of this sort.
Moreover, it is quite uncommon for words to have different preferences when appearing in different zones; it is mainly the strength of their vote that should be adjusted.
Nevertheless, ultimately this is a contingent result, depending on the nature and quantity of the training data.
Discuss how you could engineer features that would largely defeat this strategy.
A classifier that has been fed examples of relevant and nonrelevant documents for each of a set of queries can then figure out the relative weights of these signals.
If we configure the problem so that there are pairs of a document and a query which are assigned a relevance judgment of relevant or nonrelevant, then we can think of this problem too as a text classification problem.
Taking such a classification approach is not necessarily best, and we present an alternative in Section 15.4.2
Nevertheless, given the material we have covered, the simplest place to start is to approach this problem as a classification problem, by ordering the documents according to the confidence of a two-class classifier in its relevance decision.
And this move is not purely pedagogical; exactly this approach is sometimes used in practice.
In Section 6.1.2 we considered a case where we had to combine Boolean indicators of relevance; here we consider more general factors to further develop the notion of machine-learned.
In particular, the factors we now consider go beyond Boolean functions of query term presence in document zones, as in Section 6.1.2
Each R denotes a training example labeled relevant, while each N is a training example labeled nonrelevant.
The above ideas can be readily generalized to functions of many more than two variables.
There are lots of other scores that are indicative of the relevance of a document to a query, including static quality (PageRank-style measures, discussed in Chapter 21), document age, zone contributions, document length, and so on.
Providing that these measures can be calculated for a training document collection with relevance judgments, any number of such measures can be used to train a machine learning classifier.
For instance, we could train an SVM over binary relevance judgments, and order documents based on their probability of relevance, which is monotonic with the documents’ signed distance from the decision boundary.
However, approaching IR result ranking like this is not necessarily the right way to think about the problem.
Statisticians normally first divide problems into classification problems (where a categorical variable is predicted) versus regression problems (where a real number is predicted)
InREGRESSION between is the specialized field of ordinal regression where a ranking is pre-ORDINAL REGRESSION dicted.
Machine learning for ad hoc retrieval is most properly thought of as an ordinal regression problem, where the goal is to rank a set of documents for a query, given training data of the same sort.
This formulation gives some additional power, since documents can be evaluated relative to other candidate documents for the same query, rather than having to be mapped to a global scale of goodness, while also weakening the problem space, since just a ranking is required rather than an absolute measure of relevance.
Issues of ranking are especially germane in web search, where the ranking at.
Such work can and has been pursued using the structural SVM framework which we mentioned in Section 15.2.2, where the class being predicted is a ranking of results for a query, but here we will present the slightly simpler ranking SVM.
Both of the methods that we have just looked at use a linear weighting of document features that are indicators of relevance, as has most work in this area.
It is therefore perhaps interesting to note that much of traditional IR weighting involves nonlinear scaling of basic measurements (such as logweighting of term frequency, or idf)
At the present time, machine learning is very good at producing optimal weights for features in a linear combination.
The idea of learning ranking functions has been around for a number of years, but it is only very recently that sufficient machine learning knowledge, training document collections, and computational power have come together to make this method practical and exciting.
It is thus too early to write something definitive on machine learning approaches to ranking in information retrieval, but there is every reason to expect the use and importance of machine learned ranking approaches to grow over time.
While skilled humans can do a very good job at defining ranking functions by hand, hand tuning is difficult, and it has to be done again for each new document collection and class of users.
The somewhat quirky name support vector machine originates in the neural networks literature, where learning algorithms were thought of as architectures, and often referred to as “machines”
The distinctive element of this model is that the decision boundary to use is completely decided (“supported”) by a few training data points, the support vectors.
The last reference provides an introduction to the general framework of structural SVMs.
The Advances in Neural Information Processing (NIPS) conferences have become the premier venue for theoretical machine learning work, such as on SVMs.
Other venues such as SIGIR are much stronger on experimental methodology and using text-specific features to improve classifier effectiveness.
A recent comparison of most current machine learning classifiers (though on problems rather different from typical text problems) can be found in (Caruana and Niculescu-Mizil 2006)
Joachims (2002a) presents his work on SVMs applied to text problems in detail.
Zhang and Oles (2001) present an insightful comparison of Naive Bayes, regularized logistic regression and SVM classifiers.
Joachims (1999) discusses methods of making SVM learning practical over large text data sets.
In a recent large study on scaling SVMs to the entire Yahoo! directory, Liu et al.
Classifier effectiveness remains limited by the very small number of training documents for many classes.
For a more general approach that can be applied to modeling relations between classes, which may be arbitrary rather than simply the case of a hierarchy, see Tsochantaridis et al.
Moschitti and Basili (2004) investigate the use of complex nominals, proper nouns and word senses as features in text classification.
Sindhwani and Keerthi (2006) present a more efficient implementation of a transductive SVM for large data sets.
But limited training data and poor machine learning techniques meant that these pieces of work achieved only middling results, and hence they only had limited impact at the time.
Clustering algorithms group a set of documents into subsets or clusters.
TheCLUSTER algorithms’ goal is to create clusters that are coherent internally, but clearly different from each other.
In other words, documents within a cluster should be as similar as possible; and documents in one cluster should be as dissimilar as possible from documents in other clusters.
Figure 16.1 An example of a data set with a clear cluster structure.
No super-UNSUPERVISED LEARNING vision means that there is no human expert who has assigned documents.
In clustering, it is the distribution and makeup of the data that will determine cluster membership.
It is visually clear that there are three distinct clusters of points.
This chapter and Chapter 17 introduce algorithms that find such clusters in an unsupervised fashion.
The difference between clustering and classification may not seem great at first.
After all, in both cases we have a partition of a set of documents into groups.
But as we will see the two problems are fundamentally different.
In unsupervised learning, of which clustering is the most important example, we have no such teacher to guide us.
The key input to a clustering algorithm is the distance measure.
In document clustering, the distance measure is often also Euclidean distance.
Thus, the distance measure is an important means by which we can influence the outcome of clustering.
Flat clustering creates a flat set of clusters without any explicit structure thatFLAT CLUSTERING.
Chapter 17 also addresses the difficult problem of labeling clusters automatically.
A second important distinction can be made between hard and soft clustering algorithms.
Hard clustering computes a hard assignment – each documentHARD CLUSTERING is a member of exactly one cluster.
The assignment of soft clustering algo-SOFT CLUSTERING rithms is soft – a document’s assignment is a distribution over all clusters.
In a soft assignment, a document has fractional membership in several clusters.
K-means is perhaps the most widely used flat clustering algorithm due to its simplicity and efficiency.
The EM algorithm is a generalization of K-means and can be applied to a large variety of document representations and distributions.
The cluster hypothesis states the fundamental assumption we make when us-CLUSTER HYPOTHESIS ing clustering in information retrieval.
Documents in the same cluster behave similarly with respect to relevance to information needs.
The hypothesis states that if there is a document from a cluster that is relevant to a search request, then it is likely that other documents from the same cluster are also relevant.
This is because clustering puts together documents that share many terms.
Language modeling collection increased precision and/or recall Liu and Croft (2004)
In both cases, we posit that similar documents behave similarly with respect to relevance.
Table 16.1 shows some of the main applications of clustering in information retrieval.
They differ in the set of documents that they cluster – search results, collection or subsets of the collection – and the aspect of an information retrieval system they try to improve – user experience, user interface, effectiveness or efficiency of the search system.
But they are all based on the basic assumption stated by the cluster hypothesis.
The first application mentioned in Table 16.1 is search result clustering whereSEARCH RESULT CLUSTERING by search results we mean the documents that were returned in response to.
A better user interface is also the goal of Scatter-Gather, the second ap-SCATTER-GATHER plication in Table 16.1
Scatter-Gather clusters the whole collection to get groups of documents that the user can select or gather.
The selected groups are merged and the resulting set is again clustered.
This process is repeated until a cluster of interest is found.
None of the top hits cover the animal sense of jaguar, but users can easily access it by clicking on the cat cluster in the Clustered Results panel on the left (third arrow from the top)
Automatically generated clusters like those in Figure 16.3 are not as neatly organized as a manually constructed hierarchical tree like the Open Directory at http://dmoz.org.
But cluster-based navigation is an interesting alternative to keyword searching, the standard information retrieval paradigm.
This is especially true in scenarios where users prefer browsing over searching because they are unsure about which search terms to use.
As an alternative to the user-mediated iterative clustering in Scatter-Gather, we can also compute a static hierarchical clustering of a collection that is not influenced by user interactions (“Collection clustering” in Table 16.1)
Google News and its precursor, the Columbia NewsBlaster system, are examples of this approach.
In the case of news, we need to frequently recompute the clustering to make sure that users can access the latest breaking stories.
Clustering is well suited for access to a collection of news stories since news reading is not really search, but rather a process of selecting a subset of stories about recent events.
Figure 16.3 An example of a user session in Scatter-Gather.
A collection of New York Times news stories is clustered (“scattered”) into eight clusters (top row)
The user manually gathers three of these into a smaller collection International Stories and performs another scattering operation.
This process repeats until a small cluster with relevant documents is found (e.g., Trinidad)
The fourth application of clustering exploits the cluster hypothesis directly for improving search results, based on a clustering of the entire collection.
We use a standard inverted index to identify an initial set of documents that match the query, but we then add other documents from the same clusters even if they have low similarity to the query.
For example, if the query is car and several car documents are taken from a cluster of automobile documents, then we can add documents from this cluster that use terms other than car (automobile, vehicle etc)
This can increase recall since a group of documents with high mutual similarity is often relevant as a whole.
More recently this idea has been used for language modeling.
But the collection contains many documents with terms untypical of d.
By replacing the collection model with a model derived from d’s cluster, we get more accurate estimates of the occurrence probabilities of terms in d.
The inverted index supports fast nearest-neighbor search for the standard IR setting.
However, sometimes we may not be able to use an inverted index efficiently, e.g., in latent semantic indexing (Chapter 18)
In such cases, we could compute the similarity of the query to every document, but this is slow.
The cluster hypothesis offers an alternative: Find the clusters that are closest to the query and only consider documents from these clusters.
Within this much smaller set, we can compute similarities exhaustively and rank documents in the usual way.
Since there are many fewer clusters than documents, finding the closest cluster is fast; and since the documents matching a query are all similar to each other, they tend to be in the same clusters.
While this algorithm is inexact, the expected decrease in search quality is small.
Exercise 16.1Define two documents as similar if they have at least two proper names like Clinton or Sarkozy in common.
Give an example of an information need and two documents, for which the cluster hypothesis does not hold for this notion of similarity.
In your example, retrieving clusters close to the query should do worse than direct nearest neighbor search.
The objective function is often defined in terms of similarity or distance between documents.
Below, we will see that the objective in K-means clustering is to minimize the average distance between documents and their centroids or, equivalently, to maximize the similarity between documents and their centroids.
As in Chapter 14, we use both similarity and distance to talk about relatedness between documents.
For documents, the type of similarity we want is usually topic similarity or high values on the same dimensions in the vector space model.
For example, documents about China have high values on dimensions like Chinese, Beijing, and Mao whereas documents about the UK tend to have high values for London, Britain and Queen.
We approximate topic similarity with cosine similarity or Euclidean distance in vector space (Chapter 6)
If we intend to capture similarity of a type other than topic, for example, similarity of language, then a different representation may be appropriate.
When computing topic similarity, stop words can be safely ignored, but they are important cues for separating clusters of English (in which the occurs frequently and la infrequently) and French documents (in which the occurs infrequently and la frequently)
An alternative definition of hard clustering is that a document can be a full member of more than one cluster.
But in a partitional hierarchical clustering (Chapter 17) all members of a cluster are of course also members of its parent.
Some researchers distinguish between exhaustive clusterings that assignEXHAUSTIVE each document to a cluster and non-exhaustive clusterings, in which some documents will be assigned to no cluster.
Non-exhaustive clusterings in which each document is a member of either no cluster or one cluster are called exclusive.
A difficult issue in clustering is determining the number of clusters or cardi-CARDINALITY nality of a clustering, which we denote by K.
Often K is nothing more than a good guess based on experience or domain knowledge.
But for K-means, we will also introduce a heuristic method for choosing K and an attempt to incorporate the selection of K into the objective function.
Sometimes the application puts constraints on the range of K.
Since our goal is to optimize an objective function, clustering is essentially.
The brute force solution would be to enumerate all possible clusterings and pick the best.
However, there are exponentially many partitions, so this approach is not feasible.1 For this reason, most flat clustering algorithms refine an initial partitioning iteratively.
If the search starts at an unfavorable initial point, we may miss the global optimum.
Finding a good starting point is therefore another important problem we have to solve in flat clustering.
Typical objective functions in clustering formalize the goal of attaining high intra-cluster similarity (documents within a cluster are similar) and low intercluster similarity (documents from different clusters are dissimilar)
This is an internal criterion for the quality of a clustering.
An alternative to internal criteria is direct evaluation in the application of interest.
For search result clustering, we may want to measure the time it takes users to find an answer with different clustering algorithms.
This is the most direct evaluation, but it is expensive, especially if large user studies are necessary.
We can then compute an external criterion that evaluates how well the clustering matchesEXTERNAL CRITERION.
For example, we may want to say that the optimal clustering of the search results for jaguar in Figure 16.2 consists of three classes corresponding to the three senses car, animal, and operating system.
In this type of evaluation, we only use the partition provided by the gold standard, not the class labels.
The Rand index penalizes both false positive and false negative decisions during clustering.
The F measure in addition supports differential weighting of these two types of errors.
To compute purity, each cluster is assigned to the class which is most fre-PURITY quent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by N.
An upper bound on the number of clusterings is KN/K!
The exact number of different partitions of N documents into K clusters is the Stirling number of the second kind.
Purity is compared with the other three measures discussed in this chapter in Table 16.2
High purity is easy to achieve when the number of clusters is large – in particular, purity is 1 if each document gets its own cluster.
Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters.
A measure that allows us to make this tradeoff is normalized mutual infor-NORMALIZED MUTUAL INFORMATION.
Which measures increase and which stay the same after doubling the number of points? (iii) Given your assessment in (i) and the results in (ii), which measures are best suited to compare the quality of the two clusterings?
The definition assumes that documents are represented as length-normalized vectors in a real-valued space in the familiar way.
The ideal cluster in K-means is a sphere with the centroid as its center of gravity.
Our desiderata for classes in Rocchio classification were the same.
The difference is that we have no labeled training set in clustering for which we know which documents should be in the same cluster.
A measure of how well the centroids represent the members of their clusters is the residual sum of squares or RSS, the squared distance of each vectorRESIDUAL SUM OF.
Since N is fixed, minimizing RSS is equivalent to minimizing the average squared distance, a measure of how well centroids represent their documents.
The first step of K-means is to select as initial cluster centers K randomly selected documents, the seeds.
The algorithm then moves the cluster centersSEED around in space in order to minimize RSS.
As shown in Figure 16.5, this is done iteratively by repeating two steps until a stopping criterion is met: reassigning documents to the cluster with the closest centroid; and recomputing each centroid based on the current members of its cluster.
Figure 16.6 shows snapshots from nine iterations of the K-means algorithm for a set of points.
This condition limits the runtime of the clustering algorithm, but in some cases the quality of the clustering will be poor because of an insufficient number of iterations.
This criterion ensures that the clustering is of a desired quality after termination.
We now show that K-means converges by proving that RSS monotonically decreases in each iteration.
We will use decrease in the meaning decrease or does not change in this section.
First, RSS decreases in the reassignment step since each vector is assigned to the closest centroid, so the distance it contributes to RSS decreases.
Second, it decreases in the recomputation step because the new centroid is the vector ~v for which RSSk reaches its minimum.
Thus, we minimize RSSk when the old centroid is replaced with the new centroid.
RSS, the sum of the RSSk, must then also decrease during recomputation.
Since there is only a finite set of possible clusterings, a monotonically decreasing algorithm will eventually arrive at a (local) minimum.
Take care, however, to break ties consistently, e.g., by assigning a document to the cluster with the lowest index if there are several equidistant centroids.
Otherwise, the algorithm can cycle forever in a loop of clusterings that have the same cost.
While this proves the convergence of K-means, there is unfortunately no guarantee that a global minimum in the objective function will be reached.
This is a particular problem if a document set contains many outliers, doc-OUTLIER uments that are far from any other documents and therefore do not fit well into any cluster.
Frequently, if an outlier is chosen as an initial seed, then no other vector is assigned to it during subsequent iterations.
Thus, we end up with a singleton cluster (a cluster with only one document) even though thereSINGLETON CLUSTER is probably a clustering with lower RSS.
Figure 16.7 shows an example of a suboptimal clustering resulting from a bad choice of initial seeds.
Another type of suboptimal clustering that frequently occurs is one with empty clusters (Exercise 16.11)
Effective heuristics for seed selection include (i) excluding outliers from the seed set; (ii) trying out multiple starting points and choosing the clustering with lowest cost; and (iii) obtaining seeds from another method such as hierarchical clustering.
Other initialization methods compute seeds that are not selected from the vectors to be clustered.
A robust method that works well for a large variety of document distributions is to select i (e.g., i = 10) random vectors for each cluster and use their centroid as the seed for this cluster.
The same efficiency problem is addressed by K-medoids, a variant of K-K-MEDOIDS means that computes medoids instead of centroids as cluster centers.
We define the medoid of a cluster as the document vector that is closest to theMEDOID centroid.
Since medoids are sparse document vectors, distance computations are fast.
What do we do if we cannot come up with a plausible guess for K?
A naive approach would be to select the optimal value of K according to the objective function, namely the value of K that minimizes RSS.
We would end up with each document being in its own cluster.
Exercise 16.4Why are documents that do not use the same term for the concept car likely to end up in the same cluster in K-means clustering?
It can be applied to a larger variety of document representations and distributions than K-means.
In K-means, we attempt to find centroids that are good representatives.
We can view the set of K centroids as a model that generates the data.
Generating a document in this model consists of first picking a centroid at random and then adding some noise.
If the noise is normally distributed, this procedure will result in clusters of spherical shape.
The model that we recover from the data then defines clusters and an assignment of documents to clusters.
An example of a soft assignment is that a document about Chinese cars may have a fractional membership of 0.5 in each of the two clusters China and automobiles, reflecting the fact that both topics are pertinent.
A hard clustering like K-means cannot model this simultaneous relevance to two topics.
Model-based clustering provides a framework for incorporating our knowledge about a domain.
K-means and the hierarchical algorithms in Chapter 17 make fairly rigid assumptions about the data.
For example, clusters in K-means are assumed to be spheres.
The clustering model can be adapted to what we know about the underlying distribution of the data, be it Bernoulli (as in the example in Table 16.3), Gaussian with non-spherical variance (another model that is important in document clustering) or a member of a different family.
Finding good seeds is even more critical for EM than for K-means.
This is a general problem that also occurs in other applications of EM.4 Therefore, as with K-means, the initial assignment of documents to clusters is often computed by a different algorithm.
For example, a hard K-means clustering may provide the initial assignment, which EM can then “soften up.”
Berkhin (2006b) gives a general up-to-date survey of clustering methods with special attention to scalability.
Anderberg (1973) provides a general introduction to clustering for applications.
Arthur and Vassilvitskii (2006) investigate the worst-case complexity of K-means.
The K-medoid algorithm was presented by Kaufman and Rousseeuw (1990)
The EM algorithm was originally introduced by Dempster et al.
An in-depth treatment of EM is (McLachlan and Krishnan 1996)
An alternative to AIC is BIC, which can be motivated as a Bayesian model selection procedure (Schwarz 1978)
Fraley and Raftery (1998) show how to choose an optimal number of clusters based on BIC.
An application of BIC to K-means is (Pelleg and Moore 2000)
Hamerly and Elkan (2003) propose an alternative to BIC that performs better in their experiments.
Two methods for determining cardinality without external criteria are presented by Tibshirani et al.
We only have space here for classical completely unsupervised clustering.
For algorithms that can cluster very large data sets in one scan through the data see Bradley et al.
Discard documents that do not occur in one of the 10 classes acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat.
Discard documents that occur in two of these 10 classes.
Identify classes that give rise to false positives and false negatives.
What are the exact values that EM will converge to?
Perform a K-means clustering for the documents in Table 16.3
After how many iterations does K-means converge? Compare the result with the EM clustering in Table 16.3 and discuss the differences.
Derive an AIC criterion for the multivariate Bernoulli mixture model from Equation (16.12)
Flat clustering is efficient and conceptually simple, but as we saw in Chapter 16 it has a number of drawbacks.
The algorithms introduced in Chapter 16 return a flat unstructured set of clusters, require a prespecified number of clusters as input and are nondeterministic.
These advantages of hierarchical clustering come at the cost of lower efficiency.
We then discuss the optimality conditions of hierarchical clustering in Section 17.5
Section 17.7 looks at labeling clusters automatically, a problem that must be solved whenever humans interact with the output of clustering.
Section 17.9 provides pointers to further reading, including references to soft hierarchical clustering, which we do not cover in this book.
There are few differences between the applications of flat and hierarchical clustering in information retrieval.
In fact, the example we gave for collection clustering is hierarchical.
In general, we select flat clustering when efficiency is important and hierarchical clustering when one of the potential problems.
In this chapter, we only consider hierarchies that are binary trees like the one shown in Figure 17.1 – but hierarchical clustering can be easily extended to other types of trees.
In addition, many researchers believe that hierarchical clustering produces better clusters than flat clustering.
However, there is no consensus on this issue (see references in Section 17.9)
Bottomup algorithms treat each document as a singleton cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all documents.
Before looking at specific similarity measures used in HAC in Sections 17.2–17.4, we first introduce a method for depicting hierarchical clusterings graphically, discuss a few key properties of HACs and present a simple algorithm for computing an HAC.
An HAC clustering is typically visualized as a dendrogram as shown inDENDROGRAM Figure 17.1
The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where documents are viewed as singleton clusters.
We call this similarity the combination similarity of the merged cluster.
By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering.
Hierarchical clustering does not require a prespecified number of clusters.
However, in some applications we want a partition of disjoint clusters just as.
Fed to keep interest rates steady Fed keeps interest rates steady Fed keeps interest rates steady.
In those cases, the hierarchy needs to be cut at some point.
A number of criteria can be used to determine the cutting point:
Cut the dendrogram where the gap between two successive combination similarities is largest.
Adding one more cluster decreases the quality of the clustering significantly, so cutting before this steep decrease occurs is desirable.
As in flat clustering, we can also prespecify the number of clusters K and select the cutting point that produces K clusters.
The merge criteria of these four variants of HAC are shown in Figure 17.3
We assume that we use a deterministic method for breaking ties, such as always choose the merge that is the first cluster with respect to a total ordering of the subsets of the document set D.
Figure 17.3 The different notions of cluster similarity used by the four HAC algorithms.
An inter-similarity is a similarity between two documents from different clusters.
Figure 17.4 A single-link (left) and complete-link (right) clustering of eight documents.
We pay attention solely to the area where the two clusters come closest to each other.
Other, more distant parts of the cluster and the clusters’ overall structure are not taken into account.
In complete-link clustering or complete-linkage clustering, the similarity of twoCOMPLETE-LINK CLUSTERING clusters is the similarity of their most dissimilar members (see Figure 17.3, (b))
This is equivalent to choosing the cluster pair whose merge has the smallest diameter.
This complete-link merge criterion is non-local; the entire structure of the clustering can influence merge decisions.
This results in a preference for compact clusters with small diameters over long, straggly clusters, but also causes sensitivity to outliers.
A single document far from the center can increase diameters of candidate merge clusters dramatically and completely change the final clustering.
Figure 17.4 depicts a single-link and a complete-link clustering of eight documents.
The first four steps, each producing a cluster consisting of a pair of two documents, are identical.
Then single-link clustering joins the upper two pairs (and after that the lower two pairs) because on the maximumsimilarity definition of cluster similarity, those two clusters are closest.
Throughout this chapter, we equate similarity with proximity in 2D depictions of clustering.
Fed holds interest rates steady Fed to keep interest rates steady.
Fed keeps interest rates steady Fed keeps interest rates steady.
The local criterion in single-link clustering can cause undesirable elongated clusters.
There is no cut of the dendrogram in Figure 17.1 that would give us an equally balanced clustering.
Define sk to be the combination similarity of the two clusters merged in step k, and G(sk) the graph that links all data points with a similarity of at least sk.
Then the clusters after step k in single-link clustering are the connected components of G(sk) and the clusters after step k in complete-link clustering are maximal cliques of G(sk)
A clique CLIQUE is a set of points that are completely linked with each other.
Single-link and complete-link clustering reduce the assessment of cluster quality to a single similarity between a pair of documents: the two most similar documents in single-link clustering and the two most dissimilar documents in complete-link clustering.
A measurement based on one pair cannot fully reflect the distribution of documents in a cluster.
It is therefore not surprising that both algorithms often produce undesirable clusters.
Single-link clustering can produce straggling clusters as shown in Figure 17.6
Since the merge criterion is strictly local, a chain of points can be extended for long.
The last eleven merges of the single-link clustering (those above the 0.1 line) add on single documents or pairs of documents, corresponding to a chain.
Documents are split into two groups of roughly equal size when we cut the dendrogram at the last merge.
In general, this is a more useful organization of the data than a clustering with chains.
It pays too much attention to outliers, points that do not fit well into the global structure of the cluster.
Complete-link clustering does not find the most intuitive cluster structure in this example.
If I[i] = i, then i is the representative of its current cluster.
The complete-link version of EFFICIENTHAC can also be applied to documents that are not represented as vectors.
Figure 17.10 demonstrates that best-merge persistence does not hold for complete-link clustering, which means that we cannot use an NBM array to speed up clustering.
This is because the complete-link merge criterion is non-local and can be affected by points at a great distance from the area where two merge candidates meet.
We can compute the measure SIM-GA efficiently because the sum of individual vector similarities is equal to the similarities of their sums:
Equation (17.2) relies on the distributivity of the dot product with respect to vector addition.
Since this is crucial for the efficient computation of a GAAC clustering, the method cannot be easily applied to representations of documents that are not real-valued vectors.
To summarize, GAAC requires (i) documents represented as vectors, (ii) length normalization of vectors, so that self-similarities are 1.0, and (iii) the dot product as the measure of similarity between vectors and sums of vectors.
Map them onto the surface of the unit sphere in a three-dimensional space to get length-normalized vectors.
Is the group-average clustering different from the single-link and completelink clusterings?
Each iteration merges the two clusters whose centroids are closest.
In centroid clustering, the similarity of two clusters is defined as the similarity of their centroids:
Equation (17.6) shows that centroid similarity is equivalent to average similarity of all pairs of documents from different clusters.
In contrast to the other three HAC algorithms, centroid clustering is not monotonic.
The non-monotonic inversion in the hierarchical clustering of the three points appears as an intersecting merge line in the dendrogram.
Increasing similarity in a series of HAC clustering steps contradicts the fundamental assumption that small clusters are more coherent than large clusters.
An inversion in a dendrogram shows up as a horizontal merge line that is lower than the previous merge line.
Despite its non-monotonicity, centroid clustering is often used because its similarity measure – the similarity of two centroids – is conceptually simpler than the average of all pairwise similarities in GAAC.
Figure 17.11 is all one needs to understand centroid clustering.
There is no equally simple graph that would explain how GAAC works.
How many distinct cluster similarities are there in GAAC and centroid clustering?
The above definition of optimality would be of limited use if it was only applicable to a clustering together with its merge history.
If we use these definitions of combination similarity, then optimality is a property of a set of clusters and not of a process that produces a set of clusters.
We can now prove the optimality of single-link clustering by induction over the number of clusters K.
We will give a proof for the case where no two pairs of documents have the same similarity, but it can easily be extended to the case with ties.
However, the merge criteria of complete-link clustering and GAAC approximate the desideratum of approximate sphericity better than the merge criterion of single-link clustering.
Thus, even though single-link clustering may seem preferable at first because of its optimality, it is optimal with respect to the wrong criterion in many document clustering applications.
Table 17.1 summarizes the properties of the four HAC algorithms introduced in this chapter.
We recommend GAAC for document clustering because it is generally the method that produces the clustering with the best.
It does not suffer from chaining, from sensitivity to outliers and from inversions.
First, for non-vector representations, GAAC is not applicable and clustering should typically be performed with the complete-link method.
Second, in some applications the purpose of clustering is not to create a complete hierarchy or exhaustive partition of the entire document set.
For instance, first story detection or novelty detection is the task of detecting the firstFIRST STORY.
One approach to this task is to find a tight cluster within the documents that were sent across the wire in a short period of time and are dissimilar from all previous documents.
Variations of single-link clustering can do well on this task since it is the structure of small parts of the vector space – and not global structure – that is important in this case.
Again, the decision whether a group of documents are duplicates of each other is not influenced by documents that are located far away and single-link clustering is a good choice for duplicate detection.
So far we have only looked at agglomerative clustering, but a cluster hierarchy can also be generated top-down.
This variant of hierarchical clustering is called top-down clustering or divisive clustering.
The cluster is split using a flat clustering algoOnline edition (c)
This procedure is applied recursively until each document is in its own singleton cluster.
Top-down clustering is conceptually more complex than bottom-up clustering since we need a second, flat clustering algorithm as a “subroutine”
It has the advantage of being more efficient if we do not generate a complete hierarchy all the way down to individual document leaves.
For a fixed number of top levels, using an efficient flat algorithm like K-means, top-down algorithms are linear in the number of documents and clusters.
So they run much faster than HAC algorithms, which are at least quadratic.
There is evidence that divisive algorithms produce more accurate hierarchies than bottom-up algorithms in some circumstances.
Bottom-up methods make clustering decisions based on local patterns without initially taking into account the global distribution.
Top-down clustering benefits from complete information about the global distribution when making top-level partitioning decisions.
In such settings, we must label clusters, so that users can see what a cluster is about.
Labeling a cluster with the title of the document closest to the centroid is one cluster-internal method.
Titles are easier to read than a list of terms.
A full title can also contain important context that didn’t make it into the top 10 terms selected by MI.
Selecting the most frequent terms is a non-differential feature selection technique we discussed in Section 13.5
The last three columns show cluster summaries computed by three labeling methods: most highly weighted terms in centroid (centroid), mutual information, and the title of the document closest to the centroid of the cluster (title)
Terms selected by only one of the first two methods are in bold.
However, a single document is unlikely to be representative of all documents in a cluster.
An example is cluster 4, whose selected title is misleading.
Articles about hurricane Dolly only ended up in this cluster because of its effect on oil prices.
We can also use a list of terms with high weights in the centroid of the cluster as a label.
Such highly weighted terms (or, even better, phrases, especially noun phrases) are often more representative of the cluster than a few titles can be, even if they are not filtered for distinctiveness as in the differential methods.
However, a list of phrases takes more time to digest for users than a well crafted title.
Cluster-internal methods are efficient, but they fail to distinguish terms that are frequent in the collection as a whole from those that are frequent only in the cluster.
Terms like year or Tuesday may be among the most frequent in a cluster, but they are not helpful in understanding the contents of a cluster with a specific topic like oil.
We get a good sense of the documents in a cluster from scanning the selected terms.
Not only do we need to distinguish an internal node in the tree from its siblings, but also from its parent and its children.
Documents in child nodes are by definition also members of their parent node, so we cannot use a naive differential method to find labels that distinguish the parent from its children.
However, more complex criteria, based on a combination of overall collection frequency and prevalence in a given cluster, can determine whether a term is a more informative label for a child node or a parent node (see Section 17.9)
Most problems that require the computation of a large number of dot products benefit from an inverted index.
Computational savings due to the inverted index are large if there are many zero similarities – either because many documents do not share any terms or because an aggressive stop list is used.
In low dimensions, more aggressive optimizations are possible that make the computation of most pairwise similarities unnecessary (Exercise 17.10)
N, then the overall runtime of K-means cum HAC seed.
The single-link algorithm in Figure 17.9 is similar to Kruskal’s algorithm for constructing a minimumKRUSKAL’S.
A graph-theoretical proof of the correctness of Kruskal’s algorithm (which is analogous to the proof in Section 17.5) is provided by Cormen et al.
See Exercise 17.5 for the connection between minimum spanning trees and single-link clusterings.
Even without a consensus on average behavior, there is no doubt that results of EM and K-means are highly variable since they will often converge to a local optimum of poor quality.
The HAC algorithms we have presented here are deterministic and thus more predictable.
The centroid algorithm described here is due to Voorhees (1985b)
Voorhees recommends complete-link and centroid clustering over single-link for a retrieval application.
The Buckshot algorithm was originally published by Cutting et al.
The merge criterion in Ward’s method (a function of all individual distances from the centroid) is closely related to the merge criterion in GAAC (a function of all individual similarities to the centroid)
Unlike K-means and EM, most hierarchical clustering algorithms do not have a probabilistic interpretation.
The R environment (R Development Core Team 2005) offers good support for hierarchical clustering.
The R function hclust implements single-link, complete-link, group-average, and centroid clustering; and Ward’s method.
Support for clustering vectors in high-dimensional spaces is provided by the software package CLUTO (http://glaros.dtc.umn.edu/gkhome/views/cluto)
Exercise 17.5A single-link clustering can also be computed from the minimum spanning tree of aMINIMUM SPANNING TREE graph.
The minimum spanning tree connects the vertices of a graph at the smallest.
Show that single-link clustering is best-merge persistent and that GAAC and centroid clustering are not best-merge persistent.
Consider running 2-means clustering on a collection with documents from two different languages.
Would you expect the same result when running an HAC algorithm?
Keep only documents that are in the classes crude, interest, and grain.
Discard documents that are members of more than one of these three classes.
Compute a (i) single-link, (ii) complete-link, (iii) GAAC, (iv) centroid clustering of the documents.
Compute the Rand index for each of the 4 clusterings.
Suppose a run of HAC finds the clustering with K = 7 to have the highest value on some prechosen goodness measure of clustering.
Have we found the highest-value clustering among all clusterings with K = 7?
Consider the task of producing a single-link clustering of N points on a line:
Show that we only need to compute a total of about N similarities.
What is the overall complexity of single-link clustering for a set of points on a line?
The number of non-zero eigenvalues of C is at most rank(C)
We now examine some further properties of eigenvalues and eigenvectors, to set up the central idea of singular value decompositions in Section 18.2 below.
First, we look at the relationship between matrix-vector multiplication and eigenvalues.
This suggests that the effect of small eigenvalues (and their eigenvectors) on a matrix-vector product is small.
We will carry forward this intuition when studying matrix decompositions and low-rank approximations in Section 18.2
Before doing so, we examine the eigenvectors and eigenvalues of special forms of matrices that will be of particular interest to us.
For a symmetric matrix S, the eigenvectors corresponding to distinct eigenvalues are orthogonal.
Further, if S is both real and symmetric, the eigenvalues are all real.
In this section we examine ways in which a square matrix can be factored into the product of matrices derived from its eigenvectors; we refer to this process as matrix decomposition.
The square decompositions in this section are simpler and can be treated with sufficient mathematical rigor to help the reader understand how such decompositions work.
The detailed mathematical derivation of the more complex decompositions in Section 18.2 are beyond the scope of this book.
We begin by giving two theorems on the decomposition of a square matrix into the product of three matrices of a special form.
The first of these, Theorem 18.1, gives the basic factorization of a square real-valued matrix into three factors.
To understand how Theorem 18.1 works, we note that U has the eigenvectors of S as columns.
This will pave the way for the development of our main tool for text analysis, the singular value decomposition (Section 18.2)
We will build on this symmetric diagonal decomposition to build low-rank approximations to term-document matrices.
We then show in Section 18.3 how this can be used to construct an approximate version of C.
It is beyond the scope of this book to develop a full.
By multiplying Equation (18.9) by its transposed version, we have.
What does the left-hand side CCT represent? It is a square matrix with a row and a column corresponding to each of the M terms.
The entry (i, j) in the matrix is a measure of the overlap between the ith and jth terms, based on their co-occurrence in documents.
The precise mathematical meaning depends on the manner in which C is constructed based on term weighting.
Then the entry (i, j) in CCT is the number of documents in which both term i and term j occur.
In this schematic illustration of (18.9), we see two cases illustrated.
In the top half of the figure, we have a matrix C for which M > N.
Verify that the SVD of the matrix in Equation (18.12) is.
We next state a matrix approximation problem that at first seems to have little to do with information retrieval.
We describe a solution to this matrix problem using singular-value decompositions, then develop its application to information retrieval.
Thus, the Frobenius norm of X measures the discrepancy between Ck and C; our goal is to find a matrix Ck that minimizes this discrepancy, while constraining Ck to have rank at most k.
If r is the rank of C, clearly Cr = C and the Frobenius norm of the discrepancy is zero in this case.
We then derive from it an application to approximating term-document matrices.
Figure 18.2 Illustration of low rank approximation using the singular-value decomposition.
The dashed boxes indicate the matrix entries affected by “zeroing out” the smallest singular values.
What is the Frobenius norm of the error of this approximation?
We now discuss the approximation of a term-document matrix C by one of lower rank using the SVD.
The low-rank approximation to C yields a new representation for each document in the collection.
This process is known as latent semantic indexing (generally abbreviated LSI).LATENT SEMANTIC.
Next, we use the new k-dimensional LSI representation as we did the original representation – to compute similarities between vectors.
A query vector ~q is mapped into its representation in the LSI space by the transformation.
This means that if we have an LSI representation of a collection of documents, a new document not in the collection can be “folded in” to this representation using Equation (18.21)
This allows us to incrementally add documents to an LSI representation.
Of course, such incremental addition fails to capture the co-occurrences of the newly added documents (and even ignores any new terms they contain)
As such, the quality of the LSI representation will degrade as more documents are added and will eventually require a recomputation of the LSI representation.
The fidelity of the approximation of Ck to C leads us to hope that the relative values of cosine similarities are preserved: if a query is close to a document in the original space, it remains relatively close in the k-dimensional space.
This has a significant computational cost, when compared with the cost of processing~q in its native form.
Its singular value decomposition is the product of three matrices as below.
Finally we have VT , which in the context of a term-document matrix is known as the SVD document matrix:
Notice that the low-rank approximation, unlike the original matrix C, can have negative entries.
At the time of their work in the early 1990’s, the LSI computation on tens of thousands of documents took approximately a day on one machine.
On these experiments, they achieved precision at or above that of the median TREC participant.
Here are some conclusions on LSI first suggested by their work, and subsequently verified by many other experiments.
The computational cost of the SVD is significant; at the time of this writing, we know of no successful experiment with over one million documents.
This has been the biggest obstacle to the widespread adoption to LSI.
One approach to this obstacle is to build the LSI representation on a randomly sampled subset of the documents in the collection, following which the remaining documents are “folded in” as detailed with Equation (18.21)
As we reduce k, recall tends to increase, as expected.
Most surprisingly, a value of k in the low hundreds can actually increase precision on some query benchmarks.
This appears to suggest that for a suitable value of k, LSI addresses some of the challenges of synonymy.
The experiments also documented some modes where LSI failed to match the effectiveness of more traditional indexes and score computations.
Most notably (and perhaps obviously), LSI shares two basic drawbacks of vector space retrieval: there is no good way of expressing negations (find documents that contain german but not shepherd), and no way of enforcing Boolean conditions.
Hofmann (1999a;b) provides an initial probabilistic extension of the basic latent semantic indexing technique.
Spanish English mi my casa house hola hello profesor professor y and bienvenido welcome.
This model is extended to a hierarchical clustering by Rosen-Zvi et al.
Exercise 18.11Assume you have a set of documents each of which is in either English or in Spanish.
Figure 18.5 gives a glossary relating the Spanish and English words above for your own information.
Construct the appropriate term-document matrix C to use for a collection consisting of these documents.
For simplicity, use raw term frequencies rather than normalized tf-idf weights.
Make sure to clearly label the dimensions of your matrix.
State succinctly what the (i, j) entry in the matrix CTC represents.
In this and the following two chapters, we consider web search engines.
Sections 19.1–19.4 provide some background and history to help the reader appreciate the forces that conspire to make the Web chaotic, fast-changing and (from the standpoint of information retrieval) very different from the “traditional” collections studied thus far in this book.
Sections 19.5–19.6 deal with estimating the number of documents indexed by web search engines, and the elimination of duplicate documents in web indexes, respectively.
These two latter sections serve as background material for the following two chapters.
The Web is unprecedented in many ways: unprecedented in scale, unprecedented in the almost-complete lack of coordination in its creation, and unprecedented in the diversity of backgrounds and motives of its participants.
Each of these contributes to making web search different – and generally far harder – than searching “traditional” documents.
The basic operation is as follows: a client (such as a browser) sends an http request to a web server.
In this example URL, the string http refers to the protocol to be used for transmitting the data.
The HTML-encoded file contact.html holds the hyperlinks and the content (in this instance, contact information for Stanford University), as well as formatting rules for rendering this content in a browser.
Such an http request thus allows us to fetch the content of a page, something that will prove to be useful to us for crawling and indexing documents (Chapter 20)
The designers of the first browsers made it easy to view the HTML markup tags on the content of a URL.
This simple convenience allowed new users to create their own HTML content without extensive training or experience; rather, they learned from example content that they liked.
As they did so, a second feature of browsers supported the rapid proliferation of web content creation and usage: browsers ignored what they did not understand.
This did not, as one might fear, lead to the creation of numerous incompatible dialects of HTML.
What it did promote was amateur content creators who could freely experiment with and learn from their newly created web pages without fear that a simple syntax error would “bring the system down.” Publishing on the Web became a mass activity that was not limited to a few trained programmers, but rather open to tens and eventually hundreds of millions of individuals.
For most users and for most information needs, the Web quickly became the best way to supply and consume information on everything from rare ailments to subway schedules.
The mass publishing of information on the Web is essentially useless unless this wealth of information can be discovered and consumed by other users.
Early attempts at making web information “discoverable” fell into two broad categories: (1) full-text index search engines such as Altavista, Excite and Infoseek and (2) taxonomies populated with web pages in categories, such as Yahoo! The former presented the user with a keyword search interface supported by inverted indexes and ranking mechanisms building on those introduced in earlier chapters.
The latter allowed the user to browse through a hierarchical tree of category labels.
While this is at first blush a convenient and intuitive metaphor for finding web pages, it has a number of drawbacks: first, accurately classifying web pages into taxonomy tree nodes is for the most part a manual editorial process, which is difficult to scale with the size of the Web.
However, just discovering these and classifying them accurately and consistently into the taxonomy entails significant human effort.
Furthermore, in order for a user to effectively discover web pages classified into the nodes of the taxonomy tree, the user’s idea of what sub-tree(s) to seek for a particular topic should match that of the editors performing the classification.
This quickly becomes challenging as the size of the taxonomy grows; the Yahoo! taxonomy tree surpassed 1000 distinct nodes fairly early on.
Given these challenges, the popularity of taxonomies declined over time, even though variants (such as About.com and the Open Directory Project) sprang up with subject-matter experts collecting and annotating web pages for each category.
The first generation of web search engines transported classical search techniques such as those in the preceding chapters to the web domain, focusing on the challenge of scale.
The earliest web search engines had to contend with indexes containing tens of millions of documents, which was a few orders of magnitude larger than any prior information retrieval system in the public domain.
Indexing, query serving and ranking at this scale required the harnessing together of tens of machines to create highly available systems, again at scales not witnessed hitherto in a consumer-facing search application.
The first generation of web search engines was largely successful at solving these challenges while continually indexing a significant fraction of the Web, all the while serving queries with sub-second response times.
However, the quality and relevance of web search results left much to be desired owing to the idiosyncrasies of content creation on the Web that we discuss in Section 19.2
This necessitated the invention of new ranking and spam-fighting techniques in order to ensure the quality of the search results.
While classical information retrieval techniques (such as those covered earlier in this book) continue to be necessary for web search, they are not by any means sufficient.
A key aspect (developed further in Chapter 21) is that whereas classical techniques measure the relevance of a document to a query, there remains a need to gauge the authoritativeness of a document based on cues such as which website hosts it.
The essential feature that led to the explosive growth of the web – decentralized content publishing with essentially no central control of authorship turned out to be the biggest challenge for web search engines in their quest to index and retrieve this content.
Web page authors created content in dozens of (natural) languages and thousands of dialects, thus demanding many different forms of stemming and other linguistic operations.
Indeed, web publishing in a sense unleashed the best and worst of desktop publishing on a planetary scale, so that pages quickly became riddled with wild variations in colors, fonts and structure.
Some web pages, including the professionally created home pages of some large corporations, consisted entirely of images (which, when clicked, led to richer textual content) – and therefore, no indexable text.
What about the substance of the text in web pages? The democratization of content creation on the web meant a new level of granularity in opinion on virtually any subject.
This meant that the web contained truth, lies, contradictions and suppositions on a grand scale.
This gives rise to the question: which web pages does one trust? In a simplistic approach, one might argue that some publishers are trustworthy and others not – begging the question of how a search engine is to assign such a measure of trust to each website or web page.
In Chapter 21 we will examine approaches to understanding this question.
More subtly, there may be no universal, user-independent notion of trust; a web page whose contents are trustworthy to one user may not be so to another.
In traditional (non-web) publishing this is not an issue: users self-select sources they find trustworthy.
Thus one reader may find the reporting of The New York Times to be reliable, while another may prefer The Wall Street Journal.
But when a search engine is the only viable means for a user to become aware of (let alone select) most content, this challenge becomes significant.
While the question “how big is the Web?” has no easy answer (see Section 19.5), the question “how many web pages are in a search engine’s index” is more precise, although, even this question has issues.
Static web pages are those whose content does not vary fromSTATIC WEB PAGES one request for that page to the next.
For this purpose, a professor who manually updates his home page every week is considered to have a static web page, but an airport’s flight status page is considered to be dynamic.
Dynamic pages are typically mechanically generated by an application server in response to a query to a database, as show in Figure 19.1
One sign of such a page is that the URL has the character "?" in it.
Since the number of static web pages was believed to be doubling every few months in 1995, early web search engines such as Altavista had to constantly add hardware and bandwidth for crawling and indexing web pages.
The browser sends a request for flight information on flight AA129 to the web application, that fetches the information from back-end databases then creates a dynamic web page that it returns to the browser.
Figure 19.2 Two nodes of the web graph joined by a link.
We can view the static Web consisting of static HTML pages together with the hyperlinks between them as a directed graph in which each web page is a node and each hyperlink a directed edge.
Figure 19.2 shows two nodes A and B from the web graph, each corresponding to a web page, with a hyperlink from A to B.
We refer to the set of all such nodes and directed edges as the web graph.
Figure 19.2 also shows that (as is the case with most links on web pages) there is some text surrounding the origin of the hyperlink on page A.
This text is generally encapsulated in the href attribute of the <a> (for anchor) tag that encodes the hyperlink in the HTML code of page A, and is referred to as anchor text.
As one mightANCHOR TEXT suspect, this directed graph is not strongly connected: there are pairs of pages such that one cannot proceed from one page of the pair to the other by following hyperlinks.
We refer to the hyperlinks into a page as in-links and thoseIN-LINKS out of a page as out-links.
We similarly define the out-degree of a web page to be the number of links out.
This example graph is not strongly connected: there is no path from any of pages B-F to page A.
There is ample evidence that these links are not randomly distributed; for.
The remaining pages form into tubes that are small sets of pages outside SCC that lead directly from IN to OUT, and tendrils that either lead nowhere from IN, or from nowhere to OUT.
Early in the history of web search, it became clear that web search engines were an important means for connecting advertisers to prospective buyers.
A user searching for maui golf real estate is not merely seeking news or entertainment on the subject of housing on golf courses on the island of Maui, but instead likely to be seeking to purchase such a property.
Sellers of such property and their agents, therefore, have a strong incentive to create web pages that rank highly on this query.
In a search engine whose scoring was based on term frequencies, a web page with numerous repetitions of maui golf real estate would rank highly.
This led to the first generation of spam, whichSPAM (in the context of web search) is the manipulation of web page content for the purpose of appearing high up in search results for selected keywords.
To avoid irritating users with these repetitions, sophisticated spammers resorted to such tricks as rendering these repeated terms in the same color as the background.
Despite these words being consequently invisible to the human user, a search engine indexer would parse the invisible words out of.
At its root, spam stems from the heterogeneity of motives in content creation on the Web.
In particular, many web content creators have commercial motives and therefore stand to gain from manipulating search engine results.
You might argue that this is no different from a company that uses large fonts to list its phone numbers in the yellow pages; but this generally costs the company more and is thus a fairer mechanism.
A more apt analogy, perhaps, is the use of company names beginning with a long string of A’s to be listed early in a yellow pages category.
In fact, the yellow pages’ model of companies paying for larger/darker fonts has been replicated in web search: in many search engines, it is possible to pay to have one’s web page included in the search engine’s index – a model known as paid inclusion.
DifferentPAID INCLUSION search engines have different policies on whether to allow paid inclusion, and whether such a payment has any effect on ranking in search results.
Search engines soon became sophisticated enough in their spam detection to screen out a large number of repetitions of particular keywords.
Spammers responded with a richer set of spam techniques, the best known of which we now describe.
The first of these techniques is cloaking, shown in Figure 19.5
Here, the spammer’s web server returns different pages depending on whether the http request comes from a web search engine’s crawler (the part of the search engine that gathers web pages, to be described in Chapter 20), or from a human user’s browser.
The former causes the web page to be indexed by the search engine under misleading keywords.
When the user searches for these keywords and elects to view the page, he receives a web page that has altogether different content than that indexed by the search engine.
Such deception of search indexers is unknown in the traditional world of information retrieval; it stems from the fact that the relationship between page publishers and web search engines is not completely collaborative.
A doorway page contains text and metadata carefully chosen to rank highly.
When a browser requests the doorway page, it is redirected to a page containing content of a more commercial nature.
More complex spamming techniques involve manipulation of the metadata related to a page including (for reasons we will see in Chapter 21) the links into a web page.
Given that spamming is inherently an economically motivated activity, there has sprung around it an industry of Search Engine Optimizers,SEARCH ENGINE.
Web search engines frown on this business of attempting to decipher and adapt to their proprietary ranking techniques and indeed announce policies on forms of SEO behavior they do not tolerate (and have been known to shut down search requests from certain SEOs for violation of these)
Inevitably, the parrying between such SEOs (who gradually infer features of each web search engine’s ranking methods) and the web search engines (who adapt in response) is an unending struggle; indeed, the research sub-area of adversarial information retrieval has sprung upADVERSARIAL.
To combat spammers who manipulate the text of their web pages is the exploitation of the link structure of the Web – a technique known as link analysis.
The first web search engine known to apply link analysis on a large scale (to be detailed in Chapter 21) was Google, although all web search engines currently make use of it (and correspondingly, spammers now invest considerable effort in subverting it – this is known as linkLINK SPAM spam)
If the number of pages with in-degree i is proportional to 1/i2.1, what is the average in-degree of a web page?
What can we say about the average out-degree of all nodes in this snapshot?
Early in the history of the Web, companies used graphical banner advertisements on web pages at popular websites (news and entertainment sites such as MSN, America Online, Yahoo! and CNN)
The primary purpose of these advertisements was branding: to convey to the viewer a positive feeling about.
Typically these advertisements are priced on a cost per mil (CPM) basis: the cost to the company ofCPM having its banner advertisement displayed 1000 times.
Some websites struck contracts with their advertisers in which an advertisement was priced not by the number of times it is displayed (also known as impressions), but rather by the number of times it was clicked on by the user.
This pricing model is known as the cost per click (CPC) model.
In such cases, clicking on the adver-CPC tisement leads the user to a web page set up by the advertiser, where the user is induced to make a purchase.
Here the goal of the advertisement is not so much brand promotion as to induce a transaction.
The interactivity of the web allowed the CPC billing model – clicks could be metered and monitored by the website and billed to the advertiser.
The pioneer in this direction was a company named Goto, which changed its name to Overture prior to eventual acquisition by Yahoo! Goto was not, in the traditional sense, a search engine; rather, for every query term q it accepted bids from companies who wanted their web page shown on the query q.
In response to the query q, Goto would return the pages of all advertisers who bid for q, ordered by their bids.
Furthermore, when the user clicked on one of the returned results, the corresponding advertiser would make a payment to Goto (in the initial implementation, this payment equaled the advertiser’s bid for q)
First, a user typing the query q into Goto’s search interface was actively expressing an interest and intent related to the query q.
For instance, a user typing golf clubs is more likely to be imminently purchasing a set than one who is simply browsing news on golf.
Second, Goto only got compensated when a user actually expressed interest in an advertisement – as evinced by the user clicking the advertisement.
Taken together, these created a powerful mechanism by which to connect advertisers to consumers, quickly raising the annual revenues of Goto/Overture into hundreds of millions of dollars.
Current search engines follow precisely this model: they provide pure search results (generally known as algorithmic search results) as the primary response to aALGORITHMIC SEARCH user’s search, together with sponsored search results displayed separately and distinctively to the right of the algorithmic results.
Retrieving sponsored search results and ranking them in response to a query has now become considerably more sophisticated than the simple Goto scheme; the process entails a blending of ideas from information.
The lack of advertisements for the aircraft reflects the fact that few marketers attempt to sell A320 aircraft on the web.
For advertisers, understanding how search engines do this ranking and how to allocate marketing campaign budgets to different keywords and to different sponsored search engines has become a profession known as search engineSEARCH ENGINE.
This can take many forms, one of which is known as click spam.
There isCLICK SPAM currently no universally accepted definition of click spam.
It refers (as the name suggests) to clicks on sponsored search results that are not from bona fide search users.
For instance, a devious advertiser may attempt to exhaust the advertising budget of a competitor by clicking repeatedly (through the use of a robotic click generator) on that competitor’s sponsored search advertisements.
Search engines face the challenge of discerning which of the clicks they observe are part of a pattern of click spam, to avoid charging their advertiser clients for such clicks.
Exercise 19.5The Goto method ranked advertisements matching a query by bid: the highest-bidding advertiser got the top position, the second-highest the next, and so on.
What can go wrong with this when the highest-bidding advertiser places an advertisement that is irrelevant to the query? Why might an advertiser with an irrelevant advertisement bid high in this manner?
Suppose that, in addition to bids, we had for each advertiser their click-through rate: the ratio of the historical number of times users click on their advertisement to the number of times the advertisement was shown.
Suggest a modification of the Goto scheme that exploits this data to avoid the problem in Exercise 19.5 above.
It is crucial that we understand the users of web search as well.
This is again a significant change from traditional information retrieval, where users were typically professionals with at least some training in the art of phrasing queries over a well-authored collection whose style and structure they understood well.
In contrast, web search users tend to not know (or care) about the heterogeneity of web content, the syntax of query languages and the art of phrasing queries; indeed, a mainstream tool (as web search has come to become) should not place such onerous demands on billions of people.
It is clear that the more user traffic a web search engine can attract, the more revenue it stands to earn from sponsored search.
How do search engines differentiate themselves and grow their traffic? Here Google identified two principles that helped it grow at the expense of its competitors: (1) a focus on relevance, specifically precision rather than recall in the first few results; (2) a user experience that is lightweight, meaning that both the search query page and the search results page are uncluttered and almost entirely textual, with very few graphical elements.
The effect of the first was simply to save users time in locating the information they sought.
The effect of the second is to provide a user experience that is extremely responsive, or at any rate not bottlenecked by the time to load the search query or results page.
There appear to be three broad categories into which common web search queries can be grouped: (i) informational, (ii) navigational and (iii) transactional.
We now explain these categories; it should be clear that some queries will fall in more than one of these categories, while others will fall outside them.
Informational queries seek general information on a broad topic, such asINFORMATIONAL.
There is typically not a single web page that contains all the information sought; indeed, users with informational queries typically try to assimilate information from multiple web pages.
Navigational queries seek the website or home page of a single entity that theNAVIGATIONAL.
In such cases, the user’s expectation is that the very first search result should be the home page of Lufthansa.
In such cases, the search engine should return results listing services that provide form interfaces for such transactions.
Discerning which of these categories a query falls into can be challenging.
The category not only governs the algorithmic search results, but the suitability of the query for sponsored search results (since the query may reveal an intent to purchase)
For navigational queries, some have argued that the search engine should return only a single result or even the target web page directly.
Nevertheless, web search engines have historically engaged in a battle of bragging rights over which one indexes more web pages.
Does the user really care? Perhaps not, but the media does highlight estimates (often statistically indefensible) of the sizes of various search engines.
Users are influenced by these reports and thus, search engines do have to pay attention to how their index sizes compare to competitors’
For informational (and to a lesser extent, transactional) queries, the user does care about the comprehensiveness of the search engine.
Figure 19.7 shows a composite picture of a web search engine including the crawler, as well as both the web page and advertisement indexes.
The portion of the figure under the curved dashed line is internal to the search engine.
To a first approximation, comprehensiveness grows with index size, although it does matter which specific pages a search engine indexes – some pages are more informative than others.
It is also difficult to reason about the fraction of the Web indexed by a search engine, because there is an infinite number of dynamic web pages; for instance, http://www.yahoo.com/any_string returns a valid HTML page rather than an error, politely informing the user that there is no such page at Yahoo! Such a "soft 404 error" is only one example of many ways in which web servers can generate an infinite number of valid web pages.
Indeed, some of these are malicious spider traps devised to cause a search engine’s crawler (the component that systematically gathers web pages for the search engine’s index, described in Chapter 20) to stay within a spammer’s website and index many pages from that site.
We could ask the following better-defined question: given two search engines, what are the relative sizes of their indexes? Even this question turns out to be imprecise, because:
In response to queries a search engine can return web pages whose contents it has not (fully or even partially) indexed.
For one thing, search engines generally index only the first few thousand words in a web page.
Figure 19.7 The various components of a web search engine.
In some cases, a search engine is aware of a page p that is linked to by pages it has indexed, but has not indexed p itself.
As we will see in Chapter 21, it is still possible to meaningfully return p in search results.
Search engines generally organize their indexes in various tiers and partitions, not all of which are examined on every search (recall tiered indexes from Section 7.2.1)
For instance, a web page deep inside a website may be indexed but not retrieved on general web searches; it is however retrieved as a result on a search that a user has explicitly restricted to that website (such site-specific search is offered by most web search engines)
Thus, search engine indexes include multiple classes of indexed pages, so that there is no single measure of index size.
The basic hypothesis underlying these techniques is that each search engine indexes a fraction of the Web chosen independently and uniformly at random.
This involves some questionable assumptions: first, that there is a finite size for the Web from which each search engine chooses a subset, and second, that each engine chooses an independent, uniformly chosen subset.
As will be clear from the discussion of crawling in Chapter 20, this is far from true.
Then, letting |Ei| denote the size of the index of search engine Ei, we have.
Either the measurement is performed by someone with access to the index of one of the search engines (say an employee of E1), or the measurement is performed by an independent party with no access to the innards of either search engine.
In the former case, we can simply pick a random document from one index.
The latter case is more challenging; by picking a random page from one search engine from outside the search engine, then verify whether the random page is present in the other search engine.
To implement the sampling phase, we might generate a random page from the entire (idealized, finite) Web and test it for presence in each search engine.
Unfortunately, picking a web page uniformly at random is a difficult problem.
We briefly outline several attempts to achieve such a sample, pointing out the biases inherent to each; following this we describe in some detail one technique that much research has built on.
Random searches: Begin with a search log of web searches; send a random search from this log to E1 and a random page from the results.
Since such logs are not widely available outside a search engine, one implementation is to trap all search queries going out of a work group (say scientists in a research center) that agrees to have all its searches logged.
This approach has a number of issues, including the bias from the types of searches made by the work group.
Furthermore, this technique is more likely to hit one of the many sites with few pages, skewing the document probabilities; we may be able to correct for this effect if we understand the distribution of the number of pages on websites.
Random walks: If the web graph were a strongly connected directed graph, we could run a random walk starting at an arbitrary web page.
First, the Web is not strongly connected so that, even with various corrective rules, it is difficult to argue that we can reach a steady state distribution starting from any page.
Second, the time it takes for the random walk to settle into this steady state is unknown and could exceed the length of the experiment.
This approach is noteworthy for two reasons: it has been successfully built upon for a series of increasingly refined estimates, and conversely it has turned out to be the approach most likely to be misinterpreted and carelessly implemented, leading to misleading measurements.
The idea is to pick a page (almost) uniformly at random from a search engine’s index by posing a random query to it.
It should be clear that picking a set of random terms from (say) Webster’s dictionary is not a good way of implementing this idea.
For one thing, not all vocabulary terms occur equally often, so this approach will not result in documents being chosen uniformly at random from the search engine.
For another, there are a great many terms in web documents that do not occur in a standard dictionary such as Webster’s.
To address the problem of vocabulary terms not in a standard dictionary, we begin by amassing a sample web dictionary.
This could be done by crawling a limited portion of the Web, or by crawling a manually-assembled representative subset of the Web such as Yahoo! (as was done in the earliest experiments with this method)
Consider a conjunctive query with two or more randomly chosen words from this dictionary.
We can improve the estimate by repeating the experiment a large number of times.
Both the sampling process and the testing process have a number of issues.
Picking from all the results of E1 makes the experiment slower.
This is particularly so because most web search engines put up defenses against excessive robotic querying.
A sequence of research has built on this basic paradigm to eliminate some of these issues; there is no perfect solution yet, but the level of sophistication in statistics for understanding the biases is increasing.
The main idea is to address biases by estimating, for each document, the magnitude of the bias.
From this, standard statistical sampling methods can generate unbiased samples.
In the checking phase, the newer work moves away from conjunctive queries to phrase and other queries that appear to be betterbehaved.
Finally, newer experiments use other sampling methods besides random queries.
The best known of these is document random walk sampling, in which a document is chosen by a random walk on a virtual graph derived from documents.
In this graph, nodes are documents; two documents are connected by an edge if they share two or more words in common.
The graph is never instantiated; rather, a random walk on it can be performed by moving from a document d to another by picking a pair of keywords in d, running a query on a search engine and picking a random document from the results.
Details may be found in the references in Section 19.7
Exercise 19.7Two web search engines A and B each generate a large number of pages uniformly at random from their indexes.
What is the number of pages in A’s index relative to B’s?
One aspect we have ignored in the discussion of index size in Section 19.5 is duplication: the Web contains multiple copies of the same content.
By some estimates, as many as 40% of the pages on the Web are duplicates of other pages.
Many of these are legitimate copies; for instance, certain information repositories are mirrored simply to provide redundancy and access reliability.
Search engines try to avoid indexing multiple copies of the same content, to keep down storage and processing overheads.
The simplest approach to detecting duplicates is to compute, for each web page, a fingerprint that is a succinct (say 64-bit) digest of the characters on that page.
Then, whenever the fingerprints of two web pages are equal, we test whether the pages themselves are equal and if so declare one of them to be a duplicate copy of the other.
This simplistic approach fails to capture a crucial and widespread phenomenon on the Web: near duplication.
In many cases, the contents of one web page are identical to those of another except for a few characters – say, a notation showing the date and time at which the page was last modified.
Even in such cases, we want to be able to declare the two pages to be close enough that we only index one copy.
Short of exhaustively comparing all pairs of web pages, an infeasible task at the scale of billions of pages, how can we detect and filter out such near duplicates?
We now describe a solution to the problem of detecting near-duplicate web pages.
Given a positiveSHINGLING integer k and a sequence of terms in a document d, define the k-shingles of d to be the set of all consecutive sequences of k terms in d.
As an example, consider the following text: a rose is a rose is a rose.
The first two of these shingles each occur twice in the text.
Intuitively, two documents are near duplicates if the sets of shingles generated from them are nearly the same.
We now make this intuition precise, then develop a method for efficiently computing and comparing the sets of shingles for all web pages.
We give the proof in a slightly more general setting: consider a family of sets whose elements are drawn from a common universe.
View the sets as columns of a matrix A, with one row for each element in the universe.
The element aij = 1 if element i is present in the set Sj that the jth column represents.
Indeed, the first four rows of Figure 19.9 exemplify all of these four types of rows.
Exercise 19.8Web search engines A and B each crawl a random subset of the same size of the Web.
Some of the pages crawled are duplicates – exact textual copies of each other at different URLs.
Assume that duplicates are distributed uniformly amongst the pages crawled by A and B.
Further, assume that a duplicate is a page that has exactly two copies – no pages have more than two copies.
A indexes pages without duplicate elimination whereas B indexes only one copy of each duplicate page.
The two random subsets have the same size before duplicate elimination.
We exhaustively compute the Jaccard coefficient of these random subsets.
Explain why this estimator would be very difficult to use in practice.
Bush (1945) foreshadowed the Web when he described an information management system that he called memex.
The use of anchor text was first described in McBryan (1994)
Chakrabarti (2002) is a good reference for many aspects of web search and analysis.
Web crawling is the process by which we gather pages from the Web, in order to index them and support a search engine.
The objective of crawling is to quickly and efficiently gather as many useful web pages as possible, together with the link structure that interconnects them.
In Chapter 19 we studied the complexities of the Web stemming from its creation by millions of uncoordinated individuals.
In this chapter we study the resulting difficulties for crawling the Web.
The focus of this chapter is the component shown in Figure 19.7 as web crawler; it is sometimes referred to as a spider.WEB CRAWLER.
We focus instead on a range of issues that are generic to crawling from the student project scale to substantial research projects.
The remainder of this chapter describes the architecture and some implementation details for a distributed web crawler that satisfies these features.
Section 20.3 discusses distributing indexes across many machines for a web-scale implementation.
We list the desiderata for web crawlers in two categories: features that web crawlers must provide, followed by features they should provide.
Robustness: The Web contains servers that create spider traps, which are generators of web pages that mislead crawlers into getting stuck fetching an infinite number of pages in a particular domain.
Crawlers must be designed to be resilient to such traps.
Not all such traps are malicious; some are the inadvertent side-effect of faulty website development.
Politeness: Web servers have both implicit and explicit policies regulating the rate at which a crawler can visit them.
Distributed: The crawler should have the ability to execute in a distributed fashion across multiple machines.
Scalable: The crawler architecture should permit scaling up the crawl rate by adding extra machines and bandwidth.
Performance and efficiency: The crawl system should make efficient use of various system resources including processor, storage and network bandwidth.
Quality: Given that a significant fraction of all web pages are of poor utility for serving user query needs, the crawler should be biased towards fetching “useful” pages first.
Freshness: In many applications, the crawler should operate in continuous mode: it should obtain fresh copies of previously fetched pages.
A search engine crawler, for instance, can thus ensure that the search engine’s index contains a fairly current representation of each indexed web page.
For such continuous crawling, a crawler should be able to crawl a page with a frequency that approximates the rate of change of that page.
Extensible: Crawlers should be designed to be extensible in many ways to cope with new data formats, new fetch protocols, and so on.
The basic operation of any hypertext crawler (whether for the Web, an intranet or other hypertext document collection) is as follows.
The crawler begins with one or more URLs that constitute a seed set.
It picks a URL from this seed set, then fetches the web page at that URL.
The fetched page is then parsed, to extract both the text and the links from the page (each of which points to another URL)
The extracted links (URLs) are then added to a URL frontier, which at all times consists of URLs whose corresponding pages have yet to be fetched by the crawler.
Initially, the URL frontier contains the seed set; as pages are fetched, the corresponding URLs are deleted from the URL frontier.
In continuous crawling, the URL of a fetched page is added back to the frontier for fetching again in the future.
This seemingly simple recursive traversal of the web graph is complicated by the many demands on a practical web crawling system: the crawler has to be distributed, scalable, efficient, polite, robust and extensible while fetching pages of high quality.
Our treatment follows the design of the Mercator crawler that has formed the ba-MERCATOR sis of a number of research and commercial crawlers.
As a reference point, fetching a billion pages (a small fraction of the static Web at present) in a month-long crawl requires fetching several hundred pages each second.
We will see how to use a multi-threaded design to address several bottlenecks in the overall crawler system in order to attain this fetch rate.
Before proceeding to this detailed description, we reiterate for readers who may attempt to build crawlers of some basic properties any non-professional crawler should satisfy:
Only one connection should be open to any given host at a time.
A waiting time of a few seconds should occur between successive requests to a host.
The simple scheme outlined above for crawling demands several modules that fit together as shown in Figure 20.1
The URL frontier, containing URLs yet to be fetched in the current crawl (in the case of continuous crawling, a URL may have been fetched previously but is back in the frontier for re-fetching)
A DNS resolution module that determines the web server from which to fetch the page specified by a URL.
A parsing module that extracts the text and set of links from a fetched web page.
A duplicate elimination module that determines whether an extracted link is already in the URL frontier or has recently been fetched.
Crawling is performed by anywhere from one to potentially hundreds of threads, each of which loops through the logical cycle in Figure 20.1
These threads may be run in a single process, or be partitioned amongst multiple processes running at different nodes of a distributed system.
We begin by assuming that the URL frontier is in place and non-empty and defer our description of the implementation of the URL frontier to Section 20.2.3
We follow the progress of a single URL through the cycle of being fetched, passing through various checks and filters, then finally (for continuous crawling) being returned to the URL frontier.
A crawler thread begins by taking a URL from the frontier and fetching the web page at that URL, generally using the http protocol.
The fetched page is then written into a temporary store, where a number of operations are performed on it.
Next, the page is parsed and the text as well as the links in it are extracted.
The text (with any tag information – e.g., terms in boldface) is passed on to the indexer.
In addition, each extracted link goes through a series of tests to determine whether the link should be added to the URL frontier.
First, the thread tests whether a web page with the same content has already been seen at another URL.
The simplest implementation for this would use a simple fingerprint such as a checksum (placed in a store labeled "Doc FP’s" in Figure 20.1)
Next, a URL filter is used to determine whether the extracted URL should.
For instance, the crawl may seek to exclude certain domains (say, all .com URLs) – in this case the test would simply filter out the URL if it were from the .com domain.
Many hosts on the Web place certain portions of their websites off-limits to crawling, under a standard known as the Robots Exclusion Protocol.
Here is an example robots.txt file that specifies that no robot should visit any URL whose position in the file hierarchy starts with /yoursite/temp/, except for the robot called “searchengine”
The robots.txt file must be fetched from a website in order to test whether the URL under consideration passes the robot restrictions, and can therefore be added to the URL frontier.
Rather than fetch it afresh for testing on each URL to be added to the frontier, a cache can be used to obtain a recently fetched copy of the file for the host.
This is especially important since many of the links extracted from a page fall within the host from which the page was fetched and therefore can be tested against the host’s robots.txt file.
Thus, by performing the filtering during the link extraction process, we would have especially high locality in the stream of hosts that we need to test for robots.txt files, leading to high cache hit rates.
A URL (particularly one referring to a low-quality or rarely changing document) may be in the frontier for days or even weeks.
If we were to perform the robots filtering before adding such a URL to the frontier, its robots.txt file could have changed by the time the URL is dequeued from the frontier and fetched.
We must consequently perform robots-filtering immediately before attempting to fetch a web page.
As it turns out, maintaining a cache of robots.txt files is still highly effective; there is sufficient locality even in the stream of URLs dequeued from the URL frontier.
Next, a URL should be normalized in the following sense: often the HTMLURL NORMALIZATION encoding of a link from a web page p indicates the target of that link relative to the page p.
Finally, the URL is checked for duplicate elimination: if the URL is already.
When the URL is added to the frontier, it is assigned a priority based on which it is eventually removed from the frontier for fetching.
The details of this priority queuing are in Section 20.2.3
Certain housekeeping tasks are typically performed by a dedicated thread.
In checkpointing, a snapshot of the crawler’s state (say, the URL frontier) is committed to disk.
In the event of a catastrophic crawler failure, the crawl is restarted from the most recent checkpoint.
We have mentioned that the threads in a crawler could run under different processes, each at a different node of a distributed crawling system.
Such distribution is essential for scaling; it can also be of use in a geographically distributed crawler system where each node crawls hosts “near” it.
Partitioning the hosts being crawled amongst the crawler nodes can be done by a hash function, or by some more specifically tailored policy.
For instance, we may locate a crawler node in Europe to focus on European domains, although this is not dependable for several reasons – the routes that packets take through the internet do not always reflect geographic proximity, and in any case the domain of a host does not always reflect its physical location.
How do the various nodes of a distributed crawler communicate and share URLs? The idea is to replicate the flow of Figure 20.1 at each node, with one essential difference: following the URL filter, we use a host splitter to dispatch each surviving URL to the crawler node responsible for the URL; thus the set of hosts being crawled is partitioned among the nodes.
The output of the host splitter goes into the Duplicate URL Eliminator block of each other node in the distributed system.
The “Content Seen?” module in the distributed architecture of Figure 20.2 is, however, complicated by several factors:
There is nothing preventing the same (or highly similar) content from appearing on different web servers.
The result of this locality-mismatch is that most “Content Seen?” tests result in a remote procedure call (although it is possible to batch lookup requests)
Thus, caching popular fingerprints does not help (since there are no popular fingerprints)
During DNS resolution, the program that wishes to perform this translation (in our case, a component of the web crawler) contacts a DNS server that returns the translated IP address.
In practice the entire trans-DNS SERVER lation may not occur at a single DNS server; rather, the DNS server contacted initially may recursively call upon other DNS servers to complete the translation.
Due to the distributed nature of the Domain Name Service, DNS resolution may entail multiple requests and round-trips across the internet, requiring seconds and sometimes even longer.
Right away, this puts in jeopardy our goal of fetching several hundred documents a second.
A standard remedy is to introduce caching: URLs for which we have recently performed DNS lookups are likely to be found in the DNS cache, avoiding the need to go to the DNS servers on the internet.
However, obeying politeness constraints (see Section 20.2.3) limits the of cache hit rate.
There is another important difficulty in DNS resolution; the lookup implementations in standard libraries (likely to be used by anyone developing a crawler) are generally synchronous.
This means that once a request is made to the Domain Name Service, other crawler threads at that node are blocked until the first request is completed.
To circumvent this, most web crawlers implement their own DNS resolver as a component of the crawler.
Thread i executing the resolver code sends a message to the DNS server and then performs a timed wait: it resumes either when being signaled by another thread or when a set time quantum expires.
A single, separate DNS thread listens on the standard DNS port (port 53) for incoming response packets from the name service.
Upon receiving a response, it signals the appropriate crawler thread (in this case, i) and hands it the response packet if i has not yet resumed because its time quantum has expired.
A crawler thread that resumes because its wait time quantum has expired retries for a fixed number of attempts, sending out a new message to the DNS server and performing a timed wait each time; the designers of Mercator recommend of the order of five attempts.
The time quantum of the wait increases exponentially with each of these attempts; Mercator started with one second and ended with roughly 90 seconds, in consideration of the fact that there are host names that take tens of seconds to resolve.
The URL frontier at a node is given a URL by its crawl process (or by the host splitter of another crawl process)
It maintains the URLs in the frontier and regurgitates them in some order whenever a crawler thread seeks a URL.
Two important considerations govern the order in which URLs are returned by the frontier.
First, high-quality pages that change frequently should be prioritized for frequent crawling.
Thus, the priority of a page should be a function of both its change rate and its quality (using some reasonable quality estimate)
The combination is necessary because a large number of spam pages change completely on every fetch.
The second consideration is politeness: we must avoid repeated fetch requests to a host within a short time span.
The likelihood of this is exacerbated because of a form of locality of reference: many URLs link to other URLs at the same host.
As a result, a URL frontier implemented as a simple priority queue might result in a burst of fetch requests to a host.
This might occur even if we were to constrain the crawler so that at most one thread could fetch from any single host at any time.
A common heuristic is to insert a gap between successive fetch requests to a host that is an order of magnitude larger than the time taken for the most recent fetch from that host.
Figure 20.3 shows a polite and prioritizing implementation of a URL frontier.
Its goals are to ensure that (i) only one connection is open at a time to any host; (ii) a waiting time of a few seconds occurs between successive requests to a host and (iii) high-priority pages are crawled preferentially.
The two major sub-modules are a set of F front queues in the upper portion of the figure, and a set of B back queues in the lower part; all of these are FIFO queues.
The front queues implement the prioritization, while the back queues implement politeness.
In the flow of a URL added to the frontier as it makes its way through the front and back queues, a prioritizer first assigns to the URL an integer priority i between 1 and F based on its fetch history (taking into account the rate at which the web page at this URL has changed between previous crawls)
For instance, a document that has exhibited frequent change would be assigned a higher priority.
Now that it has been assigned priority i, the URL is now appended to the ith of the front queues.
Each of the B back queues maintains the following invariants: (i) it is nonempty while the crawl is in progress and (ii) it only contains URLs from a single host1
An auxiliary table T (Figure 20.4) is used to maintain the mapping from hosts to back queues.
Whenever a back-queue is empty and is being re-filled from a front-queue, table T must be updated accordingly.
The number of hosts is assumed to far exceed B.
URLs extracted from already crawled pages flow in at the top of the figure.
A crawl thread requesting a URL extracts it from the bottom of the figure.
En route, a URL flows through one of several front queues that manage its priority for crawling, followed by one of several back queues that manage the crawler’s politeness.
In addition, we maintain a heap with one entry for each back queue, the entry being the earliest time te at which the host corresponding to that queue can be contacted again.
A crawler thread requesting a URL from the frontier extracts the root of this heap and (if necessary) waits until the corresponding time entry te.
It then takes the URL u at the head of the back queue j corresponding to the extracted heap root, and proceeds to fetch the URL u.
After fetching u, the calling thread checks whether j is empty.
If so, it picks a front queue and extracts from its head a URL v.
The choice of front queue is biased (usually by a random process) towards queues of higher priority, ensuring that URLs of high priority flow more quickly into the back queues.
We examine v to check whether there is already a back queue holding URLs from its host.
If so, v is added to that queue and we reach back to the front queues to find another candidate URL for insertion into the now-empty queue j.
In any case, the thread inserts a heap entry for j with a new earliest time te based on the properties of the URL in j that was last fetched (such as when its host was last contacted as well as the time taken for the last fetch), then continues with its processing.
For instance, the new entry te could be the current time plus ten times the last fetch time.
The number of front queues, together with the policy of assigning priorities and picking queues, determines the priority properties we wish to build into the system.
The number of back queues governs the extent to which we can keep all crawl threads busy while respecting politeness.
The designers of Mercator recommend a rough rule of three times as many back queues as crawler threads.
On a Web-scale crawl, the URL frontier may grow to the point where it demands more memory at a node than is available.
The solution is to let most of the URL frontier reside on disk.
A portion of each queue is kept in memory, with more brought in from disk as it is drained in memory.
Exercise 20.1Why is it better to partition hosts (rather than individual URLs) between the nodes of a distributed crawl system?
Why should the host splitter precede the Duplicate URL Eliminator?
In the preceding discussion we encountered two recommended “hard constants” the increment on te being ten times the last fetch time, and the number of back queues being three times the number of crawl threads.
We now consider the distribution of the index across a large computer cluster2 that supports querying.
Two obvious alternative index implementations suggest themselves: parti-TERM PARTITIONING tioning by terms, also known as global index organization, and partitioning byDOCUMENT.
In the former, the dictionary of index terms is partitioned into subsets, each subset residing at a node.
Along with the terms at a node, we keep the postings for those terms.
A query is routed to the nodes corresponding to its query terms.
In principle, this allows greater concurrency since a stream of queries with different query terms would hit different sets of machines.
In practice, partitioning indexes by vocabulary terms turns out to be nontrivial.
Multi-word queries require the sending of long postings lists between sets of nodes for merging, and the cost of this can outweigh the greater concurrency.
Load balancing the partition is governed not by an a priori analysis of relative term frequencies, but rather by the distribution of query terms and their co-occurrences, which can drift with time or exhibit sudden bursts.
Achieving good partitions is a function of the co-occurrences of query terms and entails the clustering of terms to optimize objectives that are not easy to quantify.
Finally, this strategy makes implementation of dynamic indexing more difficult.
A more common implementation is to partition by documents: each node contains the index for a subset of all documents.
Each query is distributed to all nodes, with the results from various nodes being merged before presentation to the user.
This strategy trades more local disk seeks for less inter-node communication.
One difficulty in this approach is that global statistics used in scoring – such as idf – must be computed across the entire document collection even though the index at any single node only contains a subset of the documents.
These are computed by distributed “background” processes that periodically refresh the node indexes with fresh global statistics.
How do we decide the partition of documents to nodes? Based on our development of the crawler architecture in Section 20.2.1, one simple approach would be to assign all pages from a host to a single node.
A danger of such partitioning is that on many queries, a preponderance of the results would come from documents at a small number of hosts (and hence a small number of index nodes)
A hash of each URL into the space of index nodes results in a more uniform distribution of query-time computation across nodes.
At query time, the query is broadcast to each of the nodes, with the top k results from each node being merged to find the top k documents for the query.
A common implementation heuristic is to partition the document collection into indexes of documents that are more likely to score highly on most queries (using, for instance, techniques in Chapter 21) and low-scoring indexes with the remaining documents.
We only search the low-scoring indexes when there are too few matches in the high-scoring indexes, as described in Section 7.2.1
Typical connectivity queries are which URLs link to a given URL? and which URLs does a given URL link to? To this end, we wish to store mappings in memory from URL to out-links, and from URL to in-links.
Applications include crawl control, web graph analysis, sophisticated crawl optimization and link analysis (to be covered in Chapter 21)
Suppose that the Web had four billion pages, each with ten links to other pages.
Some basic properties of the web graph can be exploited to use well under 10% of this memory requirement.
At first sight, we appear to have a data compression problem – which is amenable to a variety of standard solutions.
However, our goal is not to simply compress the web graph to fit into memory; we must do so in a way that efficiently supports connectivity queries; this challenge is reminiscent of index compression (Chapter 5)
We assume that each web page is represented by a unique integer; the specific scheme used to assign these integers is described below.
We build an adjacency table that resembles an inverted index: it has a row for each web page, with the rows ordered by the corresponding integers.
The row for any page p contains a sorted list of integers, each corresponding to a web page that links to p.
This table permits us to respond to queries of the form which pages link to p? In similar fashion we build a table whose entries are the pages linked to by p.
Our description below will focus on the table for the links from each page; it should be clear that the techniques apply just as well to the table of links to each page.
To further reduce the storage for the table, we exploit several ideas:
Similarity between lists: Many rows of the table have many entries in common.
Thus, if we explicitly represent a prototype row for several similar rows, the remainder can be succinctly expressed in terms of the prototypical row.
Locality: many links from a page go to “nearby” pages – pages on the same host, for instance.
This suggests that in encoding the destination of a link, we can often use small integers and thereby save space.
We use gap encodings in sorted lists: rather than store the destination of each link, we store the offset from the previous entry in the row.
In a lexicographic ordering of all URLs, we treat each URL as an alphanumeric string and sort these strings.
To each URL, we assign its position in this ordering as the unique identifying integer.
Figure 20.6 shows an example of such a numbering and the resulting table.
We next exploit a property that stems from the way most websites are structured to get similarity and locality.
Figure 20.6 A four-row segment of the table of links.
In this case, the rows corresponding to pages in a website will have many table entries in common.
Moreover, under the lexicographic ordering of URLs, it is very likely that the pages from a website appear as contiguous rows in the table.
We adopt the following strategy: we walk down the table, encoding each table row in terms of the seven preceding rows.
The use of only the seven preceding rows has two advantages: (i) the offset can be expressed with only 3 bits; this choice is optimized empirically (the reason for seven and not eight preceding rows is the subject of Exercise 20.4) and (ii) fixing the maximum offset to a small value like seven avoids having to perform an expensive search among many candidate prototypes in terms of which to express the current row.
What if none of the preceding seven rows is a good prototype for expressing the current row? This would happen, for instance, at each boundary between different websites as we walk down the rows of the table.
In this case we simply express the row as starting from the empty set and “adding in” each integer in that row.
By using gap encodings to store the gaps (rather than the actual integers) in each row, and encoding these gaps tightly based on the distribution of their values, we obtain further space reduction.
While these ideas give us a representation of sizable web graphs that comfortably fit in memory, we still need to support connectivity queries.
What is entailed in retrieving from this representation the set of links from a page? First, we need an index lookup from (a hash of) the URL to its row number in the table.
Next, we need to reconstruct these entries, which may be encoded in terms of entries in other rows.
This entails following the offsets to reconstruct these other rows – a process that in principle could lead through many levels of indirection.
A heuristic for controlling this can be introduced into the construcOnline edition (c)
If the threshold is set too high, we seldom use prototypes and express many rows afresh.
If the threshold is too low, most rows get expressed in terms of prototypes, so that at query time the reconstruction of a row leads to many levels of indirection through preceding prototypes.
Exercise 20.4We noted that expressing a row in terms of one of seven preceding rows allowed us to use no more than three bits to specify which of the preceding rows we are using as prototype.
We noted that for the scheme in Section 20.4, decoding the links incident on a URL could result in many levels of indirection.
Construct an example in which the number of levels of indirection grows linearly with the number of URLs.
Document partitioning is found to be superior, at least when the distribution of terms is skewed, as it typically is in practice.
But the outcome depends on the details of the distributed system;
Sornil (2001) argues for a partitioning scheme that is a hybrid between term and document partitioning.
The first implementation of a connectivity server was described by Bharat et al.
The analysis of hyperlinks and the graph structure of the Web has been instrumental in the development of web search.
In this chapter we focus on the use of hyperlinks for ranking web search results.
Such link analysis is one of many factors considered by web search engines in computing a composite score for a web page on any given query.
We begin by reviewing some basics of the Web as a graph in Section 21.1, then proceed to the technical development of the elements of link analysis for ranking.
Link analysis for web search has intellectual antecedents in the field of citation analysis, aspects of which overlap with an area known as bibliometrics.
These disciplines seek to quantify the influence of scholarly articles by analyzing the pattern of citations amongst them.
Much as citations represent the conferral of authority from a scholarly article to others, link analysis on the Web treats hyperlinks from a web page to another as a conferral of authority.
Clearly, not every citation or hyperlink implies such authority conferral; for this reason, simply measuring the quality of a web page by the number of in-links (citations from other pages) is not robust enough.
For instance, one may contrive to set up multiple web pages pointing to a target web page, with the intent of artificially boosting the latter’s tally of in-links.
Nevertheless, the phenomenon of citation is prevalent and dependable enough that it is feasible for web search engines to derive useful signals for ranking from more sophisticated link analysis.
Section 21.1 develops the basic ideas underlying the use of the web graph in link analysis.
The anchor text pointing to page B is a good description of page B.
The hyperlink from A to B represents an endorsement of page B, by the creator of page A.
This is not always the case; for instance, many links amongst pages within a single website stem from the user of a common template.
For instance, most corporate websites have a pointer from every page to a page containing a copyright notice – this is clearly not an endorsement.
Accordingly, implementations of link analysis algorithms will typical discount such “internal” links.
The following fragment of HTML code from a web page shows a hyperlink pointing to the home page of the Journal of the ACM:
In this case, the link points to the page http://www.acm.org/jacm/ and the anchor text is Journal of the ACM.
Clearly, in this example the anchor is descriptive of the target page.
But then the target page (B = http://www.acm.org/jacm/) itself contains the same description as well as considerable additional information on the journal.
The Web is full of instances where the page B does not provide an accurate description of itself.
In many cases this is a matter of how the publishers of page B choose to present themselves; this is especially common with corporate web pages, where a web presence is a marketing statement.
For example, at the time of the writing of this book the home page of the IBM corporation (http://www.ibm.com) did not contain the term computer anywhere in its HTML code, despite the fact that IBM is widely viewed as the world’s largest computer maker.
Similarly, the HTML code for the home page of Yahoo! (http://www.yahoo.com) does not at this time contain the word portal.
Thus, there is often a gap between the terms in a web page, and how web users would describe that web page.
Consequently, web searchers need not use the terms in a page to query for it.
In addition, many web pages are rich in graphics and images, and/or embed their text in these images; in such cases, the HTML parsing performed when crawling will not extract text that is useful for indexing these pages.
The fact that the anchors of many hyperlinks pointing to http://www.ibm.com include the word computer can be exploited by web search engines.
For instance, the anchor text terms can be included as terms under which to index the target web page.
Thus, the postings for the term computer would include the document http://www.ibm.com and that for the term portal would include the document http://www.yahoo.com, using a special indicator to show that these terms occur as anchor (rather than in-page) text.
As with in-page terms, anchor text terms are generally weighted based on frequency, with a penalty for terms that occur very often (the most common terms in anchor text across the Web are Click and here, using methods very similar to idf)
The actual weighting of terms is determined by machine-learned scoring, as in Section 15.4.1; current web search engines appear to assign a substantial weighting to anchor text terms.
Searching for big blue on most web search engines returns the home page of the IBM corporation as the top hit; this is consistent with the popular nickname that many people use to refer to IBM.
On the other hand, there have been (and continue to be) many instances where derogatory anchor text such as evil empire leads to somewhat unexpected results on querying for these terms on web search engines.
This phenomenon has been exploited in orchestrated campaigns against specific sites.
Such orchestrated anchor text may be a form of spamming, since a website can create misleading anchor text pointing to itself, to boost its ranking on selected query terms.
Detecting and combating such systematic abuse of anchor text is another form of spam detection that web search engines perform.
The window of text surrounding anchor text (sometimes referred to as extended anchor text) is often usable in the same manner as anchor text itself; consider for instance the fragment of web textthere is good discussion of vedic scripture <a>here</a>
This has been considered in a number of settings and the useful width of this window has been studied; see Section 21.4 for references.
Exercise 21.1Is it always possible to follow directed edges (hyperlinks) in the web graph from any node (web page) to any other? Why or why not?
Given the collection of anchor-text phrases for a web page x, suggest a heuristic for choosing one term or phrase from this collection that is most descriptive of x.
Does your heuristic in the previous exercise take into account a single domain D repeating anchor text for x from multiple pages in D?
We now focus on scoring and ranking measures derived from the link structure alone.
This composite score, developed using the methods of Section 15.4.1, is used to provide a ranked list of results for the query.
Consider a random surfer who begins at a web page (a node of the web graph) and executes a random walk on the Web as follows.
At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to.
As the surfer proceeds in this random walk from node to node, he visits some nodes more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes.
The idea behind PageRank is that pages visited more often in this walk are more important.
What if the current location of the surfer, the node A, has no out-links? To address this we introduce an additional operation for our random surfer: the teleport operation.
In the teleport operation the surfer jumps from a nodeTELEPORT to any other node in the web graph.
The destination of a teleport operation is modeled as being chosen uniformly at random from all web pages.
The surfer would also teleport to his present position with probability 1/N.
A Markov chain is a discrete-time stochastic process: a process that occurs in a series of time-steps in each of which a random choice is made.
Each web page will correspond to a state in the Markov chain we will formulate.
A matrix with non-negative entries that satisfies Equation (21.1) is known as a stochastic matrix.
A key property of a stochastic matrix is that it has aSTOCHASTIC MATRIX principal left eigenvector corresponding to its largest eigenvalue, which is 1.PRINCIPAL LEFT.
This is consistent with our usage of N for the number of documents in the collection.
Figure 21.2 A simple Markov chain with three states; the numbers on the links indicate the transition probabilities.
An N-dimensional probability vector each of whose components corresponds to one of the N states of a Markov chain can be viewed as a probability distribution over its states.
We can depict the probability distribution of the surfer’s position at any time by a probability vector ~x.
We can thus compute the surfer’s distribution over the states at any time, given only the initial distribution and the transition probability matrix P.
If a Markov chain is allowed to run for many time steps, each state is visited at a (different) frequency that depends on the structure of the Markov chain.
In our running analogy, the surfer visits certain web pages (say, popular news home pages) more often than other pages.
We now make this intuition precise, establishing conditions under which such the visit frequency converges to fixed, steady-state quantity.
Following this, we set the PageRank of each node v to this steady-state visit frequency and show how it can be computed.
Definition: A Markov chain is said to be ergodic if there exists a positiveERGODIC MARKOV CHAIN integer T0 such that for all pairs of states i, j in the Markov chain, if it is.
For a Markov chain to be ergodic, two technical conditions are required of its states and the non-zero transition probabilities; these conditions are known as irreducibility and aperiodicity.
Informally, the first ensures that there is a sequence of transitions of non-zero probability from any state to any other, while the latter ensures that the states are not partitioned into sets such that all state transitions occur cyclically from one set to another.
It follows from Theorem 21.1 that the random walk with teleporting results in a unique distribution of steady-state probabilities over the states of the induced Markov chain.
This steady-state probability for a state is the PageRank of the corresponding web page.
There are many algorithms available for computing left eigenvectors; the references at the end of Chapter 18 and the present chapter are a guide to these.
We give here a rather elementary method, sometimes known as power iteration.
As t grows large, we would expect that the distribution ~xPt2
The power iteration method simulates the surfer’s walk: begin at a state and run the walk for a large number of steps t, keeping track of the visit frequencies for each of the states.
After a large number of steps t, these frequencies “settle down” so that the variation in the computed frequencies is below some predetermined threshold.
We declare these tabulated frequencies to be the PageRank values.
Note that Pt represents P raised to the tth power, not the transpose of P which is denoted PT.
The PageRank values of pages (and the implicit ordering amongst them) are independent of any query a user might pose; PageRank is thus a queryindependent measure of the static quality of each web page (recall such static quality measures from Section 7.1.4)
On the other hand, the relative ordering of pages should, intuitively, depend on the query being served.
For this reason, search engines use static quality measures such as PageRank as just one of many factors in scoring a web page on a query.
Indeed, the relative contribution of PageRank to the overall score may again be determined by machine-learned scoring as in Section 15.4.1
Arcs are annotated with the word that occurs in the anchor text of the corresponding link.
Of these, q2 has the lowest PageRank since the random walk tends to drift out of the top part of the graph – the walker can only return there through teleportation.
Thus far we have discussed the PageRank computation with a teleport operation in which the surfer jumps to a random web page chosen uniformly at random.
We now consider teleporting to a random web page chosen nonuniformly.
In doing so, we are able to derive PageRank values tailored to particular interests.
For instance, a sports aficionado might wish that pages on sports be ranked higher than non-sports pages.
Suppose that web pages on sports are “near” one another in the web graph.
Then, a random surfer who frequently finds himself on random sports pages is likely (in the course of the random walk) to spend most of his time at sports pages, so that the steady-state distribution of sports pages is boosted.
Suppose our random surfer, endowed with a teleport operation as before, teleports to a random web page on the topic of sports instead of teleporting to a uniformly chosen random web page.
We will not focus on how we collect all web pages on the topic of sports; in fact, we only need a non-zero subset S of sports-related web pages, so that the teleport operation is feasible.
This may be obtained, for instance, from a manually built directory of sports pages such as the open directory project (http://www.dmoz.org/) or that of Yahoo.
In like manner we can envision topic-specific PageRank distributions for each of several topics such as science, religion, politics and so on.
For a user interested in only a single topic from among these topics, we may invoke the corresponding PageRank distribution when scoring and ranking search results.
This gives us the potential of considering settings in which the search engine knows what topic a user is interested in.
This may happen because users either explicitly register their interests, or because the system learns by observing each user’s behavior over time.
But what if a user is known to have a mixture of interests from multiple topics? For instance, a user may have an interest mixture (or profile) that is.
A user with this mixture of interests could teleport as follows: determine first whether to teleport to the set S of known sports pages, or to the set of known politics pages.
Once we choose that a particular teleport step is to (say) a random sports page, we choose a web page in S uniformly at random to teleport to.
This in turn leads to an ergodic Markov chain with a steady-state distribution that is personalized to this user’s preferences over topics (see Exercise 21.16)
While this idea has intuitive appeal, its implementation appears cumbersome: it seems to demand that for each user, we compute a transition probOnline edition (c)
We are rescued by the fact that the evolution of the probability distribution over the states of a Markov chain can be viewed as a linear system.
In Exercise 21.16 we will show that it is not necessary to compute a PageRank vector for every distinct combination of user interests over topics; the personalized PageRank vector for any user can be expressed as a linear combination of the underlying topicspecific PageRanks.
A user of a browser can, in addition to clicking a hyperlink on the page x he is currently browsing, use the back button to go back to the page from which he arrived at x.
Can such a user of back buttons be modeled as a Markov chain? How would we model repeated invocations of the back button?
Show that for any directed graph, the Markov chain induced by a random walk with the teleport operation is ergodic.
Suppose that the web graph is stored on disk as an adjacency list, in such a way that you may only query for the out-neighbors of pages in the order in which they are stored.
You cannot load the graph in main memory but you may do multiple reads over the full graph.
Write the algorithm for computing the PageRank in this setting.
Recall the sets S and Y introduced near the beginning of Section 21.2.3
Is the set Y always the set of all web pages? Why or why not?
Is the sports PageRank of any page in S at least as large as its PageRank?
Show that the Markov chain corresponding to the walk in Exercise 21.16 is ergodic and hence the user’s personalized PageRank can be obtained by computing the steadystate distribution of this Markov chain.
We now develop a scheme in which, given a query, every web page is assigned two scores.
One is called its hub score and the other its authority score.HUB SCORE.
The ranking of one list is induced by the hub scores and that of the other by the authority scores.
This approach stems from a particular insight into the creation of web pages, that there are two primary kinds of web pages useful as results for broad-topic searches.
By a broad topic search we mean an informational query such as "I wish to learn about leukemia"
There are authoritative sources of information on the topic; in this case, the National Cancer Institute’s page on.
We will call such pages authorities; in the computation we are about to describe, they are the pages that will emerge with high authority scores.
On the other hand, there are many pages on the Web that are hand-compiled lists of links to authoritative web pages on a specific topic.
These hub pages are not in themselves authoritative sources of topic-specific information, but rather compilations that someone with an interest in the topic has spent time putting together.
The approach we will take, then, is to use these hub pages to discover the authority pages.
In the computation we now develop, these hub pages are the pages that will emerge with high hub scores.
A good hub page is one that points to many good authorities; a good authority page is one that is pointed to by many good hub pages.
We thus appear to have a circular definition of hubs and authorities; we will turn this into an iterative computation.
Suppose that we have a subset of the web containing good hub and authority pages, together with the hyperlinks amongst them.
We will iteratively compute a hub score and an authority score for every web page in this subset, deferring the discussion of how we pick this subset until Section 21.3.1
Thus, the first line of Equation (21.8) sets the hub score of page v to the sum of the authority scores of the pages it links to.
In other words, if v links to pages with high authority scores, its hub score increases.
The second line plays the reverse role; if page v is linked to by good hubs, its authority score increases.
What happens as we perform these updates iteratively, recomputing hub scores, then new authority scores based on the recomputed hub scores, and so on? Let us recast the equations Equation (21.8) into matrix-vector form.
Let A denote the adjacency matrix of the subset of the web graph that we are dealing with: A is a square matrix with one row and one column for each page in the subset.
Substituting these into one another, we may rewrite Equation (21.9) as.
In computing these eigenvector entries, we are not restricted to using the power iteration method; indeed, we could use any fast method for computing the principal eigenvector of a stochastic matrix.
Assemble the target subset of web pages, form the graph induced by their hyperlinks and compute AAT and ATA.
This method of link analysis is known as HITS, which is an acronym forHITS Hyperlink-Induced Topic Search.
Since the iterative updates captured the intuition of good hubs and good authorities, the high-scoring pages we output would give us good hubs and authorities from the target subset of web pages.
In Section 21.3.1 we describe the remaining detail: how do we gather a target subset of web pages around a topic such as leukemia?
In assembling a subset of web pages around a topic such as leukemia, we must cope with the fact that good authority pages may not contain the specific query term leukemia.
This is especially true, as we noted in Section 21.1.1, when an authority page uses its web presence to project a certain marketing image.
For instance, many pages on the IBM website are authoritative sources of information on computer hardware, even though these pages may not contain the term computer or hardware.
However, a hub compiling computer hardware resources is likely to use these terms and also link to the relevant pages on the IBM website.
Building on these observations, the following procedure has been suggested for compiling the subset of the Web for which to compute hub and authority scores.
Given a query (say leukemia), use a text index to get all pages containing leukemia.
Build the base set of pages, to include the root set as well as any page that either links to a page in the root set, or is linked to by a page in the root set.
We then use the base set for computing hub and authority scores.
The base set is constructed in this manner for three reasons:
A good authority page may not contain the query text (such as computer hardware)
If the text query manages to capture a good hub page vh in the root set, then the inclusion of all pages linked to by any page in the root set will capture all the good authorities linked to by vh in the base set.
Conversely, if the text query manages to capture a good authority page va in the root set, then the inclusion of pages which point to va will bring other good hubs into the base set.
In other words, the “expansion” of the root set into the base set enriches the common pool of good hubs and authorities.
Running HITS across a variety of queries reveals some interesting insights about link analysis.
Frequently, the documents that emerge as top hubs and authorities include languages other than the language of the query.
These pages were presumably drawn into the base set, following the assembly of the root set.
Thus, some elements of cross-language retrieval (where a query in one language retrieves documents in another) are evident here; interestingly, this cross-language effect resulted purely from link analysis, with no linguistic translation taking place.
We conclude this section with some notes on implementing this algorithm.
Any algorithm for computing eigenvectors may be used for computing the hub/authority score vector.
In fact, we need not compute the exact values of these scores; it suffices to know the relative values of the scores so that we may identify the top hubs and authorities.
To this end, it is possible that a small number of iterations of the power iteration method yields the relative ordering of the top hubs and authorities.
Experiments have suggested that in practice, about five iterations of Equation (21.8) yield fairly good results.
Moreover, since the link structure of the web graph is fairly sparse (the average web page links to about ten others), we do not perform these as matrix-vector products but rather as additive updates as in Equation (21.8)
Figure 21.6 A sample run of HITS on the query japan elementary schools.
Figure 21.6 shows the results of running HITS on the query japan elementary schools.
The figure shows the top hubs and authorities; each row lists the title tag from the corresponding HTML page.
Because the resulting string is not necessarily in Latin characters, the resulting print is (in many cases) a string of gibberish.
Each of these corresponds to a web page that does not use Latin characters, in this case very likely pages in Japanese.
There also appear to be pages in other non-English languages, which seems surprising given that the query string is in English.
In fact, this result is emblematic of the functioning of HITS – following the assembly of the root set, the (English) query string is ignored.
The base set is likely to contain pages in other languages, for instance if an English-language hub page links to the Japanese-language home pages of Japanese elementary schools.
Because the subsequent computation of the top hubs and authorities is entirely linkbased, some of these non-English pages will appear among the top hubs and authorities.
How would you interpret the entries of the matrices AAT and ATA? What is the connection to the co-occurrence matrix CCT in Chapter 18?
For the web graph in Figure 21.7, compute PageRank, hub and authority scores for each of the three pages.
Also give the relative ordering of the 3 nodes for each of these scores, indicating any ties.
PageRank: Assume that at each step of the PageRank random walk, we teleport to a random page with probability 0.1, with a uniform distribution over which particular page we teleport to.
Hint 1: Using symmetries to simplify and solving with linear equations might be easier than using iterative methods.
Hint 2: Provide the relative ordering (indicating any ties) of the three nodes for each of the three scoring measures.
Garfield (1955) is seminal in the science of citation analysis.
This was built on by Pinski and Narin (1976) to develop a journal influence weight, whose definition is remarkably similar to that of the PageRank measure.
The use of anchor text as an aid to searching and ranking stems from the work of McBryan (1994)
Extended anchor-text was implicit in his work, with systematic experiments reported in Chakrabarti et al.
Kemeny and Snell (1976) is a classic text on Markov chains.
The PageRank measure was developed in Brin and Page (1998) and in Page et al.
However, it has also been noted that the teleport operation contributes significantly to PageRank’s robustness in this sense.
Both PageRank and HITS can be “spammed” by the orchestrated insertion of links into the web graph; indeed, the Web is known to have such link farms that col-LINK FARMS lude to increase the score assigned to certain pages by various link analysis algorithms.
Bharat and Henzinger (1998) further developed these and other heuristics, showing that certain combinations outperformed the basic HITS algorithm.
Numerous other variants of HITS have been developed by a number of authors, the best know of which is perhaps SALSA (Lempel and Moran 2000)
We use the following abbreviated journal and conference names in the bibliography:
Reducing multiclass to binary: A unifying approach for margin classifiers.
GIO: A semantic web application using the information grid framework.
Data analysis in the social sciences: What about the details? In Proc.
Scaling to very very large corpora for natural language disambiguation.
How do search engines respond to some non-English queries? Journal of Information Science 31(1):13–28
Optimizing ranking functions: A connectionist approach to adaptive information retrieval.
Language identifier: A computer program for automatic natural-language identification of on-line text.
A technique for measuring the relative size and overlap of public web search engines.
The connectivity server: Fast access to linkage information on the web.
A comparison of techniques to find mirrored hosts on the WWW.
An evaluation of retrieval effectiveness for a full-text document-retrieval system.
Finding authorities and hubs from link structures on the World Wide Web.
A study of methods for systematically abbreviating English words and names.
The effect of adding relevance information in a relevance feedback environment.
The use of MMR, diversity-based reranking for reordering documents and producing summaries.
Automatic resource list compilation by analyzing hyperlink structure and associated text.
Probabilistic information retrieval approach for ranking of database query results.
Semantic search via XML fragments: A highprecision approach to IR.
Integration of heterogeneous databases without common domains using queries based on textual similarity.
Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression.
Spelling correction as an iterative process that exploits the collective knowledge of web users.
A technique for computer detection and correction of spelling errors.
On the optimality of the simple Bayesian classifier under zero-one loss.
The approximation of a matrix by another of lower rank.
A pitfall and solution in multi-class feature selection for text classification.
How many clusters? Which clustering method? Answers via model-based cluster analysis.
Optimum polynomial retrieval functions based on the probability ranking principle.
Probabilistic information retrieval as a combination of abstraction, inductive learning, and probabilistic assumptions.
Chinese word segmentation and named entity recognition: A pragmatic approach.
Citation indexes to science: A new dimension in documentation through association of ideas.
Estimating linguistic diversity on the internet: A taxonomy to avoid pitfalls and paradoxes.
Inferring probability of relevance using the method of logistic regression.
A theory of term weighting based on exploratory data analysis.
Retrieving records from a gigabyte of text on a minicomputer using statistical ranking.
Variations in relevance assessments and the measurement of retrieval effectiveness.
An investigation of linguistic features and clustering algorithms for topical document clustering.
CONSTRUE/TIS: A system for content-based indexing of a database of news stories.
Burst tries: A fast, efficient data structure for string keys.
OHSUMED: An interactive retrieval evaluation and new large test collection for research.
Do batch and user evaluation give the same results? In Proc.
Challenging conventional assumptions of automated information retrieval with real users: Boolean searching and batch retrieval evaluations.
Further analysis of whether batch and user evaluations give the same results with a question-answering task.
A probabilistic justification for using tf.idf term weighting in information retrieval.
A probabilistic analysis of the Rocchio algorithm with tfidf for text categorization.
Text categorization with support vector machines: Learning with many relevant features.
Interpreting and extending classical agglomerative clustering algorithms using a model-based approach.
A spelling correction program based on a noisy channel model.
Document language models, query models, and risk minimization for information retrieval.
The stochastic approach for link-structure analysis (SALSA) and the TKC effect.
Binary codes capable of correcting spurious insertions and deletions of ones.
Naive (Bayes) at forty: The independence assumption in information retrieval.
A loss function analysis for classification methods in text categorization.
Content analysis in mass communication: Assessment and reporting of intercoder reliability.
Optimized query execution in large search engines with global page ordering.
A statistical approach to mechanized encoding and searching of literary information.
A comparison of Chinese document indexing strategies and retrieval models.
A comparison of event models for Naive Bayes text classification.
Improving text classification by shrinkage in a hierarchy of classes.
Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering.
Tracking and summarizing news on a daily basis with Columbia’s Newsblaster.
A study on optimal parameter tuning for Rocchio text classifier.
Topic modeling for mediated access to very large document collections.
Proximal nodes: A model to query document databases by content and structure.
Parameter estimation for a simple hierarchical generative model for XML retrieval.
Document preprocessing for naive Bayes classification and clustering with mixture of multinomials.
X-means: Extending k-means with efficient estimation of the number of clusters.
Grafting: Fast, incremental feature selection by gradient descent in function space.
Citation influence for journal aggregates of scientific publications: Theory, with application to the literature of Physics.
Interactive, domain-independent identification and summarization of topically related news articles.
Methods for sampling pages uniformly from the world wide web.
A survey on the use of relevance feedback for information access systems.
On the reliability of information retrieval metrics based on graded relevance.
A comparative analysis on the bisecting K-means and the PDDP clustering algorithms.
Analysis of a very large web search engine query log.
Assigning document identifiers to enhance compressibility of web search engines indexes.
Content-based image retrieval at the end of the early years.
Use of query reformulation and relevance feedback by Excite users.
Historical note: Information retrieval and the future of an illusion.
TopX: Efficient and versatile top-k query processing for semistructured data.
Estimating the number of clusters in a data set via the gap statistic.
Data clustering by Markovian relaxation and the information bottleneck method.
Query processing and inverted indices in shared-nothing document information retrieval systems.
Support vector machine active learning with applications to text classification.
An experimental study on automatically labeling hierarchical clusters using statistical features.
Why batch and user evaluations do not give the same results.
Bricks: The building blocks to tackle query formulation in structured document retrieval.
The effectiveness and efficiency of agglomerative hierarchic clustering in document retrieval.
Variations in relevance judgments and the measurement of retrieval effectiveness.
Expert network: Effective and efficient learning from human decisions in text categorization and retrieval.
Bayesian extension to the language model for ad hoc information retrieval.
Model-based feedback in the language modeling approach to information retrieval.
A study of smoothing methods for language models applied to ad hoc information retrieval.
How reliable are the results of large-scale information retrieval experiments? In Proc.
List of Tables List of Figures Table of Notation Preface Boolean retrieval An example information retrieval problem A first take at building an inverted index Processing Boolean queries The extended Boolean model versus ranked retrieval References and further reading.
The term vocabulary and postings lists Document delineation and character sequence decoding Obtaining the character sequence in a document Choosing a document unit.
Determining the vocabulary of terms Tokenization Dropping common terms: stop words Normalization (equivalence classing of terms) Stemming and lemmatization.
Faster postings list intersection via skip pointers Positional postings and phrase queries Biword indexes Positional indexes Combination schemes.
Dictionaries and tolerant retrieval Search structures for dictionaries Wildcard queries General wildcard queries k-gram indexes for wildcard queries.
Spelling correction Implementing spelling correction Forms of spelling correction Edit distance k-gram indexes for spelling correction Context sensitive spelling correction.
Index construction Hardware basics Blocked sort-based indexing Single-pass in-memory indexing Distributed indexing Dynamic indexing Other types of indexes References and further reading.
Index compression Statistical properties of terms in information retrieval Heaps' law: Estimating the number of terms Zipf's law: Modeling the distribution of terms.
Scoring, term weighting and the vector space model Parametric and zone indexes Weighted zone scoring Learning weights The optimal weight g.
The vector space model for scoring Dot products Queries as vectors Computing vector scores.
Variant tf-idf functions Sublinear tf scaling Maximum tf normalization Document and query weighting schemes Pivoted normalized document length.
Computing scores in a complete search system Efficient scoring and ranking Inexact top K document retrieval Index elimination Champion lists Static quality scores and ordering Impact ordering Cluster pruning.
Components of an information retrieval system Tiered indexes Query-term proximity Designing parsing and scoring functions Putting it all together.
Vector space scoring and query operator interaction References and further reading.
Evaluation in information retrieval Information retrieval system evaluation Standard test collections Evaluation of unranked retrieval sets Evaluation of ranked retrieval results Assessing relevance Critiques and justifications of the concept of relevance.
A broader perspective: System quality and user utility System issues User utility Refining a deployed system.
Relevance feedback and query expansion Relevance feedback and pseudo relevance feedback The Rocchio algorithm for relevance feedback Probabilistic relevance feedback When does relevance feedback work? Relevance feedback on the web Evaluation of relevance feedback strategies Pseudo relevance feedback Indirect relevance feedback Summary.
Global methods for query reformulation Vocabulary tools for query reformulation Query expansion Automatic thesaurus generation.
The Binary Independence Model Deriving a ranking function for query terms Probability estimates in theory Probability estimates in practice Probabilistic approaches to relevance feedback.
An appraisal and some extensions An appraisal of probabilistic models Tree-structured dependencies between terms Okapi BM25: a non-binary model Bayesian network approaches to IR.
Language models for information retrieval Language models Finite automata and language models Types of language models Multinomial distributions over words.
The query likelihood model Using query likelihood language models in IR Estimating the query generation probability Ponte and Croft's Experiments.
Language modeling versus other approaches in IR Extended language modeling approaches References and further reading.
Text classification and Naive Bayes The text classification problem Naive Bayes text classification Relation to multinomial unigram language model.
Feature selection Mutual information Chi2 Feature selection Frequency-based feature selection Feature selection for multiple classifiers Comparison of feature selection methods.
Vector space classification Document representations and measures of relatedness in vector spaces Rocchio classification k nearest neighbor Time complexity and optimality of kNN.
Linear versus nonlinear classifiers Classification with more than two classes The bias-variance tradeoff References and further reading Exercises.
Support vector machines and machine learning on documents Support vector machines: The linearly separable case Extensions to the SVM model Soft margin classification Multiclass SVMs Nonlinear SVMs Experimental results.
Issues in the classification of text documents Choosing what kind of classifier to use Improving classifier performance.
Machine learning methods in ad hoc information retrieval A simple example of machine-learned scoring Result ranking by machine learning.
Flat clustering Clustering in information retrieval Problem statement Cardinality -- the number of clusters.
Hierarchical clustering Hierarchical agglomerative clustering Single-link and complete-link clustering Time complexity of HAC.
Matrix decompositions and latent semantic indexing Linear algebra review Matrix decompositions.
Term-document matrices and singular value decompositions Low-rank approximations Latent semantic indexing References and further reading.
Advertising as the economic model The search user experience User query needs.
Index size and estimation Near-duplicates and shingling References and further reading.
Web crawling and indexes Overview Features a crawler must provide Features a crawler should provide.
Link analysis The Web as a graph Anchor text and the web graph.
