Enhance your search with faceted navigation, result highlighting, relevancy ranked sorting, and more.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author(s), nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Born to code, David Smiley is a senior software engineer with a passion for programming and open source.
He has written a book, taught a class, and presented at conferences on the subject of Solr.
He has 12 years of experience in the defense industry at MITRE, using Java and various web technologies.
Recently, David has been focusing his attention on the intersection of geospatial technologies with Lucene and Solr.
David first used Lucene in 2000 and was immediately struck by its speed and novelty.
Years later he had the opportunity to work with Compass, a Lucene based library.
In 2008, David built an enterprise people and project search service with Solr, with a focus on search relevancy tuning.
He has since developed and taught a two-day Solr course for MITRE and he regularly offers technical advice to MITRE and its customers on the use of Solr.
David also has experience using Endeca's competing product, which has broadened his experience in the search field.
On a technical level, David has solved challenging problems with Lucene and Solr including geospatial search, wildcard ngram query parsing, searching multiple multi-valued fields at coordinated positions, and part-of-speech search using Lucene payloads.
In the area of geospatial search, David open sourced his geohash prefix/ grid based work to the Solr community tracked as SOLR-2155
Presently, David is collaborating with other Lucene and Solr committers on geospatial search.
Most, if not all authors seem to dedicate their book to someone.
As simply a reader of books I have thought of this seeming prerequisite as customary tradition.
That was my feeling before I embarked on writing about Solr, a project that has sapped my previously "free" time on nights and weekends for a year.
I chose this sacrifice and want no pity for what was my decision, but my wife, family and friends did not choose it.
I am married to my lovely wife Sylvie who has easily sacrificed as much as I have to work on this project.
She has suffered through the first edition with an absentee husband while bearing our first child—Camille.
The second edition was a similar circumstance with the birth of my second daughter—Adeline.
I officially dedicate this book to my wife Sylvie and my daughters Camille and Adeline, who I both lovingly adore.
I also pledge to read book dedications with new-found firsthand experience at what the dedication represents.
I would also like to thank others who helped bring this book to fruition.
Namely, if it were not for Doug Cutting creating Lucene with an open source license, there would be no Solr.
Furthermore, CNET's decision to open source what was an in-house project, Solr itself, in 2006, deserves praise.
Many corporations do not understand that open source isn't just "free code" you get for free that others write: it is an opportunity to let your code flourish in the outside instead of it withering inside.
Last, but not the least, this book would not have been completed in a reasonable time were it not for the assistance of my contributing author, Eric Pugh.
His own perspectives and experiences have complemented mine so well that I am absolutely certain the quality of this book is much better than what I could have done alone.
Eric Pugh has been fascinated by the "craft" of software development, and has been heavily involved in the open source world as a developer, committer, and user for the past five years.
He is an emeritus member of the Apache Software Foundation and lately has been mulling over how we solve the problem of finding answers in datasets when we don't know the questions ahead of time to ask.
In biotech, financial services, and defense IT, he has helped European and American companies develop coherent strategies for embracing open source search software.
As a speaker, he has advocated the advantages of Agile practices with a focus on testing in search engine implementation.
Eric became involved with Solr when he submitted the patch SOLR-284 for Parsing Rich Document types such as PDF and MS Office formats that became the single most popular patch as measured by votes! The patch was subsequently cleaned up and enhanced by three other individuals, demonstrating the power of the open source model to build great code collaboratively.
When the topic of producing an update of this book for Solr 3 first came up, I thought it would be a matter of weeks to complete it.
However, when David Smiley and I sat down to scope out what to change about the book, it was immediately apparent that we didn't want to just write an update for the latest Solr, we wanted to write a complete second edition of the book.
We added a chapter, moved around content, rewrote whole sections of the book.
David put in many more long nights than I over the past 9 months writing what I feel justifiable in calling the Second Edition of our book.
So I must thank his wife Sylvie for being so supportive of him!
I also want to thank again Erik Hatcher for his continuing support and mentorship.
Without his encouragement I wouldn't have spoken at Euro Lucene, or become involved in the Blacklight community.
I also want to thank all of my colleagues at OpenSource Connections.
My darling wife Kate, I know 2011 turned into a very busy year, but I couldn't be happier sharing my life with you, Morgan, and baby Asher.
Lastly I want to thank all the adopters of Solr and Lucene! Without you, I wouldn't have this wonderful open source project to be so incredibly proud to be a part of! I look forward to meeting more of you at the next LuceneRevolution or Euro Lucene conference.
After starting his career in the field of bioinformatics where he worked as a Biological Data Management and Analysis Consultant, he's now a Senior Application Developer with interests ranging from architecture to delivering a great user experience online.
He's passionate about open source technologies, search engines, and web application architecture.
He now works for WCN Plc, a leading provider of recruitment software solutions.
He's worked in dot-coms on almost everything related to web application development, from architecture to user experience.
He's very active in the open source community, having contributed to several projects and started many projects of his own.
In 2007 he wrote SolrNet, a popular open source Solr interface for the .NET platform.
Currently he's also researching the application of functional programming to web development as part of his Master's thesis.
Discounts Have you bought the print copy or Kindle version of this book? If so, you can get a massive 85% off the price of the eBook version, available in PDF, ePub, and MOBI.
Simply go to http://www.packtpub.com/apache-solr-3-enterprise-searchserver/book, add it to your cart, and enter the following discount code:
Newsletters Sign up for Packt's newsletters, which will keep you up to date with offers, discounts, books, and downloads.
Code Downloads, Errata and Support Packt supports all of its books with errata.
While we work hard to eradicate errors from our books, some do creep in.
Meanwhile, many Packt books have accompanying snippets of code to download.
It is Packt's fully searchable online digital book library, accessible from any device with a web browser.
If you have a Packt account, you might want to have a look at the nine free books which you can access now on PacktLib.
Preface If you are a developer building an application today then you know how important a good search experience is.
Apache Solr, built on Apache Lucene, is a wildly popular open source enterprise search server that easily delivers powerful search and faceted navigation features that are elusive with databases.
Solr supports complex search criteria, faceting, result highlighting, query-completion, query spellcheck, relevancy tuning, and more.
Apache Solr 3 Enterprise Search Server is a comprehensive reference guide for every feature Solr has to offer.
It serves the reader right from initiation to development to deployment.
It also comes with complete running examples to demonstrate its use and show how to integrate Solr with other languages and frameworks.
Through using a large set of metadata about artists, releases, and tracks courtesy of the MusicBrainz.org project, you will have a testing ground for Solr, and will learn how to import this data in various ways.
You will then learn how to search this data in different ways, including Solr's rich query syntax and "boosting" match scores based on record data.
What this book covers Chapter 1, Quick Starting Solr, will introduce Solr to you so that you understand its unique role in your application stack.
You'll get started quickly by indexing example data and searching it with Solr's sample "/browse" UI.
Chapter 2, Schema and Text Analysis, explains that the first step in using Solr is writing a Solr schema for your data.
You'll learn how to do this including telling Solr how to analyze the text for tokenization, synonyms, stemming, and more.
Chapter 3, Indexing Data, will explore all of the options Solr offers for importing data, such as XML, CSV, databases (SQL), and text extraction from common documents.
Chapter 4, Searching, you'll learn the basics of searching with Solr in this chapter.
Primarily, this covers the query syntax, from the basics to boolean options to more advanced wildcard and fuzzy searches.
Chapter 5, Search Relevancy, in this advanced chapter you will learn how Solr scores documents for relevancy ranking.
We'll review different options to influence the score, called boosting, and apply it to common examples like boosting recent documents and boosting by a user vote.
Chapter 6, Faceting, faceting is Solr's killer feature and this chapter will show you how to use it.
You'll learn about the three types of facets and how to build filter queries for a faceted navigation interface.
Chapter 7, Search Components, you'll discover how to use a variety of valuable search features implemented as Solr search components.
This includes result highlighting, query spell-check, query suggest/complete, result grouping, and more.
Chapter 8, Deployment, will guide you through deployment considerations in this chapter to include deploying Solr to Apache Tomcat, to logging, and to security.
Chapter 9, Integrating Solr, will explore some external integration options to interface with Solr.
This includes some language specific frameworks for Java, Ruby, PHP, and JavaScript, as well as a web crawler, and more.
Chapter 10, Scaling Solr, you'll learn how to tune Solr to get the most out of it.
Then we'll show you two mechanisms in Solr to scale out to multiple Solr instances when just one instance isn't sufficient.
Appendix, Search Quick Reference, is a convenient reference for common search related request parameters.
What you need for this book In Chapter 1, the Getting Started section explains what you need in detail.
Who this book is for This book is for developers who want to learn how to use Apache Solr in their applications.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: "You should use LRUCache because the cache is evicting content frequently."
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "While you can use the Solr Admin statistics page to pull back these results"
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.PacktPub.com.
If you purchased this book elsewhere, you can visit http://www.PacktPub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
The authors are also publishing book errata to include the impact that upcoming Solr releases have on the book.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Quick Starting Solr Welcome to Solr! You've made an excellent choice in picking a technology to power your search needs.
In this chapter, we're going to cover the following topics:
An introduction to Solr Solr is an open source enterprise search server.
It is a mature product powering search for public sites such as CNET, Zappos, and Netflix, as well as countless other government and corporate intranet sites.
It is written in Java, and that language is used to further extend and modify Solr through simple plugin interfaces.
However, being a server that communicates using standards such as HTTP and XML and JSON, knowledge of Java is useful but not a requirement.
In addition to the standard ability to return a list of search results for some query, Solr has numerous other features such as result highlighting, faceted navigation (as seen on most e-commerce sites), query spell correction, query completion, and a "more like this" feature for finding similar documents.
You will see many references in this book to the term faceting, also known as faceted navigation.
It's a killer feature of Solr that most people have experienced at major e-commerce sites without realizing it.
Faceting enhances search results with aggregated information over all of the documents found in the search.
Faceting information is typically used as dynamic navigational filters, such as a product category, date and price groupings, and so on.
Lucene, the underlying engine Before describing Solr, it is best to start with Apache Lucene, the core technology underlying it.
Lucene is an open source, high-performance text search engine library.
Lucene was developed and open sourced by Doug Cutting in 2000 and has evolved and matured since then with a strong online community and is the most widely deployed search technology today.
Being just a code library, Lucene is not a server and certainly isn't a web crawler either.
In order to use Lucene, you write your own search code using its API, starting with indexing documents: first you supply documents to it.
A document in Lucene is merely a collection of fields, which are name-value pairs containing text or numbers.
You configure Lucene with a text analyzer that will tokenize a field's text from a single string into a series of tokens (words) and further transform them by chopping off word stems, called stemming, substitute synonyms, and/or perform other processing.
The aforementioned process starting with the analyzer is referred to as text analysis.
Lucene indexes each document into its so-called index stored on disk.
The index is an inverted index, which means it stores a mapping of a field's terms to associated documents, along with the ordinal word position from the original text.
Finally you search for documents with a user-provided query string that Lucene parses according to its syntax.
Lucene assigns a numeric relevancy score to each matching document and only the top scoring documents are returned.
The brief description just given of how to use Lucene is how Solr works at its core.
It contains many important vocabulary words you will see throughout this book—they will be explained further at appropriate times.
Solr, a Lucene-based search server Apache Solr is an enterprise search server based on Lucene.
Lucene is such a big part of what defines Solr that you'll see many references to Lucene directly throughout this book.
Developing a high-performance, feature-rich application that uses Lucene directly is difficult and it's limited to Java applications.
Solr solves this by exposing the wealth of power in Lucene via configuration files and HTTP parameters, while adding some features of its own.
Also, there are two contrib modules that ship with Solr that really stand out:
As of the 3.1 release, there is a tight relationship between Solr and Lucene.
The source code repository, committers, and developer mailing list are the same, and they release together using the same version number.
This gives Solr an edge over other Lucene based competitors.
Comparison to database technology There's a good chance you are unfamiliar with Lucene or Solr and you might be wondering what the fundamental differences are between it and a database.
You might also wonder if you use Solr, whether you need a database.
The most important comparison to make is with respect to the data model—that is the organizational structure of the data.
The most popular category of databases is a relational database—RDBMS.
A defining characteristic of a relational database is a data model based on multiple tables with lookup keys between them and a join capability for querying across them.
RDBMSs have a very flexible data model, but this makes it harder to scale them easily.
Lucene instead has a more limiting document oriented data model, which is analogous to a single table without join possibilities.
Document oriented databases, such as MongoDB are similar in this respect, but their documents can have a rich nested structure similar to XML or JSON, for example.
Lucene's document structure is flat, but it does support multivalued fields—that is a field with an array of values.
Taking a look at the Solr feature list naturally reveals plenty of search-oriented technology that databases generally either don't have, or don't do well.
Notable features are relevancy score ordering, result highlighting, query spellcheck, and query-completion.
These features are what drew you to Solr, no doubt.
Can Solr be a substitute for your database? You can add data to it and get it back out efficiently with indexes; so on the surface it seems plausible, provided the flat document-oriented data model suffices.
The answer is that you are almost always better off using Solr in addition to a database.
Databases, particularly RDBMSs, generally excel at ACID transactions, insert/update efficiency, in-place schema changes, multi-user access control, bulk data retrieval, and supporting rich ad-hoc query features.
Solr falls short in all of these areas but I want to call attention to these:
I wrote more about this subject online: "Text Search, your Database or Solr" at  http://bit.ly/uwF1ps.
Getting started We're going to get started by downloading Solr, examine its directory structure, and then finally run it.
This sets you up for the next section, which tours a running Solr server.
Get Solr: You can download Solr from its website: http://lucene.apache.org/ solr/
The last Solr release this book was written for is version 3.4
Solr has had several relatively minor point-releases since 3.1 and it will continue.
In general I recommend using the latest release since Solr and Lucene's code are extensively tested.
For book errata describing how future Solr releases affect the book content, visit our website: http://www.solrenterprisesearchserver.com/
Lucid Imagination also provides a Solr distribution called "LucidWorks for Solr"
As of this writing it is Solr 3.2 with some choice patches that came after to ensure its stability and performance.
It's completely open source; previous LucidWorks releases were not as they included some extras with use limitations.
LucidWorks for Solr is a good choice if maximum stability is your chief concern over newer features.
Typing java –version at a command line will tell you exactly which version of Java you are using, if any.
Use latest version of Java! The initial release of Java 7 included some serious bugs that were discovered shortly before its release that affect Lucene and Solr.
Therefore, I advise you to use the latest Java release.
Java is available on all major platforms including Windows, Solaris, Linux, and Apple.
Java always comes with the Java Runtime Environment (JRE) and that's all Solr requires.
The Java Development Kit (JDK) includes the JRE plus the Java compiler and various diagnostic utility programs.
Solr is a Java-based web application, but you don't need to be particularly familiar with Java in order to use it.
Get the book supplement: This book includes a code supplement available at our website: http://www.solrenterprisesearchserver.com/
The software includes a Solr installation configured for data from MusicBrainz.org, a script to download and index that data into Solr—about 8 million documents in total, and of course various sample code and material organized by chapter.
This supplement is not required to follow any of the material in the book.
It will be useful if you want to experiment with searches using the same data used for the book's searches or if you want to see the code referenced in a chapter.
The majority of code is for Chapter 9, Integrating Solr.
Solr's installation directory structure When you unzip Solr after downloading it, you should find a relatively straightforward directory structure:
Ignore the client directory Most client libraries are maintained by other organizations, except for the Java client SolrJ which lies in the dist/ directory.
There are some "ICU" Unicode classes for multilingual support, a Chinese stemmer, and a Polish stemmer.
You'll learn more about text analysis in the next chapter.
There are modules that identify proper names in text and identify the language, for example.
This should serve as a good starting point for new Solr applications.
It is used in Solr's tutorial and we'll use it in this chapter too.
A copy of Solr's WAR file is here, which contains Solr's compiled code.
Solr's home directory and Solr cores When Solr starts, the very first thing it does is determine where the Solr home directory is.
Chapter 8, Deployment covers the various ways to tell Solr where it is, but by default it's the directory named simply solr relative to the current working directory where Solr is started.
You will usually see a solr.xml file in the home directory, which is optional but recommended.
For simpler configurations like example/solr, there is just one Solr core, which uses Solr's home directory as its core instance directory.
A Solr core holds one Lucene index and the supporting Solr configuration for that index.
Nearly all interactions with Solr are targeted at a specific core.
If you want to index different types of data separately or shard a large index into multiple ones then Solr can host multiple Solr cores on the same Java server.
A Solr core's instance directory is laid out like this:
Running Solr Now we're going to start up Jetty and finally see Solr running albeit without any data to query yet.
We're about to run Solr directly from the unzipped installation.
This is great for exploring Solr and doing local development, but it's not what you would seriously do in a production scenario.
In a production scenario you would have a script or other mechanism to start and stop the servlet engine with the operating system—Solr does not include this.
And to keep your system organized, you should keep the example directly as exactly what its name implies—an example.
So if you want to use the provided Jetty servlet engine in production, a fine choice then copy the example directory elsewhere and name it something else.
Chapter 8, Deployment, covers how to deploy Solr to Apache Tomcat, the most popular Java servlet engine.
It also covers other subjects like security, monitoring, and logging.
First go to the example directory, and then run Jetty's start.jar file by typing the following command:
You'll see about a page of output, including references to Solr.
When it is finished, you should see this output at the very end of the command prompt:
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.PacktPub.com (url)
If you purchased this book elsewhere, you can visit http://www.PacktPub.
If Jetty reports this, then it doesn't necessarily mean that Solr was deployed successfully.
You might see an error such as a stack trace in the output if something went wrong.
Even if it did go wrong, you should be able to access the web server: http://localhost:8983
Jetty will give you a 404 page but it will include a list of links to deployed web applications, which will just be Solr for this setup.
To quit Jetty (and many other command line programs for that matter), press Ctrl+C on the keyboard.
A quick tour of Solr Start up Jetty if it isn't already running and point your browser to Solr's admin site at: http://localhost:8983/solr/admin/
This tour will help you get your bearings on this interface that is not yet familiar to you.
We're not going to discuss it in any depth at this point.
The top gray area in the preceding screenshot is a header that is on every page of the admin site.
When you start dealing with multiple Solr instances—for example, development versus production, multicore, Solr clusters—it is important to know where you are.
The (example) is a reference to the name of the schema—a simple label at the top of the schema file.
If you have multiple schemas for different data sets, then this is a useful differentiator.
Next is the current working directory cwd, and Solr's home.
Arguably the name of the core and the location of the data directory should be on this overview page but they are not.
The block below this is a navigation menu to the different admin screens and configuration data.
Most recent browsers show the XML color-coded and with controls to collapse sections.
If you don't see readable results and won't upgrade or switch your browser, you can always use your browser's View source command.
After the main menu is the Make a Query text box where you can type in a simple query.
There's no data in Solr yet, so there's no point trying that right now.
Finally, the bottom Assistance area contains useful information for Solr online.
The last section of this chapter has more information on such resources.
Loading sample data Solr comes with some sample data and a loader script, found in the example/ exampledocs directory.
We're going to use that for the remainder of this chapter so that we can explore Solr more without getting into schema design and deeper data loading options.
For the rest of the book, we'll base the examples on the digital supplement to the book—more on that later.
We're going to invoke the post.jar Java program, officially called SimplePostTool with a list of Solr-formatted XML input files.
This simple program iterates over each argument given, a file reference, and HTTP posts it to Solr running on the current machine at the example server's default configuration—http://localhost:8983/solr/update.
Finally, it will send a commit command, which will cause documents that were posted prior to the last commit to be saved and visible.
Obviously, Solr must be running for this to work, so ensure that it is first.
If you are using a Unix-like environment, you have an alternate option of using the post.sh shell script, which behaves similarly by using curl.
I recommend examining the contents of the post.sh bash shell script for illustrative purposes, even if you are on Windows—it's very short.
The post.sh and post.jar programs could be used in a production scenario, but they are intended just for demonstration of the technology with the example data.
Let's take a look at one of these XML files we just posted to Solr, monitor.xml:
The XML schema for XML files that can be posted to Solr is very simple.
This file doesn't demonstrate all of the elements and attributes, but it shows most of what matters.
The other essential tag, not seen here, is <commit/> which post.jar and post.sh send in a separate post.
This syntax and command set may very well be all that you use.
More about these options and other data loading choices will be discussed in Chapter 3, Indexing Data.
A simple query On Solr's main admin page, run a simple query that searches for the word monitor.
Simply type this word in and click on the Search button.
Both this form and the Full Interface one are standard HTML forms; they are as simple as they come.
The form inputs become URL parameters to another HTTP GET request which is a Solr search returning XML.
The form only controls a basic subset of all possible parameters.
The main benefit to the form is that it applies the URL escaping for special characters in the query, and for some basic options, you needn't remember what the parameter names are.
It is convenient to use the form as a starting point for developing a search, and then subsequently refine the URL directly in the browser instead of returning to the form.
Most modern browsers, such as Firefox, provide a good XML view with syntax coloring and hierarchical structure collapse controls.
Solr can format responses in JSON and other formats but that's a topic for another time.
They have the same basic structure as the XML you're about to see, by the way.
The XML response consists of a <response/> element, which wraps the entire message.
Due to multiple layers of caching, you will find that your searches will often complete in a millisecond or less if you've run the query before.
More information on these parameters and many more are in Chapter 4, Searching.
The numFound number is the number of documents matching the query in the entire index.
Notice there is a search parameter by the same name as seen in the response header.
There is also a parameter rows which specifies how many matching documents to return—just 10 in this example.
Often, you'll want to see the score of each matching document, which is a number assigned to it based on how relevant the document is to the search query.
This search response doesn't refer to scores because it needs to be explicitly requested in the fl parameter—a comma separated field list.
A search that requests the score will have a maxScore attribute in the <result/> element, which is the maximum score of all documents that matched the search.
It's independent of the sort order or result paging parameters.
The content of the result tag is a list of documents that matched the query.
By default, Solr will list all of the stored fields.
Not all of the fields are necessarily stored—that is, you can query on them but not retrieve their value—an optimization choice.
Notice that it uses the basic data types str, bool, date, int, and float.
Also note that certain fields are multi-valued, as indicated by an arr tag.
As you start adding more options like faceting and highlighting, you will see additional XML following the result element.
Some statistics Let's take a look at the statistics admin page: http://localhost:8983/solr/ admin/stats.jsp.
If you are using a version of Solr other than 3.4, then this number may be different.
The astute reader might observe we posted fewer XML files to Solr.
The discrepancy is due to some XML files containing multiple Solr documents.
That can happen either due to an explicit delete posted to Solr or by adding a document that replaces another in order to enforce a unique primary key.
While you're at this page, notice that the request handler named /update has some stats too:
Another request handler you'll want to examine is named search, which has been processing our search queries.
These statistics are calculated only for the current running Solr, they are not stored to disk.
The sample browse interface The final destination of our quick Solr tour is to visit the so-called browse interfaceavailable at http://localhost:8983/solr/browse.
This is also a demonstration of Solritas, which formats Solr requests using templates based on Apache Velocity.
The browse UI as supplied assumes the default example Solr schema.
It will not work out of the box against another schema without modification.
Configuration files The configuration files in example/solr/conf are extremely well documented.
We're not going to go over the details here but this should give you a sense of what is where.
The as-provided state of these files is really just an example to both demonstrate features and document their configuration and should not be taken as the only way of configuring Solr.
You will observe that the names of the fields in the documents we added to Solr intuitively correspond to the sample schema.
Aside from the fundamental parts of defining the fields, you may also notice <copyField> elements, which copy an input field as provided to another field.
There are various reasons for doing this, but they boil down to needing to index data in different ways for specific search purposes.
You'll learn all that you could want to know about the schema in the next chapter.
Solr's solrconfig.xml file contains lots of parameters that can be tweaked.
At the moment, we're just going to take a peak at the request handlers, which are defined with <requestHandler> elements.
In our first query, we didn't specify any request handler, so we got the default one:
Each HTTP request to Solr, including posting documents and searches, goes through a particular request handler.
Handlers can be registered against certain URL paths by naming them with a leading "/"
When we uploaded the documents earlier, it went to the handler defined like this, in which /update is a relative URL path:
The qt URL parameter can refer to a request handler by name as well.
Requests to Solr are nearly completely configurable through URL parameters or POST'ed form parameters.
The well-documented file also explains how and when they can be added to appends, or invariants named lst blocks.
This arrangement allows you to set up a request handler for a particular application that will be searching Solr without forcing the application to specify all of its query parameters.
More information on request handlers is in Chapter 4, Searching.
Resources outside this book The following are some Solr resources other than this book:
I highly recommend that you subscribe to the Solr-users mailing list.
Notation convention: Solr's JIRA issues are referenced like this: SOLR64
You may also see issues for Lucene that follow the same convention, for example, LUCENE-1215
There are of course resources for Lucene, like the Lucene In Action book.
If you intend to dive into Solr's internals then you will find Lucene resources helpful, but that is not the focus of this book.
In the ensuing chapters, you're really going to get familiar with what Solr has to offer.
I recommend that you proceed in order from the next chapter through Chapter 7, Search Components, because these build on each other and expose nearly all of the capabilities in Solr.
These chapters are also useful as a reference to Solr's features.
You can of course skip over sections that are not interesting to you.
Chapter 9, Integrating Solr, is one you might peruse at any time, as it may have a section applicable to your Solr usage scenario.
Finally, be sure that you don't miss the appendix for a search quick-reference cheat-sheet.
The digital supplement to the book is available at http://www.
Our website should also include post-publication errata that should ideally be reviewed before continuing.
Schema and Text Analysis The foundation of Solr is based on Lucene's index—the subject of this chapter.
The following diagram shows the big picture of how various aspects of working with Solr are related.
In this chapter we are focusing on the foundational layer—the index:
In a hurry? This is a fairly important foundational chapter; that said, the subject of text analysis can be skimmed in lieu of using the predefined field types provided with Solr's example schema.
MusicBrainz.org Instead of continuing to work with the sample data that comes with Solr, we're going to use a large database of music metadata from the MusicBrainz project at http:// musicbrainz.org.
The data is free and is submitted by a large community of users.
One way MusicBrainz offers this data is in the form of a large SQL file for import into a PostgreSQL database.
In order to make it easier for you to play with this data, the online code supplement to this book includes the data in formats that can readily be imported into Solr.
Alternatively, if you already have your own data then I recommend starting with that, using this book as a guide.
Therefore, it will serve as an excellent instructional data set to discuss Solr schema choices.
The MusicBrainz database schema is quite complex, and it would be a distraction to go over even half of it.
I'm going to use a subset of it and express it in a way that has a straightforward mapping to the user interface seen on the MusicBrainz website.
Each of these tables depicted in the following diagram can be easily constructed through SQL subqueries or views from the actual MusicBrainz tables.
To describe the major tables above, I'll use some examples here from my favorite band, the Smashing Pumpkins.
By the way, I'll be using the word "entity" occasionally here in the data modeling sense—it's basically a type of thing represented by the data.
Artist, release, event, and track are all entity types with respect to MusicBrainz.
In a relational database, most tables correspond to an entity type and the others serve to relate them or provide for multiple values.
In Solr, each document will have a primary entity type, and it may contain other entities as a part of it too.
One combined index or separate indices The following discussion concerns how to manage the searching of different types of data, such as artists and releases from MusicBrainz.
In the example MusicBrainz configuration, each document of each type gets their own index but they all share the same configuration.
Although I wouldn't generally recommend it, this approach was done for convenience and to reduce the complexity for this book at the expense of a one-size-fits-all schema and configuration.
A Solr Core is an instance of Solr to include the configuration and index, sometimes the word "core" is used synonymously with "index"
Even if you have one type of data to search for in an application, you might still use multiple cores (with the same configuration) and shard the data for scaling.
Managing Solr Cores is discussed further in the deployment chapter.
One combined index A combined index might also be called an aggregate index.
As mentioned in the first chapter, an index is conceptually like a single-table relational database schema, thus sharing similarities with some NoSQL (non-relational) databases.
In spite of this limitation, there is nothing to stop you from putting different types of data (say, artists and releases from MusicBrainz) into a single index.
All you have to do is use different fields for the different document types, and use a field to discriminate between the types.
An identifier field would need to be unique across all documents in this index, no matter what the type is, so you could easily do this by concatenating the field type and the entity's identifier.
This may appear ugly from a relational database design standpoint, but this isn't a database! More importantly, unlike a database, there is no overhead whatsoever for some documents to not populate some fields.
This is where the spreadsheet metaphor can break down, because a blank cell in a spreadsheet takes up space, but not in an index.
Here's a sample schema.xml snippet of the fields for a single combined index approach:
A combined index has the advantage of being easier to maintain, since it is just one configuration.
It is also easier to do a search over multiple document types at once since this will naturally occur assuming you search on all relevant fields.
For these reasons, it is a good approach to start off with.
For the book, we've taken a hybrid approach in which there are separate Solr Cores (indices) for each MusicBrainz data type, but they all share the same configuration, including the schema.
Problems with using a single combined index Although a combined index is more convenient to set up, there are some problems that you may face while using a single combined index:
You will get scores that are of lesser quality due to sub-optimal document frequency values, a component of the IDF part of the score.
The document frequency is simply the number of documents in which a queried term exists for a specific field.
If you put different types of things into the same field, then what could be a rare word for a track name might not be for an artist name.
Prefix, wildcard, and fuzzy queries will take longer and will be more likely to reach internal scalability thresholds.
If you share a field with different types of documents, then the total number of terms to search over is going to be larger, which takes longer for these query types.
It will also match more terms than it would otherwise, while possibly generating a query that exceeds the maxBooleanClauses threshold (configurable in solrconfig.xml)
Separate indices For separate indices, you simply develop your schemas independently.
You can use a combined schema as previously described and use it for all of your cores so that you don't have to manage them separately; it's not an approach for the purists but it is convenient and it's what we've done for the book example code.
The rest of the discussion here assumes the schemas are independent.
To share the same schema field type definitions (described soon) across your schemas without having to keep them in sync, use the XInclude feature.
If you do develop separate schemas and if you need to search across your indices in one search then you must perform a distributed search, described in the last chapter.
A distributed search is usually a feature employed for a large corpus but it applies here too.
Be sure to read more about it before using it since there are some limitations.
As in the combined-schema, you will need a unique ID across all documents and you will want a field "type" to differentiate documents in your search results.
You don't need commonly named fields to search on since the query will be processed at each core using the configuration there to determine, for example, what the default search field is.
Schema design A key thing to come to grips with is that the queries you need Solr to support completely drive your Solr schema design.
Conversely, relational databases typically use standard third normal form decomposition of the data, largely because they have strong relational-join support.
Since queries drive the Solr schema design, all the data needed to match a document, that is the criteria, must be in the document matched, not in a related one.
To satisfy that requirement, data that would otherwise exist in one place is copied into related documents that need it to support a search.
For example, an artist's name in MusicBrainz would not just exist on an artist document but also in a track document to support searching for tracks by artist.
This may feel dirty because you're probably used to thinking in terms of relational databases.
Even if you're not working with a database as your source data, these concepts still apply.
So pay close attention to this important subject in any case.
At this point I'm going to outline a series of steps to follow in order to arrive at one or more Solr schemas to power searches for an application of any sort.
For specifics, we will consider the MusicBrainz.org website and hypothetically how it could work.
It goes as far as listing the fields but not into text analysis or making changes for particular search features, like faceting.
In truth, schema design is somewhat creative and it's always evolutionary—so consider these steps a guide for your first time at it, not a foolproof process.
Step 1: Determine which searches are going to be powered by Solr Any text search capability is going to be Solr powered.
At the risk of stating the obvious, I'm referring strictly to those places where a user types in a bit of text and subsequently gets some search results.
On the MusicBrainz website, the main search function is accessed through the form that is always present on the left.
There is also a more advanced form that adds a few options but is essentially the same capability, and I treat it as such from Solr's point of view.
We can see the MusicBrainz search form in the following screenshot:
Once we look through the remaining steps, we may find that Solr should additionally power some faceted navigation in areas that are not accompanied by text search (that is, the facets are of the entire data set, not necessarily limited to the search results of a text query alongside it)
An example of this at MusicBrainz is the "Top Voters" tally, which I'll address soon.
Step 2: Determine the entities returned from each search For the MusicBrainz search form, this is easy.
It just so happens that in MusicBrainz, a search will only return one entity type.
Note that internally, each result from a search corresponds to a distinct document in the Solr index and so each entity will have a corresponding document.
This entity also probably corresponds to a particular row in a database table, assuming that's where it's coming from.
The book examples and digital companion data only make use of MusicBrainz artists, releases, and tracks.
Step 3: Denormalize related data For each entity type, find all of the data in the schema that will be needed across all searches of it.
Such data includes any data queried for (that is, criteria to determine whether a document matches or not) and any data that is displayed in the search results.
The end result of denormalization is to have each document sufficiently self-contained, even if the data is duplicated across the index(es)
Again, this is because Solr does not (yet) support relational joins.
A join allows a query to match a document based on data in another document related by some field in common.
Without this feature, you need to denormalize data but that can be prohibitive in some circumstances.
This is a relatively simple case, because each track has no more than one artist or album.
Both the artist name and album name would get their own field in Solr's flat schema for a track.
Since the artist and album names are not unambiguous references, it is useful to also add the IDs for these tables into the track schema to support linking in the user interface, among other things.
Unfortunately, databases usually make this harder than it should be if it's just a simple list.
However, Solr's fields directly support the notion of multiple values.
Remember in the MusicBrainz schema that an artist of type group can have some number of other artists as members.
Although MusicBrainz's current search capability doesn't leverage this, we'll capture it anyway because it is useful for more interesting searches.
The Solr schema to store this would simply have a member name field that is multi-valued.
The member_id field alone would be insufficient, because denormalization requires that the member's name be copied into the artist.
This example is a good segue to how things can get a little more complicated…
If we only record the member name, then it is problematic to do things like have links in the UI from a band member to that member's detail page.
This is because we don't have that member's artist ID, only their name.
So we'll add a multi-valued field for the member's ID.
Multi-valued fields maintain ordering so that the two fields would have corresponding values at a given index.
Beware, there can be a tricky case when one of the values can be blank, and you need to come up with a placeholder.
The client code would have to know about this placeholder.
What you should not do is try to shove different types of data into the same field by putting both the artist IDs and names into one field.
It could introduce text analysis problems, as a field would have to satisfy both types, and it would require the client to parse out the pieces.
The exception to this is when you are merely storing it for display, not searching it.
Then you can store whatever you want in a field.
A problem with denormalizing one-to-many data comes into play when multiple fields from the other entity are brought in, and you need to search on more than one of those fields at once.
For a hypothetical example, imagine a search for releases that contain a track with a particular word in the name and with a particular minimum duration.
Both the track name and duration fields on a release would be multi-valued, and a search would have criteria for both.
Unfortunately, Solr would erroneously return releases where one track's name satisfied the criteria and a separate track's duration satisfied the criteria but not necessarily for the same track.
One work-around is searching the track index instead of the release one and using Solr's new result grouping feature to group by release.
This solution of course depends on an additional index holding entity relationships going the other way.
If you are faced with this challenge but can't create this additional index because the index would be prohibitively large for your data, then you may have to wait till Solr 4's join support.
Step 4: (Optional) Omit the inclusion of fields only used in search results It's not likely that you will actually do this, but it's important to understand the concept.
If there is any data shown on the search results that is not queryable, not sorted upon, not faceted on, nor are you using the highlighter feature for, and for that matter are not using any Solr feature that uses the field except to simply return it in search results, then it is not necessary to include it in the schema for this entity.
Let's say, for the sake of argument, that the only information queryable, sortable, and so on is a track's name, when doing a query for tracks.
You can opt not to inline the artist name, for example, into the track entity.
When your application queries Solr for tracks and needs to render search results with the artist's name, the onus would be on your application to get this data from somewhere—it won't be in the search results from Solr.
The application might look these up in a database, in some caching middleware, or perhaps even query our Solr artist index.
This clearly makes generating a search results screen more difficult, because you now have to get the data from more than one place.
Moreover, to do it efficiently, you would need to take care to query the needed data in bulk, instead of each row individually.
Additionally, it would be wise to consider a caching strategy to reduce the queries to the other data source.
It will, in all likelihood, slow down the total render time too.
However, the benefit is that you needn't get the data and store it into the index at indexing time.
It might be a lot of data, which would grow your index, or it might be data that changes often, necessitating frequent index updates.
If you are using distributed search, as discussed in Chapter 9, Integrating Solrh there is some performance gain in not sending too much data around in the requests.
This could result in 2,000 records being sent around the network.
Just sending the IDs around would be much more network efficient, but then this leaves you with the job of collecting the data elsewhere before display.
The only way to know if this works for you is to test both scenarios.
In general, if the data in question is not large then keep it in Solr.
At the other end of the extreme is storing all data in Solr.
Why not? At least in the case of MusicBrainz, it wouldn't be appropriate.
The account names listed are actually editors in MusicBrainz terminology.
This piece of the screen tallies an edit, grouped by the editor who performed the edit.
It's the edit that is the entity in this case.
The following screenshot shows the Top Voters (aka editors), which are tallied by the number of edits:
This data simply doesn't belong in an index, because there's no use case for searching edits, only lookup when we want to see the edits on some other entity like an artist.
One objective guide to help you decide on whether to put an entity in Solr or not is to ask yourself if users will ever be doing a text search on that entity—a feature where index technology stands out from databases.
If not, then you probably don't want the entity in your Solr index.
Before we continue, find a schema.xml file to follow along.
This file belongs in the conf directory for a Solr instance configuration.
For simple single-core Solr setups, this is the same as a Solr home directory.
If you are working off of the Solr distribution, you'll find it in example/solr/ conf/schema.xml.
The example schema.xml is loaded with useful field types, documentation, and field definitions used for the sample data that comes with Solr.
I prefer to initialize a Solr configuration by copying the example Solr home directory and liberally modifying it as needed, ripping out or commenting what I don't need (which is often a lot)
This is half way between starting with nothing, or starting with the example and making essential modifications.
If you do start with Solr's example configuration, be sure to revisit your configuration at some point to clean out what you aren't using.
In addition, it's tempting to keep the existing documentation comments, but you can always refer back to what comes with Solr as needed and keep your config file clean.
At the start of the file is the schema opening tag:
We've set the name of this schema to musicbrainz, the name of our application.
If we used different schema files, then we should name them differently to differentiate them.
Defining field types The first section of the schema is the definition of the field types.
This section is enclosed in the <types/> element and will consume lots of the file's content.
The field types declare the types of fields, such as booleans, numbers, dates, and various text flavors.
They are referenced later by the field definitions under the <fields/> element.
A field type has a unique name and is implemented by a Java class specified by the class attribute.
The last piece is the simple name of the class, and the part preceding it is called the package name.
In order to make configuration files in Solr more concise, the package name can be abbreviated to just solr for most of Solr's packages.
Attributes other than the name and class represent configuration options; most are applicable to all types, like omitNorms, and some are specific to the implementing class.
They can usually be overridden at the field declaration too.
In addition to these attributes, there is also the text analysis configuration that is only applicable to text fields.
Built-in field type classes There are a number of built-in field types and nearly all are present and documented to some extent in Solr's example schema.
We're not going to enumerate them all here, but instead highlight some of them worthy of more explanation.
Numbers and dates There are no less than five different field types to use to store an integer, perhaps six if you want to count string! It's about the same for float, double, long, and date.
And to think that you probably initially thought this technology only did text! I'll explain when to use which, using Integer as an example.
Most have an analogous name for the other numeric and date types.
The field types with names starting with "Trie" should serve 95% of your needs.
All of these numeric types sort in their natural numeric order instead of lexicographically.
Finally, there is a field type called ExternalFileField, which reads its float values from a plain text file instead of the index.
It was designed for sorting or influencing scores of documents based on data that might change quickly (for example, a rating or click-through) without having to re-index a document.
Remember that Lucene fundamentally cannot update just a single field; entire documents need to be reindexed.
Geospatial Solr's geospatial support spans multiple parts of Solr from field types to query parsers, to function queries.
Instead of having you read relevant parts of three chapters, I've consolidated it into Chapter 4, Searching.
Field options The attributes listed here are common attributes applicable to most if not all field types.
These options are assumed to be boolean (true/false) unless indicated, otherwise.
Some of these options can be specified in the field type definition when applicable to any field of this type.
The indented options defined below, underneath indexed (and stored) imply indexed (and stored) must be true.
The default behavior for such documents is to appear first for ascending and last for descending.
Aside from its affect on scores, it saves a little memory too.
Phrase queries won't work and scores will be less effective.
If a field is to be used by the MoreLikeThis feature, or for highlighting of a large text field, then try enabling this.
It can substantially increase the index size and indexing time so do a before-and-after measurement.
There are two more options which add more data to term vectors: termPositions and termOffsets.
This option was removed as of Solr v1.4.1 because the committers were unhappy with its implementation.
There is a helpful table on Solr's wiki showing most of the options with some use cases that need them: http://wiki.apache.org/solr/FieldOptionsByUseCase.
Field definitions The definitions of the fields in the schema are located within the <fields/> element.
In addition to the field options defined above, a field has these attributes:
For information on specifying dates, see DateMath in Chapter 4, Searching.
Dynamic field definitions The very notion of a dynamic field definition highlights the flexibility of Lucene's index, as compared to typical relational database technology.
Not only can you explicitly name fields in the schema, but you can also have some defined on the fly based on the name supplied for indexing.
Solr's example schema contains some examples of this, such as:
A dynamic field is declared just like a regular field in the same section.
However, the element is named dynamicField, and it has a name attribute that must either start or end with an asterisk (the wildcard)
It can also be just *, which is the final fallback.
The * fallback is most useful if you decide that all fields attempted to be stored in the index should succeed, even if you didn't know about the field when you designed the schema.
It's also useful if you decide that instead of it being an error, such unknown fields should simply be ignored (that is, not indexed and not stored)
In the end, a field is a field, whether explicitly defined or defined dynamically according to a name pattern.
Dynamic field definitions are just a convenience that makes defining schemas easier.
There are no performance implications of using dynamic field definitions.
Our MusicBrainz field definitions What follows is a first cut of our MusicBrainz schema definition.
There are additional fields that will be added in other chapters to explore other search features.
This is a combined schema defining all core entity types: artists, releases (AKA albums), and tracks.
I also used this abbreviation when I denormalized relationships like in r_a_name (a release's artist's name)
You'll find the sample data helpful and anyone else working on your project will thank you for it! In the examples above, I sometimes use actual values and on other occasions I list several possible values separated by |, if there is a predefined list.
Also, note that the only fields that we can mark as required are those common to all, which are ID and type, because we're doing a combined schema approach.
In our schema we're choosing to index most of the fields, even though MusicBrainz's search doesn't require more than the name of each entity type.
We're doing this so that we can make the schema more interesting to demonstrate more of Solr's capabilities.
As it turns out, some of the other information in MusicBrainz's query results actually are queryable if one uses the advanced form, checks use advanced query syntax, and your query uses those fields (example: artist:"Smashing Pumpkins")
At the time of writing this, MusicBrainz used Lucene for its text search and so it uses Lucene's query syntax.
Copying fields Closely related to the field definitions are copyField directives.
A copyField directive copies one or more input field values to another during indexing.
This directive is useful when a value needs to be copied to additional field(s) to be indexed differently.
For example, sorting and faceting require a single indexed value.
Another is a common technique in search systems in which many fields are copied to a common field that is indexed without norms and not stored.
This permits searches, which would otherwise search many fields, to search one instead, thereby drastically improving performance at the expense of reducing score quality.
This technique is usually complemented by searching some additional fields with higher boosts.
The dismax query parser, which is described in a later chapter, makes this easy.
At index-time, each supplied field of input data has its name compared against the source attribute of all copyField directives.
The source attribute might include a * wildcard so it's possible the input might match more than one copyField.
If a wildcard is used in the destination, then it must refer to a dynamic field, and furthermore the source must include a wildcard too—otherwise a wildcard in the destination is an error.
A match against a copyField has the effect of the input value being duplicated but using the field name of the dest attribute of the directive.
If maxChars is optionally specified, the copy is truncated to this many characters.
The duplicate does not replace any existing values that might be going to the field so be sure to mark the destination field as multiValued if needed.
Finally, note that copying data to additional fields means longer indexing times and larger index file sizes.
The unique key Near the bottom of the schema is the <uniqueKey> declaration specifying which field uniquely identifies each document, if any.
Although it is technically not always required, you should define a unique ID field.
In our MusicBrainz schema, the ID is a string that includes an entity type prefix type so that it's unique across the whole corpus, spanning multiple Solr Cores.
If your source data does not have an ID field that you can propagate, then you may want to consider using a Universally Unique Identifier, a UUID, according to RFC-4122
Simply have a field with a field type for the class solr.UUIDField and either provide a UUID to Solr or supply the special value of "NEW", such as with setting defaultField and Solr will generate a UUID for you automatically.
The default search field and query operator Near the bottom of the schema file are a couple of configuration elements pertaining to search defaults when interpreting a query string:
The defaultSearchField declares the particular field that will be searched for queries that don't explicitly reference one.
The solrQueryParser setting has a defaultOperator attribute which lets you specify the default search operator (that is AND, or OR) here in the schema.
These are essentially defaults for searches that are processed by Solr request handlers defined in solrconfig.xml.
These settings are optional here, and I've commented them out in the MusicBrainz schema.
Text analysis Text analysis is a topic that covers text-processing steps such as tokenization, case normalization, stemming, synonyms, and other miscellaneous text processing.
The analysis is applied to a text field at index time and as part of query string processing at search time.
It's an important part of search engines since the details have an effect on getting good search results, especially to recall—a dimension of search result quality pertaining to whether all relevant documents are in the search results.
This material is almost completely Lucene-centric and so also applies to any other software built on top of Lucene.
For the most part, Solr merely offers XML configuration for the code in Lucene that provides this capability.
For information beyond what is covered here, including writing your own analysis components, read the Lucene In Action 2 book.
Text analysis converts text for a particular field into a sequence of terms.
A term is the fundamental unit that Lucene actually indexes and searches.
The analysis is used on the original incoming value at index time; the resulting terms are ultimately recorded onto disk in Lucene's index structure where it can be searched.
The analysis is also performed on words and phrases parsed from the query string; the resulting terms are then searched in Lucene's index.
An exception to this is prefix, wildcard and fuzzy queries which all skip text analysis.
In a hurry? As a starting point, you should use the existing field types in Solr's default schema, which includes a variety of text field types for different situations.
They will suffice for now and you can return to this chapter later.
There will surely come a time when you are trying to figure out why a simple query isn't matching a document that you think it should, and it will quite often come down to your text analysis configuration.
Non-English text analysis I try to cover Solr in a comprehensive fashion, but in the area of text analysis for non-English languages I'm going to refer you to this excellent Solr wiki page: http://wiki.apache.org/solr/ LanguageAnalysis.
You'll notice that there is some variation in how to configure Solr for each of them, and that some languages have multiple options.
Most language-specific elements are the stemmer and the stop word list, and for eastern languages, the tokenizer too.
There is also a set of International Components for Unicode, ICU, related analysis components new to Solr 3.1, some of which you can use for mapping some non-Latin characters to Latin equivalents.
Configuration Solr has various field types as we've previously explained, and the most important one is solr.TextField.
This is the field type that has an analyzer configuration.
Let's look at the configuration for the text_en_splitting field type definition that comes with Solr's example schema.
I added in a character filter, albeit commented, to show what it looks like.
As you read about text analysis in this chapter, you may want to flip back to see this configuration.
The configuration example defines two analyzers, each of which specifies an ordered sequence of processing steps that convert text into a sequence of terms.
The type attribute, which can hold a value of index or query, differentiates whether the analyzer is applied at index time or query time, respectively.
If the same analysis is to be performed at both index and query times, then you can specify just one analyzer without a type.
When both are specified as in the example above, they usually only differ a little.
Analyzers, Tokenizers, Filters, oh my! The various components involved in text analysis go by various names, which are about to be defined.
They are all conceptually the same: they take in text and spit out text, sometimes filtering, sometimes adding new terms, and sometimes modifying terms.
The difference is in the specific flavor of input and output for them: either character based or token based.
An analyzer can optionally begin with one or more character filters, which operate at a streaming character level to perform manipulations on original input text.
These are most commonly used to normalize characters, like remove accents, for example.
Following any optional character filters is the tokenizer—the only mandatory piece of the chain.
This analyzer takes a stream of characters and tokenizes it into a stream of tokens, usually with a simple algorithm such as splitting on whitespace.
The remaining analysis steps, if any, are all token filters (often abbreviated to just filters), which perform a great variety of manipulations on tokens.
The final tokens at the end, usually referred to as terms at this point, are what Lucene actually indexes or searches, depending on context.
This is a convention for the names of Solr's Java classes that accept the configuration and instantiate Lucene's analysis components that have the same simple name, less the "Factory" suffix.
References to these analysis components in this book and elsewhere sometimes include the "Factory" suffix and sometimes not; no distinction is intended.
If search-time query text analysis yields more than one token, such as Wi-Fi tokenizing to Wi and Fi, then by default these tokens are simply different search terms with no relation to their position.
If this attribute is enabled, then the tokens become a phrase query, such as "WiFi" and consequently these tokens must be adjacent in the index.
This automatic phrase query generation would always happen prior to Solr 3.1 but now it is configurable and defaults to false.
I favor that choice, since you'll learn in Chapter 5, Search Relevancy how to do automatic phrase boosting to get the most relevant documents (those that would match the phrase "Wi Fi") at the top of the results.
Experimenting with text analysis Before we dive into the details of particular analysis components, it's important to become comfortable with Solr's analysis page, which is an experimentation and a troubleshooting tool that is absolutely indispensable.
You'll use this to try out different configurations to verify whether you get the desired effect, and you'll use this when troubleshooting to find out why certain queries aren't matching certain text that you think they should.
In Solr's admin pages, you'll see a link named [ANALYSIS] which takes you to this screen:
The first choice at the top of the page is required.
You pick whether you want to choose a field type directly by its name, or if you want to indirectly choose one based on the name of a field.
In this example, I'm choosing the text_en_splitting field type that has some interesting text analysis.
This tool is mainly for the text oriented field types, not boolean, date, and numeric oriented types.
At this point you can analyze index or query text or both at the same time.
You activate that analysis by putting some text into the text box; otherwise it won't do that phase.
If you are troubleshooting why a particular query isn't matching a particular document's field value, then you'd put the field value into the Index box and the query text into the Query box.
Technically that might not be the same thing as the original query string, because the query string may use various operators to target specified fields, do fuzzy queries, and so on.
You will want to check off verbose output to take full advantage of this tool.
The highlight matches option is applicable when you are doing both query and index analysis together and want to see matches in the index part of the analysis corresponding with a query.
The output after clicking on the Analyze button is a bit verbose with verbose output checked and so I've disabled it for this upcoming screenshot.
Each row shown represents one step in the chain of processing as configured in the analyzer.
Columns separate the tokens, and if more than one token shares the same column, then they share the same term position.
The distinction of the term position pertains to how phrase queries work.
One interesting thing to notice about the analysis results is that Quoting ultimately became quot after stemming and lowercasing.
Also, the word and was omitted by the StopFilter which is the second row.
Character filters Character filters, declared with the <charFilter> element, process a stream of text prior to tokenization.
This feature is not commonly used except for the first one described here which is configured to strip accents.
For further details on the characters mapped, read the comments at the top of the file.
This analysis component and quite a few others have an attribute in which you specify a configuration file.
Usually you can specify more than one file, separated by comma but some components don't support that.
They are always in the conf directory and UTF-8 encoded.
Instead of stripping markup at the analysis stage, which is very late, consider if this should be done at an earlier point with a DataImportHandler transformer, or some other pre-Solr stage.
If you need to retain the markup in Solr's stored value, then you will indeed need to perform this step here.
The regular expression specification supported by Solr is the one that Java uses.
Tokenization A tokenizer is an analysis component declared with the <tokenizer> element that takes text in the form of a character stream and splits it into so-called tokens, most of the time skipping insignificant bits like whitespace and joining punctuation.
As of Solr 3, the former StandardTokenizer was renamed to ClassicTokenizer and likewise StandardFilter was renamed to ClassicFilter.
Additionally, there is a ClassicFilter token filter that is usually configured to follow this tokenizer.
It will strip the periods out of acronyms and remove any trailing apostrophes (English possessive)
This example would be good for a semi-colon separated list.
To match only particular patterns and possibly use only a subset of the pattern as the token.
The group attribute specifies which matching group will be the token.
If you had input text like aaa 'bbb' 'ccc', then this would result in tokens bbb and ccc.
There are some other tokenizers that exist for languages such as Chinese and Russian, as well as the ICUTokenizer which detects the language (or "script") used and tokenizes accordingly.
See http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters for more information on some of these tokenizers, or the API documentation.
This analysis component is the most configurable of all and it can be a little confusing.
Use Solr's ANALYSIS screen that is described in the Experimenting with text analysis section to validate your configuration.
At this point, the resulting terms are all filtered out unless some of the following options are enabled.
Since they all default to false, you would always enable at least one of them.
Internally, this filter assigns a type to each character (like letter, number) before looking for word boundaries.
If you want to customize how the filter determines what the type of each character is, then you can provide one or more mapping files with the types option.
Lastly, if there are a certain limited number of known input words that you want this filter to skip (that is pass through), then they can be listed in a file referred to with the protected option.
Stemming Stemming is the process for reducing inflected or sometimes derived words to their stem, base, or root form.
For example, a stemming algorithm might reduce Riding and Rides, to just Ride.
Stemming is done to improve search result recall, but at the expense of some precision.
If you are processing general text, then you will improve your search results with stemming.
However, if you have text that is mostly proper nouns, such as an artist's name in MusicBrainz, then anything more than light stemming will hurt the results.
If you want to improve the precision of search results but retain the recall benefits, then you should consider indexing the data in two fields, one stemmed and one not, and then perform searches over both fields.
Many stemmers will generate stemmed tokens that are not correctly spelled words, like Bunnies becoming Bunni instead of Bunny or stemming Quote to Quot; you'll see this in Solr's analysis screen.
It includes information on a Solr token filter that performs decompounding, which is useful for certain languages (not English)
Correcting and augmenting stemming These stemmers are algorithmic instead of being based on a vetted thesaurus for the target language.
Languages have so many spelling idiosyncrasies that algorithmic stemmers are imperfect—they sometimes stem incorrectly or don't stem when they should.
Some stemmers have or used to have a protected attribute that worked similarly, but that old approach isn't advised any more.
It takes a dictionary attribute referring to a UTF8-encoded file in the conf directory of token pairs, one pair per line, and a tab is used to separate the input token from the output token (the desired stemmed form of the input)
Here is a sample excerpt of an analyzer chain showing three filters in support of stemming:
Someone searches using a word that wasn't in the original document but is synonymous with a word that is indexed, so you want that document to match the query.
Of course, the synonym need not be strictly those identified by a thesaurus, and they can be whatever you want including terminology specific to your application's domain.
Solr 3.4 adds the ability to read WordNet's "prolog" formatted file via a format="wordnet" attribute on the synonym filter.
However don't be surprised if you lose precision in the search results—it's not a clear win.
For example, "Craftsman" in context might be a proper noun referring to a brand, but WordNet would make it synonymous with "artisan"
Synonym processing doesn't know about context; it's simple and dumb.
Here is a sample analyzer configuration line for synonym processing:
The synonyms reference is to a file in the conf directory.
Here is a sample line with an explicit mapping that uses the arrow =>:
This means that if either i-pod (one token) or i then pod (two tokens) are found in the incoming token stream to this filter, then they are replaced with ipod.
There could have been multiple replacement synonyms, each of which might contain multiple tokens.
Also notice that commas are what separates each synonym which is then split by whitespace for multiple tokens.
To customize the tokenization to be something more sophisticated than whitespace, there is a tokenizerFactory attribute but it's rarely used.
These lines don't have a => and are interpreted differently according to the expand parameter.
If expand is true, then it is translated to this explicit mapping:
If expand is false then it becomes this explicit mapping, in which the first source synonym is the replacement synonym:
It's okay to have multiple lines that reference the same synonyms.
If a source synonym in a new rule is already found to have replacement synonyms from another rule, then those replacements are merged.
Multi-word (aka Phrase) synonyms For multi-word synonyms to work, the analysis must be applied at indextime and with expansion so that both the original words and the combined word get indexed.
Also, be aware that the tokenizer and previous filters can affect the tokens that the SynonymFilter sees.
So, depending on the configuration, hyphens, other punctuations may or may not be stripped out.
Index-time versus query-time, and to expand or not If you are doing synonym expansion (have any source synonyms that map to multiple replacement synonyms or tokens), then do synonym processing at either index-time or query-time, but not both.
Doing it in both places would yield correct results but would perform slower.
I recommend doing it at index-time because of these problems with doing it at query time:
However, any analysis at index-time is less flexible, because any changes to the synonyms will require a complete re-index to take effect.
Moreover, the index will get larger if you do index-time expansion—perhaps too large if you have a large set of synonyms such as with WordNet.
It's plausible to imagine the issues above being rectified at some point.
This means that for a given synonym token, there is just one token that should replace it.
This requires processing at both index-time and query-time to effectively normalize the synonymous tokens.
However, since there is query-time processing, it suffers from the problems mentioned above (with the exception of poor scores, which isn't applicable)
The benefit to this approach is that the index size would be smaller, because the number of indexed tokens is reduced.
You might also choose a blended approach to meet different goals.
For example, if you have a huge index that you don't want to re-index often, but you need to respond rapidly to new synonyms, then you can put new synonyms into both a query-time synonym file and an index-time one.
When a re-index finishes, you empty the query-time synonym file.
You might also be fond of the query-time benefits, but due to the multiple word token issue, you decide to handle those particular synonyms at index-time.
Stop words There is a simple filter called StopFilterFactory that filters out certain so-called stop words specified in a file in the conf directory, optionally ignoring case.
When used, it is present in both index and query analyzer chains.
For indexes with lots of text, common uninteresting words like "the", "a", and so on, make the index large and slow down phrase queries that use them.
A simple solution to this problem is to filter them out of fields where they show up often.
Fields likely to contain more than a sentence are ideal candidates.
The trade-off when omitting stop words from the index is that those words are no longer queryable.
This is usually fine, but in some circumstances like searching for To be or not to be, it is obviously a problem.
The ideal solution to the common word problem is not to remove them.
Solr comes with a decent set of stop words for the English language.
You may want to supplement it or use a different list altogether if you're indexing non-English text.
In order to determine which words appear commonly in your index, access the SCHEMA BROWSER menu option in Solr's admin interface.
A list of your fields will appear on the left.
In case the list does not appear at once, be patient.
For large indexes, there is a considerable delay before the field list appears because Solr is analyzing the data in your index.
Now, choose a field that you know contains a lot of text.
In the main viewing area, you'll see a variety of statistics about the field including the top-10 terms appearing most frequently.
Phonetic sounds-like analysis Another useful text analysis option to enable searches that sound like a queried word is phonetic translation.
A filter is used at both index and query-time that phonetically encodes each word into a phoneme.
There are four phonetic encoding algorithms to choose from: Caverphone, DoubleMetaphone, Metaphone, RefinedSoundex, and Soundex.
Anecdotally, DoubleMetaphone appears to be the best, even for non-English text.
However, you might want to experiment in order to make your own choice.
RefinedSoundex declares itself to be most suitable for spell check applications.
However, Solr can't presently use phonetic analysis in its spell check component (described in a later chapter)
Solr has three tools for more aggressive inexact searching: phonetic sounds-like, query spellchecking, and fuzzy searching.
The following is a suggested configuration for phonetic analysis in the schema.xml:
Note that the encoder options internally handle both upper and lower case.
In the MusicBrainz schema that is supplied with the book, a field named a_phonetic is declared to use this field type, and it has the artist name copied into it through a copyField directive.
In Chapter 4, Searching you will read about the dismax query parser that can conveniently search across multiple fields with different scoring boosts.
This is not supposed to be meaningful, but it is useful for comparing similar spellings to detect its effectiveness.
In order to use one of the other three phonetic encoding algorithms, you must use this filter:
The encoder attribute must be one of those algorithms listed in the first paragraph of this section.
Substring indexing and wildcards Usually, text indexing technology is employed to search entire words.
Occasionally however, there arises a need for a query to match an arbitrary substring of an indexed word or across them.
Solr supports wildcards on queries (for example: mus*ainz) but there is some consideration needed in the way data is indexed.
It's useful to first get a sense of how Lucene handles a wildcard query at the index level.
Lucene internally scans the sorted terms list on disk starting with the nonwildcard prefix (mus in the previous example)
One thing to note about this is that the query takes exponentially longer for each fewer prefix character.
In fact Solr configures Lucene to not accept a leading wildcard to ameliorate the problem.
Another thing to note is that stemming, phonetic, and other non-trivial text analysis will interfere with these kinds of searches.
For example, if running is stemmed to run, then runni* would not match.
Before employing these approaches, consider if what you really need is better tokenization for special codes.
Doing this will also improve query performance when the wildcard is very close to the front.
The following example configuration would appear at the end of the index analyzer chain:
It has several performance-tuning options you can investigate further at its JavaDocs, but the defaults are reasonable.
Solr does not support a query with both a leading and trailing wildcard, for performance reasons.
Given my explanation of the internals, I hope you understand why.
Wildcard queries can be slow, even if you use this reversing filter.
If they are still too slow, consider looking at the next major release of Solr, v4.x (which is "trunk" in source control as of this writing) that contains some amazing performance improvements in this area.
For still further ways to increase performance, read on to learn about n-grams.
N-grams N-gram analysis slices text into many smaller substrings ranging between a minimum and maximum configured size.
Note that Tonight fully does not pass through because it has more characters than the maxGramSize.
The following is a suggested analyzer configuration using n-grams to match substrings:
This analysis would be applied to a field created solely for the purpose of matching substrings.
Another field would exist for typical searches, and the dismaxquery parser, described in Chapter 4, Searching would be configured for searches to use both fields using a smaller boost for this field.
For the filter-factory, this input-text is a term, and the tokenizer is the entire input.
In addition to minGramSize and maxGramSize, these analyzers take a side argument that is either front or back.
If only prefix or suffix matching is needed instead of both, then an EdgeNGram analyzer is for you.
N-gram costs There is a high price to be paid for n-gramming.
Recall that in the earlier example, Tonight was split into 15 substring terms, whereas typical analysis would probably leave only one.
This translates to greater index sizes, and thus a longer time to index.
Let's look at the effects of this in the MusicBrainz schema.
The a_name field, which contains the artist name, is indexed in a typical fashion and is stored.
It is not a stored field because the artist's name is already stored in a_name.
Note the ten-fold increase in indexing time for the artist name, and a five-fold increase in disk space.
Given these costs, n-gramming, if used at all, is generally only done on a field or two of small size where there is a clear requirement for substring matches.
The costs of n-gramming are lower if minGramSize is raised and to a lesser extent if maxGramSize is lowered.
This is because it is only based on one side.
It definitely costs more to use the tokenizer-based n-grammers instead of the term-based filters used in the example before, because terms are generated that include and span whitespace.
However, with such indexing, it is possible to match a substring spanning words.
Sorting Text Usually, search results are sorted by relevancy via the magic score pseudo-field, but it is common to need to support conventional sorting by field values too.
And in addition to sorting search results, there are ramifications to this discussion in doing a range query and when showing facet results in sorted order.
Sorting limitations: A field needs to be indexed, not be multivalued, and for text it should not have multiple tokens (either there is no text analysis or it yields just one token)
It just happens that MusicBrainz already supplies alternative artist and label names for sorting.
When different from the original name, these sortable versions move words like "The" from the beginning to the end after a comma.
We've marked the sort names as indexed but not stored since we're going to sort on it but not display it—deviating from what MusicBrainz does.
Because of the special text analysis restrictions of fields used for sorting, text fields in your schema that need to be sortable will usually be copied to another field and analyzed differently.
The string type is a type that has no text analysis and so it's perfect for our MusicBrainz case.
As we're getting a sort-specific value from MusicBrainz, we don't need to derive something ourselves.
However, note that in the MusicBrainz schema there are no sort-specific release names.
We could opt to not support sorting by release name, but we're going to anyway.
That's fine, but you may want to lowercase the text, remove punctuation, and collapse multiple spaces into one (if the data isn't clean)
For the sake of variety in our example, we'll be taking the latter route; we're using a type title_sort that does these kinds of things.
By the way, Lucene sorts text by the internal Unicode code point.
You probably won't notice any problem with the sort order.
Since it isn't commonly used and it's already well documented, I'll refer you to the wiki: http://wiki.apache.org/solr/ UnicodeCollation.
If you want to ensure a certain vocabulary of words in a special field, then you might enforce it with this.
This example is for processing an e-mail address field to get only the domain of the address.
This replacement happens to be a reference to a regular expression group, but it might be any old string.
If the replace attribute is set to first, then only the first match is replaced; if replace is all, the default, then all matches are replaced.
There are some other miscellaneous Solr filters I didn't mention for various reasons.
Summary At this point, you should have a schema that you believe will suit your needs—for now anyway.
It is quite normal to start with something workable, and then subsequently make modifications to address issues, and implement features that require changes.
The only irritant with changing the schema is that you probably need to re-index all of the data.
The only exception to this would be an analysis step applied only at query-time.
In the next chapter, you'll learn about the various ways to import data into the index.
Indexing Data In this chapter we're going to explore ways to get data into Solr.
The process of doing this is referred to as indexing, although importing is used too.
You will also find some related options in Chapter 9, Integrating Solr that have to do with language bindings and framework integration, including a web crawler.
In a hurry? There are many approaches to get data into Solr and you don't need to be well versed in all of them.
The section on commit and optimize is important for everyone because it is universal.
If you plan to use a Solr integration framework that handles indexing data, such as Sunspot for Ruby on Rails, then you can follow the documentation for that framework and skip this chapter for now.
Communicating with Solr There are quite a few options in communicating with Solr to import data.
In this section we'll look at a few choices to be made, and then follow up with interaction examples.
Details on specific formats such as Solr's Update-XML comes later.
This can either be done directly using any HTTP client API of your choice, or indirectly via a Solr integration API such as SolrJ or Sunspot that will handle the HTTP interaction details.
This HTTP Solr interaction doesn't imply that the data to be indexed needs to move over this channel; you will learn soon that you can tell Solr to fetch the data.
Another option is to embed Solr into your Java application instead of running it as a server.
The SolrJ API is conveniently used for both remote and embedded use.
Push data to Solr or have Solr pull it Even though an application will be communicating with Solr over HTTP, it does not have to send Solr documents over this channel.
Solr supports what it calls remote streaming in which it's given a URL to the data.
It might be an HTTP URL, but more likely it is a file-system based URL, applicable when the data is already on Solr's machine or network drive.
Another way to ask Solr to pull data is to use the DataImportHandler (DIH) which can pull data from a database and other sources.
The DIH offers an extensible framework that can be adapted to custom data sources.
Data formats The following are various data formats for indexing data into Solr:
Other XML (Solr 3.4): Any arbitrary XML can be given to Solr along with an XSLT file  that Solr will use to translate the XML to the Update-XML format for further processing.
There is a short example of this in the DIH section, by way of comparison.
The DataImportHandler contrib module is a flexible data importing framework with out-of-the-box supports for importing arbitrary XML formats and e-mail, via the IMAP protocol.
It is best known for pulling data from a relational database, though that isn't really a format, per-se.
We'll demonstrate Solr's capability to import MusicBrainz data in XML, CSV, and from a database.
Other examples will include rich document import both via the DIH to crawl files and via Solr Cell.
Before these approaches are described, we'll discuss cURL and remote streaming, which are foundational topics.
Solr lets you use HTTP GET too, such as direct web browser access.
However, this is an inappropriate HTTP verb for anything other than retrieving data.
One way to send an HTTP POST is through the Unix command line program curl (also available on Windows through Cygwin: http://www.cygwin.com) and that's what we'll use here in the examples.
To get some basic help on how to use it, run the following command:
You'll see in a bit that you can post name-value pair options as HTML form data.
However, post.jar doesn't support that, so you'll be forced to specify the URL and put the options in the query string.
There are several ways to tell Solr to index data, and all of them are through HTTP POST:
If it's small, perhaps less than a megabyte, then this approach is fine.
If you're tempted to increase this limit, you should reconsider your approach.
Refer to the data through either a local file on the Solr server using the stream.file parameter or a URL that Solr will fetch through the stream.url parameter.
These choices are a feature that Solr calls remote streaming.
Let's say we have a Solr Update-XML file named artists.xml in the current directory.
We can post it to Solr using the following command line:
If it succeeds, then you'll have output that looks like this:
To use the stream.body feature for the preceding example, you would do this:
If the XML is short, then you can just as easily specify it literally on the command line:
In this case, it might be more appropriate to use form-string instead of -F.
Remote streaming In the preceding examples, we've given Solr the data to index in the HTTP message.
Alternatively, the POST request can give Solr a pointer to the data in the form of either a file path accessible to Solr or an HTTP URL to it.
The file path is accessed by the Solr server on its machine, not the client, and it must also have the necessary operating system file permissions too.
Just as before, the originating request does not return a response until Solr has finished processing it.
If the file is of a decent size or is already at some known URL, then you may find remote streaming faster and/or more convenient, depending on your situation.
Here is an example of Solr accessing a local file:
To use a URL, the parameter would change to stream.url, and we'd specify a URL.
We're passing a name-value parameter (stream.file and the path), not the actual data.
This can be considered a security risk; so only turn it on if Solr is protected.
Solr's Update-XML format Using an XML formatted message, you can supply documents to be indexed, tell Solr to commit changes, to optimize the index, and to delete documents.
Here is a sample XML file you can HTTP POST to Solr that adds (or replaces) a couple documents:
If you want to send multiple XML-based commands to Solr in the same message/file, then you can use an arbitrarily named root element to contain the XML elements Solr understands.
The overwrite attribute defaults to true to guarantee the uniqueness of values in the field that you have designated as the unique field in the schema, assuming you have such a field.
If you were to add another document that has the same value for the unique field, then this document would overwrite the previous document.
If you are sure that you will be adding a document that is not a duplicate, then you can set overwrite to false to get a small performance improvement since Solr won't check uniqueness of the unique key field.
The boost attribute affects the scores of matching documents in order to affect ranking in score-sorted search results.
Providing a boost value, whether at the document or field level, is optional.
The default value is 1.0, which is effectively a non-boost.
The effective boost value of a field is that specified for the document multiplied by that specified for the field.
Specifying boosts here is called index-time boosting, which is rarely done as compared to the more flexible query-time boosting.
Index-time boosting is less flexible because such boosting decisions must be decided at index-time and will apply to all of the queries.
You'll learn more about boosting and scoring in Chapter 5, Search Relevancy.
Deleting documents You can delete a document by its unique field.
To more flexibly specify which documents to delete, you can alternatively use a Lucene/Solr query:
The contents of the delete tag can be any number of id and query tags if you want to batch many deletions into one message to Solr.
Since we haven't gotten to that yet, I'll explain the preceding example.
Let's suppose that all of your documents had a timestamp field with a value of the time it was indexed, and you have an update strategy that bulk loads all of the data on a daily basis.
If the loading process results in documents that shouldn't be in the index anymore, then we can delete them immediately after a bulk load.
This query would delete all of the documents not indexed within the last 12 hours.
Twelve was chosen somewhat arbitrarily, but it needs to be less than 24 (the update process interval) and greater than the longest time it might conceivably take to bulk-load all the data.
If you want to delete the entire index in the course of development (or perform major schema changes in production), then simply delete the data directory while Solr is shut down.
Commit, optimize, and rollback Data sent to Solr is not immediately searchable, nor do deletions take immediate effect.
The easiest way to do this is to add a commit=true request parameter to a Solr update URL.
The request to Solr could be the same request that contains data to be indexed then committed or an empty request—it doesn't matter.
For example, you can visit this URL to issue a commit on our mbreleases core: http://localhost:8983/solr/mbreleases/ update?commit=true.
You can also commit changes using the XML syntax by simply sending this to Solr:
There are three important things to know about commits that are unique to Solr:
When you are bulk loading data, these concerns are not an issue since you're going to issue a final commit at the end.
But if Solr is asynchronously updated by independent clients in response to changed data, commits could come too quickly and might overlap.
To address this, Solr has two similar features, autoCommit and commitWithin.
The first refers to a snippet of XML configuration commented in solrconfig.xml in which Solr will automatically commit at a document-count threshold or time-lapse threshold (time of oldest uncommitted document)
In this case, Solr itself handles committing and so your application needn't send commits.
It will ensure a commit occurs within the specified number of milliseconds.
Also, be careful not to pick a time window shorter than how long a commit takes since then commits will start to overlap, which is very bad! Look for a future version of Solr to have what's called near realtime search—a much sought after feature.
This will make commits cheap, thereby enabling Solr to be updated asynchronously when data changes and have it be searchable almost immediately.
Lucene's index is internally composed of one or more segments.
When a buffer of indexed documents gets flushed to disk it creates a new segment.
Deletes get recorded in another file, but they go to disk too.
Sometimes, after a new segment is written, Lucene will merge some of them together.
When Lucene has just one segment, it is in an optimized state.
The more segments there are the more query performance degrades.
Of course, optimizing an index comes at a cost; the larger your index is, the longer it will take to optimize.
You can specify an optimize command in all the places you specify a commit.
It is recommended to explicitly optimize the index at an opportune time like after a bulk load of data and/or a daily interval in off-peak hours, if there are sporadic updates to the index.
The performance chapter has a tip on optimizing to more than one segment if the optimizes are taking too long.
Both commit and optimize commands take two additional boolean options that default to true:
If you were to set these to false, then commit and optimize commands return immediately, even though the operation hasn't actually finished yet.
So if you wrote a script that committed with these at their false values and then executed a query against Solr, you may find that the search will not reflect the changes yet.
By waiting for the data to flush to disk (waitFlush) and waiting for a new searcher to be ready to respond to changes (waitSearcher), this circumstance is avoided.
These options are useful for executing an optimize command from a script that simply wants to optimize the index and otherwise doesn't care when newly added data is searchable.
No matter how long a commit or optimize command takes, Solr still executes searches concurrently—there is no read lock.
All uncommitted changes can be cancelled by sending Solr the rollback command either via a URL parameter such as: http://localhost:8983/solr/mbreleases/update?rollback=true or with this XML:
Sending CSV formatted data to Solr If you have data in a CSV format or if it is more convenient for you to get CSV than XML or JSON, then you may prefer the CSV option.
You won't be able to specify an index-time boost but that's an uncommon need.
As such, you can query for CSV formatted data that is suitable to be added right back into Solr (for stored fields only, of course)
The XML and JSON query output formats are structured differently than their input formats so they don't count.
To get some CSV data out of a local PostgreSQL database for the MusicBrainz tracks, I ran this command:
And it generated about 7 million lines of output that looks like this (first three lines):
This CSV file is provided with the code supplement to the book.
To get Solr to import the CSV file, type this at the command line:
The CSV options were specified via form values (-F) here; you can alternatively encode them into the query portion of the URL—it doesn't matter.
When I actually did this I had PostgreSQL on one machine and Solr on another.
This way, I didn't have to actually generate a huge CSV file.
I could essentially stream it directly from PostgreSQL into Solr.
Details on this approach and PostgreSQL are out of the scope of this book.
Configuration options The following are the names of each configuration option with an explanation.
For the MusicBrainz track CSV file, the defaults were used with the exception of specifying how to parse the multi-valued t_r_attributes field and disabling unique key processing for performance.
If you're using curl and need to specify a tab character or some other character that isn't visible other than a space, then the easiest way to do this is to specify this parameter on the URL as a query parameter instead of with -F.
If no name is specified for a column, then its data is skipped.
Solr already does an initial pass trim, but quoting may leave spaces.
The Data Import Handler Framework Solr includes a very popular contrib module for importing data known as the DataImportHandler (DIH in short)
Furthermore, you could write your own data source or transformation step once you learn how by seeing how the existing ones are coded.
Consider DIH alternatives The DIH's capabilities really have little to do with Solr itself yet the DIH is tied to Solr (to a Solr core, to be precise)
Alternatives can run on another machine to reduce the load on Solr when there is significant processing involved.
And in being agnostic of where the data is delivered, your investment in them can be re-used for other purposes independent of Solr.
With that said, the DIH is a strong choice because it is integrated with Solr and it has a lot of capabilities.
The complete reference documentation for the DIH is here: http://wiki.apache.
In this chapter I'll demonstrate some of its features but you'll need to turn to the wiki for further details.
Setup The DIH is not considered a core part of Solr, even though it comes with the Solr download, and so you must add its Java JAR files to your Solr setup to use it.
The easiest way to add JAR files to a Solr configuration is to copy them to the <solr_home>/ lib directory; you may need to create it.
Another method is to reference them from solrconfig.xml via <lib/> tags—see Solr's example configuration for examples of that.
You will most likely need some additional JAR files as well.
If you'll be communicating with a database, then you'll need to get a JDBC driver for it.
The DIH needs to be registered with Solr in solrconfig.xml like so:
The development console Before describing a DIH configuration file, we're going to take a look at the DIH development console.
Visit this URL (modifications may be needed for your host, port, core, and so on):
If there is more than one request handler registered, then you'll see a simple page listing them with links to continue to the development console for that handler.
The screen is divided into two panes: on the left is the DIH control form and on the right is the command output as raw XML.
The editable configuration is not saved to disk! It is purely for live trialand-error debugging.
Once you are satisfied with any changes, you'll need to save them back to the file yourself and then take some action to get Solr to reload the changes, such as by clicking on the Reload Config button, and then reload the page to pick up the changes on the screen.
Furthermore, only the Debug Now button uses this text; not the buttons at the bottom.
The last section on DIH in this chapter goes into more detail on submitting a command to the DIH.
Writing a DIH configuration file The key pieces of a DIH configuration file include a data source, an entity, some transformers, and a list of fields.
There can be variable numbers of these things and sometimes they can be omitted.
At first I'll list the various types of each of these DIH components with a simple description.
Each has further details on usage that you'll need to see the wiki for.
Then I'll show you a few sample configuration files to give you a sense of how it all comes together.
Data Sources A <dataSource/> specifies, as you might guess, the source of data referenced by an entity.
The type attribute specifies the type, which defaults to JdbcDataSource.
Depending on the type, there are further configuration attributes (not listed here)
Furthermore, with the exception of JdbcDataSource, each type handles either binary or text but not both.
The following is a listing of available data source types.
It could be used for many things, even a Web Hook: http://www.webhooks.org/
If you were looking for a MailDataSource, then there isn't any.
The data to produce the documents typically comes from a referenced data source.
An entity that is an immediate child of <document> is by default a root entity, which means its documents are indexed by Solr.
If the rootEntity attribute is explicitly set to false, then the DIH recursively traverses down until it finds one that doesn't have this marking.
There can be sub-entities, which execute once for each parent document and which usually reference the parent document to narrow a query.
Documents from a sub-entity are merged into its root entity's document, producing multi-valued fields when more than one document with the same field is produced by the sub-entity.
This explanation is surely quite confusing without having seen several examples.
You may want to read this again once you get to some examples.
The entity processors have some common configuration attributes and some that are unique to each one.
This allows you to specify how many concurrent Solr indexing threads are used, in order to index documents faster.
The field element must have a column attribute that matches the corresponding named column in the SQL query.
The name attribute is the Solr schema field name that the column is going into.
If it is not specified, then it defaults to the column name.
When a column in the result can be placed directly into Solr without further processing, there is no need to specify the field declaration, because it is implied.
When importing from a database, use the SQL AS keyword to use the same names as the Solr schema instead of the database schema.
This reduces the number of <field/> elements and shortens existing ones.
An attribute of the entity declaration that we didn't mention yet is transformer.
This declares a comma-separated list of transformers that create, modify, and delete fields and even entire documents.
Usually the transformers use attributes specific to them on a given field to trigger that it should take some action, whether it be splitting the field into multiple values or formatting it or whatever.
The ScriptTransformer is powerful! You should certainly use the other transformers as appropriate, but there is nearly nothing you can't do with this one.
Your script function can emit multiple records by returning an array of them, and it can omit a record by returning null.
You may want to consider testing your script by using a separate script file and writing unit tests for it.
The former operates strictly within the DIH framework whereas the latter is applicable to any importing mechanism.
Example DIH configurations A DIH configuration file tends to look different depending on whether the source is a database, the content is XML, or if text is being extracted from documents.
It's important to understand that the various data sources, data formats, and transformers, are mostly independent.
The next few examples pick combinations to demonstrate a variety of possibilities for illustrative purposes.
If the type attribute on dataSource is not specified (it isn't here) then it defaults to JdbcDataSource.
Those familiar with JDBC should find the attributes in this example familiar, and there are others available.
For a reference to all of them, see the wiki.
Many database drivers in the default configurations (including those for PostgreSQL and MySQL) fetch all of the query results into memory instead of on-demand or using a batch/fetch size! This may work well for typical database usage in which a relatively small amount of data needs to be fetched quickly, but is completely unworkable for ETL (Extract Transform and Load) usage such as this.
Configuring the driver to stream the data will sometimes require driver-specific configuration settings.
The main piece of an <entity/> used with a database is the query attribute, which is the SQL query to be evaluated.
You'll notice that this query involves some sub-queries, which are made into arrays and then transformed into strings joined by spaces.
The particular functions used to do these sorts of things are generally database specific.
This is done to shoehorn multi-valued data into a single row in the results.
It may create a more complicated query, but it does mean that the database does all of the heavy lifting so that all of the data Solr needs for an artist is in one row.
Sub-entities There are numerous examples on the DIH wiki depicting entities within entities (assuming the parent entity is a root entity)
This is an approach to the problem of getting multiple values for the same Solr field.
I advise caution against that approach because it will generate a separate query in response to each source record, which is very inefficient.
It can be told to cache just one query to be used for future lookups but that is only applicable to data shared across records that can also fit in memory.
If all required data is in your database, I recommend the approach illustrated above instead.
Importing XML from a file with XSLT In this example, we're going to import an XML file from disk and use XSLT to do most of the work instead of DIH transformers.
Solr 3.4 added direct support for using XSLT to process input XML without requiring use of the DIH as we show in this simple example.
The entity URL is relative to the baseUrl on the data source; since it's not specified then it defaults to the current working directory of the server.
The referenced XSLT file is relative to the current working directory instead of the conf directory—a known bug: SOLR-1226
To see the referenced XSLT file, download the code supplement for the book.
Our input file is an HTML table and the XSLT file transforms it.
There are some other examples at the DIH wiki illustrating XML processing.
One of them shows how to process a Wikipedia XML file dump, which is rather interesting.
Importing multiple rich document files (crawling) In this example, we have a configuration that crawls all PDF files in a directory and then extracts text and metadata from them.
Speaking of which, there are a variety of variables that the DIH makes available for substitution, including those defined in solr.xml and solrconfig.xml.
Tika makes a variety of metadata available about documents; this example just used two.
Importing commands The DIH is issued one of several different commands to do different things.
Importing all data is called a full import, in contrast to a delta import that will be described shortly.
Commands are given to the DIH request handler with the command attribute.
We could tell the DIH to do a full import just by going to this URL: http://localhost:8983/solr/mbartists/dataimport?command=fullimport.
It uses HTTP POST, which is more appropriate than GET as discussed earlier.
Unlike the other importing mechanisms, the DIH returns an HTTP response immediately while the import continues asynchronously.
To get the current status of the DIH, go to this URL http://localhost:8983/solr/mbartists/dataimport, and you'll get output like the following:
The command attribute defaults to status, which is what this output shows.
When an import is in progress, it shows statistics on that progress along with a status state of busy.
Other boolean parameters named clean, commit, and optimize may accompany the command, and they all default to true.
Beware that these defaults are inconsistent with other Solr importing mechanisms.
No other importing mechanism will delete all documents first, and none will commit or optimize by default.
The first will reload the DIH configuration file, which is useful for picking up changes without having to restart Solr.
Delta imports The DIH supports what it calls a delta import, which is a mode of operation in which only data that has changed since the last import is retrieved.
A delta import is only supported by the SqlEntityProcessor and it assumes that your data is timestamped.
The official DIH approach to this is prominently documented on the wiki.
It uses a deltaImportQuery and deltaQuery pair of attributes on the entity, and a delta-import command.
Essentially, what you do is introduce a timestamp check in your SQL's WHERE clause using variable substitution, along with another check if the clean parameter was given to the DIH in order to control whether or not a delta or full import should happen.
Here is a concise <entity/> definition on a fictitious schema and data set showing the relevant WHERE clause:
Indexing documents with Solr Cell While most of this book assumes that the content you want to index in Solr is in a neatly structured data format of some kind, such as in a database table, a selection of XML files, or CSV, the reality is that we also store information in the much messier world of binary formats such as PDF, Microsoft Office, or even images and music files.
Your author Eric Pugh first became involved with the Solr community when he needed to ingest the thousands of PDF and Microsoft Word documents that a client had produced over the years.
The outgrowth of that early effort is Solr Cell providing a very powerful and simple framework for indexing rich document formats.
The current name came about as a derivation of "Content Extraction Library" which appeared more fitting to its author, Grant Ingersoll.
Perhaps a name including Tika would have been most appropriate considering that this capability is a small adapter to Tika.
We'll look at how to leverage Solr Cell for extracting karaoke song lyrics from MIDI files.
Just think you can build a Solr powered index of all your favorite karaoke songs! The complete reference material for Solr Cell is available at http://wiki.
Extracting text and metadata from files Every file format is different, and all of them provide different types of metadata, as well as different methods of extracting content.
The heavy lifting of providing a single API to an ever expanding list of formats is delegated to Apache Tika:
Apache Tika is a toolkit for detecting and extracting metadata and structured text content from various documents using existing parser libraries.
Tika supports a wide variety of formats, from the predictable to the unexpected.
Other formats that are supported include extracting metadata from images such as JPG, GIF, and PNG, as well as from various audio formats such as MP3, MIDI, and Wave audio.
Tika itself does not attempt to parse the individual document formats.
Instead, it delegates the parsing to various third-party libraries, while providing a high level stream of XML SAX events as the documents are parsed.
Solr Cell is a fairly thin adapter to Tika consisting of a SAX ContentHandler that consumes the SAX events and builds the input document from the fields that are specified for extraction.
Some not so obvious things to keep in mind when indexing binary documents are:
You can learn more about the Tika project at http://tika.apache.org/
Here we can see that the Tika metadata attribute Last-Modified is being mapped to the Solr field last_modified, assuming we are provided that Tika attribute.
The parameter uprefix is specifying the prefix to use when storing any Tika fields that don't have a corresponding matching Solr field.
The JAR files placed in this lib directory are available only to the karaoke core.
Solr Cell parameters Before jumping into examples, we'll review Solr Cell's configuration parameters, all of which are optional.
They are organized below and ordered roughly by when they are used.
At first, Solr Cell (or more specifically Tika) determines the format of the document.
It generally makes good guesses, but it can be assisted with these parameters:
Tika converts all input documents into a basic XHTML document, including metadata in the head section.
The metadata becomes fields and all text within the body goes into the content field.
At this point each resulting field name is potentially renamed in order to map into the schema.
The purpose of this option is to aid in debugging.
Extracting karaoke lyrics We are now ready to extract karaoke lyrics by posting MIDI files to our Solr / update/extract request handler.
In order to index the song Angel Eyes from the command line using curl, the simplest command to run is:
You could trigger a commit in the same request that submits the document with the commit=true query parameter; however, this is very inefficient if you are indexing many documents!
You've now indexed information about the song and the lyrics in the text field that forms the textual content of the MIDI file.
However, what about the metadata, for the MIDI file that Tika also exposes? Well, this is where dynamic fields come in very handy.
Every binary format has a set of metadata that to a varying extent overlaps with other formats.
Fortunately, it is very easy to specify to Solr Cell how you would want to map metadata fields by using the uprefix property.
We specify that all of the metadata_* fields should be created using dynamic fields in schema.xml:
Since handling metadata properly is something we want to standardize on, we add to the configuration element in solrconfig.xml:
If you know ahead of time some of the metadata fields and have a named field for them, then you can just map them:
Obviously, in most use cases, every time you index the same file you don't want to index a separate Solr document.
If your schema has a uniqueKey field defined such as id, then you can provide a specific ID by passing a literal value using literal.
Each time you index the file using the same ID it will delete and insert that document.
However, that implies that you have the ability to manage IDs through some third-party system like a database.
Indexing richer documents Indexing karaoke lyrics from MIDI files is a fairly trivial example.
We basically just stripped out all of the contents, and stored them in the Solr text field without trying to make any sense of the structure of the document.
However, the structure of other documents, such as PDFs, can be much more complicated, and just grabbing all the text may not lead to great searchable content.
Let's look at Take a Chance on Me, a complex PDF file that explains what a Monte Carlo simulation is, while making lots of puns about the lyrics and titles of various songs by ABBA!
Despite the complexity of the PDF document, indexing is as simple as the prior karaoke example:
The lowercase=true property that was set as a default translates Last-Modified to last_modified to make field names consistent with typical conventions:
So with these richer documents, how can we get a handle on the metadata and content that is available? Passing extractOnly=true on the URL will output what Solr Cell has extracted as an XML document, including metadata fields, without actually indexing them.
Appending wt=json makes it easier to parse out the embedded XML content:
Copy and paste the XML embedded in the JSON output and use your favorite HTML tidy tool to clean up the output.
I used TextMate's HTML plugin; another great option is the free formatter service at http://xmlindent.com/:
This returns an XHMTL document that contains the metadata extracted from the document in the <head/> stanza, as well as the basic structure of the contents expressed as XHTML.
However, if you extract the contents of the PDF on the client side and only send that over the Web, then what is sent to the Solr text field is just 5.1 KB.
Solr Cell offers a quick way to start indexing the vast amount of information stored in previously inaccessible binary formats without the overhead of resorting to custom code development per binary format.
However, depending on the files, you may be needlessly transmitting a lot of raw data, only to extract a small portion of that data.
Moreover, you may find that the tools provided by Solr Cell for parsing and selecting just the data you want to index may not be rich enough.
In that case you may be better off building a dedicated client-side tool that does all of the parsing and munging you require.
Update request processors No matter how you choose to import data, there is a final configuration point within Solr that allows manipulation of the imported data before it gets indexed.
The Solr request handlers that update data put documents on an update request processor chain.
You can specify which chain to use on the update request with the update.chain parameter (formerly update.processor prior to Solr 3.2)
It could be useful, but you'll probably always use one chain.
The following are the possible update request processors to choose from.
Although it's nice to see an NLP integration option in Solr, beware that NLP processing tends to be computationally expensive.
Instead of using UIMA in this way, consider performing this processing external to Solr and cache the results to avoid re-computation as you adjust your indexing process.
More processors are expected in the future that do interesting tasks, including a scriptable one similar to the DIH's ScriptTransformer; see SOLR-1725
It's a recognized extensibility point in Solr that consequently doesn't require modifying Solr itself.
Summary At this point, you should have a schema that you believe will suit your needs, and you should know how to get your data into it.
From Solr's native XML to JSON to CSV to databases to rich documents, Solr offers a variety of possibilities to ingest data into the index.
Chapter 9, Integrating Solr will discuss some additional language and framework integration choices for importing data.
In the end, usually one or two mechanisms will be used.
In addition, you can usually expect the need to write a little code, perhaps just a simple bash or ant script to implement the automation of getting data from your source system into Solr.
Now that we've got data in Solr, we can finally start searching it.
The next chapter will describe Solr's query syntax in detail, which includes phrase queries, range queries, wildcards, boosting, as well as the description of Solr's DateMath syntax.
The chapters after that will get to more interesting searching topics that of course depend on having data to search on!
Searching At this point, you have Solr running and some data indexed, and you're finally ready to put Solr to the test.
Searching with Solr is arguably the most fun aspect of working with it, because it's quick and easy to do.
While searching your data, you will learn more about its nature than before.
It is also a source of interesting puzzles to solve when you troubleshoot why a search didn't find a document or conversely why it did, or similarly why a document wasn't scored sufficiently high.
The subject of searching will progress into the next chapter for debugging queries, relevancy (that is, scoring) matters, function queries—an advanced capability used commonly in relevancy but also in sorting and filtering, and geospatial search.
In a hurry? This chapter has a lot of key information on searching that is important.
That said, if you're in a hurry you can skim/skip query parsers, local-params, and the query syntax—you'll use dismax instead.
When your application interacts with Solr, it will use HTTP, either directly via common APIs or indirectly through one of Solr's client APIs.
However, as we demonstrate Solr's capabilities in this chapter, we'll use Solr's web-based admin interface.
Surely you've noticed the search box on the first screen of Solr's admin interface.
It's a bit too basic; so instead click on the [FULL INTERFACE] link to take you to a query form with more options.
The following screenshot is seen after clicking on the [FULL INTERFACE] link:
Contrary to what the label FULL INTERFACE might suggest, this form only has a fraction of the options you might possibly specify to run a search.
In the Solr/Lucene Statement box, we'll leave the default of: *:* (an asterisk, colon, and then another asterisk)
That is admittedly cryptic if you've never seen it before, but it basically means "match anything in any field", which is to say, it matches all documents.
Much more about the query syntax will be discussed soon enough.
Click on the Search button, and you'll get output like this:
Browser note Use Firefox for best results when searching Solr.
Solr's search results return XML, and Firefox renders XML color coded and pretty-printed.
For other browsers (notably an older Safari or Chrome), you may find yourself having to use the View Source feature or using a response format other than XML to see the results.
Even in Firefox, however, there are cases where you will use View Source in order to look at the XML with the original indentation, which is relevant when diagnosing the scoring debug output.
Solr's generic XML structured data representation Solr has its own generic XML representation of typed and named data structures.
This XML is used for most of the response XML and it is also used in parts of solconfig.xml too.
The XML elements involved in this partial XML schema are:
The following elements represent simple values with the text of the element storing the value.
They will have a name attribute if they are underneath lst (or an equivalent element like doc), but not otherwise.
Solr's XML response format The <response/> element wraps the entire response.
The contents of the resultant element are a list of doc elements.
Each of these elements represent a document in the index.
The child elements of a doc element represent fields in the index and are named correspondingly.
The types of these elements use Solr's generic data representation, which was described earlier.
They are simple values if they are not multi-valued in the schema.
For multi-valued values, the field would be represented by an ordered array of simple values.
There was no data following the results element in our demonstration query.
However, there can be, depending on the query parameters enabling features such as faceting and highlighting.
When we cover those features, the corresponding XML will be explained.
Parsing the URL The search form is as basic as they come.
It submits the form using HTTP GET, essentially resulting in the browser loading a new URL with the form elements becoming part of the URL's query string.
Take a good look at the URL in the browser page showing the XML response.
Understanding the URL's structure is very important for grasping how searching Solr works:
Text in the URL must be UTF-8 encoded then URL-escaped so that the URL complies with its specification.
This concept should be familiar to anyone who has done web development.
Depending on the context in which the URL is actually constructed, there are API calls you should use to ensure this escaping happens properly.
The most common escaped character in URLs is a space, which is escaped as either + or %20
Fortunately, when experimenting with the URL, browsers are lenient and will permit some characters that should be escaped.
Request handlers Searching Solr and most other interactions with Solr, including indexing for that matter, is processed by what Solr calls a request handler.
Request handlers are configured in the solrconfig.xml file and are clearly labeled as such.
Most of them exist for special purposes like handling a CSV import, for example.
Our searches in this chapter have been directed to the default request handler because we didn't specify one in the URL.
The request handlers that perform searches allow configuration of two things:
Instead of using the default request handler for use by the application you are building, I recommend that you create a request handler for each type of search that your application does.
In doing so, you can change various search options more easily in Solr through re-configuration, instead of having your application hard-wired more than it has to be.
Finally, this gives you better granularity of search statistics on Solr's STATISTICS screen.
Let's say that in the MusicBrainz search interface we have a search form that searches for bands.
We have a Solr core just for artists named mbartists but this contains not only bands but also individual band members.
When the field named a_type is "group", we have a band.
To start, copy the default configuration, removing the attribute default="true", and give it a name such as bands.
We can now use this request handler with qt=bands in the URL as shown below:
An alternative to this is to precede the name with /
Let's now configure this request handler to filter searches to find only the bands, without the searching application having to specify this.
These use Solr's generic XML data structure, which was described earlier.
Query parameters There are a great number of request parameters for configuring Solr searches, especially when considering all of the components like faceting and highlighting.
Only the core search parameters not specific to any query parser are listed here.
Furthermore, in-depth explanations for some lie further in the chapter.
For the boolean parameters, a true value can be any one of true, on, or yes.
False values can be any of false, off, and no.
Search criteria related parameters The parameters affecting the query are as follows:
Prefer dismax for user queries For processing queries from users, I highly recommend using dismax or edismax, which is described later in the chapter.
It supports several features that enhance relevancy, and more limited syntax options that prevent a user from getting unexpected results or an error if they inadvertently use the lucene native syntax.
Result pagination related parameters A query could match any number of the documents in the index, perhaps even all of them, such as in our first example of *:*
Instead, you indicate to Solr with the start and rows parameters to return a contiguous series of them.
Solr is not yet optimized to do "deep paging" or return a large number of results.
Until this is addressed, the further from the beginning you need results (the greater start+rows is), Solr will respond exponentially slower.
At least for the first thousand documents or so, it shouldn't be noticeable.
As a consequence, consider preventing users and web crawlers from paging far into the results.
Output related parameters The output related parameters are explained below:
Diagnostic related parameters These diagnostic parameters are helpful during development with Solr.
Obviously, you'll want to be sure NOT to use these, particularly debugQuery, in a production setting because of performance concerns.
Finally, there is another parameter not easily categorized above called timeAllowed in which you specify a time limit in milliseconds for a query to take before it is interrupted and intermediate results are returned.
Long-running queries should be very rare and this allows you to cap them so that they don't over-burden your production server.
Only a few parsers actually do real parsing and some parsers like those for geospatial don't even use the query string.
The default query parser throughout Solr is named lucene and it has a special leading syntax to switch the parser to another and/or to specify some parameters.
Here's an example choosing the dismax parser along with two local-params and a query string of "billy corgan":
It's not common to see this syntax in the user query, q, since its parser is conveniently set via defType.
There are a few things to know about the local-params syntax:
For processing user queries, you should typically use dismax or edismax (short for extended-dismax), which are described afterwards.
The other query parsers are for special things like geospatial search, also described at the end of this chapter.
This book only explores the most useful parsers; for further information, see: http://wiki.apache.org/solr/SolrQuerySyntax.
Query syntax (the lucene query parser) Solr's native / full query syntax is implemented by the query parser named lucene.
It is based on Lucene's old syntax with a few additions that will be pointed out explicitly.
In fact, you've already seen the first addition which is local-params.
The best practice for the user query (the q parameter) is to use the dismax or edismax query parsers, not the default lucene query parser described here.
You'll read more about these query parsers in the next section.
The lucene query parser does have a couple query parameters that can be set.
Usually these aren't specified as Lucene is rarely used for the user query and because Lucene's query syntax is easily made explicit to not need these options.
Use debugQuery=on To see a normalized string representation of the parsed query tree, enable query debugging.
A final point to be made is that there are query capabilities within Lucene that are not exposed in query parsers that come with Solr.
Notably, there is a family of socalled "span queries" which allow for some advanced phrase queries that can be composed together by their relative position.
To learn more about that, I advise reading the Lucene In Action book.
There is also the ability to search for a term matching a regular expression.
Matching all the documents Lucene doesn't natively have a query syntax to match all documents.
Solr enhanced Lucene's query syntax to support it with the following syntax:
When using dismax, it's common to set q.alt to this match-everything query so that a blank query returns all results.
Mandatory, prohibited, and optional clauses Lucene has a unique way of combining multiple clauses in a query string.
It is tempting to think of this as a mundane detail common to boolean operations in programming languages, but Lucene doesn't quite work that way.
A query expression is decomposed into a set of unordered clauses of three types:
It's okay for spaces to come between + or - and the search word.
However, when Solr 4 arrives, this will no longer be the case.
If the query expression contains at least one mandatory clause, then any optional clause is just that—optional.
This notion may seem pointless, but it serves a useful function in scoring documents that match more of them higher.
If the query expression does not contain any mandatory clauses, then at least one of the optional clauses must match.
Here, Pumpkins is optional, and my favorite band will surely be at the top of the list, ahead of bands with names like Smashing Atoms:
In this example there are no mandatory clauses and so documents with Smashing or Pumpkins are matched, but not Atoms.
My favorite band is at the top because it matched both, followed by other bands containing only one of those words:
If you would like to specify that a certain number or percentage of optional clauses should match or should not match, then you can instead use the dismax query parser with the min-should-match feature, described later in the chapter.
Boolean operators The boolean operators AND, OR, and NOT can be used as an alternative syntax to arrive at the same set of mandatory, optional, and prohibited clauses that were mentioned previously.
Use the debugQuery feature, and observe that the parsedquery string normalizes-away this syntax into the previous (clauses being optional by default such as OR)
Case matters! At least this means that it is harder to accidentally specify a boolean operator.
When the AND or && operator is used between clauses, then both the left and right sides of the operand become mandatory if not already marked as prohibited.
Similarly, if the OR or || operator is used between clauses, then both the left and right sides of the operand become optional, unless they are marked mandatory or prohibited.
If the default operator is already OR then this syntax is redundant.
If the default operator is AND, then this is the only way to mark a clause as optional.
To match artist names that contain Smashing or Pumpkins try:
So to find artists with Smashing but not Atoms in the name, you can do this:
This is because it is the only optional clause and there are no explicit mandatory clauses.
Likewise, using an AND or OR would have no effect in this example.
It may be tempting to try to combine AND with OR such as:
Remember that AND is equivalent to both sides of the operand being mandatory, and thus each of the four clauses becomes mandatory.
In order to combine query clauses in some ways, you will need to use sub-queries.
Sub-queries You can use parenthesis to compose a query of smaller queries, referred to as sub-queries or nested queries.
The following example satisfies the intent of the previous example:
Using what we know previously, this could also be written as:
The preceding sub-query is interpreted as documents that must have a name with either Smashing or Pumpkins and either Green or Day in its name.
So if there was a band named Green Pumpkins, then it would match.
Solr added another syntax for sub-queries to Lucene's old syntax that allows the subquery to use a different query parser including local-params.
This is an advanced technique so don't worry if you don't understand it at first.
The syntax is a bit of a hack using a magic field named _query_ with its value being the sub-query, which practically speaking, needs to be quoted.
As an example, suppose you have a search interface with multiple query boxes, whereas each box is for searching a different field.
You could compose the query string yourself but you would have some query escaping issues to deal with.
And if you wanted to take advantage of the dismax parser then with what you know so far, that isn't possible.
Recall from the localparams definition that the parameter v can hold the query and that the $ refers to another named request parameter.
Limitations of prohibited clauses in sub-queries Lucene doesn't actually support a pure negative query, for example:
Solr enhances Lucene to support this, but only at the top level query such as in the preceding example.
This query attempts to ask the question: Which artist names contain either Smashing or do not contain Pumpkins? However, it doesn't work and only matches the first clause—(4 documents)
The second clause should essentially match most documents resulting in a total for the query that is nearly every document.
The artist named Wild Pumpkins at Midnight is the only one in my index that does not contain Smashing but does contain Pumpkins, and so this query should match every document except that one.
To make this work, you have to take the sub-expression containing only negative clauses, and add the all-documents query clause: *:*, as shown below:
Interestingly, this limitation is fixed in the edismax query parser.
Hopefully a future version of Solr will fix it universally, thereby making this work-around unnecessary.
Field qualifier To have a clause explicitly search a particular field, you need to precede the relevant clause with the field's name, and then add a colon.
Spaces may be used in-between, but that is generally not done.
This matches bands containing a member with the name Corgan.
The content of the parenthesis is a sub-query, but with the default field being overridden to be a_member_name, instead of what the default field would be otherwise.
By the way, we could have used AND instead of + of course.
Moreover, in these examples, all of the searches were targeting the same field, but you can certainly match any combination of fields needed.
Phrase queries and term proximity A clause may be a phrase query: a contiguous series of words to be matched in that order.
In the previous examples, we've searched for text containing multiple words like Billy and Corgan, but let's say we wanted to match Billy Corgan (that is, the two words adjacent to each other in that order)
Double quotes are used to indicate a phrase query, as shown in the following code:
Related to phrase queries is the notion of the term proximity, aka the slop factor or a near query.
In our previous example, if we wanted to permit these words to be separated by no more than say three words in–between, then we could do this:
For the MusicBrainz data set, this is probably of little use.
For larger text fields, this can be useful in improving search relevance.
The dismax query parser, which is described in the next chapter, can automatically turn a user's query into a phrase query with a configured slop.
Wildcard queries A plain keyword search will look in the index for an exact match, subsequent to text analysis processing on both the query and input document text (for example, tokenization, lowercasing)
But sometimes you need to express a query for a partial match expressed using wildcards.
There is a highly relevant section in the text analysis chapter on partial/substring indexing.
N-Grams is a different approach that does not work with wildcard queries.
There are a few points to understand about wildcard queries:
To find artists containing words starting with Smash, you can do:
Or perhaps those starting with sma and ending with ing:
You can also use ? to force a match of any character at that position:
That would match words that start with sma and that have at least two more characters but potentially more.
As far as scoring is concerned, each matching term gets the same score regardless of how close it is to the query pattern.
Lucene can support a variable score at the expense of performance but you would need to do some hacking to get Solr to do that.
If so, the * works intuitively on any field, if not then you get an error about leading wildcards not being supported.
The official way to find all documents with at least a value will be explained shortly.
Fuzzy queries Fuzzy queries are useful when your search term needn't be an exact match, but the closer the better.
The fewer the number of character insertions, deletions, or exchanges relative to the search term length, the better the score.
The algorithm used is known as the Levenshtein Distance algorithm, also known as the edit distance.
Fuzzy queries have the same need to lowercase and to avoid stemming just as wildcard queries do.
Without this notation, simply Smashing would match only four documents because only that many artist names contain that word.
For instance, changing the proximity to a more stringent 0.7:
If you want to use fuzzy queries, then you should consider experimenting with different thresholds.
To illustrate how text analysis can still pose a problem, consider the search for: SMASH~
So SMASH would be a perfect match, but adding the tilde results in a search term in which every character is different due to the case difference and so this search returns nothing.
As with wildcard searches, if you intend on using fuzzy searches then you should lowercase the query string.
Range queries Lucene lets you query for numeric, date, and even text ranges.
The following query matches all of the bands formed in the 1990s:
To get the fastest numerical/date range query performance, particularly when there are many indexed values, use a trie field (for example, tdate) with a precisionStep.
For most numbers in the MusicBrainz schema, we only have identifiers, and so it made sense to use the plain long field type, but there are some other fields.
In this example, we can see Solr's support for open-ended range queries by using *
Although uncommon, you can also use range queries with text fields.
For this to have any use, the field should have only one term indexed.
You can control this either by using the string field type, or by using the KeywordTokenizer.
The following example finds all documents where somefield has a term starting with B.
We effectively make the right side of the range exclusive by excluding it with another query clause.
Both sides of the range, B and C, are not processed with text analysis that could exist in the field type definition.
If there is any text analysis like lowercasing, you will need to do the same to the query or you will get no results.
Date math Solr extended Lucene's old query parser to add date literals as well as some simple math that is especially useful in specifying date ranges.
In addition, there is a way to specify the current date-time using NOW.
The syntax offers addition, subtraction, and rounding at various levels of date granularity, like years, seconds, and so on down to milliseconds.
The operations can be chained together as needed, in which case they are executed from left to right.
In the preceding example, we searched for documents where an album was released over two years ago.
Let's say what we really wanted was precision to the day.
By using / we can round down (it never rounds up):
Furthermore, they can be pluralized by adding an S as in YEARS.
This so-called DateMath syntax is not just for querying dates; it is for supplying dates to be indexed by Solr too! When supplying dates to Solr for indexing, consider concatenating a rounding operation to coursen the time granularity sufficient for your needs.
There is an example below on how to use it.
Full millisecond precision time takes up more disk space and is slower to query than a courser granularity.
Using the NOW syntax as the default attribute of a timestamp field definition makes this easy.
Score boosting You can easily modify the degree to which a clause in the query string contributes to the ultimate relevancy score by adding a multiplier.
In the following example, we search for artists that either have a member named Billy, or have a name containing the word Smashing:
Here we search for an artist name containing Billy, and optionally Bob or Corgan, but we're less interested in those that are also named Corgan:
Existence (and non-existence) queries This is actually not a new syntax case, but an application of range queries.
Suppose you wanted to match all of the documents that have an indexed value in a field.
Here we find all of the documents that have something in a_name:
This can be negated to find documents that do not have a value for a_name, as shown in the following code:
Even though it's more intuitive than a range query, do not rely on this working until (and if) a future Solr officially supports this.
Like wildcard and fuzzy queries, these are expensive, slowing down as the number of distinct terms in the field increases.
Alternatively, you could do this yourself somewhere like a DIH transformer or in your client code.
The query would then simply be fields:a_name which is as fast as it gets.
Escaping special characters The following characters are used by the query syntax, as described in this chapter:
In order to use any of these without their syntactical meaning, you need to escape them by a preceding \ such as seen here:
In some cases such as this one where the character is part of the text that is indexed, the double-quotes phrase query will also work, even though there is only one term:
If you're using SolrJ to interface with Solr, the ClientUtils.
The Dismax query parser (part 1) The lucene query parser we've been using so far for searching offers a rich syntax, but it doesn't do anything more.
A notable problem with using this parser is that the query must be well formed according to the aforementioned syntax rules, such as having balanced quotes and parenthesis.
Users might type just about anything for a query, not knowing anything about this syntax, possibly resulting in an error or unexpected results.
The features of this query parser that have a more direct relationship to scoring are described in part 2, in the next chapter.
Use of this parser is so important that we introduce it here.
You'll see references here to edismax whereby the "e" stands for "extended"
This is an evolution of dismax new to Solr 3.1 that adds features.
In a future Solr version, perhaps as soon as the next release, I expect dismax to refer to the enhanced version while the older one will likely exist under another name.
Almost always use defType=dismax The dismax (or edismax) query parser should almost always be chosen for parsing user queries – q.
Set it in the request handler definition for your app.
Furthermore, I recommend use of edismax which has seen plenty of production use.
The only consideration to consider against this is whether it's a problem for users to be able to use Solr's full syntax, whether inadvertently or maliciously.
Here is a summary of the features that the dismax query parser has over the lucene query parser:
Use debugQuery=on Enable query debugging to see a normalized string representation of the parsed query tree, considering all value-add options that dismax performs.
But first, let's take a look at a request handler I've set up for searching artists.
Solr configuration that is not related to the schema is located in solrconfig.xml.
The following definition is a simplified version of the one in this book's code supplement:
In Solr's full search interface screen, we can refer to this with a Query Type of mb_artists.
This value aligns with the qt parameter, which you will observe in the URL when you submit the form.
It wasn't necessary to set up such a request handler, because Solr is fully configurable from a URL, but it's a good practice and it's convenient for Solr's search form.
Searching multiple fields You use the qf parameter to tell the dismax query parser which fields you want to search and their corresponding score boosts.
Here is the relevant configuration line from our dismax based handler configuration earlier:
This syntax is a space-separated list of field names that can have optional boosts applied to them using the same syntax for boosting that is used in the query syntax.
This request handler is intended to find artists from a user's query.
Such a query would ideally match the artist's name, but we'll also search aliases, and bands that the artist is a member of.
Perhaps the user didn't recall the band name but knew the artist's name.
This configuration would give them the band in the search results, most likely towards the end.
The score boosts do not strictly order the results in a cascading fashion.
If in your application you are matching identifiers of some sort, then you may want to give a boost to that field that is very high, such as 1000, to virtually assure it will be on top.
One detail involved in searching multiple fields is the effect of stop words (for example, "the", "a", …) in the schema definition.
If qf refers to some fields using stop words and others that don't, then a search involving stop words will usually return no results.
With dismax, you can ensure the query analyzer chain in queried fields filter out the same set of stop words.
Limited query syntax The edismax query parser will first try to parse the user query with the full syntax supported by the lucene query parser, with a couple tweaks.
If it fails to parse, it will fall back to the limited syntax of the original dismax in the next paragraph.
Some day, this should be configurable but it is not at this time.
The aforementioned "tweaks" to the full syntax is that, or and and boolean operators can be used in a lower-case form, and pure-negative sub-queries are supported.
Anything else is escaped if needed to ensure that the underlying query is valid.
The following query example uses all of the supported features of this limited syntax:
Min-should-match With the lucene query parser, you have a choice of the default operator being OR, thereby requiring just one query clause (that is word) to match, or choosing AND to make all clauses required.
This of course only applies to clauses not otherwise explicitly marked required or prohibited in the query using + and -
But these are two extremes, and it would be useful to pick some middle ground.
The dismax parser uses a method called min-should-match, a feature which describes how many clauses should match, depending on how many there are in the query—required and prohibited clauses are not included in the numbers.
This allows you to quantify the number of clauses as either a percentage or a fixed number.
The configuration of this setting is entirely contained within the mm query parameter using a concise syntax specification that I'll describe in a moment.
This feature is more useful if users use many words in their queries—at least three.
This in turn suggests a text field that has some substantial text in it but that is not the case for our MusicBrainz data set.
Basic rules The following are the four basic mm specification formats expressed as examples:
It does not make any number negative from the standpoint of any definitions herein.
Given five queried clauses, the first requires three, whereas the second requires four.
Multiple rules Now that you understand the basic mm specification format, which is for one simple rule, I'll describe the final format, which allows for multiple rules.
Only the right-most rule that meets the clause count threshold is evaluated.
As they are ordered in an ascending order, the chosen rule is the one that requires the greatest number of clauses.
If none match because there are fewer clauses, then all clauses are required—that is a basic specification of 100%
An example of the mm specification is given in the following example:
This reads: If there are over nine clauses, then all but three are required (three are optional, and the rest are required)
If there are over two clauses, then 75% are required (rounded down)
Otherwise (one or two clauses) all clauses are required, which is the default rule.
I find it easier to interpret these rules if they are read right to left.
What to choose A simple configuration for min-should-match is making all of the search terms optional.
This is effectively equivalent to a default OR operator in the Lucene query parser.
Conversely, the other extreme is requiring all of the terms, and this is equivalent to a default AND operator.
For MusicBrainz searches, I do not expect users to be using many terms, but I expect most of them to match.
If a user searches for three or more terms, then I'll let one be optional.
You may be inclined to require all of the search terms; and that's a good common approach—it's the default in fact.
However, if just one word isn't found then there will be no search results—an occurrence that most search software tries to minimize.
Even if you make some of the words optional, the matching documents that have more of the search words will be towards the top of the search results assuming scoresorted order (you'll learn why in the next chapter)
There are other ways to approach this problem like performing a secondary search if the first returns none or too few.
Solr doesn't do this for you but it's easy for the client to do.
This approach could even tell the user that this was done which would yield a better search experience.
A default search The dismax query parser supports a default search, which is used in the event the user query, q, is not specified.
This parameter is q.alt and it is not subject to the limited syntax of dismax.
Here's an example of it used for matching all documents, from within the request handler defaults in solrconfig.xml:
This parameter is usually set to *:* to match all documents and is often specified in the request handler configuration in solrconfig.xml.
You'll see with faceting in the next section, that there will not necessarily even be a user query, and so you'll want to display facets over all of the data.
Filtering Separate from the q parameter (that is the user query), you can specify additional so-called filter queries that will filter the search results.
Arguably the user query is also a filter but you instead see the word "search" used for that.
This parameter can be added multiple times for additional filters.
A document must match all filter queries and the user query for it to be in the results.
As an example, let's say we wanted to make a search form for MusicBrainz that lets the user search for bands, not individual artists, and those that released an album in the last 10 years.
Let's also say that the user's query string is Green.
Therefore, a query that would find non-individuals would be this, combined with the user's query:
Remember that in the URL snippet above we needed to URL Encode special characters like the colons.
In general, raw user input doesn't wind up being part of a filter-query.
Instead, the filters are either known by your application in advance or are generated based on your data, for example, in faceted navigation.
Although it wouldn't necessarily be a problem for user query text to become a filter, there may be scalability issues if many unique filter queries end up being performed that don't get re-used and so consume needless memory.
Solr 3.4 added the ability to disable caching of a filter by setting the cache local-param to false.
This is useful for avoiding pollution of the filter cache when you know the query is not likely to be used again.
And if the query is a function query (discussed in Chapter 5, Search Relevancy), there is a potential performance benefit because non-cached function queries are evaluated on a fraction of the total documents instead of all of them.
Sorting The sorting specification is specified with the sort query parameter.
In the following example, suppose we searched for artists that are not individuals (a previous example in the chapter), and then we might want to ensure that those that are surely bands, get top placement ahead of those that are unknown.
Pay attention to the field types and text analysis you're using in your schema for fields that you sort on.
Basically, fields need to be single valued, indexed, and not-tokenized.
See the section on sorting in Chapter 2, Schema and Text Analysis for further information.
In addition to sorting on field values and the score, Solr supports sorting on a function query.
Function queries are usually mathematical in nature and used for things like computing a geospatial distance or a time difference between now and some field value.
Function queries are discussed in detail in the next chapter, but here's a simple example sorting by the difference between the artist begin and end date:
An interesting use-case that has nothing to do with math is a trick to sort based on multi-valued field data in limited circumstances.
For example, what if we wanted to sort on MusicBrainz releases which are declared to be of type Album (r_type is a multi-valued field remember):
To understand how to parse and understand this admittedly complicated expression, read the earlier section on query parsers and local-params, and read the definition of the query() function query in the next chapter.
We had to use the local-params v parameter to specify the query string instead of simply using the query because of syntax restrictions in the context of the how the sort parameter value is parsed.
Sorting and memory usage When you ask Solr to sort on a field, every indexed value is put into an array in memory in the "field cache"
This is more of a problem with text than other fields.
Not only does this use a lot of memory, but the first time this happens, it takes time to bring in all the values from disk.
You should add a query that sorts on the fields your app might sort on into newSearcher in solrconfig.xml.
A common specific need is filtering search results by distance from a center point and also to sort the results by that distance.
Other uses are influencing relevancy order and faceting by distance ranges.
Spatial search is similar but based on a 2-dimensional Cartesian plane instead of geospatial's sphere / planetary orientation.
For documentation on the less useful spatial search, go to Solr's wiki.
The geospatial support about to be described is entirely new to Solr 3.1
This is just the beginning; expect much more in future releases.
It may very well work quite differently in the future.
Indexing locations You need raw location data in the form of a latitude and longitude to take advantage of Solr's geospatial capabilities.
If you have named locations (for example, "Boston, MA") then the data needs to be resolved to latitudes and longitudes using a gazetteer like Geonames—http://www.geonames.org.
If all you have is free-form natural language text without the locations identified, then you'll have to perform a more difficult task that uses Natural Language Processing techniques to find the named locations.
The principle field type in Solr for geospatial is LatLonType, which stores a single latitude-longitude pair.
Under the hood, this field type copies the latitude and longitude into a pair of indexed fields using the provided field name suffix.
When providing data to this field, it is formatted as a string with the latitude and longitude separated by a comma like this:
Geohashes Another geospatial field type to be aware of is GeoHashField, but there is no point in using it at this time.
Geohashes have the opportunity for multi-valued data, indexed shape data other than points, and very fast filtering.
However the implementation presently in Solr doesn't leverage any of this—for one that does, take a look at SOLR-2155—a patch of mine.
There is a good chance this will make it into Solr 4 in some form.
Filtering by distance Perhaps the most common geospatial need is to filter search results to those documents within a distance radius from a center point.
If you are building an application in which the user is presented with a map, perhaps using Google Maps, then the center point is the center of the map the user is looking at and the distance radius is the distance from the center to the nearest map edge.
Such a query is generally specified using a Solr filter query (fq parameter) leaving q open for the possibility of a combined keyword search if desired.
Both the geofilt and bbox query parsers perform geospatial filtering.
Furthermore, as the user is probably looking at a latitude-longitude box, having the query shape be a box makes sense too.
Here is a quick example based on Solr's example schema and data set, showing the URL parameters needed to do the search:
The parameters that geofilt and bbox require can be specified as either localparams (between the parser name and closing bracket) or standard request parameters as shown above.
The advantage to the latter is that a subsequent distance sort can re-use the same parameters, as you'll see in a bit.
Sorting by distance Geospatial distance sorting is implemented using a Solr function query that calculates the distance.
There are a few such functions but the primary one you should use is geodist() which is the only one I'll document here.
It returns the Earth geospatial distance using the Haversine formula between a pair of points.
The points are each taken from the first available of: an argument, the parameter pt, or the parameter sfield.
Any of these might be absent but at least two must be specified.
When a point is specified as an argument, it can simply be a geospatial field or a pair of typical arguments (a field name or constant) to the latitude and longitude.
By design, these parameter names align with those for the geofilt and bbox query parsers, which pairs well with geodist()
Summary At this point, you've learned the essentials of searching in Solr, from request handlers, to the full query syntax, to dismax, and more.
We spent a lot of time on the query syntax because you'll see the syntax pop-up in several places across Solr, not just the user's query.
Such places include filter queries, delete queries, boost queries, facet queries, embedded within certain function queries, and query warming (discussed in later chapters)
Even if you don't wish to expose the syntax to your users, you will probably be using it for various things.
The subject of searching continues in the next chapter with a focus on relevancy / scoring matters.
This starts with an explanation of Lucene/Solr's scoring model, and then various tools Solr gives you to influence the score such as function queries—also useful in sorting and filtering.
Search Relevancy At this point you know how to search, filter, and sort.
You've undoubtedly been sorting by score in descending order, the default, but have no understanding as to where those scores came from.
This chapter is all about search relevancy, which basically means it's about scoring but it's also about other non-trivial methods of sorting to produce relevant results.
A core Solr feature enabling these more advanced techniques called "function queries" will be introduced.
In a hurry? Use the edismax query parser for user queries by setting the defType parameter.
Configure the qf (query fields) as explained in the previous chapter, set pf (phrase fields) considering the call-out tip in this chapter, and set tie to 0.1
Scoring Scoring in Lucene is an advanced subject, but in spite of this it is important to at least have a basic understanding of it.
Instead of presenting the algorithm, comprehension of which is a bit of an advanced subject not suitable for this book, we will discuss the factors influencing the score and where to look for diagnostic scoring information.
An important thing to understand about scores is not to attribute much meaning to a score by itself; it's almost meaningless.
The relative value of an individual score to the max score is more relevant.
A document scored as 0.25 might be a great match or not, there's no telling.
But if you compare this score to another from the very same search and find it to be twice as large, then it is fair to say that the document matched the query twice as well.
This being said, you will usually find that scores in the vicinity of 0.5 or better are decent matches.
These factors are the intrinsic components contributing to the score of a document in the results.
If you introduce other components of the score then that is referred to as boosting.
Usually, boosting is a simple multiplier to a field's score, either at index or query time, but it's not limited to that.
Query-time and index-time boosting At index-time, you have the option to boost a particular document specified at the document level or at a specific field.
The document level boost is the same as boosting each field by that value.
This is internally stored as part of the norms number.
Norms must not be omitted in the relevant fields in the schema.
It's uncommon to perform index-time boosting because it is not as flexible as query-time.
That said, I do find index-time boosting to have a more predictable and controllable influence on the final score.
At query-time, we described in the previous chapter how to explicitly boost a particular clause of a query higher or lower if needed using the trailing ^ syntax.
We also showed how the dismax query parser's qf parameter not only lists the fields to search but allows a boost for them as well.
There are a few more ways dismax can boost queries that you'll read about shortly.
Troubleshooting queries and scoring An invaluable tool in diagnosing scoring behavior (or why a document isn't in the result or is but shouldn't be) is enabling query debugging with the debugQuery query parameter.
There is no better way to describe it than with an example.
We would intuitively expect that documents with fields containing Smashing would get the top scores, but that didn't happen.
Execute the preceding query mentioned with debugQuery=on, and ensure that you're looking at the original indentation by using the View Source feature in your browser.
The top score is 2.664175, and there were two documents matching, neither with Smashing.
The first two documents have words that differ from smashing by only two characters (remember the case difference)
Its score was a little less, but not enough to overtake the top two.
What's going on here? Let's look at the following debug output, showing the first and the third document.
We'll skip the second, as it has the same score as the first:
What we see here is the mathematical breakdown of the various components of the score.
We expected this because the fuzzy matching was going to give higher weights to stronger matches, and it did.
However, other factors pulled the final score in the other direction.
This is because the document we wanted to score higher had a field with more indexed terms (Smashing Atoms) versus just the one that Mashina had.
So arguably, Mashina is a closer match than Smashing Atoms to the fuzzy query Smashing~
How might we "fix" this? Well it's not broken, and the number three spot in the search results isn't bad.
This is also a fuzzy query which is fairly unusual and arguably isn't a circumstance to optimize for.
The first thing to do is to try and lowercase a fuzzy query so that there isn't a case difference.
If that is insufficient then try enabling omitNorms in the schema, at the expense of no score differentiation on matches in shorter versus longer fields.
Dismax query parser (part 2) In the previous chapter you were introduced to the dismax query parser as the preferred choice for user queries, the q parameter.
The parser for user queries is set with the defType parameter.
The syntax, the fields that are queried (with boosts)qf, the min-should-match syntax—mm, and the default query—q.alt, were already described.
We're now going to complete your education on this parser by discussing the remaining features which are most closely related to scoring.
Any mention herein to dismax applies to the edismax query parser too, unless specified otherwise.
As explained in the previous chapter, edismax is the extended dismax parser that is expected to replace dismax in a future release.
Advanced topic warning The following discussion is advanced, and you needn't understand it.
Just know that a dismax query is ideal for searching multiple fields and to set the tie parameter to 0.1, which is a reasonable choice.
The boolean query mentioned above is not quite equivalent to what the dismax query actually does; the difference is in the scoring.
The dismax behavior should produce better scores for this use case, which is where you are looking in multiple fields for the same term, where some fields are deemed to be more significant than others.
An example from the API docs of this feature explains that if a user searched for albino elephant, then dismax ensures that albino matching one field and elephant matching another gets a higher score than albino matching both fields but elephant neither.
Another wrinkle on this description of dismax scoring is the tie parameter, which is between zero (the default) and one.
By raising this value above zero, the scoring begins to favor documents that matched multiple terms over those that were boosted higher.
This can be moved to the extreme by using 1—resulting in scoring that is closer to that of a boolean query.
This is interpreted as two terms to search for, and depending on how the request handler is configured, either both must be found in the document or just one.
Perhaps for one of the matching documents, Billy is the sole name of a band, and it has a member named Joel.
Great, Solr found this document and perhaps it is of interest to the user, after all, it contained both words the user typed.
However, it's a fairly intuitive observation that a document field containing the entirety of what the user typed, Billy Joel, represents a closer match to what the user is looking for.
Solr would certainly find such a document too, without question, but it's hard to predict what the relative scoring might be.
To improve the scoring, you might be tempted to automatically quote the user's query, but that would omit documents that don't have the adjacent words.
What the dismax handler can do is add a phrased version of the user's query onto the original query as an optional clause.
The queries here illustrate phrase boosting in its most basic form.
The rewritten query depicts that the original query is mandatory by using +, and it shows that we've added an optional phrase.
A document containing the phrase Billy Joel not only matches that clause of the rewritten query, but it also matches Billy and Joel—three clauses in total.
If in another document the phrase didn't match, but it had both words, then only two clauses would match.
Lucene's scoring algorithm would give a higher coordination factor to the first document, and would score it higher, all other factors being equal.
Configuring automatic phrase boosting Automatic phrase boosting is not enabled by default.
In order to use this feature, you must use the pf parameter, which is an abbreviation of "phrase fields"
You should start with the same value and then make adjustments.
Remove fields that are always one term, such as an identifier.
Also, use of common-grams or shingling, as described in Chapter 10, Scaling Solr, is highly recommended to increase performance.
Phrase slop configuration In the previous chapter, we had mentioned the phrase slop, aka term proximity, by following a phrase with a tilde and a number, as shown below:
If slop is not specified, then there is no slop, which is equivalent to a value of zero.
For more information about slop, see the corresponding discussion in the previous chapter.
Partial phrase boosting In addition to boosting the entire query as a phrase, edismax supports boosting consecutive word pairs if there are more than two queried words and consecutive triples if there are more than three queried words.
This feature is not affected by the ps (phrase slop) parameter, which only applies to the entire phrase boost.
You can certainly expect the relevancy to improve for longer queries, but of course these queries are going to be even slower now.
To speed up such queries, use common-grams or shingling, described Chapter 10, Scaling Solr.
Boosting: Boost queries Continuing with the boosting theme is another way to affect the score of documents: boost queries.
The dismax parser lets you specify multiple additional queries using bq parameter(s), which, like the automatic phrase boost, get added onto the user's query in a similar manner.
Remember that boosting only serves to affect the scoring of documents that already matched the user's query in the q parameter.
If a matched document also matches a bq query, then it will be scored higher.
For a realistic example of using a boost query, we're going to look at MusicBrainz releases data.
We don't want to sort search results based on these since we want the natural scoring algorithm to consider the user's query in the relevancy; however, we might want to influence the score based on these.
For example, let's say albums are the most relevant release type whereas a compilation is the least relevant.
And let's say that an official release is more relevant than bootleg or promotional or pseudo-releases.
We might express this using a boost query like this (defined in the request handler):
Searching releases for "airplane flies" showed that this boost query did what it should by breaking a score tie in which the release names were the same but these attributes varied.
In reality the boosting on each term, all three in this example, would be tweaked to have the relevancy boost desired by carefully examining the debugQuery output.
To understand why *:* is needed, read the previous chapter on the limitations of pure negative queries.
Boost queries are not as useful as boost functions, described in the next section—especially since edismax supports a multiplied boost, which is generally more desirable than addition.
Even in the preceding example, it's awkward to tune the boost of each query clause because of the inverse document frequency (IDF) that varies for each term.
You would need to perform a query with debugQuery enabled to look at what the score is for each of these terms, which will be different, and then use disproportionate boosts (not both as in the example) so that when multiplied by their intrinsic score, they wind up being the same.
Boosting: Boost functions Boost functions offer a powerful way to either add or multiply the result of a userspecified formula to a document's score.
By formula I refer to a composition of Solr function queries, which are described in detail next in this chapter.
To add to the score, specify the function query with the bf parameter.
You can specify bf and boost each as many times as you wish.
For a thorough explanation of function queries including useful MusicBrainz examples, see the next section.
The bf and boost parameters are actually not parsed in the same way.
The bf parameter allows multiple boost functions within the same parameter, separated by space, as an alternative to using additional bf parameters.
You can also apply a multiplied boost factor to the function in bf by appending ^100 (or another number) to the end of the function query.
This is just a convenience for using the mul() function query, described later.
Finally, ensure newSearcher in solrconfig.xml has a sample query using the boost functions you're using.
In doing so you ensure that any referenced fields are in Lucene's field cache instead of penalizing the first query with this cost.
Chapter 10, Scaling Solr has more information on performance tuning.
Add or multiply boosts? In a nutshell, if you can tame the difficulty in additive boosting (bf param) then you'll probably be more satisfied with the scoring.
Multiplicative boosting (boost param) is easier to use, especially if the intended boost query is considered less than or equal to the user query, which is usually true.
The trick is that you need to know the top score for an excellent match on the user query in order to balance out the proportions right.
Try an exact match on a title (a highly boosted field in the query) and see what the top score is.
Do this a bunch of times for a variety of documents, looking for reasonable consistency.
Simply multiply by that if you already have the function query in the 0-1 nominal range.
Even if these instructions don't seem too bad, in practice tuning additive scores is tricky since Lucene will react to every change you do by changing the queryNorm part of the score out from under you, which you have no control over.
As it does this, keep your eye on the overall ratios that you want between the added boost part and the user query part, not the particular score values.
Another bigger problem is that your experiments in gauging the maximum score of the user query will change as your data changes, which will mean some ongoing monitoring of whatever values you choose.
The other way of thinking about your boost function is as a user query score multiplier (a factor)
With multiplication you don't need to concern yourself with whatever a "good" user query score is—it has no bearing here.
The tricky part of multiplicative boosts is weighting your boost, so it has the relative impact you want.
If you simply supply your nominal range (0-1) function directly as the boost then it has the same weight as the user query.
As you shift the function's values above 0 then you reduce the influence it has relative to the user query.
It's possible to use multiplicative boosts that are weighted as more relevant than the user query but I haven't fully worked out the details.
A place to start experimenting with this is boosting the boost function by a power, say 1.7, which appeared to about double the weight.
Function queries A function query is a user-specified composition of Solr-provided functions, most of which are mathematical in nature.
It is evaluated on each matching document, taking constants and references to single-valued fields as input and returning a floatingpoint number via its score.
There are quite a few ways in which you can incorporate a function query into your searches in Solr:
Despite the multitude of options here, you'll most likely just use them in boosting with the dismax parser.
There is no field named _val_; this just triggers the query parser to treat the quoted part as a function query instead of as a field value to search.
It'll match all documents, so combine it with other required clauses to actually limit the results.
The score is added to the other parts of the query.
The score of each document in the results is the evaluation of the function query.
Field references For fields used in a function query, the constraints are the same as sorting.
Essentially this means the field must be indexed, not multi-valued, and if text fields are used then they must analyze down to no more than one token.
And like sorting, all values get stored in the field cache.
The implication of the field cache is that you should have enough memory and also that you should have a suitable query in newSearcher in solrconfig.xml so that the first search after a commit isn't penalized with the initialization cost.
If you have a multi-valued field you hoped to use, you'll instead have to put a suitable value into another field during indexing.
This might be a simple minimum, maximum, or average calculation.
If you are using the Data Import Handler (DIH, see Chapter 3, Indexing Data), you should consider a DIH transformer, or you could simply do this on the client before sending the data to Solr.
If there is no value in the field for the document then the result is zero, otherwise, numeric fields result in the corresponding numeric value.
But what about other field types? For TrieDateField you get the ms() value, explained shortly.
For older date fields, you get the ord() value, also explained shortly.
Some functions can work with the text value—in such cases you'll need to explicitly use the literal() function.
Function reference This section contains a reference of the majority of function queries in Solr.
An argument to a function can be a literal constant such as a number, a field reference, or an embedded function.
One interesting thing you can do is pull out any argument into a separate named request parameter (in the URL) of your choosing and then refer to it with a leading $:
The parameter might be in the request or configured into the request handler configuration.
If this parameter dereferencing syntax is familiar to you, then that's because it works the same way in local-params too, as explained in Chapter 4, Searching.
For the function definitions below, any argument named x, y, or z can be any expression: constants, field references, or functions.
Other arguments like a, or min require a literal constant.
If you attempt to do otherwise, then you will get an unhelpful parsing error.
Mathematical primitives These functions cover basic math operations and constants:
For example, if the value of x is found to be onethird from the smallest and largest values of x across all documents, then x is returned as one-third of the distance between minTarget and maxTarget.
This makes it impractical for many uses, as it is too slow.
Before ms() was introduced in Solr 1.4, ord() and rord() were mediocre substitutes.
A definition of ord is not sufficient to fully convey its ramification.
The original values are not distributed in a linear fashion.
They are more clumped together towards the higher values (do not consider duplicates)
To determine how high ord/rord can get, you can use Solr's web admin interface.
Click on an indexed field, and observe the distinct number.
Miscellaneous functions There are multiple ways to use the ms() function to get a date-time value since its arguments are all optional.
Note that any field reference to a time will be ambiguous to a blank value, which is zero.
Interestingly, there are a couple function queries which return the score results of another query.
It's a fairly esoteric feature but it has its uses.
One such use is to sort by whether a field has a value.
Another interesting function query is one that calculates the string distance between two strings based on a specified algorithm.
The primary one you should use is geodist() which is the only one I'll document here:
By design these parameter names align with those for the geofilt query parser, which pairs well with geodist()
There are some function queries I chose to omit for various reasons.
Solr's wiki http://wiki.apache.org/solr/FunctionQuery has the full list with some descriptions.
The vector calculations look interesting but a real use-case is not clear.
Function query boosting The overall process to function query boosting is as follows:
Decide the relative weighting of the boost relative to the user query (for example, 1/3)
Choose additive or multiplicative boosting and then apply the relative weighting according to the approach you chose (see Add or multiply boosts?)
The upcoming examples address common scenarios with ready-made formulas for you.
If you want to work on formulas instead of taking one provided here as is, I recommend a tool such as a graphing calculator or other software to plot the functions.
If you are using Mac OS X as I am, then your computer already includes Grapher, which generated the charts in this chapter.
You might be inclined to use a spreadsheet like Microsoft Excel, but that's really not the right tool.
With luck, you may find some websites that will suffice, perhaps http://www.
If your data changes in ways causing you to alter the constants in your function queries, then consider implementing a periodic automated test of your Solr data to ensure that the data fits within expected bounds.
A Continuous Integration (CI) server might be configured to do this task.
An approach is to run a search simply sorting by the data field in question to get the highest or lowest value.
Formula: Logarithm The logarithm is a popular formula for inputs that grow without bounds, but the output is also unbounded.
However, the growth of the curve is stunted for larger numbers.
This in practice is usually fine even when you ideally want the output to be capped.
The logarithm cannot be inverted without the risk of a negative score, which should be avoided.
Here is a graph of our formula, given inputs from a future example.
Smaller values make it too linear and greater values put a knee bend in the curve that seems too early.
With the logarithm, further values advance the output steadily but at a shallow slope that slowly gets shallower.
Now that you have your formula, you are ready to proceed with the other function query boosting steps.
Formula: Inverse reciprocal In general, the reciprocal of a linear function is favorable because it gives results that are bounded as the input grows without bounds.
The arrow in the following graph shows where the "horizon" (1/m) lies:
Here, max is the value that this function approaches, but never quite reaches.
You can experiment with this to see how it changes the bend in the curve below.
At this value, the result is 1, and larger inputs only increase it negligibly.
Now that you have your formula, you are ready to proceed with the other function query boosting steps.
Formula: Reciprocal The reciprocal is an excellent formula to use when you want to maximally boost at input 0 and boost decreasingly less as the input increases.
It is often used to boost newly added content by looking at how old a document is.
The arrow roughly shows where the horizon input value is.
Which translates easily to a Solr function query as recip(x,1,c,c)
Now that you have your formula, you are ready to proceed with the other function query boosting steps.
Formula: Linear If you have a value in your schema (or a computed formula) that you are certain will stay within a fixed range, then the formula to scale and shift this to the 0-1 nominal range is easy.
We're also assuming that there is a linear relationship between the desired boost effect and the input.
Below, a refers to the end of the range that will have the least significant boost.
Now that you have your formula, you are ready to proceed with the other function query boosting steps.
How to boost based on an increasing numeric field In this section I'm going to describe a few ways to boost a document based on one of its numeric fields.
The greater this number is for a document, the greater boost this document should receive.
This number might be a count of "Like" or "ThumbsUp" votes by users, or the number of times a user accessed (for example, clicked) the referenced document, or something else.
In the MusicBrainz database, there are TRM and PUID lookup counts.
These identifiers roughly correspond to a song, which in MusicBrainz appears as multiple tracks due to various releases that occur as singles, compilations, and so on.
By the way, audio fingerprints aren't perfect, and so a very small percentage of TRM IDs and PUIDs refer to songs that are completely different.
Since we're only using this to influence scoring, imperfection is not a problem.
MusicBrainz records the number of times one of these IDs are looked up from its servers, which is a good measure of popularity.
A track that contains a higher lookup count should score higher than one with a smaller value, with all other factors being equal.
This scheme could easily be aggregated to releases and artists, if desired.
In the data loading I've arranged for the sum of TRM and PUID lookup counts to be stored into our track data as t_trm_lookups with the following field specification in the schema:
Step by step… The first step is to pick a formula.
Since this is a classic case of an increasing number without bound in which the greater the number is, the greater the boost should be, the inverse reciprocal is a very good choice.
Next, we plug in our data into the formula specified earlier and we end up with this function query:
The next step is to choose between additive boosts versus multiplicative.
And let's say this function query should weigh 1/3 of the user query.
According to earlier instructions, adding to our function query will reduce its weight.
This boost absolutely had the desired effect, altering the score order as we wanted.
The goal is to change the relative order of score sorted documents.
External field values As you may recall from Chapter 3, Indexing Data, Solr does not support updating a document; instead the entire document must be added again.
If you were to consider doing this just to increase a number every time a user clicked on a document or clicked some "Thumbs-Up" button, and so on, then there is quite a bit of work Solr is doing just to ultimately increase a number.
For this specific use-case, Solr has a specialized field type called ExternalFileField which gets its data from a text file containing the field's values.
This field type is very limited–the values are limited to floating point numbers and the field can only be referenced within Solr in a function query.
You do still need to issue a commit to Solr for any changes to be picked up.
As already explained in Chapter 3, Indexing Data, don't commit too frequently since commits are slow and resource-intensive.
An application using this feature would generate this file on a periodic basis on par with the commit frequency.
How to boost based on recent dates Using dates in scores presents some different issues.
Suppose when we search for releases, we want to include a boost that is larger for more recent releases.
At first glance, this problem may seem just like the previous one, because dates increase as the scores are expected to, but it is different in practice.
Instead of the data ranging from zero to some value that changes occasionally, we now have data ranging from a non-zero value that might change rarely to a value that we always know, but changes continuously—the current date.
Instead, approach this from the other side, that is, by considering how much time there is between the current date and the document's date.
Step by step… The first step is to pick a formula.
Based on this scenario, x is the age—a time duration from the present.
Our MusicBrainz schema has r_event_date, which is a promising candidate; however, multi-valued fields are not supported by function queries.
I made a simple addition to the schema and index to record the earliest release event date: r_event_date_ earliest.
With that done, now we can calculate the age with the two-argument variant of ms()
As a reminder to show how to run these function queries while debugging, here's a URL snippet:
The book's data set has been constant but at the time I received it, I observed some releases were in the future! What I would have seen then is reproducible by substituting NOW-4YEARS instead of NOW in the function, as I write this.
The first documents (score ascending) have negative values which means they are from the future.
We can't have negative inputs, so instead we'll wrap this function with the absolute value using abs()
Another thing to fine-tune is the cache-ability of the function query.
Instead of using NOW, using NOW/DAY makes this query re-usable by subsequent requests for a 24 hour period.
At this point you can follow the final steps in the previous how-to.
Summary In this chapter, we've covered the most advanced topics the book has to offerscoring and function queries.
Next, we saw a real-world example of using the debugQuery parameter to diagnose a scoring issue.
That exercise might be the most important exercise in the chapter, since it gives you the tools to diagnose why a document matched or didn't match a query.
Next, we concluded the coverage of the dismax query parser.
Even if you aren't inclined to use fancy boosting function queries, you can improve your search relevancy simply by configuring phrase boosting.
Even if you aren't a math whiz, you should be able to use formulas provided to you here, especially if you worked through the how-to's.
You might say this is the last of the foundational chapters.
The next two cover specific search value-adds that are each fairly compartmentalized.
The stand-out feature that contributes to much of Solr's popularity is faceting, covered next in its own chapter.
It's a must-have feature for most search implementations, especially those with structured data like in e-commerce, yet there are few products that have this capability, especially in open source.
Of course search fundamentals, including highlighting, are critical too but they tend to be taken for granted.
Faceting enhances search results with aggregated information over all the documents found in the search, not the entire index.
It can answer questions about the MusicBrainz data such as:
Look through the upcoming example, which demonstrates the most common type of faceting, and review the faceting types.
Faceting in the context of the user experience is often referred to as faceted navigation, but also faceted search, faceted browsing, guided navigation, or parametric search.
The facets are typically displayed with clickable links that apply Solr filter queries to a subsequent search.
Now might be a good time for a screenshot but instead I'll direct you to a collection of them at Endeca's excellent UX Design Pattern Library: http://patterns.endeca.com/ and click on "Faceted Navigation"
If we revisit the comparison of search technology to databases, then faceting is more or less analogous to SQL's GROUP BY feature on a column with count(*)
However, in Solr, facet processing is performed subsequent to an existing search as part of a single request-response with both the primary search results and the faceting results coming back together.
In SQL, you would need to perform a series of separate queries to get the same information.
Furthermore, faceting works so fast that its search response time overhead is almost always negligible.
For more information on why implementing faceting with relational databases is hard and doesn't scale, visit this old article: http://web.archive.org/web/20090321120327/http://www.
A quick example: Faceting release types Observe the following search results.
The query parameter q is *:*, which matches all documents.
In this case, the index I'm using only has releases.
If there were non-releases in the index, then I would add a filter fq=type:Release to the URL or put this in the request handler configuration, as that is the data set we'll be using for most of this chapter.
Filter queries are used in conjunction with faceting a fair amount so be sure you are already familiar with them from Chapter 4, Searching.
It's critical to understand that the faceting numbers are computed over the entire search result—603,090 releases, which is all of the releases in this example, and not just the two rows being returned.
The facet related search parameters are highlighted at the top.
The facet.missing parameter was set using the field-specific syntax, which will be explained shortly.
Notice that the facet results (highlighted) follow the main search result and are given the name facet_counts.
In this example, we only faceted on one field, r_official, but you'll learn in a bit that you can facet on as many fields as you desire.
The name attribute, like "Official", holds a facet value, which is simply an indexed term, and the integer following it is the number of documents in the search results containing that term—the facet count.
The last facet has the count but no corresponding name.
It is a special facet to indicate how many documents in the results don't have any indexed terms.
MusicBrainz schema changes In order to get better self-explanatory faceting results out of the r_attributes field and to split its dual-meaning, I modified the schema.
It isn't truly necessary to map the numbers to their names, as the user interface, which is going to present the data, could very well map it on the fly.
At this point there are two paths to take, with similar results.
The recommended path is to modify the import process to map specific constants to their named values into these two new fields.
For example, if you were using the Data Import Handler from Chapter 3, Indexing Data this would occur in a DIH transformer.
We'll take another path here to illustrate what can be done with a little text analysis.
The stored value, if we chose to mark the fields as stored, would hold the original set of constants, which is less than ideal.
And in this scenario, we're forced to mark both fields as multivalued even though one of them isn't.
I used copyField directives to copy r_attributes into both new fields:
In order to map the constants to human-readable definitions, I created two field types: rType and rOfficial that use a regular expression to pull out the desired numbers and a synonym list to map from the constant to the human-readable definition.
I removed the constant 0, as it seemed to be bogus.
The definition of the type rOfficial is the same as rType, except it has this regular expression: ^(0|\d\d?)$
Otherwise, this would happen because the previous regular expression reduces text fitting unwanted patterns to empty strings.
It does not matter if the user interface uses the name (for example, Official) or constant (for example, 100) when applying filter queries when implementing faceted navigation, as the text analysis will let the names through and will map the constants to the names.
This is not necessarily true in a general case, but it is for the text analysis as I've configured it above.
Field requirements The principal requirement of a field that will be faceted on is that it must be indexed; it does not have to be stored.
Otherwise, faceting on r_type would show tallies for Non-Album and Track separately.
On the other hand, tag-clouds, hierarchical faceting, and termsuggest are faceting use-cases that handle tokenization just fine.
Keep in mind that with faceting, the facet values returned in search results are the actual indexed terms, and not the stored value, which isn't used.
If you have conflicting indexing needs for a field, which is not uncommon, you will find it necessary to have a copy of a field just for faceting.
Types of faceting Solr's faceting is broken down into three types.
Solr 4 includes a new variation for field values called Pivot Faceting (that is Decision Tree)
Essentially, it performs recursive faceting for a series of fields.
Solr 3 deprecated "date faceting" with the introduction of the generic range faceting.
In the rest of this chapter, we will describe how to do these different types of facets.
But before that, there is one common parameter to enable faceting:
In all of the examples here, we've obviously set facet=true.
Faceting field values Field value faceting is the most common type of faceting.
The first example in this chapter demonstrated it in action.
Solr, in essence, iterates over all of the indexed terms for the field and tallies a count for the number of searched documents that have the term.
Sophisticated algorithms and caching makes this so fast that its overhead is usually negligible.
The remaining faceting parameters can be set on a per-field basis, otherwise they apply to all faceted fields that don't have a field-specific setting.
You will usually specify them per-field, especially if you are faceting on more than one field so that you don't get your faceting configuration mixed up.
When to specify facet.method Normally you should not specify facet.method, thereby letting Solr's internal logic choose an appropriate algorithm.
Solr will use a filter cache entry for each value, so keep that in mind when optimizing that cache's size.
Solr uses enum by default for boolean fields only, as it knows there can only be two values.
Alphabetic range bucketing Solr does not directly support alphabetic range bucketing (A-C, D-F, and so on)
However, with a creative application of text analysis and a dedicated field, we can achieve this with little effort.
Let's say we want to have these range buckets on the release names.
We need to extract the first character of r_name, and store this into a field that will be used for this purpose.
After validating these changes with Solr's analysis admin screen, we then re-index the data.
For the facet query, we're going to advise Solr to use the enum method, because there aren't many facet values in total.
Faceting numeric and date ranges Solr has built-in support for faceting numeric and date fields by a range and a divided interval.
I'll demonstrate an example against MusicBrainz release dates and another against MusicBrainz track durations, and then describe the parameters and their options.
This example demonstrates a few things, not only range faceting:
There is no data after that, since the data is out of date at this point.
Here is another example, this time using range faceting on a number—MusicBrainz track durations (in seconds):
Range facet parameters All of the range faceting parameters start with facet.range.
As with most other faceting parameters, they can be made field specific in the same way.
The remainder of these range faceting parameters can be specified on a per-field basis in the same fashion that the field-value faceting parameters can.
The lower boundary of a gap-based range is included if lower is specified.
It is also included if it's the first gap range and edge is specified.
The upper boundary of a gap-based range is included if upper is specified.
It is also included if it's the last gap range and edge is specified.
The upper boundary of the before range is included if the boundary is not already included by the first gap-based range.
The lower boundary of the after range is included if the boundary is not already included by the last gap-based range.
To ensure you don't double-count, don't choose both lower and upper together and don't choose outer.
Facet queries This is the final type of faceting, and it offers a lot of flexibility.
Instead of choosing a field to facet its values on or faceting a specified range of values, we specify some number of Solr queries that each itself becomes a facet.
For each facet query specified, the number of documents matching the facet query that also match the main search is counted.
Each facet query with its facet count is returned in the results under the facet_queries section.
In general, if field value faceting or range faceting don't do what you want, you can probably turn to facet queries.
Here are search results showing a few facet queries on MusicBrainz release dates.
I've used echoParams to make the search parameters clear instead of showing a lengthy URL.
An interesting thing to note about the facet query response is that the name of each of these facets is the query itself.
Building a filter query from a facet When faceting is used, it is usually used in the context of faceted navigation in which a facet value becomes a navigation choice for the user to filter on.
In Solr that becomes an additional filter query in a subsequent search.
The total matching documents of that search should be equal to the facet value count.
In this section we'll review how to build the filter queries.
I won't show an example for facet query faceting because there's nothing to do—the facet query is a query and can be supplied directly as an fq parameter.
To keep the filter queries easier to read I won't show them URL encoded.
Consider using the term query parser for all text field value faceting as it avoids escaping problems.
You might be wondering how to generate a filter query for the facet.missing facet, as there is no value to filter on.
Chapter 4, Searching covered a little known trick to query for a field with no indexed data involving a range query.
Facet range filter queries Range faceting is the most complicated to generate filter queries for.
The facet's name attribute is the start of the range.
The end of the range is the next facet value.
On the other hand if there is a hard end, then the last range end point is simply facet.
If you set this parameter to both lower and upper then the aforementioned filter query would be correct, but by default it's just lower (which is generally a good default that doesn't double-count)
If a date falls on precisely New Year's Eve of the new 2007 year then we don't want to count that date.
But this approach might have problems for floating point numbers.
Again, adding the NOT clause and the end point is universal, if perhaps verbose.
Excluding filters (multi-select faceting) Consider a scenario where you are implementing faceted navigation and you want to let the user pick several values of a field to filter on instead of just one.
Typically, when an individual facet value is chosen, this becomes a filter.
The filter makes subsequent faceting on that field almost pointless because the filter filters out the possibility of seeing other facet choices—assuming a single-valued field.
In this scenario, we'd like to exclude this filter for this facet field.
The preceding screenshot is from http://search-lucene.com in which you can search across the mailing lists, API documentation, and other places that have information about Lucene, Solr, and other related projects.
This screenshot shows that it lets users choose more than one type of information to filter results on at the same time, by letting users pick as many check boxes as they like.
I'll demonstrate the problem that multi-select faceting solves with a MusicBrainz example and then show how to solve it.
Here is a search for releases containing smashing, faceting on r_type.
We'll leave rows at 0 for brevity, but observe the numFound value nonetheless.
At this point, the user has not chosen a filter (therefore no fq)
As a result, now the URL is as before but has &fq=r_type%3AAlbum at the end and has this output:
Notice that the other r_type facet counts are gone because of the filter, yet in this scenario we want these to show the user what their counts would be if the filter wasn't there.
The reduced numFound of 29 is good though, because at this moment the user did indeed filter on a value so far.
Solr can solve this problem with some additional metadata on both the filter query and the facet field reference using Local-Params.
The local-params syntax was described in Chapter 4, Searching where it appears at the beginning of a query to switch the query parser and to supply parameters to it.
As you're about to see, it can also be supplied at the start of facet.field—a bit of a hack, perhaps, to implement this feature.
Remember to URL Encode this added syntax when used in the URL.
The facet counts are back, but numFound remains at the filtered 29:
Hierarchical faceting Imagine if your documents have some sort of taxonomy or other hierarchical label.
It could be a geographic location organized by continent, country, state/province, and then city.
Or it might be a large company's organizational hierarchy for employees.
A faceted navigation experience for such data might be a tree browser common in file navigation with an added facet count at each node.
The preceding screenshot is from Amazon, which has a simple hierarchical interface easily implemented with Solr.
Solr does not directly feature hierarchical faceting but it has the underlying capabilities for you to achieve it without modifying Solr, as you're about to see.
There are a number of ways to go about implementing hierarchical faceting; no bestpractice approach has emerged yet.
A big cause of the variation is that applications have different requirements.
The requirement that has the biggest impact on the ultimate approach, in my opinion, is the answer to this question: Will the user navigate the tree incrementally, one level at a time? Or will the application expand the tree automatically, possibly with a threshold like a depth or facet count for large trees?
If the tree is only going to be navigated incrementally by the user, then the solution set is quite simple.
Simply specify a string field for each level that exists in the hierarchy and index the data directly into them without doing anything special.
At query time, you use the field value faceting skills you've learned in this chapter without need for any special tricks.
The details are left as an exercise to the reader.
If the data is multi-valued, then the solution is mildly more complicated.
For each field you need to prefix the value with an ancestor path to disambiguate it.
For example, if you have a country field with USA and a province field with MA and a city field with Boston, then these fields would instead hold USA, USA/MA, USA/MA/ Boston, respectively.
You'll strip these prefixes out for display purposes, and you will need to use the facet.prefix parameter in your facet queries.
If the tree is going to be automatically expanded by depth, then the simplest approach I can suggest is to use Solr 4's new Pivot faceting feature, which was committed a long time ago.
You would still need to index the data as I just described for incremental navigation.
There are more complicated solutions involving a single tokenized field, which scales better to arbitrary hierarchy depths.
I haven't found a complete approach using this technique that doesn't require a Solr patch.
Summary Faceting is possibly the most valuable and popular Solr search component.
We've covered the three types of faceting, how to build filter queries from them, and some interesting use cases such as alphabetic range bucketing and hierarchical faceting.
Now you have the essential knowledge to put it to use in faceted navigation based user interfaces and other uses.
You've actually been using them already because performing a query, enabling debug output, and faceting are each actually implemented as search components.
But there's also search result highlighting, spelling correction, term-suggest, suggesting similar documents, collapsing/rolling up search results, editorially elevating or evicting results, and more!
You've actually been using several of them already: QueryComponent performs the actual searches (notably the q parameter), DebugComponent outputs the invaluable query debugging information when setting debugQuery, and FacetComponent performs the faceting we used in Chapter 6, Faceting.
In addition, there are many more that do all sorts of useful things that can really enhance your search experience:
These aren't really search components but they similarly add value to the search experience.
In a hurry? Search features like search result highlighting, query spell-checking, and query autocomplete suggestions are high-value for most search applications; don't miss them.
Take a peak at the others to see if they are applicable to you.
The search components used and their order are of course configurable.
What follows is our request handler for MusicBrainz releases but modified to explicitly configure the components:
The named search components in the above XML comment are the default ones that are automatically registered if you do not specify the components section.
This named list is also known as the standard component list.
To specify additional components, you can either re-specify components with changes, or you can add it to the first-components or last-components lists, which are prepended and appended respectively to the standard component list.
Many components depend on other components being executed first, especially the query component, so you will usually add components to last-components.
Search components must be registered with Solr to be activated so that they can then be referred to in a components list.
Here's an example of how a search component named elevator is registered in solrconfig.xml:
The functionality in QueryComponent, FacetComponent, and DebugComponent has been described in previous chapters.
The rest of this chapter describes other search components that come with Solr.
One thing to be aware of is that a sharded request will by default go to the default request handler, even if your client issued a request to another handler.
To ensure that the relevant search components are still activated on a sharded request, you can use the shards.qt parameter just as you would qt.
The Highlight component You are probably most familiar with search highlighting when you use an Internet search engine like Google.
Most search results come back with a snippet of text from the site containing the word(s) you search for, highlighted.
In the following screenshot we see Google highlighting a search including Solr and search (in bold):
A non-obvious way to make use of the highlighting feature is to not actually do any highlighting.
Instead Solr's highlighter can be used to inform the user which fields in the document satisfied their search, not to actually highlight the matched values.
In this scenario, there would be a search that searches across many fields or a catchall field and then hl.fl (the highlighted field list) is set to *
Solr will generate a snippet, but you ignore it aside from recognizing which field matched.
A highlighting example Admittedly the MusicBrainz data set does not make an ideal example to show off highlighting because there's no substantial text, but it can be useful nonetheless.
The following is a sample use of highlighting on a search for Corgan in the artist MusicBrainz data set.
Recall that the mb_artists request handler is configured to search against the artist name, alias, and members fields.
What should be noted in this example is the manner in which the highlighting results are in the response data.
Also note that not all of the result highlighting was against the same field.
It is possible to enable highlighting and discover that some of the results are not highlighted.
Sometimes this can be due to complex text analysis; although more likely it could simply be that there is a mismatch between the fields searched and those highlighted.
Highlighting configuration Highlighting, like most parts of Solr searching, is configured through request parameters.
You can specify them in the URL, but it is more appropriate to specify the majority of these in your application's request handler in solrconfig.xml because they are unlikely to change between requests.
Some parts of the highlighting configuration have defaults configured within the <highlighting/> element if you wish to change them there.
Understand that like faceting, nearly all of these parameters can be overridden on a per-field basis.
So many configuration options! There are more highlighting configuration parameters than any other part of Solr! However, it's the simplest to use, so don't let all these options overwhelm you.
Like most things in Solr, the defaults are quite reasonable.
The following are the parameters observed by the highlighter search component:
Set this to something reasonably close to hl.snippets * hl.fragsize to maintain consistent sizing in the results.
Note that the circumstantial presence of whatever values are chosen in the original text, such as HTML with pre-existing emphasis tags, are not escaped, and in rare circumstances may lead to a false highlight.
If the output is not going to be rendered on a web page, then consider changing them.
The regex fragmenter The various options available for the regex fragmenter are as follows:
The fast vector highlighter with multi-colored highlighting New in Solr 3.1 is the fast vector highlighter (FVH), which is an alternative underlying algorithm with additional schema field requirements.
When in use for a field, highlighting on that field is faster, especially for long text common in indexing full documents.
As an added bonus, it has the option of highlighting each query term with different markup such as a distinct color.
Use the fast vector highlighter for web or document indexing If your use of Solr involves large text fields, as is common in web or document crawling, then use the FVH for that text field.
You needn't index all fields this way, just the big ones—probably just one.
Instructing Lucene to store term vectors with positions and offsets will increase disk requirements.
A very rough estimate is 20% of the stored field value size.
The following are additional configuration parameters specific to the fast vector highlighter:
The simple one does not take care to create snippets at word boundaries—see LUCENE-1824
As a quick fix, you could manually trim the start and end of the snippet to the nearest whitespace character before displaying it to the user.
If commas are present, it is treated as a list.
The fragments builder will use a consistent index into the list for a queried term.
If there are more queried terms than pre or post tags then it loops back to the front of the array recursively.
Notice that hl.tag.pre and hl.tag.post is a different pair of parameters than hl.simple.pre and hl.simple.post for the non-FVH.
Since the FVH is typically used on a subset of highlighted fields, you should configure both pairs of parameters consistently to get consistent highlighting tags.
Arguably the snippet shouldn't span values in the first place—the non-FVH will not.
Set hl.phraseLimit to 5000 to protect against pathologically bad cases on large documents.
You might even consider a value like 1 if it is sufficient to simply highlight the first possible phrase in the document instead of the highest scoring one, in exchange for a substantial improvement in highlighting performance.
The SpellCheck component One of the better ways to enhance the search experience is by offering spelling corrections.
This is sometimes presented at the top of search results with such text as "Did you mean ..."
A related technique is to use fuzzy queries using the tilde syntax.
However, fuzzy queries don't tell you what alternative spellings were used, are not as scalable for large indexes, and might require more programming than using this search component.
For spelling corrections to work, Solr must clearly have a corpus of words (a dictionary) to suggest alternatives to those in the user's query.
Solr can be configured in either of the following two ways:
Schema configuration If your dictionary is going to be based on indexed content as is recommended, then a field should be set aside exclusively for this purpose.
This is so that it can be analyzed appropriately and so that other fields can be copied into it as the index-based dictionary uses just one field.
Most Solr setups would have one field; our MusicBrainz searches, on the other hand, are segmented by the data type (artists, releases, tracks), and so one for each would be best.
For the purposes of demonstrating this feature, we will only do it for artists.
In schema.xml, we need to define the field type for spellchecking.
This particular configuration is one I recommend for most scenarios:
A field type for spellchecking is not marked as stored because the spellcheck component only uses the indexed terms.
The important thing is to ensure that the text analysis does not do stemming as the corrections presented would suggest the stems, which would look very odd to the user for most stemmer algorithms.
It's also hard to imagine a use-case that doesn't apply lowercasing.
Now we need to create a field for this data:
And we need to get data into it with some copyField directives:
Arguably, a_member_name may be an additional choice to copy as well, as the dismax search we configured (seen in the following code) searches it too, albeit at a reduced score.
This, as well as many decisions with search configuration, is subjective.
Configuration in solrconfig.xml To use any search component, it needs to be in the components list of a request handler.
The spellcheck component is not in the standard list so it needs to be added.
Within the spellchecker search component, there are one or more XML blocks named spellchecker so that different dictionaries and other options can be configured.
These might also be loosely referred to as the dictionaries, because the parameter that refers to this choice is named that way (more on that later)
A complete MusicBrainz implementation would have a different spellchecker for each MB data type, with all of them configured similarly.
Following the excerpt below is a description of all options available:
Configuring spellcheckers (dictionaries) The double layer of spellchecker configuration is perhaps a little confusing.
The outer one just names the search component—it's just a container for configuration(s)
The inner ones are distinct configurations to choose at search time.
The following options are common to both index and file based spellcheckers:
In our spellchecker named jarowinkler, we're actually referring to another spellchecker's index so that we can try other configuration options without having to duplicate the data or building time.
For a high load Solr server, an in-memory index is appealing.
Until SOLR-780 is addressed, you'll have to take care to tell Solr to build the dictionary whenever the Solr core gets loaded.
This happens at startup or if you tell Solr to reload a core.
Warning: This option name is actually common to both types of spellcheckers but is defined differently.
If there is a lot of data and lots of common words, as opposed to proper nouns, then this threshold should be effective.
If testing shows spelling candidates including strange fluke words found in the index, then introduce a threshold that is high enough to weed out such outliers.
The threshold will probably be less than 0.01—1 percent of documents.
We've not yet discussed the parameters to a search with the spellchecker component enabled.
But at this point of the configuration discussion, understand that you have a choice of just letting the user query, q get processed or you can use spellcheck.q.
Processing of the q parameter When a user query (q parameter) is processed by the spellcheck component to look for spelling errors, Solr needs to determine what words are to be examined.
The first step is to pull out the queried words from the query string, ignoring any syntax such as AND.
The next step is to process the words with an analyzer so that, among other things, lowercasing is performed.
If left unspecified, there would be no text-analysis, which would in all likelihood be a misconfiguration.
This algorithm is implemented by a spellcheck query converter—a Solr extension point.
Processing of the spellcheck.q parameter If the spellcheck.q parameter is given (which really isn't a query per se), then the string is processed with the text analysis referenced by the fieldType option of the spellchecker being used.
If a file-based spellchecker is being used, then you should set this explicitly.
Index-based spellcheckers will sensibly use the field type of the referenced indexed spelling field.
The dichotomy of the ways in which the analyzer is configured between both q and spellcheck.q, arguably needs improvement.
Building the dictionary from its source Each spellchecker requires it to be built, which is the process in which the dictionary is read and is built into the spellcheckIndexDir.
If it isn't built, then no corrections will be offered, and you'll probably be very confused.
You'll be even more confused troubleshooting the results if it was built once but is far out of date and needs to be built again.
Generally, building is required if it has never been built before, and it should be built periodically when the dictionary changes.
It need not necessarily be built for every change, but it obviously won't benefit from any such modifications.
Using buildOnOptimize or buildOnCommit is a low-hassle way to keep the spellcheck index up to date.
If you rarely optimize or if you commit too frequently then you'll instead have to issue build commands manually on a suitable time period.
Furthermore, setting spellcheckIndexDir will ensure the built spellcheck index is persisted between Solr restarts.
It is important to note that only one spellchecker (dictionary) was built.
To build more than one, separate requests must be issued.
This doesn't rebuild the index, but it basically re-establishes connections with the index (both sourceLocation for index-based spellcheckers and spellcheckIndexDir for all types)
If you've decided to have some external process build the dictionary or simply share built indexes between spellcheckers as we've done, then Solr needs to know to reload it to see the changes—a quick operation.
Issuing spellcheck requests At this point, we've covered how to configure the spellchecker and dictionaries but not how to issue requests that actually use it.
Let's finally do it! Fortunately, there aren't many search parameters governing this end of the component.
Which should you use?: spellcheck.q or q Assuming you're handling user queries to Solr that might contain some query syntax, then the default q is right, as Solr will then know to filter out possible uses of Lucene/Solr's syntax such as AND, OR, fieldname:word, and so on.
If not, then spellcheck.q is preferred, as it won't go through that unnecessary processing.
Although counter-intuitive, raising this number affects the suggestion ordering—the results get better! The internal algorithm sees ~10 times as many as this number and then it orders them by closest match.
This parameter is good for situations where the user's query returns very few results, or when no results and no spellcheck suggestion was offered due to the query being correctly spelled (correctlySpelled is true in extended output)
When this happens, consider automatically issuing a secondary search with this parameter enabled and with spellcheck.
If this is nonzero, then the spellchecker will not return collations that yield no results.
Without this limit, queries with many misspelled words could yield a combinatoric explosion of possibilities.
It adds the collation hits (number of documents found) and a mapping of misspelled words to corrected words.
Furthermore, ensure the collation is verified to return results by setting spellcheck.
Example usage for a misspelled query We'll try out a typical spellcheck configuration that we've named a_spell.
I've disabled showing the query results with rows=0 because the actual query results aren't the point of these examples.
In this example, it is imagined that the user is searching for the band Smashing Pumpkins, but with a misspelling.
Here are the search results for Smashg Pumpkins using the a_spell dictionary:
In this scenario, I intentionally chose a misspelling that is closer to another word: "smash"
Were it not for maxCollationTries, the suggested collation would be "smash Pumpkins" which would return no results.
There are a few things I want to point out regarding the spellchecker response:
There is an extension point to the spellchecker to customize the ordering—search Solr's wiki for comparatorClass for further information.
You could write one that orders results based on a formula fusing both the suggestion score and document frequency.
It is typically displayed as a drop-down menu that appears automatically after typing.
Finally, at this point you could index the field with an EdgeNGramTokenizer and perform searches against it, or use a KeywordTokenizer and then use one of the approaches listed for query term completion below.
I recommend reading this excellent article by Jay Hill on doing this with EdgeNGrams: http://www.lucidimagination.com/blog/2009/09/08/auto-suggestfrom-popular-queries-using-edgengrams/
Monitor your user's queries! Even if you don't plan to do query log completion, you should capture useful information about each request for ancillary usage analysis, especially to monitor which searches return no results.
Capture the request parameters, the response time, the result count, and add a timestamp.
There are other interesting query completion concepts I've seen on sites too, and some of these can be combined effectively.
I'll now describe the three approaches to implementing Query term completion.
It's a popular type of query completion, and the three approaches highlight different technologies within Solr.
Query term completion via facet.prefix Most people don't realize that faceting can be used to implement query term completion, but it can.
This approach has the unique and valuable benefit of returning completions filtered by filter queries (that is faceted navigation state), and by query words prior to the last one being completed.
This means the completion suggestions that, if chosen, will result in a search that has results, which is not the case for the other techniques.
However, there are limits to its scalability in terms of memory use and inappropriateness for real-time search applications…
Faceting on a tokenized field is going to use an entry in the field value cache (based on UnInvertedField) to hold all words in memory.
It will use a hefty chunk of memory for many words and it's going to take a non-trivial amount of time to build this cache on every commit during the auto-warming phase.
For a data point, consider MusicBrainz' largest field: t_name (track name)
The mandatory initialization per-commit makes this approach unsuitable for real-time-search applications (See Chapter 10, Scaling Solr for more information)
Perform a trivial query to trigger its initialization and measure how long it takes.
For this example, we have a search box searching track names and it contains:
All of the words here except the last one become the main query for the termsuggest.
If there isn't anything, then we'd want to ensure that the request handler used would search for all documents.
The faceted field is a_spell, and we want to sort by occurrence.
We also want there to be at least one occurrence, and we don't want more than five suggestions.
This leaves the facet.prefix faceting parameter to make this work.
This parameter filters the facet values to those starting with this value.
Remember that facet values are the final result of text analysis, and therefore are probably lowercased for fields you might want to do term completion on.
You'll need to pre-process the prefix value similarly, or else nothing will be found.
We're going to set this to ja, the last word that the user has partially typed.
When setting this up for real, I recommend creating a request handler just for term completion with many of these parameters defined there, so that they can be configured separately from your application.
In this example, we're going to use Solr's JSON response format.
This is exactly the information needed to populate a pop-up menu of choices that the user can conveniently choose.
However, there are some issues to be aware of with this feature:
Query term completion via the Suggester New in Solr 3 is a high-speed approach to implement term completion, called the Suggester.
The Suggester is not its own search component; it's an extension of the spell-check component.
This means that it's not necessarily as up-to-date as your index and it needs to be built, but the Suggester only takes a couple seconds or so for this, and you are not forced to do this per-commit, unlike with faceting.
The Suggester principally features the fastest search performance—a handful of milliseconds per search at most.
The FST uses ~1/20th the memory of faceting's UnInvertedField data structure.
Additionally, the Suggester uniquely includes a method of loading its dictionary from a file that optionally includes a sorting weight.
Solr does not include a sample configuration of how to use the Suggester, so I'll provide one here.
We're going to use it for MusicBrainz artist name completion.
The first part of this is a request handler definition just for using the Suggester.
Increase weightBuckets In my experience, increasing weightBuckets to 100 or more improves the fidelity of the weights, resulting in more relevant suggestions.
It is unclear at what value it is negligible or not.
The second part of this is an instantiation of the spell-check search component but named suggest.
What makes this use the Suggester is the classname setting referencing the implementation code.
The dictionary here is loaded from the a_spell field in the main index, but if a file is desired, then you can provide the sourceLocation parameter.
The document frequency threshold for suggestions is commented here because MusicBrainz has unique names that we don't want filtered out.
As the Suggester is based on the spell-check component, it needs to be built, which is the process of loading the dictionary into memory.
If you try to get suggestions beforehand, there will be no results.
Suggestions only take a couple seconds or so to build and so I recommend building it automatically on startup via a firstSearcher warming query in solrconfig.xml:
To be kept up-to-date, it needs to be re-built from time to time.
If commits are infrequent, you should use the buildOnCommit setting.
For more information about the Suggester, see the wiki: http://wiki.
You'll find information on alternatives to the FST implementation, and other details.
However, some secrets of the Suggester are still undocumented, buried in the code.
Query term completion via the Terms component The Terms component is used to expose raw indexed term information, including term frequency, for an indexed field.
It has a lot of options for paging into this voluminous data and filtering out terms by term frequency.
For implementing suggestion functionality, the terms component has the benefit of using no Java heap memory and consequently there is no initialization penalty.
It's always up to date with the indexed data, like faceting but unlike the Suggester.
The performance is typically good but for high query load on large indexes, it will suffer compared to the other approaches.
An interesting feature unique to this approach is a regular expression term match option.
This can be used for case-insensitive matching, but it probably doesn't scale too many terms.
Here is how to set it up in solrconfig.xml for suggestions of MusicBrainz artist names:
And here is a request to complete the word sma:
The QueryElevation component At times, it may be desired to make editorial / manual modifications to the search results of particular user queries.
This might be done as a solution to a popular user query that doesn't score an expected document sufficiently high—if it even matched at all.
The query might have found nothing at all, perhaps due to a common misspelling.
The opposite may also be true: the top result for a popular user query might yield a document that technically matched according to your search configuration, but certainly isn't what you were looking for.
Another usage scenario is implementing a system akin to paid keywords for certain documents to be on top for certain user queries.
This feature isn't a general approach to fix queries not yielding effective search results; it is a band-aid for that problem.
If a query isn't returning an expected document scored sufficiently high enough (if at all), then use Solr's query debugging to observe the score computation.
You may end up troubleshooting text analysis issues too, if a search query doesn't match an expected document—perhaps by adding a synonym.
The end result may be tuning the boosts or applying function queries to incorporate other relevant fields into the scoring.
When you are satisfied with the scoring and just need to make an occasional editorial decision, then this component is for you.
Configuration This search component is not in the standard component list and so it must be registered with a handler in solrconfig.xml.
Here we'll add it to the mb_artists request handler definition, just for this example, anyway.
This excerpt also reveals the registration of the search component using the same name as that referenced in last-components.
A name was chosen reflecting the fact that this elevation configuration is only for artists.
There are three named configuration parameters for a query elevation component, and they are explained as follows:
This presents an interesting option if the elevation choices need to be loaded more often.
In this elevation file, we've specified that when a user searches for corgan, the Smashing Pumpkins then Green Day should appear in the top two positions in the search results (assuming typical sort of a descending score) and that the artist Starchildren is to be excluded.
Note that query elevation kicks in when the configured query text matches the user's query exactly, while taking into consideration configured text analysis.
Thus a search for billy corgan would not be affected by this configuration.
This component is quite simple with unsurprising results, so an example of this in action is not given.
Normally a default sort results in the first document having the same score as the maximum score, but in this case that happens at the third position, as the first two were inserted by this component.
Moreover, normally a result document has a score greater than 0, but in this case one was inserted by this component that never matched the user's query.
The MoreLikeThis component Have you ever searched for something and found a link that wasn't quite what you were looking for but was reasonably close? If you were using an Internet search engine such as Google, then you may have tried the "more like this…" link next to a search result.
Some sites use other language like "find similar..." or "related documents…" As these links suggest, they show you pages similar to another page.
The MLT capability in Solr can be used in the following three ways:
The essences of the internal workings of MLT operate like this:
Otherwise get the stored text, and re-analyze it to derive the terms (slower)
If the input document is posted as text to the request handler, then analyze it to derive the terms.
The analysis used is that configured for the first field listed in mlt.fl.
Construct a query with these interesting terms across all of the fields listed in mlt.fl.
Configuration parameters In the following configuration options, the input document is either each search result returned if MLT is used as a component, or it is the first document returned from a query to the MLT request handler, or it is the plain text sent to the request handler.
Parameters specific to the MLT search component Using the MLT search component adorns an existing search with MLT results for each document returned.
Parameters specific to the MLT request handler Using the MLT request handler is more like a regular search except that the results are documents similar to the input document.
Additionally, any filters (the fq parameter) that are specified are also in effect.
Additionally, remember to configure the MLT request handler in solrconfig.xml.
An example of this is shown later in the chapter.
Common MLT parameters These parameters are common to both the search component and request handler.
Some of the thresholds here are for tuning which terms are interesting to MLT.
In general, expanding thresholds (that is, lowering minimums and increasing maximums) will yield more useful MLT results at the expense of performance.
Usage advice For ideal query performance, ensure that termVectors is enabled for the field(s) referenced in mlt.fl.
In order to further increase performance, use fewer fields, perhaps just one dedicated for use with MLT.
Using the copyField directive in the schema makes this easy.
The disadvantage is that the source fields cannot be boosted differently with mlt.qf.
However, you might have two fields for MLT as a compromise.
The field needn't be stored if its data is copied from some other field that is stored.
During an experimentation period, look for interesting terms that are not so interesting for inclusion in the stop word list.
Lastly, some of the configuration thresholds which scope the interesting terms can be adjusted based on experimentation.
The MusicBrainz data set is not conducive to applying the MLT feature, because it doesn't have any descriptive text.
If there were perhaps an artist description and/or widespread use of user-supplied tags, then there might be sufficient information to make MLT useful.
However, to provide an example of the input and output of MLT, we will use MLT with MusicBrainz anyway.
We'll be using the request handler method, the recommended approach.
The MLT request handler needs to be configured in solrconfig.xml.
The important bit is the reference to the class, the rest of it is our prerogative.
This configuration shows that we're basing the MLT on just track names.
Let's now try a query for tracks similar to the song "The End is the Beginning is the End" by The Smashing Pumpkins.
The query was performed with echoParams to clearly show the options used:
The fact that so many documents were found is not material to any MLT response; all it takes is one interesting term in common.
The interesting terms were deliberately requested so that we can get an insight on the basis of the similarity.
The fact that is and the were included shows that we don't have a stop list for this field—an obvious thing we'd need to fix.
Nearly any stop list is going to have such words.
For further diagnostic information on the score computation, set debugQuery to true.
This is a highly advanced method but exposes information invaluable to understand the scores.
Doing so in our example shows that the main reason the top hit was on top was not only because it contained all of the interesting terms as did the others in the top 5, but also because it is the shortest in length (a high fieldNorm)
The #5 result had "Beginning" twice, which resulted in a high term frequency (termFreq), but it wasn't enough to bring it to the top.
The Stats component The Stats component computes some mathematical statistics of specified numeric fields in the index.
The following statistics are computed over the non-null values (except missing which counts the nulls):
Configuring the stats component This component performs a simple task and so as expected, it is also simple to configure.
This parameter can be set multiple times in order to perform statistics on more than one field.
Due to bug SOLR-1782, a stats.facet field should not be multivalued, and it should be limited to a string.
If you don't heed this advice then the results are in question and you may get an error!
Statistics on track durations Let's look at some statistics for the duration of tracks in MusicBrainz at:
An example using stats.facet would produce a much longer result, which won't be given here in order to leave space for other components.
The Clustering component The Clustering component is a Solr contrib module that provides an extension point to integrate a clustering engine.
Clustering is a technology that groups documents into similar clusters, using sophisticated statistical techniques.
Each cluster is identified by a few words that were used to distinguish the documents in that cluster from the other clusters.
As with the MoreLikeThis component which also uses statistical techniques, the quality of the results is hit or miss.
The primary means of navigation / discovery of your data should generally be search and faceting.
For so-called un-structured text use cases, there are, by definition, few attributes to facet on.
Clustering search results and presenting tag-clouds (a visualization of faceting on words) are generally exploratory navigation methods of last-resort in the absence of more effective document metadata.
Presently, there are two search-result clustering algorithms available as part of the Carrot2 open source project that this module has adapters for.
The clustering component has an extension point to support document clustering with anticipation of a solution coming from the Apache Mahout open source project, but it has yet to materialize.
Document clustering is different than search-result clustering as in that it is calculated on the entire corpus in advance of searches.
To get started with exploring this feature, I'll direct you to Solr's wiki: http://wiki.
There is "quick start" set of instructions in which you'll be clustering Solr's example documents in under five minutes.
It should be easy to copy the necessary configuration to your Solr instance and modify it to refer to your document's fields.
As you dive into the technology, Carrot2's powerful GUI workbench should be of great help in tuning it to get more effective results.
For a public demonstration of Carrot2's clustering, go here: http://search.
Result grouping/Field collapsing Result Grouping and Field Collapsing are two names that suggest different ways of looking at the same Solr feature.
Result grouping, the preferred name, is the ability to group the search results by a field value or some other criteria.
This is very useful in using Solr for reporting but also for presenting top search results from multiple document categories.
Field collapsing, the original name, is having the ability to collapse (that is remove / withhold) search result documents that have the same field value as a previous document.
A common use-case is to mimic how Google only shows one result for a website.
The feature is roughly similar to a SQL group by query; but the SQL incarnation returns aggregate summary rows for each group whereas in Solr you get back a configurable number of the top rows for each group.
This important feature was officially first released in Solr 3.3
For an example of this feature in MusicBrainz, consider attempting to provide a search for tracks where the tracks collapse to the artist.
If a search matches multiple tracks produced by the same artist, then only the highest scoring track will be returned for that artist.
That particular document in the results can be said to have "rolled-up" or "collapsed" those that were removed.
A track's artist ID is stored in t_a_id with field type long.
The grouping capability in Solr 3 is limited to grouping on string fields; doing otherwise yields an error.
I've highlighted the beginning part of the grouping, which reflects that a grouped response has a fairly different response structure than a regular one.
The matches number is 87 which is equivalent to numFound attribute when there is no groupingthe number of matching documents.
Each group begins by showing the group's value and then a document list structure that looks just like normal search results.
Instead of this grouped response, you'll see a parameter soon that will use the normal response format but it lacks information like how many documents are in the group.
Configuring result grouping Result grouping is actually not a Solr component but it feels like one because it modifies the response and it has its own set of parameters that start with a common prefix.
Due to the ways it must interact with the search, it is implemented internally as an adjunct to the Query component.
Grouping is either by field value, as we have explained thus far, or by documents that match a given query.
This is analogous to how the facet component features field value faceting (facet.field) and facet queries (facet.query), but for grouping.
You can set the corresponding grouping parameters any number of times, but you must set at least one of them or you'll get an error.
The following are a list of the query parameters to configure result grouping:
Additionally, the field can only be a string; however, Solr 4 lifts that restriction.
This parameter can be set multiple times, which adds separate group lists to the response.
Grouping on multiple fields as one grouping is not supported.
You can achieve this by creating a combined field in the index and grouping on that.
Remember that result grouping is fairly new, with improvements to it happening at a quick pace.
The TermVector component This component is used to expose the raw term vector information for fields that have this option enabled in the schema—termVectors set to true.
It lists each indexed term in order with the offsets into the original text, term frequency, and document frequency.
It's not that useful so I'll refer you to the wiki for further information:
By now it should be clear why the text search capability of your database is inadequate for all but basic needs.
Even Lucene-based solutions don't necessarily have the extensive feature-set that you've seen here.
You may have once thought that searching was a relatively basic thing, but Solr search components really demonstrate how much more there is to it.
The chapters thus far have aimed to show you the majority of the features in Solr and to serve as a reference guide for them.
In the next chapter, you're going to learn about various deployment concerns, such as logging, testing, security, and backups.
Deployment Now that you have identified the information you want to make searchable, built the Solr schema to support your expected queries, and made the tweaks to the configuration you need, you're ready to deploy your Solr based search platform into production.
While the process of deployment may seem simple after all of the effort you've gone through in development, it brings its own set of challenges.
In this chapter, we'll look at the issues that come up when going from "Solr runs on my desktop" to "Solr is ready for the enterprise"
Deployment methodology for Solr There are a number of questions that you need to ask yourself in order to inform the development of a smooth deployment strategy for Solr.
The deployment process should ideally be fully scripted and integrated into the existing Configuration Management (CM) process of your application.
Configuration Management is the task of tracking and controlling changes in the software.
Questions to ask The list of questions that you'll want to answer to work in conjunction with your operations team include:
Installing Solr into a Servlet container Solr is deployed as a simple WAR (Web application archive) file that packages up servlets, JSP pages, code libraries, and all of the other bits that are required to run Solr.
Therefore, Solr can be deployed into any Java EE Servlet Container that meets the Servlet 2.4 specification, such as Apache Tomcat, Websphere, JRun, and GlassFish, as well as Jetty, which ships with Solr to run the example app.
Differences between Servlet containers The key thing to resolve when working with Solr and the various Servlet containers is that technically you are supposed to compile a single WAR file and deploy that into the Servlet container.
It is the container's responsibility to figure out how to unpack the components that make up the WAR file and deploy them properly.
In contrast, with Apache Tomcat, you place the solr.war file into the /webapp directory.
When you either start up Tomcat, or Tomcat notices the new .war file, it unpacks it into the /webapp directory.
Therefore, you will have the original /webapp/solr.war and the newly unpacked (exploded) /webapp/ solr version.
The Servlet specification carefully defines what makes up a WAR file.
However, it does not define exactly how to unpack and deploy the WAR files, so your specific steps will depend on the Servlet container you are using.
For information specific to various servlet containers, see Solr's wiki: http://wiki.apache.org/solr/SolrInstall.
If you are not strongly predisposed to choosing a particular Servlet container, then consider Jetty, which is a remarkably lightweight, stable, and fast Servlet container.
While written by the Jetty project, they have provided a reasonably unbiased summary of the various reasons to choose Jetty at http://www.webtide.com/choose/jetty.jsp.
Defining solr.home property Probably the biggest thing that trips up folks deploying into different containers is specifying the solr.home property.
Solr stores all of its configuration information outside of the deployed webapp, separating the data part from the code part for running Solr.
In the example app, while Solr is deployed and running from a subdirectory in /work, the solr.home directory is pointing to the top level /solr directory, where all of the data and configuration information is kept.
You can think of solr.home as being analogous to where the data and configuration is stored for a relational database like MySQL.
You don't package your MySQL database as part of the WAR file, and nor do you package your Lucene indexes.
By default, Solr expects the solr.home directory to be a subdirectory called /solr in the current working directory as defined by the Servlet container.
Alternatively, you may find it easier to specify the solr.home property by appending it to the JAVA_OPTS system variable.
Or lastly, you may choose to use JNDI with Tomcat to specify the solr.home property as well as where the solr.war file is located.
By configuring the stanza appropriately, I was able to load up the solr.war (cit) file and home directory from the example configuration shipped with Jetty using Tomcat instead.
Note the somewhat confusing JNDI name for solr.home is solr/ home.
This is because JNDI is a tree structure, with the home variable being specified as a node of the Solr branch of the tree.
By specifying multiple different context stanzas, you can deploy multiple separate Solr instances in a single Tomcat instance.
Logging Solr's logging facility provides a wealth of information, from basic performance statistics, to what queries are being run, to any exceptions encountered by Solr.
The log files should be one of the first places to look when you want to investigate any issues with your Solr deployment.
For example, the default configuration for managing the server logs in Jetty is defined in jetty.xml:
The log directory is created in the subdirectory of the Jetty directory.
If you have multiple drives and want to store your data separately from your application directory, then you can specify a different directory.
Depending on how much traffic you get, you should adjust the number of days to preserve the log files.
I recommend you keep the log files for as long as possible by archiving them.
The search request data in these files is some of the best data available to help you improve the relevancy of your search results.
By using web analytics tools such as a venerable commercial package WebTrends or the open source AWStats package to parse your request logs, you can quickly visualize how often different queries are run, and what search terms are frequently being used.
This leads to a better understanding of what your users are searching for.
Tailing the HTTP logs is one of the best ways to keep an eye on a deployed Solr.
You'll see each request as it comes in and can gain a feel for what types of transactions are being performed, whether it is frequent indexing of new data, or different types of searches being performed.
A pause in the logging will quickly highlight Garbage Collection issues!
The request time data will let you quickly see performance issues.
You can see the first request is a POST to the /solr/ update URL from a browser running locally (127.0.0.1) with the date.
The request was successful, with a 200 HTTP status code being recorded.
The second line shows a request for the admin page being made, which also was successful and took a slow 3,816 milliseconds, primarily because in Jetty, the JSP page is compiled the first time it is requested.
The last line shows a search for dell being made to the /solr/select URL.
On a faster machine with more memory and a properly "warmed" Solr cache, you can expect a few 10s of millisecond result time.
Unfortunately, you don't get to see the number of results returned, as this log only records the request.
While you may not see things quite the same way Neo did in the movie The Matrix, you will get a good gut feeling about how Solr is performing!
AWStats is a full-featured open source request log file analyzer under the GPL license and available from http://awstats.
Solr application logging Logging events is a crucial part of any enterprise system.
Veteran Java programmers know that the history of Java and logging is complicated, resulting in a fragmented landscape.
The default distribution of Solr targets Java's built-in logging (aka JDK logging), but now alternative more powerful packages like Log4j or Logback are easily supported.
Configuring logging output By default, Solr's JDK logging configuration sends its logging messages to the standard error stream:
Obviously, in a production environment, Solr will be running as a service, which won't be continuously monitoring the standard error stream.
You will want the messages to be recorded to a log file instead.
By specifying two log handlers, you can send output to the console as well as log files.
The log files are appended, so that you can restart Solr and not lose previous logging information.
Note, if you are running Solr as a service, it is probably going to redirect the STDERR output from the ConsoleHandler to a log file as well.
In that case, you will want to remove the java.util.
You might choose to configure Solr to use it instead, for any number of reasons:
Of course, you must also place Log4j's JAR file in that directory.
Make sure that you download the distribution that matches the version of SLF4J packaged with Solr or upgrade Solr's versions.
As Erik Hatcher in a post to the solr-dev mailing list memorably called it: JARmageddon.
Jetty startup integration Regardless of which logging solution you go with, you don't want to make the startup arguments for Solr more complex.
You can leverage Jetty's configuration to specify these system properties during startup.
Managing log levels at runtime Sometimes you need more information than you are typically logging to debug a specific issue, so Solr provides an admin interface at http://localhost:8983/ solr/admin/logging to change the logging verbosity of the components in Solr.
While you can't change the overall setup of your logging strategy, such as the appenders or file rollover strategies at runtime, you can change the level of detail to log without restarting Solr.
SolrCore to FINE level of logging, then make a search request to see more detailed information.
One thing to remember is that these customizations are NOT persisted through restarts of Solr.
If you find that you are reapplying log configuration changes after every restart, then you should change your default logging setup to specify custom logging detail levels.
Even if you adjust the logging levels here to something more detailed, you still probably won't see the messages in the console.
One of the challenges with logging is that you need to log enough details to troubleshoot issues, but not so much that your log files become ridiculously large and you can't winnow through the information to find what you are looking for.
Tools have arisen to manage those log files and make actionable decisions on the information stored within.
Splunk is one commercial product, another is Loggly, a cloud based logging tool that is based on Solr! More information is available at http://www.splunk.com/ and http://www.loggly.com.
A SearchHandler per search interface? Two questions to answer early on when configuring Solr and thinking about who the consumers of the search services are:
If you are providing generic search functionality to an unknown set of clients, then you may have just a single request handler handling search requests at /solr/ select, which provides full access to the index.
However, it is likely that Solr is powering interfaces for one or more applications that you know are going to make certain specific kinds of searches.
For example, say you have an e-commerce site that supports searching for products.
In that case, you may want to only display products that are available for purchase.
A specifically named request handler that always returns the stock products (using appends, as fq can be specified multiple times) and limits the rows to 50 (using invariants) would be appropriate:
However, the administrators of the same site would want to be able to find all products, regardless of if they are in stock or not.
They would be using a different search interface and so you would provide a different request handler that returns all of the information available about your products:
Later on, if your site needs to change, or if the internal searching site changes, particularly with respect to tuning search relevancy, you can easily modify the appropriate request handler without impacting other applications interacting with Solr.
However, this doesn't look quite as clean as having specific URLs like /solr/allproducts.
A fully named request handler can also have access to them controlled by use of Servlet security (see the Securing Solr from prying eyes section later in this chapter)
Leveraging Solr cores Recall from Chapter 2, Schema and Text Analysis that you can either put different types of data into a single index or use separate indexes.
Up to this point, the only way you would know how to use separate indexes is to actually run multiple instances of Solr.
However, adding another complete instance of Solr for each type of data you want to index is a rather cumbersome process!
Solr cores allow multiple separate indexes to exist within a single Solr server instance as well as bringing features like hot core reloading and swapping that make administration easier.
Each Solr core consists of its own configuration files and data.
Performing searches and indexing in a multicore setup is almost the same as using Solr without cores.
You just add the name of the core to the individual URLs.
Other than the introduction of the core name in the URL, you still perform all of your management tasks, searches, and updates in the same way as you always did in a single core setup.
Configuring solr.xml When Solr starts up, it checks for the presence of a solr.xml file in the solr.home directory.
If one exists, then it loads up all the cores defined in solr.xml.
We've used multiple cores in the sample Solr setup shipped with this book to manage the various indexes used in the examples.
Notice that three of the cores: mbtracks, mbartists, and mbreleases all share the same instanceDir of mbtype? This allows you to make configuration changes in one location and affect all three cores.
Each core is configured via a fairly obvious set of properties:
You can supply the property value in a number of ways:
Include fragments of XML with XInclude XInclude stands for XML Inclusions and is a W3C standard for merging a chunk of XML into another document.
Solr has support for using XInclude tags in solrconfig.xml to incorporate a chunk of xml at load time.
If the XML file defined by the href attribute isn't found, then the xi:fallback included file is returned.
The fallback metaphor is primarily if you are including XML files that are loaded via HTTP and might not be available due to network issues.
Managing cores While there isn't a nice GUI for managing Solr cores the way there is for some other options, the URLs you use to issue commands to Solr cores are very straightforward, and they can easily be integrated into other management applications.
The response by default is XML, but you can also return results in JSON by appending wt=json to the command.
If you specify persistent="true" in solr.xml, then these changes will be preserved through a reboot by overwriting solr.xml to reflect the changes.
We'll cover a couple of the common commands using the example Solr setup in ./examples.
The individual URLs listed below are stored in plain text files in ./ examples/8/ to make it easier to follow along in your own browser:
Why use multicore? Solr's support of multiple cores in a single instance enables you to serve multiple indexes of data in a single Solr instance.
Multiple cores also address some key needs for maintaining Solr in a production environment:
We strongly encourage you to start with the multiple core approach, even if your solr.xml only has a single core listed! While slightly more complex then just having a single index, using multi core allows you to take advantage of all the administrative goodness of cores.
We expect the concept of a single core will be deprecated in the future as multiple cores are the key to Solr's support for massively distributed indexes and/or huge numbers of individual indexes.
You can learn more about Solr core related features at http://wiki.apache.org/ solr/CoreAdmin.
Monitoring Solr performance Ensuring that Solr is meeting the SLA expectations of the enterprise is the goal of monitoring.
Solr provides both XML and JMX hooks to allow you to integrate Solr into your enterprise monitoring platform.
Don't have your own monitoring platform? There are two offerings, available from New Relic (http://newrelic.com) and Sematext (http://sematext.com/spm/) that provide a comprehensive monitoring solution.
Both are cloud based which communicate via a small agent installed into Solr and provide a wealth of statistics and analysis about the JVM, as well as Solr specific metrics such as request response time and throughput, cache hit rate, and indexing performance.
Stats.jsp From the admin interface, when you click on the Statistics link you receive a web page of information about a specific index.
However, what isn't immediately obvious is that this information is actually being served up to the browser as XML with an embedded link to an XSL style sheet that transforms it in the browser into HTML.
This means that if you perform a GET request on stats.jsp you get back XML:
Open the downloaded file and you will see all the data as XML.
Below is an excerpt of the statistics available for the cache that stores individual documents and the standard request handler with the metrics you might want to monitor highlighted.
MBeans can be managed remotely by a wide variety of management consoles such as the JConsole GUI that comes with Java and the webbased JMX Console that comes with the JBoss application server.
As of Version 1.4, Solr exposes information about its components through MBeans.
However, actual management operations, such as re-indexing information, are not exposed through JMX.
You can leverage JMX to monitor the status of Solr, such as finding out how many documents have been indexed, and in large enterprise environments the JMX standard simplifies integrating monitoring tasks into existing monitoring platforms.
This is an easier way to quickly query for JMX information.
Starting Solr with JMX In solrconfig.xml, the stanza <jmx/> needs to be uncommented to enable JMX support.
In order to actually start up with JMX, you need to provide some extra parameters to support remote connections, including the port to be connected to:
In a production environment, you would want to require usernames and passwords for access.
J2SE ships with JConsole, a GUI client for connecting to JMX servers.
In order to connect to Solr, choose the Remote tab, and enter localhost for the Host or IP and 3000 for the Port.
As we have started without requiring authentication, you do not need to enter a username and password:
For Solr, the key tabs to use in JConsole are Memory and MBeans.
Memory provides a visual charting of the consumption of memory and can help you monitor low memory situations and when to start optimizing your indexes (as discussed in Chapter 9, Integrating Solr)
You can also monitor the various components of Solr by choosing the MBeans tab.
In order to find out how many documents you've indexed, you would look at the SolrIndexSearch Mbean.
While you can pull this type of information out of the admin statistics web page, the JMX standard provides a much simpler method that can be easily integrated into other tools.
In order to save yourself typing in the extra startup parameters, see the previous Jetty Startup Integration section for how to add these JMX startup parameters like -Dcom.
Take a walk on the wild side! Use JRuby to extract JMX information While JConsole is useful as a GUI application, it is hard to integrate into a larger environment.
However, by leveraging the standard nature of JMX, we can easily script access to Solr components to use in our own monitoring systems.
This makes it easy to expose extra information to our users such as "15 documents are available for searching"
There are a number of scripting packages for Java that you might look at, including Jython, Groovy, and BeanShell; however, in this example we are going to use JRuby.
JRuby is an implementation of the Ruby language running on the Java Virtual Machine that blends the library support of Java with the simplicity of the Ruby language in a winning combination.
JRuby is very simple to install on Windows and Unix using your operating system's package manager.
Once you have JRuby installed, you need to install the jmx4r gem that provides the simple interface to JMX.
The Ruby standard is to package functionality in gems, which are similar to traditional Java JAR files.
Assuming you have started Solr with JMX enabled on port 3000, you are now ready to interactively query Solr for status through JMX using the JRuby Interactive Browser (JIRB) tool.
Start JIRB from the command line by running the following command:
You now have an interactive connection to the running Solr through JMX.
You may need to use JConsole to figure out the name of the MBean that you want.
Simply select the Info tab for a specific MBean, and use the MBean name attribute.
Once you have the MBean, you can view all available attributes in a hash data structure by typing the following snippet of code:
Returning to our previous example of finding out how many documents are in the index, you just need to issue the following:
You can now integrate this Ruby script into some sort of regular process that saves the number of documents in your database, so you can display that information to your users.
You also can now start getting information about other parts of the system, like how many search queries have been issued per second, and how long they are averaging, by looking at the search handler MBean:
In order to see all the available Solr Mbean's and their JMX names, just issue:
Ruby is a wonderful language for scripting utility tasks that interact with Solr and other systems.
It's a good library to look at for tips on using JRuby.
Securing Solr from prying eyes Solr, by default, comes completely open.
Anyone can make search requests, anyone can upload documents, anyone can access the administration interface, and anyone can delete data.
However, it isn't difficult to lock down Solr to use in any kind of environment.
We can do this by making use of the standard practices, which you would apply to any kind of web application or server software.
Limiting server access The single biggest thing you can do to secure Solr is to lock down who has access to the server.
Using standard firewall techniques, you can control what IP addresses are allowed to connect to the Solr through the 8983 port.
Unless you have very unusual needs, you won't expose Solr to the Internet directly; instead users will access Solr through some sort of web application, that in turn forwards requests to Solr, collects the results, and displays them to your users.
By limiting the IP addresses that can connect to Solr to just those belonging to your web farm, you've ensured that random Internet users and internal users don't mess with Solr.
If you lock down access via IP addresses, then don't forget that if you have external processes uploading content, you need to make sure those IP addresses are added.
Using IP addresses to control access is crude and basic; it doesn't help if someone is connecting to Solr from one of the valid IP addresses.
Fortunately, Solr is just a WAR file deployed in a Servlet container, so you can use all of the capabilities of Servlet containers to control access.
The realm-name is what ties the security constraints to the users configured in Jetty.
Customizing web.xml in Jetty Sometimes cracking open a WAR file just to customize the web.xml can be a pain.
This is a nice trick if you have just a single webapp in the Jetty container.
We've specified that the user named administrator has the roles of content_updater and admin, and therefore can access any /update and /admin URLs.
However, the user eric can only access the /update URLs:
Adding authentication introduces an extra roadblock for automated scripts that need to interact with Solr to upload information.
However, if you use BASIC authentication, then you can easily pass the username and password as part of the URL request.
The only downside is that the password is being transmitted in cleartext, and you should wrap the entire request in SSL for maximum security:
Normally you wouldn't want to store passwords in plain text on the server in a file such as realm.properties that isn't encrypted.
Securing public searches Although typically you access Solr through an intermediate web application, you may want to expose Solr directly to the Internet, albeit in a limited way.
One scenario for this is exposing a search in an RSS/Atom feed made possible with Solr's XSLT support (see Chapter 4, Searching for more on XSLT)
Another is using JavaScript, AJAX, and JSONP callbacks from the browser to directly connect to Solr and issue searches.
There may be other scenarios where firewall rules and/or passwords might still be used to expose parts of Solr, such as for modifying the index, but some search requests must be exposed to direct Internet access.
In this case, you need to configure the exposed request handlers with invariants and/or appends clauses as applicable.
For a limited example of this, see the A SearchHandler per search interface? section earlier in this chapter.
If there are certain records that need to be excluded from public access, then you'll need to specify an appropriate fq (filter query)
If there are certain fields on documents that need to be kept private, then this can be problematic to completely secure, especially if you are working with sensitive data.
It's simple enough to specify fl (field list) through invariants, but there are a good number of other parameters that might expose the data (for example, highlighting, maybe faceting) in ways you didn't realize.
Therefore, if you are working with sensitive data then exposing Solr in this way is not recommended.
Controlling JMX access If you have started Solr with JMX enabled, then you should also have a JMX username and password configured.
While today the JMX interface only exposes summary information about the Solr components and memory consumption, in future versions actual management options like triggering optimizing indexes will most likely be exposed through JMX.
So, putting JMX access under lock and key is a good idea.
Securing index data One of the weaknesses of Solr due to the lack of a built-in security model is that there aren't well defined approaches for controlling which users can manipulate the indexes by adding, updating, and deleting documents, and who can search which documents.
Nevertheless, there are some approaches for controlling access to documents being searched.
Controlling document access You can start off with some of the ideas talked about in the A SearchHandler per search interface? section to control search access to your index.
However, if you need to control access to documents within your index and must control it based on the user accessing the content, then one approach is to leverage the faceted search capabilities of Solr.
You may want to look back at Chapter 5, Search Relevancy to refresh your memory on faceting.
For example, you may have a variety of documents that have differing visibility depending on if someone is a member of the public or an internal publicist.
The public can only see a subset of the data, but a publicist can see more information, including information that isn't ready for public viewing.
When indexing documents, you should store in a separate multiValued field the roles that a user must belong to in order to gain access to the document:
A document that was for everyone would be indexed with the role values Public and Publicist.
Another document that was for internal use would just have the Publicist role.
Then, at query time, you could append extra request parameters to limit what is returned depending on the roles that someone belonged to by treating the roles as a facet:
In the preceding example, we are querying for music that is accessible by anyone with the role public.
Obviously, this requires significant logic to be implemented on the client side interfacing with Solr, and is not as robust a solution as we may wish.
Other things to look at Remote streaming is the ability to give Solr the URL to a remote resource or local file and have Solr download the contents as a stream of data.
This can be very useful when indexing large documents as it reduces the amount of data that your updating process needs to move around.
However, it means that if you have the /debug/dump request handler enabled, then the contents of any file can be exposed.
If you have this turned on, then make sure that you are monitoring the log files, and also that access to Solr is tightly controlled.
The example application has this function turned on by default.
In addition, in a production environment, you want to comment out the /debug/ dump request handler, unless you are actively debugging an issue.
Just as you need to be wary of a SQL injection attack for a relational database, there is a similar concern for Solr.
Solr should not be exposed to untrusted clients if you are concerned about the risk of a denial of service attack.
This is also a concern if you are lax in how your application acts as a broker to Solr.
It's fairly easy to bring down Solr by, say asking it to sort by every field in the schema, which would result in sudden exorbitant memory usage.
There are other similar attacks if an attacker can submit an arbitrary function query as part of their query.
Summary We briefly covered a wide variety of the issues that surround taking a Solr configuration that works in a development environment and getting it ready for the rigors of a production environment.
Solr's modular nature and stripped down focus on search allows it to be compatible with a broad variety of deployment platforms.
Solr offers a wealth of monitoring options, from log files, to HTTP request logs, to JMX options.
Nonetheless, for a really robust solution, you must define what the key performance metrics are that concern you, and then implement automated solutions for tracking them.
Now that we have set up our Solr server, we need to take advantage of it to build better applications.
In the next chapter, we'll look at how to easily integrate Solr search through various client libraries.
Integrating Solr As the saying goes, if a tree falls in the woods and no one hears it, did it make a sound? Similarly, if you have a wonderful search engine, but your users can't access it, do you really have a wonderful search engine? Fortunately, Solr is very easy to integrate into a wide variety of client environments via its modern easy-to-use RESTlike interface and multiple data formats.
There are so many possible topics we could have covered in this chapter and only so much space available.
For information on accessing Solr from .NET, Python, and many other client environments, see: http://wiki.apache.org/solr/ IntegratingSolr.
That page also contains information on integrating Solr with other frameworks and applications, such as Django.
There is a similar page that we started on the wiki called the "Solr Ecosystem": http://wiki.apache.org/solr/ SolrEcosystem.
It is larger in scope than the "Integrating Solr" page, referencing document processing pipelines, crawlers, and more.
In a hurry? This chapter covers a wide variety of integrations with Solr.
If you are in a hurry, jump to the next section Inventory of examples to find source code that you can immediately start using.
Then read the sections that apply to the environment you are working in.
We will be using our MusicBrainz dataset to power these examples.
You can download the full sample code for these integrations from our website http://www.
This includes a prebuilt Solr and scripts to load mbtracks with seven million records and mbartists with 400,000 records.
When you have downloaded the zipped file, you should follow the setup instructions in the README.txt file.
Working with included examples We have included a wide variety of sample integrations that you can run as you work through this chapter.
The examples stored in ./examples/9/ of the downloadable ZIP  file are as self-contained as we could make them, are detailed in this chapter, and you shouldn't run into any problems making them work.
Check the support section of the book website for any errata.
Inventory of examples This is a quick summary of the various examples of using Solr, available unless otherwise noted in ./examples/9/
Solritas, the integrated search UI The contrib module velocity, nicknamed Solritas, is a simple template engine that lets you build user interfaces directly in Solr using Apache Velocity, a very simple macro language to generate the HTML.
It's similar to JSPs, ASPs, PHPs, and so on, but simpler with a syntax consisting of just a handful of commands.
It is very simple to pick up as you can see in the following snippet of code for rendering two lines of HTML displaying the ID and name of an artist pulled from the first Solr document in a list of results:
Java methods like getFieldValue() are easily called in Velocity, allowing you to access the full power of Java within a scripting environment that is evaluated at runtime.
Velocity also supports building your own functions, like the #field() function for displaying a field from a document.
You can try out an interface optimized for searching for MusicBrainz artists by browsing to http://localhost:8983/solr/mbartists/browse/
This web interface supports faceted browsing, auto-completion of queries, boosting of artists based on recency of release, More Like This based on artist name, and even "Did You Mean" spell checking!
When the browser invokes the URL, Solr hands the request off to a request handler with the name /browse which is a search request handler that works like any other.
The point where the request takes a different turn is in rendering the response, which in Solr is configured with the wt parameter.
Short for "writer type", the choices are better known as response writers.
Instead of letting it default to xml, it's set to velocity.
The Velocity response writer uses the v.layout and v.template and parameters to determine which template file to use for the overall page layout as well as what template for the specific page to render.
The templates are located in the conf/velocity/ directory relative to the Solr core, and they end in .vm.
Note that the use of parameters to choose the template allows you to override it in the URL if desired.
Pros and Cons of Solritas As great as it is to impress your boss by quickly building a remarkably full featured search interface using Solritas, there are some cons to keep in mind:
However, some aspects of what I really love about Solritas are:
The example application that ships with Solr also has some good examples of exposing Solr features such as spatial search using Velocity.
SolrJ: Simple Java interface SolrJ is the simple Java client interface to Solr that insulates you from the dirty details of parsing and sending messages back and forth between your application and Solr, and by default communicates using a fast binary format instead of XML.
You work in the familiar world of objects like SolrQuery, QueryResponse, and SolrDocument.
SolrJ is a core part of the Solr project, and typically, though not always, it is updated as soon as new features are added to Solr on the server side.
We'll demonstrate using SolrJ to index web pages downloaded from MusicBrainz.
If you want to run Heritrix yourself, proceed to the next section.
Using Heritrix to download artist pages Heritrix is an extremely full featured and extensible web crawler used by the InternetArchive for archiving the contents of the Internet.
The InternetArchive is a non-profit organization established to preserve websites by taking regular snapshots of them.
You may be more familiar with the site under the name The Wayback Machine.
Going into the full details of using Heritrix is outside the scope of this book.
You will see a web console with a single Engine configured.
Click on it to see the profiles configured for it.
Click on Launch to start the crawler covering the MusicBrainz.org site.
You will see the console interface of the crawler and can monitor the progress of downloading content:
The crawler can take a while to complete a crawl, as it is designed to not overload the sites being crawled and we only have 25 threads configured.
The pages being crawled are stored in the compact text format called an ARC file that contains multiple web resources individually compressed using .gzip.
There are various options for checkpointing the resulting ARC files as they are generated so that you can start using them while it continues to crawl.
Learn more about checkpointing and more advanced features at Heritrix's site at http://crawler.archive.org/
You can view the meta information of the resources in an ARC file such as the timestamp, mime type, and URL by running the arcreader command line client (the first response is listed below):
SolrJ-based client for Indexing HTML Solr does provide some basic support for working with HTML documents that can that makes indexing simpler.
There are two new field types defined: html-text and html-shingle.
However, htmlshingle is designed specifically for multiword phrase searches by using a technique called shingling that results in faster phrase queries at the expense of more disk use and indexing time.
The html-text field is indexed in a more straightforward manner.
We delve more into shingling in Chapter 10, Scaling Solr.
With the url being the unique key and the docText being the default field for searches, Host and path fields give us something with which to facet our results.
Maven is an Apache project at http://maven.apache.org/ that brings a specific approach to structuring Java projects and introduced the concept of public repositories for storing JAR dependencies.
Solr uses Ant as the official supported build, but it includes a Maven build option as well.
For each Solr release, the build artifacts get published to Maven's central repository.
In order to index the web pages stored in the ARC format, execute the JAR file, passing in parameters that specify the directory in which the ARC files are located, whether you are using a remote or local Solr connection, and the specific connection information.
In order to connect to your already running Solr, run:
You should see a long list of URLs being indexed, along with how many milliseconds it took to process all of the documents:
In order to index the ARC files into an embedded Solr, run:
You will see similar output as before, but interleaved with the logging messages generated by the embedded Solr instance as well:
SolrJ client API SolrJ has a very straightforward object model for representing interaction with Solr.
You can play with the basic methods for interacting with Solr by running the BrainzSolrClient class in your IDE.
BrainzSolrClient merely provides some settings to pass into an instance of Indexer, the main class that parses ARC records and indexes them into Solr.
Starting a connection to a remote Solr is very simple, with the only parameter being the URL to the Solr instance.
Note the inclusion of the crawler core in the URL:
Solr supports the ability to specify requests and responses in a Java binary format called javabin that is much smaller and faster than XML.
By default, the SolrJ client performs updates using the javabin format.
You need to make sure that both your SolrJ client and Solr server are using the same version when using the javabin format.
Embedding Solr One of the interesting aspects of SolrJ is that because Solr and SolrJ are both written in Java, you can instantiate Solr and interact with it directly instead of starting up Solr as a separate process.
While this speeds up indexing by removing the cost of transporting data over the wire, it does tie you to running your client on the same local box as Solr so that it can access the Solr configuration files and Lucene indexes directly.
It's typically simpler and almost as fast to use Solr's remote streaming feature (the stream.file parameter) to maximize indexing performance with CSV uploads and rich document extraction.
The following is an example of using an EmbeddedSolrServer class and SolrJ to interface with Solr.
Starting up an embedded Solr is a bit more complex, as you are starting Solr with a specific core instead of running it in a separate servlet container:
The SolrConfig and CoreDescriptor classes wrap the information about solrconfig.xml and your specific named core.
Both of these are used to define the SolrCore, which is then registered in a CoreContainer.
You can customize the query, for instance, by adding faceting to find out the most popular hosts and paths indexed by the crawler using the methods provided by SolrJ client:
The result in XML makes it easy to display results faceted by host and path:
As part of the indexing process, we want to clear out the existing index.
So we use the deleteByQuery, and specify the entire index.
Obviously, this can be very dangerous, and if you have a really large Solr index it will take a while to actually commit that change to the file system.
Note, in real life you would want to use the multiple core feature of Solr to build a new index in the background and just swap it with the live one when indexing was completed!
Any type of query that you would want to do with Solr is available through the SolrJ client.
SolrJ also makes deleting documents simple by providing two easy methods: deleteByQuery() and deleteById()
Removing the Contact Us page is as simple as running:
Indexing In the following example, you can see the heart of the loop for parsing through the list of ARC files and extracting and indexing the information.
As previously mentioned, we only want HTML pages to be indexed, so we check for a MIME type of text/html.
Every 100 documents that are added to Solr triggers a commit to be issued to Solr.
At the end of the loop, a single optimize request is issued:
This requires more memory on the client side and it's hard to know how many documents to batch.
It opens a configurable number of HTTP connections on-demand with a document queue buffer in front of each.
That means it returns immediately if the buffers have capacity, and an indexing error would not be thrown since it is triggered in another thread.
You can see the performance gain by running the following command:
If indexing speed is of critical importance to you, consider updating to Solr 4, the current trunk branch, which hasn't been released yet.
There are some incredible indexing performance improvements already completed there that you can achieve if you have many CPUs, plenty of RAM, and a fast disk.
Indexing POJOs POJOs (Plain Old Java Objects) typically follow the JavaBean naming pattern for properties that each have a getter and setter method.
Moreover, in many use cases, you want to index information that is exposed as Java objects, such as a product versus document oriented data such as the ARC records in the previous example.
Often these objects are backed by a relational database of some type, and you manage them through object relational mapping tools such as Hibernate, JPA, or JDO.
Working with objects can provide much richer types of manipulations than working with documents and allows you to leverage the power of strong typing to validate your code.
Annotations provide a richer means of supplying extra information to tools and libraries beyond what is in the Java code itself.
However, unlike JavaDoc tags, annotations can be read from source files, class files, and reflectively at runtime.
Solr leverages annotations to markup a POJO with information that SolrJ needs to know how to properly index it.
In the following example, RecordItem has the properties id and html mapped to the differently named Solr fields url and docText while host and path map to the identically named Solr fields host and path:
Indexing the RecordItem POJOs is very similar to using the SolrDocument directly:
Performing a query that returns results as POJOs is very similar to returning normal results.
You build your SolrQuery object the exact same way as you normally would, and perform a search returning a QueryResponse object.
However, instead of calling getResults() and parsing a SolrDocumentList object, you would ask for the results as POJOs:
You can then go and process the search results, for example rendering them in HTML with a JSP.
Originally, the conversion of Java SolrDocument objects into XML documents and sending them over the wire to the Solr server was considered fairly slow, and therefore embedded Solr offered big performance advantages.
However, as of Solr 1.4, the binary javabin format is used to transfer messages, which is more compact and requires less processing than XML.
The common thinking is that storing a document in Solr is typically a much smaller portion of the time spent on indexing compared to the actual parsing of the original source document to extract its fields.
Additionally, by putting both your data importing process and your Solr process on the same computer, you are limiting yourself to only the CPUs available on that computer.
If your importing process requires significant processing, then by using the HTTP interface you can have multiple processes spread out on multiple computers munging your source data.
There are several use cases where using embedded Solr is really attractive:
In-process indexing If you expect to index large amounts of content from a single file system, which is mounted on the same server as Solr, and indexed in a fairly straightforward manner as quickly as possible, then embedded Solr can be very useful.
This is especially true if you don't want to go through the hassle of firing up a separate process or have concerns about having a servlet container, such as Jetty, running.
Consider writing a custom DIH DataSource instead Instead of using SolrJ for fast importing, consider using Solr's DataImportHandler (DIH) framework.
Using the DIH gives you supporting infrastructure like starting and stopping imports, a debugging interface, chained transformations, and the ability to integrate with data available from other DIH data-sources.
A good example of an open source project that took the approach of using embedded Solr is Solrmarc.
Solrmarc (hosted at http://code.google.com/p/solrmarc/) is a project to parse MARC records, a standardized machine format for storing bibliographic information.
Solrmarc uses an embedded Solr just to index the content.
After it is optimized, the index is moved to a Solr server to perform search queries.
Standalone desktop applications In my mind, the most compelling reason for using the embedded Solr approach is when you have a rich client application developed using technologies such as Swing or JavaFX which is running in a much more constrained client environment.
Adding search functionality using the Lucene libraries directly is a more complicated lowerlevel API and it doesn't have any of the value-add that Solr offers, like faceting.
By using embedded Solr you can leverage the much higher level API's of Solr for search, and you don't need to worry about the environment your client application exists in blocking access to ports or exposing the contents of a search index through HTTP.
It also means that you don't need to manage spawning another Java process to run a Servlet container, leading to fewer dependencies.
Additionally, you still get to leverage skills in working with the typically server based Solr on a client application—a win-win situation for most Java developers!
Upgrading from legacy Lucene A common situation is when you have an existing Java-based web application that was architected prior to Solr becoming the well-known and stable product that it is today.
Many web applications leverage Lucene as the search engine with a custom layer to make it work with a specific Java web framework such as Struts.
As these applications grow older, and Solr has progressed, revamping them to keep up with the features that Solr offers has become increasingly difficult.
However, these applications have many ties into their homemade Lucene based search engines.
Performing the incremental step of migrating from directly interfacing with Lucene to directly interfacing with Solr through embedded Solr can reduce risk.
Risk is minimized by limiting the impact of the change to the rest of the application by isolating change to the specific set of Java classes that previously interfaced directly with Lucene.
Moreover, this does not require a separate Solr server process to be deployed.
A future incremental step would be to leverage the scalability aspects of Solr by moving away from the embedded Solr to interfacing with a separate Solr server.
Using JavaScript with Solr During the Web 1.0 epoch, JavaScript was primarily used to provide basic clientside interactivity such as a roll-over effect for buttons in the browser for what were essentially static pages generated by the server.
However, in today's Web 2.0 environment, AJAX has led to JavaScript being used to build much richer web applications that blur the line between client-side and server-side functionality.
Solr's support for the JavaScript Object Notation format (JSON) for transferring search results between the server and the web browser client makes it simple to consume Solr information by modern Web 2.0 applications.
While JSON is very simple to use in concept, it does come with its own set of quirks related to security and browser compatibility.
Anytime you are performing an eval() you are risking crashing the browser.
To learn more about the JSON format, the various client libraries that are available, and how it is and is not like XML, visit the homepage at http://www.json.org.
As you may recall from Chapter 4's discussion of query parameters, you change the format of the response from Solr from the default XML to JSON by specifying the JSON writer type as a parameter in the URL via wt=json.
There is another parameter affecting the JSON, Ruby, and Python formats for field value faceting: json.nl.
Yes, it's not just for JSON, and it technically affects output of Solr's so-called NamedList internal data but only in rare circumstances.
The default choice, flat, is inconvenient to work with despite its succinctness, so other options are available.
Note that the map choice does not retain the ordering once it is materialized in memory.
Here is a table showing the affects of each choice on faceting on the MusicBrainz artist type:
You may find that you run into difficulties while parsing JSON in various client libraries, as some are stricter about the format than others.
Solr does output very clean JSON, such as quoting all keys and using double quotes and offers some formatting options for customizing handling of lists of data.
This can be invaluable for finding issues like an errant trailing comma.
Wait, what about security? If requests to Solr come from a web browser, then you must consider security.
You may recall from Chapter 8, Deployment that one of the best ways to secure Solr is to limit what IP addresses can access your Solr install through firewall rules.
Obviously, if users on the Internet are accessing Solr through JavaScript, then you can't do this.
However, if you look back at Chapter 8, Deployment, there is information on how to expose a read-only request handler that can be safely exposed to the Internet without exposing the complete admin interface.
Also make sure that any filters that MUST be applied to your data, such as a filter query enforcing only active products are shown is applied as an appends parameter in your request handler.
Additionally, you might proxy Solr requests to ensure the parameters meet a whitelist, to include their values.
This can be where you apply various business rules such as preventing a malicious user from passing parameters such as rows=1000000!
Building a Solr powered artists autocomplete widget with jQuery and JSONP It's well established now in the search industry that some form of query autocompletion remarkably improves the effectiveness of a search application.
There are several fundamentally different types of autocompletion—be sure to read about them in Chapter 7, Search Components.
Here is a screenshot of Google showing completions based on search queries it has seen before:
Building an autocomplete text box powered by Solr is very simple by leveraging the JSON output format and the very popular jQuery JavaScript library's Autocomplete widget.
It has gone through explosive usage growth in 2008 and is one of the most popular AJAX frameworks.
You can read the doc and see a live demo of various auto-completions online at http://jqueryui.com/demos/autocomplete/
We start with a very simple HTML form that has a single text input box with the id="artist" attribute:
We then add a function that runs, after the page has loaded, to turn our basic input field into a text field with suggestions:
The source: function( request, response ) function supplies the list of suggestions to display via a $.ajax callback.
The dataType: "jsonp" option informs jQuery that we want to retrieve our data using JSONP.
This allows you to work around web browser cross-domain scripting issues of running Solr on a different URL and/or port from the originating web page.
These items are tacked onto the URL, which is passed to Solr.
The response() function is called to convert the JSON result data from Solr into the format Autocomplete requires.
It consists of a map() function that takes the returned JSON data structure for the documents returned and calls an anonymous function for each document.
The anonymous function parses out the value to use as the label and value, in our case just the artist name.
You now have a nice Solr powered text autocomplete field so that when you enter Rolling, you get a list of all of the artists including the Stones:
One thing that we haven't covered is the pretty common use case for an Autocomplete widget that populates a text field with data that links back to a specific row in a table in a database.
For example, in order to store a list of artists, I would want the Autocomplete widget to simplify the process of looking up the artists, but would need to store the list of selected artists in a database.
You can still leverage Solr's superior search ability, but tie the resulting list of artists to the original database record through a primary key ID, which is indexed as part of the Solr document.
If you try to lookup the primary key of an artist using the name of the artist, then you may run into problems such as having multiple artists with the same name or unusual characters that don't translate cleanly from Solr to the web interface to your database record.
Instead, a hidden field stores the primary key of the artist and is used in your serverside processing in place of the text typed into the search box:
We use the change() function to ensure freeform text that doesn't result in a match is ignored by clearing out the artist_id form field and returning false from the function:
Change the field artist_id from input type="hidden" to type="text" so that you can see the ID changing more easily as you select different artists.
Make sure you click away from the suggestion box to see the change occur!
Where should I get my results to display as suggestions? There are many approaches for supplying the list of suggestions for autocomplete, and even the nomenclature of autosuggest, autocomplete, or suggest as you type have loosely defined meanings.
If the user is looking for a specific document by name or other identifier, then simple searchresults based autocompletion as we've done here is very effective.
This is sometimes called "Search as Navigation" because you can skip the search results page.
For broader searching, query log or search-term based completion is typically used.
For more detail, flip back to the information in Chapter 7, Search Components on query completion.
It is an off-shoot of an older project call SolrJS which is now defunct.
When it comes to integrating Solr into your web application, if you are comfortable with JavaScript, then this can be a very effective way to add a really nice AJAX view of your search results without changing the underlying web application.
If you're working with an older web framework that is brittle and hard to change, such as IBM's Lotus Notes and Domino framework, then this keeps the integration from touching the actual business objects, and keeps the modifications in the client layer via HTML and JavaScript.
The AJAX Solr project homepage is at http://evolvingweb.github.com/ajaxsolr/ and provides a great demo of searching Reuters business news wire results:
It comes with widgets like autocompletion of field values, a tag cloud, a facet view, a country code, and calendar based date ranges, as well as displaying the results with paging.
They all inherit from an AbstractWidget and follow pretty much the same pattern.
They are configured in the file reuters/ js/reuters.js by passing in a set of options.
Here is an example of configuring the autocomplete widget to populate the search box with autocomplete suggestions drawn from the topics, organizations, and exchanges fields:
A central AjaxSolr.Manager object coordinates the event handling between the various widgets, makes the queries to Solr, and messages the widgets.
Shown above is the call to add the widget to the AjaxSolr.Manager object.
Working with AJAX Solr and creating new widgets for your specific display purposes comes easily to anyone who comes from an object-oriented background.
The various widgets that come with AJAX Solr serve more as a foundation and source of ideas rather than as a finished set of widgets.
You'll find yourself customizing them extensively to meet your specific display needs.
It is based on the Reuters example with a custom widget for term autocompletion using the facet.prefix technique.
We did not configure Solr to load these facets via Solr's firstSearcher event in solrconfig.xml because this is the only demo that uses it and it takes up to 30 seconds to load given the large index.
Using XSLT to expose Solr via OpenSearch A relatively unknown, but powerful way to integrate with Solr is via its support for XSLT, eXtensible Stylesheet Language Transformations.
There are various implementations of this specification and Java includes one.
Solr provides a query response writer that executes a provided XSLT stylesheet, to transform Solr's XML search results into some other format.
Here is an example of transforming search results into an RSS feed:
The wt parameter triggers the use of XSLT, and the tr parameter supplies the name of the stylesheet to be used.
There are some caveats to keep in mind for XSLT support.
So if you use more than one XSLT stylesheet then you are likely to find Solr constantly recompiling it.
Additionally, because Solr has to have the entire XML document in memory first to render the XSL stylesheet, you may run into memory issues if you are returning large numbers of results.
Need a debugger for Solr queries? Want to understand how Solr determined the score for the documents you returned? You can use the example.xsl to quickly transform your results to HTML and expose the query debugging information in an easy to read format.
OpenSearch based Browse plugin In this section we will show how to use XSLT to support OpenSearch.
OpenSearch is a collection of simple formats/standards for search engine interoperability.
About half of the standard governs an XML document that describes the web interface of a search engine to invoke searches on it, including various metadata.
The other half defines a set of XML elements for formatting search results, typically Atom/RSS.
This standard enables you to build a single interface that works with multiple different search engines.
OpenSearch was originally developed by A9, a subsidiary of Amazon, and has seen some adoption in the market, especially by the browsers to power their toolbar search boxes.
Installing the Search MBArtists plugin This example builds on the Velocity based UI for the mbartists core described earlier, and works best with Firefox.
Open the browse interface for the mbartists core and you will be able to add a custom Search MBArtists plugin to the search bar:
Browsers that understand this link will allow the user to add the described search engine.
Open the results in Chrome and you can see the results in the Atom standard XML syntax:
Type into Firefox's search bar, and not only will you be able to perform searches that open up in the browse interface, but you can also get autocompletion of your queries!
While OpenSearch is somewhat interesting for exposing your website search functionality through browsers, it's even more exciting if you are setting up federated search or trying to plug Solr into another system and need a common lingua franca that won't change and isn't Solr specific.
Accessing Solr from PHP applications There are a number of ways to access Solr from PHP, and none of them seem to have taken hold of the market as the single best approach.
So keep an eye on the wiki page at http://wiki.apache.org/solr/SolPHP for new developments.
Adding the URL parameter wt=php produces simple PHP output in a typical array data structure:
The same response using the Serialized PHP output specified by wt=phps URL parameter is a much less human-readable format that is more compact to transfer over the wire:
Think twice before using the php writer types Un-serializing potentially untrusted data can increase security vulnerability.
Additionally, the future of these writer types is in some doubt as PHP client abstraction projects such solr-php-client and Solarium both use JSON in preference to the php writer types.
Interestingly enough, this project leverages the JSON writer type to communicate with Solr instead of the PHP writer type, showing the prevalence of JSON for facilitating inter-application communication in a language agnostic manner.
The developers chose JSON over XML because they found that JSON parsed much quicker than XML in most PHP environments.
Moreover, using the native PHP format requires using the eval() function, which has a performance penalty and opens the door for code injection attacks.
Installing the demo in your specific local environment is left as an exercise for the reader.
Notice that we are using the artist ID value -1
Solr doesn't care what the ID field contains, just that it is present.
Using -1 ensures that we can find Susan Boyle by ID later!
Sending the documents to Solr and triggering the commit and optimize operations is as simple as:
Here we are looking for Susan Boyle based on her ID of -1, highlighting the result using a blue font:
Successfully running the demo creates Susan Boyle and issues a number of queries, producing a page similar to the one below.
Notice that if you know the ID of the artist, it's almost like using Solr as a relational database to select a single specific row of data.
Solarium may be what you want! Solarium (http://www.solarium-project.org/) attempts to improve on other PHP client libraries by not just abstracting away the HTTP communication layer but also more fully modeling the concepts expressed by Solr.
It has objects that allow you to easily build complex filter queries and faceting logic.
Drupal options Drupal is a very successful open source Content Management System (CMS) that has been used for building everything from the WhiteHouse.gov site to political campaigns and university websites.
Drupal's built-in search has always been considered adequate, but not great, so the option of using Solr to power search has been very popular.
The module has had significant adoption and is the basis of some other Drupal search related modules.
In order to see the Apache Solr module in action, just visit the Drupal.org and perform a search to see the faceted results.
Hosted Solr by Acquia Acquia is a company providing commercially supported Drupal distributions, and also offers hosted Solr search, for Drupal sites that want better search than the builtin MySQL based search.
Acquia's adoption of Solr as a better solution for Drupal than Drupal's own search shows the rapid maturing of the Solr community and platform.
Acquia maintains "in the cloud" (Amazon EC2), a large infrastructure of Solr servers saving individual Drupal administrators from the overhead of maintaining their own Solr server.
A module provided by Acquia is installed into your Drupal and monitors for content changes.
Every five or ten minutes, the module sends content that either hasn't been indexed, or needs to be re-indexed, up to the indexing servers in the Acquia network.
When a user performs a search on the site, the query is sent up to the Acquia network, where the search is performed, and then Drupal is just responsible for displaying the results.
Acquia's hosted search option supports all of the usual Solr goodies including faceting.
Drupal has always been very database intensive, with only moderately complex pages performing 300 individual SQL queries to render! Moving the load of performing searches off one's Drupal server into the cloud drastically reduces the load of indexing and performing searches on Drupal.
Acquia has developed some slick integration beyond the standard Solr features based on their tight integration into the Drupal framework, which include:
Acquia's hosted search product is a great example of Platform as a Service (PaaS), and hosted Solr search is a very common integration approach for many organizations that don't wish to manage their own Java infrastructure or need to customize the behavior of Solr drastically.
Ruby on Rails integrations There has been a lot of churn in the Ruby on Rails world for adding Solr support, with a number of competing libraries attempting to support Solr in the most Rails-native way.
Rails brought to the forefront the idea of Convention over Configuration, the principle that sane defaults and simple rules should suffice in most situations versus complex configuration expressed in long XML files.
The various libraries for integrating Solr in Ruby on Rails applications establish conventions in how they interact with Solr.
However, often there are a lot of conventions to learn, such as suffixing String object field names with _s to match up with the dynamic field definition for String in Solr's schema.xml.
The Ruby query response writer The Ruby hash structure looks very similar to the JSON data structure with some tweaks to fit Ruby, such as translating nulls to nils, using single quotes for escaping content, and the Ruby => operator to separate key-value pairs in maps.
Adding a wt=ruby parameter to a standard search request returns results that can be eval() into a Ruby hash structure like this:
Note: Evaluating these results has the same security implications that using the JSON and PHP writers have!
This allows you to do queries that are backed by Solr searches, but still work with your normal ActiveRecord objects.
Let's go ahead and build a small Rails application that we'll call MyFaves that both allows you to store your favorite MusicBrainz artists in a relational model and allows you to search for them using Solr.
This is great for quickly doing development since you don't need to download and set up your own Solr.
Typically, you are starting with a relational database already stuffed with content that you want to make searchable.
However, in our case we already have a fully populated index of artist information, so we are actually going to take the basic artist information out of the mbartists index of Solr and populate our local myfaves database used by the Rails application.
We'll then fire up the version of Solr shipped with Sunspot, and see how sunspot_rails manages the lifecycle of ActiveRecord objects to keep Solr's indexed content in sync with the content stored in the relational database.
Setting up MyFaves project This example assumes you have Rails 3.x already installed.
We'll start with the standard plumbing to get a Rails application set up with our basic data model:
This generates a basic application backed by a SQLite database.
Now we need to specify that our application depends on Sunspot.
We'll also be working with roughly 399,000 artists, so obviously we'll need some page pagination to manage that list, otherwise pulling up the artists /index listing page will timeout.
Add the will_paginate gem declaration to your Gemfile and re-run bundle install:
You should see an empty listing page for all of the artists.
Now that we know the basics are working, let's go ahead and actually leverage Solr.
Populating MyFaves relational database from Solr Step one will be to import data into our relational database from the mbartists Solr index.
The searchable block maps the attributes of the Artist ActiveRecord object to the artist fields in Solr's schema.xml.
Since Sunspot is designed to store any kind of data in Solr that is stored in your database, it needs a way of distinguishing among various types of data model objects.
For example, if we wanted to store information about our User model object in Solr in addition to the Artist object then we need to provide a field in the schema to distinguish the Solr document for the artist with the primary key of 5 from the Solr document for the user with the primary key of Artist, which maps directly to our ActiveRecord class name of Artist.
Most scripts typically work with some sort of batch size of records that are pulled from one system and then inserted into Solr.
The larger the batch size, the more efficient the pulling and processing of data typically is at the cost of more memory being consumed, and the slower the commit and optimize operations are.
When you run the populate.rb script, play with the batch size parameter to get a sense of resource consumption in your environment.
The parameters for populate.rb are available at the top of the script:
The connection to Solr is handled by the RSolr library.
A request to Solr is simply a hash of parameters that is passed as part of the GET request.
We use the *:*query to find all of the artists in the index and then iterate through the results using the start parameter:
In our MusicBrainz Solr schema, the ID field functions as the primary key and looks like Artist:11650 for The Smashing Pumpkins.
We wrap the insert statement a.save! in a begin/rescue/end structure so that if we've already inserted an artist with a primary key, then the script continues.
This allows us to run the populate script multiple times without erroring out:
We've successfully migrated the data we need for our MyFaves application out of Solr and we're ready to use the version of Solr that's bundled with Sunspot.
On the initial startup rake will create a new top level ./solr directory and populate the conf directory with default configuration files for Solr (including schema.xml, stopwords.txt, and so on) pulled from the Sunspot gem.
Build Solr indexes from a relational database Now we are ready to trigger a full index of the data from the relational database into Solr.
Browse to http://localhost:8982/solr/admin/schema.jsp to see the list of dynamic fields generated by following Convention over Configuration pattern of Rails applied to Solr.
ActiveRecord model objects is based on the database column type.
Therefore, when sunspot_rails indexes a model object, it sends a document to Solr with the various suffixes to leverage the dynamic column creation.
You can see the list of dynamic fields generated through the schema browser at http://localhost:8982/solr/admin/schema.jsp.
Sunspot adds some new methods to our ActiveRecord model objects such as search() that lets us load ActiveRecord model objects by sending a query to Solr.
Here we find the group Smash Mouth by searching for matches to the word smashing:
The raw results from Solr are stored in the variable search.hits.
The variable search.results returns the ActiveRecord objects from the database.
Let's also verify that Sunspot is managing the full lifecycle of our objects.
Assuming Susan Boyle isn't yet entered as an artist, let's go ahead and create her:
Check the log output from your Solr running on port 8982, and you should also have seen an update query triggered by the insert of the new Susan Boyle record:
Then there should be another corresponding update issued to Solr to remove the document:
You can verify this by doing a search for Susan Boyle directly, which should return no rows at http://localhost:8982/solr/select/?q=Susan+Boyle.
Complete MyFaves website Now, let's go ahead and put in the rest of the logic for using our Solr-ized model objects to simplify finding our favorite artists.
We'll store the list of favorite artists in the browser's session space for convenience.
The only other edits you should need to make are:
You should now be able to run ./script/rails start and browse to http:// localhost:3000/
You will be prompted to enter search by entering the artist's name.
If you don't receive any results, then make sure you have started Solr using rake sunspot:solr:start.
Also, if you have only loaded a subset of the full 399,000 artists, then your choices may be limited.
You can load all of the artists through the populate.rb script and then run rake sunspot:reindex, although it will take a long time to complete.
Something good to do just before you head out for lunch or home for the evening!
The URL we are hitting is /artists.json, with the .json suffix telling Rails that we want JSON data back instead of normal HTML.
If we ended the URL with .xml, then we would have received XML formatted data about the artists.
We provide a slightly different parameter to Rails to specify the JSONP callback to use.
Unlike the previous example, where we used json.wrf, which is Solr's parameter name for the callback method to call, we use the more standard parameter name callback.
We changed the ArtistController index method to handle the autocomplete widgets data needs through JSONP.
If there is a q parameter, then we know the request was from the autocomplete widget, and we ask Solr for the @artists to respond with.
Later on, we render @artists into JSON objects, returning only the name and id attributes to keep the payload small.
We also specify that the JSONP callback method is what was passed when using the callback parameter:
At the end of all of this, you should have a nice autocomplete interface for quickly picking artists.
When you are selecting Sunspot as your integration method, you are implicitly agreeing to the various conventions established for indexing data into Solr.
If you are used to working with Solr directly you may find understanding the Sunspot DSL for querying a bit of an obstacle.
But if your background is in Rails, or you are building very complex queries, then learning the DSL will pay off in productivity and ability to maintain complex expressive queries.
Which Rails/Ruby library should I use? The two most common high level libraries for interacting with Solr are acts_as_solr and Sunspot.
However, in the last couple of years, Sunspot has become the more popular choice, and comes in a version designed to work explicitly with Rails called sunspot_rails that allows Rails ActiveRecord database objects to be transparently backed by a Solr index for full text search.
For lower-level client interface to Solr from Ruby environments, there are two libraries duking it out to be the client of choice: solr-ruby, a client library developed by the Apache Solr project and rsolr, which is a reimplementation of a Ruby centric client library.
Both of these solutions are solid and act as great low level API libraries.
However, rsolr has gained more attention, has better documentation, and some nice features such as a direct embedded Solr connection through JRuby.
In order to perform a select using solr-ruby, you would issue:
In order to perform a select using rsolr, you would issue:
So you can see that doing a basic search is pretty much the same in either library.
Differences crop up more as you dig into the details on parsing and indexing records.
You can learn more about solr-ruby on the Solr Wiki at http://wiki.
Think about if you really need another layer of abstraction between you and Solr.
Making a call to Solr using wt=ruby and evaluating the results may be the simplest solution.
Nutch for crawling web pages A very common source of data to to be searchable is content in web pages, either from the Internet or inside the firewall.
The long-time popular solution for crawling and searching web pages is Nutch, a former Lucene sub-project.
Nutch is focused on performing Internet scale web crawling similar to Google with components such as a web crawler, a link graphing database, and parsers for HTML and other common formats found on the Internet.
Nutch is designed to scale horizontally over multiple machines during crawling using the bigdata platform Hadoop to manage the work.
Nutch has gone through varying levels of activity and community involvement and recently reached version 1.3
Previously Nutch used its own custom search interface based on Lucene, but it now leverages Solr for search.
This allows Nutch to focus on web crawling, while Solr works as a generic search tool with features such as query spellcheck and faceting that Nutch previously couldn't match.
Nutch natively understands web relevancy concepts such as the value of links towards calculating a page rank score, and how to factor in what an HTML <title/> tag is, when building the scoring model to return results.
Nutch works off of a seed list of URLs that are used as the source of web pages to crawl.
Look at the script seed_urls.rb to see the logic used for extracting the URL seed list.
The topN parameter controls how many documents at each level to crawl, with 5 meaning that only five artist pages in total are crawled.
The biggest change you might make is to set stored="false" on the content field to reduce the index size if you are doing really big crawls.
For more information about the plugins that extend Nutch, and how to configure Nutch for more sophisticated crawling patterns, look at the documentation at http://nutch.apache.org.
Maintaining document security with ManifoldCF A frequent requirement for search engines is to maintain document level security.
While a public search engine may expose all documents to all users, many intranet oriented search engines maintain information that that is accessible to only a subset of users.
Historically, the solution to maintaining document level security has been a roll-your-own with the most common approaches being:
Implement a post processing filter on the document result set that removes documents that don't match a specific user access controls.
This approach is nice because you can just wrap your calls to Solr with your own proprietary security model, and doesn't require any changes to your indexing logic.
Also, the filtering process can often be very expensive to perform.
You may be able to perform deeper integration with Solr by writing your own query parser that taps into Solr's filter queries and avoid the post processing step.
The other approach is to enrich your indexed document with information about who can access which documents.
This access information is exposed by using filter queries to control who can access which documents.
Apache ManifoldCF (CF meaning Connector Framework) provides a framework for extracting content from multiple repositories, enriching it with document level security information, and outputting the resulting document into Solr based on the security model found in Microsoft's Active Directory platform.
Working with ManifoldCF requires understanding the interaction between extracting content from repositories via a Repository Connector, outputting the documents and security tokens via an Output Connector into Solr, listing a specific user's access tokens from an Authority Connector, and finally performing a search that filters the document results based on the list of tokens.
ManifoldCF takes care of ensuring that as content and security classifications for content are updated in the underlying repositories it is synched to Solr, either on a scheduled basis or a constantly monitoring basis.
Connectors ManifoldCF provides connectors that index into Solr content from a number of enterprise content repositories including SharePoint, Documentum, Meridio, LiveLink, and FileNet.
Competing with DataImportHandler and Nutch, ManifoldCF also crawls web pages, RSS feeds, JDBC databases, and remote Windows shares and local file systems, while adding the document level security tokens where applicable.
The most compelling use case for ManifoldCF is leveraging ActiveDirectory to provide access tokens for content indexed in Microsoft SharePoint repositories, followed by just gaining access to content in the other enterprise content repositories.
Putting ManifoldCF to use While the sweet spot for using ManifoldCF is with an Authority like ActiveDirectory, we're going to reuse our MusicBrainz.org data and come up with a simple scenario for playing with ManifoldCF and Solr.
The data will be streamed through Manifold and out to our /manifoldcf Solr core with the list of genres used as the access tokens.
To simulate an Authority service that translates a username to a list of access tokens, we will use our own GenreAuthority.
GenreAuthority will take the first character of the supplied username, and return a list of genres that start with the same character.
So a call to ManifoldCF for the username paul@example.com would return the access tokens pop and punk.
A search for "Chris" would match on "Chris Isaak" since he is tagged with pop, but "Chris Cagle" would be filtered out since he plays only American and country music.
ManifoldCF ships with Jetty as a servlet container, hence the very similar start command to the one Solr uses!
Browse to http://localhost:8345/mcf-crawler-ui/ to access the ManifoldCF user interface which exposes the following main functions:
Go ahead and choose the Status and Job Management screen and trigger the indexing job.
Click Refresh a couple of times, and you will see the artist content being indexed into Solr.
To see the various genres being used as access tokens browse:
Now that you have data in Solr, this is only half the challenge.
At the time of writing, neither ManifoldCF nor Solr have a component that hooked ManifoldCF based permissions directly into Solr.
However, based on code from the upcoming ManifoldCF in Action book (http://code.google.com/p/manifoldcfinaction/), you can easily add a Search Component to your request handler.
You are now ready to perform your first query! Do a search for Chris, specifying your username as paul@example.com and you should see only pop and punk music artists being returned!
Summary As you've seen, Solr offers a plethora of integration options, from its ability to customize its output using the various query response writers, to clients for specific languages, to frameworks that enable powerful front ends for both indexing content as well as providing a jump start in developing the search user interface.
The simplicity of using HTTP GET to request actions to be performed by Solr and responding with simple documents makes it very straightforward to integrate Solr based search into your applications regardless of what your preferred development environment is.
If you are looking to explore more integration options with your favorite language, framework, or other software, then visit: http://wiki.apache.org/solr/ IntegratingSolr and http://wiki.apache.org/solr/SolrEcosystem.
In the next chapter, we are going to look at how to scale Solr to meet growing demand by covering approaches for scaling an individual Solr server as well as scaling out by leveraging multiple Solr servers working cooperatively.
Scaling Solr You've deployed Solr, and the world is beating a path to your door, leading to a sharp increase in the number of queries being issued, and meanwhile you've indexed tenfold the amount of information you originally expected.
You discover that Solr is taking longer to respond to queries and index new content.
When this happens, it's time to start looking at what configuration changes you can make to Solr to support more load.
In a hurry? If you flipped to this chapter because you need to scale Solr, look at the section Solr caching as well as how to use replication to share load over multiple Solr servers described in Moving to multiple Solr servers (Scale horizontally)
Tuning complex systems Tuning any complex system, whether it's a database, a message queuing system, or the deep dark internals of an operating system, is something of a black art.
Researchers and vendors have spent decades figuring out how to measure the performance of systems and coming up with approaches for maximizing the performance of those systems.
For some systems that have been around for decades, such as databases, you can just search online for Tuning Tips for X Database and find explicit rules that suggest what you need to do to gain performance.
However, even with those well-researched systems, it still can be a matter of trial and error.
In order to measure the impact of your changes, you should look at a couple of metrics and optimize for these three parameters:
In order to get a sense of what the Steady State is for your application, you can gather the statistics by using the SolrMeter product to put your Solr under load.
We'll discuss in the next section how to build a load testing script with SolrMeter that accurately mirrors your real world interactions with Solr.
This effort will give you a tool that can be run repeatedly and allows more of an apple-to-apple comparison of the impact of changes to your configuration.
Solr, out of the box, is already very performant, with extensive effort spent by the community to ensure that there are minimal bottlenecks.
But the tuning of Solr hasn't matured to where there are hard and fast rules for optimization that you should follow by rote step to increase scalability.
Most tuning will trade off increases in search performance at the expense of disk index size, indexing speed, and/or memory requirements (and vice versa)
The three system changes to perform in increasing complexity are:
There is some great research being performed on measuring the limits of scaling Solr by a consortium of libraries called the HathiTrust.
You can follow their research (and others working in this space) by following links from http://wiki.apache.org/solr/SolrPerformanceData.
Testing Solr performance with SolrMeter One of the biggest challenges when doing performance testing is to know when you've accomplished your goals.
When performance testing Solr, you typically are tweaking configuration values such as cache sizes and query parameters in response to two ongoing activities: the pace of documents being indexed into Solr, and the pace of queries being issued to Solr.
SolrMeter makes it very easy to control the pace of these two activities through a simple GUI tool.
SolrMeter brings together both basic load testing functionality with some visualization and analytics of your Solr instance.
While you can use the Solr Admin statistics page to pull back these results, you are only seeing a snapshot in time.
In the following screenshot, you can see a visualization of the queryResultCache over time.
The middle four slopes were created because I began the Update Console at second 75 indexing new data.
You can easily see the impact of commits on the caches.
This type of visualization can help you go beyond just using the default caching configurations.
Just extract the entire line after the ? character logged by the GET requests in the Solr log.
That is great for repeating the same set of queries so you are doing A/B testing as you tweak the various settings.
SolrMeter also supports exporting the query time and a histogram of query time in CSV format to make your own graphs.
You can also use SolrMeter to place a "base" load on a Solr, and then use other testing tools that offer more scripting or analytics options to ensure that what works just fine when Solr isn't under load continues to meet expectations when Solr is under load.
Using SolrMeter you can quickly set this scenario up, and then use another tool like JMeter that drives your front-end search user interface to ensure your application meets your expected SLA when Solr is under load.
Or you can easily change settings such as cache configurations or faceting settings and see the impact of these changes on performance.
I like to build my list of queries for load testing by extracting a day's worth of queries from your existing search engine log or HTTP web server log files.
This gives me a realistic set of data so I am tuning to what my users actually search for, not what I think they search for!
Optimizing a single Solr server (Scale up) There are a large number of different options that Solr gives you for enhancing performance of a specific Solr instance, and for most of these options, deciding to modify them depends on the specific performance result you are trying to tune for.
This section is structured from most generally useful to more specific optimizations.
Configuring JVM settings to improve memory usage Solr runs inside a Java Virtual Machine (JVM), an environment that abstracts your Java-based application from the underlying operating system.
There are many different parameters that you can tune the JVM for.
However, most of them are "black magic", and changing them from the defaults can quickly cause problems if you don't know what you're doing.
Additionally, the folks who write JVMs spend a lot of time coming up with sophisticated algorithms that mean the JVM will usually tune itself better than you can.
However, there is a fairly simple configuration change that most Java server applications benefit from (not just Solr), which is to set the initial and maximum heap memory allocated to the JVM to the same value and specify that you are running a server application, so the JVM can tune its optimization strategy for a long running process:
Of course, the question now is how much memory should be allocated to the Java heap.
If you specify the largest practical value, which is the actual memory you have, less some for the operating system and other processes, this is a sub-optimal configuration too.
Operating systems make use of available memory as caches for disk access, and Solr searches benefit substantially from this, while indexing does not.
I recommend measuring how much heap you need by picking some highish value, then run a full battery of queries against Solr so all its caches get filled, then use JConsole to perform a full garbage collection.
At that point you can see how much memory it's using.
With that figure, estimate some breathing room of perhaps 20%
You can modify this process to incorporate concurrent indexing as applicable.
The ultimate figure is of course highly dependent on the size of your Solr caches and other aspects of the Solr configuration; therefore, tuning the heap size should be one of the later steps.
Look back at the discussion about JMX in Chapter, Deployment for more details on using JConsole.
The memory used is outside of the Java heap so you do not need to modify any JVM startup options.
Be sure to test your system thoroughly to validate the performance improvement and make sure RAM requirements are satisfied.
Enabling downstream HTTP caching Solr has great support for using HTTP caching headers to enable downstream HTTP software to cache results.
Web browsers, intermediate proxy servers, and web servers can decide if they need to re-query for updated results by using various rules.
For example, often applications allow a user to take a query and make it an Alert that will e-mail them results if there is a match.
This leads to the same search running over and over, even if the results are almost always the same.
Placing an intermediate caching server, such as Squid, in front of Solr should reduce the load on Solr and potentially reduce Solr's internal "query cache" requirements, thus freeing up more RAM.
In order to specify that you want Solr to do HTTP caching, you need to configure the <httpCaching/> stanza in solrconfig.xml.
We've also specified through must-revalidate that any shared cache, such as a Squid proxy, needs to check back with Solr to see if anything has changed, even if the max-age hasn't expired, which acts as an extra check.
During development you may want to set never304="true" to ensure that you are always looking at the results of fresh queries and aren't misled by looking at cached results, unless you are using eTags and the browser properly honors them.
By running curl with the mbartists core, we can see additional cache related information in the header, as well as the full XML response from Solr (not listed)
So let's look at what we get back if we take advantage of the Last-Modified header information by specifying that we have downloaded the content after the last modified time:
Specifying an If-Modified-Since time just one second after the Last-Modified time means that Solr gives us back a 304 Not Modified code and doesn't have to return all of the XML data over the wire, which is much faster and reduces the load on the server.
When you ran the curl command you didn't receive any of the XML result data back in your console!
Entity tags are a newer method for uniquely identifying responses that are more robust and flexible than using the Last-Modified date.
An ETag is a string that identifies a specific version of a component.
In the case of Solr, they are generated by combining the current version of the index with the etagSeed value.
Every time the index is modified, the current ETag value will change.
If we add the fake artist "The Eric Band" to the mbartists index, and then run our previous query, we'll see that the ETag has changed because the version of the Solr index has changed:
To take advantage of the HTTP protocol level caching supplied by Solr you need to make sure your client respects the caching directives returned by Solr.
Remember, the fastest query response possible from Solr's perspective is the query that it doesn't have to make!
Solr caching Caching is a key part of what makes Solr fast and scalable, and the proper configuration of caches is a common topic on the solr-user mailing list! Solr uses multiple Least Recently Used in-memory caches.
The caches are associated with individual Index Searchers, which represent a snapshot view of the data.
Following a commit, new index searchers are opened and then auto-warmed.
Auto-warming is when the cached queries of the former searcher are rerun to populate the new searcher.
Following auto-warming, predefined searches are run as configured in solrconfig.xml.
Put some representative queries in the newSearcher and firstSearcher listeners, particularly for queries that need sorting on fields.
Once complete, the new searcher will begin servicing new incoming requests.
Each auto-warming query and predefined search increases the commit time so make sure those searches are actually increasing the cache hit ratio and don't over do it!
There are a number of different caches configured in solrconfig.xml:
The documented wisdom on sizing this cache is to be larger than the max results * max concurrent queries being executed by Solr to prevent documents from being re-fetched during a query.
As this cache contains the fields being stored, it can grow large very quickly.
Tuning caches Using the statistics admin page, you can get a sense of how large you need to make your caches.
If the hit ratio for your caches is low, then it may be that they aren't caching enough to be useful.
However, if you find that the caches have a significant number of evictions, then that implies they are filling up too quickly and need to be made larger.
Caches can be increased in size as long as Solr has sufficient RAM to operate in.
If your hit ratio for a cache is very low, then you should evaluate reducing its size, perhaps turning it off altogether by commenting out the cache configuration sections in solrconfig.xml.
This will reduce memory needs that aren't being used effectively and may also help improve performance by removing the overhead of managing the caches.
Indexing performance There are several aspects of Solr tuning that increase indexing performance.
We'll start with optimizing the schema, then look at sending data to Solr in bulk, and then finish with Lucene's merge factor and optimization.
But first, one of the easiest things you can do is increase the buffer size Solr uses to accumulate data before flushing it to disk.
If you will be loading a lot of data at once, then increase this value.
It is hard-limited to 2048 which almost nobody should choose.
Many experts seem to find that a number in the vicinity of 128 to be good for many apps.
Designing the schema Good schema design is probably one of the most important things you can do to enhance the scalability of Solr.
You should refer to Chapter 2, Schema and Text Analysis, for a refresher on many of the design questions that are inextricably tied to scalability.
The biggest schema issue to look at for maximizing scalability is: Are you storing the minimum information you need to meet the needs of your users? There are a number of attributes in your schema field definitions, which inform us about what is being indexed:
If you need faster indexing, reduce the text analysis you perform in schema.xml to only what you need.
For example, if you are certain the input is plain ASCII text then don't bother mapping accented characters to ASCII equivalents.
Sending data to Solr in bulk Indexing documents into Solr is often a major bottleneck due to the volume of data that needs to be indexed initially compared to the pace of ongoing updates.
The best way to speed up indexing is to index documents in batches.
Solr supports sending multiple documents in a single add operation, and this will lead to a drastic speedup in performance.
However, as the size of your individual documents increase, performance may start to decrease.
It places documents to be added onto queues that separate background threads stream to Solr on independent connections.
Solr receives the documents from different connections in different threads and can load data faster.
Look back at Chapter 9's crawler example to learn more.
The preceding table shows that sending the documents in batches certainly helps the performance, but only up to a point.
The other interesting data point is that in this benchmark, running multiple threads did not offer appreciable greater performance over a single process.
This is because the processing logic in threaded_test.rb for building the XML documents was so slight on the client side that having multiple threads didn't help.
However, if your indexing script is doing more complex business logic, such as multiple SQL calls (often an issue with the DIH), extracting text from rich documents, or calling external services for operations like entity extraction, in order to build the correct document to be submitted to Solr then using a multi threaded process, or multiple separate processes, will increase index throughput.
Don't overlap commits During indexing you may find that you are starting to see this error message:
Every time a commit happens, a new searcher is created, which invokes the searcher warm up process for populating the cache, which can take a while.
In order to deal with this, reduce how often commits are happening.
You can also reduce the amount of time auto-warming takes by reducing the autowarmCount and removing the newSearch query.
Of course, this will lead to slower initial queries as well.
If you are bulk loading data, then you don't need real-time display of the changes and can just do a single commit at the end.
Alternatively, to prevent overlapping commits from happening by each thread, you should use the autoCommit feature to let Solr decide when to commit.
Disabling unique key checking By default, if you specify a uniqueKey for your schema, when indexing content, Solr checks the uniqueness of the document being indexed so that you don't end up with multiple documents sharing the same primary key.
If you know you have unique keys and don't have those documents in the index when doing a bulk load of data, then you can disable this check.
For an update request in any format supported by Solr, add overwrite=false as a request parameter in the URL.
Index optimization factors There are some other factors that can impact how often you want commit and optimize operations to occur.
If you are using Solr's support for scaling horizontally through replication of indexes, then each time a commit or optimize occurs you are causing the transfer of updated indexes to all the slave servers.
This is okay if your commits are small and you are just generating small segment files that are quickly transferred.
But when you perform an optimization you cause the entire index to be replicated.
If you have a multi gigabyte index, you may decide that optimizing the index is not something you want to do on a frequent basis because each time you optimize you cause the entire index to be replicated.
Optimizing your index is no longer quite as important as it used to be.
Optimizing the index saves all the individual segment files into a single segment and removes deleted documents, which reduces the size and number of binary files that Lucene has to operate over.
However, the automatic merging of segments by Lucene, specified by the mergeFactor setting puts an upper limit on how many segment files will be generated and keeps your index in a near optimal state.
If your index is so large that optimizations are taking longer than desired or using more disk space during optimization than you can spare, but you still want to take advantage of removing deleted documents from your indexes then consider adding the maxSegments parameter to the optimize command.
In the Update-XML format, this would be the maxSegments attribute on the optimize element, and in the URL it would be a correspondingly named parameter.
By default this parameter is 1 since an optimize results in a single Lucene "segment"
By setting it larger than 1 but less than the mergeFactor, you permit partial optimization to no more than the specified number of segments.
Of course, the index won't be fully optimized and therefore searches will be slower.
Think about if you can have two strategies for indexing your content.
One that is used during bulk loads that focuses on minimizing commits/ optimizes and indexes your data as quickly as possible, and then a second strategy used during day-to-day routine operations that potentially indexes documents more slowly, but commits and optimizes more frequently to reduce the impact on any search activity being performed.
Another setting that causes a fair amount of debate is the mergeFactor setting, which controls how many segments Lucene should build before merging them together on disk.
The rule of thumb is that the more static your content is, the lower the merge factor you want.
If your content is changing frequently, or if you have a lot of content to index, then a higher merge factor is better.
So, if you are doing sporadic index updates, then a merge factor of 2 is great, because you will have fewer segments, which leads to faster searching.
Check out the great blog post by Lucene in Action author Mike McCandless that has a visualization of what happens during segment merging.
Enhancing faceting performance There are a few items to look at when ensuring that faceting performs well.
First of all, faceting and filtering (the fq parameter) go hand-in-hand, thus monitor the filter cache to ensure that it is adequately sized.
The filter cache is used for faceting itself as well.
In particular, any facet.query or facet.date based facets will store an entry for each facet count returned.
You should ensure that the resulting facets are as reusable as possible from query to query.
For example, it's probably not a good idea to have direct user input to be involved in either a facet.query or in fq because of the variability.
As for dates, try to use fixed intervals that don't change often or round NOW relative dates to a chunkier interval (for example, NOW/DAY instead of just NOW)
For text faceting (example facet.field), the filterCache is not used unless you explicitly set facet.method to enum.
Finally, you should add representative faceting queries to firstSearcher in solrconfig.xml so that when Solr executes its first user query, the relevant caches are already warmed up.
Using term vectors A term vector is a list of terms resulting from the text analysis of a field's value.
It optionally contains the term frequency, document frequency, and numerical offset into the text.
Without them, the same information can be derived at runtime but that's slower.
While disabled by default, enabling term vectors for a field in schema.
Improving phrase search performance For indexes reaching a million documents or more, phrase searches can be slow.
If you are using the automatic phrase boosting features of the dismax query parser (excellent for relevancy) then more phrase queries are occurring than you may be aware of.
What slows down phrase searches are the presence of terms in the phrase that show up in a lot of documents.
In order to ameliorate this problem, the particularly common and uninteresting words like "the" can be filtered out through a stop filter.
But this thwarts searches for a phrase like "to be or not to be" and prevents disambiguation in other cases where these words, despite being common, are significant.
Besides, as the size of the index grows, this is just a band-aid for performance as there are plenty of other words that shouldn't be considered for filtering out yet are common.
Shingling (sometimes called word-grams) is a clever solution to this problem, which combines pairs of consecutive terms into one so-called shingle.
The original terms still get indexed, but only the shingles are used in phrase queries.
Shingles naturally have a very low frequency relative to single words.
Consider the text "The quick brown fox jumped over the lazy dog"
Use of shingling in a typical configuration would yield the indexed terms (shingles) "the quick", "quick brown", "brown fox", "fox jumped", "jumped over", "over the", "the lazy", and "lazy dog" in addition to all of the original nine terms.
Since so many more terms are indexed, naturally there is a commensurate increase in indexing time and resulting size.
Common-grams is a more selective variation of shingling that only shingles when one of the consecutive words is in a configured list.
Given the sentence above using an English stop word list, the indexed terms would be "the quick", "over the", "the lazy", and the original nine terms.
As a side benefit, these techniques also improve search relevancy since the TF and IDF factors are using coarser units (the shingles) than individual terms.
In our MusicBrainz data set, there are nearly seven million tracks, and that is a lot! These track names are ripe for either shingling or common-grams.
Despite the high document count, the documents are small and so the actual index is only a couple gigabytes.
Here is a variation of the MusicBrainz title field called title_commonGrams.
Notice that the filter's class name varies from index to query time, which is very unusual.
To come up with a list of common words for common-grams, use stop words and add some of the Top Terms list in Solr's schema browser as a guide for the field in question.
You could try a more sophisticated methodology, but this is a start.
Shingle filters go in the same position, but they are configured a little differently:
You might choose to save additional index space and search performance by adding a stop filter after shingling or common-grams for typical stop-words so long as they don't need to be searchable by themselves.
This wasn't done here since it's not a good decision for short fields.
Evaluating the search performance improvement of shingling proved to be tricky for the limited time I gave to it.
Some rough (non-scientific) testing showed that a search for Hand in my Pocket against the shingled field versus the non-shingled field was two to three times faster.
I've seen very compelling search performance numbers using common-grams from others online but I didn't evaluate it.
Shingling and common-grams increase phrase search performance at the expense of indexing speed and disk use.
In the following table I present the relative cost of these two techniques on the track name field in MusicBrainz compared to the cost of doing typical text analysis.
These percentages may look high and might scare you away but remember that these are based purely on one aspect (the index portion) of one field.
The overall index time and disk use didn't change dramatically, not even with shingling.
You should try it on your data to see the effects.
Use of either is recommended for most applications Given that shingling takes over five times as long and uses twice as much disk space, it makes more sense on small to medium scale indexes where phrase boosting is used to improve relevancy.
Moving to multiple Solr servers (Scale horizontally) Once you've optimized Solr running on a single server, and reached the point of diminishing returns for optimizing further, the next step is to split the querying load over multiple slave instances of Solr.
The ability to scale horizontally is a hallmark of modern scalable Internet systems, and Solr shares this.
Replication Introduced in Solr 1.4 is an HTTP-based replication strategy that is tightly integrated into Solr.
Configuration is done through the already familiar solrconfig.xml, and replication encompasses transferring both data as well as configuration files such as solrconfig.xml and works across both Unix and Windows environments.
The admin interface provides monitoring and control of replication—for example, to force the start of replication or aborting a stalled replication.
Replication works by taking advantage of the fact that changes to indexes are appended to the end of the index data structure.
This means that when you modify a single document as part of a 3 GB index, you are not changing the 3 GB segment file, but instead recording that change to a much smaller segment file, measured in megabytes or less! The master Solr server exposes an overall index version that is incremented each time a change is made, and exposes all the segment files to the slaves.
The individual slave Solr instances periodically poll the master to compare their list of segments with what the master Solr offers, and then download via HTTP the segments that they are missing.
Once the files are downloaded the slaves internally perform a commit operation that adds the new segments to their index and exposes the new data to users querying them.
Prior to Solr 1.4, replication was performed using Unix shell scripts that transferred data between servers through rsync, scheduled using cron.
This replication was based on the fact that by using rsync, you could replicate only Lucene segments that had been updated from the master to the slave servers.
The script-based solution has worked well for many deployments, but suffers from being relatively complex, requiring external shell scripts, cron jobs, and rsync daemons in order to be setup.
Starting multiple Solr servers We'll test running multiple separate Solr servers by firing up multiple copies of our example Solr using separate ports and data directories.
A very typical replication pattern is one master and two slaves.
You should now have three terminal windows running three separate Solr instances on separate ports.
If you look at the scripts you'll see we are passing in a different port number for Jetty, a different data directory (named with the port number to make it easier to distinguish) to store the slave indexes in, and enabling the instance to function as a slave:
You actually can have multiple Solr servers share an index, just make sure only one Solr makes changes to the index.
When changes are made, make sure you tell the others to reopen their view of the index by performing a commit operation, despite the fact that they have not made any changes! This can be really useful when your indexes are huge and you don't want to make copies of them, but have plenty of RAM and CPU available.
The pollInterval controls how often the slave checks the master for updated segments.
If the master index is constantly changing, but you don't need to have those changes immediately reflected in the slaves, then a longer poll interval, such as 15 minutes, will reduce the overall amount of data that needs to be transferred over the network.
Also, this highlights that in a replicated environment, optimizations are expensive not because of the time it takes to perform the operation, but because of the network download time.
For updated information on setting up replication view the wiki at http://wiki.
Load balancing searches across slaves Go ahead and fire up three separate instances of Solr.
Two of the servers will serve up results for search queries, while one server will function as the master copy of the index.
Indexing into the master server In a new terminal session, we're going to take a CSV file of the MusicBrainz album release data to use as our sample data.
Unzip the file so you have the full dataset with over 600K releases running:
Make sure you supply the correct path for the stream.file parameter:
You can monitor the progress of streaming the release data by using the statistics page at http://localhost:8983/solr/mbreleases/admin/stats.jsp#update and looking at the docPending value.
Refresh the page, and it will count up to the total 603,090 documents.
Configuring slaves Once the indexing is done, check the number of documents indexed; it should be 603,090
Each slave has been configured via the pollInterval value to check the master Solr for updated segments every 60 seconds.
You can keep starting more slave servers by passing in a separate port and data directories that will each start polling the same master Solr.
You can trigger a replication by using the Replication admin page for each slave.
The page will reload showing you how much of the data has been replicated from your master server to the slave server.
Alternatively, you can specify the masterUrl as part of the URL and manually trigger an update:
Configuring load balancing We now have three Solr's running, one master and two slaves in separate terminals.
We don't have a single URL that we can provide to clients, which leverages the pool of slave Solr servers.
For the purposes of this example, we are going to use a simple Java load balancer called Distributor to provide a proxy in front of our two slaves that sends traffic in a very simple round robin fashion.
HAProxy For production use, I like HAProxy, a simple and powerful HTTP proxy server to do a round robin load balancing between the pool of slaves.
This allows us to have a single IP address, and have requests redirected to one of the pool of servers, without requiring configuration changes on the client side.
Going into the full configuration of HAProxy is out of the scope of this book; for more information visit http://haproxy.1wt.eu/
Distributor runs a test against each target server in the target_group to check if it is available or not by checking a URL's response for various conditions.
Solr includes a request handler called PingRequestHandler at /admin/ping designed for this purpose.
Adding the healthcheck adds an Enable/Disable link to the admin web page, and a file named server-enabled is created/deleted in the data directory.
If you click on Disable, then /admin/ping will always return false.
Go ahead and issue some queries and you will see them logged by whichever slave server you are directed to.
If you then stop Solr on one slave server and do another search request, you will be transparently forwarded to the other slave server! The admin page for each slave will reflect the actual address and port the slave is running on.
There is a SolrJ client side interface that does load balancing as well.
LBHttpSolrServer requires the client to know the addresses of all of the slave servers and isn't as robust as a proxy, though it does simplify the architecture.
Sharding indexes Sharding is the process of breaking a single logical index in a horizontal fashion across records versus breaking it up vertically by entities.
It is a common database scaling strategy when you have too much data for a single database server to handle.
In Solr terms, sharding is breaking up a single Solr core across multiple Solr servers, each with identical schemas, as compared to breaking up a single Solr core over multiple cores with differing schemas on a single server through a multi core setup.
Solr has the ability to take a single query and run it over multiple Solr shards, and then aggregate the results together into a single result set that is returned to the client.
You should use sharding if your queries start to take too long to execute on a single server that isn't otherwise heavily taxed.
You typically only need sharding when you have millions of documents to be searched and complex queries that require significant amounts of CPU and memory to process.
If running a single query is fast enough, and you need to handle more users, then use the whole index replication approach instead!
Sharding isn't a completely transparent operation the way replicating whole indexes is from an external perspective.
The key constraint is when indexing documents you want to evenly balance the distribution of documents across the shards so each shard is more or less evenly sized to maintain relevancy.
Solr doesn't have any logic for evenly distributing indexed data over shards.
When querying for data, you supply a shards parameter that lists which Solr shards to aggregate results from.
This means a lot of knowledge of the structure of the Solr architecture is required on the client side.
Lastly, every document needs a unique key (ID), because you are breaking up the index based on documents, and these documents are distinguished from each other by their document ID.
Assigning documents to shards There are a number of approaches you can take for splitting your documents across servers.
Assuming your servers share the same performance characteristics, such as if you are sharding across multiple EC2 servers, then you want to break your data up more or less equally across the servers.
We could distribute our mbreleases data based on the release names.
All release names that start between A and M would go to one shard, the remaining N through Z would be sent to the other shard.
However, the chance of an even distribution of release names isn't very likely! A better approach to evenly distribute documents is to perform a hash on the unique ID and take the mod of that value to determine which shard it should be distributed to like in this chunk of Ruby code:
As long as the number of shards doesn't change, every time you index the same document, it will end up on the same shard! With reasonably balanced document distribution, the individual shards calculation of what documents are relevant should be good enough.
If you have many more documents on one server versus another, then the one with fewer documents will seem as relevant as the one with many documents, as relevancy is calculated on a per-server basis.
You might want to change this algorithm if you have a pool of servers supporting your shards that are of varying capacities and if relevance isn't a key issue for you.
For your higher capacity servers, you might want to direct more documents to be indexed on those shards.
You can do this by using the existing logic, and then by just listing your higher capacity servers in the SHARDS array multiple times.
Searching across shards (distributed search) The ability to search across shards as if they were a single logical data set is known as distributed search, and it's built into the search request handler and most search components.
It is activated simply by issuing an otherwise normal search request with the shards request parameter, which is a comma-delimited list of URL-like references to Solr instances to search across.
You can issue the search request to one of the shards or to another Solr instance that is configured like them.
The data in the Solr instance receiving the request is not searched unless it is referenced in the shards parameter.
Under the hood, a distributed search results in one or more requests to each shard depending on the components used.
These internal sharded requests will go to the default request handler, which is not necessarily the original one.
This can be overridden with the shards.qt parameter if needed.
The data from each shard is merged appropriately and the final results are returned normally.
Here are the results of the previous example search with nothing unusual about it:
The URLs listed in the shards parameter do not include the leading transport protocol, just the host with the port and path to the Solr core.
You will get no results if you specify http:// in the shard URLs.
You can pass as many shards as you want up to the length a GET URI is allowed, which is at least 4,000 characters.
A common technique to shorten the URL and simplify configuration is to use a separate request handler with the shards parameter defined in the request handler.
This relieves the end users from needing to know which shards to query.
You can also POST the parameters in the body of the request to get around the URI limit.
You can verify that the results are distributed and then combined by issuing the same search for r_a_name:Joplin to each individual shard and then adding up the numFound values.
There are a few key points to keep in mind when using shards to support distributed search:
Combining replication and sharding (Scale deep) Once you've scaled horizontally by either replicating indexes across multiple servers or sharding a single index, and then discover that you still have performance issues it's time to combine both approaches to provide a deep structure of Solr servers to meet your demands.
This is conceptually quite simple, and getting it set up to test is fairly straightforward.
The challenge typically is keeping all of the moving pieces up-to-date, and making sure that you are keeping your search indexes up-to-date.
These operational challenges require a mature set of processes and sophisticated monitoring tools to ensure that all shards and slaves are up-to-date and are operational.
In order to tie the two approaches together, you continue to use sharding to spread out the load across multiple servers.
Without sharding, it doesn't matter how large your pool of slave servers is because you need more CPU power than what just one slave server has to handle an individual query.
Once you have sharded across the spectrum of shard servers, you treat each one as a Master Shard server, configured in the same way as we did in the previous replication section.
This develops a tree of a master shard server with a pool of slave servers.
Then, to issue a query, you have multiple small pools of one slave server per shard that you issue queries against, managed at the application level.
You can even have dedicated Solr for doing the distributed search, one that doesn't have data.
Data updates are handled by updating the top Master Shard servers and then replicated down to the individual slaves, grouped together into small groups of distributed sharded servers.
Obviously, this is a fairly complex setup and requires a fairly sophisticated load balancer to frontend this whole collection, but it does allow Solr to handle extremely large data sets.
Near real time search Real time search is defined as the ability to search for content immediately after adding/updating it—perhaps within a second or so.
This means that if an end-user is performing some sort of add/update action on content, then the system is able to process it fast enough so that if they then search for that content as fast as they can, they will always be able to search for it.
Near real time search (often abbreviated as NRT) allows for a larger time window—most would say less than 30 seconds.
This time window is also known as the index latency.
Many users want real time search, but unless you have a trivially small amount of data, that simply isn't possible yet.
You'll have to settle for near real time search or worse.
Reducing the index latency is an ongoing effort with the biggest leap forward expected in Solr 4.0
Here are a series of tips, in no particular order, to consider in your quest for the holy grail of real time search with Solr.
You won't get there but you'll get close, especially if you have time to dedicate to this challenge.
Where next for scaling Solr? While Solr offers some impressive scaling techniques through replication and sharding of data, it assumes that you know a priori what your scaling needs are.
The distributed search of Solr doesn't adapt to real time changes in indexing or query load and doesn't provide any fail-over support.
SolrCloud is an ongoing effort to build a fault tolerant, centrally managed support for clusters of Solr instances and is part of the trunk development path (Solr 4.0)
SolrCloud introduces the idea that a logical collection of documents (otherwise known as an index) is distributed across a number of slices.
Each slice is made up of shards, which are the physical pieces of the collection.
In order to support fault tolerance, there may be multiple replicas of a shard distributed across different physical nodes.
To keep all this data straight, Solr embeds Apache ZooKeeper as the centralized service for managing all configuration information for the cluster of Solr instances, including mapping which shards are available on which set of nodes of the cluster.
At the time of writing, you still need to manage creating the various shards and replica copies yourself during index time.
However, compared to the current sharding approach where you have to supply via the shards parameter the addresses of all the shards to search over, with SolrCloud you can just add a distrib=true parameter to your query to let Solr figure out what the relevant shards are to query:
Of course, you can still search a specific set of shards with failover replicas delimited using the | character:
In this case we queried for all the documents in the collection mbreleases, made up of two shards, each shard consisting of two replicas, each in their own individual Solr server.
The first release will be in Solr 4.0; however, it is already in a useable state for many use cases and it is well worth investigation.
SOLR-1873 is the JIRA issue for tracking the progress of this development.
Summary Solr offers many knobs and levers for increasing performance; the biggest challenge can be figuring out which knobs and levers to use! Make sure you budget time to try a number of approaches, and take a stepwise approach to trying different approaches out.
From turning the simpler knobs for enhancing the performance of a single server, to pulling the big levers of scaling horizontally through replication and sharding, performance and scalability with appropriate hardware are issues that can be solved fairly easily.
Moreover, for those projects where truly massive search infrastructure is required, the ability to shard over multiple servers and then delegate to multiple slaves provides an almost linear scalability capacity.
Search Quick Reference This appendix is a convenient reference for common search related request parameters.
It is assumed you already read the related material in the book and are just looking for something to jog your memory.
You can find an electronic PDF version of this appendix here: http://www.solrenterprisesearchserver.com.
Quick reference A  means the parameter can appear a variable number of times.
Recommend: *:* (all docs) qf The query fields, including optional boosts.
XML, importing from file with XSLT  96 directory structure, Solr.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cutting-edge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Create secure, reliable, and easy-to-use web services using Apache Axis2
Attain a more flexible and extensible framework with the world class Axis2 architecture.
Learn all about AXIOM - the complete XML processing framework, which you also can use outside Axis2
Over 100 recipes to discover new ways to work with Apache's Enterprise Search Server.
Apache Solr to make your search engine quicker and more effective.
Deal with performance, setup, and configuration problems in no time.
Discover little-known Solr functionalities and create your own modules to customize Solr to your company's needs.
Part of Packt's Cookbook series; each chapter covers a different aspect of working with Solr.
Implement engineering practices in your application development process with Apache Maven.
Master Wicket by example by implementing real-life solutions to every day tasks.
The Apache Wicket Cookbook covers the full spectrum of features offered by the Wicket web framework.
Implement advanced user interactions by following the live examples given in this Cookbook.
Create reusable components and speed up your web application development.
A quick tour of Solr Loading sample data A simple query Some statistics The sample browse interface.
Chapter 2: Schema and Text Analysis MusicBrainz.org One combined index or separate indices One combined index Problems with using a single combined index.
Step 4: (Optional) Omit the inclusion of fields only used in search results.
The schema.xml file Defining field types Built-in field type classes Numbers and dates Geospatial.
Our MusicBrainz field definitions Copying fields The unique key The default search field and query operator.
Example DIH configurations Importing from databases Importing XML from a file with XSLT Importing multiple rich document files (crawling)
Request handlers Query parameters Search criteria related parameters Result pagination related parameters Output related parameters Diagnostic related parameters.
Query parsers and local-params Query syntax (the lucene query parser) Matching all the documents Mandatory, prohibited, and optional clauses Boolean operators.
Field qualifier Phrase queries and term proximity Wildcard queries Fuzzy queries.
The Dismax query parser (part 1) Searching multiple fields Limited query syntax Min-should-match Basic rules Multiple rules What to choose.
Function queries Field references Function reference Mathematical primitives Other math ord and rord Miscellaneous functions.
How to boost based on an increasing numeric field Step by step… External field values.
How to boost based on recent dates Step by step…
Field requirements Types of faceting Faceting field values Alphabetic range bucketing.
Facet queries Building a filter query from a facet Field value filter queries Facet range filter queries.
The SpellCheck component Schema configuration Configuration in solrconfig.xml Configuring spellcheckers (dictionaries) Processing of the q parameter Processing of the spellcheck.q parameter.
Building the dictionary from its source Issuing spellcheck requests Example usage for a misspelled query.
Query complete / suggest Query term completion via facet.prefix Query term completion via the Suggester Query term completion via the Terms component.
The Stats component Configuring the stats component Statistics on track durations.
Logging HTTP server request access logs Solr application logging Configuring logging output Logging using Log4j Jetty startup integration Managing log levels at runtime.
Securing index data Controlling document access Other things to look at.
Nutch for crawling web pages Maintaining document security with ManifoldCF Connectors Putting ManifoldCF to use.
Indexing performance Designing the schema Sending data to Solr in bulk Don't overlap commits Disabling unique key checking Index optimization factors.
Enhancing faceting performance Using term vectors Improving phrase search performance.
Load balancing searches across slaves Indexing into the master server Configuring slaves.
Configuring load balancing Sharding indexes Assigning documents to shards Searching across shards (distributed search)
Combining replication and sharding (Scale deep) Near real time search.
