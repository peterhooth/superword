Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Characteristics of the types of data handled by search engines.
With fast-growing technologies like social media, cloud computing, mobile applications, and big data, these are exciting times to be in computing.
One of the main challenges facing software architects is the need to handle massive volumes of data consumed and produced by a huge global user base.
In addition, users expect online applications to always be available and responsive.
To address the scalability and availability needs of modern web applications, we’ve seen a growing interest in specialized, non-relational data storage and processing technologies, collectively known as NoSQL (Not only SQL)
These systems share a common design pattern of matching the storage and processing engine to specific types of data rather than forcing all data into the once de facto standard relational model.
In other words, NoSQL technologies are optimized to solve a specific class of problems for specific types of data.
The need to scale has led to hybrid architectures composed of a variety of NoSQL and relational databases; gone are the days of the one-size-fits-all data processing solution.
This book is about a specific NoSQL technology, Apache Solr, which, like its non-relational brethren, is optimized for a unique class of problems.
Specifically, Solr is a scalable, readyto-deploy enterprise search engine that’s optimized to search large volumes of text-centric data and return results sorted by relevance.
That was a bit of a mouthful, so let’s break the previous statement down into its basic parts:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Scalable—Solr scales by distributing work (indexing and query processing) to multiple servers in a cluster.
Ready to deploy—Solr is open-source, is easy to install and configure, and provides a preconfigured example to help you get started.
Optimized for search—Solr is fast and can execute complex queries in subsecond speed, often only 10’s of milliseconds.
Large volumes of documents—Solr is designed to deal with indexes containing millions of documents.
Text-centric—Solr is optimized for searching natural language text, like emails, web pages, resumes, PDF documents, and social messages like tweets or blogs.
Results sorted by relevance—Solr returns documents in ranked order based on how relevant each document is to the user’s query.
In this book, you’ll learn how to use Solr to design and implement scalable search solutions.
We’ll begin our journey by learning about the types of data and uses cases Solr supports.
This will help you understand where Solr fits into the big picture of modern application architectures and which problems Solr is designed to solve.
Therefore, rather than speculate on why you’re considering Solr, we’ll get right down to the hard questions you need to answer about your data and use cases in order to decide if a search engine is right for you.
In the end, it comes down to understanding your data and users and then picking a technology that works for both.
Let’s start by looking at the properties of data that a search engine is optimized to handle.
A hallmark of modern application architectures is matching the storage and processing engine to your data.
If you’re a programmer, then you know to select the best data structure based on how you use the data in an algorithm, that is, you don’t use a linked list when you need fast random lookups.
There are four main characteristics of data search engines like Solr are optimized to handle.
A possible fifth characteristic is having a large volume of data to deal with, that is, “big data,” but our focus is on what makes a search engine special among other NoSQL technologies.
It goes without saying that Solr can deal with large volumes of data.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Although these are the four main characteristics of data that search engines like Solr handle efficiently, you should think of them as rough guidelines and not strict rules.
Let’s dig into each of these characteristics to see why they’re important for search.
For now, we’ll focus on the high-level concepts and get into the “how” in later chapters.
We think “unstructured” is a little ambiguous because any text document based on human language has implicit structure.
You can think of the term “unstructured” as being from the perspective of a computer, which sees text as a stream of characters.
The character stream must be parsed using language-specific rules to extract the structure and make it searchable, which is exactly what search engines do.
We think the label “text-centric” is more appropriate for describing the type of data Solr handles because a search engine is specifically designed to extract the implicit structure of text into its index to improve searching.
Text-centric data implies that the text of a document contains “information” that users are interested in finding.
Of course, a search engine also supports non-text data like dates and numbers, but its primary strength is handling text data based on natural language.
Also, the “centric” part is important because if users aren’t interested in the information in the text, then a search engine may not be the best solution for your problem.
For example, consider an application where employees create travel expense reports.
Each report contains a number of structured data fields like date, expense type, currency, and amount.
In addition, each expense may include a notes field where employees can provide a brief description of the expense.
This would be an example of data that contains text but isn’t “text-centric” in that it’s unlikely that the accounting department needs to search the notes field when generating monthly expense reports.
Put simply, despite data containing text fields doesn’t mean it’s a natural fit for a search engine.
Take a moment and think about whether your data is “text-centric,” The main consideration for us is whether or not the text fields in your data contain information that users will want to query.
If yes, then a search engine is probably a good choice.
First, though, let’s be clear that Solr does allow you to update existing documents in your index.
You can think of read-dominant as meaning that documents are read much more often then they’re created or updated.
But don’t take this to mean that you can’t write a lot of data or that you have limits on how frequently you can write new data.
In fact, one of the key features in Solr 4 is near-real-time search, which allows you to index thousands of documents per second and have them be searchable almost immediately.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The key point behind read-dominant data is that when you do write data to Solr, it’s intended to be read and reread many, many times over its lifetime.
You can think of a search engine as being optimized for executing queries (a read operation), for example, as opposed to storing data (a write operation)
Also, if you have to update existing data in a search engine often, then that could be an indication that a search engine might not be the best solution for your needs.
Another NoSQL technology, like Cassandra, might be a better choice when you need fast random-writes to existing data.
In a search engine, a document is a self-contained collection of fields where each field only holds data and doesn’t contain nested fields.
In other words, a document in a search engine like Solr has a flat structure and doesn’t depend on other documents.
The “flat” concept is slightly relaxed in Solr in that a field can have multiple values but fields don’t contain sub-fields.
That is, you can store multiple values in a single field but you can’t nest fields inside of other fields.
The flat, document-oriented approach in Solr works well with data that’s already in document format, such as a web page, blog, or PDF document, but what about modeling normalized data stored in a relational database? In this case, you need to denormalize data spread across multiple tables into a flat, self-contained document structure.
You also want to consider which fields in your documents must be stored in Solr and which should be stored in another system, such as a database.
Put simply, a search engine isn’t the place to store data unless it’s useful for search or displaying results.
For example, if you have a search index for online videos, then you don’t want to store the binary video files in Solr.
Rather, large binary fields should be stored in another system, such as a content distribution network (CDN)
In general, you should store the minimal set of information for each document needed to satisfy search requirements.
This is a clear example of not treating Solr as a general data storage technology; Solr’s job is to find videos of interest and not to manage large binary files.
This means that documents in a search index don’t need to have a uniform structure.
In a relational database, every row in a table has the same structure.
Of course, there should be some overlap between the fields in documents in the same index but they don’t have to be identical.
For example, imagine a search application for finding homes for rent or sale.
Listings will obviously share fields like location, number of bedrooms, and number of bathrooms, but they’ll also have different fields based on the listing type.
A home for sale would have fields for listing price and annual property taxes, whereas a home for rent would have a field for monthly rent and pet policy.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
To summarize, search engines in general and Solr in particular are optimized to handle.
Overall, this implies that Solr isn’t a general-purpose data storage and processing technology, which is one of the main differentiating factors of NoSQL (not only SQL) technologies.
The whole point of having a wide variety of options for storing and processing data is that you don’t have to find a one-size-fits-all technology.
Search engines are good at specific things and quite horrible at others.
This means in most cases, you’re going to find Solr complements relational databases and other NoSQL technologies more than it replaces them.
Now that we’ve talked about the type of data Solr is optimized to handle, let’s think about the primary use cases a search engine like Solr is designed to handle.
These use cases are intended to help you understand how a search engine is different than other data processing technologies.
In this section, we look at some of the things you can do with a search engine like Solr.
As with our discussion of the types of data in section 1.1.1, use these as guidelines and not strict rules.
Before we get into specifics though, you should keep in mind that the bar for excellence in search is high.
Modern users are accustomed to web search engines like Google and Bing being fast and effective at serving modern web information needs.
Moreover, most popular websites have powerful search solutions to help people find information quickly.
When you’re evaluating a search engine like Solr and designing your search solution, make sure you put user experience as a high priority.
But it’s worth mentioning because keyword search is the most typical way users will begin working with your search solution.
It’d be rare for a user to want to fill out a complex search form initially.
Given that basic keyword search will be the most common way users will interact with your search engine, then it stands to reason that this feature must provide a great user experience.
In general, users want to type in a few simple keywords and get back great results.
This may sound like a simple task of matching query terms to documents but consider a few of the issues that must be addressed to provide a great user experience:
Relevant results must be returned quickly, within a second or less in most cases.
Spell correction in case the user misspells some of the query terms.
Auto-suggestions to save users some typing, particularly for mobile applications.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Phrase handling, that is, does the user want documents matching all words or any of the words in a phrase.
Proper handling of queries with common words like “a,” “an,” “of,” and “the”
Giving the user a way to see more results if the top results aren’t satisfactory.
As you can see, a number of issues exist that make a seemingly simple feature hard to implement without a specialized approach.
But with a search engine like Solr, these features come out of the box and are easy to implement.
Once you give users a powerful tool to execute keyword searches, you need to consider how to display the results.
This brings us to our next use case of ranking results based on their relevance to the user’s query.
In a SQL query to a relational database, a row either matches a query or not and results are sorted based on one of the columns.
On the other hand, a search engine returns documents sorted in descending order by a score that indicates the strength of the match of the document to the query.
How strength of match is calculated depends on a number of factors but in general a higher score means the document is more relevant to the query.
Ranking documents by relevancy is important for a couple of reasons.
First, modern search engines typically store a large volume of documents, often millions or billions of documents.
Without ranking documents by relevance to the query, users can become overloaded with results without any clear way to navigate them.
Second, users are more comfortable and accustomed to getting results from other search engines using only a few keywords.
Users are impatient and expect the search engine to “do what I mean, not what I say.” This is true of search solutions backing mobile applications where users on the go will enter short queries with potential misspellings and expect it to “just work.”
To influence ranking, you can assign more weight or “boost” certain documents, fields, or specific terms.
For example, you can boost results by their age to help push newer documents towards the top of search results.
For many users, though, this is only the first step in a more interactive session where the search results give them the ability to keep exploring.
One of the primary use cases of a search engine is to drive an information discovery session.
Frequently, your users won’t know exactly what they’re looking for and typically don’t have any idea what information is contained in your system.
A good search engine helps users narrow in on their information needs.
The central idea here is to return some documents from an initial query, as well as some tools to help users refine their search.
In other words, in addition to returning matching documents, you also return tools that give your users an idea of what to do next.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This is known as faceted-search and is one of the main strengths of Solr.
We’ll see an example of faceted search for a real estate search in section 1.2
DON’T USE A SEARCH ENGINE TO DO … Lastly, let’s consider a few use cases where a search engine wouldn’t be useful.
More documents for the same query can be retrieved using Solr’s built-in paging support.
Consider a query that matches a million documents—if you request all of those documents back at once, you should be prepared to wait a long time.
The query itself will likely execute quickly but reconstructing a million documents from the underlying index structure will be extremely slow, as engines like Solr store fields on disk in a format from which it’s easy to create a few documents, but takes a long time to reconstruct many documents when generating results.
Another use case where you shouldn’t use a search engine is for deep analytics tasks that require access to a large subset of the index.
Even if you avoid the previous issue by paging through results, the underlying data structure of a search index isn’t designed for retrieving large portions of the index at once.
We’ve touched on this previously, but we’ll reiterate that search engines aren’t the place for querying across relationships between documents.
Solr does support querying using a parent-child relationship, but doesn’t provide support for navigating complex relational structures.
In chapter 3, you’ll learn some techniques to adapt relational data to work with Solr’s flat document structure.
Lastly, there’s no direct support in most search engines for document-level security, at least not in Solr.
If you need fine-grained permissions on documents, then you’ll have to handle that outside of the search engine.
Now that we’ve seen the types of data and use cases where a search engine is the right solution, it’s time to dig into what Solr does and how it does it from a high level.
In the next section, you’ll learn what Solr does and how it approaches important software design principles like integration with external systems, scalability, and high-availability.
This will help you understand what specific features Solr provides and the motivation for their existence.
But, before we get into the specifics of what Solr is, let’s make sure you know what Solr isn’t:
Now, imagine we need to design a real estate search web application for potential.
The central use case for this application will be searching for homes to buy across the United States using a web browser.
Figure 1.1 depicts a screen shot from this fictitious web application.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
What’s important is the type of experience that Solr can support.
Figure 1.1 Mock-up screen shot of fictitious search application to depict Solr features.
Let’s take a quick tour of the screen shot in figure 1.1 to illustrate some of Solr’s key features.
First, starting at the top-left corner, working clock-wise, Solr provides powerful features to support a keyword search box.
As we discussed in section 1.1.2, providing a great user experience with basic keyword search requires complex infrastructure that Solr provides out-of-the-box.
Specifically, Solr provides spell checking, auto-suggesting as the user types, synonym handling, phrase queries, and text-analysis tools to deal with linguistic variations in query terms, such as “buying a house” or “purchase a home.”
Solr also provides a powerful solution for implementing geo-spatial queries.
In figure 1.1, matching home listings are displayed on a map based on their distance from the latitude / longitude of the center of some fictitious neighborhood.
With Solr’s geo-spatial support, you can sort documents by geo-distance or even rank documents by geo-distance.
It’s also important that geo-spatial searches are fast and efficient to support a user interface that allows users to zoom in and out and move around on a map.
Once the user performs a query, the results can be further categorized using Solr’s faceting support to show features of the documents in the result set.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In figure 1.1, search results are categorized into three facets for features, home style, and listing type.
Now that we have a basic idea of the type of functionality we need to support our real estate search application, let’s see how we’d implement these features with Solr.
To begin, we need to know how Solr matches home listings in the index to queries entered by users, as this is the basis for all search applications.
Solr is built on Apache Lucene, a popular Java-based open source information retrieval library.
For now, we’ll touch on the key concepts behind information retrieval starting with the formal definition taken from one of the prominent academic texts on modern search concepts:
Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers)
In the case of our example real estate application, the user’s primary information need is finding a home to purchase based on location, home style, features, and price.
Our search index will contain home listings from across the U.S., which definitely qualifies as a “large collection.” In a nutshell, Solr uses Lucene to provide the core infrastructure for indexing documents and executing searches to find documents.
Under the covers, Lucene is a Java-based library for building and managing an inverted index, which is a specialized data structure for matching query terms to text-based documents.
Figure 1.2 provides a simplified depiction of a Lucene inverted index for our example real estate search application.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 1.2 The key data structure supporting information retrieval is the inverted index.
You might be thinking that a relational database could easily return the same results using a SQL query, which is true for this simple example.
But one key difference between a Lucene query and a database query is that in Lucene, results are ranked by their relevance to a query and database results can only be sorted by one of the table columns.
In other words, ranking documents by relevance is a key aspect of information retrieval and helps differentiate it from other types of search.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
It might surprise you that search engines like Google also use an inverted index for searching the web.
In fact, the need to build a web-scale inverted index led to the invention of MapReduce.
MapReduce is a programming model that distributes large-scale data processing operations across a cluster of commodity servers by formulating an algorithm into two phases: map and reduce.
With roots in functional programming, MapReduce was adapted by Google for building its massive inverted index to power web search.
Using MapReduce, the map phase produces unique term and document ID where the term occurs.
In the reduce phase, terms are sorted so that all term / docID pairs are sent to the same reducer process for each unique term.
The reducer sums up all term frequencies for each term to generate the inverted index.
Apache Hadoop provides an open source implementation of MapReduce and is used by the Apache Nutch open source project to build a Lucene inverted index for web-scale search.
A thorough discussion of Hadoop and Nutch are beyond the scope of this book, but we encourage you to investigate these projects if you need to build a large-scale search index.
Now that we know that Lucene provides the core infrastructure to support search, let’s look at what value Solr adds on top of Lucene, starting with how you define how your index is structured using Solr’s flexible schema.xml configuration document.
Although Lucene provides the infrastructure for indexing documents and executing queries, what’s missing from Lucene is an easy way to configure how you want your index to be structured.
With Lucene you need to write Java code to define fields and how to analyze those fields.
Solr adds a simple, declarative way to define the structure of your index and how you want fields to be represented and analyzed using an XML configuration document named schema.xml.
Under the covers, Solr translates the schema.xml document into a Lucene index.
This saves you programming and makes your index structure easier to understand and communicate to others.
On the other hand, a Solr built index is 100% compatible with a programmatically built Lucene index.
Solr also adds a few nice constructs on top of the core Lucene indexing functionality.
Copy fields provide a way to take the raw text contents of one or more fields and have them applied to a different field.
Dynamic fields allow you to apply the same field type to many different fields without explicitly declaring them in schema.xml.
This is useful for modeling documents that have many fields.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In terms of our example real estate application, it might surprise you that we can use the Solr example server out-of-the-box without making any changes to the schema.xml.
This shows how flexible Solr’s schema support is, because the example Solr server is designed to support product search but works fine for our real estate search example.
At this point, we know that Lucene provides a powerful library for indexing documents, executing queries, and ranking results.
And, with schema.xml, you have a flexible way to define the index structure using an XML configuration document instead of having to program to the Lucene API.
Now you need a way access these services from the web.
In the next section, we learn how Solr runs as a Java web application and integrates with other technologies using proven standards such as XML, JSON, and HTTP.
Solr is a Java web application that runs in any modern Java Servlet engine like Jetty or Tomcat, or a full J2EE application server like JBoss or Oracle AS.
Figure 1.3 depicts major software components of a Solr server.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Admittedly, figure 1.3 is a little overwhelming at first glance.
Take a moment to scan over the diagram to get a feel for some of the terminology.
Don’t worry if you’re not familiar with all of the terms and concepts represented in the diagram--after reading this book, you should have a strong understanding of the all concepts presented in figure 1.3
As we mentioned in the introduction to this chapter, the Solr designers recognized that Solr fits better as a complementary technology that works within existing architectures.
In fact, you’ll be hard-pressed to find an environment where Solr doesn't drop right in.
As we'll see in chapter 2, you can start the example Solr server in a couple of minutes after you finish the download.
To achieve the goal of easy integration, Solr’s core services need to be accessible from many different applications and languages.
Solr provides simple REST-like services based on proven standards of XML, JSON, and HTTP.
As a brief aside, we avoid the “RESTful” label for Solr’s HTTP-based API as it doesn’t strictly adhere to all REST (representational state transfer) principles.
For instance, in Solr you use HTTP POST to delete documents instead of HTTP DELETE.
A REST-like interface is nice as a foundation, but often times, developers like to have access to a client library in their language of choice to abstract away some of the boilerplate machinery of invoking a web service and processing the response.
The good news here is that most popular languages have a Solr client library including Python, PHP, Java, .NET, and Ruby.
One hallmark of modern application architectures is the need for flexibility in the face of rapidly changing requirements.
One of the ways Solr helps this situation is that you don’t have to do all things in Solr with one index, because Solr supports running multiple cores in a single engine.
In figure 1.3, we’ve depicted multiple cores as separate layers all running in the same Java web application environment.
Think of each core as a separate index and configuration and there can be many cores in a single Solr instance.
This allows you to manage multiple cores from one server so that you can share server resources and administration tasks like monitoring and maintenance.
Solr provides an API for creating and managing multiple cores.
One use of Solr’s multicore support is data partitioning, such as having one core for recent documents and another core for older documents, known as chronological sharding.
Another use of Solr’s multicore support is to support multitenant applications.
In our real estate application, we might use multiple cores to manage different types of listings that are different enough to justify having different indexes for each.
Consider real estate listings for rural land instead of homes.
Buying rural land is a different process than buying a home in a city, so it stands to reason that we might want to manage our land listings in a separate core.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Of course, these are high-level abstractions for complex subsystems in Solr; we’ll learn about each later in the book.
Each of these systems is composed of a modular “pipeline” that allows you to plug-in new functionality.
This means that instead of overriding the entire query-processing engine in Solr, you plug-in a new search component into an existing pipeline.
This makes the core Solr functionality easy to extend and customize to meet your specific application needs.
Lucene is an extremely fast search library and Solr takes full advantage of Lucene’s speed.
But regardless of how fast Lucene is, a single server will reach its limits in terms of how many concurrent queries from different users it can handle due to CPU and I/O constraints.
As a first pass to scalability, Solr provides flexible cache management features that help your server avoid recomputing expensive operations.
Specifically, Solr comes preconfigured with a number of caches to save expensive recomputations, such as caching the results of a query filter.
Caching only gets you so far so at some point, you’re going to need to scale out your capacity to handle more documents and higher query throughput by adding more servers.
For now let’s focus on the two most common dimensions of scalability in Solr.
First is query throughput, which is the number of queries your engine can support per second.
Even though Lucene can execute each query quickly, it’s limited in terms of how many concurrent requests a single server can handle.
For higher query throughput, you add replicas of your index so that more servers can handle more requests.
This means if your index is replicated across three servers, then you can handle roughly three times the number of queries per second because each server handles one-third of the query traffic.
In practice, it’s rare to achieve perfect linear scalability so adding three servers may only allow you to handle two and one half times the query volume of one server.
The other dimension of scalability is the number of documents indexed.
If you’re dealing with large volumes of documents, then you’ll likely reach a point where you have too many documents in a single instance and query performance will suffer.
To handle more documents, you split the index into smaller chunks called “shards” and then distribute the searches across the shards.
One trend in modern computing is building software architectures that can scale horizontally using virtualized commodity hardware.
Put simply, add more commodity servers to handle more traffic.
Fueling this trend towards using virtualized commodity hardware are cloud-computing providers such as Amazon EC2
Although Solr will run on virtualized hardware, you should be aware that search is I/O and memory intensive.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Therefore, if search performance is a top priority for your organization, then you should consider deploying Solr on higher end hardware with high-performance disks, ideally solid-state drives (SSD)
Scalability is important, but ability to survive failures is also important for a modern system.
In the next section, we discuss how Solr handles software and hardware failures.
Beyond scalability, you need to consider what happens if one or more of your servers fails, particularly if you’re planning to deploy Solr on virtualized hardware or commodity hardware.
The bottom line is that you must plan for failures.
Even the best architectures and high-end hardware will experience failures.
Let’s assume you have four shards for your index and the server hosting shard #2 loses power.
At this point, Solr can’t continue indexing documents and can’t service queries, so your search engine is effectively “down.” To avoid this situation, you can add replicas of each shard.
In this case, when shard #2 fails, Solr reroutes indexing and query traffic to the replica and your Solr cluster remains online.
The impact of this failure is that indexing and queries can still be processed but may not be as fast because you’ve one less server to handle requests.
At this point, you’ve seen that Solr has a modern, well-designed architecture that’s scalable and fault-tolerant.
Although these are important aspects to consider if you’ve already decided to use Solr, you still might not be convinced that Solr is the right choice for your needs.
In the next section, we describe the benefits of Solr from the perspective of different stakeholders, such as the software architect, system administrator, and CEO.
Let’s begin by addressing why Solr is attractive to software architects.
When evaluating new technology, software architects must consider a number of factors, but chief among those are stability, scalability, and fault-tolerance.
In terms of stability, Solr is a mature technology supported by a vibrant community and seasoned committers.
One thing that shocks new users to Solr and Lucene is that it isn’t unheard of to deploy from source code pulled directly from the trunk rather than waiting for an official release.
We won’t advise you either way on whether this is acceptable for your organization.
We only point this out because it’s a testament to the depth and breadth of.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Put simply, if you have a nightly build off trunk where all the automated tests pass, then you can be sure the core functionality is solid.
As an architect, you’re probably most curious about the limitations of Solr’s approach to scalability and fault-tolerance.
First, you should realize that the sharding and replication features in Solr have been rewritten in Solr 4 to be robust and easier to manage.
Under the covers, SolrCloud uses Apache Zookeeper to distribute configuration across a cluster of Solr nodes and to keep track of cluster state.
Here are some highlights of the new SolrCloud features in Solr:
Queries can be sent to any node in a cluster to trigger a full distributed search across all shards with fail-over and load-balancing support built-in.
But this isn’t to say that Solr scaling doesn’t have room for improvement.
First, not all features work in distributed mode, such as joins.
Second, the number of shards for an index is a fixed value that can’t be changed without reindexing all of the documents.
We’ll get into all of the specifics of SolrCloud in chapter 16, but we want to make sure architects are aware that Solr scaling has come a long way in the past couple of years yet still has some more work to do.
As a system administrator, high among your priorities in adopting a new technology like Solr is whether it fits into your existing infrastructure.
Out of the box, Solr embeds Jetty, the open source Java servlet engine provided by Oracle.
Otherwise, Solr is a standard Java web application that deploys easily to any Java web application server like JBoss and Oracle AS.
All access to Solr can be done via HTTP and Solr is designed to work with caching HTTP reverse proxies like Squid and Varnish.
Solr also works with JMX so you can hook it up to your favorite monitoring application, such as Nagios.
Lastly, Solr provides a nice administration console for checking configuration settings, statistics, issuing test queries, and monitoring the health of SolrCloud.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Although it’s unlikely that a CEO will be reading this book, here are some key talking points about Solr in case your CEO stops you in the hall.
First, executive types like to know an investment in a technology today is going to payoff in the long term.
As you’ll see in the next chapter, Solr “just works” and you can have it up and running in minutes.
Another concern is what happens if the Solr guy walks out the door—will business come to a halt? It’s true that Solr is complex technology but having a vibrant community behind it means you have help when you need it.
And, you have access to the source code, which means if something is broken and you need a fix, you can do it yourself.
Many commercial service providers also can help you plan, implement, and maintain your Solr installation; many of which offer training courses for Solr.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This may be one for the CFO, but Solr doesn’t require much initial investment to get started.
Without knowing the size and scale of your environment, we’re confident in saying that you can start up a Solr server in a few minutes and be indexing documents quickly.
A modest server running in the cloud can handle millions of documents and many queries with sub-second response times.
Finally, let’s do a quick run-down of Solr’s main features organized around the following categories:
Providing a great user experience with your search solution will be a common theme throughout this book so let’s start by seeing how Solr helps make your users happy.
Solr provides a number of important features that help you deliver a search solution that’s easy to use, intuitive, and powerful.
But you should note that Solr only exposes a REST-like HTTP API and doesn’t provide search-related UI components in any language or framework.
You’ll have to roll up your sleeves and develop your own search UI components that take advantage of some of the following user experience features:
Geo-spatial search PAGINATION AND SORTING Rather than returning all matching documents, Solr is optimized to serve paginated requests where only the top N documents are returned on the first page.
If users don’t find what they’re looking for on the first page, then you can request subsequent pages using simple API request parameters.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In our real estate example (figure 1.1) we saw how search results from a basic keyword search were organized into three facets: Features, Home Style, and Listing Type.
AUTO-SUGGEST Most users will expect your search application to “do the right thing” even if they provide incomplete information.
Auto-suggest helps users by allowing them to see a list of suggested terms and phrases based on documents in your index.
Solr’s auto-suggest features allows user to start typing a few characters and receive a list of suggested queries as they type.
This reduces the number of incorrect queries, particularly because many users may be searching from a mobile device with small keyboards.
Auto-suggest gives users examples of terms and phrases available in the index.
Again, users expect to be able to type misspelled words into the search box and the search engine should handle it gracefully.
Auto-correct—Solr can make the spell correction automatically based on whether the misspelled term exists in the index.
Did you mean that Solr can return a suggested query that might produce better results so that you can display a hint to your users, such as “Did you mean highlands?” if you user typed in “hilands.”
Most useful for longer format documents, so you can help users find relevant information in longer documents by quickly scanning highlighted sections in search results.
GEO-SPATIAL Geographical location is a first-class concept in Solr 4 in that it has built-in support for indexing latitude and longitude values as well as sorting or ranking documents by geographical distance.
Solr can find and sort documents by distance from a geo-location (latitude and longitude)
In the real estate example, matching listings are displayed on an interactive map where users can zoom in/out and move the map center point to find near-by listings using geo-spatial search.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Another exciting addition to Solr 4 is that you can index geographical shapes such as polygons, which allows you to find documents that intersect geographical regions.
This might be useful for finding home listings in specific neighborhoods using a precise geographical representation of a neighborhood.
As we discussed in section 1.1, Solr is optimized to work with specific types of data.
In this section, we provide an overview of key features that help you model data for search, including:
Multilingual support FIELD COLLAPSING / GROUPING Although Solr requires a flat, denormalized document, Solr allows you to treat multiple documents as a group based on some common property shared by all documents in a group.
Field grouping, also known as field collapsing, allows you to return unique groups instead of individual documents in the results.
The classic example of field collapsing is threaded email discussions where emails matching a specific query could be grouped under the original email message that started the conversation.
Phrase queries with slop to allow for some distance between terms.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
But in Solr, joins are more like sub-queries in SQL in that you don’t build documents by joining data from other documents.
For example, with Solr joins, you can return child documents of parents that match your search criteria.
One example where Solr joins are useful would be grouping all retweets of a Twitter message into a single group.
This is helpful to avoid returning many documents containing the same information in search results.
For example, if your search engine is based on news articles pulled from multiple RSS feeds, then it’s likely that you’ll have many documents for the same news story.
Rather than returning multiple results for the same story, you can use clustering to pick a single representative story.
With Solr this is easy because it integrates with Apache Tika project that supports most popular document formats.
Solr has language detection built-in and provides language-specific text analysis solutions for many languages.
In general, version 4 is a huge milestone for the Apache Solr community as it addresses many of the major pain-points discovered by real users over the past several years.
We selected a few of the main features to highlight here but we’ll also point out new features in Solr 4 throughout the book.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Easy sharding and replication using Zookeeper NEAR-REAL-TIME SEARCH Solr’s Near-Real-Time (NRT) search feature supports applications that have a high velocity of documents that need to be searchable within seconds of being added to the index.
With NRT, you can use Solr to search rapidly changing content sources such as breaking news and social networks.
For example, if the price of a home in our example real estate application from section 1.2 changes, then we can send an atomic update to Solr to change the price field specifically.
You might be wondering what happens if two different users attempt to change the same document concurrently.
In this case, Solr guards against incompatible updates using optimistic concurrency.
In a nutshell, Solr uses a special version field named _version_ to enforce safe update semantics for documents.
In the case of two different users trying to update the same document concurrently, the user that submits updates last will have a stale version field so their update will fail.
REAL-TIME GET At the beginning of this chapter, we stated that Solr is a NoSQL technology.
Solr’s real-time get feature definitely fits within the NoSQL approach by allowing you to retrieve the latest version of a document using its unique identifier regardless of whether that document has been committed to the index.
This is similar to using a key-value store like Cassandra to retrieve data using a row key.
Prior to Solr 4, a document wasn’t retrievable until it was committed to the Lucene index.
With the real-time get feature in Solr 4, you can safely decouple the need to retrieve a document by its unique ID from the commit process.
This can be useful if you need to update an existing document after it’s sent to Solr without having to do a commit first.
As we’ll learn in chapter 5, commits can be expensive and impact query performance.
Solr’s transaction log sits between the client application and the Lucene index.
It also plays a role in servicing real-time get requests as documents are retrievable by their unique identifier regardless of whether they’re committed to Lucene.
The transaction log allows Solr to decouple update durability from update visibility.
This means that documents can be on durable storage but aren’t visible in search results yet.
This gives your application control over when to commit documents to make them visible in search results without risking data loss if a server fails before you commit.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
With SolrCloud, scaling is simple and automated because Solr uses Apache Zookeeper to distribute configuration and manage shard leaders and replicas.
In Solr, Zookeeper is responsible for assigning shard leaders and replicas and keeps track of which servers are available to service requests.
SolrCloud bundles Zookeeper so you don’t need to do any additional configuration or setup to get started with SolrCloud.
We hope you now have a good sense for what types of data and use cases Solr supports.
As you learned in section 1.1, Solr is optimized to handle data that’s text-centric, readdominant, document-oriented, and has a flexible schema.
We also learned that search engines like Solr aren’t general-purpose data storage and processing solutions but are intended to power keyword search, ranked retrieval, and information discovery.
Using the example of a fictitious real estate search application, we saw how Solr builds upon Lucene to add declarative index configuration and web services based on HTTP, XML, and JSON.
Solr 4 can be scaled in two dimensions to support millions of documents and high-query traffic using sharding and replication.
We also touched on some key reasons why to choose Solr based on the perspective of key stakeholders.
We saw how Solr addresses the concerns of software architects, system administrators, and even the CEO.
Lastly, we covered some of Solr’s main features and gave you pointers where you can learn more about each feature in this book.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
It’s natural to have a sense of uneasiness when you start using an unfamiliar technology.
You can put your mind at ease because Solr is designed to be easy to install and set up.
In the spirit of being agile, you can start out simple and incrementally add complexity to your Solr configuration.
For example, Solr allows you to split a large index into smaller subsets, called shards, as well as add replicas to increase your capacity to serve queries.
But you don’t need to worry about index sharding or replication until you need them.
By the end of this chapter, you’ll have Solr running on your computer, know how to start and stop Solr, know your way around the web-based administration console, and have a basic understanding of key Solr terminology such as solr home, core, and collection.
You may have heard of SolrCloud and wondered what the difference is between Solr 4 and SolrCloud.
Technically, SolrCloud is the code name for a sub-set of features in Solr 4 that makes it easier to configure and run a scalable, fault-tolerant cluster of Solr servers.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Also, SolrCloud doesn’t have anything to do with running Solr in a cloud-computing environment like Amazon EC2, although you can run Solr in a cloud.
We presume that the “cloud” part of the name reflects the underlying goal of the SolrCloud feature set to enable elastic scalability, high-availability, and ease of use we’ve all come to expect from cloud-based services.
Let’s get started by downloading Solr from the Apache website and installing it on your computer.
Before we can get to know Solr, we have to get it running on your local computer.
This starts with downloading the binary distribution of Solr 4.1 from Apache and extracting the downloaded archive.
Once installed, we’ll show you how to start the example Solr server and verify that it’s running by visiting the Solr administration console from your web browser.
Throughout this process, we assume you’re comfortable executing simple commands from the command line of your chosen operating system.
There is no GUI installer for Solr, but you’ll soon see that the process is so simple you won’t need a GUI-driven installer.
Installing Solr is a bit of a misnomer in that all you really need to do is download the binary distribution (.zip or .tgz) and extract it.
To verify you have the correct version of Java, open a command-line on your computer and do:
All client interaction with Solr happens over HTTP so you can use any language that provides an HTTP client library.
In addition, a number of open source client libraries are available for Solr for popular languages like .NET, Python, Ruby, PHP, and of course Java.
Assuming you’ve Java installed, you’re now ready to install Solr.
Apache provides source and binary distributions of Solr; for now, we’ll focus on the installation steps using the binary distribution.
We cover how to build Solr from source in the appendix.
In your browser, go to the Solr home page http://lucene.apache.org/solr and click on the Download button for Apache Solr 4.1 on the right; there is also a button for downloading.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This will direct you to a mirror site for Apache downloads; it’s advisable to download from a mirror site to avoid overloading the main Apache site.
After downloading, move the downloaded file to a permanent location on your computer.
For example, on Windows, you could move it to the C:\ root directory, or on Linux, choose a location like /opt/solr.
For Windows users, we highly recommend that you extract Solr to a directory that doesn’t have spaces in the name, i.e.
Your mileage may vary on this, but being a Java-based software, you're likely to run into issues with paths that contain a space.
There is no formal installer needed because Solr is self-contained in a single archive fileall you need to do is extract it.
When you extract the archive, all files will be created under the solr-4.1.0 directory.
On Windows, you can use the built-in Zip extraction support or a tool like WinZip.
This will create the directory structure shown in figure 2.1
We'll refer to the top-level directory as $SOLR_INSTALL in the rest of this chapter.
We refer to the location where you extracted the Solr archive (.zip or .tgz) as $SOLR_INSTALL in the rest of this chapter.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Solr home will be a different path, so we didn't want to use $SOLR_HOME as the alias for the top-level directory where you extracted Solr.
So now that Solr is installed, you’re ready to start it up.
To start Solr, open a command line and do the following:
Remember that $SOLR_INSTALL is the alias we're using to represent the directory where.
During initialization, you'll see some log messages printed to the console.
If all goes well, you should see the following log message at the bottom:
To be clear, you now have a running version of Solr 4.1 on your computer.
You can verify that Solr started correctly by directing your web browser to the Solr administration page at: http://localhost:8983/solr.
Figure 2.2 provides a screen shot of the Solr administration console; please take a minute to get acquainted with the layout and navigational tools in the console.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Click on the link labeled "collection1" to access more tools such as the query form.
Figure 2.3 illustrates what is now running on your computer.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 2.3 Solr from a systems perspective showing the Solr web application (solr.war) running in Jetty on top of Java.
There is one Solr home directory set per Jetty server using Java system property solr.solr.home.
Solr can host multiple cores per server and each core has a separate directory containing core-specific configuration and index (data) under Solr home, e.g.
The most common issue if the server doesn't start correctly is the default port 8983 is already in use by another process.
This is easy to resolve by changing the port Solr binds to by changing your start command to specify a different port for Jetty to bind to using: java Djetty.port=8080 -jar start.jar.
We recommend just staying with Jetty when first learning Solr.
If your organization already uses Tomcat or some other Java web application server, such as Resin, then you can deploy the Solr WAR file.
Since we're just getting to know Solr in this chapter, we'll refer you to the Appendix B to learn how to deploy the Solr WAR.
Solr uses Jetty to make the initial setup and configuration process a no-brainer.
However, this doesn’t mean that Jetty is a bad choice for production deployment.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
However, if you have some choice, then we recommend you try out Jetty.
It's fast, stable, mature, and easy to administer and customize.
In fact, Google uses Jetty for their AppEngine, see http://www.infoq.com/news/2009/08/google-chose-jetty/, which gives great credibility to Jetty as a solid platform for running Solr in even the most demanding environments!
Now we have a running server, let's take a minute to understand where Solr gets its configuration information and where it manages its Lucene index.
Understanding how the example server you just started is configured will help you when you're ready to start configuring a Solr server for your application.
In Solr, a “core” is composed of a set of configuration files, Lucene index files, and Solr's transaction log.
One Solr server running in Jetty can host multiple cores.
Recall in chapter 1, we designed a real estate search application that had multiple cores, one for houses and a separate core for land listings.
We used two separate cores because the indexed data was different enough to justify having two different index structures.
As a brief aside, Solr also uses the term "collection", which really only has meaning in the context of a Solr cluster where a single index is distributed across multiple servers.
Consequently, we feel it's easier to focus on understanding what a Solr core is for now.
We'll return to the distinction between core and collection in chapter 13 when we discuss SolrCloud.
Solr home is a directory structure that encapsulates one or more cores, which are configured by solr.xml.
Solr also provides a core admin API that allows you to create, update, and delete cores programmatically from your application.
Behind the scenes the core admin API makes changes to the solr.xml configuration file.
Listing 2.1 shows the default solr.xml for the example server.
Out of the box, you don’t need to make any changes to this file.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Each Solr process has one and only one solr home directory, which is set by a global Java system property: solr.solr.home.
Figure 2.4 shows a directory listing of the default Solr home "solr" for the example server.
Figure 2.4 Directory listing of the default Solr home directory for the Solr examples.
It contains a single core named "collection1," which is configured in solr.xml.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
For now, just take a moment to scan figure 2.3 so that you have a sense for the basic structure.
The example directory contains two other solr home directories for exploring advanced functionality.
Also, the example/multicore directory provides an example of a multi-core configuration.
We'll learn more about these features later in the book.
For now, let's continue with the simple example by adding some documents to the index, which you'll need to work through the examples in section 2.2 below.
When you first start Solr, there are no documents in the index.
It's just an empty server waiting to be filled with data to search.
For now, we'll gloss over the details in order to get some example data in Solr index so that we can try out some queries.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 2.5 shows what you should see after executing the find all documents query.
Figure 2.5 Screenshot of the Query form on the Solr administration console.
You can verify that the example documents were indexed correctly by executing the find all documents query *:*
At this point, we have a running Solr instance with some example documents loaded.
Without a doubt, Solr's main strength is powerful query processing.
Think about it this way, who cares how scalable or fast a search engine is if the results it returns aren't useful or accurate? In this section, you'll see Solr query processing in action, which we think will help you see why Solr is such a powerful search technology.
Throughout this section, pay close attention to the link between each query we execute and the documents that Solr returns, especially the order of the documents in the results.
This will help you start to think like a search engine, which will come in handy in chapter 3 when we cover core search concepts.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You've already used Solr’s query form to execute the "find all documents" query (*:*)
Let's take a quick tour of the other features on this form so that you get a sense for the types of queries Solr supports.
Figure 2.6 provides some annotations of key sections of this form.
Take a minute to read through each annotation in the diagram.
Figure 2.6 Annotated screen shot of Solr's query form to illustrate the main features of Solr query processing, such as filters, results format, sorting, paging, and search components.
Take a moment to fill-out the form and execute the query in your own environment.
Do the two documents that Solr returned make sense? Table 2.1 provides an overview of the form fields we're using for this example.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In this example, we filter results that have manufacturer field "manu" equal to "Belkin"
Start should be incremented by the page size to advance to the next page.
List of fields to return for each document in the result set.
The "score" field is a built-in field that holds each document's relevancy score for the query.
You have to request the score field explicitly as is done in this example.
When you fill out the query form, an HTTP GET request is created and sent to Solr.
The form field names shown in table 2.1 correspond to parameters passed to Solr in the HTTP GET request.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
But, if you don't want to wait that long and want to see more queries in action, then we recommend looking at the Solr tutorial provided with Solr.
Lastly, we probably don’t have to tell you that this form isn't designed for end users; the Solr team built the query form so that developers and administrators have a way to send queries without having to formulate HTTP requests manually or develop a client application just to send a query to Solr.
But, let's be clear that with Solr, you’re responsible for developing the user interface to Solr.
As we'll see in section 2.2.5 below, Solr provides an example search UI, called Solritas, to help you get started building your own awesome search application.
We've seen what gets sent to Solr, so now let's learn about what comes back from Solr in the results.
The key point in this section is that Solr returns documents that match the query as well as additional information that can be processed by your Solr client to deliver a quality search experience.
The operative phrase being "by your Solr client"! Solr returns the raw materials that you need to translate into a quality search experience for your users.
As you can see, the results are in XML format and are sorted by lowest to highest price.
Paging doesn't really come into play with this result set because there are only two results total.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
So far, we've only seen results returned as XML, but Solr also supports other formats such as CSV, JSON, and language specific formats for popular languages.
For instance, Solr can return a Python specific format that allows the response to be safely parsed into a Python object tree using the eval function.
As we touched upon on chapter 1, the key differentiator between Solr's query processing and that of a database or other NoSQL data store is ranked retrieval—the process of sorting documents by their relevance to a query, where the most relevant documents are listed first.
Let's see ranked retrieval at work with some of the example documents you indexed in section 2.1.4
This should return three documents sorted in descending order by score.
Take a moment to scan the results and decide if you agree with the ranking for this simple query.
Intuitively, the ordering makes sense because the query term "iPod" occurs three times in the first document listed, twice in the name and once in the features; it only occurs once in.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The actual numeric value of the score field isn’t as important as it's used internally by Lucene to do the ranking.
The key take-away is that every document that matches a query is assigned a relevance score for that specific query and results are returned in descending order by score; the higher the score, the more relevant the document is to the query.
Next, change your query to be "iPod power" and you'll see that the same three documents are returned and are in the same order.
This is because all three documents contain both query terms in either their name or features field.
This makes sense because "power" occurs twice in the second document so its relevance to the "iPod power" query is much higher than its relevance to the "iPod" query.
Again, the same three documents are returned but in a different order.
Now the top document in the results is "Belkin Mobile Power Cord for iPod w/ Dock" because it contains the term "power" in the name and features field and we told Solr that "power" is twice as important as "iPod" for this query.
Now you have a taste of what ranked retrieval looks like.
Let's move on and see some other features of query processing, starting with how to work with queries that return more than three documents using paging and sorting.
Our example Solr index only contains 32 documents, but a production Solr instance typically has millions of documents.
You can imagine that in a Solr instance for an electronics superstore, a query for "iPod" would probably match thousands of products and accessories.
To ensure results are returned quickly, especially on bandwidth constrained mobile devices, you don't want to return thousands of results at once, even if the most relevant are listed first.
Paging is a first-class concept in Solr query processing in that every query includes parameters that control the page size (rows) and starting position (start)
By default, Solr uses a page size of 10, but you can control that using the "rows" parameter in the query request.
To request the "next" page in the results, you increment the start parameter by the page size.
For example, if you're on the first page of results (start=0), then to get the next page, you increment start parameter by the page size, i.e.
It's important to use as small a page size as possible to satisfy your requirements because the underlying Lucene index isn’t optimized for returning many documents at once.
Rather, Lucene is optimized for query processing so the underlying data structures are.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Once the search results are identified, Solr must re-construct each document, in most cases by reading data off disk.
It uses intelligent caching to be as efficient as possible, but in comparison to query processing, results construction is a slow process, especially for large page sizes.
Consequently, you’ll get much better performance from Solr using small page sizes.
However, you can request Solr to sort results by other fields in your documents.
You've already seen an example of this in section 2.2.1 where we sorted results by the price field in ascending order, which produces the lowest priced products at the top.
Sorting and paging go hand-in-hand because the sort order determines the page position for results.
To help get you thinking about sorting and paging, consider the question of whether Solr will return deterministic results when paging without specifying a sort order? On the surface, this seems obvious because the results are descending sorted by score if you don't specify a sort parameter.
But, what if all documents in a query have the same score? For example, if your query is "inStock:true" then all matching documents will have the same score; be sure to verify this yourself using the Query form.
It turns out that Solr will indeed return all documents when you page through the results even though the score is the same.
This works because Solr finds all documents that match a query and then applies the sorting and paging offsets to the entire set of documents.
In other words, Solr keeps track of the entire set of documents that match a query independently of the sorting and paging offsets.
The query form contains a list of check boxes that enable advanced functionality during query processing; in Solr speak these additional features are called "search components"
As shown in figure 2.6, the form contains check boxes that reveal additional form fields to activate the following search components:
If you click on any of these checkboxes, you'll see that it’s not clear what to do when you look at the form.
That’s because using these search components from the query form requires some additional knowledge that we can't cover quickly in this getting started chapter.
Rest assured that we cover each of these components in-depth later in the book.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
For now, though, we can see some of these search components in action using Solr's example search interface, called "Solritas", available in your local Solr instance at: http://localhost:8983/solr/collection1/browse.
Navigate to this URL in your web browser and you’ll see a screen that looks like figure 2.8
Figure 2.8 The Solritas Simple example, which illustrates how to use various search components, such as faceting, More Like This, hit highlighting, and spatial, to provide a rich search experience for your users.
As shown at the top of figure 2.8, Solr provides three examples to choose from: Simple, Spatial, and Group By.
We'll briefly cover the key aspects of the Simple example here and encourage you to explore the other two examples on your own.
Take a moment to scan over figure 2.8 to identify the various search components at work.
One of the more interesting search components in this example is the facet search component shown on the left side of the page, starting with the header Field Facets.
The facet component categorizes field values in the search results into useful sub-sets to help the user refine their query and discover new information.
For instance, when we search for "video", Solr returns three example documents and the faceting component categorizes the.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Click on the music facet link and you'll see the results are filtered from three documents down to only one.
The idea here is that in addition to search results, you can help users refine their search criteria by categorizing the results into different filters.
Take a few minutes to explore the various facets shown in the example.
Next, let's take a look at another search component that isn’t immediately obvious from figure 2.8, namely the spell check component.
To see how spell checking works, type "vydeoh" in the search box instead of "video"
Of course no results are found, as shown in figure 2.9, but Solr does return a link that effectively asks the user if they meant "video" and if so, they can re-search using the link.
Figure 2.9 Example of how the spell check component allows your search UI to prompt the user to research using the correct spelling of a query term, in this case, Solr found "video" as the closest match for "vydeoh"
There's a lot of powerful functionality packed into the three Solritas examples and we encourage you to spend a little time with each.
For now, let's move on and tour the rest of the administration console.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
At this point, you should have a good feel for the query form, so let's take a quick tour of the rest of the administration console shown in figure 2.10
Figure 2.10 The Solr administration console; explore each page using the toolbar on the left.
Rather than spending your time reading about the administration panel, we think it's better to just start clicking through some of the pages yourself.
Thus, we leave it as an exercise for you to visit all the links in the administration console to get a sense for what is available on each page.
To get you started, here are some highlights of what the administration console provides:
Get a dump of all active threads in your JVM from Thread Dump.
In addition to the main pages described above, there are a number of core specific pages for each core in your server.
Recall that the example server we've been working with has only one core named "collection1"
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
View core specific properties such as the number of Lucene segments from the main core page, e.g.
Send a quick request to a core to make sure it’s alive using Ping.
See how your index is replicated to other servers from Replication.
Analyze text from Analysis; you’ll learn all about text analysis in chapter 6, including how to use the Analysis form.
Determine how fields in your documents are analyzed from Schema Browser.
Get information about the top terms for a field using Load Term Info on the Schema Browser.
View statistics about core Solr cache regions, such as how many documents are in the documentCache from PlugIns / Stats.
Manage the data import handler from Dataimport; this isn’t enabled in the example server.
We'll dig into the details for most of these pages in various places throughout the book, when it's more appropriate.
For instance, you'll learn all about the Analysis page in chapter 6 when we cover text analysis.
For now, take a few moments to explore these pages on your own.
To give your self-guided tour some direction, see if you can answer the following questions about your Solr server.
What’s the value of the lucene-spec version property for your Solr server?
What’s the value of the maxDoc property for the collection1 core?
What’s the top term for the manu field? (hint: select the "manu" field in the schema browser and click on the Load Term Info button)
What’s the current size of your documentCache? (hint: think stats)
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
What’s the analyzed value of the name "Belkin Mobile Power Cord for iPod w/ Dock"? (hint: select the name field on the Analyzed page)
Let's now turn our attention to what needs to be done to start customizing Solr for your specific needs.
So now that you've had a chance to work with the example server, you might be.
First, you could just use the example directory as-is and start making changes to it to meet your needs.
However, we think it's better to keep a copy of example around and make your application specific changes in a clone of example.
This allows you to refer back to example in case you break something when working on your own application.
If you choose the latter approach, then you need to select a name for the directory that is more appropriate for your application than "example"
For example, if we were building the real estate search application described in chapter 1, then we might name the directory: realestate.
Once you've settled on a name, do the following steps to create a clone of the example directory in Solr:
Update solr.xml to point to the name of your new collection by replacing "collection1"
Note that you don't need to make any changes to the Solr configuration files, such.
These files are designed to provide a good experience out-of-the-box and let you adapt them to your needs iteratively without having to swallow such a big pill at once.
There may come a time when you want to start with a fresh index.
When you restart Solr, you’ll have a fresh index with 0 documents.
Restart Solr from your new directory using the same process from section 2.1.2
For example, to restart our clone for the realestate application, we’d do:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Lastly, you might be wondering about setting JVM options, configuring backups, monitoring, setting Solr up as a service, and so on? We feel these are important concerns when you’re ready to go to production so we cover these questions in chapter 12, when we discuss taking Solr to production.
To recap, we started by installing Solr 4.1 from the binary distribution Apache provided.
In reality, the installation process was only a matter of choosing an appropriate directory where to extract the downloaded archive (.zip or .tgz) and then doing the extraction.
Next, we started the example Solr server and added some example documents using the post.jar command-line application.
After adding documents, we introduced you to Solr's query form, where you learned the basic components of a Solr query.
Specifically, you learned how to construct queries containing a main query parameter “q” as well as an optional filter “fq.” You saw how to control which fields are returned using the “fl” parameter and how to control the ordering of results using “sort.” We also touched on ranked retrieval concepts where results are ordered by relevancy score.
We also introduced you to search components and provided some insights into how they work in Solr using the Solritas example user interface.
Specifically, you saw an example of how the facet component allows users to refine their search criteria using dynamically generated filters called facets.
We also touched on how the spell check component allows you to prompt users “did you mean X? ” when their query contains a misspelled term.
Next, we gave you some tips on what other tools are available in the Solr administration console.
You’ll find many great tools and statistics available about Solr, so we hope you were able to answer the questions we posed as you walked through the administration console in your browser.
Lastly, we presented the steps to clone the example directory to begin customizing it for your own application.
We think this is a good way to start so that you always have a working example to refer to as you customize Solr for your specific needs.
So now that you have a running Solr instance, it’s time to learn about core Solr concepts.
In chapter 3 you'll gain a better understanding of core search concepts that will help you throughout the rest of your Solr journey.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
How Solr performs complex queries using terms, phrases, and fuzzy matching.
How Solr calculates scores for matching queries to most relevant documents.
How to balance returning relevant results versus ensuring they’re all returned.
How search scales across servers to handle billions of documents and queries.
Now that we have Solr up and running, it’s important to gain a basic understanding of how a search engine operates and why you’d choose to use Solr to store and retrieve your content as opposed to a traditional database.
In this chapter, we’ll provide a solid understanding of how a search engine stores documents in its internal index, how it calculates a relevancy score to ensure only the “best” results are returned for display, and how Solr is able to scale to handle billions of documents as it still maintains lightning-fast search response times.
Our main goal for this chapter is to provide you with the theoretical underpinnings necessary to understand and maximize your use of Solr.
If you have a solid background in search technology or information retrieval already, then you may wish to skip some or all of this chapter, but if not, it will help you understand more advanced topics later in this book and maximize the quality of your users’ search experience.
Although the content in this chapter is generally applicable to most search engines, we’ll be specifically focusing upon Solr’s implementation of each of the concepts.
By the end of this chapter, you should have a solid understanding of how Solr’s internal index works, how to perform complex Boolean and fuzzy queries with Solr, how Solr’s default relevancy scoring model works, and how Solr’s.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Let’s begin with a discussion of the core concepts behind search in Solr, including how the search index works, how a search engine matches queries and documents, and how Solr enables powerful query capabilities to make finding content a problem of the past.
Search engines, and Solr in particular, help to solve a particular class of problem very well – those requiring the ability to search across large amounts of unstructured text and pull back the most relevant results.
In this section, we’ll describe the core features of a modern search engine, including a explanation of a search “document”, an overview of the inverted search index at the core of Solr’s fast full-text searching capabilities, and a broad overview of how this inverted search index enables arbitrarily complex term, phrase, and partial matching queries.
It is important, however, that we have a solid understanding of the kind of information which we can put into Solr to be searched upon (a document), and how that information is structured.
Every piece of data submitted to Solr for processing is a document.
A document could be a newspaper article, a resume or social profile, or in an extreme case even an entire book.
Each document contains one or more fields, each of which is modeled as a particular field type: string, tokenized text, boolean, date-time, lat/long, etc.
The number of potential field types is infinite because a field type is composed of zero or more analysis steps that change how the data in the field is processed and mapped into the Solr index.
Each field is defined in Solr’s schema (discussed in chapter 5) as a particular field type, which allows Solr to know how to handle the content as it is received.
Listing 3.1 shows an example document, defining the values for each field.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
When we run a search against Solr, we can search on one or more of these fields (or even fields not contained in this particular document), and Solr will return documents that contain content in those fields matching the query specified in the search.
It is worth noting that, unlike Lucene, Solr is not schema-less.
This does not mean that every document must contain every field, only that all possible fields must be mapped to a particular field type should they appear in a document and need to be processed.
A document, then, is a collection of fields that map to particular field types defined in a schema.
Each field in a document is analyzed according to its field type and stored in a search index in order to later retrieve the document by sending in a related query.
The primary search results returned from a Solr query are documents containing one or more fields.
Before we dive into an overview of how search works in Solr, it is helpful to understand what fundamental problem search engines are solving.
Let’s say, for example, you were tasked with creating some search functionality that helps users search for books.
Figure 3.1 Example search interface, as would be seen on a typical website, demonstrating how a user would submit a query to your application.
Now, imagine a customer wants to find a book on purchasing a new home and searches for “buying a home.” Some potentially relevant books titles you may want to return for this search are listed in table 3.1
Table 3.1 Books relevant to the query “buying a home”
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
All other book titles, as listed in table 3.2, would not be considered relevant for customers interested in purchasing a new home.
Table 3.2 Books not relevant to the query “buying a home”
In addition, customers will only ever see results for future queries if the query matches the full book title exactly.
Perhaps a more forgiving approach would be to search for each single word within a customer’s query:
Table 3.3 Results from database “like” query requiring a fuzzy match for every term.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Of course, you may believe that requiring documents to match all of the words your customers include in their queries is overly restrictive.
You could easily make the search experience more flexible by only requiring a single word to exist in any matching book titles, by issuing the following SQL query:
Additionally, because this query is performing only partial string matching on each keyword, any book title that merely contains the letter “a” is also returned.
The preceding example, which required all of the terms, also matched on the letter “a”, but we did not experience this problem of returning too many results because the other keywords were more restrictive.
Table 3.4 Results from database “like” query only requiring a fuzzy match at least one term.
We have just seen that the first query (requiring all words to match) resulted in many.
It does not understand linguistic variations of words such as “buying” vs “buy”
It does not understand synonyms of words such as “buying” and “purchasing”
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
There is no sense of relevancy ordering in the results – books which only match one of the queried words often show up higher than books matching multiple or all of the words in the customer’s query.
These queries will become slow as the size of the book catalog grows or the number of customer queries grows, as the query must scan through every book’s title to find partial matches instead of using an index to look up the words.
Search engines like Solr really shine in solving problems like the ones listed above.
Solr is able to perform text analysis on content and on search queries to determine textually similar words, understand and match on synonyms, remove unimportant words like “a”, “the”, and “of”, and score each result based upon how well it matches the incoming query to ensure that the best results are returned first and that your customers do not have to page through countless less relevant results to find the content they were expecting.
Solr accomplishes all of this utilizing an index that maps content to documents instead of mapping documents to content like a traditional database model.
This “inverted index” is at the heart of how search engines work.
Solr utilizes Lucene’s inverted index to power its fast searching capabilities, as well as.
While we’ll not get into many of the internal Lucene data structures in this book (I recommend picking up a copy of Lucene in Action if you want a deeper dive), it is important to understand the high-level structure of the inverted index.
Recalling our previous book-searching example, we can get a feel for what an index mapping each term to each document would look like from table 3.5
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Table 3.5  Mapping of text from multiple documents into an inverted index.
The left table contains nine original documents with their content, while the right table represents an inverted search index containing each of the terms from the original content mapped back to the documents in which they can be found.
Table 3.5 demonstrates the process of mapping the content from documents into an.
While a traditional database representation of multiple documents would contain a document’s id mapped to one or more content fields containing all of the words/terms in that document, an inverted index inverts this model and maps each.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You can tell from looking at table 3.5 that the original input text was split on spaces and that each term was transformed into lowercase text before being inserted into the inverted index, but everything else remained the same.
A few final important details should be noted about the inverted index:
All terms in the index map to one or more documents.
Terms in the inverted index are sorted in ascending alphanumeric order.
This view of the inverted index is greatly simplified – we’ll see in section 3.1.6 that additional information can also be stored in the index to improve Solr’s querying and scoring capabilities.
As you will see in the next section, the structure of Lucene’s inverted index allows many powerful query capabilities that maximize both the speed and flexibility of keyword based searching.
Now that we’ve seen what content looks like in Lucene’s inverted index, let’s jump into.
In this section, we’ll go over the basics of looking up terms and phrases in this inverted search index and utilizing Boolean logic and fuzzy queries to enhance these lookup capabilities.
Referring back to our book-searching example, let’s take a look at a simple query of “new house”, as portrayed in figure 3.2
Figure 3.2 Simple search to demonstrate nuances of query interpretation.
We saw in the last section that all of the text in our content field was broken up into individual terms when inserted into the Lucene index.
Now that we have an incoming query, we need to select from among several options for querying the index:
Search for two different terms, “new” and “house”, requiring both to match.
Search for two different terms, “new” and “house”, requiring only one to match.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
All of these options are perfectly valid approaches, depending upon your use case, and thanks to Solr’s powerful querying capabilities built using Lucene, are very easy to accomplish using boolean logic.
There are two identical ways to write this query using the standard query parser in Solr:
These two are logically identical, and in fact, the second example gets parsed and ultimately reduced down to the first example.
The “+” symbol is a unary operator which means that part of the query immediately following it is required to exist in any documents matched, whereas the “AND” keyword is a binary operator which means that the part of the query immediately preceding and the part of the query immediately following it are both required.
By default, Solr is also configured to treat any part of the query without an explicit operator as an optional parameter, making the following identical:
While the default configuration in Solr assumes that a term or phrase by itself is an optional term, this is configurable on a per-query basis using the q.op url parameter with many of Solr’s query handlers.
Do note, however, that if you change the default operator from OR to AND that now parts of the query without an operator will always be required unless you explicitly place an OR between them to override the default AND operator.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In the queries above, no document which contains the word “rental” will be returned, only documents matching “new” or “house.”
The Solr query syntax can represent arbitrarily complex queries through grouping terms together using parenthesis like the following examples:
The use of required terms, optional terms, negated terms, and grouped expressions provides a very powerful and flexible set of query capabilities that allow arbitrarily complex lookup operations against the search index, as we’ll see in the following section.
With a basic understanding of terms, phrases, and Boolean queries in place, we can now dive into exactly how Solr is able to utilize the internal Lucene inverted index to find matching documents.
Let us recall our index of books from earlier, re-produced in table 3.6
Table 3.6  Inverted index of terms from a collection of book titles.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
If a customer passes in a query now of new home, how exactly is Solr able to find.
As such both terms must be looked up separately in the Lucene Index:
Assuming our default operator is an OR, this query would result in a union of the result sets for both terms, as pictured in the Venn diagram in figure 3.3
Figure 3.3  Results returned from an Union query using the “OR” operator.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 3.4  Results returned from an Intersection query using the “AND” operator.
In addition to union and intersection queries, negating particular terms is also common.
Figure 3.5 demonstrates a breakdown of the results expected for each of the result set permutations of this two-term search query (assuming a default OR operator)
Figure 3.5  Graphical representation of using common boolean query operators.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
As you can see, the ability to search for required terms, optional terms, negated terms, and grouped terms provides a very powerful mechanism for looking up single keywords.
As we’ll see in the following section, Solr also provides the ability to query for multi-term phrases.
We saw earlier that, in addition to querying for terms in our Lucene Index, that it is also.
Recalling that the index contains only individual terms, however, you may be wondering exactly how we can search for full phrases.
The short answer is that each term in a phrase query is still looked up in the Lucene Index individually, just as if the query new home had been submitted instead of “new home.” Once the overlapping document set is found, however, a feature of the index that we conveniently left out of our initial inverted index discussion is utilized.
This feature, called Term Positions, is the optional recording of the relative position of terms within a document.
Table 3.7 demonstrates how documents (on the left side of the table) map into an inverted index containing Term Positions (on the right side of the table)
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The term position actually goes one step further, telling us where in the document each term appears.
Table 3.8 shows a condensed version of the inverted index focused upon only the primary terms under discussion: new and home.
This makes sense, as the original book titles for these books were “Buying a New Home” and “Becoming a new Home Owner.” We have thus seen the power of term positions – they allow us to reconstruct the original positions of our indexed terms within their respective documents, making it possible to search for specific phrases at query time.
Searching for specific phrases is not the only benefit provided by term positions, though.
We’ll see in the next section another great example of their use to improve our search results quality.
It’s not always possible to know up-front exactly what will be found in the Solr index for any given search, so Solr provides the ability to perform several types of fuzzy matching queries.
Fuzzy matching is defined as the ability to perform inexact matches on terms in the search index.
For example, someone may want to search for any words that start with a particular prefix (known as wildcard searching), may want to find spelling variations within one or two characters (known as fuzzy or edit distance searching), or may even want to match two terms within some maximum distance of each other (known as proximity searching)
For use cases in which multiple variations of the terms or phrases queried may exist across the documents being searched, these fuzzy matching capabilities serve as a very powerful tool.
In this section, we’ll explore multiple fuzzy matching query capabilities in Solr, including wildcard searching, range searching, edit distance searching, and proximity searching.
One way to do this would be to create a query which enumerates all of the possible variations:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Requiring that this list of words be turned into a query up-front can be an unreasonable expectation for customers, or even for you on behalf of your customers.
Since all of the variations you could match already exist in the Solr index, you can use the asterisk (*) wildcard character to perform this same function for you:
In addition to matching the end of a term, a wildcard character can be used inside of the.
The asterisk wildcard (*) shown above matches zero or more characters in a term.
While the wild card functionality in Solr is fairly robust, it is only possible, by default, to use wildcards inside or at the end of a term.
If you needed to match all terms ending in “ing” (like caring, liking, and smiling), for example, you would receive an exception if you tried running this search:
The reason for this is that Solr searches through the inverted index for terms which begin with the characters provided before the wildcard, and without some initial characters to begin that lookup, it is too expensive to walk the entire index to find matching terms.
If you really need to be able to search using these leading wildcards, a solution to this problem does exist, but it will require you to perform some additional configuration.
Do note, however, that turning this feature on requires dual-storing all terms in the Solr index, increasing the size of the index and thus slowing overall searches down somewhat.
Turning this capability on is thus not recommended unless it is needed within your search application.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
One last important point to note about wildcard searching is that wildcards are only meant to work on individual search terms, not on phrase searches, as demonstrated by the following example:
This can be useful when you want to search for a particular subset of documents falling within a range.
Each of the above range queries surrounds the range with square brackets, which is the.
Solr also supports exclusive range searching through use of curly braces:
It is important to note that the ordering of terms of range queries is exactly that – the order in which they are found in the Solr index, which is an alphanumeric sorted order.
Numeric types in Solr, at least the ones we’ll recommend in the coming chapters, compensate for this by indexing the incoming content in a special way, but it is important to understand that the sort order within the Solr index is dependent upon how the data within the field is processed when it is written to the Solr index.
Solr provides the ability to handle character variations using edit distance.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Solr achieves these fuzzy edit distance searches through the use of the tilde (~) character as follows:
The above query matches both the original term (administrator) and any other terms.
An edit distance here is defined as an insertion, a deletion, a substitution, or a transposition of characters.
Likewise the term sadministrator is one edit distance away because it has one insertion (the “s” that was added to the beginning), and the term administratro is one edit distance away because it has transposed the last two characters (“or” became “ro”)
It is also possible to modify the strictness of edit distance searches to allow matching of terms with an edit distance of greater than one:
This edit distance principle is applicable beyond just searching for alternate characters within a term – it can also be applied between terms for variations of phrases.
Let’s say that that you want to search across a Solr index of employee profiles for executives within your company.
One way to do this would be to enumerate each of the possible executive titles within your company:
Query: “chief executive officer” OR “chief financial officer” OR “chief marketing officer” OR “chief technology officer” OR …
Of course, this assumes you know all of the possible titles, which may be unrealistic if you’re searching across other companies with which you’re poorly acquainted, or if you have a more challenging use case.
Another possible strategy is to search for each term independently:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Query: chief AND officer This should match all of the possible use cases, but it will also match any document which.
One problematic example would be a document containing the text “One chief concern arising from the incident was the safety of the police officer on duty.” This document is clearly a poor match for our use case, but it and similar bad matches would be returned given the above query.
Thankfully, Solr provides a simple solution to this problem: proximity searching.
In the above example, a good strategy would be to ask Solr to bring back all documents which contain the term “chief” near officer “officer”
This can be accomplished through the following example queries: Query:  “chief officer”~1
Query:  “chief officer”~2 Examples: “chief business development officer”, “officer, police chief”
Query:  “chief officer”~N Finds chief within N edit distances of officer.
The edit distances above can be seen as nothing more than sloppy phrase searches.
In fact, an exact phrase search of “chief development officer” could easily be rewritten as “chief development officer”~0
These queries will yield the exact same results, because an edit distance of zero is the very definition of an exact phrase search.
Both mechanisms make use of the term positions stored in the Solr index (which we discussed in section 3.1.6) to calculate the edit distances.
At this point, you should have a basic grasp of how Solr stores information in its inverted index and queries that index to find matching documents.
This includes looking up terms, using Boolean logic to create arbitrarily complex queries, and getting results back as a result of the set intersections of each of the term lookups.
We also discussed how Solr stores term positions and is able to use those to find exact phrases and even fuzzy phrase matches through the use of proximity queries and edit distance calculations.
For fuzzy searching within single terms, we examined the use of wildcards and edit distance searching to find misspellings or very similar words.
While Solr’s query capabilities will be expanded upon in chapter 6, these key operations serve as the foundation for generating most Solr queries.
They also prepare us nicely with the needed background for our discussion of Solr’s keyword relevancy scoring model, which we’ll discuss in the next section.
Finding matching documents is the first critical step in creating a great search experience, but it is only the first step.
No customer is willing to wade through page after page of search results to find the document he or she is seeking.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Solr does a very good job out of the box at ensuring the ordering of search results brings back the best matches at the top of the results list.
It does this by calculating a relevancy score for each document and then sorting the search results from the top score to the lowest.
This section will provide an overview of how these relevancy scores are calculated and what factors influence them.
We’ll dig into both the theory behind Solr’s default relevancy calculation and also into the specific calculations used to calculate the relevancy scores, providing intuitive examples along the way to ensure you leave this section with a solid understanding of what, to many, can be the most eluding aspect of working with Solr.
We’ll start by discussing the Similarity class, which is responsible for most aspects of a query’s relevancy score calculation.
Solr’s relevancy scores are based upon a Similarity class which can be defined on a per-field basis in Solr’s Schema.xml (discussed later in chapter 5)
The Similarity class is a Java class that defines how a relevancy score is calculated based upon the results of a query.
While you can choose from multiple similarity classes, or even write your own, it is important to understand Solr’ default Similarity implementation and the theory behind why it works so well.
First, it makes use of a Boolean model (described in the last section) to filter out any documents that do not match the customer’s query.
Then, it utilizes a Vector Space model for scoring, drawing the query as a vector, as well as an additional vector for each document.
The similarity score for each document is based upon the cosine between the query vector and that document’s vector, as depicted in figure 3.6
The smaller the angle between the query term vector and a document term vector, the more similar the query and the document are considered to be.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In this Vector Space scoring model a term vector is calculated for each document and is compared with the corresponding term vector for the query.
The similarity of two vectors can be found by calculating a cosine between them: with a cosine of 1 being a perfect match and a cosine of zero representing no similarity.
More intuitively, the closer the two vectors appear together, as in the image above, the more similar they are.
Thus, the smaller the angle between vectors, or the larger the cosine, the closer the match.
Of course, the most challenging part of this whole process is actually coming up with reasonable vectors which represent the important features of the query and of each document for comparison.
Let’s take a look at the entire, complicated relevancy formula for the DefaultSimilarity class.
We’ll then go line by line to explain intuitively what each component of the relevancy formula is attempting to accomplish.
Given a query (q) and a document (d), the similarity score for the document to the query can be calculated as shown in figure 3.7
Each component in this formula will be explained in detail in the following sections.
Wow – that equation can be quite overwhelming, especially at first glance.
Fortunately, it is much more intuitive when each of the pieces are broken down.
The math is presented above for reference, but you will likely never need to really dig into the full equation unless you decide to overwrite the similarity for your search application.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
At a high level, the important concepts are demonstrated by the high-level formula namely, Term Frequency (tf), Inverse Document Frequency (idf), Term Boosts (t.getBoost), the Field Normalization (norm), the Coordination Factor (coord), and the Query Normalization (queryNorm)
If you were searching through a search index filled with newspaper articles for an article on the President of the United States, would you prefer to find articles which only mention the president once, or articles which discuss the president consistently throughout the article? What if an article just happens to contain the phrases “President” and “United States” each one time (perhaps out of context) – should it be considered as relevant as an article that contains these phrases multiple times?
Clearly the second article above is more relevant than the first, and the identification of the phrases “President” and “United States” multiple times throughout the article provides a strong indication that the content of the second article is more closely related to this query.
Furman University, one of the top liberal arts universities in the southern United States.
Furman also consistently ranks among the most beautiful campuses to visit and ranks among the top 50 liberal arts colleges nation-wide each year.
Today, international leaders met with the President of the United States to discuss options for dealing with growing instability in global financial markets.
President Obama indicated that the United States is cautiously optimistic about the potential for significant improvements in several struggling world economies pending the results of upcoming elections.
The President indicated that the United States will take whatever actions necessary to promote continued stability in the global financial markets.
In general, a document is considered to be more relevant for a particular topic (or query term) if the topic appears multiple times.
This is the basic premise behind the TF (Term Frequency) component of the default Solr relevancy formula.
The more times the search term appears within a document, the more relevant that document is considered.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Imagine if someone were to search for the book The Cat in the Hat by Dr.
Seuss, and the top results returned were those which included a high term frequency for the words “the” and “in” instead of “cat” and “hat”
Common sense would indicate that words which are more rare across all documents are likely to be better matches for a query than terms which are more common.
Inverse Document Frequency (IDF) is a measure of how “rare” a search term is, and it is calculated by finding the document frequency (how many total documents the search term appears within), and calculating it’s inverse (see the full formula in section 3.2.1 for the actual calculation)
Because the Inverse Document Frequency appears for the term in both the query and the document, it is squared in the relevancy formula.
Figure 3.8 shows an visual example of the “rare-ness” of each of the words in the title “The Cat in the Hat,” with a higher IDF being represented as a larger term.
Figure 3.8  Visual depiction of the relative significance of terms as measured by Inverse Document Frequency.
The terms which are more rare are depicted as larger, indicating a larger Inverse Document Frequency.
Likewise, if someone were searching for a profile for “an experienced Solr development team lead,” we wouldn’t expect documents to rank higher which best match the words “an”, “team”, or “experienced”
Instead, we would expect the important terms to resemble the largest terms in figure 3.9
Figure 3.9 Another demonstration of relative score of terms derived from Inverse Document Frequency.
Once again, a higher Inverse Document Frequency indicates a more rare and more relevant term, which is depicted here using larger text.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Clearly the user is looking for someone who knows Solr who can be a team lead, so these terms stand out with considerably more weight when found in any document.
Term Frequency and Inverse Document Frequency, when multiplied together in the relevancy calculation, provide a nice counter-balance to each other.
The term frequency elevates terms which appear multiple times within a document, while the inverse document frequency penalizes those terms which appear commonly across many documents.
Thus common words such as “the”, “an”, and “of” in the English language ultimately yield very low scores, even though they may appear many times in any given document.
Query-time boosting is the most flexible and easiest to understand form of boosting, utilizing the following syntax:
Query boosts can also be used to “penalize” certain terms if a boost of less than 1.0 is used:
In addition to query time boosting, it is also possible to boost documents or fields within.
These boosts are factored into the Field Norm, which is covered in the following section.
The default Solr relevancy formula calculates three kinds of normalization factors.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This byte packs a lot of information in: the boost set on the document when indexed, the boost set on the field when indexed, and a length normalization factor which penalizes longer documents and helps shorter documents (under the assumption that finding any given keyword in a longer document is more likely and thus less relevant)
The actual field norms are calculated using the formula in figure 3.10
Field Norms factor in the matching documents boost, the matching field’s boost, and a length normalization factor which penalizes longer documents.
These three fairly separate pieces of data are stored as a single byte in the Solr index, which is the only reason they are combined together into this single “field norms” variable.
The d.getBoost() component represents the boost applied to the document when it is sent to Solr, and the f.getBoost() component represents the boost applied to the field for which the norm is being calculated.
It is worth mentioning that Solr actually allows the same field to be added to a document multiple times (performing some magic under the covers to actually map each separate entry for the field into the same underlying Lucene field)
Because duplicate fields are ultimately mapped to the same underlying field, if multiple copies of the field exist, the f.getBoost() actually becomes the product of the field boost for each of the multiple fields with the same name.
The Length Norm is computed by taking the square root of the number of terms in the field for which it is calculated.
The purpose of the Length Norm is to adjust for documents of varying lengths, such that longer documents do not maintain an unfair advantage simply by having a larger likelihood of containing any particular term a given number of times.
For example, let’s say that you perform a search for the keyword “Beijing”
Common sense would indicate that document in which Beijing is proportionally more prevalent is probably a better match, everything else being equal.
This is what the Length Norm attempts to take into account.
The overall Field Norm, calculated from the product of the document boost, the field boost, and the length norm, is encoded into a single byte which is stored in the Solr index.
Because the amount of information being encoded from this product is larger than a single byte can store, some precision loss does occur during this encoding.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
It does not affect the overall relevancy ordering, as the same queryNorm is applied to all documents.
It merely serves as a normalization factor to attempt to make scores between queries comparable.
It utilizes the sum of the squared weights for each of the query terms to generate this factor, which is multiplied with the rest of the relevancy score to normalize it.
The Query Norm should not affect the relative weighting of each document that matches a given query.
The Coord factor’s role is to measure how much of the query each document matches.
Query:        Accountant AND (“San Francisco” OR “New York” OR “Paris”) You may prefer to find an accountant with offices in each of the cities you mentioned, as.
If all four of these terms match, the Coord factor is 4/4
The idea here behind the Coord factor is that, all things being equal, documents which contain more of the terms in the query should score higher than documents which only match a few.
We have now discussed all of the major components of the default relevancy algorithm in Solr.
We discussed term frequency, inverse document frequency, the two most key components of the relevancy score calculation.
We then went through boosting and normalization factors, which refine the scores calculated by term frequency and inverse document frequency alone.
With a solid conceptual understanding and a detailed overview of the specific components of the relevancy scoring formula, we’re now set to discuss Precision and Recall, two important aspects for measuring the overall quality of the result sets returned from any search system.
The information retrieval concepts of Precision (a measure of accuracy) and Recall (a measure of thoroughness) are very simple to explain, but are also very important to understand when building any search application or understanding why the results being returned are not meeting your business requirements.
We’ll provide a brief summary here of each of these key concepts.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The precision of a search results set (the documents which match a query) is a measurement answering attempting to answer the question “Were the documents which came back the ones I was looking for?”
Let’s return to our earlier example from section 3.1 about searching for a book on the topic of buying a new home.
We’ve determined that by our internal company measurements that the books in table 3.10 would be considered good matches for such a query.
All other book titles, for purposes of this example, would not be considered relevant for.
For this example, if all of the documents which were supposed to be returned (documents Matches / 3 Total Matches), which would be perfect.
If, however, all results came back, the precision would only be .5, since half of the results which were returned were not correct – that is, they were not “precise.”
As you can see, Precision is a measure of how “good” each of the results of a query are, but it pays no attention to how thorough they are – a query which returns one single correct document out of a million other correct documents is still considered perfectly precise.
Because Precision only considers the overall accuracy of the results that come back and not the comprehensiveness of the result set, we need to counterbalance the Precision measurement with one that takes thoroughness into account – Recall.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Whereas Precision measures how “correct” each of the results being returned is, Recall is.
This underlies the critical difference between Precision and Recall – Precision is high when the results returned are “correct”, whereas Recall is high when the correct results are “present.” Recall does not care that all of the results are correct, whereas Precision does not care that all of the results are present.
In the next section, we’ll talk about strategies for striking an appropriate balance between Precision and Recall.
Though there is clearly tension between the two, Precision and Recall are not mutually.
Maximizing for full Precision and full Recall is the ultimate goal of just about every search relevancy-tuning endeavor.
With a contrived example (or a hand-tuned set of results), this seems easy, but in reality, this is a very challenging problem.
Many techniques can be undertaken within Solr to improve either Precision or Recall, though most are geared more toward increasing Recall in terms of the full document set being returned.
Aggressive textual analysis (to find multiple variations of words) is a great.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
One common way to approach the Precision versus Recall problem in Solr is to actually attempt to solve for both: measuring for Recall across the entire result set and measuring for Precision only within the first page (or few pages) of search results.
Following this model, “better” matches will be boosted to the top of the search results based upon how well you tune your use of Solr's relevancy scoring calculations, but you will also find that many poorer matches appear at the bottom of your search results list if you actually went to the last page of the search results.
This is only one way to approach the problem, however.
Since many websites, for example, want to appear to have as much content as possible, and since those sites know that visitors will never actually go beyond the first few pages, they can actually show very precise results on the first few pages which yielding a very high Recall value across the entire result set since they are increasing the chances of pulling back more content by being very lenient in which keywords are able to match the initial query.
The decision on how to best balance Precision and Recall is ultimately dependent upon your use case.
In scenarios like legal discovery, there is a very heavy emphasis placed on Recall, as there are legal ramifications if any documents are missed.
For other use cases, the requirement may simply be to find a few really great matches and find nothing that does not exactly match every term within the query.
Most search applications fall somewhere between these two extremes, and striking the right balance between Precision and Recall is a never-ending challenge – mostly because there is generally no one right answer.
Regardless, understanding the concepts of Precision and Recall and why changes you make swing you more towards one of these two conceptual goals (and likely away from the other) is critical to effectively improving the quality of your search results.
We have an entire chapter dedicated to relevancy tuning, chapter 16, so you can be sure you will see this tension between precision and recall surface again.
One of the most appealing aspects of Solr, beyond it’s speed, relevancy, and powerful text searching features, is how well it scales.
Solr is able to scale to handle billions of documents and an infinite number of queries by adding servers.
Chapter 12 will provide an in-depth overview of scaling Solr in production, but this section will lay the groundwork for how to think about the necessary characteristics for operating a scalable search engine.
Specifically, we’ll discuss the nature of Solr documents as denormalized documents and why this enables linear scaling across servers, how distributed searching works, the conceptual shift from thinking about servers to thinking about clusters of servers, and some of the limits of scaling Solr.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Central to Solr is the concept of all documents being denormalized.
A completely denormalized document is one in which all fields are self-contained within the document, even if the values in those fields are duplicated across many documents.
This concept of denormalized data is common to many NoSQL technologies.
A good example of denormalization is a user profile document having a “city”, “state”, and “postalCode” field, even though in most cases the “city” and “state” field will be exactly the same across all documents for each unique “postalCode” value.
This is in contrast to a normalized document where relationships between parts of the document may be broken up into multiple smaller documents, the pieces of which can be joined back together at query time.
A normalized document would only have a “postalCode” field, and a separate location document would exist for each unique “postalCode” so that the “city” and “state” would not need to be duplicated on each user profile document.
If you have any training whatsoever in building normalized tables for relational databases, please leave that training at the door when thinking about modeling content into Solr.
Figure 3.11 demonstrates a traditional normalized database table model, with big “X” over it to make it obvious that this is not the kind of datamodeling strategy you will use with Solr.
Figure 3.11 Solr documents do not follow the traditional normalized model of a relational database.
This figure demonstrates how NOT to think of Solr Documents.
Instead of thinking in terms of multiple entities with relationship to each other, a Solr Document is modeled as a flat, denormalized data structure, as shown in listing 3.2
Notice that the information in figure 3.11 represents two users working at a company called “Code Monkeys R Us, LLC”
While this figure shows the data nicely normalized into separate tables for the employees’ personal information, location, and company, this is not.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Listing 3.2 shows the denormalized representation for each of these employees as mapped to a Solr Document.
In a traditional relational database, a query can be constructed which will join data from multiple tables when resolving a query.
While some basic “join” functionality does now exist Solr (which will be discussed in chapter 14), it is only recommended for cases where it is impractical to actually denormalize content.
Solr knows about terms that map to documents but does not natively know about any relationships between documents.
That is, if you wanted to search for all users (in the previous example) who work for companies in Decatur, GA, you would.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
While this denormalized document data model may sound limiting, it also provides a sizable advantage – extreme scalability.
Because we can make the assumption that each document is self-contained, this means that we can also partition documents across multiple servers without having to keep related documents on the same server (since documents are independent of one another)
This fundamental assumption of document independence allows queries to be parallelized across multiple partitions of documents and multiple servers to improve query performance, and this ultimately allows Solr to scale horizontally to handle querying billions of documents.
This ability to scale across multiple partitions and servers is called Federated or Distributed Searching, and will be covered in the following section.
The world would be a much simpler place if every important data operation could be run using a single server… it would also be a much more boring world.
In reality, sometimes your search servers become overloaded by either too many queries at a time or by too much data needing to be searched through for a single server to handle.
In the latter case, it is necessary to break your content into two or more separate Solr indexes, each of which contains separate partitions of your data.
Then, every time a search is run, it will actually be sent to both servers, and the results will be returned and aggregated before being returned from the search engine.
Solr includes this kind of distributed searching capability out of the box.
We’ll discuss how to manually segment your data into multiple partitions in chapter 12 when we talk about scaling Solr for production.
Conceptually, each solr index (called a Solr Core) is available through it’s own unique url, and each of those Solr Cores can be told to perform an aggregated search across other Solr shards using the following syntax:
The “shards” parameter is used to specify the location of one or more Solr Cores.
A shard is a partition of your index, so the shards parameter on the url tells Solr to aggregate results from multiple partitions of your data which are found in separate Solr Cores.
There is no requirement that separate Solr cores be located on separate machines.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The important take-away here is the nature of scaling Solr.
The reason for this is that a federated search across multiple Solr Cores is run in parallel on each of those index partitions.
Thus, if you divide one Solr index into two Solr indexes with the same number of documents, the distributed search across the two indexes should be approximately 50% faster, minus any aggregation overhead.
This should also theoretically scale to any other number of servers (in reality, you will eventually hit a limit at some point)
The conceptual formula for determining total query speed after adding an additional index partition (assuming the same total number of documents) is thus as follows: (Query Speed on N+1 indexes) = Aggregation Overhead.
Since Solr scales nearly linearly, you should be able to reduce your query times proportional to the additional number of Solr cores (partitions) you add, assuming you’re not constrained by server resources due to heavy load.
In the last section we introduced the concept of distributed searching to enable scaling to handle large document sets.
It is also possible to add multiple essentially identical servers into your system to balance the load of high query volumes.
Both of these scaling strategies rely on a conceptual shift away from thinking about servers and toward thinking about “clusters” of servers.
A cluster of servers is essentially defined as multiple servers, working in concert, to perform the same function.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Notice that the servers, for this use case, are mutually dependent.
If one becomes unavailable for searching, they all become unavailable for searching and begin failing, as indicated in the exception in listing 3.3
It is thus important to think in terms of “clusters” of servers instead of single servers when building out Solr solutions which must scale beyond a single box, as those servers are essentially combining to serve as a single computing resource.
As we wrap up our discussions of the key concepts behind searching at scale, we should be clear that Solr does have its limitations, several of which will be discusses in the next section.
Solr is an incredibly powerful document-based NoSQL datastore which supports full text searching and data analytics.
We have already discussed the powerful benefits of Solr’s inverted index and complex keyword-based Boolean query capabilities.
We have also seen how important relevancy is, and we’ve seen that Solr can scale essentially linearly across multiple servers to handle additional content or query volumes.
What then, are the use cases where Solr is not a good solution – what are the limits of Solr?
One limit, as we have already seen, is that Solr is NOT relational in any way across documents.
It is not well suited for joining significant amounts of data across different fields on different documents, and it cannot perform join operations at all across multiple servers.
While this is a functional limit of Solr, as compared to relational databases, this assumption of independence of documents is a tradeoff common among many NoSQL technologies, as it enables them to scale well beyond the limits of relational databases.
We have also already discussed the denormalized nature of Solr documents – data which is redundant must be repeated across each document for which that data applies.
This can be particularly problematic when the data in one field which is shared across many documents changes.
For example, let’s say that you were creating a search engine of social networking user profiles, and one of your user’s, John Doe becomes friends with another user named Coco.
Now, I not only need to update John’s and Coco’s profiles, but I also need to update the “second level connections” field for all of John’s and Coco’s friends, which could represent.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This harkens back to the notion of Solr not being relational in any way.
An additional limitation of Solr is that it currently serves primarily as a document storage mechanism – that is, you can insert, delete and update documents, but not single fields (very easily)
Solr does currently have some minimal capability to update a single field, but only if the field is attributed in such a way that its original value is stored in the index, which can be very wasteful.
Even then, Solr is internally still updating the entire document based upon re-indexing all of the stored fields internally.
What this means is that, whenever a new field is added to Solr or the contents of an existing field has changes, every single document in the Solr index must be reprocessed in its entirety before the data will be populated for the new field.
Many other NoSQL systems suffer from this same problem, but it is worth noting that data updates across the corpus require a non-trivial amount of document management to ensure the updates make it to Solr and in a timely fashion.
Solr is also optimized for a very specific use case, which is taking search queries with small numbers of search terms and rapidly looking up each of those terms to find matching documents, calculating relevancy scores and ordering them all, and then only returning a few actual results for display.
Solr is not optimized, however, at processing very long (1000’s of terms) queries or returning back very large result sets to users.
One final limitation of Solr worth mentioning is its elastic scalability: the ability to automatically add and remove servers and redistribute content to handle load.
While Solr scales very well across servers, it does not yet elastically scale by itself in a fully automatic way.
Recent work with Solr Cloud (covered in chaper 13), utilizing Apache Zookeeper for cluster management, is a great first step in this direction, but there are still many features to be worked out, such as automatic content resharding and pluggable sharding strategies which are currently being discussed but are not yet fully implemented.
In this chapter, we’ve discussed the key search concepts that serve as the foundation for most of Solr’s search capabilities.
We discussed the structure of Solr’s inverted index and how it maps terms to documents in a way that allows quick execution of complex Boolean.
We also discussed how fuzzy queries and phrase queries use position information to match misspellings and variations of terms and phrases in the Solr index.
We took a deep dive into Solr relevancy, laying out the default relevancy formula Solr uses and explaining conceptually how each piece of relevancy scoring works and why it exists.
We then provided a brief overview of the concepts of Precision and Recall, which serve as two opposing forces within the field of information retrieval and provide us with a good conceptual framework with which to judge whether or not our search results are meeting our goals.
Finally, we discussed key concepts for how Solr scales, including discussions of content denormalization within documents and federated distributed searching to ensure query.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
We ended with a discussion of thinking in terms of “clusters” as opposed to “servers” as we scale our search architectures, and we ended by discussing some of the limitations of Solr and use cases for when Solr may not be a great fit.
At this point, you should have all of the conceptual background necessary to understand the core features of Solr throughout the rest of this book and should have a solid grasp the most important search concepts for building a killer search application.
In the next chapter, we’ll begin digging into Solr’s key configuration settings, which will enable more fine-grained control over many of the features discussed in this chapter.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Up to this point, you've taken much of what has been presented on faith without learning how Solr actually works.
We'll change that in this chapter and the next by looking under-thehood to learn how Solr is configured and how configuration settings impact Solr's behavior.
As we learned in chapter 2, Solr works out-of-the-box without making any configuration changes.
But, at some point, you're going to need to make some configuration changes to optimize Solr for your specific search application requirements.
Broadly speaking, most of the configuration you'll do with Solr focuses around three main XML files:
In chapter 5, we'll learn all about schema.xml, which drives how your index is structured.
As most of Solr's configuration is specified in XML documents, this chapter contains numerous code listings showing XML snippets from solrconfig.xml and solr.xml.
However, our main focus is on the concepts behind the configuration settings rather than the specific XML syntax, which is mostly self-explanatory.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
To begin let's see what happens from a configuration perspective when you start the Solr server.
Recall from chapter 2 that Solr runs as a Java Web application in Jetty.
Figure 4.1 depicts how solr.xml and solrconfig.xml are used during the Solr initialization process.
Figure 4.1 Depiction of how solr.xml and solrconfig.xml are used to configure Solr during initialization.
The solr.xml file identifies one or more cores to initialize.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The initial configuration only has a single core named "collection1", but in general there can be many cores defined in solr.xml.
Now that we've seen how Solr identifies configuration files during startup, let's turn our attention to understanding the main sections of the solrconfig.xml, as that will give you an idea of what's to come in the rest of this chapter.
To illustrate the concepts in solrconfig.xml, we'll build upon the work done in chapter 2 by using the pre-configured example server and the Solritas example search UI.
To begin, we recommend that you start up the example server we used in chapter 2 using:
This will display the active solrconfig.xml for the collection1 core running on your computer.
Listing 4.2 shows a condensed version of the solconfig.xml to give you an idea of the main elements.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
As you can see, solrconfig.xml has a number of complex sections.
The good news is that you don’t have to worry about these until you encounter a specific need.
On the other hand, we do think it is a good idea to make a mental note of what is in solrconfig.xml as it shows how flexible Solr is and what types of behavior you can control and extend.
When organizing this chapter, we chose to present the configuration settings in an order that builds on previous sections rather than following the order of elements in the XML document.
For example, we present Solr's request handling framework before we discuss caching even though cache-related settings come before request handler settings in solrconfig.xml.
We took this approach because you should understand how requests are handled before you worry about optimizing a specific type of request with caching.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
However, we save a discussion of index-related settings for the next chapter when you can learn about them after gaining a basic understanding of the indexing process.
Specifically, you can ignore the following elements until chapter 5:
As you work through solrconfig.xml, you will encounter common XML elements that Solr uses to represent various data structures and types.
Table 4.1 provides a brief description and example of the types of elements Solr uses throughout the solrconfig.xml document.
You will also encounter these elements in XML search results, so please spend a minute getting familiar with this Solr-specific syntax.
Table 4.1 Solr's XML elements for data structures and typed values.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Learning about configuration is not the most exciting of tasks so to help keep you interested, we recommend that you experiment with configuration changes as you work through this chapter.
However, your changes won't be applied until you reload the Solr core.
In other words, Solr doesn't "watch" for changes to solrconfig.xml and apply them automatically; you have to take an explicit action to apply configuration changes.
For now, the easiest way to apply configuration changes is to use the Reload button from the Core Admin page of the administration console as shown in figure 4.2
Figure 4.2 Reload a core to apply configuration changes from the Core Admin page.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
If you're running Solr locally, then go ahead and click on the Reload button for the collection1 core just to verify the functionality works.
Also, we'll see another way to reload cores programmatically using the Core Admin API at the end of this chapter.
Now that we've covered some of the configuration background, let's start our tour of solrconfig.xml by looking at some miscellaneous settings for the Solr server.
Listing 4.3 shows the configuration settings we'll discuss in this section.
If you're just starting out, then use the version that is specified in the example server, such as:
In this case, you can either re-index all your documents or use Lucene's built-in index upgrade tool1
As that is a problem for the future, we'll refer you to the JavaDoc for instructions on how to run the upgrade tool.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Each <lib> element identifies a directory and a regular expression to match files in the directory.
Notice that the dir attribute uses relative paths, which are evaluated from the core directory root, commonly referred to as the core instanceDir.
Consequently, the two example <lib> elements shown above result in the following JAR files being added to Solr's classpath:
Note that the version number for the apache-solr-langid JAR file may be different depending on the exact version of Solr 4 you are using.
Alternatively, you can use the path attribute to identify a single JAR file, such as:
In a nutshell, an MBean is a Java object that exposes configuration properties and statistics using the Java Management Extensions (JMX) API.
This allows Solr to be integrated into your existing system administration infrastructure.
However, you don't need an external JMX-enabled monitoring tool to see Solr's MBeans in action.
The Solr administration console provides access to all of Solr's MBeans.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
We will see a few more examples of inspecting Solr's MBeans from the administration console throughout this chapter.
For now, let's move on to learning how Solr processes requests.
Solr's main purpose is to search, so it follows that handling search requests is one of the most important processes in Solr.
In this section, you'll learn how Solr processes search requests and how to customize request handling to better fit your specific search requirements.
For example, if you want to query Solr, then you send an HTTP GET request.
Alternatively, if you want to index a document in Solr, you use an HTTP POST request.
Listing 4.4 shows a HTTP GET request to query the example Solr server.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You can either input this URL into a Web browser, use a command-line tool like cURL, or our recommended approach is to use the example driver application that comes with the book.
To execute this request using the example driver, you simply do:
The output when running this utility for listing 4.4 is:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 4.4 shows the sequence of events and main components involved in handling this Solr request.
Figure 4.4 Sequence of events to process a request to the /select request handler.
A client application sends an HTTP GET request to http://localhost:8983/solr/collection1/select?q=...
Query parameters are passed along in the query string of the GET request.
Jetty accepts the request and routes it to Solr's unified request dispatcher using the /solr context in the request path.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Solr's request dispatcher uses the "collection1" part of the request path to determine the core name.
Next, the dispatcher locates the /select request handler registered in solrconfig.xml for the collection1 core.
The /select request handler processes the request using a pipeline of search components (covered in section 4.2.4 below)
After the request is processed, results are formatted by a response writer component and returned to the client application, by default the /select handler returns results as XML.
The main purpose of the request dispatcher is to locate the correct core to handle the request, such as collection1, and then route the request to the appropriate request handler registered in the core, in this case /select.
In practice, the default configuration for the request dispatcher is sufficient for most applications.
On the other hand, it is common to define a custom search request handler or to customize one of the existing handlers, such as /select.
Let's dig into how the /select handler works to gain a better understanding of how to customize a request handler.
Listing 4.5 shows the definition of the /select request handler from solrconfig.xml.
This shorthand notation helps reduce clutter in Solr's configuration documents.
In Solr, there are two main types of request handlers:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
We'll learn more about update handlers in the next chapter when we cover indexing.
For now, let's concentrate on how search request handlers process queries, as depicted in figure 4.5
Figure 4.5 Search request handler made up of parameter decoration (defaults, appends, invariants), firstcomponents, components, and last-components.
The search handler structure depicted in figure 4.5 is designed to make it easy for you to adapt Solr's query processing pipeline for your application.
For example, you can define your own request handler or, more commonly, add a custom search component to an existing request handler, such as /select.
In general, a search handler is comprised of the following phases, where each phase can be customized in solrconfig.xml:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A request handler does not need to define all phases in solrconfig.xml.
As you can see from listing 4.5, the /select only defines the defaults section.
In practice, customized request handlers are commonly used to simplify client applications.
For instance, the Solritas example we introduced in chapter 2 uses a custom request handler /browse to power a feature-rich search experience while keeping the client-side code for Solritas very simple.
Hiding complexity from client code is at the heart of Web services and object-oriented design.
Solr adopts this proven design pattern by allowing you to define a custom search request handler for your application, which allows you to hide complexity from your Solr client.
For example, rather than requiring every query to send the correct parameters to enable spell correction, you can use a custom request handler that has spell correction enabled by default.
The Solr example server comes pre-configured with a great example of this design pattern at work to support the Solritas example application.
Listing 4.6 shows an abbreviated definition of the /browse request handler from solrconfig.xml.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
We recommend that you take a minute to go through all the sections of the /browse request handler in the actual solrconfig.xml file.
One thing that should stand out to you is that a great deal of effort was put into configuring this handler, in order to demonstrate many of the great features in Solr.
When starting out with Solr, you definitely do not need to configure something similar for your application all at once.
In other words, you can build up a custom request handler over time as you gain experience with Solr.
Let's see the /browse request handler in action using the Solritas example.
With the example Solr server running, direct your browser to http://localhost:8983/solr/collection1/browse.
Enter "iPod" into the search box as shown in Figure 4.6
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 4.6 Screen shot of Solritas example powered by the /browse request handler.
Take a moment to scan over figure 4.6 to see all the search features activated for this simple query.
Behind the scenes, the Solritas search form submits a query to the /browse request handler.
That's an impressive list of features for a simple request like q=iPod! As you may have guessed, these features are enabled using default parameters in the /browse request handler.
For example, the default value for the response writer type parameter "wt" is.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Velocity is an open source templating engine written in Java2
From the log message shown above, the only parameter sent by the form was "q", so all other parameters are set by defaults.
Let's do a little experiment to see the actual query that gets processed.
Instead of using response writer type "velocity", let's set the wt parameter to "xml" so we can see the response in raw form without the HTML decoration provided by Velocity.
Also, in order to see all the query parameters, we need to set the echoParams value to "all"
This is a good example of overriding default values by explicitly passing parameters from the client.
Notice how the number of parameters actually sent to the /browse request handler is quite large.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A Many more default parameters in this request not shown here From looking at listing 4.7, it should be clear that parameter decoration for a search.
Specifically, the defaults list provides two main benefits to your application.
First, helps simplify client code by establishing sensible defaults for your application in one place.
For instance, setting the response writer type "wt" to "velocity" means that client applications do not need to worry about setting this parameter.
Moreover, if you ever swap out Velocity for another templating engine, your client code does not need to change!
Second, as you can see from listing 4.7, the actual request includes a number of complex parameters needed to configure search components used by Solritas.
For example, there are over twenty different parameters to configure the faceting component for Solritas.
By preconfiguring complex components like faceting, you can establish consistent behavior for all queries while keeping your client code simple.
The /browse handler serves as a good example of what is possible with Solr query processing, but it's also unlikely that it can be used by your application because the default parameters are tightly coupled to the Solritas data model.
For example, range faceting is configured for the price, popularity, and manufacturedate_dt fields.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
From listing 4.6, notice that the /browse request handler specifies:
This configuration means that the default set of search components is applied and then the spellcheck component is applied.
This is a very common design pattern for search request handlers.
In fact, you'll be hard-pressed to come up with an example of where you need to redefine the <components> phase for a search handler.
At a high-level, the query component parses and executes queries using the active searcher, which is discussed in section 4.3 below.
The specific query parsing strategy is controlled by the "defType" parameter.
The query component identifies all documents in the index that match the query.
The set of matching documents can then be used by other components in the query processing chain, such as the facet component.
The query component is always enabled and all other components need to be explicitly enabled using query parameters.
The key take-away for now is that faceting is built-in to every search request and it just needs to be enabled with query request parameters.
To see an example of the More Like This component in action, search for "hard drive" in the.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 4.8 Example of similar items found by the More Like This search component.
To see an example of what the stats component produces, execute GET request as shown in listing 4.8:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A Request statistics for the price field #B Summary statistics returned for the price field.
The parsed query value is returned to help you track down query formulation issues.
To see the debug component at work, direct your browser to the following URL:
Listing 4.9 shows a snippet of the XML output produced by the debug component:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Notice how a single term query "iPod" entered by the user results in a fairly complex query composed of many different boosts on numerous fields.
The more complex query is created by the edismax query parser, which is enabled by the defType parameter under defaults.
Listing 4.10 shows the definition of the spellcheck component from solrconfig.xml:
Notice that the name of the component "spellcheck" matches what is listed in the <lastcomponents> section of the /browse request handler.
The key take-away at this point is seeing how a search component is added to the search request-handling pipeline using <last-components>
At this point, you should have a solid understanding of how Solr processes query.
Before we move on to another configuration topic, you should be aware that the Solr administration console provides access to all active search request handlers under Plugins / Stats > QUERYHANDLER.
Figure 4.9 shows properties and statistics for the /browse search handler, which as you might have guessed is just another MBean.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Now let's turn our attention to configuration settings that help optimize query performance.
The <query> element contains settings that allow you to optimize query performance using techniques like caching, lazy field loading, and new searcher warming.
It goes without saying that designing for optimal query performance from the start is critical to the success of your search application.
In this section, you'll learn about managing searchers, which is one of the most important techniques for optimizing query performance.
In Solr, queries are processed by a component called a searcher.
There is only one "active" searcher in Solr at any given time.
All query components for all search request handlers execute queries against the active searcher.
The active searcher has a read-only view of a snapshot of the underlying Lucene index.
It follows that if you add a new document to Solr, then it is not visible in search results from.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This raises the question of how do new documents become visible in search results? The answer, of course, is to close the current searcher and open a new one that has a read-only view of the updated index.
This is what it means to commit documents to Solr.
The actual commit process in Solr is more complicated but we'll save a thorough discussion of the nuances of commits for the next chapter.
For now you can think of a commit as a black-box operation that makes new documents and any updates to your existing index visible in search results by opening a new searcher.
On the CORE page, take note of the searcherName property (in the diagram it is "Searcher@25082661 main")
Let's trigger the creation of a new searcher by re-sending all the example documents to your server as we did in section 2.1.4 using:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A new searcher was created because the post.jar command sent a commit after adding the example documents.
So now that we know a commit creates a new searcher to make new documents and updates visible, let's think about the implications of creating a new searcher.
However, there could be queries currently executing against the old searcher so Solr must wait for all in-progress queries to complete.
Also, any cached objects that are based on the current searcher's view of the index must be invalidated.
We'll learn more about Solr cache management in the next section.
For now, think about a cached result set from a specific query.
As some of the documents in the cached results may have been deleted and new documents may now match the query, it should be clear that the cached result set is not valid for the new searcher.
Because pre-computed data, such as a cached query result set, must be invalidated and re-computed, it stands to reason that opening a new searcher on your index is potentially an expensive operation.
When the user requests the next page, all of the previously computed filters and cached documents are no longer valid.
Without some care, the user is likely to experience some slowness, especially if their query is complex.
The good news is that Solr has a number of tools to help alleviate this situation.
First and foremost, Solr supports the concept of warming a new searcher in the background and keeping the current searcher active until the new one is fully warmed.
Solr takes the approach that it is better to serve stale results for a short period of time rather than allowing query performance to slow down significantly.
This means that Solr does not close the current searcher until a new searcher is warmed up and ready to execute queries with optimal performance.
Warming a new searcher is much like a sprinter in track and field.
Before a sprinter goes full speed in a race, she makes sure her muscles are warmed up and ready perform at full speed when the gun fires to start the race.
Just as a sprinter wouldn't start a race with cold muscles, nor should Solr activate a "cold" searcher.
We'll learn more about autowarming caches in the next section when we dig into Solr's cache management features.
A cache-warming query is a pre-configured query (in solrconfig.xml) that gets executed against a new searcher in order to populate the new searcher's caches.
Listing 4.11 shows the configuration of cache warming queries for the example server.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Also, note that the actual queries are commented out! This is intentional because there is a cost to executing warming queries and the Solr developers wanted to ensure you configure warming queries explicitly for your application.
In other words, the cache warming queries are application specific so the out-of-the-box defaults are strictly for example purposes.
For now, it's sufficient to make a mental note that you need to revisit this topic once you have a more thorough understanding of Solr query construction.
We should also mention that you do not need to have any warming queries for your application.
If query performance begins to suffer after commits, then you'll know it is time to consider using warming queries.
Each query takes time to execute so having many warming queries configured can lead to long delays in opening new searchers.
Thus, it's best to keep the list of warming queries to the minimal set of the most important queries for your search application.
You might be wondering what the problem with a new searcher taking a long time to warm-up is.
It turns out that warming too many searchers in your application concurrently can consume too many resources (CPU and memory), thus leading to a degraded search experience.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
We'll leave it as an exercise for the reader to determine if there's value in configuring warming queries for the first searcher.
Most Solr users just use the same queries for warming new and first searchers.
As a brief aside, Solr supports using XInclude to pull XML elements from other files into solrconfig.xml.
The <useColdSearcher> element covers the case where a new search request is made and there is no currently registered searcher.
On the other hand, if <useColdSearcher> is true, then Solr will immediately register the warming searcher regardless of how "warm" it is.
Returning to our track-and-field analogy, false would mean the starting official waits to start the race until our sprinter is fully warmed-up, regardless of how long that takes.
Conversely, a true value means that the race will start immediately regardless of how warmed-up our sprinter is.
This is especially true if your searchers take considerable time to warm-up.
Once this threshold is reached, new commit requests will fail, which is a good thing because allowing too many warming searchers to run in the background can quickly eat up memory and CPU resources on your server.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
So hopefully you now have a good sense for what a searcher is and how to configure Solr to manage searchers correctly for your application.
Now let's look at more ways to optimize query performance using caching.
Solr provides a number of built-in caches to improve query performance.
Before we get into the details of specific Solr caches, it's important to understand cache management fundamentals in Solr.
There are four main concerns when working with Solr caches:
Broadly speaking, proper cache management in Solr is not a set-it and forget-it type process.
You will need to keep an eye on your caches and fine-tune them based on actual usage of Solr.
Remember that the Solr administration console is your friend when it comes to monitoring important components like caches and searchers.
Solr keeps all cached objects in memory and does not overflow to disk, as is possible with some caching frameworks.
Consequently, Solr requires you to set an upper limit on the number of objects in each cache.
Solr will evict objects when the cache reaches the upper limit using either a Least Recently Used (LRU) or Least Frequently Used (LFU) eviction policy.
Least Recently Used (LRU) evicts objects when a cache reaches its maximum threshold based on the time when an object was last requested from the cache.
When a cache is full and a new object is added, the LRU policy will remove the oldest entry where age is determined by the last time each object in the cache was requested.
Solr also provides a Least Frequently Used (LFU) policy that evicts objects based on how frequently they are requested from the cache.
This is beneficial for applications that want to give priority to more popular items in the cache, rather than just those that have been used recently.
For example, Solr's filter cache is a good candidate for using the LFU eviction policy because filters are typically expensive to create and store so you want to keep the filter cache small and give priority to the most popular filters in your application.
We'll learn more about the filter cache in the next section.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A common misconception with cache sizing is to make your cache sizes really large if you have the memory available.
The problem with this approach is that once a cache becomes invalidated after a commit, there can be many objects that need to be garbage collected by the JVM.
Without proper tuning of garbage collection, this can lead to long pauses in your server caused by full garbage collection.
For now, the important lesson is to avoid defining overly large caches and let some objects in the cache be evicted periodically.
Hit ratio indicates how much benefit your application is getting from its cache.
Conversely, a low hit ratio is an indication that Solr is not benefiting from caching.
Eviction count shows how many objects have been evicted from the cache based on the eviction policy described above.
It follows that having a large number of evictions is an indication that the maximum size of your cache may be too small for your application.
Also, eviction count and hit ratio are interrelated, as a high eviction count will lead to a suboptimal hit ratio.
However, in Solr, this is not a concern because all objects in a cache are linked to a specific searcher instance and are immediately invalidated when a searcher is closed.
Recall that a searcher is a read-only view of a snapshot of your index; consequently all cached objects remain valid until the searcher is closed.
AUTO-WARMING NEW CACHES As we discussed in section 4.3, Solr creates a new searcher after a commit but it does not close the old searcher until the new searcher is fully warmed.
It turns out that some of the keys in the soon-to-be-closed searcher's cache can be used to populate the new searcher's cache, a process known as "auto-warming" in Solr.
Note that auto-warming a cache is different than using a warming query to populate a cache, as we discussed above in section 4.3.2
Every Solr cache supports an autowarmCount attribute that indicates either the maximum number of objects or a percentage of the old cache size to auto-warm.
How the objects are actually auto-warmed depends on the specific cache.
The key point for now is that you can configure Solr's caches to refresh a subset of cached objects when opening a new searcher, but as with any optimization technique, you need to be careful to not over do it.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
At this point you should have a basic understanding of cache management concepts in Solr.
Now, let's learn about the specific types of caches Solr uses to optimize query performance, starting with one of the most important caches: filter cache.
In Solr, a filter restricts search results to documents that meet the filter criteria but does not affect scoring.
A Filter query "fq" on the manu field When Solr executes this query, it computes and caches an efficient data structure that indicates which documents in your index match the filter.
For the example server, there are 2 documents that match this filter.
Now consider what happens if another query is sent to Solr with the same filter query (fq=manu:Belkin) but a different query, such as q=USB.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 4.11 Execute a query with a filter query "fq" clause to see the filter cache in action.
Next, navigate to the to the Plugins / Stats page for the collection1 and click on the CACHE link.
Figure 4.12 shows the properties and statistics for the filterCache MBean.
Reexecute the same query several times and you will see the filterCache statistics change.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Of course, it's difficult to fully appreciate the value of caching filters for a small index, but imagine an index with millions of documents and you can see how caching filters can really help optimize query performance.
In fact, using filters to optimize queries is one of the most powerful features in Solr, mainly because filters are re-usable across queries.
For now, we'll save a deeper discussion of filters for chapter 7 and turn our focus to how the filter cache is configured in solrconfig.xml.
Listing 4.13 shows the default configuration for the filter cache.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
AUTO-WARMING THE FILTER CACHE A filter can be a powerful tool for optimizing queries, but you can also get into trouble if you don't manage cached filters correctly.
Filters can be expensive to create and store in memory if you have a large number of documents in your index, or if the filter criteria is complex.
If a filter is generic enough to apply to multiple queries in your application, then it makes sense to cache the resulting filter.
In addition, you probably want to auto-warm some of the cached filters when opening a new searcher.
Let's look under the hood of the filter cache to understand what happens during autowarming of objects in the filter cache.
By now, you should know that objects cannot just be moved from the old cache to the new cache because the underlying index has changed, thus invalidating cached objects like filters.
Of course each object in the cache has a key.
For the filter cache, the key is the filter query, such as manu:Belkin.
To warm the new cache, a subset of keys are pulled from the old cache and then executed against the new searcher, which re-computes the filter.
In other words, auto-warming the filter cache requires Solr to re-execute the filter query with the new searcher.
Consequently, auto-warming the filter cache can be a source of performance and resource utilization problems in Solr.
When warming the new searcher, Solr must execute 100 filter queries.
Under this scenario, you'll quickly run into problems where you are warming too many searchers in the background.
We recommend that you enable auto-warming for the filter cache but set the autowarmCount attribute to a small number, less than 10 to start out.
In addition, we think the LFU eviction policy is more appropriate for the filter cache because it allows you to keep the filter cache small and give priority to the most popular filters in your application.
Here are the recommended configuration settings for the filter cache:
Of course, you need to do some experimentation with these parameters depending on how many filters your application uses and how frequently you commit against your index.
In terms of memory usage per cached filter, Solr has different filter representations based on the size of the matching document set.
As an upper limit, you can figure that any filter that matches many documents in your index will require MaxDoc bits of memory.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The query result cache holds result sets for a query.
If you execute this query more than once, then subsequent results are served from the query result cache rather than re-executing the same query against the index.
This can be a powerful solution for reducing the cost of computationally expensive queries.
Internal Lucene document IDs can change from one searcher to the next, so the cached values must be recomputed when warming the query result cache.
To auto-warm the query result cache, Solr needs to re-execute queries, which can be expensive.
So the same advice we gave about keeping the autowarmCount attribute small for the filter cache applies the query result cache.
That said, we do recommend setting the autowarmCount attribute for this cache to something other than the default zero so that you get some benefit from auto-warming recent queries.
Beyond sizing, Solr provides a few miscellaneous settings to help you fine-tune your usage of the query result cache.
For example, imagine your application shows 10 documents per page and that in most cases your users only look at the first and second pages.
However, if you set it too large, then every query is paying the price of loading more documents than you are showing to the user.
If your users rarely go beyond page 1, then it is better to set this element to the page size.
You can imagine a result set holding millions of documents in the cache would greatly impact available memory in Solr.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
However, the documents in the index have many more fields, such as category, popularity, manufacture date, etc.
Of course, our example documents do not have many fields so it's easy to overlook the benefit of this setting.
In practice, most documents have many fields so it is a good idea to load fields lazily.
The query result cache holds a list of internal document IDs that match a query, so even if the query results are cached, Solr still needs to load the documents from disk to produce search results.
The document cache is used to store documents loaded from disk in memory keyed by their internal document ID.
It follows that the query result cache uses the document cache to find cached versions of documents in the cached result set.
This raises the question whether it makes sense to auto-warm the document cache? There's actually a good argument to be made against auto-warming this cache because there's no way to ensure the documents you are warming have any relation to queries and filters being auto-warmed from the query result and filter caches.
In other words, you could be spending time re-creating documents that may not actually benefit your warmed filter and query result caches.
The last cache we'll mention is the field value cache, which is strictly used by Lucene and is not managed by Solr.
The field value cache provides fast access to stored field values by internal document ID.
The field value cache is used during sorting and when building documents for the response.
As this is an advanced topic, we'll refer you to the Lucene JavaDoc3 for more information.
At this point, you know how Solr processes queries using a request handling pipeline and.
Let's finish our tour of solrconfig.xml by learning how to control what gets returned from a query.
Solr provides great flexibility in what is returned to a client in the response.
In this section, we'll cover query response writers and document transformers.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
After query processing completes, Solr passes the results to a response writer based on the "wt" parameter in the request.
A response writer transforms the Java objects into a serialized form to be returned to the client application.
As we've seen in numerous examples, the default value for "wt" is XML.
Let's take a look at a few of the response writer definitions from solrconfig.xml to see how they work.
Listing 4.14 provides the definition of the JSON response writer.
The Solritas example uses the Velocity writer to demonstrate how Solr can be configured to produce a full Web-based UI using a custom response writer.
The actual definition of the Velocity response writer is quite simple:
Let's move on to a new feature in Solr 4 that allows you to enrich or otherwise transform.
A document transformer allows you to add fields dynamically to documents in the response.
For instance, you could use a document transformer to enrich a document with a value from an external database.
Let's work with one of the built-in document transformers to see how they work.
This could be useful for a search performance monitoring application to track the relationship between user clicks and document ranking.
First, you need to activate the transformer by adding the following configuration to solrconfig.xml:
A The name of the field to produce in response documents #B Return explanation as a named-list.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Next, you need to reload the core to pick-up this configuration change, see section 4.1.2
After reloading the core, execute the query passing [explain] in the field list parameter "fl" as in listing 4.15:
A Request the [explain] field for each document in the field list parameter "fl" #B [explain] field produced for each document using a document transformer.
Table 4.2 provides a summary of Solr's built-in document transformers:
Before we wrap up this chapter, we want to introduce the Core Admin API, which allows you to programmatically create, update, reload, rename, swap, and unload cores.
You've already seen how to reload a core from the Solr administration panel in section 4.1.2
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
To begin, let's use the STATUS action to get some basic status information about the collection1 core.
As you might have guessed the Core Admin API is based on HTTP so requesting status is as simple as sending the following HTTP GET request as in listing 4.16
The general format for a Core Admin API request is to specify an action and a core name.
To begin, make sure the example server is running and enter the following URL into your browser:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The operation should have failed because there is no directory named "sia" under Solr home.
To resolve this problem, open a command-line and recursively copy the collection1 directory to sia; on Unix/Linux based systems, you can do:
Verify that your new SolrInAction core is active by going to the Solr administration.
You should now see SolrInAction on the left navigation panel below collection1
Also, take a peek at the solr.xml and you should see something like:
Of course in practice you probably don't want two cores with the same data but there's no harm in having the same data for now.
Now, let's use the API to reload the SolrInAction core after making a configuration change.
For the sake of this exercise, change the autowarmCount to 10 for each of the caches defined in solrconfig.xml for the SolrInAction core at:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
After making your changes, you need to reload the core to apply the configuration changes.
This is done using the RELOAD action in the Core Admin API as shown in listing 4.18
If you go to the plug-in stats page in the admin console for the SolrInAction core, you'll.
Definitely give yourself a pat on the back after working through this long chapter! We know learning about configuration is not the most interesting of topics.
At this point you should have a solid understanding of how to configure Solr, especially for optimizing queryprocessing performance.
Specifically, you learned that Solr's request processing pipeline is composed of a unified request dispatcher and a highly configurable request handler.
We saw how a search request handler has four phases and you can customize each phase.
The /browse handler for the Solritas application served as a good example of using default parameters and custom components (spellcheck) to enable a feature-rich search experience while simplifying client application code.
We also learned how Solr processes query requests using a read-only view of the index with a component called a searcher.
There is only one active searcher in Solr at any point in time and a new searcher needs to be created before updates to the index are visible.
Closing the currently active searcher and opening a new one can be an expensive operation that affects query performance.
To minimize impact on query performance, Solr allows you to configure static queries to warm-up a new searcher.
Properly managing the new searcher warm-up process is one of the most important configuration tasks you'll need to do for your search application.
Solr also provides a number of important caches that need to be fine-tuned for your application.
We looked at the filter, query result, document, and field value caches.
For each cache, you need to set a maximum size and eviction policy based on actual usage of your application.
The Solr administration console provides key statistics, such as the hit ratio, to help you determine if your caches are sized correctly.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Caches can be auto-warmed when creating a new searcher, which also helps optimize query performance.
For example, you can use cache auto-warming to pre-populate Solr's filter cache with the most popular filters used by queries in your application.
Cache autowarming, while powerful, can also lead to large wait times waiting for a new searcher to warm-up.
So we advised to start with small autowarmCount values and monitor searcher warm-up time closely.
Next, we covered how to control the format of responses using a response writer.
The default response writer produces XML, but you can easily request other formats such as JSON using the "wt" query parameter.
We closed the chapter with a quick overview of the Core Admin API.
Specifically, we showed you how to request the status for an existing core, create a new core, and reload a core using simple HTTP GET requests.
As we mentioned above, we chose to skip over the index-related settings in solrconfig.xml until we covered the basics of indexing.
In the next chapter, you will learn about the Solr indexing process and index-related configuration settings.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Field types for structured data like dates and language codes.
How to index XML, JSON, CSV and other document types.
In Chapter 3, we learned how Solr finds documents using an inverted index, which in its simplest form, is a dictionary of terms and a list of documents where each term occurs.
Solr uses this index to match terms in a user’s query with documents where those terms occur.
In this chapter, we learn how Solr processes documents to build the index.
At the end of this chapter, you’ll know how to get documents indexed in Solr and will understand key concepts like fields, field types, and schema design.
On the other hand, you will still be able to follow along with most examples without actually running Solr if you prefer to read this chapter and then come back to doing the hands-on activities another time.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Throughout this chapter and the next, we will design and implement an indexing and text analysis solution for searching micro-blog content from popular social media sites like Twitter.
We use the term "micro-blog" as a generic term for the short, informal messages and other medium people share with each other on social networks.
Examples of micro-blogs are tweets on Twitter, Facebook posts, and check-ins on Foursquare.
In this chapter, we define the fields and field types to represent micro-blogs in Solr and learn how to add documents to Solr.
In chapter 6, we learn how to do text analysis on micro-blog content using built-in Solr tools.
Let’s get started by looking at the type of documents we will be working with in this example and how users might want to search them.
To begin, table 5.1 shows some fields from a fictitious tweet that we’ll use throughout this chapter to learn about indexing documents in Solr.
Even if you are not interested in analyzing social media content, the lessons we learn by working through this example have broad applicability for most search applications.
Each document in a Solr index is made up of fields, where each field has a specific type that determines how it is stored, searched, and analyzed.
Take a moment to think about how a user might find microblogs using these fields.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Of course you could just index all these fields but if you are developing a large-scale system to support millions of documents and high query volumes, then you only want to include the fields that will actually be searched by your users.
For example, the user_id field is an internal identifier for Twitter so it’s unlikely users will ever want to search on this field.
In general, each field increases the size of your index so you should only include fields that add value for users.
The favourites_count field is the number of favorites the author of the tweet has, not the number of favorites for the tweet.
This field is interesting because it has useful information from a user interface perspective but doesn’t seem like a good candidate as a parameter to a search query.
We’ll address how to handle these display-oriented fields in section 5.2 when we discuss stored vs.
Now, let’s think about how users might build a query using these fields, as that will help us decide how to represent these fields in our Solr index.
Figure 5.1 depicts a fictitious search form based on the fields for our example micro-blog search application.
Each field that we identified as being useful from a search perspective is represented on the form.
This is a key point in designing your search application in that you need to think about how users will search a specific field in your index as that will help determine how the field is defined in Solr.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The favourites_count field is used for displaying results but is not used for searching.
So now we have a conceptual understanding of the fields in our example application and an idea of how users will search for documents using these fields.
Next, let’s get a high-level understanding of how to add documents to Solr.
At a high-level, the Solr indexing process distills down to three key tasks:
Figure 5.2 provides a high-level overview of these three basic steps to getting your document indexed in Solr.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Next, in step 2, we send the XML document to Solr’s document update service using HTTP POST.
In step 3, each field is analyzed based on the configuration defined in schema.xml before being added to the index.
Solr supports several formats for indexing your document including XML, JSON, and CSV.
In Figure 5.2, we chose XML because its self-describing format makes it easy to understand.
Here is how our example tweet would look using the Solr XML format:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Notice that each field from Table 5.1 is represented in the XML and that the syntax is rather simple—you simply define the field name and value for each field! What you don’t see is anything about text analysis or field type.
This is because you define how fields are analyzed in the schema.xml document depicted in the diagram.
Recall from our discussion in Chapter 2 that Solr provides a simple HTTP-based interface to all of its core services, including a document update service for adding and updating documents.
At the top left of the diagram in Figure 5.2, we depict sending the XML for our example tweet using an HTTP POST to a document update service in Solr.
We’ll go into more details about how to add specific document types, such as XML, JSON, and CSV later in the chapter.
For now, think of the document update service as an abstract component that validates the contents of each field in a document and then invokes the text analysis process.
After each field is analyzed, the resulting text is added to the index, thus making the document available for search.
We will spend more time on how the actual indexing process works in section 5.5 below.
A high-level overview of the indexing process is sufficient for now, as we need to focus on more foundational concepts first.
Specifically, we need to understand how Solr uses the schema.xml depicted in figure 5.2 to drive the indexing process.
The schema.xml defines the fields and field types for your documents.
For simple applications, the fields to search and their types may be obvious.
In general, though, it helps to do some up-front planning about your schema.
With our example micro-blog search application, we dove right in and defined what a document is and which fields we want to index.
In practice, this process is not always obvious for a real application, so it helps to do some up-front design and planning work.
In this section, we learn about some key design considerations for search applications.
Specifically, we’ll learn to answer the following key questions about your search application:
What are the fields in your document that can be searched by users?
Which fields should be displayed to users in the search results?
Let’s begin by determining the appropriate granularity of a document in your search application as that impacts how you answer the other questions.
Determining what is a document in your Solr index drives the entire schema design process.
In some cases it is obvious, such as with our tweet example, where the text content is typically short, so each tweet will be a document.
However, if the content you want to index is very large, such as a technical computer book, you may want to treat sub-sections of a large document as the indexed unit.
The key is to think about what your users will want to.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Let’s look at a different example to help you think about what is a document for your index.
Imagine searching for "text analysis" on Web site that sells technical computer books.
If the site treated each book as a single document, then the user would see Solr in Action in the search results, but would then need to page through the table of contents or index to find specific places where "text analysis" occurs in the book.
In Figure 5.3, the left-side image depicts how search results might look when an entire book is indexed as a single document.
Figure 5.3 Comparison of search results when indexing entire book as document vs.
On the other hand, if the site treated individual chapters in each book as documents in the index, then the search results might show the user the Text Analysis chapter in Solr in Action as the top result, as seen on the right in Figure 5.3
However, since text analysis is a central concept in search, most of the other chapters in this book and other search books would be included as highly relevant results as well.
So being too granular can overwhelm users with too many results to wade through.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You may also need to consider the type of content you are indexing, as splitting a technical computer book by chapter seems to make sense but splitting a fiction novel by chapter doesn’t seem like a good approach.
In the end, it’s your choice on what makes a document in your index, but definitely consider how document granularity impacts user experience.
In general, you want your documents to be as granular as possible without causing your users to miss the forest for the trees.
As a quick aside, we should note that Solr offers a feature called hit highlighting that allows you to highlight relevant sections of longer documents in search results.
This is useful when you cannot break long documents up into smaller units but still want to help your users quickly navigate to highly relevant sections in large documents.
For example, we could use hit highlighting to show the first paragraph of Chapter 6 in this book in the search results of a query for "text analysis"
Once you’ve identified what a document is, you should determine how to uniquely identify each document in the index.
For a chapter, it might be the ISBN number plus the chapter number.
Solr does not require a unique identifier for each document but if supplied, Solr uses it to avoid duplicating documents in your index.
For those with a database background, the unique identifier is similar to a primary key for a row in a table.
If a document with the same unique key is added to the index, then Solr overwrites the existing record with the latest document.
We’ll return to the discussion of unique keys when we discuss distributed search later in the book.
For our example micro-blog search application from section 5.1, the tweet already includes a unique identifier field: id.
However, if we index content from a variety of social media sources, then we would probably add something like "twitter:" as a prefix to differentiate this document from a Facebook post with the same numeric id value.
Once you have determined what a document is for your index and how to uniquely identify each document, the next step is to determine the indexed fields in the document.
The best way to think about indexed fields is to ask whether a typical user could develop a meaningful query using that field? Another way to decide if a field should be indexed is if you removed this field from the search form would your users miss it?
For example, every book has a title and an author.
When searching for books, people generally expect to find books of interest based on title and author, so these fields should be indexed.
On the other hand, although every book has an editor, users typically do not search.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Thus, editor name would need to not be an indexed field.
Conversely, if you were building a search index for the book publishing industry, then it’s very likely that your users would want to search by editor name so you would include that as an indexed field.
Determining which fields to include in the index is specific to every search application.
Take a moment to think about the indexed fields for your documents.
Keep these fresh in your mind, as you’ll need to refer to them as you work through the rest of this chapter.
As we already discussed, the screen_name, type, timestamp, lang, and text should be indexed for our micro-blog example.
The id and user_id fields are used internally by Twitter and are unlikely to be missed if you don’t allow users to search by these fields.
Although users probably won’t search by editor name to find a book to read, we may still want to display the editor’s name in the search results.
In general, your documents may contain fields that are not very useful from a search perspective but are still useful for displaying search results.
The favourites_count field is a good example of a stored field that is not indexed but is useful for display purposes.
You can imagine users would find it useful to see which authors have more favorites than others in search results but it’s unlikely that users would want to search by this field.
In addition to displaying stored fields, you can also sort by them, such as to see tweets from authors with more favorites at the top of search results.
Of course a field may be indexed and stored, such as the screen_name, timestamp, and text fields in our micro-blog search application.
Each of these fields can be searched and displayed in results.
As a search application architect, one of your goals should be to minimize the size of your index.
If you’re considering Solr, then most likely you have an application that needs to scale to handle large volumes of documents and users.
Each stored field in your index consumes disk space and requires CPU and I/O resources to read the stored value for returning in search results.
Thus, you should choose your stored fields wisely especially for large-scale applications.
At this point you should have a good idea about the types of questions you need to.
Once you’ve settled on a plan, it’s time to rollup your sleeves and work with Solr’s schema.xml to implement your design.
As we saw in figure 5.2, schema.xml is the main configuration document Solr uses to understand how to index your documents.
Let’s take a quick preview of the main sections of schema.xml so we have an idea of what’s in-store for us over the next couple of sections in this chapter.
In the next few sections, we build a valid schema.xml document for our example micro-blog search application.
The schema.xml file is in the conf directory for your Solr core.
For instance, the schema.xml for the example Solr server is in:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
To see the full example schema, click on the [SCHEMA] link from the Solr administration page, which loads the schema.xml into your browser.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
At a quick glance, it’s easy to be overwhelmed by all the details in this document.
By the end of this chapter, you’ll have a clear understanding of all these details and will be wellequipped to craft your own schema.xml.
For now, notice that there are three main sections of the schema.xml document:
Field Types under the <types> element determine how dates, numbers, and text fields are handled in Solr.
Solr uses the field definitions from schema.xml to build the internal structure of the index for your documents.
In this section, we learn how to define fields, dynamic fields, and copy fields in schema.xml.
Listing 5.3 defines the indexed and stored fields for our example application.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
With these field definitions, Solr knows how to index micro-blog documents so they can be searched using a form similar to figure 5.1
When defining a field in schema.xml, there are a few required attributes you must provide to Solr.
Each field has a unique name that is used when constructing queries.
Each field must define a type attribute that identifies the <fieldType> to use for that field; we cover field types in detail in the next section.
Each field must also define whether it is indexed and/or stored.
As we discussed in section 5.2, indexed fields can be searched and stored fields can be returned in search results for display and sorting purposes.
Of course, a field can be both, as is the case for most of the fields in our example.
Also, notice that there are no "nested" fields in Solr; all fields are siblings of each other in schema.xml implying a flat document structure.
As we discussed in Chapter 3, documents in Solr need to be de-normalized into a flat structure and must contain all the fields needed to support your search requirements.
In other words, there is no relational structure that allows you to join with other documents to pull in additional information to service queries or generate results.
You’ll undoubtedly encounter content on the Web about doing document joins in Solr.
For now, it’s important to understand that Solr joins are more like sub-queries in SQL than joins.
A typical use case is to find the parent documents of child documents that match your search criteria using Solr joins.
For example, in our example micro-blog application, we could use Solr joins to bring back the original post instead of re-tweets.
One thing that can be confusing is that when a field is stored, Solr stores the original value and not the analyzed value.
For example, in listing 5.3, we declared the text field with indexed="true" and stored="true"
This means that the text field will be searchable and you can return the original text in search results.
Of course, if you don’t return a field in search results, then it doesn’t need to be stored.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
There's also a case to be made for storing all fields for a document.
If you plan to update documents after they are indexed, then you will need to store all fields.
We'll learn more about updating documents in section 5.6.3 later in the chapter.
So far, our micro-blog search application only uses a small number of simple fields.
Let’s add a few more fields into the mix to exercise some of Solr’s strengths in dealing with more complex document structures.
Specifically, let’s add support for a "links" field that contains zero or more links associated with each document.
As you’ve probably seen on Twitter, users can embed links to related content, such as a photo or article on the Web.
Here’s an example of another fictitious tweet with two links embedded:
The links in this document are shortened URLs provided by bitly.com that resolve to the following Web sites shown in table 5.2:
Table 5.2 Actual URLs for shortened links in the example tweet.
For example, you can imagine users wanting to find all tweets that link to the Solr In Action MEAP page on manning.com.
Since this example contains two links, we need a way to encode two values for one field.
In Solr, fields that can have more than one value per document are called multi-valued fields.
In schema.xml, you declare a multi-valued field by setting multiValued="true" on the field definition:
When adding a document that has multiple links, you simply add multiple "link" fields in the XML document, as is depicted in listing 5.4
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A Reuse the same field name to populate a multi-valued field during indexing When searching, you can query for link:"http://manning.com/" and Solr will look.
So far, our micro-blog documents have only a small number of fields, which made it easy.
In practice, not all documents are so simple or sparse.
Let’s look at another type of field, called dynamic fields, that help deal with larger and more complex document structures.
In other words, dynamic fields uses a special naming scheme to apply the same field definition to any fields that match a glob-style pattern.
Dynamic fields help address a few common problems that occur when building search applications, including:
Let’s look at each one of these use cases to get a good feel for what you can do with dynamic fields.
However, you should note that you do not need to use dynamic fields with Solr.
It’s perfectly acceptable to not use dynamic fields if none of these use cases applies to your application.
Also, Solr ignores the dynamic field definitions in your schema.xml until you start indexing documents that make use of them.
Thus, in practice, most Solr users keep the extensive list of dynamic fields provided with the Solr example schema so they are there when you need them, but are simply ignored otherwise.
Moreover, each of these fields are both stored and indexed.
That is, other than the name, each field definition is exactly the same.
Now imagine that in addition to these three fields, we have dozens of "string" fields that are also stored and indexed.
Of course you can type in an explicit definition for each field or you can define a single <dynamicField> element to account for all of these string fields using a suffix pattern on the field name:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You can also use dynamic fields for multi-valued fields, such as our links field in the previous section.
Of course, if your documents don’t have a common base schema, then they should probably not be in the same index! In our social media example, if we index documents from Twitter, Facebook, YouTube, and Google+, then documents from each of these sources will have some fields that are unique to each social network.
We think it’s more intuitive and maintainable to handle these source-specific fields as dynamic fields.
For example, instead of defining many fields for each source as in the example below:
A Many Facebook specific string fields that are stored and indexed #B Many Twitter specific string fields that are stored and indexed.
You can accomplish the sample using a single "string" dynamic field with the "*_s" suffix pattern as the name:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
New social networks seem to come online everyday so we wouldn’t want to re-work our schema.xml just to handle documents from these new sources.
With dynamic fields, you can include new fields introduced by your new document source without making any changes to the schema.xml.
For example, suppose you wanted to add support for a new social network that includes a field that captures the phase of the moon when the content was posted to the network (perhaps it’s a dating site)
Lastly, although dynamic fields can be a handy feature on the indexing side, there’s no real magic on the query side.
When querying for documents that were indexed with dynamic fields, you must use the full field name in the query.
In other words, you cannot formulate queries to find a match in all string fields by querying with a prefix or suffix pattern: *_s:coffee.
However, if you want to find matches in more than one field, dynamic or static, then Solr provides a clever way to do that with copy fields.
In Solr, copy fields allow you to populate one field from one or more other fields.
Specifically, copy fields support two use cases that are common in most search applications:
The intent of this approach is to help your users find documents quickly without having to fill-out a complicated form; think about how successful a simple search box has been for Google.
In our tweet example, you might think it’s an easy decision—just search the tweet text and you’re done.
However, with this approach, users would not find our example tweet if they searched for "@thelabdude" because that information is contained in the.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In addition, if a tweet contains shortened bit.ly-style URLs, then searches in the text field for the actual "resolved" URL will not match as those are stored in the links field.
What we really want here is a catch-all search field that contains the text from the screen_name, text, and resolved link fields.
Thankfully, Solr makes it easy to create a single catch-all search field from many other fields in your document using the <copyField> directive.
First, you need to define a destination field that other fields will be copied into—let’s name this field catch_all:
A Catch all field should not be stored as it is populated from another field #B Destination field must be multivalued if any of the source fields are multi-valued.
This looks like any other field except there are two important aspects to the definition.
First, notice that this field is not stored (stored="false"), which makes sense because we probably don’t want to display a blob of many fields concatenated together to our users.
In fact, even if you wanted to do this, you can’t because there really is no original value for Solr to return for copy fields.
Remember that Solr returns the original value for a stored field.
In our case, the link field is multi-valued, so we must define our copy field as multi-valued as well.
Now that we’ve defined the destination field, we need to tell Solr which fields to copy from using the <copyField> directive.
Take a moment to think about why this is the case.
The best way to make sense of this is that you must define the source and destination fields first and then you connect them with the copy field directive after all fields have been defined.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
For instance, as we’ll see in chapter 6, stemming is a technique that transforms terms into a common base form, known as a "stem", in order to improve recall.
With stemming, the terms "fishing", "fished" and "fishes" all have a common stem of "fish"
Thus, stemming can help your users find documents without having to think about all the possible linguistic forms of a word, which is a good approach for general text search fields.
On the other hand, consider how stemming would affect a type-ahead suggestion box (auto-suggest)
In this case, stemming would work against your users in that you could only suggest the stemmed values and not the full terms.
For example, with stemming enabled, your search application would not be able to suggest "humane" or "humanities" when the user started typing "human" in the auto-suggest box.
Solr copy fields give you the flexibility to enable or disable certain text analysis features like stemming without having to duplicate storage in your index.
In this case, the text field has field type "stemmed_text" which presumably means the text is stemmed.
Under the covers, Solr sends the raw, un-analyzed contents of the text field to the auto_suggest field, which allows a different text analysis strategy.
To reiterate, you cannot return the original value of the auto_suggest field in search results so it has stored="false"
In section 5.2.2, we discussed how it is a good idea to make your documents uniquely identifiable in the index using some unique ID value.
To recap, if you provide a unique identifier field for each of your documents, then Solr will avoid creating duplicates during indexing.
In addition, if you plan to distribute your Solr index across multiple servers, then you must provide a unique identifier for your documents.
For these reasons, we recommend defining a unique identifier for your documents from the start.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In fact, we’ve seen instances where Solr does not return results correctly if you don’t use "string" as the type for text-based keys.
So save yourself some trouble and just use string or one of the other primitive field types for your unique key field.
At this point, we’ve covered all the basic aspects of defining fields in schema.xml.
It’s now time to dig into the next major section of Solr’s schema.xml to learn how to define field types.
In this section, we learn to define field types for handling structured data like dates, language codes, and usernames.
In chapter 6, we learn how to define field types for text fields like the body text of our example tweet.
In general, Solr provides a number of built-in field types for structured data, such as numbers, dates, and geo-location fields.
Figure 5.4 shows a class diagram of some of the more commonly used field types in Solr.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Let’s begin our discussion of field types for non-text data by looking at one of the most common field types: string.
For our example tweet, in addition to the text field, we decided the screen_name, type, timestamp, and lang should also be indexed fields.
Now we need to decide the appropriate type for each field.
It turns out that each of these fields contains structured data that does not need to be analyzed.
For example, the lang field contains a standard ISO-639-1 language code used to identify the language of the tweet, such as "en"
Users can query the lang field to find English tweets as shown in Figure 5.5
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 5.5 Mirco-blog search application Web form used to find documents using structured "non-text" fields.
Since the language code is already standardized, we don’t want Solr to make any changes to it during indexing and query processing.
Solr provides the string field type for fields that contain structured values that should not be altered in any way.
Here is how the string field type is defined in schema.xml:
Behind the scenes, all field types are implemented by a Java class, in this case.
The sortMissingLast and omitNorms attributes are advanced options that we’ll discuss in more detail in section 5.4.4
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
If we use the string field type for our lang field, Solr will take the value "en" from our document and store it in the index unaltered as "en" during indexing.
At query time, you also need to pass the exact value "en" to match English documents.
In Figure 5.5, the user selected "English" which needs to be translated into "en" when processing the form.
A common approach to searching on date fields is to allow users to specify a date range.
On the query side, this would be a range query on the timestamp field:
A "trie" is an advanced tree-based data structure that allows for efficient searching for numeric and date values by varying degrees of precision.
During indexing, Solr needs to know how to parse a date.
Recall from section 5.1, the example XML document we sent to Solr for indexing included the timestamp field as:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
If you send Solr a date in some other format, you will get a validation error during indexing and the document will be rejected.
This goes back to understanding how your users need to query using dates.
For example, if your users only expect to query for documents by day, then there’s no point in indexing a date with second or millisecond precision.
On the other hand, if you need to sort documents by date, then hour-level granularity may be too coarse, in which case you may want to do minute-level granularity.
During indexing, Solr supports Date Math operations to help you achieve the correct precision for a date field with little effort.
For example, let’s say you decide that you only need to index your dates at the hour-level of granularity.
This saves space in your index, but also means that users cannot get more specific than hour ranges when searching.
When indexing, you can send your date with /HOUR on the end; the / tells Solr to "round-down" to a specific granularity.
Let’s see how we can index our example tweet with hour granularity:
Lastly, it should be noted that the tdate field is a good choice for fields that you need to do date range queries on but it comes at the cost of requiring more space in your index because more tokens are stored per date value.
According to the Solr JavaDocs, a precisionStep="6" is a good value for long fields which is how Solr stores dates in the index.
We dive into the details of how to choose the right precisionStep in section 5.4.4 below.
For the most part, numeric fields behave as you would expect in Solr.
This is not an intuitive field to search by but is useful from a display and sorting perspective.
In other words, you can imagine users wanting to sort matching tweets by this field to see content from more popular authors.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Because we don’t need to support range queries on this field, we chose precisionStep="0" which works best for sorting without incurring the additional storage costs associated with a higher precision step used for faster range queries.
Also, note that you shouldn’t index a numeric field that you need to sort by as a string field because Solr will do a lexical sort instead of a numeric sort if the underlying type is string-based.
Up to this point, we’ve discussed the main concepts for indexing fields that contain.
We’ll return to specific cases for these types of non-text fields in later chapters.
For now, let’s wrap-up this section with a short discussion of some of the advanced configuration options for field types.
Solr supports optional attributes for field types to enable advanced behavior.
Table 5.4 Overview of advanced attributes for field type elements in schema.xml.
Let's take a closer look at precisionStep as that is a common source of confusion for.
You can safely skip the following discussion and come back to this after you have your search application implemented and are looking for ways to improve performance of sorting and range queries.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
As we discussed in section 5.4.3, Solr uses a trie data structure to support efficient range queries and sorting of numeric and date values.
Let's learn how to choose the best value for precisionStep to support range queries and sorting in your Solr instance.
First, decide if you even need to worry about precisionStep by asking whether you have any numeric or date fields in your index that users would like to find documents across a range of values in those fields.
For each of these fields, think about the range of possible values that will be indexed—are there potentially millions of unique values or just a handful? In Solr terminology, the number of unique values in a field as called the "cardinality" of the field.
For example, consider a Solr index for finding houses for sale across the United States.
Homebuyers typically search for houses in a specific area and price range.
Home price seems like a good example of a field that needs to support efficient range searches.
Next, we need to decide on the best field type for listing price.
You can imagine that most house prices will be rounded to the nearest dollar, as it's rare to see a home price with cents, so an int or long field should suffice.
Keep in mind that you want to be as frugal with your field types as possible, i.e.
This reduces the size of your index on disk and reduces memory usage during searching and sorting.
We can define a field for home listing price as:
In the Solr example schema.xml, the "tint" field type is defined as:
The intuition behind a trie-based field is that Lucene generates multiple terms for each value in the field, where each term has less precision.
In other words, two different home prices will have overlapping terms at lower precisions.
Lucene does this to reduce the number of terms that have to be matched to satisfy a range query.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Let's compare this to using precisionStep="4" for listing price to see the impact of using a smaller step size.
In general, a smaller precision step leads to more terms being indexed per value, which increases the size of your index.
However, more terms also equates to faster range queries because Lucene can narrow the search space quicker with more terms.
The intuition here is that Lucene can search the center of a range using the lowest possible precision in the trie.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Notice that the index sizes differ by about 8-bytes per document, which agrees with table.
To summarize, when selecting a precision step, you have to balance space considerations with range query performance.
For our TrieInt listing price field, a precision step of 4 leads to more terms being indexed per unique price but slightly faster range searches, especially when the cardinality of a field is large (many unique values)
We now have enough background on the indexing process to begin adding documents to Solr.
In this section we learn how to send documents to Solr for indexing and get a glimpse of what happens behind the scenes.
At the end of this section, you’ll be able to start indexing documents to Solr from your application.
Let’s begin by learning how to index the example tweets we’ve been working with in this chapter.
As we touched on in section 5.1, Solr allows you to add documents using a simple XML document structure.
Listing 5.12 shows this structure for the two example tweets we used previously in this chapter.
However, in this case, we changed the names of the fields to use dynamic fields.
We do this for convenience as you can add these documents to the example Solr server without making any changes to schema.xml.
If you were building a real application, then you may want to declare the fields from listing 5.3 explicitly in your schema.xml, but using dynamic fields works fine for this example.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Let’s send this XML document to Solr to index these two tweets.
The Solr example includes a simple command-line application that allows you to post XML documents into the example server.
Open a command-line on your workstation and execute the following commands as seen in listing 5.13
The two example tweets should now be indexed in your example Solr server.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Behind the scenes, the post.jar application sent the XML document over HTTP to Solr’s update handler at URL: http://localhost:8983/solr/collection1/update.
The update handler supports adding, updating, and deleting documents; we cover the update request handler in more detail in section 5.6 below.
Beyond XML, Solr’s update request handler also supports the popular JSON (JavaScript Object Notation) and CSV (comma-separated values) data formats.
For example, instead of indexing our example tweets using XML, we could have used JSON as shown in listing 5.14
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
If you walked through both exercises of adding the XML and JSON documents, then you might think there are 4 documents in your index.
However, because we are using the "id" field in our documents, there will only be 2 documents in the index.
Verify this by yourself by re-issuing the type_s:post query as before.
This demonstrates how Solr will update an existing document using the unique key field, which in the example schema.xml is "id"
If you look closely at the output from the post.jar application, you’ll notice that it also sends a commit to Solr after POSTing the documents.
Regardless of how you send documents to Solr, they are not searchable until they are committed.
The commit process is quite involved and is covered in detail in section 5.6 below.
Let’s continue our discussion of how to index documents by learning about a popular Java-based client for Solr called SolrJ.
SolrJ is a Java-based client library provided with the core Solr project to communicate with your Solr server from a Java application.
In this section, we’ll implement a simple SolrJ client to send documents using Java.
If you’re not a Java developer or your application is not written in Java, then you’ll be happy to know there are many other Solr client libraries.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
For a complete list of client libraries, see the Integrating Solr page in the Solr wiki3
Listing 5.15 provides a simple example using SolrJ to add our two example tweet documents to the index and then doing a hard commit.
After committing, the example code sends the match all docs query (*:*) to Solr to get the documents we indexed back in the search results.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
As you can see from this basic example, the SolrJ API makes it very easy to connect to Solr, add documents, send queries, and process results.
To begin, all you need is the URL of the Solr server, which in our example was http://localhost:8983/solr/collection1
Behind the scenes SolrJ uses the Apache HttpComponents Client library to communicate with the Solr server using HTTP.
In section 5.5.1, we saw how Solr supports XML and JSON, so you may be wondering if SolrJ is using one of those formats to connect to Solr.
It turns out that SolrJ actually uses an internal binary protocol called javabin by default.
When doing Java-to-Java communication, the javabin protocol is more efficient than using XML or JSON.
In listing 5.15, each invocation of the add method resulted in a.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
We’ve seen how we can send documents to Solr using basic HTTP POST using the post.jar application and using the popular SolrJ client from Java.
These are not the only ways to get your documents into Solr.
Being a mature, widely-adopted open source technology, Solr offers a number of powerful utilities for adding documents from other systems.
In this section we introduce you to three popular tools available for populating your Solr index:
Each of these tools is powerful and could easily justify taking an entire chapter to describe each tool.
So for now, we just want to give brief mention of these tools so that you are aware of these options for populating your index.
At a high-level, you provide the database connection parameters and a SQL query to Solr and the DIH component queries the database and transforms the results into documents.
We cover the DIH in detail in chapter 12, so for now let’s look at another tool for indexing rich binary documents like PDF and MS Word documents.
Behind the scenes, Solr Cell uses the Apache Tika project to do the extraction.
Specifically, Tika provides components that know how to detect the type of document and then parse the binary documents to extract text and metadata.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Thus, if your application needs to crawl hyper-linked pages on a massive scale, then Nutch is probably a good place for you to start.
Now that you've seen how to send documents to Solr for indexing, let's learn how those request are actually processed in Solr using a component called the update handler.
In the previous section, we sent new documents to Solr using HTTP POST requests.
The request to "add" these new documents was handled by Solr's update handler.
In general, the update handler processes all updates to your index as well as commit and optimize requests.
Table 5.8 provides an overview of common request types supported by the update handler.
Table 5.8 Overview of common requests processed by the update handler.
Add Add one or more documents to the index, see listing 5.12 for a full example.
Delete Delete a document by ID, such as deleting document with ID=1
Atomic Update Update one or more fields of an existing document using optimistic locking; see section 5.6.4 below.
Commit Commits documents to the index with options to do a soft or hard commit and whether to block on the client until the new searcher is open and warmed.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Optimize Optimize the index by merging segments and removing deletes.
Although table 5.8 shows examples of update requests using XML, the update request handler supports other formats, such as JSON, CSV, and javabin.
Behind the scenes, the update request handler looks at the Content-Type HTTP header to determine the format of the request, such as Content-Type: text/xml.
Listing 5.16 shows the configuration of the update handler in solrconfig.xml:
One of the most important tasks performed by the update handler is to process requests to commit documents to the index to make them visible in search results.
In this section, we dig into the details of how Solr makes documents available for searching by committing them to the index.
When a document is added to Solr, it will not be returned in search results until it is committed to the index.
In other words, from a query perspective, a document is not visible until it is committed.
In Solr 4, there are two types of commits: soft and normal or sometimes called "hard" commits.
Let’s look at how normal commits work as that will help you understand soft commits.
For our purposes, you can think of a searcher as a read-only view of all committed documents in the index.
Refer to chapter 4.3 for a detailed discussion of how a searcher works.
For now, let it suffice to say that a hard commit can be an.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
After a normal commit succeeds, the newly committed documents are safely persisted to durable storage and will survive server restarts due to normal maintenance operations or a server crash.
For high availability, you still need to have a solution to fail-over to another server if the disk fails.
For now, you can think of a soft commit as a mechanism to make documents searchable in near real-time by skipping the costly aspects of hard commits, such as flushing to durable storage and warming a new searcher.
As soft commits are less expensive, you can issue a soft commit every second to make newly indexed documents searchable within about a second of adding them to Solr.
However, keep in mind that you still need to do a hard commit at some point to ensure documents are eventually flushed to durable storage.
To summarize, a hard commit makes documents searchable but is expensive because it has to flush documents to durable storage and warm-up a new searcher.
In contrast, a soft commit also makes documents searchable but they are not flushed to durable storage and a new searcher is not warmed up.
AUTO-COMMIT For either normal or soft commits, you can configure Solr to automatically commit documents using one of three strategies.
Commit all documents once a user-specified threshold of uncommitted documents is reached.
Solr's auto-commit behavior for hard and soft commits is configured in solrconfig.xml.
When performing an auto-commit, the normal behavior is to open a new searcher.
In this case, the documents will be flushed to disk, but will not be visible in search results.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In this scenario, it may make sense to only pay the penalty of warming up a new searcher once, after all documents are indexed rather than warming up a new searcher 100 times.
Of course, your client application can also send an intermittent hard commit request every 1M documents so that some documents are visible in search results sooner.
The main point is that you want to think about whether you need to open a new searcher after every auto-commit.
A new searcher is always opened and warmed when you send a <commit> request; the waitSearcher attribute indicates whether your client code should block until the new searcher is fully warmed up.
You can also configure Solr to do soft-commits automatically using the <autoSoftCommit> element in solrconfig.xml.
However, you will want to use much smaller values for soft commits, such as every second (1000 ms) as shown in the XML snippet below.
A Do a soft commit every second (1000 ms) Now let's turn our attention to another powerful feature of the update handler that helps.
Solr uses a transaction log to ensure updates accepted by Solr are saved on durable storage until they are committed to the index.
Imagine the scenario where your client application sends a commit every 10,000 documents.
If Solr crashes after the client sends some documents to be indexed but before your client sends the commit, then without a transaction log, these un-committed documents will be lost.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A Default location is tlog sub-directory of data Every update request is logged to the transaction log.
The transaction log continues to grow until you issue a commit.
During a commit, the active transaction log is processed and then a new transaction log file is opened.
Figure 5.7 illustrates the steps involved in processing an update request.
Figure 5.7 Sequence of events and main components used to process update requests, such as adding a new document.
A few of the components in figure 5.7 should be familiar to you, such as the request dispatcher and response writer.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Let's walk through the sequence of events in figure 5.7 to highlight some of the important concepts:
The client can send the request as JSON, XML, or Solr's internal binary javabin format.
We saw an example client built using SolrJ in listing 5.15
Solr's request dispatcher uses the "collection1" part of the request path to determine the core name.
Next, the dispatcher locates the /update request handler registered in solrconfig.xml for the collection1 core.
When adding or updating documents, the update handler uses schema.xml to process each field in each document in the request.
In addition, the request handler invokes a configurable chain of update request processors to perform additional work on each document during indexing.
We'll see an example of this in chapter 6 where we use an update request processor to do language detection during indexing.
Once the update request is securely saved to durable storage, a response is sent to the client application using a response writer.
At this point, the client application knows the update request is successful and can continue processing.
With the transaction log, your main concern is balancing the trade-off between the length of your transaction log, i.e.
If your transaction log grows very large, then a restart may take a long time to process the updates, thus delaying your recovery process.
In contrast, committing micro-blog documents like we used in this chapter every 100,000 documents would not be a problem.
The key take-away is that you need to consider the size of your transaction log when configuring your auto-commit settings.
However, this implies that at some point, your client application must issue a full hard commit to make all updates visible in search results.
You can update existing documents in Solr by sending a new version of the document.
However, unlike a database where you can update a specific column in a row, with Solr you must update the entire document.
Behind the scenes, Solr deletes the existing document and creates a new one; this occurs whether you change one field or all fields.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
From a client perspective, your application must send a new version of the document in its entirety.
For some applications where documents can be created from other sources, this is not such a big deal.
For others that use Solr as a primary data store, re-creating a document in its entirety just to update a single field can be problematic.
In practice, this requires users to query for the entire document, apply the updates, and then send the fully specified document back to Solr.
This pattern of requesting all fields for an existing document, updating a subset of fields, and then sending the new version to Solr is very common in practice.
Consequently, atomic updates are new feature in Solr that allow you to send updates to only the fields you want to change.
This brings Solr more inline with how database updates work.
Solr will still delete and create a new document, but this is transparent to your client application code.
FIELD-LEVEL UPDATES Returning to our micro-blog search example, let's imagine that we want to index a new field on existing documents that holds the number of times the tweet has been re-tweeted.
We'll use this new field as an indication of popularity of a tweet.
To keep things simple, we'll update this field once a day.
You can imagine a daily volume statistic would also be useful but we'll just stick with an aggregated value to keep things simple.
Let's name our new field retweet_count_ti, which indicates we are using a dynamic field so we don't have to update the schema.xml to add this new field.
The "_ti" suffix applies the following dynamic field (from schema.xml):
A Slightly un-intuitive but updates must be wrapped in an <add> element.
It follows that all fields must be stored for this to work because the client application is only sending the id field and the new field.
All other fields must be pulled from the existing document.
We used the update="set" directive to set the retweet_count_ti field.
Alternatively, since our update process runs daily, we can just count them for the previous day and increment the existing value using update="inc"
In addition to set and inc, you can also use "add" to append a new value to a multi-valued field.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In a nutshell, we'll pay users to classify each document as either being positive, neutral, or negative.
The sentiment field could be useful for allowing users to find negative information about a product or restaurant.
Once classified, each micro-blog document needs to be updated in Solr with the sentiment label.
In our re-tweet count example, we updated the retweet_count_ti field once a day using an automated process.
However, with sentiment classification, updates to the sentiment_s field can happen at anytime.
Thus, it's conceivable that two users will attempt to update the sentiment label on same document at the same time.
Of course we could implement some cumbersome process that requires users to explicitly lock a document before labeling, but that would slow them down unnecessarily.
Also, we probably don't want to pay for a document to be classified twice.
Hence, we need some way to guard against concurrent updates to the same document—enter optimistic concurrency control.
To avoid conflicts, Solr supports optimistic concurrency control using a special version tracking field, named _version_
The special version field should be defined in your schema.xml as:
When a new document is added, Solr assigns a unique version number automatically.
When you need to guard against concurrent updates, you simply include the exact version the update is based on in the update request.
Consider the following update request that includes a specific _version_:
When Solr processes this update, it will compare the _version_ value in the request with the latest version of the document, pulled from either the index or the transaction log.
If they do not match, then the update request fails and an error is returned to the user.
A client application can handle the error response to let the user know the document was already classified by another user.
This approach is called "optimistic" because it assumes that most updates will work on the initial attempt and that conflicts are rare.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Consequently, real-time get and atomic updates rely on the transaction log being enabled for your index.
Solr gives you a few other options for handling concurrent updates with the _version_ field.
As illustrated by this simple example, atomic updates are a powerful new addition to.
With Solr 4, you can now update existing documents simply by sending the fields that need to be updated along with the unique identifier of the document to update.
In chapter 4, we delayed a discussion of index management settings in solrconfig.xml until you had more background with Solr indexing.
You are now ready to tackle Solr's index management settings.
In this section, we focus on the index-related settings that you are most likely to need to change, beginning with how indexed documents are stored.
It should be said that most of the index-related settings in Solr are for expert use only.
What this really means is that you should take caution when making changes and that the default settings are appropriate for most Solr installations.
When documents are committed to the index, they are written to durable storage using a component called a Directory.
The Directory component provides the following key benefits to Solr:
Hides details of reading from and writing to durable storage, such as using JDBC to store documents in a database.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Implements a storage-specific locking mechanism to prevent index corruption, such as OS-level locking for file system based storage.
Enables extending the behavior of a base Directory implementation to support specific use cases like near real-time search.
Solr provides several different Directory implementations and there is no one best Directory implementation for all Solr installations! Thus, you will need to do some research to decide on the best implementation for your specific application of Solr.
In practice, this depends on your operating system, JVM type, and use cases.
However, as you learned in chapter 4, Solr tries to be well-configured out-of-the-box.
Let's dig into how Solr's index storage is configured by default, which will help you decide if you need to change the default configuration.
The location of the data directory is controlled by the <dataDir> element in solrconfig.xml:
The solr.data.dir property defaults to "data" but can be overridden in solr.xml for each core, such as:
A Directory to store data for the collection1 core The first thing you need to consider is whether the data directory for your index has.
Also, it's important that your data directory supports fast reads and writes, with a little more priority given to read performance.
Strategies for optimizing disk I/O is beyond the scope of this book, but here are some simple pointers to keep in mind:
Each core should not have to compete for the disk with other processes.
If you have multiple cores on the same server, it's a good idea to use separate physical disks for each index.
Use high quality, fast disks or even better, consider using Solid State Drives (SSD) if your budget allows.
Spend some quality time with your system administrators to discuss RAID options for your servers.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 5.8 Solr Directory implementation selected at runtime depending on your specific operation system version and JVM type.
Based on figure 5.8, there are three possible file system based Directory options for Solr:
MMapDirectory: Uses memory-mapped I/O when reading the index; best option for installations on 64-bit Windows, Solaris, or Linux operating systems with the Sun JVM.
SimpleFSDirectory: Uses a Java RandomAccessFile; should be avoided unless you are running on 32-bit Windows.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
NIOFSDirectory: Uses java.nio optimizations to avoid synchronizing reads from the same file; should be avoided on Windows due to a long-standing JVM bug.
You can determine which Directory implementation is enabled for your Solr server using the Core Admin page on the Solr administration console.
You can override the default selection by explicitly setting the directory factory in solrconfig.xml.
If we want to change that to use MMapDirectory, then solrconfig.xml should be changed to:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A segment is a self-contained, read-only sub-set of a full Lucene index; once a segment is flushed to durable storage, it is never altered.
When new documents are added to your index, they are written to a new segment.
Consequently, there can be many active segments in your index.
Each query must read data from all segments to get a complete results set.
At some point, having too many small segments can negatively impact query performance.
Combining many smaller segments into fewer larger segments is commonly known as segment merging.
Let it suffice to say that optimize can be a very expensive operation in terms of memory, CPU, and disk I/O in Solr, especially for large indexes; it is not uncommon for a full optimization to take hours for a large index.
One of the most common questions on the Solr user mailing list is whether to "optimize" your index.
This is understandable because who doesn't want an "optimized" index? However, current wisdom in the Solr community suggests that rather than optimizing your index, it is better to fine-tune Solr's segment merge policy.
Moreover, having an optimized index doesn't mean that a slow query will suddenly become fast.
Conversely, you may find that query performance is acceptable with an un-optimized index.
EXPERT-LEVEL MERGE SETTINGS By default, all the segment-merging settings are commented out in solrconfig.xml.
This is by design because the default settings should work for most installations, especially when you're just getting started.
You should also notice that each element is labeled as an "expert" level setting.
Table 5.10 provides an overview of segment merge related elements from solrconfig.xml.
This should not be confused with commits, which force all buffered documents to be written to durable storage.
Increase this value to buffer more documents in memory and reduce disk I/O during indexing.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Determining the best value for your index depends on average document size, available RAM, and desired indexing throughput.
To be clear, just because these expert-level settings are commented out, segment merging is still enabled in your index, running in the background5
For now, we recommend that you avoid optimizing and just use the default configuration for segment merging until you have a good reason to change these settings.
It's very likely that "doing nothing" when it comes to segment merging is the right approach for your server6
If indexing throughput becomes an issue for your application, then you can revisit these settings.
Unfortunately, we're not able to be more specific when it comes to tuning the merge process because it depends on so many environment specific factors.
This implies that deletes do not actually delete documents from existing segments.
It turns out that deleted documents are not removed from your index until segments containing deletes are merged.
In a nutshell, Lucene keeps track of deletes in a separate data structure and then applies the deletes when merging.
For the most part, you don't have to worry about how this works.
At this point you should have a good understanding of the Solr indexing process.
To recap, we began the chapter by learning about the schema design process.
Specifically, we discussed considerations about document granularity, document uniqueness, and how to determine if a field should be indexed, stored, or both.
Next, we learned how to define fields in schema.xml, including multi-valued and dynamic fields.
We saw how dynamic fields are useful for supporting documents with many fields and documents coming from diverse sources.
You also learned how to use Solr’s <copyField> directive in order to populate a catch-all text search field or to apply different text analysis to the same text during indexing.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Next, we saw how to work with structured data using Solr’s support for strings, dates, and numeric field types.
We used Solr’s round-down operator (/) to index date values at different precisions, such as hour-level precision using /HOUR.
We also learned that Solr provides Trie-based fields to support efficient range queries and sorting on numeric and date fields.
We saw how to use the precisionStep attribute for numeric and date fields to balance the trade-off between having a larger index size and range query performance.
Armed with an understanding of schema.xml, we learned how to send XML and JSON documents to Solr using HTTP and SolrJ.
We briefly introduced some additional tools provided by Solr for importing documents from a relational database (DIH) and indexing rich binary documents like PDF and MS Word using the extracting request handler.
After documents are processed, they need to be committed before they can be searched using either normal commits or soft commits for near-real-time search.
We showed how Solr uses a transaction log to avoid losing un-committed updates.
Beyond adding new documents, you learned how to update existing documents using Solr's atomic update support.
You can guard against concurrent updates using optimistic concurrency control using the special _version_ field.
We closed out this chapter by returning to a discussion of index-related settings from solrconfig.xml.
Specifically, we showed you where and how Solr stores the index using a Directory component.
You also learned about segment merging and that it is a good idea to avoid optimizing your index or changing segment merge settings until you have a better understanding of your indexing throughput requirements.
In the next chapter, we continue learning about the indexing process by diving deep into text analysis.
After finishing chapter 6, you will have a solid foundation for designing and implementing a powerful indexing solution for your application.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In Chapter 5, we learned how the Solr indexing process works and learned to define nontext fields in schema.xml.
In this chapter, we get a little deeper into the indexing process by learning about text analysis.
Text analysis removes the linguistic variations between terms in the index and terms provided by users when searching, so that a user querying for “buying a new house” matches documents with terms “home” and “purchase”
In this chapter, you’ll learn, for example, how to configure Solr to establish a match between queries containing “house” and documents containing “home”
When done correctly, text analysis allows your users to query using natural language without having to think about all the possible forms of their search terms.
You don’t want your users to have to construct queries like: “buying house or purchase home or buying a home or purchasing a house …”
Allowing users to find information they seek using natural language is fundamental to providing a good user experience.
Given the broad adoption and sophistication of Google and similar search engines, users are conditioned to expect search engines to be very intelligent and intelligence in search starts with great text analysis!
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The state-of-the-art of text analysis goes well beyond removing superficial differences between terms to address more complex issues like language-specific parsing, part-ofspeech tagging, and lemmatization.
Don’t worry if you’re not familiar with some these terms as we’ll cover them in more detail below.
What’s important is that Solr has an extensive framework for doing basic text analysis tasks, such as removing very common words, known as stop words, as well as doing more complex analysis tasks.
To accommodate such power and flexibility, Solr’s text analysis framework can seem overly complex and daunting to new users.
As we like to say, Solr makes solving very difficult text analysis problems possible and simple tasks a little too cumbersome.
However, after working through this chapter, we’re confident that you’ll be able to harness this powerful framework to analyze most any content you’ll encounter.
The main goal of this chapter is demonstrate how Solr approaches text analysis and to help you think about how to construct analysis solutions for your documents.
To this end, we’ll tackle a somewhat complex text analysis problem to demonstrate the mechanics and strategies you need to be successful.
Specifically, we’ll cover these fundamental components of text analysis with Solr:
Basic elements of text analysis in Solr: analyzer, tokenizer, and chain of token filters.
Defining a custom field type in schema.xml to analyze text during indexing and query processing.
Common text analysis strategies such as removing stop words, case-folding, synonym expansion, and stemming.
Once we have a solid understanding of the basic building blocks, we’ll tackle a harder analysis problem to exercise some of the more advanced features Solr provides for text analysis.
Specifically, we’ll see how to analyze micro-blog content from sites like Twitter.
Tweets present some unique challenges that require us to think hard about how users will use our search solution.
Collapse repeated characters down to a maximum of two in terms like “yummm”
Use a custom token filter to resolve shortened bit.ly style URLs.
To recap, we’re designing and implementing a solution to search micro-blogs from popular social media sites like Twitter.
Since the main focus of this chapter is on text analysis, let’s take a closer look at the text field in our example micro-blog document.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
As was discussed in the introduction above, a primary goal of text analysis is to allow your users to search using natural language without having to worry about all the possible forms of their search terms.
In Figure 6.1, the user searched the text field for “San Francisco north beach coffee”, which is a natural query to expect given all the great coffee houses in North Beach; maybe our user is trying to find a great place to drink coffee in North Beach by searching social media content.
We assert that our example tweet should be a strong match for this query even though the exact terms “San Francisco”, “north beach”, and “coffee” do not occur in our sample tweet.
Yes, “North Beach” is in our document, but case matters in search unless you take specific action to make your search index case-insensitive.
We assert that our example document should be a strong match for this query because of the relationships between terms used in the user’s search query and terms in the document shown in table 6.1:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Table 6.1 Query terms that match terms in our example tweet.
Figure 6.2 shows the transformations we’ll make to the text using Solr’s text analysis framework, which for now you can think of as a black box.
In the remaining sections of this chapter, we’ll open up the black box to see how it works.
Figure 6.2 Tweet text transformed into a more optimal form for searching.
Each box indicates a unique term in the Solr index after text analysis process is applied to the text.
The extra space between some terms indicates stop words that were excluded from the index.
Can you spot all the transformations that were applied? Table 6.2 provides a summary of the key transformations that were applied to the text using various Solr text analysis tools that we’ll go through in detail below.
Notice that each transformation taken individually is usually quite simple but collectively they make a big difference in improved user experience with your search application.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Table 6.2 Overview of the transformations made to the micro-blog text using Solr’s text analysis tools.
Notice how all transformations are performed using built-in Solr text analysis tools, i.e.
Don’t worry if some of the Solr class names look a bit daunting as we’ll cover each one of.
For now, let’s consider a few of the interesting transformations occurring on this text, all of which are provided by built-in Solr tools.
Solr also allows you to replace characters and terms using regular expressions.
For instance, repeating letters in a word is very common in social media content like tweets in.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
However, from a search perspective, “yummm” and “yumm” are basically equivalent so we can reduce the number of unique terms in our index by collapsing these repeated letters down to a maximum of two.
Later in the chapter, we’ll see how to use regular expressions with Solr to make this transformation.
It’s worth highlighting that all of the transformations shown above were provided by built-in Solr tools, meaning that we only had to configure them and didn’t write any Java code.
While Solr’s built-in arsenal is powerful, sometimes you need to extend its capabilities.
We’ll see how to do this with Solr’s Plug-In framework to deal with bit.ly-style shortened URLs in social media content in section 6.4.3
At this point, you should have a good feel for where we are headed with text analysis in Solr and might be wondering how to get started.
In other words, now that we know Solr provides all these great tools to transform text, how do we actually apply these tools to our documents during indexing? In the next section, we’ll start to work with field types for doing text analysis.
The example schema provided with Solr defines an extensive list of field types applicable for most search applications.
Of course, if none of the pre-defined Solr field types meets your needs, then you can build your own field type using the Solr Plug-In framework.
We’ll see an example of the Solr Plug-In framework in the last section of this chapter.
If all your fields contained structured data like language codes and timestamps, you actually wouldn’t need to use Solr since a relational database is very efficient at indexing and searching “structured” data.
Consequently, the example Solr schema pre-defines a number of powerful field types for analyzing text.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A Use a short, descriptive name based on the type of data #B Define the analyzer to use for indexing documents.
For fields that handle text data, you should specify the value of the class attribute to be solr.TextField.
This tells Solr that you want the text to be analyzed.
Also, you should use a name that gives a clue about the type of text that will be analyzed using this field type; for example, text_general is a good all-purpose type for when you don’t know the language of the text you are analyzing.
Take a moment to think about why you might use different analyzers for indexing and querying.
It turns out that you often need to do some additional analysis for processing queries than is needed for indexing a document.
For example, adding synonyms is typically done during query text analysis only to avoid inflating the size of your index and to make it easier to manage synonyms.
Although you can define two separate analyzers, the analysis applied to query terms must be compatible with how the text was analyzed during indexing.
For example, consider the case where an analyzer is configured to lowercase terms during indexing but does not lowercase query terms; users searching for “North Beach” would not find our example tweet because the index contains the lowercased forms “north” and “beach”
Technically, there is also a pre-processing phase before tokenization where you can apply character filters.
We’ll discuss character filtering in more detail in section 6.3.1, so for now let’s concentrate on tokenization and token filters.
In the tokenization phase, text is split into a stream of tokens using some form of parsing.
More common is the StandardTokenizer, which performs intelligent parsing to split.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
To define a tokenizer, you need to specify the Java implementation class of the factory.
In Solr, you must specify the factory class instead of the underlying Tokenizer implementation class because most tokenizers do not provide a default no-arg constructor.
By using the factory approach, Solr gives you a standard way to define any tokenizer in XML.
Behind the scenes, each factory class knows how to translate the XML configuration properties to construct an instance of the specific tokenizer implementation class.
All tokenizers produce a stream to tokens that can be processed by zero or more filters that perform some sort of transformation of the token.
Transformation – Changing the token to a different form such as lowercasing all letters or stemming.
Token injection – Adding a token to the stream, as is done with the synonym filter.
Token removal – Removing terms as is done by the stop word filter.
Filters can be chained together to apply a series of transformations on each token.
The order of the filters is important as you wouldn’t want to have a filter that depended on the case of your tokens listed after a filter that lowercases all tokens.
Let’s see this process in action to process our example tweet text, starting with the StandardTokenizer.
At this point, you should have a good understanding of the schema design process and.
The first step in basic text analysis is to determine how to parse the text into a stream of tokens using a tokenizer.
Let’s start by using the StandardTokenizer, which has been the classic go-to solution for many Solr and Lucene projects because it does a great job of splitting text on whitespace and punctuation but also handles acronyms and contractions with ease.
To see this tokenizer in action, let’s use it to parse our example tweet:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Not bad! The StandardTokenizer split the tweet successfully into a stream of 23 separate tokens.
Splits on whitespace and standard punctuation characters such as period, comma, semi-colon, etc.
Preserves Internet domain names and email addresses as a single token.
Next, let’s look at several common token filters provided by Solr to do basic text analysis.
Returning to our example tweet, there are a number of issues with the token stream that should be addressed before this text is added to the index.
First, there are a few extremely common terms such as “a” or “in” that only serve a grammatical purpose and add little value in differentiating one document from another.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Solr’s StopFilterFactory removes stop words from the token stream during analysis as.
Removing stop words during indexing helps reduce the size of your index and can improve search performance as it reduces the number of documents Solr has to rank for queries that contain stop words.
To analyze our example tweet, we defined the StopFilterFactory in listing 6.1 as:
Here are the English stop words included in the example Solr server:
Solr provides custom stop word lists for many languages in the lang sub-directory.
Google owns a patent on its approach to handling stop words where they include all stop words during indexing and selectively remove stop words from queries based on comparing sets of documents retrieved with and without using the stop words, see: http://www.google.com/patents/US7945579
Google’s patented approach to stop words is a great example of how search providers use advanced text analysis to achieve competitive advantage.
Even with something as simple as removing stop words, there is not a one-size fits all solution.
For example, terms starting with a capital letter in the middle of a sentence typically indicate a proper noun that can greatly improve precision of your results if users seek the proper.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
For example a user searching for “North Beach” is probably more interested in documents about the popular San Francisco neighborhood and would be less interested in a document containing “the waves are stronger on the north beach of the island”
However, using a more nuanced approach to lowercasing terms assumes that your users use the right case when searching.
In most cases, you’ll want apply the lowercase filter but still need to determine where in the filter chain to apply it.
If you have synonym list in all lowercase, then you’ll need to apply the lowercase filter before your synonym filter.
Figure 6.4 shows the resulting text after applying the stop word and lowercase filters to the example tweet:
Figure 6.4 Resulting text to be indexed after splitting using the StandardTokenizer and applying the stop word and lowercase filter.
The terms with red (x) are the stop words and would not be included in your index.
So now we have a basic text analysis solution for our sample tweet.
Let’s apply what we learned so far to see some actual text analysis in action.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The form also allows you to see if a query would match a sample document without having to actually index the document.
Let’s try a query “San Francisco coffee” to see if it matches our example tweet as we.
The easiest way to get started is to use Solr’s admin console, which provides a simple Web form to analyze your content.
Figure 6.5 How to find the link to the Analysis form from the Solr admin panel.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Once you enter the field type and text to analyze, click on the Analyse Values button to see the result.
Below the form, Solr reports the steps it takes to analyze the text for indexing using the text_general field type.
Notice how the text is first parsed with the StandardTokenizer, abbreviated as ST and then each token passes through the StopFilter (SF) and LowercaseFilter (LCF)
Next, let’s try a query against our example tweet to see if it is a match; enter “drinking a latte” in the text box labeled Field Value (query) as shown in the screen shot in figure 6.7
When you click on the Analyse Values button, Solr will highlight any terms in the document that match the query, in this case: “drinking” and “latte”
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 6.7 Search for “drinking a latte” on the analysis panel and Solr will highlight matching terms “drinking” and “latte” in the example document.
Next, enter “San Francisco drink cafe ipad” and click on the Analyse Values button again.
Although this is a nonsensical query, we would expect our sample document to be a match.
We’re using this nonsensical query to demonstrate that seemingly small differences in terms can lead to highly relevant documents being missed by your users.
Case in point, none of the query terms match the example document! The problem of course is that while it’s easy for a human to see the similarity between the terms in the query and the example document, to Solr, the terms have no relation to each other! We will use better text analysis to overcome this mismatch.
Overall, as a first pass, we resolved a number of text parsing and basic analysis issues with very little effort.
On the other hand, there are a number of outstanding issues that will make finding this tweet difficult for your users.
How many potential issues do you see in the text? Compare your results to figure 6.8:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 6.8 Remaining text analysis issues with our sample tweet after applying the lowercase and stop filters.
Unless you are indexing tweets or content from other social media sources, you may not encounter any of the analysis issues shown in figure 6.8
In general, the main point is that you need to study a representative sample of the documents in your index to determine the type of analysis needed as we’ve done here.
At this point, it should be clear that the basic text analysis provided by the text_general general field type is not sufficient to meet our needs.
Consequently, we need to implement a new custom field type building on the tools we’ve already learned about as well as learning a few new ones.
We left off having made good progress with analyzing social media text, but there were a.
In this section, we address these remaining issues by introducing a few more of Solr’s built-in text analysis tools.
To begin, since none of the predefined field types meet all of our needs, we will define a new custom field type in schema.xml.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Table 6.3 provides an overview of the new tools we’ll cover in this section.
Table 6.3 List of additional Solr text analysis tools needed to analyze micro-blog text.
KStemFilterFactory Stemming on English text; less aggressive than the Porter stemmer.
Don’t worry if this list of complicated names looks overwhelming, as we’ll work through each of these tools in the sections below.
Let’s start with a solution for removing repeated letters from words like “yummm” using a regular expression.
In Solr, a CharFilter (or character filter) is a pre-processor on an incoming stream of.
Much like token filters, CharFilters can be chained together to add, change, or remove characters from text.
As with most Solr features, you can implement your own using the Plug-In framework.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
To configure this factory, you need to define two attributes: pattern and replacement.
The pattern is a regular expression that identifies the characters we want to replace in our text.
The replacement attribute specifies the value you want to replace the matched characters with.
Don’t worry if you’re not a regular expression expert, in most cases you can find the expression you need online using Google or similar search engine.
Let’s use this <charFilter> to solve those pesky terms with repeated letters like yummm.
In regex speak, the [a-zA-Z] is a character class that identifies a single letter in lower or uppercase.
The parenthesis around the character class identifies the matching letter as a captured group.
So that covers the pattern to match, what about the replacement? Our goal is to collapse repeated letters down to a maximum of two so what we need is a way to address the part of a term that matches ([a-zA-Z])
With regular expressions, the ([a-zA-Z]) part of our expression is called a captured group and is addressable as $1
For example, in yummm, ([a-zA-Z]) evaluates to the first “m” and the entire expression evaluates to “mmm”, so “mmm” gets replaced to “mm”
It turns out that a few of the issues we are having with the example tweet are caused by.
In addition, it’s splitting hyphenated terms into two tokens, such as “i-Pad” becoming “i” and “Pad”
Unfortunately, the means that searches for “iPad” won’t match our example document as we saw in the previous section.
In terms of hashtags and mentions, you might wonder why we care about preserving those special characters on the front?
Specifically, @Manning used in a social context has a very special meaning in that it identifies a specific social account, in this case the one used by Manning Publishers.
This is very different than a tweet about Peyton Manning.
Also, if we apply stemming, then “manning” will become “man”
This is an extreme case in that our example tweet will be a match for queries with the term “man” in them!
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
For example, #fail is a hashtag commonly used to denote a person’s dissatisfaction with another person, place or thing such as a brand.
Now we just need to figure out how to do it.
First, let’s make sure we understand how they are being removed.
If you are a Java developer, then your first inclination might be to extend StandardTokenizer and override the behavior that strips off these two special characters.
In general, we want to preserve hashtags and mentions so that we have flexibility to differentiate between documents with #fail and documents with just fail.
The general lesson here is that sometimes you need to preserve context about special terms during text analysis.
If you recall, the StandardTokenizer split our example tweet into 23 separate tokens.
In contrast the whitespace tokenizer produces a different set of tokens, as depicted in figure 6.9:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
So we solved our hashtag, mention, and hyphenated term issue but we introduced a few more issues in the process.
At a high-level, this filter splits a token into sub-words using various parsing rules.
First let’s see how this filter helps us preserve the special characters on our hashtags and mentions.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The leading backslash on the hash sign is so that Solr won’t interpret that line as a comment when reading the wdfftypes.txt file.
With this simple mapping, hashtags and mentions are preserved in our text.
In our example tweet, the author incorrectly used “i-Pad” instead of “iPad”; what would be nice is for our tweet to match all possible forms used in queries, ie.
Think for a minute about what needs to happen during indexing and query analysis to ensure all three forms produce matches.
Table 6.4 gives you an overview of how each of these options works.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In search, it’s often the case that doing simple things can make a big difference.
Stemming transforms words into a base form using language-specific rules.
This stemmer is less aggressive in its transformations than other popular stemmers like the PorterStemmer.
For now, we apply the KStemFilterFactory to remove “ing” from terms like “drinking” and “learning”1
Table 6.5 Comparing stems produced by the KStemmer and Porter algorithms.
It should be clear from these few examples that the Porter stemming algorithm is much.
The problem with being too aggressive is that your search application may end up matching documents that have little to do with a user’s query.
For example, if a user searches for “wedding in July”, the term “wedding” is stemmed to.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Also notice that “operating” and “operative” both stem to “oper” using Porter so documents containing “covert operative” and “operating system” will both be a match for a query for “operating capital”
In general, a stemmer expands the set of documents that match a query but can negatively impact precision of the results.
This filter injects synonyms for important terms into the token stream.
In most cases, synonyms are injected only during query time analysis.
This helps reduce the size of your index and is easier to maintain changes to the synonym list.
If you inject synonyms during indexing and you discover a new synonym for one of your terms, then you will have to re-index all of your documents to apply the change.
On the other hand, if you only inject synonyms during query processing, new synonyms can be introduced without re-indexing.
In our example, there are a few tokens that could benefit from synonyms including: SF, latte, and caffe.
To map synonyms for these terms in Solr, add the following to the synonyms.txt file identified in your filter definition:
There are some solutions available as community extensions to Solr, such as a utility to convert the WordNet database of English synonyms into Solr’s format.
After applying these additional filters to our example tweet, we are left with the following.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
So now let’s return to the Solr Analysis form to see if the previous query we tried is a match.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The query terms shown in table 6.6 are now matches to terms in our example tweet based on text analysis.
Table 6.6 Query terms matching our example tweet based on text analysis.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Of course this query is a bit contrived for example purposes.
The key take-away is that Solr provides a wealth of built-in text analysis tools that allow you to implement flexible solutions to handle complex text.
The ultimate goal being a search application that makes it easier for your users to find relevant documents using natural language without having to think about linguistic differences in text.
After applying the schema.xml, restart your Solr server and then use the post.sh script to add the tweets.xml document to your server.
Feel free to experiment with various queries to see Solr text analysis in action.
Let’s now turn our attention to more advanced topics in text analysis.
Throughout this chapter we built a solution to parse and analyze very complex social.
We covered the main tools Solr provides but in reality we’ve only touched on the most common tools available.
Before we wrap up this chapter, we want to provide you with an overview of some of the advanced techniques you can use to fine-tune your analysis.
Let’s begin by looking at some of the more advanced attributes you can apply to text fields in schema.xml.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
We think it is important to be aware of these options, as you will undoubtedly encounter them when looking at the Solr example schema.xml.
Also, you should be aware that each of these advanced attributes only apply to text fields and do not impact non-text fields like dates and strings.
Table 6.7 Overview of advanced attributes for field elements in schema.xml.
Norms help give shorter documents a little boost during relevance scoring.
Thus, if most of your documents are of similar size, then you could consider omitting norms to help reduce the size of your index.
Norms are omitted by default for primitive types such as dates, strings, and numeric fields.
The MoreLikeThis feature requires term vectors to be enabled so that Solr can compute a similarity measure between two documents.
Storing term vectors can be expensive for large indexes, so only enable these if you really need them.
As we discussed in chapter 3, a norm is floating-point value (Java float) based on a document length norm, document boost, and field boost.
Under the covers, Lucene encodes this floating point value into a single byte, which if you think about it is pretty cool.
The document length norm is used to boost smaller documents.
Without going into too much detail, Lucene gives smaller documents a slight boost over longer documents to improve relevance scoring.
Conceptually, if a query term matches a short document and a long document, both containing the term once, Lucene considers the.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In this case, term weight is just the term frequency (1) divided by the total number of terms in the document (N)
Thus, Lucene gives the shorter document a slight boost, which is encoded in the norm.
It stands to reason that if your documents are of similar length and you are not using field and document boosts at index time, then you can set omitNorms=true to save memory during searching.
However, when just starting out, we recommend that you use the default value (omitNorms=false) so that results ranking benefits from the document length normalization.
Let’s take a quick look at another advanced attribute for fields, which is used to improve the performance of document similarity calculations.
Solr also provides a feature to compute a similarity between documents, commonly known as More Like This.
The More Like This feature in Solr finds documents in the index that are very similar to a specific document.
Under the covers, this feature uses document term vectors to compute the similarity.
The term vector for any document can be computed at query time using information stored in the index.
However, for better performance, term vectors can be pre-computed and stored during indexing.
The optional termVectors attribute allows you to enable term vectors to be stored for each document during indexing.
If you decide to enable this feature after indexing documents, then you will need to need to reindex all documents.
We will revisit these attributes throughout the rest of the book, when applicable.
For example, we’ll look closely at the termPositions and termOffsets attributes in Chapter 9 when we discuss hit highlighting.
For now, let’s turn our attention to another advanced text analysis topic—namely multi-lingual analysis and language detection.
In other words, the solution we built for analyzing English micro-blog content won’t work very well for German or French tweets.
Each language will have its own parsing rules for tokenizing, stop words list, and stemming rules.
In general, you will need to develop a specific <fieldType> for each language you want to analyze for your index.
That said many of the techniques you learned in this chapter are still applicable for analyzing languages other than English.
This raises the question of how to select the right text analyzer during indexing? Assuming you want to index all your documents regardless of language in the same index, a simple solution would be to use a unique field for each language.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Of course, if you know a document is French ahead of time, then you can manually populate the text_fr field when constructing your document to be indexed.
For example, assume we have the following tweet, which is a famous quote from Voltaire:
The Solr XML document to index this French tweet is shown in listing 6.6:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Notice how we specify the lang as “fr” and use the text_fr field explicitly.
This approach is fine if you already know the text is French, but what do you do when you don’t know the language ahead of time? Part of Solr’s appeal is its large community of active users, contributors, and committers, so as you might have guessed, Solr has a built-in solution for language detection!
BUILT-IN LANGUAGE DETECTION Solr language detection solution was designed to work with documents that are primarily.
In other words, your mileage may vary if you send a document that contains a mixture of languages.
Mixed language documents aside, let’s apply Solr’s language detection solution to our social media search application to see how it works.
Solr’s solution should work well for our social media search application in that blogs, tweets, and comments are typically written in a single language.
What we hope to see is that we can send a document, such as a tweet, in any major language, have Solr detect the language, and then apply the correct text analysis based on the language.
Continuing with our Voltaire quote, we want Solr to determine the document is French and then populate our lang field with the value “fr”
Once the language is known, Solr will look for a field named “text_fr” to populate so that this text gets analyzed using the correct field type designed for French text.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Also, LangDetect is an open-source project released under the commercial friendly Apache 2.0 license; it’s hosted at http://code.google.com/p/language-detection/
Listing 6.8 shows how to configure language detection in solrconfig.xml.
The processor pulls the text and determines the language using statistical analysis on the patterns in the text.
Once the language is detected, the processor populates the language code in the field specified in the langid.langField parameter, which in our example is lang.
Lastly, the processor is configured by the langid.map field to map the text to a specific field in the index by combining the name of the incoming field used for language detection “text” and the language code “fr”
Thus, for our example, the processor will map French text to the text_fr field.
With these settings in place, you can now send documents containing text in any of the 53 languages supported by LangDetect and Solr will do the right thing.
Of course, you still need to define the fields and field types to handle specific languages as we did for French.
We’ll wrap-up this chapter with an answer to the question of what do you do when Solr.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Consequently, it will be rare to encounter text analysis requirements that cannot be addressed with one of the built-in tools.
The Solr PlugIn framework can be used to develop extensions for a number of Solr components beyond text analysis.
Thus, we’ll only touch on the basics as they relate to text analysis.
We’ll use the Plug-In framework to extend other components of Solr in later chapters.
To begin, we need a requirement that Solr can’t solve with built-in tools.
Recall from our previous discussion of multi-valued fields where we indexed zero or more URLs into the “links” field.
For tweets, any links in the text are going to be shortened links provided by an online service like bitly.com.
For instance, the shortened bit.ly URL for the Solr home page http://lucene.apache.org/solr/ is http://bit.ly/3ynriE.
From a search perspective, the shortened URL isn’t very useful as it’s hard to imagine someone entering “bit.ly/3ynriE” in a search box! What we want is for a document with a shortened link to be found when someone searches for the resolved URL, e.g.
Thus, during indexing, we need to extract the shortened URL and replace it with the resolved URL.
To get the resolved URL, we can use an HTTP HEAD request and follow redirects until we reach the resolved URL or, in the case of bit.ly, we can use their Web service API.
Now that we know the problem to solve and have a basic understanding of how we want to solve it, we need to determine where in the Solr text analysis process to plug-in our solution.
In other words, we need to determine the type of plug-in we need to build.
Recall that an analyzer brings together a tokenizer and chain of token filters into a single component.
That feels a little heavy-handed since we want to utilize our existing tokenizer and chain of filters.
Our solution involves replacing one token, a shortened URL, with a different token, a fully-resolved URL.
Thus, for this requirement, it makes sense to build a custom TokenFilter, which in Solr is the most common and easiest way to customize text analysis.
That said, you can also build your own analyzer, tokenizer, or char filter using a similar process to what we illustrate below.
The factory class is needed so that Solr can instantiate configured instances of your TokenFilter using configuration supplied in the schema.xml file.
As our main focus here is to learn how to.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Listing 6.9 shows a skeleton of the custom TokenFilter class for resolving shortened URLs:
If you choose to actually implement this filter, we encourage you to consider the impact your solution will have on indexing performance.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A rough outline of a solution would use a distributed caching solution, such as memcached, to avoid resolving links more than once and would take full advantage of Web service APIs provided by URL shortening services like bitly.com.
Typically, the API-based approach will allow for batching up many shortened URLs into a single request, which will be more efficient than using HTTP HEAD requests to resolve the links by following redirects.
The Factory is responsible for taking attributes specified in schema.xml and converting them into parameters needed to create the TokenFilter.
The example shown here only supports bit.ly URLs so in a real application, you would want to extend this regular expression to support all possible shortened URL sources.
Next, take a moment to think about where in the chain of filters in the text_microblog field type you should put this filter.
The Java implementation for our factory is shown in listing 6.10
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Only a few lines of code are needed to plug-in a custom TokenFilter! The key take-away here is that Solr uses your factory class as the intermediary between the filter definition in schema.xml and a configured instance of your custom TokenFilter used during text analysis.
The last thing you need to do is to place a JAR containing your Plug-In classes in a location where Solr can locate them during initialization.
When the server starts up, it makes all JAR files in the plugins directory available to the Solr ClassLoader.
If Solr complains about not being able to find your custom classes during initialization, try putting the full path to your plugins directory.
To recap, we began the chapter by learning how to define field types to do basic text analysis.
This is where we learned that field types for unstructured text-based fields are composed of either one analyzer for both indexing and query processing or two separate, but compatible analyzers for indexing and query processing.
Each analyzer is made up of a tokenizer and chain of token filters.
To test our simple analysis solution, we used Solr’s Analysis form to see an example document pass through the StandardTokenizer and chain of simple filters to remove stop words and lowercase terms.
Our testing demonstrated that the basic text analysis solution was not sufficient to deal with all the nuances of our micro-blog content.
Consequently, we leveraged more built-in Solr tools to tackle these complex requirements.
The WordDelimiter also proved useful for handling hyphenated terms in a more robust manner.
We also saw how to use stemming and synonym expansion to improve our search application’s matching capabilities.
Overall, we developed a powerful solution for analyzing micro-blog content using only built-in tools and not a single line of custom code.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Finally, we finished our tour of text analysis by looking at some advanced concepts.
Specifically, we covered advanced attributes for fields, such as setting omitNorms=true to reduce memory and storage in your index if you don’t need index-time boosts.
Next, we saw how to use Solr’s built-in solution for language detection to handle documents in other languages.
Lastly, we developed a custom TokenFilter to resolve shortened URLs.
The key take-away was that Solr makes it easy to implement a custom analyzer, tokenizer, token filter, or char filter to implement exotic requirements.
In the next chapter, we learn how to query Solr and process results.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
An introduction to faceting and an overview of common use cases.
Field Facets and the ability to see the top values in any field per query.
Using bucketized Range Facets to understand number and date ranges values in your search documents.
Getting matched document counts for any number of arbitrarily complex queries by utilizing Query Facets.
Implementing multi-select faceting and even faceting upon values not included in your search results.
An introduction to advanced faceting topics covered in later chapters.
Faceting is one of the most powerful features of Solr, as compared to traditional databases and other NoSQL data stores.
Faceted search, also called faceted navigation or faceted browsing, is a capability which allows those running searches to see a high-level breakdown of their search results based upon one or more aspects (facets) of their documents, allowing them to select filters to drill into those search results.
When running a search on a news site, you would expect to see options to filter your results by timeframe (last hour, last 24 hours, last week) or by category (politics, technology, local, business)
When searching on a job search site, you would likewise expect to see options to filter results by city, job category, industry, or even company name.
These filtering options generally display not only the available values for each of these facets, but also a count of the total search results matching each of those values.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This allows users to quickly see a birds-eye-view of their results set, without having to actually look through every single search result.
Faceting in Solr enables this kind of dynamic metadata to be brought back with each set of search results.
While the most basic form of faceting simply shows a breakdown of unique values within a field shared across many documents (a list of categories, for example), Solr also provides many advanced faceting capabilities.
These include faceting based upon the resulting values of a function, based upon ranges of values, or even based upon arbitrary queries.
Solr also allows for hierarchical and multi-dimensional faceting and multi-select faceting, which allows returning facet counts for documents that may have actually been filtered out a search result.
We will investigate each of these capabilities in this chapter, leaving you with the knowledge to implement your own world-class faceted search experience.
A basic understanding of how the Lucene index stores terms (covered in chapter 3) is also useful, but not required.
Let us begin with some examples demonstrating how faceting in Solr enables navigating through and visualizing search results at a glance.
In this section, we you will see a high-level overview of faceting, including several.
Faceted search is generally composed of two separate steps, calculating and displaying facets to users (which I will refer to going forward as “bringing back facets” or sometimes just “faceting”), and allowing users to select one or more facet values by which their results should be filtered (which I will refer to as “selecting” or “filtering” on a facet)
At this point, you may be wondering exactly what a facet looks like when it is brought back from the search engine.
If you were searching on a set of restaurant documents, you might expect several facets to be represented as a navigational element in the user interface, such as the one depicted in figure 8.1 depicting the facets to be returned for the query “hamburger.”
Figure 8.1  Navigational element in a search results user interface depicting a breakdown of various “facets” of the content returned for a search query of “hamburger.”
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This navigational element provides a clear visual demonstration of what faceting is, and it also provides an initial glimpse into the power faceting provides.
Each of the categories brought back (“Restaurant Type”, “State”, “Price Range”, and “City”) are individually considered one facet.
You can think of each facet as a slice of information that describes the results of a search.
It is important to note that, while the information in figure 8.1 is useful, there are many potentially better way to visualize these facets.
One downside of including each of the values above is that you can only display a few at a time for each facet.
Figure 8.2 demonstrates an alternate way to view the State facet.
Figure 8.2  Representation of a facet on “State” in a visually appealing way.
This visualization allows all 50 States within the United States to be displayed in the user interface at one time, preventing the user from being overwhelmed by information.
As you can see, by utilizing modern visualization techniques, it is possible to utilize facets as representing important meta-data about a user’s search results to provide an enhanced searching experience.
Figure 8.3 illustrates a similarly appropriate visualization for the Restaurant Type facet.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 8.3  Visualization representing the Restaurant Type facet as a pie chart.
While geographic maps and pie charts may be useful for demonstrating discrete values, representing continuous values such as numbers and dates can often be best represented through line graphs, as demonstrated in figure 8.4
Figure 8.4  The price range facet is visually represented as a continuous line graph, allowing users to interpret  all of the values in the facet with one quick glance.
The line graph demonstrating the Price Range facet is particularly interesting because this visualization can be used to represent any range of values which can plotted in a continuous.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This is not a chapter on data visualization, so we will not belabor the point here, but it is important to take away from this section that faceting provides an incredibly powerful ability to generate real-time analytics on user searches which can greatly enhance your users’ search experiences.
Before we dive into the mechanics of implementing facets in Solr, it is also important to note that facets are calculated in real-time for each set of search results.
The count is NOT how many times the value exists across all documents (as a value may exist multiple times within a single document), it is only a count of the number of documents matched.
The fact that all facet values are calculated based upon a search result set means that every time a new search is fired, different facet values and counts will be returned for each facet.
This allows users to run a search, see the facets that are returned from the search result, and then perform another search which filters on any values for which they want to limit the next set of search results.
Notice that all cities in the City facet are now located in California since all documents within our search results must now be in California.
While figure 8.5 only filters on a single value (the state of California), you can actually apply as many filters as you want on any given search, and each facet will calculate its values based upon all of the filters being applied.
If you explore the Price Range facet for the first search (nation-wide) and compare them to the results of the second search (state of California), you would be able to spot a noticeable price difference between California and the rest of the United States, as figure 8.6 will visually demonstrate.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
A noticeable trend is evident when the facet values are compared side-by-side: Restaurants in California, on the whole, are more expensive than restaurants in the rest of the country.
You will see in section 8.5 how to apply these filters to facet values, but the key take-away from this section is that facets can provide rich insights into the results of any given search, allowing your users to easily measure the quality of their search and drill-in to the aspects of the results they find most interesting.
Before we dive into the mechanics of faceting, we need to load up some sample data into.
Solr that will be used for the examples in rest of this chapter.
Our sample data will be a small subset of documents similar to our restaurants example from section 8.1
This section will get you up and running with a handful of restaurant documents upon which you will be able to facet throughout the rest of this chapter.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The schema in listing 8.1 defines each of the fields upon which we will attempt to facet in.
Each of the documents upon which we will facet is contained in listing 8.2
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
After downloading or recreating files from the above listings in your current home directory, it is now time to start Solr and index the example restaurant documents.
The last line above actually indexes the restaurants from a text file using the post.jar utility that comes with Solr.
If everything is successful, you should see an output like the following:
Once you have successfully indexed the restaurant documents, you should be able to hit the standard Solr search handler and verify that your documents in the engine, along with all of the fields from the example documents, using the following url:
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please note that all listings and other Solr responses in this chapter are in JSON format and not XML.
Even though XML is Solr’s default response type, JSON is a more humanreadable format which is more compact and is thus easier to use for demonstration purposes.
A parameter of “wt=json” was added onto the Solr request to change the response type from XML to JSON format.
If you have not changed your default response type to JSON (Solr’s default is a XML), then it will be necessary to add this “wt=json” parameter to all of the Solr requests in this chapter to return them in the same format.
You will also notice that all Solr responses in this chapter appear nicely indented.
This can be accomplished by adding an “indent=on” parameter to your Solr url.
Requesting this indented response format is not recommended for a production application (it works, but adds unnecessary extra processing time to your application), but it does make Solr responses more human-readable, which is why it has been used for all the examples in this chapter.
The response you receive from Solr should include all twenty of the example restaurant.
With the twenty example documents now searchable, we are ready to begin running faceted searches.
We will begin with the most common form of faceting: faceting upon each of the unique values within a field.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Field faceting is the most common form of faceting: requesting the unique values found in a particular field back, along with the number of documents in which they are found, when a search is performed.
In section 8.1, several Field Facets were demonstrated from the restaurants example: a facet on “Restaurant Type” field, a facet on the “State” field, and a facet on the “City” field.
In this section, you will learn how to construct a Solr query requesting a Field Facet, and you will learn the various faceting parameters that allow you to tweak how the facet values are calculated and returned from Solr.
For demonstration purposes for the rest of the chapter, we are going to be faceting upon the documents we indexed in section 8.2
Listing 8.3  Facet results for a Field Facet on a single-valued field.
This example demonstrates the most basic form of faceting in Solr: Field Faceting on a.
In this kind of faceting, each unique value is examined, along with a count of documents in which that value is found.
Since there is only one value per document, the sum of all of the counted values (company names in this case) will also equal the total number of documents.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Not all fields in Solr contain a single value, however.
The tags field is an example of a multi-valued field.
Let us see what happens when we try to facet upon the tags field in listing 8.4
It is interesting to note that the tags “breakfast” and “coffee” were the two most.
This is because two of the restaurants, Starbucks and McDonalds, fell into both of these categories.
If you were to search for breakfast or for coffee, this would become readily apparent from the search results returned from Solr.
It is also important to note that sum of the counts for each of the tags facet values is much greater than 20, which is the total number of documents in the search engine.
This is because each document contains more than one term, which means that many of the individual terms map to the same document (and thus most of the documents are counted more than once)
In section 8.1 we discussed several methods for visualizing facets.
This tag faceting example lends itself to what is probably a distinctly obvious kind of visualization: a tag cloud.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Figure 8.7 demonstrates mapping the facet values from this multi-valued Field Facet result on the tags field.
Figure 8.7  A tag cloud representation of a Field Facet on a multi-valued tags field.
The size of the text is relative to the number of documents in which the phrase was found.
This demonstrates again that there are many ways to visualize facets.
Tag clouds are common ways for users to see a high-level overview of the results of their search from a categorical standpoint.
In addition to using tags like in this example, it is also common to facet upon a raw content field containing everyday language to glean these kinds of insights (such as the full-text of a restaurant’s description in this case)
The problem with using a raw content field, of course, is that much more noise exists in such a field, since everyday language contains junk words (such as stopwords like “and”, “of”, and “like”), making it less reliable in describing the document than explicit tags would be.
An additional important item to keep in mind when you are faceting is that the values returned for a field facet are based upon the indexed values for a field.
In other words, if you pass in the term “San Francisco, CA”, but you actually tokenize the field as a text field which splits on spaces and commas and also lowercases the text, then the actual values in the Solr index for that field would be “ca”, “francisco”, and “san”
Thus, unless you want to actually bring back facet counts for each of those terms individually and lowercased, you have to consider how you may want to facet on a field when you create the field definition in Solr’s schema.xml.
At this point you should have a solid grasp of what a facet is, and you should also have a good feel for requesting a facet on either a single-valued field or a multi-valued field.
Thusfar, however, you have only been exposed to the default settings for bringing back a facet.
Faceting seems easy when you are only dealing with twenty documents, but what happens when there are thousands or millions of unique terms that would come back from Solr on a faceting request?  Fortunately, Solr has many faceting options which allow finetuning how facets are returned on a per-query basis.
This list of Field Facet options is given in table 8.1
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Table 8.1  A listing of the Field Faceting parameters which can be specified on the Solr url to modify faceting behavior.
One important take-away from table 8.1 is that multiple facets can be requested by specifying the facet.field parameter multiple times.
This parameter may be specified multiple times to return multiple facets.
The fc (field cache) method loops over the documents that match the query and finds the terms within those documents.
The fc method is faster for fields which contain many unique values, whereas the enum method is faster for fields which contain few values.
The fc method is the default for all fields except Boolean fields.
The default is 0, meaning that the filterCache should always be used.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Listing 8.5  Mixing Field Faceting parameters on a field-by-field basis.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
As you can see, this listing combines multiple facet options on multiple fields in a custom.
While the query required many parameters, the flexibility these faceting options provide can be well worth the additional complexity.
At this point you have learned how to request Field Facets back in Solr so that you can see how many documents match each unique value in any of your indexed fields.
While this is a powerful feature, faceting in Solr is actually much more powerful.
The next realm of faceting we will explore is Query Faceting, the ability to bring back facet counts for literally any query – no matter how complex – which also matches your search results.
While it is great to be able to return the top values within any indexed field as a facet, as discussed in the last section, it can also be extremely useful to bring back counts for arbitrary sub-queries so that you know how many results might match a future search.
The best way to demonstrate this capability is through an example.
You could certainly accomplish this by running three different queries, as indicated in listing 8.6
The example in listing 8.6 demonstrates the most brute-force method for finding search result counts for sub-queries in Solr – running each sub-query as a separate search and.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
While the pain may not seem enormous in this small, contrived example, it is nevertheless unnecessary.
Listing 8.7 demonstrates how such a query can be easily combined into a single query using Query Facets.
Listing 8.7  Running a single Query Facet to obtain document counts for sub-queries.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This example demonstrates how Query Facets can effectively be used to create new buckets of information at query time in any Solr query.
In reality, you could have just as easily created a new field in Solr called “pricerange” which contained each of these bucketized values.
Had you done so, you could have just performed a field facet upon the new pricerange field to pull back each of the five bucketized values.
Of course, this would also required you to map these bucketizing rules at index time when you are first feeding your content to Solr, a process which can be painful, especially as your amount of content in Solr begins to grow.
Query Facets provide a nice alternative which allows you complete flexibility at query time to specify and re-define which buckets should be calculated and returned.
While the examples in this section have been simple, they demonstrate the complete flexibility that faceting upon any arbitrary query provides.
Because Solr provides many powerful query capabilities including nested query handlers and function queries, the possibilities for advanced query-based faceting are only limited by one’s imagination.
The ability to extend faceting in this way is tremendously powerful - anything you can search upon, you can facet upon.
While Query Facets are incredibly flexible, they can become burdensome at times to request from Solr, as every single value upon which you want to generate facet counts must be explicitly specified.
As we will see in the next section, Solr also provides a convenient Range Faceting capability that makes faceting upon numeric and date values much easier in this regard.
Range faceting, as its name implies, provides the ability to bucketize numeric and date field values into ranges such that the ranges (and their counts) get returned from Solr as a facet.
This can be particularly useful as a replacement for creating many different Query Facets to represent multiple ranges.
In the last section, a Query Facet was demonstrated based upon the price field in our example data from section 8.2
Had the values needed from the search engine been evenly spread out, similar facet counts could have been returned from Solr using a Range Facet, as indicated in listing 8.9
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You can adjust the gap to create larger or more granular buckets based upon the needs of your application.
This is a great time saver if you want to bring back all of the range buckets within your range, as it prevents you from writing countless facet.query parameters manually to accomplish a similar effect.
Table 8.2 is included as a reference for each available Range Faceting parameter and its available options.
Table 8.2  A listing of the Range Faceting parameters which can be specified on the Solr url to modify faceting behavior.
Determines which field a Range Facet should be calculated upon.
This parameter may be specified multiple times to return multiple facets.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The numerical or date value at which the first range should begin.
No value lower than this will be included in the counts for this facet.
The numerical or date value at which the first range should end.
No value higher than this will be included in the counts for this facet.
If hardened is set to “true” then the final range will stop at the upper bound, leaving a potentially smaller final bucket.
If hardened is false, then final bucket size will be increased above the upper bound such that its size it is the same size as the other buckets (the size of the gap)
Indicates additional ranges that should be included in the ranges.
The “before” option creates a bucket for all values prior to the lower bound.
The “after” option creates a bucket for all values after the upper bound.
The between option creates a bucket for all values between the lower and upper bounds.
This parameter may be specified multiple times to include multiple values.
The “all” option is a shortcut for saying “before, after, and between”
If the “none” option is present, it will override any other parameters which are specified.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This parameter may be specified multiple times to include multiple values.
The Solr parameters in table 8.2 demonstrate the rich options available when performing Range Faceting in Solr.
Range Faceting often provides a more convenient and succinct query syntax than Query Faceting when faceting upon ranges of number or date values.
Query Faceting can alternatively be used when the range queries become overly complicated, allowing for some very powerful queries, as we saw in section 8.5
Of the three types of faceting we discussed, Field Faceting is the most widely used and simplest to use.
For each of these three faceting types, we have explored how you can requests facets back from Solr.
What we have yet to discuss is how you would go about refining your subsequent search once a facet is selected, which will be the topic of the following section.
Returning facets from Solr is the first step toward allowing your users to refine their search results.
Once you’ve shown the breakdown of faceted values to your users, however, the next step is to allow them to actually click on one or more facet values to apply that value as a filter.
In this section, we will discuss the best approaches for applying these filters.
At the most basic level, applying filters upon a facet is no more difficult than just adding.
In other words, assuming we wanted to return three facets with our searches, one a Field Facet on the “state” field, one a Field Facet on the “city” field, and one a Query Facet on the “price” field.
Our initial query and results would look similar to listing 8.10
Listing 8.10  Faceting upon the tags field in the example restaurant data.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In this example, how would you go about filtering upon a facet value?  You actually already know how to do this – you simply add a filter to your query for each selected facet value.
Listing 8.11 demonstrates a second search, where the user has clicked the state of “California”
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
First, they run a base search bringing back facets, and then they select a facet to run a subsequent search and narrow down their search results with a filter.
This could continue, with the user running a third search, such as in listing 8.12
It is worth noting that each of the examples you have seen thus far operate on a field containing only one value.
As such, as soon as you filter upon that value, no documents.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This makes sense in our single valued fields – if a document can only have one “price” or only appear in one “state”, there is no way it could appear in a facet where another price or state was selected.
Any field which is marked as multivalued in Solr’s schema.xml file or that is of a field type which is analyzed into multiple tokens may actually contribute multiple terms to a facet.
We can see this in action by utilizing the multivalued “tags” field in the example Solr index.
Listing 8.13 shows several searches that successively apply filters on the “tags” field.
Listing 8.13  Applying several filters on a faceted field containing multiple values.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
It is also worth noting that, even though these examples specify each selected filter value as its own fq parameter on the Solr url, there is no requirement that this be done.
This will require less lookups in the Solr Filter Cache (discussed in chapter 13), and will also provide more control over how your filter values interact.
For example, there is nothing requiring you to “AND” together each of the facets selected.
It could be a perfectly valid use case for you to “OR” values together, such as allowing users to select multiple cities in their facet while filtering to only the cities selected.
The discussion of multi-select faceting in the following section will highlight how you might accomplish the display of such a facet with multiple filters selected at once, even on a single-valued field.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
All of our examples of applying facet filters thus far have applied filters based upon a.
In reality, the terms brought back in facets may be more challenging to deal with, such as multi-word terms.
What happens, for example, if you want to facet upon the multi-word term Los Angeles?  As you already know, the filter fq=city:Los Angeles is invalid, as it syntactically says to find documents containing a city of “Los” and the term “Angeles” in the default field(s)
In order to allow for phrases separated by a space, most Solr developers decide to quote all terms they facet upon.
Unfortunately, there is even a problem with just blindly quoting the terms you are filtering upon: if the term has quotes within it, the syntax will break unless you escape it.
If quoting and escaping quotes were not already enough trouble, you also have to be mindful of all text processing taking place on the field upon which you are faceting.
As you saw in chapter 5, text analysis upon a field can be defined differently for content indexing vs.
Thus, if any kind of mis-match occurs (which is generally a bad sign, of course), it is possible that a value you are trying to filter upon does not actually match the same number of documents as reported by the facet.
The one downside of using the Term query parser for all of your facet filters is that this query parser does not support Boolean syntax, so if you want to combine multiple facet values together in a filter, you must utilize the Nested query parser and its syntax.
Listing 8.14 demonstrates utilizing both approaches: using a separate filter for each facet term and also combining multiple facet terms into a single filter utilizing the Term query parser.
Listing 8.14  Utilizing the Term query parser to filter on facet values.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
If you end up utilizing the Nested query parser syntax in approach 2, you will still need to escape quotes within your terms (since the whole nested query is in quotes), but this is a minor inconvenience if you want the capability to still use Boolean logic in your facet filters.
In this section, you have seen that applying filters to your facets is no different than applying any other filter in Solr, while seeing that it is possible to apply a separate filter per facet value or one filter for multiple facet terms.
At this point, you should be able to request and filter upon all of the basic facet types, but there is still more to explore.
In the next section, you will uncover some useful ways to re-name facets for display purposes and to even bring back facet counts for documents which have already been filtered out.
When requesting a facet back from Solr, the name of the facet is not always the most useful for purposes of displaying results back to the user or even handling them within your application stack.
Solr actually provides a very convenient ability to rename facets when they are returned, making facets much more user friendly for many use cases.
Solr also provides the ability to bring back facet counts for documents that have been filtered out.
This can be incredibly useful for implementing multi-select faceting – an ability to filter search results but still see the number of documents that would have matched had the filter not been applied.
In this section, you will be introduced to the concepts of the keys, tags, and excludes local params (local params were introduced in chapter 7), which enable these useful facet re-naming and multi-select capabilities.
All facets have a name which allows developers to distinguish them from each other.
Using the key local param, however, it is easy to rename any facet, as demonstrated in listing 8.15
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The ability to rename a facet demonstrated in listing 8.15 can be very useful in many.
It allows your search application to request Query Facets, for example, without requiring the application to interpret the queries from the result set during a post-processing stage.
It also allows for user-friendly names to be assigned to facets regardless of the underlying field or query associated with the facet, which can make displaying the results in a user interface more straightforward.
Additionally, by enabling each facet to be assigned a unique name, this capability to specify keys allows for more than one facet to be defined on the same field (which can be useful for Field Facets or Range Facets), each coming back under a unique name.
One last advantage of this approach is that it allows multiple fields to be mapped into the same name depending upon query-time rules.
When filters are applied to a Solr query request, the results must include every single filter.
One of the problems this presents, however, is that often times it is useful to see facet counts for values which have been already been excluded from the query.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
By default, after filtering upon a facet, the facet values which are returned for that facet no longer include the documents which were filtered out.
This is problematic if you want to allow your user to select multiple values to include in the search, as they will never be able to “OR” any additional facet filters for the values since they are no longer visible as options.
Even though you would expect your search results to only display documents in California given the above user interface, you would also expect the facets to continue displaying the other states so that you could expand your query.
The same principle applies for price ranges and cities – it is silly to only allow your users to search for one value per facet at a time.
Fortunately, Solr has a solution to this problem through a feature called Facet Exclusions.
Facet Exclusions allow you to “add back” documents removed by any groups of filters that were applied on your search request.
By adding back the removed documents to the facet counts, you can make facet counts on each facet effectively ignore any filters applied based upon that facet.
The parameters necessary to implement this are the “tag” local param and the “ex” local param.
Listing 8.16  Using tags and excludes to implement multi-select faceting.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Even though you cannot see the actual search results in listing 8.16, it is important to keep in mind that the documents returned from Solr are still constrained to the state of California (because the filter was applied to the query), even though state facet is not limited by that filter.
It is also important to understand that all of the other facets not tagged with the “tagForState” tag are constrained by still constrained to the state of California for the same reason.
You may also note that each of the other requested facets (on city and price) also contained exclusion tags, but that those exclusion tags did not correspond with any tagged filters.
While these exclusion tags were unnecessary, they did not cause any problems and.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
If someone were to re-run the query and add a filter on one of the additional facet values, the currently unused exclusion tag would kick into effect.
Whether you actually choose to apply exclusion tags on facets prior to the existence of any filters containing the excluded tags is up to you, but the point here is that doing so, while possibly wasteful syntax-wise, will not negatively impact the returned results.
It is possible to build some very interesting user interfaces and data analytics capabilities by mixing and matching tags and facets, but those use cases go well beyond what this chapter can cover.
You should feel free to experiment with these capabilities in Solr if you want to learn more.
At this point you have seen all of the major use cases for facets, including Field Faceting, Query Faceting, and Range Faceting.
There are several additional aspects of faceting that have not yet been discussed.
In the next section, we will touch on some of the more advanced topics related to faceting which will be discussed in later chapters.
This chapter provides a solid overview of the most used faceting capabilities in Solr, but this is not the last time you will see faceting discussed.
Faceting makes heavy use of Solr’s caches, so you will need to optimize your use of Solr’s built in caches in order to maximize the performance of your faceting requests.
One of these advanced faceting capabilities is called Pivot Faceting, and it provides the ability for you to facet in many dimensions.
For example, say it was not sufficient for your application to just know the top values from the “tags” field, and that you really needed to know the top tags per city.
How would you go about accomplishing this? You could run a first search and get a facet for all the cities, and then run subsequent searches for each city to get its tags facet.
Unfortunately, this approach does not scale very well and can easily result in you having to run dozens or hundreds of searches as your document set gets larger.
Pivot facets, however, allow you to facet across multiple dimensions to pull back these kinds of calculations in a single search.
Congratulations on wrapping up an in-depth chapter on one of Solr’s most powerful features.
As you have seen, faceting provides a fast way to allow users to see a high-level overview of what kinds of documents their queries match.
You would be hard pressed to find a major search-powered website today which does not provide some form of faceting to allow users to drill down and explore their search results.
Using Solr, you have the ability to bring back the top values within each field using Field Facets, to bring back bucketed ranges of numbers or date values utilizing Range Facets, or to bring back the counts of any number of arbitrarily complex queries by utilizing Query Faceting.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You also saw that it is possible to utilize keys to rename facets as they are being returned, learned how to use tags and Facet Excludes to implement multi-select faceting which returns counts even for documents which are filtered out by a query, and explored multiple ways of applying filters to a query once facets are clicked upon by your users.
Finally, you encountered a brief introduction to the importance of effectively utilizing Solr’s caches (covered further in chapter 12) for faceting performance optimization, and you heard about multi-dimensional Pivot Faceting, which will be discussed in detail in the “Complex data operations” chapter.
At this point, you should be able to implement some fairly sophisticated search application utilizing all but the most complex forms of faceting available in Solr.
In the next chapter, you will learn how to use another very common Solr feature, Hit Highlighting, which allows the snippets of text matched in each document during a search to be returned for display in your list of search results, providing a potentially important insight to your users as to whether a document in your search results is worth exploring.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Returning multiple groups of query results in a single search request.
Ensuring variety in search results by ensuring multiple categories are represented.
Result Grouping is a useful capability in Solr for ensuring an optimal mix of search results is returned for a user’s query.
Result Grouping, also commonly referred to as Field Collapsing, is the ability to ensure only one document (or some limited number) is returned for each unique value (actual or dynamically computed) within a field.
This can be useful if you have multiple similar documents – say products from the same company, locations for the same restaurant chain, or companies with multiple offices – but do not want an entire page of search results to only represent a single product, restaurant, or company.
You have probably seen implementations of this capability when using your favorite web search engine at some point.
If you have ever seen search results telling you that many results matched your query, but only one (or the top few) were being displayed, you have encountered Field Collapsing.
Often times, such a message will provide a link users can click to see the fully-expanded list of search results, along with a count of how many additional documents would have been returned had the search results not been collapsed.
In addition to collapsing search results to remove duplicate documents, the Result Grouping functionality in Solr provides several other useful features.
In many ways, you can view Result Grouping in Solr as a more verbose form of Faceting.
Instead of only returning separate Facet sections along with the counts for each value, however, Result Grouping actually returns the unique values and their counts (like Faceting) plus some number of.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
One way in which grouping is very different than Faceting, however, is that it returns the requested groups within the search results section, which means the groups and values within the groups are sorted based upon the sorts specified for the documents in the query.
Grouping can be performed based upon field values, functions, or queries, making its applications numerous.
While this may sound complex initially, we will cover several use cases to demonstrate this incredibly useful and (mostly) straightforward feature.
Before jumping in, however, it will be useful to clarify the sometimes confusing distinction between the names “Result Grouping” and “Field Collapsing.”
One question commonly asked is why the Result Grouping functionality in Solr is referred.
Field Collapsing is the more commonly used term for this functionality in other search engines, referring to the act of returning a normal set of results with duplicate values removed.
An early version of this functionality was being developed for Solr and was unofficially called the “Field Collapsing” patch.
The decision was later made to make this capability more generic, however, and the “Result Grouping” name was chosen to signify the more generic capabilities now available.
While returning a single results set collapsed on the duplicate values within a field (“Field Collapsing”) is certainly a supported option using Solr’s Result Grouping functionality, it is also possible to return multiple result sets – or “groups” from a single query.
The “Result Grouping” name thus signifies a more generic capability than the traditional “Field Collapsing” use-case.
This chapter will demonstrate multiple uses for Solr’s Result Grouping functionality, starting with the most common one – “Field Collapsing” to removing duplicate documents from a set of search results.
Let’s say you are running an online e-commerce website that sells user-posted items.
While on the one hand it may be nice to run a search for an item and see thousands of copies of identical products, you may actually provide a much better user experience if instead you only show one document per unique item (possibly along with a count of how many of each item exists)
This will allow some diversity in the search results, just in case the item that shows up at the top is not exactly what the user was trying to find.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In order to demonstrate this capability in action, some example product documents have been added in the source code that accompanies this book.
After following the steps in Appendix A to obtain a clean version of Solr, you can start up Solr and add the example documents using the commands in Listing 11.1
With our e-commerce search engine up and running, we can now demonstrate Solr’s Result Grouping capabilities in action.
Listing 11.2 demonstrates a standard search request for the query “Spider-man” without grouping turned on.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The results from listing 11.2 almost certainly represent a bad user experience, as they seem to present every format of the same product as separate products, listing it multiple times.
By turning Grouping on, the results appear much more diverse, providing a cleaner user experience, as demonstrated in Listing 11.3
Listing 11.3  Grouped search results collapsing on the product field.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
First, notice that grouping must be turned on by specifying the group=true parameter.
Second, the group.limit parameter was set to 1, indicating that only one document should be returned for each unique value within a Group.
You will also notice that the results format in listing 11.3 is substantially different than the default Solr results format.
This more verbose format is necessary to communicate all of the information associated with grouped search results – the name of the field that is grouped upon, the unique terms within that field (groupValue) which defines each group, and the total number of results (matches) before collapsing occurred in each group.
Unfortunately, it can often be inconvenient to support parsing out two separate Solr results formats to handle grouping.
Thankfully, if you are looking to remove duplicates and do not need all of this additional grouping information, Solr provides a group.main parameter which, if set to true, will merge the results from each group back into a flat list and return it in the main results format.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Listing 11.4  Flattening grouped results into the main search results format.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The main disadvantages of using the group.main option, of course, is that you lose access to the total number of un-collapsed results within each group, but if this is not important in your search application then this may be a fair trade-off in return for not having to handle two different search results formats.
You also lose the name of the group in this simple format, but this can often be derived from the results by returning the field in each document that you grouped upon (assuming it is a single-valued field)
The other disadvantage of using the group.main format is that it only supports a single group being requested.
If you specify group.main=true, however, Solr will only return the last group that you specify in the Solr url.
There is also another option in-between the advanced grouping format (the default) and the group.main format: the simple grouping format.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
You have seen throughout this section how to collapse the results of a query into groups so as to remove duplicate documents.
You have also seen three formats in which grouped search results can be returned: the default advanced grouping format, the simple grouping format, and the returning of a single collapsed group in the main search results.
Each of these formats strikes a balance between backwards compatibility with the standard search results format and the richness of information available to describe the identified groups.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The collapsing of results down to a single document per unique field value represents the essence of what is meant by the use of the traditional term “Field Collapsing.”
Throughout the rest of this chapter, we will see some more advanced use cases for the more generic grouping capabilities available in Solr.
The next section will begin by demonstrating how to request multiple documents per group in a single query.
Collapsing to a single document per unique field value is not the only practical use-case for Solr’s Grouping functionality, of course.
Returning to our e-commerce search engine example from the previous section, imagine that if instead of only collapsing duplicate documents, we could actually guarantee that we would return a fixed number of results from each product category.
For example, going back to our earlier search (in section 11.2) for “spider-man”, imagine that instead of returning a flat list of collapsed (de-duplicated) documents, that instead, we returned up to three documents per unique category.
If you still have your Solr instance up and running from listing 11.1 (if not, you can easily go back and restart it), we can run this query.
The example documents contain a type field that we can group upon, requesting a limit of three documents per group.
Additionally, let us request a maximum of 5 total groups be returned.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
Even though the group.limit is set to 3, this only sets the upper limit, as any group without three documents cannot return a full three documents.
Second, notice that the rows=5 parameter is controlling not how many documents are returned, but instead it is controlling how many groups are returned.
In the default advanced grouping format, both the rows parameter and the start parameter apply to the groups instead of to the documents within each group.
That is, rows controls how many groups are returned, while group.limit controls how many documents are returned within each group.
Likewise, the start parameter controls the group offset for paging through groups, while the group.offset parameter controls the document offset for paging through the documents within each group.
One final very important aspect of grouping can be gleaned from listing 11.6: the way sorting interacts with grouping.
Conceptually, all groups are sorted based upon the order in which their top document is sorted.
What this means is that, assuming groupings were not enabled, all of the documents would appear in their sorted order (by relevancy score by default)
If the highest sorted document were to have a type field value of “Movies”, the second highest a value of “Comic Books”, the third highest again a value of “Movies” and the fourth highest a value of “Clothing”, then the sorted order of the groups would be “Movies”, “Comic Books”, and then “Clothing”
Inside of each group, the documents are also sorted, by default, in the order of their first appearance.
While grouping on the values within a field provides so very useful search capabilities, as shown in the above examples, much more is possible with Solr’s Result Grouping capabilities.
The next section will begin by demonstrating the ability to group on more than just field values - you will see how to group by arbitrary queries and functions, as well.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
In addition to supporting grouping by unique field values, Solr also supports two additional grouping use cases.
The first of these is similar to grouping on a field, but it instead allows grouping by dynamically computed values by utilizing function queries.
The second additional use case is Solr’s query grouping capability, which essentially allows multiple queries to be run concurrently and returned as separate result sets.
Grouping based upon functions is accomplished using the group.func parameter.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
As you can see in listing 11.7, grouping by a function is conceptually the same as.
Functions can be nested, as you see in this case which nests three map functions, which means you have full control over the values that are computed if you want to manipulate them by combining multiple functions together.
If function grouping is too limiting for your use case, it is also possible to group by a query so that you can specify your own arbitrary values upon which to group.
In section 11.3, you saw that it is possible to collapse search results so that no more than a few documents are returned matching a particular value within a field (using the group.field parameter)
In addition to grouping on pre-defined field values, it can also be useful to dynamically group on arbitrary queries.
For example, a customer-centric user experience could be highly customized to return three sets of search results: those within 50 kilometers of the user, those within the customer’s price range, and those within the customer’s favorite category.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
To demonstrate this capability utilizing our dataset for this chapter, let us see what it would look like to request three somewhat arbitrary query groups: one query that matches all movies, one query that matches anything in a DVD format, and one query which is for the specific product named “The Hunger Games.”  Listing 11.8 demonstrates how to accomplish this.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This is actually true for any kind of grouping query (group.field, group.func, or group.query) – any number of groups can be returned from Solr within a single request.
Second, a query group is essentially a way to perform multiple sub-searches of the original search.
In this case, the initial query was a wide-open search (q=*:*), which allows you to essentially run as many queries as you want within a single request, each returning separate sets of results.
Third, while it is true that a document will only appear once with a grouped result set, it is important to note that when multiple group parameters are used in a Solr request, each set of grouped results can contain that document again.
In other words, it is as if you are literally running multiple searches within the same request, because each of the separately requested query groups can contain the same documents if the query in their corresponding group.query parameter matches those documents.
With this ability to run multiple sub-searches in a single Solr request, some interesting interactions take place between Solr’s grouping functionality and the results paging and document sorting which also occur during the request.
Grouping, because of its richer search results structure, introduces some additional complexity when paging through and sorting search results.
You will recall from chapter 7 that Solr utilizes the rows parameter to determine how many documents to return from Solr for a standard search query.
When grouping results, however, there is an extra layer of complexity – what is it you are actually trying to put a limit upon?  Do you wish to return a certain number of documents per group, a certain number across all groups, or a certain number of groups?  Similar questions exist when using Solr’s start parameter to page through grouped search results and the sort parameter to sort the grouped search results.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
What does it mean to page through and sort search results in the context of multiple groups of search results? In order to handle this additional complexity, Solr’s grouping functionality applies these global parameters – rows, start, and sort – to the groups themselves.
The rows parameter determines how many groups to return, the start parameter controls paging through available groups, and the sort parameter controls how groups are sorted (based upon their top document) as opposed to how documents are sorted across groups.
Should you additionally need to increase the number of documents per group, page through the results within a group, or sort the results within the groups differently, Solr’s Result Grouping functionality has separate parameters to control this.
As indicated in section 11.3, the group.limit parameter specifies the maximum number of results to return per group, performing the behavior that the rows parameter performs on a non-grouped search.
The group.offset parameter allows you to page through the results within a group, performing the behavior that the start parameter provides on a non-grouped search.
Finally, the group.sort parameter allows you to re-sort the documents within your groups, even though they have already been sorted initially by the sort parameter to determine the order in which the groups will appear.
Some very interesting uses for this two-pass sorting could be implemented, with one phase finding the relevant groups of documents and the other phase sorting the documents within that group based upon some other business need.
At the end of the day, perhaps the easiest way to think of paging, sorting, and limiting the results in a grouped request is to think of the groups as the actual documents Solr is returning.
You can page, sort, and limit those “document groups” just like you would page, sort, and limit single documents in a standard search request.
Solr then provides the additional group parameters (group.limit, group.offset, and group.sort) to help you refine the documents within the groups for display.
While Solr’s Grouping capabilities prove useful for many use-cases, there are a few aspects of this functionality that can be a bit tricky to navigate.
Understanding these details is important for determining how to partition your data, what Facet counts represent, and how your query performance will be impacted.
By default, Facet counts are based upon the original query results, not the grouped results.
This means that whether you turn grouping on for a query or not, the facet counts will be the same.
It is possible, however, to only return facet counts for “collapsed” results by setting the parameter group.facet=true.
If you were to turn faceting on and facet on the type field using the ecommerce data for this chapter, you would get the results in listing 11.9
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
The first characteristic of listing 11.9 that you should notice is that while eleven documents matched the filter for type:Movies, only six documents were returned in the grouped search results when the results were collapsed by the product field.
This is because several of the movies contained duplicate documents corresponding with different movie formats (DVD, Blue-ray, and even VHS), so the duplicate documents with the same product name were were collapsed out.
The second characteristic you may notice is that the facet counts.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
From a customer’s perspective, however, there are not eleven unique movies upon which they should be able to facet.
The customer most likely does not care about all the format variations and simply wants to see that there are six unique movies.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
This ability to group on a field and then facet on the collapsed group can come in very handy for situations like the above.
Unfortunately the group.facet=true parameter is global (it is turned on or off for all requested facets), and it cannot be applied to multiple requested results groups in the current version of Solr.
As such, if you do turn on grouped faceting, you should be aware that it only applies to the first requested result grouping.
One very important consideration when utilizing Solr’s Result Grouping functionality is how it interacts with distributed search.
Unlike standard searches, Result Grouping cannot be said to fully work in distributed mode… instead, it is more accurate to say that it works in a pseudo-distributed mode.
Grouping does return aggregated results in distributed mode, but the results are only the aggregates of the groups calculated locally on each Solr core.
Why does this matter? It matters because if the values you are grouping on are randomly distributed across Solr cores then the counts of grouped documents are going to be inaccurate.
If you were grouping a query for products by a field containing the product manufacturer’s name (to see all the unique products for that manufacturer), your total count of groups would be roughly the sum of the group counts for each Solr core you search against in a distributed search.
If and only if your documents are partitioned into separate shards by manufacturer name would you get the correct group count, since each group is guaranteed to only exist on one shard.
If your data is not sharded by the field you are grouping on and you are performing a distributed search, the count of groups that is returned will merely be an upper limit.
In addition to these data partitioning limitations, a few of the grouping parameters simply do not currently work in a distributed mode: group.truncate and group.func, so be careful using these if you think your data will grow beyond what one Solr core can handle.
You saw in section 11.2 how to return grouped results in the simple (flat) grouped format.
You also saw that you could return the first group in the main default results format (like a non-grouped query) by setting the group.main=true parameter.
While both of these options are useful, you should consider wisely whether you can live without the extra information provided by the advanced grouping format.
Without the advanced format, you cannot request the number for groups, for example.
If you are using grouping to collapse documents, this means that you will not know how many unique values were found without using the advanced format.
Because it can often be challenging to change the response format in your application down the line, you should carefully consider which format to use during your initial application development.
Please post comments or corrections to the Author Online forum: http://www.manning-sandbox.com/forum.jspa?forumID=828
While grouping is a very powerful feature, it is considerably slower than a standard Solr query.
Based upon unscientific measurements, some have seen an average query take about three times longer to execute when grouping was requested for the purpose of collapsing down to one unique value (sometimes more, sometimes less depending upon the complexity of the query)
Because Result Grouping is internally implemented as two actual searches, turning this cache on can increase the speed of queries by caching one of those searches.
It is an advanced feature, but it has been demonstrated to improve the performance of Boolean queries, wildcard queries, and fuzzy queries.
Be careful, though, as it has also been shown to decrease performance on very simple queries such as term queries and match all (*:*) queries.
You will need to measure the performance impact for your own search application, including whether to turn group caching on or not, but a query using Result Grouping is likely to demonstrate a performance slowdown relative to a non-grouped query.
You should certainly take the performance impact into consideration when determining whether Solr’s grouping capabilities make sense for your use case.
Result Grouping can also be used to modify the results used for faceting, essentially preventing duplicate documents from being considered each time for facet counts.
You also saw how to return multiple documents per group and how to group on fields, functions, and queries.
Finally, you got to see some of the trickier gotchas associated with using Solr’s Result Grouping functionality, and you saw both the performance impacts of Result Grouping as well as the formats in which grouped results can be returned.
The last five chapters have covered many of Solr’s key search features, and by now you should feel prepared to build a world-class search application based upon Solr.
Taking that search application to production will require some additional work, however, and that effort is the subject of our next chapter.
