Global hash table that all Web processes on all machines could access simultaneously, instantly machines could access simultaneously, instantly seeing one another's changes.
Scale out, not up: many little machines, not big machines.
I didn't want all 40+ machines caching independently and duplicating information.
First written in Perl (high memory usage, can’t handle tons of network connections at once)
Slab allocator Allocates only large chunks of memory, slicing them up into little chunks for particular classes of items, then maintaining freelists for each class whenever an object is freed.
Memcached currently generates slab classes whenever an object is freed.
Client 1 takes the full list of servers (A, B, C), hashes the key against them, then lets say ends up picking server B.
Client 2 takes the full list of servers (A, B, C), hashes the key against them, then ends up picking server B.
April, 2007 – Last.fm developers implemented consistent hashing algorithm for memcached.
Conceptually, these numbers are placed on a circle called the continuum.
Each number links to the server it was hashed from, so servers appear at several points on the continuum, by each of the numbers they hashed to.
If you hash your key to a value near 232 and there are no points on the continuum greater than your hash, return the first server in the continuum.
This works well, except the size of the intervals assigned to each server is pretty hit and miss.
Since it is essentially random it is possible to have a very non-uniform distribution of objects between servers.
The solution to this problem is to introduce the idea of "virtual nodes", which are replicas of server points in the circle.
So whenever we add a server we create a number of points in the circle for it.
