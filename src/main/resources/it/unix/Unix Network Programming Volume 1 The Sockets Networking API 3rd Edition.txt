Everyone will want this book because it provides a great mix of practical experience, historical perspective, and a depth of understanding that only comes from being intimately involved in the field.
To build today's highly distributed, networked applications and services, you need deep mastery of sockets and other key networking APIs.
One book delivers comprehensive, start-to-finish guidance for building robust, high-performance networked systems in any environment: UNIX Network Programming, Volume 1, Third Edition.
Richard Stevens, this edition has been fully updated by two leading network programming experts to address today's most crucial standards, implementations, and techniques.
Everyone will want this book because it provides a great mix of practical experience, historical perspective, and a depth of understanding that only comes from being intimately involved in the field.
To build today's highly distributed, networked applications and services, you need deep mastery of sockets and other key networking APIs.
One book delivers comprehensive, start-to-finish guidance for building robust, high-performance networked systems in any environment: UNIX Network Programming, Volume 1, Third Edition.
Richard Stevens, this edition has been fully updated by two leading network programming experts to address today's most crucial standards, implementations, and techniques.
The authors also update and extend Stevens' definitive coverage of these crucial UNIX networking standards and techniques:
Since 1990, network programmers have turned to one source for the insights and techniques they need: W.
Now, there's an edition specifically designed for today's challenges-and tomorrow's.
Copyright Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and Addison-Wesley was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals.
The authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions.
No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein.
The publisher offers discounts on this book when ordered in quantity for bulk purchases and special sales.
A CIP catalog record for this book can be obtained from the Library of Congress.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form, or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior consent of the publisher.
For information on obtaining permission for use of material from this work, please submit a written request to:
Foreword When the original text of this book arrived in 1990, it was quickly recognized as the definitive reference for programmers to learn network programming techniques.
Since then, the art of computer networking has changed dramatically.
Today, UUCP networks are a rarity and new technologies such as wireless networks are becoming ubiquitous! With these changes, new network protocols and programming paradigms have been developed.
But, programmers have lacked a good reference from which to learn the intricacies of these new techniques.
Readers who have a dog-eared copy of the original book will want a new copy for the updated programming techniques and the substantial new material describing nextgeneration protocols such as IPv6
Everyone will want this book because it provides a great mix of practical experience, historical perspective, and a depth of understanding that only comes from being intimately involved in the field.
I've already enjoyed and learned from reading this book, and surely you will, too.
Introduction This book is for people who want to write programs that communicate with each other using an application program interface (API) known as sockets.
Some readers may be very familiar with sockets already, as that model has become synonymous with network programming.
Others may need an introduction to sockets from the ground up.
The goal of this book is to offer guidance on network programming for beginners as well as professionals, for those developing new networkaware applications as well as those maintaining existing code, and for people who simply want to understand how the networking components of their system function.
All the examples in this text are actual, runnable code tested on Unix systems.
However, many non-Unix systems support the sockets API and the examples are largely operating systemindependent, as are the general concepts we present.
Virtually every operating system (OS) provides numerous network-aware applications such as Web browsers, email clients, and filesharing servers.
We discuss the usual partitioning of these applications into client and server and write our own small examples of these many times throughout the text.
Presenting this material in a Unix-oriented fashion has the natural side effect of providing background on Unix itself, and on TCP/IP as well.
Where more extensive background may be interesting, we refer the reader to other texts.
Four texts are so commonly mentioned in this book that we've assigned them the following abbreviations:
If one understands the implementation of a feature, the use of that feature in an application makes more sense.
Changes from the Second Edition Sockets have been around, more or less in their current form, since the 1980s, and it is a tribute to their initial design that they have continued to be the network API of choice.
The changes we've made to the text are summarized as follows:
This new edition contains updated information on IPv6, which was only in draft form at the time of publication of the second edition and has evolved somewhat.
The coverage of the X/Open Transport Interface (XTI) has been dropped.
That API has fallen out of common use and even the most recent POSIX specification does not bother to cover it.
The coverage of TCP for transactions (T/TCP) has been dropped.
Three chapters have been added to describe a relatively new transport protocol, SCTP.
This reliable, message-oriented protocol provides multiple streams between endpoints and transport-level support for multihoming.
It was originally designed for transport of telephony signaling across the Internet, but provides some features that many applications could take advantage of.
A chapter has been added on key management sockets, which may be used with Internet Protocol Security (IPsec) and other network security services.
The machines used, as well as the versions of their variants of Unix, have all been updated, and the examples have been updated to reflect how these machines behave.
In many cases, examples were updated because OS vendors fixed bugs or added features, but as one might expect, we've discovered the occasional new bug here and there.
The machines used for testing the examples in this book were:
See Figure 1.16 for details on how these machines were used.
Volume 2 of this UNIX Network Programming series, subtitled Interprocess Communications, builds on the material presented here to cover message passing, synchronization, shared memory, and remote procedure calls.
Using This Book This text can be used as either a tutorial on network programming or as a reference for experienced programmers.
Part 2 covers the basic socket functions for both TCP and UDP, along with SCTP, I/O multiplexing, socket options, and basic name and address conversions.
Chapter 2 and perhaps Appendix A should be referred to as necessary, depending on the reader's background.
Most of the chapters in Part 3, "Advanced Sockets," can be read independently of the others in that part of the book.
To aid in the use of this book as a reference, a thorough index is provided, along with summaries on the end papers of where to find detailed descriptions of all the functions and structures.
To help those reading topics in a random order, numerous references to related topics are provided throughout the text.
The best way to learn network programming is to take these programs, modify them, and enhance them.
Actually writing code of this form is the only way to reinforce the concepts and techniques.
Numerous exercises are also provided at the end of each chapter, and most answers are provided in Appendix E.
A current errata for the book is also available from the same Web site.
Acknowledgments The first and second editions of this book were written solely by W.
His books have set a high standard and are largely regarded as concise, laboriously detailed, and extremely readable works of art.
In providing this revision, the authors struggled to maintain the quality and thorough coverage of Rich's earlier editions and any shortcomings in this area are entirely the fault of the new authors.
The coverage of this new and interesting topic simply would not exist without Randall's work.
The feedback from our reviewers was invaluable for catching errors, pointing out areas that required more explanation, and suggesting improvements to our text and code examples.
Numerous individuals and their organizations went beyond the normal call of duty to provide either a loaner system, software, or access to a system, all of which were used to test some of the examples in the text.
In a trend that Rich Stevens instituted (but contrary to popular fads), we produced camera-ready copy of the book using the wonderful Groff package written by James Clark, created the illustrations using the gpic program (using many of Gary Wright's macros), produced the tables using the gtbl program, performed all the indexing, and did the final page layout.
Dave Hanson's loom program and some scripts by Gary Wright were used to include the source code in the book.
A set of awk scripts written by Jon Bentley and Brian Kernighan helped in producing the final index.
The authors welcome electronic mail from any readers with comments, suggestions, or bug fixes.
When writing programs that communicate across a computer network, one must first invent a protocol, an agreement on how those programs will communicate.
Before delving into the design details of a protocol, high-level decisions must be made about which program is expected to initiate communication and when responses are expected.
For example, a Web server is typically thought of as a long-running program (or daemon) that sends network messages only in response to requests coming in from the network.
The other side of the protocol is a Web client, such as a browser, which always initiates communication with the server.
This organization into client and server is used by most network-aware applications.
Deciding that the client always initiates requests tends to simplify the protocol as well as the programs themselves.
Of course, some of the more complex network applications also require asynchronous callback communication, where the server initiates a message to the client.
But it is far more common for applications to stick to the basic client/server model shown in Figure 1.1
Clients normally communicate with one server at a time, although using a Web browser as an example, we might communicate with many different Web servers over, say, a 10-minute time period.
But from the server's perspective, at any given point in time, it is not unusual for a server to be communicating with multiple clients.
Later in this text, we will cover several different ways for a server to handle multiple clients at the same time.
The client application and the server application may be thought of as communicating via a network protocol, but actually, multiple layers of network protocols are typically involved.
TCP, in turn, uses the Internet Protocol, or IP, and IP communicates with a datalink layer of some form.
If the client and server are on the same Ethernet, we would have the arrangement shown in Figure 1.3
Client and server on the same Ethernet communicating using TCP.
Even though the client and server communicate using an application protocol, the transport layers communicate using TCP.
Note that the actual flow of information between the client and server goes down the protocol stack on one side, across the network, and up the protocol stack on the other side.
Also note that the client and server are typically user processes, while the TCP and IP protocols are normally part of the protocol stack within the kernel.
We have labeled the four layers on the right side of Figure 1.3
The client and server need not be attached to the same local area network (LAN) as we show in Figure 1.3
For instance, in Figure 1.4, we show the client and server on different LANs, with both LANs connected to a wide area network (WAN) using routers.
Client and server on different LANs connected through a WAN.
Many companies build their own WANs and these private WANs may or may not be connected to the Internet.
The remainder of this chapter provides an introduction to the various topics that are covered in detail later in the text.
We start with a complete example of a TCP client, albeit a simple one, that demonstrates many of the function calls and concepts that we will encounter throughout the text.
This chapter also shows a complete TCP server that works with our client.
To simplify all our code, we define our own wrapper functions for most of the system functions that we call throughout the text.
We can use these wrapper functions most of the time to check for an error, print an appropriate message, and terminate when an error occurs.
We also show the test network, hosts, and routers used for most examples in the text, along with their hostnames, IP addresses, and operating systems.
Most discussions of Unix these days include the term "X," which is the standard that most vendors have adopted.
We describe the history of POSIX and how it affects the Application Programming Interfaces (APIs) that we describe in this text, along with the other players in the standards arena.
Let's consider a specific example to introduce many of the concepts and terms that we will encounter throughout the book.
Figure 1.5 is an implementation of a TCP time-of-day client.
This client establishes a TCP connection with a server and the server simply sends back the current time and date in a human-readable format.
This is the format that we will use for all the source code in the text.
The text describing portions of the code notes the starting and ending line numbers in the left margin, as shown shortly.
The horizontal rules at the beginning and end of a code fragment specify the source code filename: the file daytimetcpcli.c in the directory intro for this example.
Since the source code for all the examples in the text is freely available (see the Preface), this lets you locate the appropriate source file.
Compiling, running, and especially modifying these programs while reading this text is an excellent way to learn the concepts of network programming.
Throughout the text, we will use indented, parenthetical notes such as this to describe implementation details and historical points.
If we compile the program into the default a.out file and execute it, we will have the following output:
Whenever we display interactive input and output, we will show our typed input in bold and the computer output like this.
We will always include the name of the system as part of the shell prompt (solaris in this example) to show on which host the command was run.
Figure 1.16 shows the systems used to run most of the examples in this book.
The hostnames usually describe the operating system (OS) as well.
There are many details to consider in this 27-line program.
We mention them briefly here, in case this is your first encounter with a network program, and provide more information on these topics later in the text.
We include our own header, unp.h, which we will show in Section D.1
This header includes numerous system headers that are needed by most network programs and defines various constants that we use (e.g., MAXLINE)
This is the definition of the main function along with the command-line arguments.
We have written the code in this text assuming an American National Standards Institute (ANSI) C compiler (also referred to as an ISO C compiler)
The function returns a small integer descriptor that we can use to identify the socket in all future function calls (e.g., the calls to connect and read that follow)
The set of parentheses around the function call and assignment is required, given the precedence rules of C (the less-than operator has a higher precedence than assignment)
As a matter of coding style, the authors always place a space between the two opening parentheses, as a visual indicator that the left-hand side of the comparison is also an assignment.
This style is copied from the Minix source code [Tanenbaum 1987]
We use this same style in the while statement later in the program.
We will encounter many different uses of the term "socket." First, the API that we are using is called the sockets API.
In the preceding paragraph, we referred to a function named socket that is part of the sockets API.
In the preceding paragraph, we also referred to a TCP socket, which is synonymous with a TCP endpoint.
If the call to socket fails, we abort the program by calling our own err_sys function.
It prints our error message along with a description of the system error that occurred (e.g., "Protocol not supported" is one possible error from socket) and terminates the process.
This function, and a few others of our own that begin with err_, are called throughout the text.
We fill in an Internet socket address structure (a sockaddr_in structure named servaddr) with the server's IP address and port number.
Nevertheless, we use it throughout the text, instead of the ANSI C memset function, because bzero is easier to remember (with only two arguments) than memset (with three arguments)
Almost every vendor that supports the sockets API also provides bzero, and if not, we provide a macro definition of it in our unp.h header.
A C compiler cannot catch this error because both arguments are of the same type.
Nevertheless, it was an error, and one that could be avoided by using bzero, because swapping the two.
This may be your first encounter with the inet_pton function.
It is new with IPv6 (which we will talk more about in Appendix A)
Do not worry if your system does not (yet) support this function; we will provide an implementation of it in Section 3.7
The connect function, when applied to a TCP socket, establishes a TCP connection with the server specified by the socket address structure pointed to by the second argument.
We must also specify the length of the socket address structure as the third argument to connect, and for Internet socket address structures, we always let the compiler calculate the length using C's sizeof operator.
In the unp.h header, we #define SA to be struct sockaddr, that is, a generic socket address structure.
Everytime one of the socket functions requires a pointer to a socket address structure, that pointer must be cast to a pointer to a generic socket address structure.
The problem is that "struct sockaddr" is 15 characters and often causes the source code line to extend past the right edge of the screen (or page, in the case of a book), so we shorten it to SA.
We will talk more about generic socket address structures when explaining Figure 3.3
We read the server's reply and display the result using the standard I/O fputs function.
We must be careful when using TCP because it is a byte-stream protocol with no record boundaries.
The server's reply is normally a 26-byte string of the form.
Normally, a single segment containing all 26 bytes of data is returned, but with larger data sizes, we cannot assume that the server's reply will be returned by a single read.
In this example, the end of the record is being denoted by the server closing the connection.
This technique is also used by version 1.0 of the Hypertext Transfer Protocol (HTTP)
For example, the Simple Mail Transfer Protocol (SMTP) marks the end of a record with the two-byte sequence of an ASCII carriage return followed by an ASCII linefeed.
Remote Procedure Call (RPC) and the Domain Name System (DNS) place a binary count containing the record length in front of each record that is sent when using TCP.
The important concept here is that TCP itself provides no record markers: If an application wants to delineate the ends of records, it must do so itself and there are a few common ways to accomplish this.
Unix always closes all open descriptors when a process terminates, so our TCP socket is now closed.
As we mentioned, the text will go into much more detail on all the points we just described.
To modify the program to work under IPv6, we must change the code.
Only five lines are changed, but what we now have is another protocol-dependent program; this.
In Chapter 11, we will discuss the functions that convert between hostnames and IP addresses, and between service names and ports.
We purposely put off the discussion of these functions and continue using IP addresses and port numbers so we know exactly what goes into the socket address structures that we must fill in and examine.
This also avoids complicating our discussion of network programming with the details of yet another set of functions.
In any real-world program, it is essential to check every function call for an error return.
We find that most of the time, this is what we want to do.
Occasionally, we want to do something other than terminate when one of these functions returns an error, as in Figure 5.12, when we must check for an interrupted system call.
Since terminating on an error is the common case, we can shorten our programs by defining a wrapper function that performs the actual function call, tests the return value, and terminates on an error.
The convention we use is to capitalize the name of the function, as in.
Whenever you encounter a function name in the text that begins with an uppercase letter, that is one of our wrapper functions.
It calls a function whose name is the same but begins with the lowercase letter.
When describing the source code that is presented in the text, we always refer to the lowest level function being called (e.g., socket), not the wrapper function (e.g., Socket)
While these wrapper functions might not seem like a big savings, when we discuss threads in Chapter 26, we will find that thread functions do not set the standard Unix errno variable when an error occurs; instead, the errno value is the return value of the function.
This means that every time we call one of the pthread_ functions, we must allocate a variable, save the return.
To avoid cluttering the code with braces, we can use C's comma operator to combine the assignment into errno and the call of err_sys into a single statement, as in the following:
Alternately, we could define a new error function that takes the system's error number as an argument.
But, we can make this piece of code much easier to read as just.
With careful C coding, we could use macros instead of functions, providing a little run-time efficiency, but these wrapper functions are rarely the performance bottleneck of a program.
Our choice of capitalizing the first character of a function name is a compromise.
Our style seems the least distracting while still providing a visual indication that some other function is really being called.
This technique has the side benefit of checking for errors from functions whose error returns.
Throughout the rest of this book, we will use these wrapper functions unless we need to check for an explicit error and handle it in some way other than terminating the process.
We do not show the source code for all our wrapper functions, but the code is freely available (see the Preface)
When an error occurs in a Unix function (such as one of the socket functions), the global variable errno is set to a positive value indicating the type of error and the function normally returns –1
Our err_sys function looks at the value of errno and prints the corresponding error message string (e.g., "Connection timed out" if errno equals ETIMEDOUT)
The value of errno is set by a function only if an error occurs.
Its value is undefined if the function does not return an error.
All of the positive error values are constants with all-uppercase names beginning with "E," and are normally defined in the <sys/errno.h> header.
Storing errno in a global variable does not work with multiple threads that share all global variables.
Throughout the text, we will use phrases such as "the connect function returns ECONNREFUSED" as shorthand to mean that the function returns an error (typically with a return value of –1), with errno set to the specified constant.
We can write a simple version of a TCP daytime server, which will work with the client from Section 1.2
We use the wrapper functions that we described in the previous section and show this server in Figure 1.9
The creation of the TCP socket is identical to the client code.
The server's well-known port (13 for the daytime service) is bound to the socket by filling in an Internet socket address structure and calling bind.
We specify the IP address as INADDR_ANY, which allows the server to accept a client connection on any interface, in case the server host has multiple interfaces.
Later we will see how we can restrict the server to accepting a client connection on just a single interface.
By calling listen, the socket is converted into a listening socket, on which incoming connections from clients will be accepted by the kernel.
These three steps, socket, bind, and listen, are the normal steps for any TCP server to prepare what we call the listening descriptor (listenfd in this example)
It specifies the maximum number of client connections that the kernel will queue for this listening descriptor.
We say much more about this queueing in Section 4.5
Normally, the server process is put to sleep in the call to accept, waiting for a client connection to arrive and be accepted.
A TCP connection uses what is called a three-way handshake to establish a connection.
When this handshake completes, accept returns, and the return value from the function is a new descriptor (connfd) that is called the connected descriptor.
This new descriptor is used for communication with the new client.
A new descriptor is returned by accept for each client that connects to our server.
The style used throughout the book for an infinite loop is.
The next library function, ctime, converts this integer value into a human-readable string such as.
A carriage return and linefeed are appended to the string by snprintf, and the result is written to the client by write.
If you're not already in the habit of using snprintf instead of the older sprintf, now's the time to learn.
Calls to sprintf cannot check for overflow of the destination buffer.
Virtually all vendors provide it as part of the standard C library, and many freely available versions are also available.
We use snprintf throughout the text, and we recommend using it instead of sprintf in all your programs for reliability.
It is remarkable how many network break-ins have occurred by a hacker sending data to cause a server's call to sprintf to overflow its buffer.
Other functions that we should be careful with are gets, strcat, and strcpy, normally calling fgets, strncat, and strncpy instead.
Even better are the more recently available functions strlcat and strlcpy, which ensure the result is a properly terminated string.
The server closes its connection with the client by calling close.
This initiates the normal TCP connection termination sequence: a FIN is sent in each direction and each FIN is acknowledged by the other end.
We will say much more about TCP's three-way handshake and the four TCP packets used to terminate a TCP connection in Section 2.6
As with the client in the previous section, we have only examined this server briefly, saving all the details for later in the book.
As with the client, the server is protocol-dependent on IPv4
If multiple client connections arrive at about the same time, the kernel queues them, up to some limit, and returns them to accept one at a time.
This daytime server, which requires calling two library functions, time and ctime, is quite fast.
But if the server took more time to service each client (say a few seconds or a minute), we would need some way to overlap the service of one client with another client.
The server that we show in Figure 1.9 is called an iterative server because it iterates through each client, one at a time.
There are numerous techniques for writing a concurrent server, one that handles multiple clients at the same time.
The simplest technique for a concurrent server is to call the Unix fork function (Section 4.7), creating one child process for each client.
If we start a server like this from a shell command line, we might want the server to run for a long time, since servers often run for as long as the system is up.
This requires that we add code to the server to run correctly as a Unix daemon: a process that can run in the background, unattached to a terminal.
Two client/server examples are used predominantly throughout the text to illustrate the various techniques used in network programming:
To provide a roadmap for the different topics that are covered in this text, we will summarize the programs that we will develop, and give the starting figure number and page number in which the source code appears.
Figure 1.10 lists the versions of the daytime client, two versions of which we have already seen.
Different versions of the daytime client developed in the text.
Different versions of the daytime server developed in the text.
Different versions of the echo client developed in the text.
Different versions of the echo server developed in the text.
A common way to describe the layers in a network is to use the International Organization for Standardization (ISO) open systems interconnection (OSI) model for computer communications.
This is a seven-layer model, which we show in Figure 1.14, along with the approximate mapping to the Internet protocol suite.
We consider the bottom two layers of the OSI model as the device driver and networking hardware that are supplied with the system.
The upper three layers of the OSI model are combined into a single layer called the application.
This is the Web client (browser), Telnet client, Web server, FTP server, or whatever application we are using.
With the Internet protocols, there is rarely any distinction between the upper three layers of the OSI model.
The sockets programming interfaces described in this book are interfaces from the upper three layers (the "application") into the transport layer.
This is the focus of this book: how to write applications using sockets that use either TCP or UDP.
We already mentioned raw sockets, and in Chapter 29 we will see that we can even bypass the IP layer completely to read and write our own datalink-layer frames.
Why do sockets provide the interface from the upper three layers of the OSI model into the transport layer? There are two reasons for this design, which we note on the right side of Figure 1.14
The lower four layers know little about the application, but handle all the communication details: sending data, waiting for acknowledgments, sequencing data that arrives out of order, calculating and verifying checksums, and so on.
The second reason is that the upper three layers often form what is called a user process while the lower four layers are normally provided as part of the operating system (OS) kernel.
Unix provides this separation between the user process and the kernel, as do many other contemporary operating systems.
Figure 1.15 shows the development of the various BSD releases, noting the major TCP/IP developments.
Therefore, starting in 1989, Berkeley provided the first of the BSD networking releases, which contained all the networking code and various other pieces of the BSD system that were not constrained by the Unix source code license requirement.
These releases were "publicly available" and eventually became available by anonymous FTP to anyone.
We note that these two releases were then used as the base for other systems: BSD/OS, FreeBSD, NetBSD, and OpenBSD, most of which are still being actively developed and enhanced.
Many Unix systems started with some version of the BSD networking code, including the sockets API, and we refer to these implementations as Berkeley-derived implementations.
We also note that Linux, a popular, freely available implementation of Unix, does not fit into the Berkeley-derived classification: Its networking code and sockets API were developed from scratch.
Figure 1.16 shows the various networks and hosts used in the examples throughout the text.
For each host, we show the OS and the type of hardware (since some of the operating systems run on more than one type of hardware)
The name within each box is the hostname that appears in the text.
The topology shown in Figure 1.16 is interesting for the sake of our examples, but the machines are largely spread out across the Internet and the physical topology becomes less interesting in practice.
Instead, virtual private networks (VPNs) or secure shell (SSH) connections provide connectivity between these machines regardless of where they live physically.
Networks and hosts used for most examples in the text.
The notation "/24" indicates the number of consecutive bits starting from the leftmost bit of the address used to identify the network and subnet.
Section A.4 will talk about the /n notation used today to designate subnet boundaries.
We show the network topology in Figure 1.16 for the hosts used for the examples throughout this text, but you may need to know your own network topology to run the examples and exercises on your own network.
Although there are no current Unix standards with regard to network configuration and administration, two basic commands are provided by most Unix systems and can be used to discover some details of a network: netstat and ifconfig.
Check the manual (man) pages for these commands on your system to see the details on the information that is.
Also be aware that some vendors place these commands in an administrative directory, such as /sbin or /usr/sbin , instead of the normal /usr/bin , and these directories might not be in your normal shell search path (PATH )
We also specify the -n flag to print numeric addresses, instead of trying to find names for the networks.
The loopback interface is called lo and the Ethernet is called eth0
We normally specify the -n flag to print numeric addresses.
This also shows the IP address of the default router.
Given the interface names, we execute ifconfig to obtain the details for each interface.3
This shows the IP address, subnet mask, and broadcast address.
The MULTICAST flag is often an indication that the host supports multicasting.
Some implementations provide a -a flag, which prints information on all configured interfaces.
One way to find the IP address of many hosts on the local network is to ping the broadcast address (which we found in the previous step)
At the time of this writing, the most interesting Unix standardization activity was being done by The Austin Common Standards Revision Group (CSRG)
In this text, we will refer to this standard as simply The POSIX Specification, except in sections like this one where we are discussing specifics of various older standards.
The easiest way to acquire a copy of this consolidated standard is to either order it on CD-ROM or access it via the Web (free of charge)
The POSIX standards have also been adopted as international standards by ISO and the International Electrotechnical Commission (IEC), called ISO/IEC.
The POSIX standards have an interesting history, which we cover only briefly here:
It specified the C language interface into a Unix-like kernel and covered the following areas: process primitives (fork, exec, signals, and timers), the environment of a process (user IDs and process groups), files and directories (all the I/O functions), terminal I/O, system databases (password file and group file), and the tar and cpio archive formats.
The first POSIX standard was a trial-use version in 1986 known as "IEEE-IX." The name "POSIX" was suggested by Richard Stallman.
Appended to the title was "Part 1: System Application Program Interface (API) [C Language]," indicating that this standard was the C language API.
Throughout this text, we will refer to this standard as POSIX.2
Three chapters on threads were added, along with additional sections on thread synchronization (mutexes and condition variables), thread scheduling, and synchronization scheduling.
Throughout this text, we will refer to this standard as POSIX.1
This standard also contains a Foreword stating that ISO/IEC 9945 consists of the following parts:
Over one-quarter of the 743 pages are an appendix titled "Rationale and Notes." This appendix contains historical information and reasons why certain features were included or omitted.
Often, the rationale is as informative as the official standard.
Until the introduction of The Single Unix Specification Version 3, this POSIX work was the most relevant to the topics covered in this book.
This is the networking API standard and it defines two APIs, which it calls Detailed Network Interfaces (DNIs):
Throughout this text, we will refer to this standard as POSIX.1g.
The current status of the various POSIX standards is available from.
It is an international consortium of vendors and end-user customers from industry, government, and academia.
Here is a brief background on the standards they produced:
The latest name for this set of specifications is the "X/Open Single Unix Specification," although it is also called "Unix 95."
The networking services that are part of Unix 98 are defined for both the sockets and XTI APIs.
We will avoid this use of XNS and refer to this X/Open document as just the Unix 98 network API standard.
Getting over 50 companies to agree on a single standard is certainly a landmark in the history of Unix.
Historically, most Unix systems show either a Berkeley heritage or a System V heritage, but these differences are slowly disappearing as most vendors adopt the standards.
The main differences still existing deal with system administration, one area that no standard currently addresses.
The focus of this book is on The Single Unix Specification Version 3, with our main focus on the sockets API.
The Internet Engineering Task Force (IETF) is a large, open, international community of network designers, operators, vendors, and researchers concerned with the evolution of the Internet architecture and the smooth operation of the Internet.
Internet standards normally deal with protocol issues and not with programming APIs.
These are informational RFCs, not standards, and were produced to speed the deployment of portable applications by the numerous vendors working on early releases of IPv6
From a programming perspective, the LP64 model means we cannot assume that a pointer can be stored in an integer.
We must also consider the effect of the LP64 model on existing APIs.
In both instances, there is no need for a 64-bit datatype: The length of a socket address structure is a few hundred bytes at most, and the use of long for the XTI structure members was a mistake.
The solution is to use datatypes designed specifically to handle these scenarios.
These two examples introduce many of the terms and concepts that are expanded on throughout the rest of the book.
Throughout the text, we will use the wrapper functions developed in Section 1.4 to reduce the size of our code, yet still check every function call for an error return.
The Single Unix Specification Version 3, known by several other names and called simply The POSIX Specification by us, is the confluence of two long-running standards efforts, finally drawn together by The Austin Group.
Compile and test the TCP daytime client in Figure 1.5
Run the program a few times, specifying a different IP address as the command-line argument each time.
What happens? Find the errno value corresponding to the error that is printed.
Next, change the single call to write into a loop that calls write for each byte of the result string.
Compile this modified server and start it running in the background.
Start this client, specifying the IP address of the host on which the modified server is running as the command-line argument.
What value is printed as the client's counter? If possible, also try to run the client and server on different hosts.
This chapter provides an overview of the protocols in the TCP/IP suite that are used in the examples throughout the book.
Our goal is to provide enough detail from a network programming perspective to understand how to use the protocols and provide references to more detailed descriptions of their actual design, implementation, and history.
We need to understand the services provided by these transport protocols to the application, so that we know what is handled by the protocol and what we must handle in the application.
There are features of TCP that, when understood, make it easier for us to write robust clients and servers.
Also, when we understand these features, it becomes easier to debug our clients and servers using commonly provided tools such as netstat.
We cover various topics in this chapter that fall into this category: TCP's three-way handshake, TCP's connection termination sequence, and TCP's TIME_WAIT state; SCTP's four-way handshake and SCTP's connection termination; plus SCTP, TCP, and UDP buffering by the socket layer, and so on.
Although the protocol suite is called "TCP/IP," there are more members of this family than just TCP and IP.
The leftmost application, tcpdump, communicates directly with the datalink using either the BSD packet filter (BPF) or the datalink provider interface (DLPI)
We mark the dashed line beneath the nine applications on the right as the API, which is normally sockets or XTI.
The interface to either BPF or DLPI does not use sockets or XTI.
We also note in Figure 2.1 that the traceroute program uses two sockets: one for IP and another for ICMP.
We now describe each of the protocol boxes in this figure.
There is no guarantee that UDP datagrams ever reach their intended destination.
The word "association" is used when referring to a connection in SCTP because SCTP is multihomed, involving a set of IP addresses and a single port for each side of an association.
These messages are normally generated by and processed by the TCP/IP networking software itself, not user processes, although we show the ping and traceroute programs, which use ICMP.
It is sometimes used when a diskless node is booting.
Each Internet protocol is defined by one or more documents called a Request for Comments (RFC), which are their formal specifications.
The solution to Exercise 2.1 shows how to obtain RFCs.
Additional details on the TCP/IP protocols themselves are in TCPv1
The application writes a message to a UDP socket, which is then encapsulated in a UDP datagram, which is then further encapsulated as an IP datagram, which is then sent to its destination.
There is no guarantee that a UDP datagram will ever reach its final destination, that order will be preserved across the network, or that datagrams arrive only once.
The problem that we encounter with network programming using UDP is its lack of reliability.
If a datagram reaches its final destination but the checksum detects an error, or if the datagram is dropped in the network, it is not delivered to the UDP socket and is not automatically retransmitted.
If we want to be certain that a datagram reaches its destination, we can build lots of features into our application: acknowledgments from the other end, timeouts, retransmissions, and the like.
The length of a datagram is passed to the receiving application along with the data.
We have already mentioned that TCP is a byte-stream protocol, without any record boundaries at all (Section 1.2), which differs from UDP.
We also say that UDP provides a connectionless service, as there need not be any long-term relationship between a UDP client and server.
For example, a UDP client can create a socket and send a datagram to a given server and then immediately send another datagram on the same socket to a different server.
Similarly, a UDP server can receive several datagrams on a single UDP socket, each from a different client.
A TCP client establishes a connection with a given server, exchanges data with that server across the connection, and then terminates the connection.
When TCP sends data to the other end, it requires an acknowledgment in return.
If an acknowledgment is not received, TCP automatically retransmits the data and waits a longer amount of time.
Note that TCP does not guarantee that the data will be received by the other endpoint, as this is impossible.
It delivers data to the other endpoint if possible, and notifies the user (by giving up on retransmissions and breaking the connection) if it is not possible.
Therefore, TCP cannot be described as a 100% reliable protocol; it provides reliable delivery of data or reliable notification of failure.
For example, the RTT on a LAN can be milliseconds while across a WAN, it can be seconds.
Furthermore, TCP continuously estimates the RTT of a given connection, because the RTT is affected by variations in the network traffic.
A segment is the unit of data that TCP passes to IP.
If the segments arrive out of order, the receiving TCP will reorder the two segments based on their sequence numbers before passing the data to the receiving application.
If TCP receives duplicate data from its peer (say the peer thought a segment was lost and retransmitted it, when it wasn't really lost, the network was just overloaded), it can detect that the data has been duplicated (from the sequence numbers), and discard the duplicate data.
If a UDP datagram is duplicated in the network, two copies can be delivered to the receiving host.
Also, if a UDP client sends two datagrams to the same destination, they can be reordered by the network and arrive out of order.
At any time, the window is the amount of room currently available in the receive buffer, guaranteeing that the sender cannot overflow the receive buffer.
The window changes dynamically over time: As data is received from the sender, the window size decreases, but as the receiving application reads data from the buffer, the window size increases.
It is possible for the window to reach 0: when TCP's receive buffer for a socket is full and it must wait for the application to read data from the buffer before it can take any more data from the peer.
It is easy for a fast UDP sender to transmit datagrams at a rate that the UDP receiver cannot keep up with, as we will show in Section 8.13
This means that an application can send and receive data in both directions on a given connection at any time.
This means that TCP must keep track of state information such as sequence numbers and window sizes for each direction of data flow: sending and receiving.
After a full-duplex connection is established, it can be turned into a simplex connection if desired (see Section 6.6)
The word "association" is used in SCTP instead of "connection" to avoid the connotation that a connection involves communication between only two IP addresses.
An association refers to a communication between two systems, which may involve more than two addresses due to multihoming.
Like UDP, the length of a record written by the sender is passed to the receiving application.
A lost message in one of these streams does not block delivery of messages in any of the other streams.
This approach is in contrast to TCP, where a loss at any point in the single stream of bytes blocks delivery of all future data on the connection until the loss is repaired.
An endpoint can have multiple redundant network connections, where each of these networks has a different connection to the Internet infrastructure.
Similar robustness can be obtained from TCP with help from routing protocols.
For example, BGP connections within a domain (iBGP) often use addresses that are assigned to a virtual interface within the router as the endpoints of the TCP connection.
The domain's routing protocol ensures that if there is a route between two routers, it can be used, which would not be possible if the addresses used belonged to an interface that went down, for example.
SCTP's multihoming feature allows hosts to multihome, not just routers, and allows this multihoming to occur across different service providers, which the routing-based TCP method cannot allow.
To aid in our understanding of the connect, accept, and close functions and to help us debug TCP applications using the netstat program, we must understand how TCP connections are established and terminated, and TCP's state transition diagram.
The following scenario occurs when a TCP connection is established:
The server must be prepared to accept an incoming connection.
This is normally done by calling socket, bind, and listen and is called a passive open.
This causes the client TCP to send a "synchronize" (SYN) segment, which tells the server the client's initial sequence number for the data that the client will send on the connection.
Normally, there is no data sent with the SYN; it just contains an IP header, a TCP header, and possible TCP options (which we will talk about shortly)
The server must acknowledge (ACK) the client's SYN and the server must also send its own SYN containing the initial sequence number for the data that the server will send on the connection.
The server sends its SYN and the ACK of the client's SYN in a single segment.
The minimum number of packets required for this exchange is three; hence, this is called TCP's three-way handshake.
We show the client's initial sequence number as J and the server's initial sequence number as K.
The acknowledgment number in an ACK is the next expected sequence number for the end sending the ACK.
Since a SYN occupies one byte of the sequence number space, the acknowledgment number in the ACK of each SYN is the initial sequence number plus one.
Similarly, the ACK of each FIN is the sequence number of the FIN plus one.
An everyday analogy for establishing a TCP connection is the telephone system [Nemeth 1997]
The socket function is the equivalent of having a telephone to use.
Having the client's identity returned by accept (where the identify is the client's IP address and port number) is similar to having the caller ID feature show the caller's phone number.
One difference, however, is that accept returns the client's identity only after the connection has been established, whereas the caller ID feature shows the caller's phone number before we choose whether to answer the phone or not.
If the DNS is used (Chapter 11), it provides a service analogous to a telephone book.
With this option, the TCP sending the SYN announces its maximum segment size, the maximum amount of data that it is willing to accept in each TCP segment, on this connection.
The sending TCP uses the receiver's MSS value as the maximum size of a segment that it sends.
Both endsystems must support this option for the window scale to be used on a connection.
To provide interoperability with older implementations that do not support this option, the following rules apply.
But, it can scale its windows only if the other end also sends the option with its SYN.
Similarly, the server's TCP can send this option only if it receives the option with the client's SYN.
This logic assumes that implementations ignore options that they do not understand, which is required and common, but unfortunately, not guaranteed with all implementations.
This option is needed for high-speed connections to prevent possible data corruption caused by old, delayed, or duplicated segments.
Since it is a newer option, it is negotiated similarly to the window scale option.
As network programmers there is nothing we need to worry about with this option.
They are also called the "long fat pipe options," since a network with either a high bandwidth or a long delay is called a long fat pipe.
While it takes three segments to establish a connection, it takes four to terminate a connection.
One application calls close first, and we say that this end performs the active close.
This end's TCP sends a FIN segment, which means it is finished sending data.
The other end that receives the FIN performs the passive close.
The receipt of the FIN is also passed to the application as an end-offile (after any data that may have already been queued for the application to receive), since the receipt of the FIN means the application will not receive any additional data on the connection.
Sometime later, the application that received the end-of-file will close its socket.
The TCP on the system that receives this final FIN (the end that did the active close) acknowledges the FIN.
Since a FIN and an ACK are required in each direction, four segments are normally required.
We use the qualifier "normally" because in some scenarios, the FIN in Step 1 is sent with data.
A FIN occupies one byte of sequence number space just like a SYN.
Therefore, the ACK of each FIN is the sequence number of the FIN plus one.
This is called a half-close and we will talk about this in detail with the shutdown function in Section 6.6
The sending of each FIN occurs when a socket is closed.
We indicated that the application calls close for this to happen, but realize that when a Unix process terminates, either voluntarily (calling exit or having the main function return) or involuntarily (receiving a signal that terminates the process), all open descriptors are closed, which will also cause a FIN to be sent on.
Although we show the client in Figure 2.3 performing the active close, either end—the client or the server—can perform the active close.
Often the client performs the active close, but with some protocols (notably HTTP/1.0), the server performs the active close.
The operation of TCP with regard to connection establishment and connection termination can be specified with a state transition diagram.
There are 11 different states defined for a connection and the rules of TCP dictate the transitions from one state to another, based on the current state and the segment received in that state.
For example, if an application performs an active open in the CLOSED state, TCP sends a SYN and the new state is SYN_SENT.
If TCP next receives a SYN with an ACK, it sends an ACK and the new state is ESTABLISHED.
The two arrows leading from the ESTABLISHED state deal with the termination of a connection.
If an application calls close before receiving a FIN (an active close), the transition is to the FIN_WAIT_1 state.
But if an application receives a FIN while in the ESTABLISHED state (a passive close), the transition is to the CLOSE_WAIT state.
We denote the normal client transitions with a darker solid line and the normal server transitions with a darker dashed line.
We also note that there are two transitions that we have not talked about: a simultaneous open (when both ends send SYNs at about the same time and the SYNs cross in the network) and a simultaneous close (when both ends send FINs at the same time)
One reason for showing the state transition diagram is to show the 11 TCP states with their names.
These states are displayed by netstat, which is a useful tool when debugging client/server applications.
Figure 2.5 shows the actual packet exchange that takes place for a complete TCP connection: the connection establishment, data transfer, and connection termination.
We also show the TCP states through which each endpoint passes.
It is okay for the MSS to be different in each direction (see Exercise 2.5)
Once a connection is established, the client forms a request and sends it to the server.
We assume this request fits into a single TCP segment (i.e., less than 1,460 bytes given the server's announced MSS)
The server processes the request and sends a reply, and we assume that the reply fits in a single segment (less than 536 in this example)
Notice that the acknowledgment of the client's request is sent with the server's reply.
This is called piggybacking and will normally happen when the time it takes the server to process the request and generate the reply is less than around 200 ms.
If the server takes longer, say one second, we would see the acknowledgment followed later by the reply.
We then show the four segments that terminate the connection.
Notice that the end that performs the active close (the client in this scenario) enters the TIME_WAIT state.
It is important to notice in Figure 2.5 that if the entire purpose of this connection was to send a one-segment request and receive a one-segment reply, there would be eight segments of overhead involved when using TCP.
If UDP was used instead, only two packets would be exchanged: the request and the reply.
But switching from TCP to UDP removes all the reliability that TCP provides to the application, pushing lots of these details from the transport layer (TCP) to the UDP application.
Another important feature provided by TCP is congestion control, which must then be handled by the UDP application.
Nevertheless, it is important to understand that many applications are built using UDP because the application exchanges small amounts of data and UDP avoids the overhead of TCP connection establishment and connection termination.
Undoubtedly, one of the most misunderstood aspects of TCP with regard to network programming is its TIME_WAIT state.
We can see in Figure 2.4 that the end that performs the active close goes through this state.
The duration that this endpoint remains in this state is twice the maximum segment lifetime (MSL), sometimes called 2MSL.
Every implementation of TCP must choose a value for the MSL.
The MSL is the maximum amount of time that any given IP datagram can live in a network.
Although this is a hop limit and not a true time limit, the assumption is made that a packet with the maximum hop limit of 255 cannot exist in a network for more than MSL seconds.
The way in which a packet gets "lost" in a network is usually the result of routing anomalies.
A router crashes or a link between two routers goes down and it takes the routing protocols seconds or minutes to stabilize and find an alternate path.
During that time period, routing loops can occur (router A sends packets to router B, and B sends them back to A) and packets can get caught in these loops.
In the meantime, assuming the lost packet is a TCP segment, the sending TCP times out and retransmits the packet, and the retransmitted packet gets to the final destination by some alternate path.
But sometime later (up to MSL seconds after the lost packet started on its journey), the routing loop is corrected and the packet that was lost in the loop is sent to the final destination.
This original packet is called a lost duplicate or a wandering duplicate.
To allow old duplicate segments to expire in the network2
The first reason can be explained by looking at Figure 2.5 and assuming that the final ACK is lost.
The server will resend its final FIN, so the client must maintain state information, allowing it to resend the final ACK.
If it did not maintain this information, it would respond with an RST (a different type of TCP segment), which would be interpreted by the server as an error.
If TCP is performing all the work necessary to terminate both directions of data flow cleanly for a connection (its full-duplex close), then it must correctly handle the loss of any of these four segments.
This example also shows why the end that performs the active close is the end that remains in the TIME_WAIT state: because that end is the one that might have to retransmit the final ACK.
This latter connection is called an incarnation of the previous connection since the IP addresses and ports are the same.
To do this, TCP will not initiate a new incarnation of a connection that is currently in the TIME_WAIT state.
By enforcing this rule, we are guaranteed that when we successfully establish a TCP connection, all old duplicates from previous incarnations of the connection have expired in the network.
Berkeley-derived implementations will initiate a new incarnation of a connection that is currently in the TIME_WAIT state if the arriving SYN has a sequence number that is "greater than" the ending sequence number from the previous incarnation.
This requires the server to perform the active close, since the TIME_WAIT state must exist on the end that receives the next SYN.
However, SCTP's handshakes are different than TCP's, so we describe them here.
The following scenario, similar to TCP, occurs when an SCTP association is established:
The server must be prepared to accept an incoming association.
This preparation is normally done by calling socket, bind, and listen and is called a passive open.
The client issues an active open by calling connect or by sending a message, which implicitly opens the association.
This causes the client SCTP to send an INIT message (which stands for "initialization") to tell the server the client's list of IP addresses, initial sequence number, initiation tag to identify all packets in this association, number of outbound streams the client is requesting, and number of inbound streams the client can support.
The server acknowledges the client's INIT message with an INIT-ACK message, which contains the server's list of IP addresses, initial sequence number, initiation tag, number of outbound streams the server is requesting, number of inbound streams the server can support, and a state cookie.
The state cookie contains all of the state that the server needs to ensure that the association is valid, and is digitally signed to ensure its validity.
The client echos the server's state cookie with a COOKIE-ECHO message.
This message may also contain user data bundled within the same packet.
The server acknowledges that the cookie was correct and that the association was established with a COOKIE-ACK message.
This message may also contain user data bundled within the same packet.
The minimum number of packets required for this exchange is four; hence, this process is called SCTP's four-way handshake.
We show a picture of the four segments in Figure 2.6
The SCTP four-way handshake is similar in many ways to TCP's three-way handshake, except for the cookie generation, which is an integral part.
The INIT carries with it (along with its many parameters) a verification tag, Ta, and an initial sequence number, J.
The tag Ta must be present in every packet sent by the peer for the life of the association.
The initial sequence number J is used as the starting sequence number for DATA messages termed DATA chunks.
The peer also chooses a verification tag, Tz, which must be present in each of its packets for the life of the association.
Along with the verification tag and initial sequence number, K, the receiver of the INIT also sends a cookie, C.
The cookie contains all the state needed to set up the SCTP association, so that the server's SCTP stack does not need to keep information about the associating client.
At the conclusion of the four-way handshake, each side chooses a primary destination address.
The primary destination address is used as the default destination to which data will be sent in the absence of network failure.
The four-way handshake is used in SCTP to avoid a form of denial-of-service attack we will discuss in Section 4.5
SCTP's four-way handshake using Cookies formalizes a method of protection against this attack.
Many TCP implementations use a similar method; the big difference is that in TCP, the cookie state must be encoded into the initial sequence number, which is only 32 bits.
When one end shuts down an association, the other end must stop sending new data.
The receiver of the shutdown request sends the data that was queued, if any, and then completes the shutdown.
All chunks are tagged with the tag exchanged in the INIT chunks; a chunk from an old connection will arrive with an incorrect tag.
The operation of SCTP with regard to association establishment and termination can be specified with a state transition diagram.
As in Figure 2.4, the transitions from one state to another in the state machine are dictated by the rules of SCTP, based on the current state and the chunk received in that state.
For example, if an application performs an active open in the CLOSED state, SCTP sends an INIT and the new state is COOKIE-WAIT.
If SCTP next receives an INIT ACK, it sends a COOKIE ECHO and the new state is COOKIE-ECHOED.
If SCTP then receives a COOKIE ACK, it moves to the ESTABLISHED state.
This final state is where most data transfer occurs, although DATA chunks can be piggybacked on COOKIE ECHO and COOKIE ACK chunks.
The two arrows leading from the ESTABLISHED state deal with the termination of an association.
If an application calls close before receiving a SHUTDOWN (an active close), the transition is to the SHUTDOWN-PENDING state.
However, if an application receives a SHUTDOWN while in the ESTABLISHED state (a passive close), the transition is to the SHUTDOWN-RECEIVED state.
Figure 2.9 shows the actual packet exchange that takes place for a sample SCTP association: the.
We also show the SCTP states through which each endpoint passes.
In this example, the client piggybacks its first data chunk on the COOKIE ECHO, and the server replies with data on the COOKIE ACK.
In general, the COOKIE ECHO will often have one or more DATA chunks bundled with it when the application is using the one-to-many interface style (we will discuss the one-to-one and one-to-many interface styles in Section 9.2)
The unit of information within an SCTP packet is a "chunk." A "chunk" is self-descriptive and contains a chunk type, chunk flags, and a chunk length.
New features are defined by adding either of these two items, and allowing normal SCTP processing rules to report unknown parameters and unknown chunks.
The dynamic address extension, which allows cooperating SCTP endpoints to dynamically1
The partial reliability extension, which allows cooperating SCTP endpoints, under application direction, to limit the retransmission of data.
When a message becomes too old to send (according to the application's direction), the message will be skipped and thus no longer sent to the peer.
This means that not all data is assured of arrival at the other end of the association.
At any given time, multiple processes can be using any given transport: UDP, SCTP, or TCP.
All three transport layers use 16-bit integer port numbers to differentiate between these processes.
When a client wants to contact a server, the client must identify the server with which it wants to communicate.
TCP, UDP, and SCTP define a group of well-known ports to identify well-known services.
For example, every TCP/IP implementation that supports FTP assigns the well-known port of 21 (decimal) to the FTP server.
Clients, on the other hand, normally use ephemeral ports, that is, short-lived ports.
These port numbers are normally assigned automatically by the transport protocol to the client.
Clients normally do not care about the value of the ephemeral port; the client just needs to be certain that the ephemeral port is unique on the client host.
The Internet Assigned Numbers Authority (IANA) maintains a list of port number assignments.
These port numbers are controlled and assigned by the IANA.
When possible, the same port is assigned to a given service for TCP, UDP, and SCTP.
For example, port 80 is assigned for a Web server, for both TCP and UDP, even though all implementations currently use only TCP.
At the time that port 80 was assigned, SCTP did not yet exist.
New port assignments are made for all three protocols, and RFC 2960 states that all existing TCP port numbers should be valid for the same service using SCTP.
These are not controlled by the IANA, but the IANA registers and lists the uses of these ports as a convenience to the community.
When possible, the same port is assigned to a given service for both TCP and UDP.
Figure 2.10 shows this division, along with the common allocation of the port numbers.
These ports can only be assigned to a socket by an appropriately privileged process.
All the IANA well-known ports are reserved ports; hence, the server allocating this port (such as the FTP server) must have superuser privileges when it starts.
Therefore, many newer systems allocate ephemeral ports differently to provide more ephemeral ports, either using the IANA-defined ephemeral range or a larger range (e.g., Solaris as we show in Figure 2.10)
There are a few clients (not servers) that require a reserved port as part of the client/server authentication: the rlogin and rsh clients are common examples.
These clients call the library function rresvport to create a TCP socket and assign an unused port in the range 513–1023 to the socket.
Notice that the BSD reserved ports and the rresvport function both overlap with the upper half of the IANA well-known ports.
The rresvport function chose to start at the top of the 512–1023 range and work down.
The socket pair for a TCP connection is the four-tuple that defines the two endpoints of the connection: the local IP address, local port, foreign IP address, and foreign port.
A socket pair uniquely identifies every TCP connection on a network.
For SCTP, an association is identified by a set of local IP addresses, a local port, a set of foreign IP addresses, and a foreign port.
In its simplest form, where neither endpoint is multihomed, this results in the same four-tuple socket pair used with TCP.
However, when either of the endpoints of an association are multihomed, then multiple four-tuple sets (with different IP addresses but the same port numbers) may identify the same association.
The two values that identify each endpoint, an IP address and a port number, are often called a socket.
We can extend the concept of a socket pair to UDP, even though UDP is connectionless.
For example, bind lets the application specify the local IP address and local port for TCP, UDP, and SCTP sockets.
With a concurrent server, where the main server loop spawns a child to handle each new connection, what happens if the child continues to use the well-known port number while servicing a long request? Let's examine a typical sequence.
It is now waiting for a client request, which we show in Figure 2.11
We use a colon to separate the IP address from the port number because that is what HTTP uses and is commonly seen elsewhere.
When we specify the local IP address as an asterisk, it is called the wildcard character.
If the host on which the server is running is multihomed (as in this example), the server can specify that it wants only to accept incoming connections that arrive destined to one specific local interface.
We assume the ephemeral port chosen by the client TCP is 1500 for this example.
When the server receives and accepts the client's connection, it forks a copy of itself, letting the child handle the client, as we show in Figure 2.13
At this point, we must distinguish between the listening socket and the connected socket on the server host.
Notice that the connected socket uses the same local port (21) as the listening socket.
The next step assumes that another client process on the client host requests a connection with the same server.
On the server, the two connections are distinct: the socket pair for the first connection differs from the socket pair for the second connection because the client's TCP chooses an unused port for the second connection (1501)
Notice from this example that TCP cannot demultiplex incoming segments by looking at just the.
All other TCP segments destined for port 21 are delivered to the original server with the listening socket.
We first describe these limits and then tie them all together with regard to how they affect the data an application can transmit.
This is intended for host-to-host interconnects, such as HIPPI, which often have no inherent MTU.
Many networks have an MTU which can be dictated by the hardware.
Other datalinks, such as point-to-point links using the Point-toPoint Protocol (PPP), have a configurable MTU.
The smallest MTU in the path between two hosts is called the path MTU.
Today, the Ethernet MTU of 1,500 bytes is often the path MTU.
The path MTU need not be the same in both directions between any two hosts because routing in the Internet is often asymmetric [Paxson 1996]
That is, the route from A to B can differ from the route from B to A.
The fragments are not normally reassembled until they reach the final destination.
A box labeled as an IPv6 router may indeed perform fragmentation, but only on datagrams that the router itself generates, never on datagrams that it is forwarding.
When this box generates IPv6 datagrams, it is really acting as a host.
For example, most routers support the Telnet protocol and this is used for router configuration by administrators.
The IP datagrams generated by the router's Telnet server are generated by the router, not forwarded by the router.
Since fragmentation is the exception, rather than the rule, IPv6 contains an option header with the fragmentation information.
Certain firewalls, which usually act as routers, may reassemble fragmented packets to allow inspection of the entire packet contents.
This allows the prevention of certain attacks at the cost of additional complexity in the firewall device.
It also requires the firewall device to be part of the only path to the network, reducing the opportunities for redundancy.
For example, if TCP uses this technique with IPv4, then it sends all its datagrams with the DF bit set.
If some intermediate router returns an ICMP "destination unreachable, fragmentation needed but DF bit set" error, TCP decreases the amount of data it sends per datagram and retransmits.
Path MTU discovery is problematic in the Internet today; many firewalls drop all ICMP messages, including the fragmentation required message, meaning that TCP never gets the signal that it needs to decrease the amount of data it is sending.
As of this writing, an effort is beginning in the IETF to define another method for path MTU discovery that does not rely on ICMP errors.
Therefore, many IPv4 applications that use UDP (e.g., DNS, RIP, TFTP, BOOTP, SNMP) prevent applications from generating IP datagrams that exceed this size.
We saw the MSS option on the SYN segments in Figure 2.5
The goal of the MSS is to tell the peer the actual value of the reassembly buffer size and to try to avoid fragmentation.
The MSS is often set to the interface MTU minus the fixed sizes of the IP and TCP headers.
Therefore, the MSS value of 65,535 is considered a special case that designates "infinity." This value is used only.
If TCP is using the jumbo payload option and receives an MSS announcement of 65,535 from the peer, the limit on the datagram sizes that it sends is just the interface MTU.
If this turns out to be too large (i.e., there is a link in the path with a smaller MTU), then path MTU discovery will determine the smaller value.
This smallest MTU size is used to split large user messages into smaller pieces that can be sent in one IP datagram.
The SCTP_MAXSEG socket option can influence this value, allowing the user to request a smaller fragmentation point.
Given all these terms and definitions, Figure 2.15 shows what happens when an application writes data to a TCP socket.
Steps and buffers involved when an application writes to a TCP socket.
When an application calls write, the kernel copies all the data from the application buffer into the socket send buffer.
If there is insufficient room in the socket buffer for all the application's data (either the application buffer is larger than the socket send buffer, or there is already data in the socket send buffer), the process is put to sleep.
The kernel will not return from the write until the final byte in the application buffer has been copied into the socket send buffer.
Therefore, the successful return from a write to a TCP socket only tells us that we can reuse our application buffer.
It does not tell us that either the peer TCP has received the data or that the peer application has received the data.
Each datalink has an output queue, and if this queue is full, the packet is discarded and an error is returned up the protocol stack: from the datalink to IP and then from IP to TCP.
Figure 2.16 shows what happens when an application writes data to a UDP socket.
Steps and buffers involved when an application writes to a UDP socket.
This time, we show the socket send buffer as a dashed box because it doesn't really exist.
If an application writes a datagram larger than the socket send buffer size, EMSGSIZE is returned.
Since UDP is unreliable, it does not need to keep a copy of the application's data and does not need an actual send buffer.
The application data is normally copied into a kernel buffer of some form as it passes down the protocol stack, but this copy is discarded by the datalink layer after the data is transmitted.
If a UDP application sends large datagrams (say 2,000-byte datagrams), there is a much higher probability of fragmentation than with TCP, because TCP breaks the application data into MSS-sized chunks, something that has no counterpart in UDP.
The successful return from a write to a UDP socket tells us that either the datagram or all fragments of the datagram have been added to the datalink output queue.
If there is no room on the queue for the datagram or one of its fragments, ENOBUFS is often returned to the application.
Unfortunately, some implementations do not return this error, giving the application no indication that the datagram was discarded without even being transmitted.
Figure 2.17 shows what happens when an application writes data to an SCTP socket.
Steps and buffers involved when an application writes to an SCTP socket.
SCTP, since it is a reliable protocol like TCP, has a send buffer.
When the application calls write, the kernel copies all the data from the application buffer into the socket send buffer.
If there is insufficient room in the socket buffer for all of the application's data (either the application buffer is larger than the socket send buffer, or there is already data in the socket send buffer), the process is put to sleep.
This sleeping assumes the normal default of a blocking socket.
The kernel will not return from the write until the final byte in the application buffer has been copied into the socket send buffer.
Therefore, the successful return from a write to an SCTP socket only tells the sender that it can reuse the application buffer.
It does not tell us that either the peer SCTP has received the data, or that the peer application has received the data.
The sending SCTP must await a SACK in which the cumulative acknowledgment point passes the sent data before that data can be removed from the socket buffer.
Figure 2.18 lists several standard services that are provided by most implementations of TCP/IP.
Notice that all are provided using both TCP and UDP and the port number is the same for both protocols.
Often these services are provided by the inetd daemon on Unix hosts (Section 13.5)
These standard services provide an easy testing facility using the standard Telnet client.
For example, the following tests both the daytime and echo servers:
In these two examples, we type the name of the host and the name of the service (daytime and echo)
Notice that when we connect to the daytime server, the server performs the active close, while with the echo server, the client performs the active close.
These "simple services" are often disabled by default on modern systems due to denial-of-service and other resource utilization attacks against them.
Figure 2.19 summarizes the protocol usage of various common Internet applications.
The first two applications, ping and traceroute, are diagnostic applications that use ICMP.
The three popular routing protocols demonstrate the variety of transport protocols used by routing protocols.
The next five are UDP-based applications, followed by seven TCP applications and four that use both UDP and TCP.
The final five are IP telephony applications that use SCTP exclusively or optionally UDP, TCP, or SCTP.
While most applications on the Internet use TCP (the Web, Telnet, FTP, and email), there is a need for all three transport layers.
In Section 22.4, we will discuss the reasons to choose UDP instead of TCP.
In Section 23.12, we will discuss the reasons to choose SCTP instead of TCP.
When a TCP connection is established, it goes from the CLOSED state to the ESTABLISHED state, and when it is terminated, it goes back to the CLOSED state.
There are 11 states in which a TCP connection can be, and a state transition diagram gives the rules on how to go between the states.
Understanding this diagram is essential to diagnosing problems using the netstat command and understanding what happens when an application calls functions such as connect, accept, and close.
TCP's TIME_WAIT state is a continual source of confusion with network programmers.
This state exists to implement TCP's full-duplex connection termination (i.e., to handle the case of the final ACK being lost), and to allow old duplicate segments to expire in the network.
When an SCTP association is established, it goes from the CLOSED state to the ESTABLISHED state, and when it is terminated, it goes back to the CLOSED state.
There are eight states in which an SCTP association can be, and a state transition diagram gives the rules on how to go between the states.
Feel free to skip ahead to the solution if you cannot visit http://www.iana.org.
Watching the packets, we never see more than 1,460 bytes of data in either direction.
If a selective acknowledgment shows that data is acknowledged beyond the cumulative acknowledgment point, why can't the data be freed?
We begin with socket address structures, which will be found in almost every example in the text.
These structures can be passed in two directions: from the process to the kernel, and from the kernel to the process.
The latter case is an example of a value-result argument, and we will encounter other examples of these arguments throughout the text.
The address conversion functions convert between a text representation of an address and the binary value that goes into a socket address structure.
Most socket functions require a pointer to a socket address structure as an argument.
Each supported protocol suite defines its own socket address structure.
The names of these structures begin with sockaddr_ and end with a unique suffix for each protocol suite.
There are several points we need to make about socket address structures in general using this example:
Before this release, the first member was sin_family , which was historically an unsigned short.
Not all vendors support a length field for socket address structures and the POSIX specification does not require this member.
Having a length field simplifies the handling of variable-length socket address structures.
Even if the length field is present, we need never set it and need never examine it, unless we are dealing with routing sockets (Chapter 18 )
It is used within the kernel by the routines that deal with socket address structures from various protocol families (e.g., the routing table code)
This function copies the socket address structure from the process and explicitly sets its sin_len member to the size of the structure that was passed as an argument to these four functions.
The five socket functions that pass a socket address structure from the kernel to the process, accept , recvfrom , recvmsg , getpeername , and getsockname , all set the sin_len member before returning to the process.
Unfortunately, there is normally no simple compile-time test to determine whether an implementation defines a length field for its socket address structures.
It is acceptable for a POSIX-compliant implementation to define additional structure members, and this is normal for an Internet socket address structure.
Figure 3.2 lists these three POSIX-defined datatypes, along with some other POSIX datatypes that we will encounter.
The POSIX specification defines these with a note that they are obsolete.
Both the IPv4 address and the TCP or UDP port number are always stored in the structure in network byte order.
We must be cognizant of this when using these members.
We will say more about the difference between host byte order and network byte order in Section 3.4
We must be certain that we are referencing the IPv4 address correctly, especially when it is used as an argument to a function, because compilers often pass structures differently from integers.
This was used with class A, B, and C addresses to fetch the appropriate bytes of the address.
But with the advent of subnetting and then the disappearance of the various address classes with classless addressing (Section A.4 ), the need for the union disappeared.
Socket address structures are used only on a given host: The structure itself is not communicated between different hosts, although certain fields (e.g., the IP address and port) are used for communication.
A socket address structures is always passed by reference when passed as an argument to any socket functions.
But any socket function that takes one of these pointers as an argument must deal with socket address structures from any of the supported protocol families.
A problem arises in how to declare the type of pointer that is passed.
With ANSI C, the solution is simple: void * is the generic pointer type.
The socket functions are then defined as taking a pointer to the generic socket address structure, as shown here in the ANSI C function prototype for the bind function:
This requires that any calls to these functions must cast the pointer to the protocol-specific socket address structure to be a pointer to a generic socket address structure.
From an application programmer's point of view, the only use of these generic socket address structures is to cast pointers to protocol-specific structures.
Recall in Section 1.2 that in our unp.h header, we define SA to be the string "struct sockaddr " just to shorten the code that we must write to cast these pointers.
But from an application programmer's perspective, it would be simpler if the pointer type was void * , omitting the need for the explicit cast.
The SIN6_LEN constant must be defined if the system supports the length member for socket address structures.
The use of the flow label field is still a research topic.
A new generic socket address structure was defined as part of the IPv6 sockets API, to overcome some of the shortcomings of the existing struct sockaddr.
Unlike the struct sockaddr , the new struct sockaddr_storage is large enough to hold any socket address type supported by the system.
The sockaddr_storage type provides a generic socket address structure that is different from struct sockaddr in two ways:
If any socket address structures that the system supports have alignment requirements, the sockaddr_storage provides the strictest alignment requirement.
The sockaddr_storage is large enough to contain any socket address structure that the system supports.
In this figure, we assume that the socket address structures all contain a one-byte length field, that the family field also occupies one byte, and that any field that must be at least some number of bits is exactly that number of bits.
Two of the socket address structures are fixed-length, while the Unix domain structure and the.
To handle variable-length structures, whenever we pass a pointer to a socket address structure as an argument to one of the socket functions, we pass its length as another argument.
We show the size in bytes (for the 4.4BSD implementation) of the fixed-length structures beneath each structure.
When passing pointers to these structures, we must be careful how we handle the length field, both the length field in the socket address structure itself (if supported by the implementation) and the length to and from the kernel.
We mentioned that when a socket address structure is passed to any socket function, it is always passed by reference.
The length of the structure is also passed as an argument.
But the way in which the length is passed depends on which direction the structure is being passed: from the process to the kernel, or vice versa.
Three functions, bind, connect, and sendto, pass a socket address structure from the process to the kernel.
One argument to these three functions is the pointer to the socket address structure and another argument is the integer size of the structure, as in.
Since the kernel is passed both the pointer and the size of what the pointer points to, it knows exactly how much data to copy from the process into the kernel.
Four functions, accept, recvfrom, getsockname, and getpeername, pass a socket address structure from the kernel to the process, the reverse direction from the previous scenario.
Two of the arguments to these four functions are the pointer to the socket address structure along with a pointer to an integer containing the size of the structure, as in.
The reason that the size changes from an integer to be a pointer to an integer is because the size is both a value when the function is called (it tells the kernel the size of the structure so that the kernel does not write past the end of the structure when filling it in) and a result when the function returns (it tells the process how much information the kernel actually stored in the structure)
We will see an example of value-result arguments in Figure 4.11
We have been talking about socket address structures being passed between the process and the kernel.
For an implementation such as 4.4BSD, where all the socket functions are system calls within the kernel, this is correct.
But in some implementations, notably System V, socket functions are just library functions that execute as part of a normal user process.
How these functions interface with the protocol stack in the kernel is an implementation detail that normally does not affect us.
Nevertheless, for simplicity, we will continue to talk about these structures as being passed between the process and the kernel by functions such as bind and connect.
We will see in Section C.1 that System V implementations do indeed pass socket address structures between processes and the kernel, but as part of STREAMS messages.
Two other functions pass socket address structures: recvmsg and sendmsg (Section 14.5)
But, we will see that the length field is not a function argument but a structure member.
With network programming, the most common example of a value-result argument is the length of a returned socket address structure.
But, we will encounter other value-result arguments in this text:
The middle three arguments for the select function (Section 6.3)
The first of the two length arguments for the sysctl function (Section 18.4)
There are two ways to store the two bytes in memory: with the low-order byte at the starting address, known as little-endian byte order, or with the high-order byte at the starting address, known as big-endian byte order.
Little-endian byte order and big-endian byte order for a 16bit integer.
In this figure, we show increasing memory addresses going from right to left in the top, and from left to right in the bottom.
We also show the most significant bit (MSB) as the leftmost bit of the 16-bit value and the least significant bit (LSB) as the rightmost bit.
The terms "little-endian" and "big-endian" indicate which end of the multibyte value, the little end or the big end, is stored at the starting address of the value.
Unfortunately, there is no standard between these two byte orderings and we encounter systems that use both formats.
We refer to the byte ordering used by a given system as the host byte order.
The program shown in Figure 3.10 prints the host byte order.
The string CPU_VENDOR_OS is determined by the GNU autoconf program when the software in this book is configured, and it identifies the CPU type, vendor, and OS release.
We show some examples here in the output from this program when run on the various systems in Figure 1.16
There are currently a variety of systems that can change between little-endian and bigendian byte ordering, sometimes at system reset, sometimes at run-time.
We must deal with these byte ordering differences as network programmers because networking protocols must specify a network byte order.
The sending protocol stack and the receiving protocol stack must agree on the order in which the bytes of these multibyte fields will be transmitted.
The Internet protocols use big-endian byte ordering for these multibyte integers.
In theory, an implementation could store the fields in a socket address structure in host byte order and then convert to and from the network byte order when moving the fields to and from the protocol headers, saving us from having to worry about this detail.
But, both history and the POSIX specification say that certain fields in the socket address structures must be maintained in network byte order.
Our concern is therefore converting between host byte order and network byte order.
We use the following four functions to convert between these two byte orders.
In the names of these functions, h stands for host, n stands for network, s stands for short, and l stands for long.
The terms "short" and "long" are historical artifacts from the Digital VAX implementation of 4.2BSD.
When using these functions, we do not care about the actual values (big-endian or little-endian) for the host byte order and the network byte order.
What we must do is call the appropriate function to convert a given value between the host and network byte order.
On those systems that have the same byte ordering as the Internet protocols (big-endian), these four functions are usually defined as null macros.
Most Internet standards use the term octet instead of byte to mean an 8-bit quantity.
This represents four bytes in the order in which they appear on the wire; the leftmost bit is the most significant.
However, the numbering starts with zero assigned to the most significant bit.
This is a notation that you should become familiar with to make it easier to read protocol definitions in RFCs.
The code worked fine on these workstations, but would not work when ported to little-endian machines (such as VAXes)
There are two groups of functions that operate on multibyte fields, without interpreting the data, and without assuming that the data is a null-terminated C string.
We need these types of functions when dealing with socket address structures because we need to manipulate fields such as IP addresses, which can contain bytes of 0, but are not C character strings.
The functions beginning with str (for string), defined by including the <string.h> header, deal with nullterminated C character strings.
The first group of functions, whose names begin with b (for byte), are from 4.2BSD and are still provided by almost any system that supports the socket functions.
The second group of functions, whose names begin with mem (for memory), are from the ANSI C standard and are provided with any system that supports an ANSI C library.
We first show the Berkeley-derived functions, although the only one we use in this text is bzero.
We use it because it has only two arguments and is easier to remember than the threeargument memset function, as explained on p.
You may encounter the other two functions, bcopy and bcmp, in existing applications.
This is our first encounter with the ANSI C const qualifier.
Worded another way, the memory pointed to by the const pointer is read but not modified by the function.
The return value is zero if the two byte strings are identical; otherwise, it is nonzero.
The ANSI C memmove function must be used when the fields overlap.
One way to remember the order of the two pointers for memcpy is to remember that they are written in the same left-to-right order as an assignment statement in C:
One way to remember the order of the final two arguments to memset is to realize that all of the ANSI C memXXX functions require a length argument, and it is always the final argument.
The comparison is done assuming the two unequal bytes are unsigned chars.
We will describe two groups of address conversion functions in this section and the next.
They convert Internet addresses between ASCII strings (what humans prefer to use) and network byte ordered binary values (values that are stored in socket address structures)
You will probably encounter these functions in lots of existing code.
We describe these two functions in the next section and use them throughout the text.
An undocumented feature of inet_aton is that if addrptr is a null pointer, the function still performs its validation of the input string but does not store any result.
This can lead to problems, depending on the C compiler, when comparing the return value of the function (an unsigned value) to a negative constant.
The string pointed to by the return value of the function resides in static memory.
This means the function is not reentrant, which we will discuss in Section 11.18
Finally, notice that this function takes a structure as its argument, not a pointer to.
It is more common to pass a pointer to the structure.
The letters "p" and "n" stand for presentation and numeric.
The presentation format for an address is often an ASCII string and the numeric format is the binary value that goes into a socket address structure.
If family is not supported, both functions return an error with errno set to EAFNOSUPPORT.
The first function tries to convert the string pointed to by strptr, storing the binary result through the pointer addrptr.
If the input string is not a valid presentation format for the specified family, 0 is returned.
The len argument is the size of the destination, to prevent the function from overflowing the caller's buffer.
To help specify this size, the following two definitions are defined by including the <netinet/in.h> header:
If len is too small to hold the resulting presentation format, including the terminating null, a null pointer is returned and errno is set to ENOSPC.
The strptr argument to inet_ntop cannot be a null pointer.
The caller must allocate memory for the destination and specify its size.
On success, this pointer is the return value of the function.
Figure 3.11 summarizes the five functions that we have described in this section and the previous section.
Even if your system does not yet include support for IPv6, you can start using these newer functions by replacing calls of the form.
A basic problem with inet_ntop is that it requires the caller to pass a pointer to a binary address.
This address is normally contained in a socket address structure, requiring the caller to know the format of the structure and the address family.
That is, to use it, we must write code of the form.
To solve this, we will write our own function named sock_ntop that takes a pointer to a socket address structure, looks inside the structure, and calls the appropriate function to return the presentation format of the address.
This is the notation we use for functions of our own (nonstandard system functions) that we use throughout the book: the box around the function prototype and return value is dashed.
The header is included at the beginning is usually our unp.h header.
The function uses its own static buffer to hold the result and a pointer to this buffer is the return value.
Notice that using static storage for the result prevents the function from being re-entrant or thread-safe.
We made this design decision for this function to allow us to easily call it from the simple examples in the book.
Returns: 0 if addresses are of the same family and ports are equal, else nonzero.
Returns: 0 if addresses are of the same family and ports are equal, else nonzero.
As with all the functions in the text, we provide a wrapper function whose name begins with "S" for all of these functions that return values other than void and normally call the wrapper function from our programs.
We do not show the source code for all these functions, but it is freely available (see the Preface)
Stream sockets (e.g., TCP sockets) exhibit a behavior with the read and write functions that differs from normal file I/O.
A read or write on a stream socket might input or output fewer bytes than requested, but this is not an error condition.
The reason is that buffer limits might be reached for the socket in the kernel.
All that is required to input or output the remaining bytes is for the caller to invoke the read or write function again.
Some versions of Unix also exhibit this behavior when writing more than 4,096 bytes to a pipe.
This scenario is always a possibility on a stream socket with read, but is normally seen with write only if the socket is nonblocking.
Nevertheless, we always call our writen function instead of write, in case the implementation returns a short count.
We provide the following three functions that we use whenever we read from or write to a stream socket:
All return: number of bytes read or written, –1 on error.
Figure 3.17 readline function: Read a text line from a descriptor, one byte at a time.
Our three functions look for the error EINTR (the system call was interrupted by a caught signal, which we will discuss in more detail in Section 5.9) and continue reading or writing if the error occurs.
We handle the error here, instead of forcing the caller to call readn or writen again, since the purpose of these three functions is to prevent the caller from having to handle a short count.
Note that our readline function calls the system's read function once for every byte of data.
This is very inefficient, and why we've commented the code to state it is "PAINFULLY SLOW." When faced with the desire to read lines from a socket, it is quite tempting to turn to the standard I/O library (referred to as "stdio")
We will discuss this approach at length in Section 14.8, but it can be a dangerous path.
The same stdio buffering that solves this performance problem creates numerous logistical problems that can lead to well-hidden bugs in your application.
The reason is that the state of the stdio buffers is not exposed.
To explain this further, consider a line-based protocol between a client and a server, where several clients and servers using that protocol may be implemented over time (really quite common; for example, there are many Web browsers and Web servers independently written to the HTTP specification)
Good "defensive programming" techniques require these programs to not only expect their counterparts to follow the network protocol, but to check for unexpected network traffic as well.
Such protocol violations should be reported as errors so that bugs are noticed and fixed (and malicious attempts are detected as well), and also so that network applications can recover from problem traffic and continue working if possible.
Using stdio to buffer data for performance flies in the face of these goals since the application has no way to tell if unexpected data is being held in the stdio buffers at any given time.
There are many line-based network protocols such as SMTP, HTTP, the FTP control connection protocol, and finger.
So, the desire to operate on lines comes up again and again.
But our advice is to think in terms of buffers and not lines.
Write your code to read buffers of data, and if a line is expected, check the buffer to see if it contains that line.
Figure 3.18 shows a faster version of the readline function, which uses its own buffering rather than stdio buffering.
Most importantly, the state of readline's internal buffer is exposed, so callers have visibility into exactly what has been received.
Even with this feature, readline can be problematic, as we'll see in Section 6.3
For that matter, mixing readn and readline calls will not work as expected unless readn is modified to check the internal buffer as well.
The internal function my_read reads up to MAXLINE characters at a time and then returns them, one at a time.
The only change to the readline function itself is to call my_read instead of read.
A new function, readlinebuf, exposes the internal buffer state so that callers can check and see if more data was received beyond a single line.
Unfortunately, by using static variables in readline.c to maintain the state information across successive calls, the functions are not re-entrant or thread-safe.
We will develop a thread-safe version using thread-specific data in Figure 26.11
Socket address structures are an integral part of every network program.
We allocate them, fill them in, and pass pointers to them to various socket functions.
Sometimes we pass a pointer to one of these structures to a socket function and it fills in the contents.
We always pass these structures by reference (that is, we pass a pointer to the structure, not the structure itself), and we always pass the size of the structure as another argument.
When a socket function fills in a structure, the length is also passed by reference, so that its value can be updated by the function.
Socket address structures are self-defining because they always begin with a field (the "family") that identifies the address family contained in the structure.
Newer implementations that support variable-length socket address structures also contain a length field at the beginning, which contains the length of the entire structure.
Although we will use these two functions in the coming chapters, they are protocol-dependent.
A better technique is to manipulate socket address structures as opaque objects, knowing just the pointer to the structure and its size.
The return value from a read can be less than what we asked for, but this does not indicate an error.
To help read and write a byte stream, we developed three functions, readn, writen, and readline, which we will use throughout the text.
However, network programs should be written to act on buffers rather than lines.
This chapter describes the elementary socket functions required to write a complete TCP client and server.
We will first describe all the elementary socket functions that we will be using and then develop the client and server in the next chapter.
We will also describe concurrent servers, a common Unix technique for providing concurrency when numerous clients are connected to the same server at the same time.
Each client connection causes the server to fork a new process just for that client.
Figure 4.1 shows a timeline of the typical scenario that takes place between a TCP client and server.
First, the server is started, then sometime later, a client is started that connects to the server.
We assume that the client sends a request to the server, the server processes the request, and the server sends a reply back to the client.
This continues until the client closes its end of the connection, which sends an end-of-file notification to the server.
The server then closes its end of the connection and either terminates or waits for a new client connection.
This argument is often referred to as domain instead of family.
The socket type is one of the constants shown in Figure 4.3
Not all combinations of socket family and type are valid.
Figure 4.5 shows the valid combinations, along with the actual protocols that are valid for each pair.
The boxes marked "Yes" are valid but do not have handy acronyms.
You may also encounter the corresponding PF_xxx constant as the first argument to socket.
We will say more about this at the end of this section.
There are other values for the family and type arguments.
But, TCP is a byte stream protocol, and supports only SOCK_STREAM sockets.
Similar to the way that a routing socket (AF_ROUTE) is an interface to the kernel's routing table, the key socket is an interface into the kernel's key table.
On success, the socket function returns a small non-negative integer value, similar to a file descriptor.
We have not yet specified either the local protocol address or the foreign protocol address.
While there is no guarantee that this equality between the two will always be true, should anyone change this for existing protocols, lots of existing code would break.
The 4.1cBSD version of socket took four arguments, one of which was a pointer to a sockproto structure.
Specifying this structure was the only way to specify the protocol family.
But, it then defines only one family value in the addrinfo structure (Section 11.6), intended for use in either a call to socket or in a socket address structure!
The connect function is used by a TCP client to establish a connection with a TCP server.
The second and third arguments are a pointer to a socket address structure and its size, as described in Section 3.3
The socket address structure must contain the IP address and port number of the server.
We saw an example of this function in Figure 1.5
The client does not have to call bind (which we will describe in the next section) before calling connect: the kernel will choose both an ephemeral port and the source IP address if necessary.
In the case of a TCP socket, the connect function initiates TCP's three-way handshake (Section 2.6)
The function returns only when the connection is established or an error occurs.
If the client TCP receives no response to its SYN segment, ETIMEDOUT is returned.
If no response is received after a total of 75 seconds, the error is returned.
Some systems provide administrative control over this timeout; see Appendix E of TCPv1
If the server's response to the client's SYN is a reset (RST), this indicates that no process is waiting for connections on the server host at the port specified (i.e., the server process is probably not running)
This is a hard error and the error ECONNREFUSED is returned to the client as soon as the RST is received.
An RST is a type of TCP segment that is sent by TCP when something is wrong.
Three conditions that generate an RST are: when a SYN arrives for a port that has no listening server (what we just described), when TCP wants to abort an existing connection, and when TCP receives a segment for a connection that does not exist.
If the client's SYN elicits an ICMP "destination unreachable" from some intermediate router, this is considered a soft error.
The client kernel saves the message but keeps sending SYNs with the same time between each SYN as in the first scenario.
It is also possible that the remote system is not reachable by any route in the local system's forwarding table, or that the connect call returns without waiting at all.
Many earlier systems, such as 4.2BSD, incorrectly aborted the connection establishment attempt when the ICMP "destination unreachable" was received.
For example, it could be that the condition is caused by a routing problem that will be corrected.
Notice that ENETUNREACH is not listed in Figure A.15, even when the error indicates that the destination network is unreachable.
Network unreachables are considered obsolete, and applications should just treat ENETUNREACH and EHOSTUNREACH as the same error.
We can see these different error conditions with our simple client from Figure 1.5
We first specify the local host (127.0.0.1), which is running the daytime server, and see the output.
To see a different format for the returned reply, we specify a different machine's IP address (in this example, the IP address of the HP-UX machine)
That is, there is no host on the subnet with a host ID of 100, so when the client host sends out ARP requests (asking for that host to respond with its hardware address), it will never receive an ARP reply.
We only get the error after the connect times out (around four minutes with Solaris 9)
Notice that our err_sys function prints the human-readable string associated with the ETIMEDOUT error.
Our next example is to specify a host (a local router) that is not running a daytime server.
Our final example specifies an IP address that is not reachable on the Internet.
If we watch the packets with tcpdump, we see that a router six hops away returns an ICMP host unreachable error.
As with the ETIMEDOUT error, in this example, connect returns the EHOSTUNREACH error only after waiting its specified amount of time.
If connect fails, the socket is no longer usable and must be closed.
In Figure 11.10, we will see that when we call connect in a loop, trying each IP address for a given host until one works, each time connect fails, we must close the socket descriptor and call socket again.
The bind function assigns a local protocol address to a socket.
Historically, the man page description of bind has said "bind assigns a name to an unnamed socket." The use of the term "name" is confusing and gives the connotation of domain names (Chapter 11) such as foo.bar.com.
The second argument is a pointer to a protocol-specific address, and the third argument is the size of this address structure.
With TCP, calling bind lets us specify a port number, an IP address, both, or neither.
If a TCP client or server does not do this, the kernel chooses an ephemeral port for the socket when either connect or listen is called.
It is normal for a TCP client to let the kernel choose an ephemeral port, unless the application requires a reserved port (Figure 2.10), but it is rare for a TCP server to let the kernel choose an ephemeral port, since servers are known by their well-known port.
Exceptions to this rule are Remote Procedure Call (RPC) servers.
They normally let the kernel choose an ephemeral port for their listening socket since this port is then registered with the RPC port mapper.
Clients have to contact the port mapper to obtain the ephemeral port before they can connect to the server.
A process can bind a specific IP address to its socket.
The IP address must belong to an interface on the host.
For a TCP client, this assigns the source IP address that will be used for IP datagrams sent on the socket.
For a TCP server, this restricts the socket to receive incoming client connections destined only to that IP address.
Normally, a TCP client does not bind an IP address to its socket.
As we said, calling bind lets us specify the IP address, the port, both, or neither.
Result when specifying IP address and/or port number to bind.
If we specify a port number of 0, the kernel chooses an ephemeral port when bind is called.
But if we specify a wildcard IP address, the kernel does not choose the local IP address until either the socket is connected (TCP) or a datagram is sent on the socket (UDP)
We saw the use of this in Figure 1.9 with the assignment.
In C we cannot represent a constant structure on the right-hand side of an assignment.
If we tell the kernel to choose an ephemeral port number for our socket, notice that bind does not return the chosen value.
Indeed, it cannot return this value since the second argument to bind has the const qualifier.
To obtain the value of the ephemeral port assigned by the kernel, we must call getsockname to return the protocol address.
Next, each organization's domain name maps into a different IP address, but typically on the same subnet.
All these IP addresses are then aliased onto a single network interface (using the alias option of the ifconfig command on 4.4BSD, for example) so that the IP layer will accept incoming datagrams destined for any of the aliased addresses.
Finally, one copy of the HTTP server is started for each organization and each copy binds only the IP address for that organization.
An alternative technique is to run a single server that binds the wildcard address.
The server then handles the client request based on the IP address to which the connection was issued.
One advantage in binding a non-wildcard IP address is that the demultiplexing of a given destination IP address to a given server process is then done by the kernel.
We must be careful to distinguish between the interface on which a packet arrives versus the destination IP address of that packet.
In Section 8.8, we will talk about the weak end system model and the strong end system model.
Most implementations employ the former, meaning it is okay for a packet to arrive with a destination IP address that identifies an interface other than the interface on which the packet arrives.
Binding a non-wildcard IP address restricts the datagrams that will be delivered to the socket based only on the destination IP address.
It says nothing about the arriving interface, unless the host employs the strong end system model.
A common error from bind is EADDRINUSE ("Address already in use")
The listen function is called only by a TCP server and it performs two actions:
When a socket is created by the socket function, it is assumed to be an active socket, that is, a client socket that will issue a connect.
The listen function converts an unconnected socket into a passive socket, indicating that the kernel should accept incoming connection requests directed to this socket.
In terms of the TCP state transition diagram (Figure 2.4), the call to listen moves the socket from the CLOSED state to the LISTEN state.
The second argument to this function specifies the maximum number of connections the kernel should queue for this socket.
This function is normally called after both the socket and bind functions and must be called before calling the accept function.
To understand the backlog argument, we must realize that for a given listening socket, the kernel maintains two queues:
An incomplete connection queue, which contains an entry for each SYN that has arrived from a client for which the server is awaiting completion of the TCP three-way handshake.
A completed connection queue, which contains an entry for each client with whom the TCP three-way handshake has completed.
Figure 4.7 depicts these two queues for a given listening socket.
The two queues maintained by TCP for a listening socket.
When an entry is created on the incomplete queue, the parameters from the listen socket are copied over to the newly created connection.
The connection creation mechanism is completely automatic; the server process is not involved.
Figure 4.8 depicts the packets exchanged during the connection establishment with these two queues.
When a SYN arrives from a client, TCP creates a new entry on the incomplete queue and then responds with the second segment of the three-way handshake: the server's SYN with an ACK of the client's SYN (Section 2.6)
This entry will remain on the incomplete queue until the third segment of the three-way handshake arrives (the client's ACK of the server's SYN), or until the entry times out.
Berkeley-derived implementations have a timeout of 75 seconds for these incomplete entries.
If the three-way handshake completes normally, the entry moves from the incomplete queue to the end of the completed queue.
When the process calls accept, which we will describe in the next section, the first entry on the completed queue is returned to the process, or if the queue is empty, the process is put to sleep until an entry is placed onto the completed queue.
There are several points to consider regarding the handling of these two queues.
The backlog argument to the listen function has historically specified the maximum value for the sum of both queues.
There has never been a formal definition of what the backlog means.
The 4.2BSD man page says that it "defines the maximum length the queue of pending connections may grow to." Many man pages and even the POSIX specification copy this definition verbatim, but this definition does not say whether a pending connection is one in the SYN_RCVD state, one in the ESTABLISHED state that has not yet been accepted, or either.
The historical definition in this bullet is the Berkeley implementation, dating back to 4.2BSD, and copied by many others.
The reason for adding this fudge factor appears lost to history [Joy 1994]
But if we consider the backlog as specifying the maximum number of completed connections that the kernel will queue for a socket ([Borman 1997b], as discussed shortly), then the reason for the fudge factor is to take into account incomplete connections on the queue.
If you do not want any clients connecting to your listening socket, close the listening socket.
Assuming the three-way handshake completes normally (i.e., no lost segments and no retransmissions), an entry remains on the incomplete connection queue for one RTT, whatever that value happens to be between a particular client and server.
The median is often used for this statistic, since a few large values can noticeably skew the mean.
This was adequate in the 1980s when busy servers would handle only a few hundred connections per day.
Busy HTTP servers must specify a much larger backlog, and newer kernels must support larger values.
Many current systems allow the administrator to modify the maximum value for the backlog.
A problem is: What value should the application specify for the backlog, since 5 is often inadequate? There is no easy answer to this.
Another method is to assume some default but allow a commandline option or an environment variable to override the default.
We can provide a simple solution to this problem by modifying our wrapper function for the listen function.
We allow the environment variable LISTENQ to override the value specified by the caller.
Figure 4.9 Wrapper function for listen that allows an environment variable to specify backlog.
Manuals and books have historically said that the reason for queuing a fixed number of connections is to handle the case of the server process being busy between successive calls to accept.
This implies that of the two queues, the completed queue should normally have more entries than the incomplete queue.
Again, busy Web servers have shown that this is false.
The reason for specifying a large backlog is because the incomplete connection queue can grow as client SYNs arrive, waiting for completion of the three-way handshake.
Some implementations do send an RST when the queue is full.
This behavior is incorrect for the reasons stated above, and unless your client specifically needs to interact with such a server, it's best to ignore this possibility.
Coding to handle this case reduces the robustness of the client and puts more load on the network in the normal RST case, where the port really has no server listening on it.
Data that arrives after the three-way handshake completes, but before the server calls accept, should be queued by the server TCP, up to the size of the connected socket's receive buffer.
For seven different operating systems there are five distinct columns, showing the variety of interpretations about what backlog means!
The program to measure these values is shown in the solution for Exercise 15.4
As we said, historically the backlog has specified the maximum value for the sum of both queues.
The hacker writes a program to send SYNs at a high rate to the victim, filling the incomplete connection queue for one or more TCP ports.
We use the term hacker to mean the attacker, as described in [Cheswick, Bellovin, and Rubin 2003]
Additionally, the source IP address of each SYN is set to a random number (this is called IP spoofing) so that the server's SYN/ACK goes nowhere.
This also prevents the server from knowing the real IP address of the hacker.
By filling the incomplete queue with bogus SYNs, legitimate SYNs are not queued, providing a denial of service to legitimate clients.
There are two commonly used methods of handling these attacks, summarized in [Borman 1997b]
But what is most interesting in this note is revisiting what the listen backlog really means.
It should specify the maximum number of completed connections for a given socket that the kernel will queue.
The purpose of having a limit on these completed connections is to stop the kernel from accepting new connection requests for a given socket when the application is not accepting them (for whatever reason)
If a system implements this interpretation, as does BSD/OS 3.0, then the application need not specify huge backlog values just because the server handles lots of client requests (e.g., a busy Web server) or to provide protection against SYN flooding.
The kernel handles lots of incomplete connections, regardless of whether they are legitimate or from a hacker.
But even with this interpretation, scenarios do occur where the traditional value of 5 is inadequate.
If the completed connection queue is empty, the process is put to sleep (assuming the default of a blocking socket)
The cliaddr and addrlen arguments are used to return the protocol address of the connected peer process (the client)
If accept is successful, its return value is a brand-new descriptor automatically created by the kernel.
This new descriptor refers to the TCP connection with the client.
When discussing accept, we call the first argument to accept the listening socket (the descriptor created by socket and then used as the first argument to both bind and listen), and we call the return value from accept the connected socket.
A given server normally creates only one listening socket, which then exists for the lifetime of the server.
The kernel creates one connected socket for each client connection that is accepted (i.e., for which the TCP three-way handshake completes)
When the server is finished serving a given client, the connected socket is closed.
This function returns up to three values: an integer return code that is either a new socket descriptor or an error indication, the protocol address of the client process (through the cliaddr pointer), and the size of this address (through the addrlen pointer)
If we are not interested in having the protocol address of the client returned, we set both cliaddr and addrlen to null pointers.
The connected socket is closed each time through the loop, but the listening socket remains open for the life of the server.
We also see that the second and third arguments to accept are null pointers, since we were not interested in the identity of the client.
We will now show how to handle the value-result argument to accept by modifying the code from Figure 1.9 to print the IP address and port of the client.
Figure 4.11 Daytime server that prints client IP address and port.
We define two new variables: len, which will be a value-result variable, and cliaddr, which will contain the client's protocol address.
We initialize len to the size of the socket address structure and pass a pointer to the cliaddr structure and a pointer to len as the second and third arguments to accept.
If we run our new server and then run our client on the same host, connecting to our server twice in a row, we have the following output from the client:
In the first case, the kernel sets the source IP address to the loopback address; in the second case, it sets the address to the IP address of the Ethernet interface.
As a final point, our shell prompt for the server script changes to the pound sign (#), the commonly used prompt for the superuser.
If we do not have superuser privileges, the call to bind will fail:
Before describing how to write a concurrent server in the next section, we must describe the Unix fork function.
This function (including the variants of it provided by some systems) is the only way in Unix to create a new process.
If you have never seen this function before, the hard part in understanding fork is that it is called once but it returns twice.
It returns once in the calling process (called the parent) with a return value that is the process ID of the newly created process (the child)
Hence, the return value tells the process whether it is the parent or the child.
The reason fork returns 0 in the child, instead of the parent's process ID, is because a child has only one parent and it can always obtain the parent's process ID by calling getppid.
A parent, on the other hand, can have any number of children, and there is no way to obtain the process IDs of its children.
If a parent wants to keep track of the process IDs of all its children, it must record the return values from fork.
All descriptors open in the parent before the call to fork are shared with the child after fork returns.
We will see this feature used by network servers: The parent calls accept and then calls fork.
The connected socket is then shared between the parent and child.
Normally, the child then reads and writes the connected socket and the parent closes the connected socket.
A process makes a copy of itself so that one copy can handle one operation while the other copy does another task.
We will see many examples of this later in the text.
Since the only way to create a new process is by calling fork, the process first calls fork to make a copy of itself, and then one of the copies (typically the child process) calls exec (described next) to replace itself with the new program.
The only way in which an executable program file on disk can be executed by Unix is for an existing process to call one of the six exec functions.
We will often refer generically to "the exec function" when it does not matter which of the six is called.
We refer to the process that calls exec as the calling process and the newly executed program as the new program.
Older manuals and books incorrectly refer to the new program as the new process, which is wrong, because a new process is not created.
The differences in the six exec functions are: (a) whether the program file to execute is specified by a filename or a pathname; (b) whether the arguments to the new program are listed one by one or referenced through an array of pointers; and (c) whether the environment of the calling process is passed to the new program or whether a new environment is specified.
All six return: -1 on error, no return on success.
These functions return to the caller only if an error occurs.
Otherwise, control passes to the start of the new program, normally the main function.
The relationship among these six functions is shown in Figure 4.12
Normally, only execve is a system call within the kernel and the other five are library functions that call execve.
The three functions in the top row specify each argument string as a separate argument to the exec function, with a null pointer terminating the variable number of arguments.
The three functions in the second row have an argv array, containing pointers to the argument strings.
This argv array must contain a null pointer to specify its end, since a count is not specified.
The two functions in the left column specify a filename argument.
This is converted into a pathname using the current PATH environment variable.
If the filename argument to execlp or execvp contains a slash (/) anywhere in the string, the PATH variable is not used.
The four functions in the right two columns specify a fully qualified pathname argument.
The four functions in the left two columns do not specify an explicit environment pointer.
Instead, the current value of the external variable environ is used for building an environment list that is passed to the new program.
The envp array of pointers must be terminated by a null pointer.
Descriptors open in the process before calling exec normally remain open across the exec.
We use the qualifier "normally" because this can be disabled using fcntl to set the FD_CLOEXEC descriptor flag.
The inetd server uses this feature, as we will describe in Section 13.5
For something as simple as a daytime server, this is fine.
But when a client request can take longer to service, we do not want to tie up a single server with one client; we want to handle multiple clients at the same time.
The simplest way to write a concurrent server under Unix is to fork a child process to handle each client.
Figure 4.13 shows the outline for a typical concurrent server.
When a connection is established, accept returns, the server calls fork, and the child process services the client (on connfd, the connected socket) and the parent process waits for another connection (on listenfd, the listening socket)
The parent closes the connected socket since the child handles the new client.
In Figure 4.13, we assume that the function doit does whatever is required to service the client.
When this function returns, we explicitly close the connected socket in the child.
This is not required since the next statement calls exit, and part of process termination is to close all open descriptors by the kernel.
Whether to include this explicit call to close or not is a matter of personal programming taste.
We said in Section 2.6 that calling close on a TCP socket causes a FIN to be sent, followed by the normal TCP connection termination sequence.
Why doesn't the close of connfd in Figure 4.13 by the parent terminate its connection with the client? To understand what's happening, we must understand that every file or socket has a reference count.
This is a count of the number of descriptors that are currently open that refer to this file or socket.
This will occur at some time later when the child closes connfd.
We can also visualize the sockets and connection that occur in Figure 4.13 as follows.
First, Figure 4.14 shows the status of the client and server while the server is blocked in the call to accept and the connection request arrives from the client.
Immediately after accept returns, we have the scenario shown in Figure 4.15
The connection is accepted by the kernel and a new socket, connfd, is created.
This is a connected socket and data can now be read and written across the connection.
The next step in the concurrent server is to call fork.
Notice that both descriptors, listenfd and connfd, are shared (duplicated) between the parent and child.
The next step is for the parent to close the connected socket and the child to close the listening socket.
Status of client/server after parent and child close appropriate sockets.
The child is handling the connection with the client and the parent can call accept again on the listening socket, to handle the next client connection.
The normal Unix close function is also used to close a socket and terminate a TCP connection.
The default action of close with a TCP socket is to mark the socket as closed and return to the process immediately.
The socket descriptor is no longer usable by the process: It cannot be used as an argument to read or write.
But, TCP will try to send any data that is already queued to be sent to the other end, and after this occurs, the normal TCP connection termination sequence takes place (Section 2.6)
In that section, we will also describe what a TCP application must do to be guaranteed that the peer application has received any outstanding data.
At the end of Section 4.8, we mentioned that when the parent process in our concurrent server closes the connected socket, this just decrements the reference count for the descriptor.
Since the reference count was still greater than 0, this call to close did not initiate TCP's four-packet connection termination sequence.
This is the behavior we want with our concurrent server with the connected socket that is shared between the parent and child.
If we really want to send a FIN on a TCP connection, the shutdown function can be used (Section 6.6) instead of close.
We will describe the motivation for this in Section 6.5
We must also be aware of what happens in our concurrent server if the parent does not call close for each connected socket returned by accept.
First, the parent will eventually run out of descriptors, as there is usually a limit to the number of descriptors that any process can have open at any time.
But more importantly, none of the client connections will be terminated.
This will prevent TCP's connection termination sequence from occurring, and the connection will remain open.
These two functions return either the local protocol address associated with a socket (getsockname) or the foreign protocol address associated with a socket (getpeername)
Notice that the final argument for both functions is a value-result argument.
That is, both functions fill in the socket address structure pointed to by localaddr or peeraddr.
We mentioned in our discussion of bind that the term "name" is misleading.
These functions have nothing to do with domain names (Chapter 11)
After connect successfully returns in a TCP client that does not call bind, getsockname returns the local IP address and local port number assigned to the connection by the kernel.
After calling bind with a port number of 0 (telling the kernel to choose the local port number), getsockname returns the local port number that was assigned.
In a TCP server that binds the wildcard IP address (Figure 1.9), once a connection is established with a client (accept returns successfully), the server can call getsockname to obtain the local IP address assigned to the connection.
The socket descriptor argument in this call must be that of the connected socket, and not the listening socket.
When a server is execed by the process that calls accept, the only way the server can obtain the identity of the client is to call getpeername.
This is what happens whenever inetd (Section 13.5) forks and execs a TCP server.
Since the child starts with a copy of the parent's memory image, the socket address structure is available to the child, as is the connected socket descriptor (since the descriptors are shared between the parent and child)
But when the child execs the real server (say the Telnet server that we show), the memory image of the child is replaced with the new program file for the Telnet server (i.e., the socket address structure containing the peer's address is lost), and the connected socket descriptor remains open across the exec.
One of the first function calls performed by the Telnet server is getpeername to obtain the IP address and port number of the client.
Obviously the Telnet server in this final example must know the value of connfd when it starts.
First, the process calling exec can format the descriptor number as a character string and pass it as a command-line argument to the newly execed program.
Alternately, a convention can be established that a certain descriptor is always set to the connected socket before calling exec.
Since we do not know what type of socket address structure to allocate, we use a sockaddr_storage value, since it can hold any socket address structure supported by the system.
Since the POSIX specification allows a call to getsockname on an unbound socket, this function should work for any open socket descriptor.
All clients and servers begin with a call to socket, returning a socket descriptor.
Clients then call connect, while servers call bind, listen, and accept.
Most TCP servers are concurrent, with the server calling fork for every client connection that it handles.
While these two models have been used successfully for many years, in Chapter 30 we will look at other server design options that use threads and processes.
Print the local IP address and local port assigned to the TCP socket using sock_ntop.
In what range (Figure 2.10) are your system's ephemeral ports?
The child then completes the service of the client before the call to fork returns to the parent.
What happens in the two calls to close in Figure 4.13?
Remove the call to bind, but allow the call to listen.
We will now use the elementary functions from the previous chapter to write a complete TCP client/server example.
Our simple example is an echo server that performs the following steps:
The client reads a line of text from its standard input and writes the line to the server.1
The server reads the line from its network input and echoes the line back to the client.2
The client reads the echoed line and prints it on its standard output.3
Figure 5.1 depicts this simple client/server along with the functions used for input and output.
We show two arrows between the client and server, but this is really one full-duplex TCP connection.
The fgets and fputs functions are from the standard I/O library and the writen and readline functions were shown in Section 3.9
While we will develop our own implementation of an echo server, most TCP/IP implementations provide such a server, using both TCP and UDP (Section 2.12)
We will also use this server with our own client.
A client/server that echoes input lines is a valid, yet simple, example of a network application.
All the basic steps required to implement any client/server are illustrated by this example.
To expand this example into your own application, all you need to do is change what the server does with the input it receives from its clients.
Besides running our client and server in their normal mode (type in a line and watch it echo), we examine lots of boundary conditions for this example: what happens when the client and server are started; what happens when the client terminates normally; what happens to the client if the server process terminates before the client is done; what happens to the client if the server host crashes; and so on.
By looking at all these scenarios and understanding what happens at the network level, and how this appears to the sockets API, we will understand more about what goes on at these levels and how to code our applications to handle these scenarios.
In all these examples, we have "hard-coded" protocol-specific constants such as addresses and ports.
First, we must understand exactly what needs to be stored in the protocol-specific address structures.
Second, we have not yet covered the library functions that can make this more portable.
Our TCP client and server follow the flow of functions that we diagrammed in Figure 4.1
Binding the wildcard address tells the system that we will accept a connection destined for any local interface, in case the system is multihomed.
Our choice of the TCP port number is based on Figure 2.10
The socket is converted into a listening socket by listen.
The server blocks in the call to accept, waiting for a client connection to complete.
For each client, fork spawns a child, and the child handles the new client.
As we discussed in Section 4.8, the child closes the listening socket and the parent closes the connected socket.
We take the server's IP address from the command-line argument and the server's well-known port (SERV_PORT) is from our unp.h header.
This function, shown in Figure 5.5, handles the client processing loop: It reads a line of text from standard input, writes it to the server, reads back the server's echo of the line, and outputs the echoed line to standard output.
The loop terminates when fgets returns a null pointer, which occurs when it encounters either an end-of-file (EOF) or an error.
Our Fgets wrapper function checks for an error and aborts if one occurs, so Fgets returns a null pointer only when an end-of-file is encountered.
Only by understanding these boundary conditions, and their interaction with the TCP/IP protocols, can we write robust clients and servers that can handle these conditions.
We first start the server in the background on the host linux.
When the server starts, it calls socket, bind, listen, and accept, blocking in the call to accept.
Before starting the client, we run the netstat program to verify the state of the server's listening socket.
Here we show only the first line of output (the heading), plus the line that we are interested in.
This command shows the status of all sockets on the system, which can be lots of output.
We must specify the -a flag to see listening sockets.
We then start the client on the same host, specifying the server's IP address of 127.0.0.1 (the loopback address)
We could have also specified the server's normal (nonloopback) IP address.
The client calls socket and connect, the latter causing TCP's three-way handshake to take place.
When the three-way handshake completes, connect returns in the client and accept returns in the server.
The client calls str_cli, which will block in the call to fgets, because we have not typed a line of input yet.
When accept returns in the server, it calls fork and the child calls str_echo.
This function calls readline, which calls read, which blocks while waiting for a line to be sent from the client.
The server parent, on the other hand, calls accept again, and blocks while waiting for the next client connection.
We have three processes, and all three are asleep (blocked): client, server parent, and server child.
When the three-way handshake completes, we purposely list the client step first, and then the server steps.
The reason can be seen in Figure 2.5: connect returns when the second segment of the handshake is received by the client, but accept does not return until the third segment of the handshake is received by the server, one-half of the RTT after connect returns.
We purposely run the client and server on the same host because this is the easiest way to experiment with client/server applications.
Since we are running the client and server on the same host, netstat now shows two additional lines of output, corresponding to the TCP connection:
If we were running the client and server on different hosts, the client host would display only the client's socket, and the server host would display only the two server sockets.
We can also use the ps command to check the status and relationship of these processes.
We have used very specific arguments to ps to only show us the information that pertains to this discussion.
The PID and PPID columns show the parent and child relationships.
Also, the PPID of the parent is the shell (bash)
The STAT column for all three of our network processes is "S," meaning the process is sleeping (waiting for something)
When a process is asleep, the WCHAN column specifies the condition.
The WCHAN values for our three network processes therefore make sense.
At this point, the connection is established and whatever we type to the client is echoed back.
We type in two lines, each one is echoed, and then we type our terminal EOF character (ControlD), which terminates the client.
This time we pipe the output of netstat into grep, printing only the lines with our server's well-known port.
We can follow through the steps involved in the normal termination of our client and server:
Part of process termination is the closing of all open descriptors, so the client socket is closed by the kernel.
This sends a FIN to the server, to which the server TCP responds with an ACK.
This is the first half of the TCP connection termination sequence.
The closing of the connected socket by the child causes the final two segments of the TCP connection termination to take place: a FIN from the server to the client, and an ACK from the client (Figure 2.5)
Finally, the SIGCHLD signal is sent to the parent when the server child terminates.
This occurs in this example, but we do not catch the signal in our code, and the default action of the signal is to be ignored.
The STAT of the child is now Z (for zombie)
We need to clean up our zombie processes and doing this requires dealing with Unix signals.
In the next section, we will give an overview of signal handling.
A signal is a notification to a process that an event has occurred.
By this we mean that a process doesn't know ahead of time exactly when a signal will occur.
The SIGCHLD signal that we described at the end of the previous section is one that is sent by the kernel whenever a process terminates, to the parent of the terminating process.
Every signal has a disposition, which is also called the action associated with the signal.
We set the disposition of a signal by calling the sigaction function (described shortly) and we have three choices for the disposition:
We can provide a function that is called whenever a specific signal occurs.
This function is called a signal handler and this action is called catching a signal.
Our function is called with a single integer argument that is the signal number and the function returns nothing.
For most signals, calling sigaction and specifying a function to be called when the signal occurs is all that is required to catch a signal.
But we will see later that a few signals, SIGIO, SIGPOLL, and SIGURG, all require additional actions on the part of the process to catch the signal.
We can ignore a signal by setting its disposition to SIG_IGN.
We can set the default disposition for a signal by setting its disposition to SIG_DFL.
The default is normally to terminate a process on receipt of a signal, with certain signals also generating a core image of the process in its current working directory.
There are a few signals whose default disposition is to be ignored: SIGCHLD and SIGURG (sent on the arrival of out-of-band data, Chapter 24) are two that we will encounter in this text.
The POSIX way to establish the disposition of a signal is to call the sigaction function.
This gets complicated, however, as one argument to the function is a structure that we must allocate and fill in.
An easier way to set the disposition of a signal is to call the signal function.
Different implementations provide different signal semantics when it is called, providing backward compatibility, whereas POSIX explicitly spells out the semantics when sigaction is called.
The solution is to define our own function named signal that just calls the POSIX sigaction function.
This provides a simple interface with the desired POSIX semantics.
We include this function in our own library, along with our err_XXX functions and our wrapper functions, for example, that we specify when building any of our programs in this text.
This function is shown in Figure 5.6 (the corresponding wrapper function, Signal, is not shown here as it would be the same whether it called our function or a vendor-supplied signal function)
Figure 5.6 signal function that calls the POSIX sigaction function.
The normal function prototype for signal is complicated by the level of nested parentheses.
To simplify this, we define the Sigfunc type in our unp.h header as.
A pointer to a signal handling function is the second argument to the function, as well as the return value from the function.
The sa_handler member of the sigaction structure is set to the func argument.
Any signal that is blocked cannot be delivered to a process.
We set the sa_mask member to the empty set, which means that no additional signals will be blocked while our signal handler is running.
When the flag is set, a system call interrupted by this signal will be automatically restarted by the kernel.
We will talk more about interrupted system calls in the next section when we continue our example.
If the signal being caught is not SIGALRM, we specify the SA_RESTART flag, if defined.
The reason for making a special case for SIGALRM is that the purpose of generating this signal is normally to place a timeout on an I/O operation, as we will show in Section 14.2, in which case, we want the blocked system call to be interrupted by the signal.
Some older systems, notably SunOS 4.x, automatically restart an interrupted system call.
If this flag is defined, we set it if the signal being caught is SIGALRM.
We call sigaction and then return the old action for the signal as the return value of the signal function.
Throughout this text, we will use the signal function from Figure 5.6
We summarize the following points about signal handling on a POSIX-compliant system:
Older systems removed the signal handler each time it was executed.
While a signal handler is executing, the signal being delivered is blocked.
Furthermore, any additional signals that were specified in the sa_mask signal set passed to sigaction when the handler was installed are also blocked.
If a signal is generated one or more times while it is blocked, it is normally delivered only one time after the signal is unblocked.
We will see an example of this in the next section.
The POSIX real-time standard, 1003.1b, defines some reliable signals that are queued, but we do not use them in this text.
It is possible to selectively block and unblock a set of signals using the sigprocmask function.
This lets us protect a critical region of code by preventing certain signals from being caught while that region of code is executing.
The purpose of the zombie state is to maintain information about the child for the parent to fetch at some later time.
If a process terminates, and that process has children in the zombie state, the parent process ID of all the zombie children is set to 1 (the init process), which will inherit the children and clean them up (i.e., init will wait for them, which removes the zombie)
Some Unix systems show the COMMAND column for a zombie process as <defunct>
They take up space in the kernel and eventually we can run out of processes.
Whenever we fork children, we must wait for them to prevent them from becoming zombies.
To do this, we establish a signal handler to catch SIGCHLD, and within the handler, we call wait.
We will describe the wait and waitpid functions in Section.
It must be done sometime before we fork the first child and needs to be done only once.
Warning: Calling standard I/O functions such as printf in a signal handler is not recommended, for reasons that we will discuss in Section 11.18
We call printf here as a diagnostic tool to see when the child terminates.
The portable way to handle zombies is to catch SIGCHLD and call wait or waitpid.
The client TCP sends a FIN to the server and the server responds with an ACK.
The receipt of the FIN delivers an EOF to the child's pending readline.
The parent is blocked in its call to accept when the SIGCHLD signal is delivered.
The sig_chld function executes (our signal handler), wait fetches the child's PID and termination status, and printf is called from the signal handler.
Since the signal was caught by the parent while the parent was blocked in a slow system call (accept), the kernel causes the accept to return an error of EINTR (interrupted system call)
The parent does not handle this error (Figure 5.2), so it aborts.
The purpose of this example is to show that when writing network programs that catch signals, we must be cognizant of interrupted system calls, and we must handle them.
In this specific example, running under Solaris 9, the signal function provided in the standard C library does not cause an interrupted system call to be automatically restarted by the kernel.
If we run the same example under 4.4BSD, using its library version of the signal function, the kernel restarts the interrupted system call and accept does not return an error.
To handle this potential problem between different operating systems is one reason we define our own version of the signal function that we use throughout the text (Figure 5.6)
As part of the coding conventions used in this text, we always code an explicit return in our.
When reading the code, the unnecessary return statement acts as a reminder that the return may interrupt a system call.
We used the term "slow system call" to describe accept, and we use this term for any system call that can block forever.
For example, there is no guarantee that a server's call to accept will ever return, if there are no clients that will connect to the server.
Similarly, our server's call to read in Figure 5.3 will never return if the client never sends a line for the server to echo.
Other examples of slow system calls are reads and writes of pipes and terminal devices.
A notable exception is disk I/O, which usually returns to the caller (assuming no catastrophic hardware failure)
The basic rule that applies here is that when a process is blocked in a slow system call and the process catches a signal and the signal handler returns, the system call can return an error of EINTR.
For portability, when we write a program that catches signals (most concurrent servers catch SIGCHLD), we must be prepared for slow system calls to return EINTR.
Portability problems are caused by the qualifiers "can" and "some," which were used earlier, and the fact that support for the POSIX SA_RESTART flag is optional.
Even if an implementation supports the SA_RESTART flag, not all interrupted system calls may automatically be restarted.
Most Berkeley-derived implementations, for example, never automatically restart select, and some of these implementations never restart accept or recvfrom.
To handle an interrupted accept, we change the call to accept in Figure 5.2, the beginning of the for loop, to the following:
Notice that we call accept and not our wrapper function Accept, since we must handle the failure of the function ourselves.
What we are doing in this piece of code is restarting the interrupted system call.
This is fine for accept, along with functions such as read, write, select, and open.
But there is one function that we cannot restart: connect.
If this function returns EINTR, we cannot call it again, as doing so will return an immediate error.
When connect is interrupted by a caught signal and is not automatically restarted, we must call select to wait for the connection to complete, as we will describe in Section 16.3
In Figure 5.7, we called the wait function to handle the terminated child.
There are three macros that we can call that examine the termination status and tell us if the child terminated normally, was killed by a signal, or was just stopped by job control.
Additional macros let us then fetch the exit status of the child, or the value of the signal that killed the child, or the value of the job-control signal that stopped the child.
We will use the WIFEXITED and WEXITSTATUS macros in Figure 15.10 for this purpose.
If there are no terminated children for the process calling wait, but the process has one or more children that are still executing, then wait blocks until the first of the existing children terminates.
First, the pid argument lets us specify the process ID that we want to wait for.
A value of -1 says to wait for the first of our children to terminate.
There are other options, dealing with process group IDs, but we do not need them in this text.
This option tells the kernel not to block if there are no terminated children.
We now illustrate the difference between the wait and waitpid functions when used to clean up terminated children.
To do this, we modify our TCP client as shown in Figure 5.9
The purpose of establishing multiple connections is to spawn multiple children from the concurrent server, as shown in Figure 5.8
Figure 5.9 TCP client that establishes five connections with server.
When the client terminates, all open descriptors are closed automatically by the kernel (we do not call close, only exit), and all five connections are terminated at about the same time.
This causes five FINs to be sent, one on each connection, which in turn causes all five server children to terminate at about the same time.
This causes five SIGCHLD signals to be delivered to the parent at about the same time, which we show in Figure 5.10
Client terminates, closing all five connections, terminating all five children.
It is this delivery of multiple occurrences of the same signal that causes the problem we are about to see.
We first run the server in the background and then our new client.
The first thing we notice is that only one printf is output, when we expect all five children to have terminated.
If we execute ps, we see that the other four children still exist as zombies.
Establishing a signal handler and calling wait from that handler are insufficient for preventing zombies.
The problem is that all five signals are generated before the signal handler is executed, and the signal handler is executed only one time because Unix signals are normally not queued.
In the example we just ran, with the client and server on the same host, the signal handler is executed once, leaving four zombies.
But if we run the client and server on different hosts, the signal handler is normally executed two times: once as a result of the first signal being generated, and since the other four signals occur while the signal handler is executing, the handler is called only one more time.
But sometimes, probably dependent on the timing of the FINs arriving at the server host, the signal handler is executed three or even four times.
The correct solution is to call waitpid instead of wait.
This version works because we call waitpid within a loop, fetching the status of any of our children that have terminated.
We must specify the WNOHANG option: This tells waitpid not to block if there are running children that have not yet terminated.
In Figure 5.7, we cannot call wait in a loop, because there is no way to prevent wait from blocking if there are running children that have not yet terminated.
It correctly handles a return of EINTR from accept and it establishes a signal handler (Figure 5.11) that calls waitpid for all terminated children.
Figure 5.12 Final (correct) version of TCP server that handles an error of EINTR from accept.
The purpose of this section has been to demonstrate three scenarios that we can encounter with network programming:
We must catch the SIGCHLD signal when forking child processes.1
We must handle interrupted system calls when we catch signals.2
A SIGCHLD handler must be coded correctly using waitpid to prevent any zombies from being left around.
There is another condition similar to the interrupted system call example in the previous section that can cause accept to return a nonfatal error, in which case we should just call accept again.
The sequence of packets shown in Figure 5.13 has been seen on busy servers (typically busy Web servers)
Receiving an RST for an ESTABLISHED connection before accept is called.
Here, the three-way handshake completes, the connection is established, and then the client TCP sends an RST (reset)
On the server side, the connection is queued by its TCP, waiting for the server process to call accept when the RST arrives.
An easy way to simulate this scenario is to start the server, have it call socket, bind, and listen, and then go to sleep for a short period of time before calling accept.
While the server process is asleep, start the client and have it call socket and connect.
Berkeleyderived implementations handle the aborted connection completely within the kernel, and the server process never sees it.
Most SVR4 implementations, however, return an error to the process as the return from accept, and the error depends on the implementation.
These SVR4 implementations return an errno of EPROTO ("protocol error"), but POSIX specifies that the return must be ECONNABORTED ("software caused connection abort") instead.
The reason for the POSIX change is that EPROTO is also returned when some fatal protocol-related events occur on the streams subsystem.
Returning the same error for the nonfatal abort of an established connection by the client makes it impossible for the server to know whether to call accept again or not.
In the case of the ECONNABORTED error, the server can ignore the error and just call accept again.
The steps involved in Berkeley-derived kernels that never pass this error to the process can be followed in TCPv2
When the server gets around to calling accept, it will never know that a connection that was completed has since been removed from the queue.
We will return to these aborted connections in Section 16.6 and see how they can present a problem when combined with select and a listening socket in the normal blocking mode.
We will now start our client/server and then kill the server child process.
This simulates the crashing of the server process, so we can see what happens to the client.
We must be careful to distinguish between the crashing of the server process, which we are about to describe, and the crashing of the server host, which we will describe in Section 5.14
We start the server and client and type one line to the client to verify that all is okay.
We find the process ID of the server child and kill it.
As part of process termination, all open descriptors in the child are closed.
This causes a FIN to be sent to the client, and the client TCP responds with an ACK.
This is the first half of the TCP connection termination.
The SIGCHLD signal is sent to the server parent and handled correctly (Figure 5.12).3
The client TCP receives the FIN from the server TCP and responds with an ACK, but the problem is that the client process is blocked in the call to fgets waiting for a line from the terminal.
Running netstat at this point shows the state of the sockets.5
From Figure 2.4, we see that half of the TCP connection termination sequence has taken place.
We can still type a line of input to the client.
Here is what happens at the client starting from Step 1:
When we type "another line," str_cli calls writen and the client TCP sends the data to the server.
This is allowed by TCP because the receipt of the FIN by the client TCP only indicates that the server process has closed its end of the connection and will not be sending any more data.
The receipt of the FIN does not tell the client TCP that the server process has terminated (which in this case, it has)
We will cover this again in Section 6.6 when we talk about TCP's half-close.
When the server TCP receives the data from the client, it responds with an RST since the process that had that socket open has terminated.
We can verify that the RST was sent by watching the packets with tcpdump.
Our client is not expecting to receive an EOF at this point (Figure 5.5) so it quits with the error message "server terminated prematurely."
What we have described also depends on the timing of the example.
The client's call to readline may happen before the server's RST is received by the client, or it may happen after.
If the readline happens before the RST is received, as we've shown in our example, the result is an unexpected EOF in the client.
But if the RST arrives first, the result is an ECONNRESET ("Connection reset by peer") error return from readline.
The problem in this example is that the client is blocked in the call to fgets when the FIN arrives on the socket.
The client is really working with two descriptors—the socket and the user input—and instead of blocking on input from only one of the two sources (as str_cli is currently coded), it should block on input from either source.
What happens if the client ignores the error return from readline and writes more data to the server? This can happen, for example, if the client needs to perform two writes to the server before reading anything back, with the first write eliciting the RST.
The rule that applies is: When a process writes to a socket that has received an RST, the SIGPIPE signal is sent to the process.
The default action of this signal is to terminate the process, so the process must catch the signal to avoid being involuntarily terminated.
If the process either catches the signal and returns from the signal handler, or ignores the signal, the write operation returns EPIPE.
A frequently asked question (FAQ) on Usenet is how to obtain this signal on the first write, and not the second.
Following our discussion above, the first write elicits the RST and the second write elicits the signal.
It is okay to write to a socket that has received a FIN, but it is an error to write to a socket that has received an RST.
To see what happens with SIGPIPE, we modify our client as shown in Figure 5.14
All we have changed is to call writen two times: the first time the first byte of data is written to the socket, followed by a pause of one second, followed by the remainder of the line.
The intent is for the first writen to elicit the RST and then for the second writen to generate SIGPIPE.
If we run the client on our Linux host, we get:
We start the client, type in one line, see that line echoed correctly, and then terminate the server child on the server host.
We then type another line ("bye") and the shell tells us the process died with a SIGPIPE signal (some shells do not print anything when a process dies without dumping core, but the shell we're using for this example, bash, tells us what we want to know)
The recommended way to handle SIGPIPE depends on what the application wants to do when this occurs.
If there is nothing special to do, then setting the signal disposition to SIG_IGN is easy, assuming that subsequent output operations will catch the error of EPIPE and terminate.
If special actions are needed when the signal occurs (writing to a log file perhaps), then the signal should be caught and any desired actions can be performed in the signal handler.
Be aware, however, that if multiple sockets are in use, the delivery of the signal will not tell us which socket encountered the error.
If we need to know which write caused the error, then we must either ignore the signal or return from the signal handler and handle EPIPE from the write.
This scenario will test to see what happens when the server host crashes.
To simulate this, we must run the client and server on different hosts.
We then start the server, start the client, type in a line to the client to verify that the connection is up, disconnect the server host from the network, and type in another line at the client.
This also covers the scenario of the server host being unreachable when the client sends data (i.e., some intermediate router goes down after the connection has been established)
When the server host crashes, nothing is sent out on the existing network connections.
That is, we are assuming the host crashes and is not shut down by an operator (which we will cover in Section 5.16)
We type a line of input to the client, it is written by writen (Figure 5.5), and is sent by the client TCP as a data segment.
The client then blocks in the call to readline, waiting for the echoed reply.
If we watch the network with tcpdump, we will see the client TCP continually retransmitting the data segment, trying to receive an ACK from the server.
When the client TCP finally gives up (assuming the server host has not been rebooted during this time, or if the server host has not crashed but was unreachable on the network, assuming the host was still unreachable), an error is returned to the client process.
Since the client is blocked in the call to readline, it returns an error.
Assuming the server host crashed and there were no responses at all to the client's data segments, the error is ETIMEDOUT.
Although our client discovers (eventually) that the peer is down or unreachable, there are times when we want to detect this quicker than having to wait nine minutes.
The solution is to place a timeout on the call to readline, which we will discuss in Section 14.2
The scenario that we just discussed detects that the server host has crashed only when we send data to that host.
If we want to detect the crashing of the server host even if we are not actively sending it data, another technique is required.
In this scenario, we will establish a connection between the client and server and then assume the server host crashes and reboots.
In the previous section, the server host was still down when we sent it data.
Here, we will let the server host reboot before sending it data.
The easiest way to simulate this is to establish the connection, disconnect the server from the network, shut down the server host and then reboot it, and then reconnect the server host to the network.
We do not want the client to see the server host shut down (which we will cover in Section 5.16)
As stated in the previous section, if the client is not actively sending data to the server when the server host crashes, the client is not aware that the server host has crashed.
This assumes we are not using the SO_KEEPALIVE socket option.
We type a line to verify that the connection is established.
We type a line of input to the client, which is sent as a TCP data segment to the server host.3
When the server host reboots after crashing, its TCP loses all information about connections that existed before the crash.
Therefore, the server TCP responds to the received data segment from the client with an RST.
Our client is blocked in the call to readline when the RST is received, causing readline to return the error ECONNRESET.
If it is important for our client to detect the crashing of the server host, even if the client is not actively sending data, then some other technique (such as the SO_KEEPALIVE socket option or some client/server heartbeat function) is required.
The previous two sections discussed the crashing of the server host, or the server host being unreachable across the network.
We now consider what happens if the server host is shut down by an operator while our server process is running on that host.
This gives all running processes a short amount of time to clean up and terminate.
If we do not catch SIGTERM and terminate, our server will be terminated by the SIGKILL signal.
When the process terminates, all open descriptors are closed, and we then follow the same sequence of steps discussed in Section 5.12
As stated there, we must use the select or poll function in our client to have the client detect the termination of the server process as soon as it occurs.
Before any TCP client and server can communicate with each other, each end must specify the socket pair for the connection: the local IP address, local port, foreign IP address, and foreign port.
In Figure 5.15, we show these four values as bullets.
The foreign IP address and foreign port must be specified by the client in the call to connect.
The two local values are normally chosen by the kernel as part of the connect function.
The client has the option of specifying either or both of the local values, by calling bind before connect, but this is not common.
As we mentioned in Section 4.10, the client can obtain the two local values chosen by the kernel by calling getsockname after the connection is established.
Figure 5.16 shows the same four values, but from the server's perspective.
The local port (the server's well-known port) is specified by bind.
Normally, the server also specifies the wildcard IP address in this call.
If the server binds the wildcard IP address on a multihomed host, it can determine the local IP address by calling getsockname after the connection is established (Section 4.10)
The two foreign values are returned to the server by accept.
As we mentioned in Section 4.10, if another program is execed by the server that calls accept, that program can call getpeername to determine the client's IP address and port, if necessary.
In our example, the server never examines the request that it receives from the client.
The server just reads all the data up through and including the newline and sends it back to the client, looking for only the newline.
This is an exception, not the rule, and normally we must worry about the format of the data exchanged between the client and server.
Let's modify our server so that it still reads a line of text from the client, but the server now expects that line to contain two integers separated by white space, and the server returns the sum of those two integers.
Our client and server main functions remain the same, as does our str_cli function.
We call sscanf to convert the two arguments from text strings to long integers, and then snprintf is called to convert the result into a text string.
This new client and server work fine, regardless of the byte ordering of the client and server hosts.
We now modify our client and server to pass binary values across the socket, instead of text strings.
We will see that this does not work when the client and server are run on hosts with different byte orders, or on hosts that do not agree on the size of a long integer (Figure 1.17)
We define one structure for the two arguments, another structure for the result, and place both definitions in our sum.h header, shown in Figure 5.18
We call readn to read the reply, and print the result using printf.
We read the arguments by calling readn, calculate and store the sum, and call writen to send back the result structure.
If we run the client and server on two machines of the same architecture, say two SPARC machines, everything works fine.
But when the client and server are on two machines of different architectures (say the server is on the big-endian SPARC system freebsd and the client is on the little endian Intel system linux), it does not work.
The problem is that the two binary integers are sent across the socket in little-endian format by the client, but interpreted as big-endian integers by the server.
We see that it appears to work for positive integers but fails for negative integers (see Exercise 5.8)
The most common formats are big-endian and little-endian, as we described in Section 3.4
There is no guarantee that a short, int, or long is of any certain size.
Different implementations pack structures differently, depending on the number of bits used for the various datatypes and the alignment restrictions of the machine.
Therefore, it is never wise to send binary structures across a socket.
There are two common solutions to this data format problem:
This assumes that both hosts have the same character set.
Explicitly define the binary formats of the supported datatypes (number of bits, big- or littleendian) and pass all data between the client and server in this format.
The first version of our echo client/server totaled about 150 lines (including the readline and writen functions), yet provided lots of details to examine.
The first problem we encountered was zombie children and we caught the SIGCHLD signal to handle this.
Our signal handler then called waitpid and we demonstrated that we must call this function instead of the older wait function, since Unix signals are not queued.
This led us into some of the details of POSIX signal handling (additional information on this topic is provided in Chapter 10 of APUE)
The next problem we encountered was the client not being notified when the server process terminated.
We saw that our client's TCP was notified, but we did not receive that notification since we were blocked, waiting for user input.
We will use the select or poll function in Chapter 6 to handle this scenario, by waiting for any one of multiple descriptors to be ready, instead of blocking on a single descriptor.
We also discovered that if the server host crashes, we do not detect this until the client sends data to the server.
Our simple example exchanged lines of text, which was okay since the server never looked at the lines it echoed.
Sending numeric data between the client and server can lead to a new set of problems, as shown.
Type in a few lines to verify that the client and server work.
Terminate the client by typing your EOF character and note the time.
Use netstat on the client host to verify that the client's end of the connection goes through the TIME_WAIT state.
Execute netstat every five seconds or so to see when the TIME_WAIT state ends.
Are the final two segments exchanged (a FIN from the client that is ACKed by the server)? If so, when, and if not, why?
Change the server's port number to 13, the daytime server.
When the connection is established, sleep for two seconds, write a few bytes to the socket, sleep for another two seconds, and write a few more bytes to the socket.
In Section 5.12, we saw our TCP client handling two inputs at the same time: standard input and a TCP socket.
We encountered a problem when the client was blocked in a call to fgets (on standard input) and the server process was killed.
The server TCP correctly sent a FIN to the client TCP, but since the client process was blocked reading from standard input, it never saw the EOF until it read from the socket (possibly much later)
What we need is the capability to tell the kernel that we want to be notified if one or more I/O conditions are ready (i.e., input is ready to be read, or the descriptor is capable of taking more output)
This capability is called I/O multiplexing and is provided by the select and poll functions.
We will also cover a newer POSIX variation of the former, called pselect.
Some systems provide more advanced ways for processes to wait for a list of events.
A poll device is one mechanism provided in different forms by different vendors.
I/O multiplexing is typically used in networking applications in the following scenarios:
When a client is handling multiple descriptors (normally interactive input and a network socket), I/O multiplexing should be used.
It is possible, but rare, for a client to handle multiple sockets at the same time.
We will show an example of this using select in Section 16.5 in the context of a Web client.
If a TCP server handles both a listening socket and its connected sockets, I/O multiplexing is normally used, as we will show in Section 6.8
If a server handles both TCP and UDP, I/O multiplexing is normally used.
We will show an example of this in Section 8.15
If a server handles multiple services and perhaps multiple protocols (e.g., the inetd daemon that we will describe in Section 13.5), I/O multiplexing is normally used.
Before describing select and poll, we need to step back and look at the bigger picture, examining the basic differences in the five I/O models that are available to us under Unix:
You may want to skim this section on your first reading and then refer back to it as you encounter the different I/O models described in more detail in later chapters.
As we show in all the examples in this section, there are normally two distinct phases for an input operation:
For an input operation on a socket, the first step normally involves waiting for data to arrive on the network.
When the packet arrives, it is copied into a buffer within the kernel.
The second step is copying this data from the kernel's buffer into our application buffer.
The most prevalent model for I/O is the blocking I/O model, which we have used for all our examples so far in the text.
Using a datagram socket for our examples, we have the scenario shown in Figure 6.1
We use UDP for this example instead of TCP because with UDP, the concept of data being "ready" to read is simple: either an entire datagram has been received or it has not.
With TCP it gets more complicated, as additional variables such as the socket's low-water mark come into play.
In the examples in this section, we also refer to recvfrom as a system call because we are differentiating between our application and the kernel.
Regardless of how recvfrom is implemented (as a system call on a Berkeley-derived kernel or as a function that invokes the getmsg system call on a System V kernel), there is normally a switch from running in the application to running in the kernel, followed at some time later by a return to the application.
In Figure 6.1, the process calls recvfrom and the system call does not return until the datagram arrives and is copied into our application buffer, or an error occurs.
The most common error is the system call being interrupted by a signal, as we described in Section 5.9
We say that our process is blocked the entire time from when it calls recvfrom until it returns.
When we set a socket to be nonblocking, we are telling the kernel "when an I/O operation that I request cannot be completed without putting the process to sleep, do not put the process to sleep, but return an error instead." We will describe nonblocking I/O in Chapter 16, but Figure 6.2 shows a summary of the example we are considering.
The first three times that we call recvfrom, there is no data to return, so the kernel immediately returns an error of EWOULDBLOCK instead.
The fourth time we call recvfrom, a datagram is ready, it is copied into our application buffer, and recvfrom returns successfully.
When an application sits in a loop calling recvfrom on a nonblocking descriptor like this, it is called polling.
The application is continually polling the kernel to see if some operation is ready.
This is often a waste of CPU time, but this model is occasionally encountered, normally on systems dedicated to one function.
With I/O multiplexing, we call select or poll and block in one of these two system calls, instead of blocking in the actual I/O system call.
Figure 6.3 is a summary of the I/O multiplexing model.
We block in a call to select, waiting for the datagram socket to be readable.
But the advantage in using select, which we will see later in this chapter, is that we can wait for more than one descriptor to be ready.
Another closely related I/O model is to use multithreading with blocking I/O.
That model very closely resembles the model described above, except that instead of using select to block on multiple file descriptors, the program uses multiple threads (one per file descriptor), and each thread is then free to call blocking system calls like recvfrom.
We can also use signals, telling the kernel to notify us with the SIGIO signal when the descriptor is ready.
We call this signal-driven I/O and show a summary of it in Figure 6.4
We first enable the socket for signal-driven I/O (as we will describe in Section 25.2) and install a signal handler using the sigaction system call.
The return from this system call is immediate and our process continues; it is not blocked.
When the datagram is ready to be read, the SIGIO signal is generated for our process.
We can either read the datagram from the signal handler by calling recvfrom and then notify the main loop that the data is ready to be processed (this is what we will do in Section 25.3), or we can notify the main loop and let it read the datagram.
Regardless of how we handle the signal, the advantage to this model is that we are not blocked while waiting for the datagram to arrive.
The main loop can continue executing and just wait to be notified by the signal handler that either the data is ready to process or the datagram is ready to be read.
Asynchronous I/O is defined by the POSIX specification, and various differences in the real-time.
In general, these functions work by telling the kernel to start the operation and to notify us when the entire operation (including the copy of the data from the kernel to our buffer) is complete.
The main difference between this model and the signal-driven I/O model in the previous section is that with signal-driven I/O, the kernel tells us when an I/O operation can be initiated, but with asynchronous I/O, the kernel tells us when an I/O operation is complete.
This system call returns immediately and our process is not blocked while waiting for the I/O to complete.
We assume in this example that we ask the kernel to generate some signal when the operation is complete.
This signal is not generated until the data has been copied into our application buffer, which is different from the signal-driven I/O model.
As of this writing, few systems support POSIX asynchronous I/O.
We are not certain, for example, if systems will support it for sockets.
Our use of it here is as an example to compare against the signal-driven I/O model.
Figure 6.6 is a comparison of the five different I/O models.
It shows that the main difference between the first four models is the first phase, as the second phase in the first four models is the same: the process is blocked in a call to recvfrom while the data is copied from the kernel to the caller's buffer.
Asynchronous I/O, however, handles both phases and is different from the first four.
A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes.
An asynchronous I/O operation does not cause the requesting process to be blocked.
Using these definitions, the first four I/O models—blocking, nonblocking, I/O multiplexing, and signal-driven I/O—are all synchronous because the actual I/O operation (recvfrom) blocks the process.
Only the asynchronous I/O model matches the asynchronous I/O definition.
This function allows the process to instruct the kernel to wait for any one of multiple events to occur and to wake up the process only when one or more of these events occurs or when a specified amount of time has passed.
As an example, we can call select and tell the kernel to return only when:
That is, we tell the kernel what descriptors we are interested in (for reading, writing, or an exception condition) and how long to wait.
The descriptors in which we are interested are not restricted to sockets; any descriptor can be tested using select.
Berkeley-derived implementations have always allowed I/O multiplexing with any descriptor.
We start our description of this function with its final argument, which tells the kernel how long to wait for one of the specified descriptors to become ready.
A timeval structure specifies the number of seconds and microseconds.
Wait forever— Return only when one of the specified descriptors is ready for I/O.
For this, we specify the timeout argument as a null pointer.
Wait up to a fixed amount of time— Return when one of the specified descriptors is ready for I/O, but do not wait beyond the number of seconds and microseconds specified in the timeval structure pointed to by the timeout argument.
Do not wait at all— Return immediately after checking the descriptors.
The wait in the first two scenarios is normally interrupted if the process catches a signal and returns from the signal handler.
This means that for portability, we must be prepared for select to return an error of EINTR if we are catching signals.
Although the timeval structure lets us specify a resolution in microseconds, the actual resolution supported by the kernel is often more coarse.
For example, many Unix kernels round the timeout value up to a multiple of 10 ms.
There is also a scheduling latency involved, meaning it takes some time after the timer expires before the kernel schedules this process to run.
Of course, that's a very large timeout (over three years) and likely not very useful, but the point is that the timeval structure can represent values that are not supported by select.
The const qualifier on the timeout argument means it is not modified by select on return.
For example, if we specify a time limit of 10 seconds, and select returns before the timer expires with one or more of the descriptors ready or with an error of EINTR, the timeval structure is not updated with the number of seconds remaining when the function returns.
If we wish to know this value, we must obtain the system time before calling select, and then again when it returns, and subtract the two (any robust program will take into account that the system time may be adjusted by either the administrator or by a daemon like ntpd occasionally)
Therefore, for portability, assume the timeval structure is undefined upon return, and initialize it before each call to select.
The three middle arguments, readset, writeset, and exceptset, specify the descriptors that we want the kernel to test for reading, writing, and exception conditions.
The presence of control status information to be read from the master side of a pseudoterminal that has been put into packet mode.
A design problem is how to specify one or more descriptor values for each of these three arguments.
We allocate a descriptor set of the fd_set datatype, we set and test the bits in the set using these macros, and we can also assign it to another descriptor set across an equals sign (=) in C.
What we are describing, an array of integers using one bit per descriptor, is just one possible way to implement select.
Nevertheless, it is common to refer to the individual descriptors within a descriptor set as bits, as in "turn on the bit for the listening descriptor in the read set."
We will see in Section 6.10 that the poll function uses a completely different representation: a variable-length array of structures with one structure per descriptor.
It is important to initialize the set, since unpredictable results can occur if the set is allocated as an automatic variable and not initialized.
Any of the middle three arguments to select, readset, writeset, or exceptset, can be specified as a null pointer if we are not interested in that condition.
Indeed, if all three pointers are null, then we have a higher precision timer than the normal Unix sleep function (which sleeps for multiples of a second)
The maxfdp1 argument specifies the number of descriptors to be tested.
Its value is the maximum descriptor to be tested plus one (hence our name of maxfdp1)
Its value is often 1024, but few programs use that many descriptors.
The maxfdp1 argument forces us to calculate the largest descriptor that we are interested in and then tell the kernel this value.
The reason this argument exists, along with the burden of calculating its value, is purely for efficiency.
When we call the function, we specify the values of the descriptors that we are interested in, and on return, the result indicates which descriptors are ready.
Any descriptor that is not ready on return will have its corresponding bit cleared in the descriptor set.
To handle this, we turn on all the bits in which we are interested in all the descriptor sets each time we call select.
The two most common programming errors when using select are to forget to add one to the largest descriptor number and to forget that the descriptor sets are value-result arguments.
The return value from this function indicates the total number of bits that are ready across all the descriptor sets.
If the timer value expires before any of the descriptors are ready, a value of 0 is returned.
A return value of –1 indicates an error (which can happen, for example, if the function is interrupted by a caught signal)
Early releases of SVR4 had a bug in their implementation of select: If the same bit was on in multiple sets, say a descriptor was ready for both reading and writing, it was counted only once.
We have been talking about waiting for a descriptor to become ready for I/O (reading or writing) or to have an exception condition pending on it (out-of-band data)
A socket is ready for reading if any of the following four conditions is true:
The number of bytes of data in the socket receive buffer is greater than or equal to the current size of the low-water mark for the socket receive buffer.
A read operation on the socket will not block and will return a value greater than 0 (i.e., the data that is ready to be read)
We can set this low-water mark using the SO_RCVLOWAT socket option.
The read half of the connection is closed (i.e., a TCP connection that has received a FIN)
A read operation on the socket will not block and will return 0 (i.e., EOF)
The socket is a listening socket and the number of completed connections is nonzero.
An accept on the listening socket will normally not block, although we will describe a timing condition in Section 16.6 under which the accept can block.
A read operation on the socket will not block and will return an error (–1) with errno set to the specific error condition.
These pending errors can also be fetched and cleared by calling getsockopt and specifying the SO_ERROR socket.
A socket is ready for writing if any of the following four conditions is true:
The number of bytes of available space in the socket send buffer is greater than or equal to the current size of the low-water mark for the socket send buffer and either: (i) the socket is connected, or (ii) the socket does not require a connection (e.g., UDP)
This means that if we set the socket to nonblocking (Chapter 16), a write operation will not block and will return a positive value (e.g., the number of bytes accepted by the transport layer)
We can set this low-water mark using the SO_SNDLOWAT socket option.
This low-water mark normally defaults to 2048 for TCP and UDP sockets.
A write operation on the socket will generate SIGPIPE (Section 5.12)
A socket using a non-blocking connect has completed the connection, or the connect has failed.
A write operation on the socket will not block and will return an error (–1) with errno set to the specific error condition.
These pending errors can also be fetched and cleared by calling getsockopt with the SO_ERROR socket option.
A socket has an exception condition pending if there is out-of-band data for the socket or the socket is still at the out-of-band mark.
Our definitions of "readable" and "writable" are taken directly from the kernel's soreadable and sowriteable macros on pp.
Similarly, our definition of the "exception condition" for a socket is from the soo_select function on these same pages.
Notice that when an error occurs on a socket, it is marked as both readable and writable by select.
The purpose of the receive and send low-water marks is to give the application control over how much data must be available for reading or how much space must be available for writing before select returns a readable or writable status.
As long as the send low-water mark for a UDP socket is less than the send buffer size (which should always be the default relationship), the UDP socket is always writable, since a connection is not required.
Figure 6.7 summarizes the conditions just described that cause a socket to be ready for select.
Summary of conditions that cause a socket to be ready for select.
We said earlier that most applications do not use lots of descriptors.
It is rare, for example, to find an application that uses hundreds of descriptors.
But, such applications do exist, and they often use select to multiplex the descriptors.
But, current versions of Unix allow for a virtually unlimited number of descriptors per process (often limited only by the amount of memory and any administrative limits), so the question is: How does this affect select?
These macros * manipulate such bit fields (the filesystem macros use chars)
The only way to increase the size of the descriptor sets is to increase the value of FD_SETSIZE and then recompile the kernel.
Some vendors are changing their implementation of select to allow the process to define FD_SETSIZE to a larger value than the default.
BSD/OS has changed the kernel implementation to allow larger descriptor sets, and it also provides four new FD_xxx macros to dynamically allocate and manipulate these larger sets.
From a portability standpoint, however, beware of using large descriptor sets.
The problem with that earlier version was that we could be blocked in the call to fgets when something happened on the socket.
Our new version blocks in a call to select instead, waiting for either standard input or the socket to be readable.
Figure 6.8 shows the various conditions that are handled by our call to select.
If the peer TCP sends data, the socket becomes readable and read returns greater than 0 (i.e., the number of bytes of data)
If the peer TCP sends a FIN (the peer process terminates), the socket becomes readable and read returns 0 (EOF)
If the peer TCP sends an RST (the peer host has crashed and rebooted), the socket becomes readable, read returns –1, and errno contains the specific error code.
Figure 6.9 shows the source code for this new version.
The function fileno converts a standard I/O file pointer into its corresponding descriptor.
In the call, the write-set pointer and the exception-set pointer are both null pointers.
The final argument (the time limit) is also a null pointer since we want the call to block until something is ready.
If, on return from select, the socket is readable, the echoed line is read with readline and output by fputs.
If the standard input is readable, a line is read by fgets and written to the socket using writen.
Notice that the same four I/O functions are used as in Figure 5.5, fgets, writen, readline, and fputs, but the order of flow within the function has changed.
Instead of the function flow being driven by the call to fgets, it is now driven by the call to select.
First, let's go back to our original version, Figure 5.5
It operates in a stop-and-wait mode, which is fine for interactive use: It sends a line to the server and then waits for the reply.
This amount of time is one RTT plus the server's processing time (which is close to 0 for a simple echo server)
We can therefore estimate how long it will take for a given number of lines to be echoed if we know the RTT between the client and server.
The ping program is an easy way to measure RTTs.
If we consider the network between the client and server as a full-duplex pipe, with requests going from the client to the server and replies in the reverse direction, then Figure 6.10 shows our stop-and-wait mode.
We also assume that there is no server processing time and that the size of the request is the same as the reply.
We show only the data packets between the client and server, ignoring the TCP acknowledgments that are also going across the network.
Since there is a delay between sending a packet and that packet arriving at the other end of the pipe, and since the pipe is full-duplex, in this example, we are only using one-eighth of the pipe's capacity.
This stop-and-wait mode is fine for interactive input, but since our client reads from standard input and writes to standard output, and since it is trivial under the Unix shells to redirect the input and output, we can easily run our client in a batch mode.
When we redirect the input and output, however, the resulting output file is always smaller than the input file (and they should be identical for an echo server)
To see what's happening, realize that in a batch mode, we can keep sending requests as fast as the network can accept them.
The server processes them and sends back the replies at the same rate.
Filling the pipe between the client and server: batch mode.
Here we assume that after sending the first request, we immediately send another, and then another.
We also assume that we can keep sending requests as fast as the network can accept them, along with processing replies as fast as the network supplies them.
There are numerous subtleties dealing with TCP's bulk data flow that we are ignoring here, such as its slow-start algorithm, which limits the rate at which data is sent on a new or idle connection, and the returning ACKs.
But we cannot close the connection after writing this request because there are still other requests and replies in the pipe.
The cause of the problem is our handling of an EOF on input: The function returns to the main function, which then terminates.
But in a batch mode, an EOF on input does not imply that we have finished reading from the socket; there might still be requests on the way to the server, or replies on the way back from the server.
What we need is a way to close one-half of the TCP connection.
That is, we want to send a FIN to the server, telling it we have finished sending data, but leave the socket descriptor open for reading.
This is done with the shutdown function, which is described in the next section.
In general, buffering for performance adds complexity to a network application, and the code in Figure 6.9 suffers from this complexity.
Consider the case when several lines of input are available from the standard input.
But, fgets only returns a single line and leaves any remaining data sitting in the stdio buffer.
The reason for this is that select knows nothing of the buffers used by stdio—it will only show readability from the viewpoint of the read system call, not calls like fgets.
For this reason, mixing stdio and select is considered very error-prone and should only be done with great care.
The same problem exists with the call to readline in the example in Figure 6.9
Instead of data being hidden from select in a stdio buffer, it is hidden in readline's buffer.
Recall that in Section 3.9 we provided a function that gives visibility into readline's buffer, so one possible solution is to modify our code to use that function before calling select to see if data has already been read but not consumed.
But again, the complexity grows out of hand quickly when we have to handle the case where the readline buffer contains a partial line (meaning we still need to read more) as well as when it contains one or more complete lines (which we can consume)
The normal way to terminate a network connection is to call the close function.
But, there are two limitations with close that can be avoided with shutdown:
With shutdown, we can initiate TCP's normal connection termination sequence (the four segments beginning with a FIN in Figure 2.5), regardless of the reference count.
Since a TCP connection is full-duplex, there are times when we want to tell the other end that we have finished sending, even though that end might have more data to send us.
This is the scenario we encountered in the previous section with batch input to our str_cli function.
Figure 6.12 shows the typical function calls in this scenario.
The action of the function depends on the value of the howto argument.
SHUT_RD The read half of the connection is closed— No more data can be received on the socket and any data currently in the socket receive buffer is discarded.
The process can no longer issue any of the read functions on the socket.
Any data received after this call for a TCP socket is acknowledged and then silently discarded.
By default, everything written to a routing socket (Chapter 18) loops back as possible input to all routing sockets on the host.
Some programs call shutdown with a second argument of SHUT_RD to prevent the loopback copy.
An alternative way to prevent this loopback copy is to clear the SO_USELOOPBACK socket option.
Any data currently in the socket send buffer will be sent, followed by TCP's normal connection termination sequence.
The process can no longer issue any of the write functions on the socket.
Figure 7.12 will summarize the different possibilities available to the process by calling shutdown and close.
The operation of close depends on the value of the SO_LINGER socket option.
The three SHUT_xxx names are defined by the POSIX specification.
The former notifies us as soon as the server closes its end of the connection and the latter lets us handle batch input correctly.
This version also does away with line-centric code and operates instead on buffers, eliminating the complexity concerns raised in Section 6.5
As long as this flag is 0, each time around the main loop, we select on standard input for readability.
When we read the EOF on the socket, if we have already encountered an EOF on standard input, this is normal termination and the function returns.
But if we have not yet encountered an EOF on standard input, the server process has prematurely terminated.
We now call read and write to operate on buffers instead of lines and allow select to work for us as expected.
When we encounter the EOF on standard input, our new flag, stdineof, is set and we call shutdown with a second argument of SHUT_WR to send the FIN.
Here also, we've changed to operating on buffers instead of lines, using read and writen.
Before showing the code, let's look at the data structures that we will use to keep track of the clients.
Figure 6.14 shows the state of the server before the first client has established a connection.
The server has a single listening descriptor, which we show as a bullet.
The server maintains only a read descriptor set, which we show in Figure 6.15
We also show an array of integers named client that contains the connected socket descriptor for each client.
Data structures for TCP server with just a listening socket.
When the first client establishes a connection with our server, the listening descriptor becomes readable and our server calls accept.
The new connected descriptor returned by accept will be 4, given the assumptions of this example.
Figure 6.16 shows the connection from the client to the server.
From this point on, our server must remember the new connected socket in its client array, and the connected socket must be added to the descriptor set.
Sometime later a second client establishes a connection and we have the scenario shown in Figure 6.18
The client TCP sends a FIN, which makes descriptor 4 in the server readable.
We then close this socket and update our data structures accordingly.
In summary, as clients arrive, we record their connected socket descriptor in the first available entry in the client array (i.e., the first entry with a value of –1)
We must also add the connected socket to the read descriptor set.
The variable maxi is the highest index in the client array that is currently in use and the variable maxfd (plus one) is the current value of the first argument to select.
Figure 6.21 shows the first half of this version of the server.
Figure 6.21 TCP server using a single process and select: initialization.
The steps to create the listening socket are the same as seen earlier: socket, bind, and listen.
We initialize our data structures assuming that the only descriptor that we will select on initially is the listening socket.
The last half of the function is shown in Figure 6.22
Figure 6.22 TCP server using a single process and select loop.
If the listening socket is readable, a new connection has been established.
We use the first unused entry in the client array to record the connected socket.
The number of ready descriptors is decremented, and if it is 0, we can avoid the next for loop.
This lets us use the return value from select to avoid checking descriptors that are not ready.
A test is made for each existing client connection as to whether or not its descriptor is in the descriptor set returned by select.
If so, a line is read from the client and echoed back to the client.
If the client closes the connection, read returns 0 and we update our data structures accordingly.
We never decrement the value of maxi, but we could check for this possibility each time a client closes its connection.
Nevertheless, in Section 16.6, we will describe a problem with this server that is easily fixed by making the listening socket nonblocking and then checking for, and ignoring, a few errors from accept.
Unfortunately, there is a problem with the server that we just showed.
Consider what happens if a malicious client connects to the server, sends one byte of data (other than a newline), and then goes to sleep.
The server will call read, which will read the single byte of data from the client and then block in the next call to read, waiting for more data from this client.
The server is then blocked ("hung" may be a better term) by this one client and will not service any other clients (either new client connections or existing clients' data) until the malicious client either sends a newline or terminates.
The basic concept here is that when a server is handling multiple clients, the server can never block in a function call related to a single client.
Doing so can hang the server and deny service to all other clients.
It does something to the server that prevents it from servicing other legitimate clients.
The pselect function was invented by POSIX and is now supported by many of the Unix variants.
This allows the program to disable the delivery of certain signals, test some global variables that are set by the handlers for these now-disabled signals, and then call pselect, telling it to reset the signal mask.
Our program's signal handler for SIGINT just sets the global intr_flag and returns.
If our process is blocked in a call to select, the return from the signal handler causes the function to return with errno set to EINTR.
But when select is called, the code looks like the following:
The problem is that between the test of intr_flag and the call to select, if the signal occurs, it will be lost if select blocks forever.
With pselect, we can now code this example reliably as.
When pselect is called, it replaces the signal mask of the process with an empty set (i.e., zeromask) and then checks the descriptors, possibly going to sleep.
But when pselect returns, the signal mask of the process is reset to its value before pselect was called (i.e., SIGINT is blocked)
We will say more about pselect and show an example of it in Section 20.5
There is one other slight difference between the two select functions.
The first member of the timeval structure is a signed long integer, while the first member of the timespec structure is a time_t.
The signed long in the former should also be a time_t, but was not changed retroactively to avoid breaking existing code.
SVR4 removed this limitation, allowing poll to work with any descriptor.
The first argument is a pointer to the first element of an array of structures.
Each element of the array is a pollfd structure that specifies the conditions to be tested for a given descriptor, fd.
The conditions to be tested are specified by the events member, and the function returns the status for that descriptor in the corresponding revents member.
Having two variables per descriptor, one a value and one a result, avoids value-result arguments.
Recall that the middle three arguments for select are value-result.
Each of these two members is composed of one or more bits that specify a certain condition.
Figure 6.23 shows the constants used to specify the events flag and to test the revents flag against.
We have divided this figure into three sections: The first four constants deal with input, the next three deal with output, and the final three deal with errors.
Notice that the final three cannot be set in events, but are always returned in revents when the corresponding condition exists.
There are three classes of data identified by poll: normal, priority band, and high-priority.
Similarly, POLLOUT is equivalent to POLLWRNORM, with the former predating the latter.
With regard to TCP and UDP sockets, the following conditions cause poll to return the specified revent.
Unfortunately, POSIX leaves many holes (i.e., optional ways to return the same condition) in its definition of poll.
All regular TCP data and all UDP data is considered normal.
The presence of an error for a TCP connection can be considered either normal data or an error (POLLERR)
In either case, a subsequent read will return –1 with errno set to the appropriate value.
This handles conditions such as the receipt of an RST or a timeout.
The availability of a new connection on a listening socket can be considered either normal data or priority data.
The completion of a nonblocking connect is considered to make a socket writable.
The number of elements in the array of structures is specified by the nfds argument.
Historically, this argument has been an unsigned long, which seems excessive.
The timeout argument specifies how long the function is to wait before returning.
A positive value specifies the number of milliseconds to wait.
Figure 6.24 shows the possible values for the timeout argument.
The constant INFTIM is defined to be a negative value.
If the system does not provide a timer with millisecond accuracy, the value is rounded up to the nearest supported value.
As with select, any timeout set for poll is limited by the implementation's clock resolution.
If we are no longer interested in a particular descriptor, we just set the fd member of the pollfd structure to a negative value.
Then the events member is ignored and the revents member is set to 0 on return.
We do not have that problem with poll since it is the caller's responsibility to allocate an array of pollfd structures and then tell the kernel the number of elements in the array.
There is no fixed-size datatype similar to fd_set that the kernel knows about.
But, from a portability perspective today, more systems support select than poll.
Also, POSIX defines pselect, an enhanced version of select that handles signal blocking and provides increased time resolution.
In the previous version using select , we had to allocate a client array along with a descriptor set named rset (Figure 6.15 )
With poll , we must allocate an array of pollfd structures to maintain the client information instead of allocating another array.
Recall from the previous section that any entry in the array of pollfd structures passed to poll with a negative value for the fd member is just ignored.
We declare OPEN_MAX elements in our array of pollfd structures.
Determining the maximum number of descriptors that a process can have open at any one time is difficult.
But one of the possible returns from sysconf is "indeterminate," meaning we still have to guess a value.
We use the first entry in the client array for the listening socket and set the descriptor for the remaining entries to –1
We also set the POLLRDNORM event for this descriptor, to be notified by poll when a new connection is ready to be accepted.
The variable maxi contains the largest index of the client array currently in use.
The second half of our function is shown in Figure 6.26
We call poll to wait for either a new connection or data on existing connection.
When a new connection is accepted, we find the first available entry in the client array by looking for the first one with a negative descriptor.
When an available entry is found, we save the descriptor and set the POLLRDNORM event.
The two return events that we check for are POLLRDNORM and POLLERR.
The second of these we did not set in the events member because it is always returned when the condition is true.
The reason we check for POLLERR is because some implementations return this event when an RST is received for a connection, while others just return POLLRDNORM.
In either case, we call read and if an error has occurred, it will return an error.
When an existing connection is terminated by the client, we just set the fd member to –1
There are five different models for I/O provided by Unix:
The default is blocking I/O, which is also the most commonly used.
We will cover nonblocking I/O and signal-driven I/O in later chapters and have covered I/O multiplexing in this chapter.
True asynchronous I/O is defined by the POSIX specification, but few implementations exist.
The most commonly used function for I/O multiplexing is select.
We tell the select function what descriptors we are interested in (for reading, writing, and exceptions), the maximum amount of time to wait, and the maximum descriptor number (plus one)
Most calls to select specify readability, and we noted that the only exception condition when dealing with sockets is the arrival of out-of-band data (Chapter 24)
Since select provides a time limit on how long a function blocks, we will use this feature in Figure 14.3 to place a time limit on an input operation.
We used our echo client in a batch mode using select and discovered that even though the end of the user input is encountered, data can still be in the pipe to or from the server.
To handle this scenario requires the shutdown function, and it lets us take advantage of TCP's half-close feature.
The dangers of mixing stdio buffering (as well as our own readline buffering) with select caused us to produce versions of the echo client and server that operated on buffers instead of lines.
This lets us avoid race conditions when signals are being caught and we talk more about this in Section 20.5
The poll function from System V provides functionality similar to select and provides additional information on STREAMS devices.
There are various ways to get and set the options that affect a socket:
This chapter starts by covering the setsockopt and getsockopt functions, followed by an example that prints the default value of all the options, and then a detailed description of all the socket options.
This detailed coverage can be skipped during a first reading of this chapter, and the individual sections referred to when needed.
We also describe the fcntl function, because it is the POSIX way to set a socket for nonblocking I/O, signal-driven I/O, and to set the owner of a socket.
The size of this variable is specified by the final argument, as a value for setsockopt and as a value-result for getsockopt.
Summary of socket and IP-layer socket options for getsockopt and setsockopt.
There are two basic types of options: binary options that enable or disable a certain feature (flags), and options that fetch and return specific values that we can either set or examine.
The column labeled "Flag" specifies if the option is a flag option.
When calling getsockopt for these flag options, *optval is an integer.
The value returned in *optval is zero if the option is disabled, or nonzero if the option is enabled.
Similarly, setsockopt requires a nonzero *optval to turn the option on, and a zero value to turn the option off.
Subsequent sections of this chapter will give additional details on the options that affect a socket.
Our union contains one member for each possible return value from getsockopt.
We define function prototypes for four functions that are called to print the value for a given socket option.
Our sock_opts structure contains all the information necessary to call getsockopt for each socket option and then print its current value.
The final member, opt_val_str, is a pointer to one of our four functions that will print the option value.
We allocate and initialize an array of these structures, one element for each socket option.
Figure 7.3 Declarations for our program to check the socket options.
We create a socket on which to try the option.
We call getsockopt but do not terminate if an error is returned.
Many implementations define some of the socket option names even though they do not support the option.
If getsockopt returns success, we call our function to convert the option value to a string and print the string.
In Figure 7.3, we showed four function prototypes, one for each type of option value that is returned.
Recall that the final argument to getsockopt is a value-result argument.
The first check we make is that the size of the value returned by getsockopt is the expected size.
The string returned is off or on, depending on whether the value of the flag option is zero or nonzero, respectively.
Running this program under FreeBSD 4.8 with KAME SCTP patches gives the following output:
Some socket options have timing considerations about when to set or fetch the option versus the state of the socket.
This is important with TCP because the connected socket is not returned to a server by accept until the three-way handshake is completed by the TCP layer.
To ensure that one of these socket options is set for the connected socket when the three-way handshake completes, we must set that option for the listening socket.
We start with a discussion of the generic socket options.
For example, even though the SO_BROADCAST socket option is called "generic," it applies only to datagram sockets.
This option enables or disables the ability of the process to send broadcast messages.
You cannot broadcast on a point-to-point link or any connection-based transport protocol such as SCTP or TCP.
Since an application must set this socket option before sending a broadcast datagram, it prevents a process from sending a broadcast when the application was never designed to broadcast.
For example, a UDP application might take the destination IP address as a command-line argument, but the application never intended for a user to type in a broadcast address.
When enabled for a TCP socket, the kernel keeps track of detailed information about all the packets sent or received by TCP for the socket.
These are kept in a circular buffer within the kernel that can be examined with the trpt program.
This option specifies that outgoing packets are to bypass the normal routing mechanisms of the underlying protocol.
For example, with IPv4, the packet is directed to the appropriate local interface, as specified by the network and subnet portions of the destination address.
If the local interface cannot be determined from the destination address (e.g., the destination is not on the other end of a point-to-point link, or is not on a shared network), ENETUNREACH is returned.
The equivalent of this option can also be applied to individual datagrams using the MSG_DONTROUTE flag with the send, sendto, or sendmsg functions.
This option is often used by routing daemons (e.g., routed and gated) to bypass the routing table and force a packet to be sent out a particular interface.
When an error occurs on a socket, the protocol module in a Berkeley-derived kernel sets a variable named so_error for that socket to one of the standard Unix Exxx values.
The process can be immediately notified of the error in one of two ways:
If the process is blocked in a call to select on the socket (Section 6.3), for either readability or writability, select returns with either or both conditions set.
If the process is using signal-driven I/O (Chapter 25), the SIGIO signal is generated for either the process or the process group.
The integer value returned by getsockopt is the pending error for the socket.
If there is data queued for the socket, that data is returned by read instead of the error condition.
There is a bug in the code shown on p.
This is the first socket option that we have encountered that can be fetched but cannot be set.
When the keep-alive option is set for a TCP socket and no data has been exchanged across the socket in either direction for two hours, TCP automatically sends a keep-alive probe to the peer.
This probe is a TCP segment to which the peer must respond.
The peer responds with an RST, which tells the local TCP that the peer host has crashed and rebooted.
The socket's pending error is set to ECONNRESET and the socket is closed.
There is no response from the peer to the keep-alive probe.
If there is no response at all to TCP's keep-alive probes, the socket's pending error is set to ETIMEDOUT and the socket is closed.
This can occur either because of a network failure or because the remote host has crashed and the last-hop router has detected the crash.
Undoubtedly the most common question regarding this option is whether the timing parameters can be modified (usually to reduce the two-hour period of inactivity to some shorter value)
However, such questions usually result from a misunderstanding of the purpose of this option.
If the peer process crashes, its TCP will send a FIN across the connection, which we can easily detect with select.
Also realize that if there is no response to any of the keep-alive probes (scenario 3), we are not guaranteed that the peer host has crashed, and TCP may well terminate a valid connection.
In fact, this function might more properly be called "make-dead" rather than "keep-alive" since it can terminate live connections.
This option is normally used by servers, although clients can also use the option.
Servers use the option because they spend most of their time blocked waiting for input across the TCP connection, that is, waiting for a client request.
But if the client host's connection drops, is powered off, or crashes, the server process will never know about it, and the server will continually wait for input that can never arrive.
The keep-alive option will detect these half-open connections and terminate them.
Some servers, notably FTP servers, provide an application timeout, often on the order of minutes.
This is done by the application itself, normally around a call to read, reading the next client command.
This is often a better method of eliminating connections to missing clients, since the application has complete control if it implements the timeout itself.
The settings made by SO_KEEPALIVE on a SCTP socket are ignored and do not affect the SCTP heartbeat mechanism.
Figure 7.6 summarizes the various methods that we have to detect when something happens on the other end of a TCP connection.
When we say "using select for readability," we mean calling select to test whether a socket is readable.
By default, close returns immediately, but if there is any data still remaining in the socket send buffer, the system will try to deliver the data to the peer.
This option requires the following structure to be passed between the user process and the kernel.
Calling setsockopt leads to one of the following three scenarios, depending on the values of the two structure members:
The value of l_linger is ignored and the previously discussed TCP default applies: close returns immediately.
That is, TCP discards any data still remaining in the socket send buffer and sends an RST to the peer, not the normal four-packet connection termination sequence (Section 2.6)
We will show an example of this in Figure 16.21
Occasional USENET postings advocate the use of this feature just to avoid the TIME_WAIT state and to be able to restart a listening server even if connections are still in use with the server's well-known port.
Instead, the SO_REUSEADDR socket option should always be used in the server before the call to bind, as we will describe shortly.
The TIME_WAIT state is our friend and is there to help us (i.e., to let old duplicate segments expire in the network)
Instead of trying to avoid the state, we should understand it (Section 2.7)
There are certain circumstances which warrant using this feature to send an abortive close.
That is, if there is any data still remaining in the socket send buffer, the process is put to sleep until either: (i) all the data is sent and acknowledged by the peer TCP, or (ii) the linger time expires.
If the socket has been set to nonblocking (Chapter 16), it will not wait for the close to complete, even if the linger time is nonzero.
When using this feature of the SO_LINGER option, it is important for the application to check the return value from close, because if the linger time expires before the remaining data is sent and acknowledged, close returns EWOULDBLOCK and any remaining data in the send buffer is discarded.
We now need to see exactly when a close on a socket returns given the various scenarios we looked at.
We assume that the client writes data to the socket and then calls close.
We assume that when the client's data arrives, the server is temporarily busy, so the data is added to the socket receive buffer by its TCP.
Similarly, the next segment, the client's FIN, is also added to the socket receive buffer (in whatever manner the implementation records that a FIN has been received on the connection)
As we show in this scenario, the client's close can return before the server reads the remaining data in its socket receive buffer.
Therefore, it is possible for the server host to crash before the server application reads this remaining data, and the client application will never know.
The client can set the SO_LINGER socket option, specifying some positive linger time.
When this occurs, the client's close does not return until all the client's data and its FIN have been acknowledged by the server TCP.
But we still have the same problem as in Figure 7.7: The server host can crash before the server application reads its remaining data, and the client application will never know.
The basic principle here is that a successful return from close, with the SO_LINGER socket option set, only tells us that the data we sent (and our FIN) have been acknowledged by the peer TCP.
This does not tell us whether the peer application has read the data.
If we do not set the SO_LINGER socket option, we do not know whether the peer TCP has acknowledged the data.
One way for the client to know that the server has read its data is to call shutdown (with a second argument of SHUT_WR) instead of close and wait for the peer to close its end of the connection.
Using shutdown to know that peer has received our data.
Another way to know that the peer application has read our data is to use an application-level acknowledgment, or application ACK.
For example, in the following, the client sends its data to the server and then calls read for one byte of data:
The server reads the data from the client and then sends back the one-byte application-level ACK:
We are guaranteed that when the read in the client returns, the server process has read the data we sent.
Figure 7.12 summarizes the two possible calls to shutdown and the three possible calls to close, and the effect on a TCP socket.
When this option is set, out-of-band data will be placed in the normal input queue (i.e., inline)
When this occurs, the MSG_OOB flag to the receive functions cannot be used to read the out-ofband data.
Every socket has a send buffer and a receive buffer.
The receive buffers are used by TCP, UDP, and SCTP to hold received data until it is read by the application.
With TCP, the available room in the socket receive buffer limits the window that TCP can advertise to the other end.
The TCP socket receive buffer cannot overflow because the peer is not allowed to send data beyond the advertised window.
This is TCP's flow control, and if the peer ignores the advertised window and sends data beyond the window, the receiving TCP discards it.
With UDP, however, when a datagram arrives that will not fit in the socket receive buffer, that datagram is discarded.
Recall that UDP has no flow control: It is easy for a fast sender to overwhelm a slower receiver, causing datagrams to be discarded by the receiver's UDP, as we will show in Section 8.13
In fact, a fast sender can overwhelm its own network interface, causing datagrams to be discarded by the sender itself.
These two socket options let us change the default sizes.
When setting the size of the TCP socket receive buffer, the ordering of the function calls is important.
This is because of TCP's window scale option (Section 2.6), which is exchanged with the peer on the SYN segments when the connection is established.
For a client, this means the SO_RCVBUF socket option must be set before calling connect.
For a server, this means the socket option must be set for the listening socket before calling listen.
Setting this option for the connected socket will have no effect whatsoever on the possible window scale option because accept does not return with the connected socket until TCP's three-way handshake is complete.
That is why this option must be set for the listening socket.
The sizes of the socket buffers are always inherited from the listening socket by the newly created connected socket: pp.
The TCP socket buffer sizes should be at least four times the MSS for the connection.
If we are dealing with unidirectional data transfer, such as a file transfer in one direction, when we say "socket buffer sizes," we mean the socket send buffer size on the sending host and the socket receive buffer size on the receiving host.
For bidirectional data transfer, we mean both socket buffer sizes on the sender and both socket buffer sizes on the receiver.
The minimum MSS multiple of four is a result of the way that TCP's fast recovery algorithm works.
The receiver sends a duplicate acknowledgment for each segment it receives after a lost segment.
If the window size is smaller than four segments, there cannot be three duplicate acknowledgments, so the fast recovery algorithm cannot be invoked.
To avoid wasting potential buffer space, the TCP socket buffer sizes should also be an even multiple of the MSS for the connection.
This is another reason to set these two socket options before establishing a connection.
This is not a crucial requirement; the additional space in the socket buffer above the.
Another consideration in setting the socket buffer sizes deals with performance.
Figure 7.13 shows a TCP connection between two endpoints (which we call a pipe) with a capacity of eight segments.
We show four data segments on the top and four ACKs on the bottom.
Even though there are only four segments of data in the pipe, the client must have a send buffer capacity of at least eight segments, because the client TCP must keep a copy of each segment until the ACK is received from the server.
First, TCP's slow-start algorithm limits the rate at which segments are initially sent on an idle connection.
Next, TCP often acknowledges every other segment, not every segment as we show.
What is important to understand is the concept of the full-duplex pipe, its capacity, and how that relates to the socket buffer sizes on both ends of the connection.
The capacity of the pipe is called the bandwidth-delay product and we calculate this by multiplying the bandwidth (in bits/sec) times the RTT (in seconds), converting the result from bits to bytes.
The bandwidth is the value corresponding to the slowest link between two endpoints and must somehow be known.
If the socket buffer sizes are less than this, the pipe will not stay full, and the performance will be less than expected.
Most implementations have an upper limit for the sizes of the socket send and receive buffers, and sometimes this limit can be modified by the administrator.
Unfortunately, there is no simple way for an application to determine this limit.
Alternately, an application can try setting the socket buffers to the desired value, and if that fails, cut the value in half and try again until it succeeds.
Finally, an application should make sure that it's not actually making the socket buffer smaller when it sets it to a preconfigured "large" value; calling getsockopt first to retrieve the system's default and seeing if that's large enough is often a good start.
Every socket also has a receive low-water mark and a send low-water mark.
The receive low-water mark is the amount of data that must be in the socket receive buffer for select to return "readable." It defaults to 1 for TCP, UDP, and SCTP sockets.
The send low-water mark is the amount of available space that must exist in the socket send buffer for select to return "writable." This low-water mark normally defaults to 2,048 for TCP sockets.
With UDP, the low-water mark is used, as we described in Section 6.3, but since the number of bytes of available space in the send buffer for a UDP socket never changes (since UDP does not keep a copy of the datagrams sent by the application), as long as the UDP socket send buffer size is greater than the socket's low-water mark, the UDP socket is always writable.
Recall from Figure 2.16 that UDP does not have a send buffer; it has only a send buffer size.
These two socket options allow us to place a timeout on socket receives and sends.
Notice that the argument to the two sockopt functions is a pointer to a timeval structure, the same one used with select (Section 6.3)
This lets us specify the timeouts in seconds and microseconds.
The receive timeout affects the five input functions: read, readv, recdv, recvfrom, and recvmsg.
The send timeout affects the five output functions: write, writev, send, sendto, and sendmsg.
We will talk more about socket timeouts in Section 14.2
These two socket options and the concept of inherent timeouts on socket receives and sends were added with 4.3BSD Reno.
In Berkeley-derived implementations, these two values really implement an inactivity timer and not an absolute timer on the read or write system call.
SO_REUSEADDR allows a listening server to start and bind its well-known port, even if previously established connections exist that use this port as their local port.
A connection request arrives and a child process is spawned to handle that client.b.
The listening server terminates, but the child continues to service the client on the existing connection.
By default, when the listening server is restarted in (d) by calling socket, bind, and listen, the call to bind fails because the listening server is trying to bind a port that is part of an existing connection (the one being handled by the previously spawned child)
But if the server sets the SO_REUSEADDR socket option between the calls to socket and bind, the latter function will succeed.
All TCP servers should specify this socket option to allow the server to be restarted in this situation.
This scenario is one of the most frequently asked questions on USENET.
SO_REUSEADDR allows a new server to be started on the same port as an existing server that is bound to the wildcard address, as long as each instance binds a different local IP address.
This is common for a site hosting multiple HTTP servers using the IP alias technique (Section A.4)
The first HTTP server would call bind with the wildcard as the local IP address and a local port of 80 (the wellknown port for HTTP)
But, this second call to bind fails unless SO_REUSEADDR is set before the call.
Again, SO_REUSEADDR is required for this final call to succeed.
This "default" server handles requests destined for 198.69.10.2 in addition to any other IP aliases that the host may have configured.
The wildcard means "everything that doesn't have a better (more specific) match." Note that this scenario of allowing multiple servers for a given service is handled automatically if the server always sets the SO_REUSEADDR socket option (as we recommend)
With TCP, we are never able to start multiple servers that bind the same IP address and the same port: a completely duplicate binding.
For security reasons, some operating systems prevent any "more specific" bind to a port that is already bound to the wildcard address, that is, the series of binds described here would not work with or without SO_REUSEADDR.
On such a system, the server that performs the wildcard bind must be started last.
This is to avoid the problem of a rogue server binding to an IP address and port that are being served already by a system service and intercepting legitimate requests.
This is a particular problem for NFS, which generally does not use a privileged port.
SO_REUSEADDR allows a single process to bind the same port to multiple sockets, as long as each bind specifies a different local IP address.
This is common for UDP servers that need to know the destination IP address of client requests on systems that do not provide the IP_RECVDSTADDR socket option.
This technique is normally not used with TCP servers since a TCP server can always determine the destination IP address by calling getsockname after the connection is established.
However, a TCP server wishing to serve connections to some, but not all, addresses belonging to a multihomed host should use this technique.
SO_REUSEADDR allows completely duplicate bindings: a bind of an IP address and port, when that same IP address and port are already bound to another socket, if the transport protocol supports it.
This feature is used with multicasting to allow the same application to be run multiple times on the same host.
When a UDP datagram is received for one of these multiply bound sockets, the rule is that if the datagram is destined for either a broadcast address or a multicast address, one copy of the datagram is delivered to each matching socket.
But if the datagram is destined for a unicast address, the datagram is delivered to only one socket.
If, in the case of a unicast datagram, there are multiple sockets that match the datagram, the.
Instead of overloading SO_REUSEADDR with the desired multicast semantics that allow completely duplicate bindings, this new socket option was introduced with the following semantics:
This option allows completely duplicate bindings, but only if each socket that wants to bind the same IP address and port specify this socket option.
The problem with this socket option is that not all systems support it, and on those that do not support the option but do support multicasting, SO_REUSEADDR is used instead of SO_REUSEPORT to allow completely duplicate bindings when it makes sense (i.e., a UDP server that can be run multiple times on the same host at the same time and that expects to receive either broadcast or multicast datagrams)
We can summarize our discussion of these socket options with the following recommendations:
When writing a multicast application that can be run multiple times on the same host at the same time, set the SO_REUSEADDR socket option and bind the group's multicast address as the local IP address.
Any future datagrams that arrive destined to port 5555 and the IP address that we bound to our socket are delivered to our socket, not to the other socket bound to the wildcard address.
For most wellknown services, HTTP, FTP, and Telnet, for example, this is not a problem because these servers all bind a reserved port.
Hence, any process that comes along later and tries to bind a more specific instance of that port (i.e., steal the port) requires superuser privileges.
NFS, however, can be a problem since its normal port (2049) is not reserved.
One underlying problem with the sockets API is that the setting of the socket pair is done with two function calls (bind and connect) instead of one.
With such a function, the need for SO_REUSEADDR disappears, other than for multicast UDP servers that explicitly need to allow completely duplicate bindings of the same IP address and port.
This option is typically used by a process that inherits a socket when it is started.
This option applies only to sockets in the routing domain (AF_ROUTE)
This option defaults to ON for these sockets (the only one of the SO_xxx socket options that defaults to ON instead of OFF)
When this option is enabled, the socket receives a copy of everything sent on the socket.
Another way to disable these loopback copies is to call shutdown with a second argument of SHUT_RD.
We defer discussion of the multicasting socket options until Section 21.6
If this option is set for a raw IP socket (Chapter 28), we must build our own IP header for all the datagrams we send on the raw socket.
Normally, the kernel builds the IP header for datagrams sent on a raw socket, but there are some applications (notably traceroute) that build their own IP header to override values that IP would place into certain header fields.
When this option is set, we build a complete IP header, with the following exceptions:
If we set the IP identification field to 0, the kernel will set the field.
If the source IP address is INADDR_ANY, IP sets it to the primary IP address of the outgoing interface.
Some implementations take any IP options that were set using the IP_OPTIONS socket option and append these to the header that we build, while others require our header to also contain any desired IP options.
Some fields must be in host byte order, and some in network byte order.
We show an example of this option in Section 29.7
Setting this option allows us to set IP options in the IPv4 header.
This requires intimate knowledge of the format of the IP options in the IP header.
This socket option causes the destination IP address of a received UDP datagram to be returned as ancillary data by recvmsg.
We will show an example of this option in Section 22.2
This socket option causes the index of the interface on which a UDP datagram is received to be returned as ancillary data by recvmsg.
We will show an example of this option in Section 22.2
This option lets us set the type-of-service (TOS) field (which contains the DSCP and ECN fields, Figure A.1) in the IP header for a TCP, UDP, or SCTP socket.
If we call getsockopt for this option, the current value that would be placed into the DSCP and ECN fields in the IP header (which defaults to 0) is returned.
There is no way to fetch the value from a received IP datagram.
Applications should generally leave the setting of the ECN field to the kernel, and should specify zero values in the low two bits of the value set with IP_TOS.
With this option, we can set and fetch the default TTL (Figure A.1) that the system will use for unicast packets sent on a given socket.
As with the TOS field, calling getsockopt returns the default value of the field that the system will use in outgoing datagrams—there is no way to obtain the value from a received datagram.
We will set this socket option with our traceroute program in Figure 28.19
We defer discussion of the multicasting socket options until Section 21.6
We note that many of these options make use of ancillary data with the recvmsg function, and we will describe this in Section 14.6
This socket option specifies the byte offset into the user data where the checksum field is located.
If this value is non-negative, the kernel will: (i) compute and store a checksum for all outgoing packets, and (ii) verify the received checksum on input, discarding packets with an invalid checksum.
The kernel always calculates and stores the checksum for ICMPv6 raw sockets.
If a value of -1 is specified (the default), the kernel will not calculate and store the checksum for outgoing packets on this raw socket and will not verify the checksum for received packets.
All protocols that use IPv6 should have a checksum in their own protocol header.
Rather than forcing the application using the raw socket to perform source address selection, the kernel will do this and then calculate and store the checksum incorporating the standard IPv6 pseudoheader.
Setting this option disables the automatic insertion of a fragment header for UDP and raw sockets.
When this option is set, output packets larger than the MTU of the outgoing interface will be dropped.
No error needs to be returned from the system call that sends the packet, since the packet might exceed the path MTU en-route.
This option specifies the next-hop address for a datagram as a socket address structure, and is a privileged operation.
We will say more about this feature in Section 22.8
When this option is retrieved, the current MTU as determined by path-MTU discovery is returned (see Section 22.9)
Setting this option specifies that any received IPv6 destination options are to be returned as ancillary data by recvmsg.
We will describe the functions that are used to build and process these options in Section 27.5
Setting this option specifies that the received hop limit field is to be returned as ancillary data by recvmsg.
There is no way with IPv4 to obtain the received TTL field.
Setting this option specifies that any received IPv6 hop-by-hop options are to be returned as ancillary data by recvmsg.
We will describe the functions that are used to build and process these options in Section 27.5
Setting this option specifies that the path MTU of a path is to be returned as ancillary data by recvmsg (without any accompanying data) when it changes.
Setting this option specifies that a received IPv6 routing header is to be returned as ancillary data by recvmsg.
Setting this option specifies that the received traffic class (containing the DSCP and ECN fields) is to be returned as ancillary data by recvmsg.
Setting the socket option specifies the default hop limit for outgoing datagrams sent on the socket, while fetching the socket option returns the value for the hop limit that the kernel will use for the socket.
We will set this socket option with our traceroute program in Figure 28.19
Setting it to 0 causes path MTU discovery to occur for all destinations.
Setting it to–1 specifies that path MTU discovery is performed for unicast destinations but the minimum MTU is used when sending to multicast destinations.
This option defaults to OFF, although some systems have an option to turn it ON by default.
Most of the IPv6 options for header modification assume a UDP socket with information being passed between the kernel and the application using ancillary data with recvmsg and sendmsg.
A TCP socket fetches and stores these values using getsockopt and setsockopt instead.
The socket option is the same as the type of the ancillary data, and the buffer contains the same information as would be present in the ancillary data.
This socket option allows us to fetch or set the MSS for a TCP connection.
The value returned is the maximum amount of data that our TCP will send to the other end; often, it is the MSS announced by the other end with its SYN, unless our TCP chooses to use a smaller value than the peer's announced MSS.
If this value is fetched before the socket is connected, the value returned is the default value that will be used if an MSS option is not received from the other end.
Also be aware that a value smaller than the returned value can actually be used for the connection if the timestamp option, for example, is in use, because this option occupies 12 bytes of TCP options in each segment.
The maximum amount of data that our TCP will send per segment can also change during the life of a connection if TCP supports path MTU discovery.
If the route to the peer changes, this value can go up or down.
We note in Figure 7.1 that this socket option can also be set by the application.
This is not possible on all systems; it was originally a read-only option.
Since this option controls the amount of data that TCP sends per segment, it makes sense to forbid the application from increasing the value.
Once the connection is established, this value is the MSS option announced by the peer, and we cannot exceed that value.
Our TCP, however, can always send less than the peer's announced MSS.
The purpose of the Nagle algorithm is to reduce the number of small packets on a WAN.
The algorithm states that if a given connection has outstanding data (i.e., data that our TCP has sent, and for which it is currently awaiting an acknowledgment), then no small packets will be sent on the connection in response to a user write operation until the existing data is acknowledged.
The definition of a "small" packet is any packet smaller than the MSS.
The two common generators of small packets are the Rlogin and Telnet clients, since they normally send each keystroke as a separate packet.
On a fast LAN, we normally do not notice the Nagle algorithm with these clients, because the time required for a small packet to be acknowledged is typically a few milliseconds—far less than the time between two successive characters that we type.
But on a WAN, where it can take a second for a small packet to be acknowledged, we can notice a delay in the character echoing, and this delay is often exaggerated by the Nagle algorithm.
Consider the following example: We type the six-character string "hello!" to either an Rlogin or Telnet client, with exactly 250 ms between each character.
The RTT to the server is 600 ms and the server immediately sends back the echo of each character.
We assume the ACK of the client's character is sent back to the client along with the character echo and we ignore the ACKs that the client sends for the server's echo.
Each character is sent in a packet by itself: the data segments from left to right, and the ACKs from right to left.
If the Nagle algorithm is enabled (the default), we have the eight packets shown in Figure 7.15
The first character is sent as a packet by itself, but the next two characters are not sent, since the connection has a small packet outstanding.
At time 600, when the ACK of the first packet is received, along with the echo of the first character, these two characters are sent.
Until this packet is ACKed at time 1200, no more small packets are sent.
The Nagle algorithm often interacts with another TCP algorithm: the delayed ACK algorithm.
This algorithm causes TCP to not send an ACK immediately when it receives data; instead, TCP will wait some small amount of time (typically 50–200 ms) and only then send the ACK.
The hope is that in this small amount of time, there will be data to send back to the peer, and the ACK can piggyback with the data, saving one TCP segment.
This is normally the case with the Rlogin and Telnet clients, because the servers typically echo each character sent by the client, so the ACK of the client's character piggybacks with the server's echo of that character.
The problem is with other clients whose servers do not generate traffic in the reverse direction on which ACKs can piggyback.
These clients can detect noticeable delays because the client TCP will not send any data to the server until the server's delayed ACK timer expires.
These clients need a way to disable the Nagle algorithm, hence the TCP_NODELAY option.
Another type of client that interacts badly with the Nagle algorithm and TCP's delayed ACKs is a client that sends a single logical request to its server in small pieces.
There are three ways to fix this type of client:
Use writev (Section 14.4) instead of two calls to write.
A single call to writev ends up with one call to TCP output instead of two calls, resulting in one TCP segment for our example.
Set the TCP_NODELAY socket option and continue to call write two times.
This is the least desirable solution, and is harmful to the network, so it generally should not even be considered.
The relatively large number of socket options for SCTP (17 at present writing) reflects the finer grain of control SCTP provides to the application developer.
Several options used to get information about SCTP require that data be passed into the kernel (e.g., association ID and/or peer address)
While some implementations of getsockopt support passing data both into and out of the kernel, not all do.
On systems on which getsockopt does support this, it is simply a wrapper around getsockopt.
Otherwise, it performs the required action, perhaps using a custom ioctl or a new system call.
We recommend always using sctp_opt_info when retrieving these options for maximum portability.
During association initialization, either endpoint may specify an adaption layer indication.
This indication is a 32-bit unsigned integer that can be used by the two applications to coordinate any local application adaption layer.
This option allows the caller to fetch or set the adaption layer indication that this endpoint will provide to peers.
When fetching this value, the caller will only retrieve the value the local socket will provide to all future peers.
To retrieve the peer's adaption layer indication, an application must subscribe to adaption layer events.
The SCTP_ASSOCINFO socket option can be used for three purposes: (i) to retrieve information about an existing association, (ii) to change the parameters of an existing association, and/or (iii) to set defaults for future associations.
When retrieving information about an existing association, the sctp_opt_info function should be used instead of getsockopt.
This value represents the total number of data bytes that can yet be sent.
This field is dynamic; as the local endpoint sends data, this value decreases.
As the remote application reads data that has been received, this value increases.
This value cannot be changed by this socket option call.
This value is dynamic as well and is influenced by the SO_SNDBUF socket option.
This value cannot be changed by this socket option call.
Each state cookie sent to a peer has a lifetime associated with it to prevent replay attacks.
The sasoc_cookie_life can be reduced for greater protection against cookie replay attacks but less robustness to network delay during association initiation.
This option allows us to fetch or set the autoclose time for an SCTP endpoint.
The autoclose time is the number of seconds an SCTP association will remain open when idle.
Idle is defined by the SCTP stack as neither endpoint sending or receiving user data.
The default is for the autoclose function to be disabled.
The autoclose option is intended to be used in the one-to-many-style SCTP interface (Chapter 9)
When this option is set, the integer passed to the option is the number of seconds before an idle connection should be closed; a value of 0 disables autoclose.
Only future associations created by this endpoint will be affected by this option; existing associations retain their current setting.
Autoclose can be used by a server to force the closing of idle associations without the server needing to maintain additional state.
A server using this feature needs to carefully assess the longest idle time expected on all its associations.
Setting the autoclose value smaller than needed results in the premature closing of associations.
An application that wishes to send a large number of messages, all with the same parameters, can use this option to set up the default parameters and thus avoid using ancillary data or the sctp_sendmsg call.
When receiving a message with the recvmsg function or sctp_recvmsg function, this field will hold the value the peer placed in the stream sequence number (SSN) field in the SCTP DATA chunk.
The lifetime field is used by SCTP stacks to know when to discard an outgoing message due.
If the two endpoints support the partial reliability option, then the lifetime is also used to specify how long a message is valid after its first transmission.
When receiving a message with the recvmsg function or sctp_recvmsg function, this field will hold the value the peer placed in the transport sequence number (TSN) field in the SCTP DATA chunk.
When receiving a message with the recvmsg function or sctp_recvmsg function, this field will hold the current cumulative TSN the local SCTP stack has associated with its remote peer.
Note that all default settings will only affect messages sent without their own sctp_sndrcvinfo structure.
Any send that provides this structure (e.g., sctp_sendmsg or sendmsg function with ancillary data) will override the default settings.
Besides setting the default values, this option may be used to retrieve the current default parameters by using the sctp_opt_info function.
When disabled by this option, SCTP will return the error EMSGSIZE and not send the message.
The default behavior is for this option to be disabled; SCTP will normally fragment user messages.
This option may be used by applications that wish to control message sizes, ensuring that every user application message will fit in a single IP packet.
An application that enables this option must be prepared to handle the error case (i.e., its message was too big) by either providing application-layer fragmentation of the message or a smaller message.
This socket option allows a caller to fetch, enable, or disable various SCTP notifications.
An SCTP notification is a message that the SCTP stack will send to the application.
An application that is not prepared to use either recvmsg or sctp_recvmsg should not enable events.
Further details on notifications can be found in Section 9.14
This option retrieves information about a peer address, including the congestion window, smoothed RTT and MTU.
This option may only be used to retrieve information about a specific peer address.
This unique value can be used as a shorthand method to represent the association for almost all SCTP operations.
An unconfirmed address is one that the peer had listed as a valid address, but the local SCTP endpoint has not been able to confirm that the peer holds that address.
An SCTP endpoint confirms an address when a heartbeat or user data, sent to that address, is acknowledged.
Note that an unconfirmed address will also not have a valid retransmission timeout (RTO) value.
Active addresses represent addresses that are considered available for use.
One interesting use for this option is to translate an IP address structure into an association identification that can be used in other calls.
Another possibility is for the application to track performance to each address of a multihomed peer and update the primary address of the association to the peer's best address.
This option can be used to get or set the default initial parameters used on an SCTP socket when sending out the INIT message.
The option uses the sctp_initmsg structure, which is defined as:
This value is not confirmed until after the association finishes the initial handshake, and may be negotiated downward via peer endpoint limitations.
This value will be overridden by the SCTP stack if it is greater than the maximum allowable streams the SCTP stack supports.
During exponential backoff of the initial timer, this value replaces RTO.max as the ceiling for retransmissions.
Note that when setting these fields, any value set to 0 will be ignored by the SCTP socket.
This socket option allows the application to fetch or set the maximum burst size used when sending packets.
When an SCTP implementation sends data to a peer, no more than SCTP_MAXBURST packets are sent at once to avoid flooding the network with packets.
An implementation may apply this limit by either: (i) reducing its congestion window to the current flight size plus the maximum burst size times the path MTU, or (ii) using this value as a separate micro-control, sending at most maximum burst packets at any single send opportunity.
This socket option allows the application to fetch or set the maximum fragment size used during SCTP fragmentation.
When an SCTP sender receives a message from an application that is larger than this value, the SCTP sender will break the message into multiple pieces for transport to the peer endpoint.
The size that the SCTP sender normally uses is the smallest MTU of all addresses associated with the peer.
This option overrides this value downward to the value specified.
Note that the SCTP stack may fragment a message at a smaller boundary than requested by this option.
This smaller fragmentation will occur when one of the paths to the peer endpoint has a smaller MTU than the value requested in the SCTP_MAXSEG option.
This value is an endpoint-wide setting and may affect more than one association in the one-tomany interface style.
This option is OFF by default (i.e., the Nagle algorithm is ON by default)
SCTP's Nagle algorithm works identically to TCP's except that it is trying to coalesce multiple DATA chunks as opposed to simply coalescing bytes on a stream.
For a further discussion of the Nagle algorithm, see TCP_MAXSEG.
This socket option allows an application to fetch or set various parameters on an association.
The caller provides the sctp_paddrparams structure, filling in the association identification.
Any other value changes the heartbeat interval to this value in milliseconds.
When setting the default parameters, the value of SCTP_ISSUE_HB is not allowed.
When an address is declared INACTIVE, if it is the primary address, an alternate address will be chosen as the primary.
This socket option fetches or sets the address that the local endpoint is using as primary.
The primary address is used, by default, as the destination address for all messages sent to a peer.
To set this value, the caller fills in the association identification and the peer's address that should be used as the primary address.
The caller passes this information in a sctp_setprim structure, which is defined as:
If the operation is a setsockopt function call, then the value in this field represents the new peer address the requester would like to be made into the primary destination address.
Note that retrieving the value of this option on a one-to-one socket that has only one local address associated with it is the same as calling getsockname.
This socket option can be used to fetch or set various RTO information on a specific association or the default values used by this endpoint.
When fetching, the caller should use sctp_opt_info instead of getsockopt for maximum portability.
The caller provides a sctp_rtoinfo structure of the following form:
If this field contains the value 0, then the system's default parameters are affected by the call.
The initial RTO is used when sending an INIT chunk to the peer.
This value is in milliseconds and has a default value of 3,000
If the updated value is larger than the RTO maximum, then the RTO maximum is used as the RTO instead of the calculated value.
Anytime an update is made to the RTO timer, the RTO minimum value is checked against the new value.
If the new value is smaller than the minimum, the minimum replaces the new value.
We provide guidance on setting these timers for performance in Section 23.11
Setting this option causes a message to be sent that requests that the peer set the specified local address as its primary address.
The caller provides an sctp_setpeerprim structure and must fill in both the association identification and a local address to request the peer mark as its primary.
The address provided must be one of the local endpoint's bound addresses.
This feature is optional, and must be supported by both endpoints to operate.
If the local endpoint does not support the feature, an error of EOPNOTSUPP will be returned to the caller.
If the remote endpoint does not support the feature, an error of EINVAL will be returned to the caller.
Note that this value may only be set and cannot be retrieved.
This socket option will retrieve the current state of an SCTP association.
The caller should use sctp_opt_info instead of getaddrinfo for maximum portability.
A detailed depiction of the states an SCTP endpoint goes through during association setup or shutdown can be found in Figure 2.8
The primary address is the default address used when sending data to the peer endpoint.
Before describing the function and how it affects a socket, we need to look at the bigger picture.
Figure 7.20 summarizes the different operations performed by fcntl, ioctl, and routing sockets.
The first six operations can be applied to sockets by any process; the second two (interface operations) are less common, but are still general-purpose; and the last two (ARP and routing table) are issued by administrative programs such as ifconfig and route.
There are multiple ways to perform the first four operations, but we note in the final column that POSIX specifies that fcntl is the preferred way.
We also note that POSIX provides the sockatmark function (Section 24.3) as the preferred way to test for the out-of-band mark.
The remaining operations, with a blank final column, have not been standardized by POSIX.
We also note that the first two operations, setting a socket for nonblocking I/O and for signal-driven I/O, have been set historically using the FNDELAY and FASYNC commands with fcntl.
The fcntl function provides the following features related to network programming:
The F_SETOWN command lets us set the socket owner (the process ID or process group ID) to receive the SIGIO and SIGURG signals.
The former signal is generated when signal-driven I/O is enabled for a socket (Chapter 25) and the latter signal is generated when new out-ofband data arrives for a socket.
The F_GETOWN command returns the current owner of the socket.
We will describe both of these features in more detail later.
For now, we note that typical code to enable nonblocking I/O, using fcntl, would be:
Beware of code that you may encounter that simply sets the desired flag.
While this sets the nonblocking flag, it also clears all the other file status flags.
The only correct way to set one of the file status flags is to fetch the current flags, logically OR in the new flag, and then set the flags.
The following code turns off the nonblocking flag, assuming flags was set by the call to fcntl shown above:
The two signals SIGIO and SIGURG are different from other signals in that they are generated for a socket only if the socket has been assigned an owner with the F_SETOWN command.
The integer arg value for the F_SETOWN command can be either a positive integer, specifying the process ID to receive the signal, or a negative integer whose absolute value is the process group ID to receive the signal.
The difference between specifying a process or a process group to receive the signal is that the former causes only a single process to receive the signal, while the latter causes all processes in the process group (perhaps more than one) to receive the signal.
When a new socket is created by socket, it has no owner.
Socket options run the gamut from the very general (SO_ERROR) to the very specific (IP header options)
The latter should always be set for a TCP server before it calls bind (Figure 11.12)
The SO_KEEPALIVE socket option is set by many TCP servers and automatically terminates a halfopen connection.
The nice feature of this option is that it is handled by the TCP layer, without requiring an application-level inactivity timer; its downside is that it cannot tell the difference between a crashed client host and a temporary loss of connectivity to the client.
The SO_LINGER socket option gives us more control over when close returns and also lets us force an RST to be sent instead of TCP's four-packet connection termination sequence.
We must be careful sending RSTs, because this avoids TCP's TIME_WAIT state.
Much of the time, this socket option does not provide the information that we need, in which case, an application-level ACK is required.
Every TCP and SCTP socket has a send buffer and a receive buffer, and every UDP socket has a receive buffer.
The most common use of these options is for bulk data transfer across long fat pipes: TCP connections with either a high bandwidth or a long delay, often using the RFC 1323 extensions.
After connect returns success, fetch these same two socket options and print their values.
Have the values changed? Why? Run the program connecting to a server on your local network and also run the program connecting to a server on a remote network.
Does the MSS change? Why? You should also run the program on any different hosts to which you have access.
Type in a line or two at the client to verify the operation, and then terminate the client by entering your EOF character.
What happens? After you terminate the client, run netstat on the client host and see if the socket goes through the TIME_WAIT state.
Try to start multiple instances of the sock program as a TCP server (-s command-line option) on the same port, binding the wildcard address, one of your host's interface addresses, and the loopback address.
Do you need to specify the SO_REUSEADDR option (the -A command-line option)? Use netstat to see the listening sockets.
If your implementation supports SO_REUSEPORT, try using it (-T command-line option)
Draw a timeline that shows the interaction of the Nagle algorithm with delayed ACKs.
Connectivity is maintained between the two peers, but there is no application data exchanged across the connection.
When the keep-alive timer expires every two hours, how many TCP segments are exchanged across the connection?
Read [Lanciani 1996] to find out why this option exists.
There are some fundamental differences between applications written using TCP versus those that use UDP.
Nevertheless, there are instances when it makes sense to use UDP instead of TCP, and we will go over this design choice in Section 22.4
Some popular applications are built using UDP: DNS, NFS, and SNMP, for example.
Figure 8.1 shows the function calls for a typical UDP client/server.
The client does not establish a connection with the server.
Instead, the client just sends a datagram to the server using the sendto function (described in the next section), which requires the address of the destination (the server) as a parameter.
Similarly, the server does not accept a connection from a client.
Instead, the server just calls the recvfrom function, which waits until data arrives from some client.
Figure 8.1 shows a timeline of the typical scenario that takes place for a UDP client/server exchange.
We can compare this to the typical TCP exchange that was shown in Figure 4.1
In this chapter, we will describe the new functions that we use with UDP sockets, recvfrom and.
We will also describe the use of the connect function with a UDP socket, and the concept of asynchronous errors.
These two functions are similar to the standard read and write functions, but three additional arguments are required.
Both return: number of bytes read or written if OK, –1 on error.
The first three arguments, sockfd, buff, and nbytes, are identical to the first three arguments for read and write: descriptor, pointer to buffer to read into or write from, and number of bytes to read or write.
We will describe the flags argument in Chapter 14 when we discuss the recv, send, recvmsg, and sendmsg functions, since we do not need them with our simple UDP client/server example in this chapter.
The to argument for sendto is a socket address structure containing the protocol address (e.g., IP address and port number) of where the data is to be sent.
The size of this socket address structure is specified by addrlen.
The recvfrom function fills in the socket address structure pointed to by from with the protocol address of who sent the datagram.
The number of bytes stored in this socket address structure is also returned to the caller in the integer pointed to by addrlen.
Note that the final argument to sendto is an integer value, while the final argument to recvfrom is a pointer to an integer value (a value-result argument)
The final two arguments to recvfrom are similar to the final two arguments to accept: The contents of the socket address structure upon return tell us who sent the datagram (in the case of UDP) or who initiated the connection (in the case of TCP)
The final two arguments to sendto are similar to the final two arguments to connect: We fill in the socket address structure with the protocol address of where to send the datagram (in the case of UDP) or with whom to establish a connection (in the case of TCP)
Both functions return the length of the data that was read or written as the value of the function.
In the typical use of recvfrom, with a datagram protocol, the return value is the amount of user data in the datagram received.
Since UDP is connectionless, there is no such thing as closing a UDP connection.
If the from argument to recvfrom is a null pointer, then the corresponding length argument (addrlen) must also be a null pointer, and this indicates that we are not interested in knowing the protocol address of who sent us data.
Both recvfrom and sendto can be used with TCP, although there is normally no reason to do so.
We will now redo our simple echo client/server from Chapter 5 using UDP.
Our UDP client and server programs follow the function call flow that we diagrammed in Figure 8.1
This function is a simple loop that reads the next datagram arriving at the server's port using recvfrom and sends it back using sendto.
Despite the simplicity of this function, there are numerous details to consider.
Since UDP is a connectionless protocol, there is nothing like an EOF as we have with TCP.
Next, this function provides an iterative server, not a concurrent server as we had with TCP.
There is no call to fork, so a single server process handles any and all clients.
In general, most TCP servers are concurrent and most UDP servers are iterative.
There is implied queuing taking place in the UDP layer for this socket.
Indeed, each UDP socket has a receive buffer and each datagram that arrives for this socket is placed in that socket receive buffer.
When the process calls recvfrom, the next datagram from the buffer is returned to the process in a first-in, first-out (FIFO) order.
This way, if multiple datagrams arrive for the socket before the process can read what's already queued for the socket, the arriving datagrams are just added to the socket receive buffer.
There are two connected sockets and each of the two connected sockets on the server host has its own socket receive buffer.
Figure 8.6 shows the scenario when two clients send datagrams to our UDP server.
There is only one server process and it has a single socket on which it receives all arriving datagrams and sends all responses.
That socket has a receive buffer into which all arriving datagrams are placed.
The function dg_echo never looks inside this protocol-dependent structure: It simply passes a pointer to the structure to recvfrom and sendto.
The UDP client main function is shown in Figure 8.7
An IPv4 socket address structure is filled in with the IP address and port number of the server.
This structure will be passed to dg_cli, specifying where to send datagrams.
A UDP socket is created and the function dg_cli is called.
There are four steps in the client processing loop: read a line from standard input using fgets, send the line to the server using sendto, read back the server's echo using recvfrom, and print the echoed line to standard output using fputs.
Our client has not asked the kernel to assign an ephemeral port to its socket.
With a TCP client, we said the call to connect is where this takes place.
With a UDP socket, the first time the process calls sendto, if the socket has not yet had a local port bound to it, that is when an ephemeral port is chosen by the kernel for the socket.
As with TCP, the client can call bind explicitly, but this is rarely done.
Notice that the call to recvfrom specifies a null pointer as the fifth and sixth arguments.
This tells the kernel that we are not interested in knowing who sent the reply.
There is a risk that any process, on either the same host or some other host, can send a datagram to the client's IP address and port, and that datagram will be read by the client, who will think it is the server's reply.
The main function allocates and initializes a socket address structure of some protocol type and then passes a pointer to this structure, along with its size, to dg_cli.
If a client datagram is lost (say it is discarded by some router between the client and server), the client will block forever in its call to recvfrom in the function dg_cli, waiting for a server reply that will never arrive.
Similarly, if the client datagram arrives at the server but the server's reply is lost, the client will again block forever in its call to recvfrom.
A typical way to prevent this is to place a timeout on the client's call to recvfrom.
Just placing a timeout on the recvfrom is not the entire solution.
For example, if we do time out, we cannot tell whether our datagram never made it to the server, or if the server's reply never made it back.
If the client's request was something like "transfer a certain amount of money from account A to account B" (instead of our simple echo server), it would make a big difference as to whether the request was lost or the reply was lost.
We will talk more about adding reliability to a UDP client/server in Section 22.5
At the end of Section 8.6, we mentioned that any process that knows the client's ephemeral port number could send datagrams to our client, and these would be intermixed with the normal server replies.
What we can do is change the call to recvfrom in Figure 8.8 to return the IP address and port of who sent the reply and ignore any received datagrams that are not from the server to whom we sent the datagram.
There are a few pitfalls with this, however, as we will see.
We do this so we can use any host running the standard echo server with our client.
We then recode the dg_cli function to allocate another socket address structure to hold the structure returned by recvfrom.
In the call to recvfrom, we tell the kernel to return the address of the sender of the datagram.
We first compare the length returned by recvfrom in the value-result argument and then compare the socket address structures themselves using memcmp.
Section 3.2 says that even if the socket address structure contains a length field, we need never set it or examine it.
However, memcmp compares every byte of data in the two socket address structures, and the length field is set in the socket address structure that the kernel returns; so in this case we must set it when constructing the sockaddr.
This new version of our client works fine if the server is on a host with just a single IP address.
But this program can fail if the server is multihomed.
We run this program to our host freebsd4, which has two interfaces and two IP addresses.
We specified the IP address that does not share the same subnet as the client.
If a system implemented what this RFC calls the strong end system model, it would accept an arriving datagram only if that datagram arrived on the interface to which it was addressed.
The IP address returned by recvfrom (the source IP address of the UDP datagram) is not the IP address to which we sent the datagram.
When the server sends its reply, the destination IP address is 172.24.37.78
Since the server has not bound an IP address to its socket (the server has bound the wildcard address to its socket, which is something we can verify by running netstat on freebsd), the kernel chooses the source address for the IP datagram.
Also, since it is the primary IP address of the interface, if we send our datagram to a nonprimary IP address of the interface (i.e., an alias), this will also cause our test in Figure 8.9 to fail.
One solution is for the client to verify the responding host's domain name instead of its IP address by looking up the server's name in the DNS (Chapter 11), given the IP address returned by recvfrom.
Another solution is for the UDP server to create one socket for every IP address that is configured on the host, bind that IP address to the socket, use select across all these sockets (waiting for any one to become readable), and then reply from the socket that is readable.
Since the socket used for the reply was bound to the IP address that was the destination address of the client's request (or the datagram would not have been delivered to the socket), this guaranteed that the source address of the reply was the same as the destination address of the request.
We will show an example of this in Section 22.6
On a multihomed Solaris system, the source IP address for the server's reply is the destination IP address of the client's request.
The scenario described in this section is for Berkeley-derived implementations that choose the source IP address based on the outgoing interface.
The next scenario to examine is starting the client without starting the server.
If we do so and type in a single line to the client, nothing happens.
The client blocks forever in its call to recvfrom , waiting for a server reply that will never appear.
But, this is an example where we need to understand more about the underlying protocols to understand what is happening to our networking application.
First we start tcpdump on the host macosx , and then we start the client on the same host, specifying the host freebsd4 as the server host.
We then type a single line, but the line is not echoed.
Figure 8.10 tcpdump output when server process not started on server host.
First we notice that an ARP request and reply are needed before the client host can send the UDP datagram to the server host.
We left this exchange in the output to reiterate the potential for an ARP request-reply before an IP datagram can be sent to another host or router on the local network.
This ICMP error, however, is not returned to the client process, for reasons that we will describe shortly.
Instead, the client blocks forever in the call to recvfrom in Figure 8.8
The error was caused by sendto , but sendto returned successfully.
Recall from Section 2.11 that a successful return from a UDP output operation only means there was room for the resulting IP datagram on the interface output queue.
The basic rule is that an asynchronous error is not returned for a UDP socket unless the socket has been connected.
We will describe how to call connect for a UDP socket in Section 8.11
Why this design decision was made when sockets were first implemented is rarely understood.
Consider a UDP client that sends three datagrams in a row to three different servers (i.e., three different IP addresses) on a single UDP socket.
The client then enters a loop that calls recvfrom to read the replies.
Two of the datagrams are correctly delivered (that is, the server was running on two of the three hosts) but the third host was not running the server.
This ICMP error message contains the IP header and UDP header of the datagram that caused the error.
The client that sent the three datagrams needs to know the destination of the datagram that caused the error to distinguish which of the three datagrams caused the error.
But how can the kernel return this information to the process? The only piece of information that recvfrom can return is an errno value; recvfrom has no way of returning the destination IP address and destination UDP port number of the datagram in error.
The decision was made, therefore, to return these asynchronous errors to the process only if the process connected the UDP socket to exactly one peer.
Linux returns most ICMP "destination unreachable" errors even for unconnected sockets, as long as the SO_BSDCOMPAT socket option is not enabled.
We return to this problem of asynchronous errors with UDP sockets in Section 28.7 and show an easy way to obtain these errors on unconnected sockets using a daemon of our own.
Figure 8.11 shows as bullets the four values that must be specified or chosen when the client sends a UDP datagram.
The client must specify the server's IP address and port number for the call to sendto.
Normally, the client's IP address and port are chosen automatically by the kernel, although we mentioned that the client can call bind if it so chooses.
If these two values for the client are chosen by the kernel, we also mentioned that the client's ephemeral port is chosen once, on the first sendto, and then it never changes.
The client's IP address, however, can change for every UDP datagram that the client sends, assuming the client does not bind a specific IP address to the socket.
The reason is shown in Figure 8.11: If the client host is multihomed, the client could alternate between two destinations, one going out the datalink on the left, and the other going out the datalink on the right.
In this worst-case scenario, the client's IP address, as chosen by the kernel based on the outgoing datalink, would change for every datagram.
What happens if the client binds an IP address to its socket, but the kernel decides that an outgoing datagram must be sent out some other datalink? In this case the IP datagram will contain a source IP address that is different from the IP address of the outgoing datalink (see Exercise 8.6)
Figure 8.12 shows the same four values, but from the server's perspective.
There are at least four pieces of information that a server might want to know from an arriving IP datagram: the source IP address, destination IP address, source port number, and destination port number.
Figure 8.13 shows the function calls that return this information for a TCP server and a UDP server.
A TCP server always has easy access to all four pieces of information for a connected socket, and these four values remain constant for the lifetime of a connection.
Since UDP is connectionless, the destination IP address can change for each datagram that is sent to the server.
We will show how to determine the destination address of a UDP datagram in Section 22.2, after we cover the recvmsg function.
We mentioned at the end of Section 8.9 that an asynchronous error is not returned on a UDP socket unless the socket has been connected.
Indeed, we are able to call connect (Section 4.3) for a UDP socket.
But this does not result in anything like a TCP connection: There is no threeway handshake.
Instead, the kernel just checks for any immediate errors (e.g., an obviously unreachable destination), records the IP address and port number of the peer (from the socket address structure passed to connect), and returns immediately to the calling process.
Overloading the connect function with this capability for UDP sockets is confusing.
If the convention that sockname is the local protocol address and peername is the foreign protocol address is used, then a better name would have been setpeername.
Similarly, a better name for the bind function would be setsockname.
An unconnected UDP socket, the default when we create a UDP socket.
A connected UDP socket, the result of calling connect on a UDP socket.
With a connected UDP socket, three things change, compared to the default unconnected UDP socket:
We can no longer specify the destination IP address and port for an output operation.
That is, we do not use sendto, but write or send instead.
Anything written to a connected UDP socket is automatically sent to the protocol address (e.g., IP address and port) specified by connect.
Similar to TCP, we can call sendto for a connected UDP socket, but we cannot specify a destination address.
The POSIX specification states that when the fifth argument is a null pointer, the sixth argument is ignored.
We do not need to use recvfrom to learn the sender of a datagram, but read, recv, or recvmsg instead.
The only datagrams returned by the kernel for an input operation on a connected UDP socket are those arriving from the protocol address specified in connect.
Datagrams destined to the connected UDP socket's local protocol address (e.g., IP address and port) but arriving from a protocol address other than the one to which the socket was connected are not passed to the connected socket.
This limits a connected UDP socket to exchanging datagrams with one and only one peer.
Technically, a connected UDP socket exchanges datagrams with only one IP address, because it is possible to connect to a multicast or broadcast address.
Asynchronous errors are returned to the process for connected UDP sockets.
The corollary, as we previously described, is that unconnected UDP sockets do not receive asynchronous errors.
The POSIX specification states that an output operation that does not specify a destination address on an unconnected UDP socket should return ENOTCONN, not EDESTADDRREQ.
Figure 8.15 summarizes the three points that we made about a connected UDP socket.
The application calls connect, specifying the IP address and port number of its peer.
It then uses read and write to exchange data with the peer.
Datagrams arriving from any other IP address or port (which we show as "???" in Figure 8.15) are not passed to the connected socket because either the source IP address or source UDP port does not match the protocol address to which the socket is connected.
These datagrams could be delivered to some other UDP socket on the host.
If there is no other matching socket for the arriving datagram, UDP will discard it and generate an ICMP "port unreachable" error.
The DNS provides another example, as shown in Figure 8.16
Example of DNS clients and servers and the connect function.
A DNS client can be configured to use one or more servers, normally by listing the IP addresses of the servers in the file /etc/resolv.conf.
If a single server is listed (the leftmost box in the figure), the client can call connect, but if multiple servers are listed (the second box from the right in the figure), the client cannot call connect.
Also, a DNS server normally handles any client request, so the servers cannot call connect.
A process with a connected UDP socket can call connect again for that socket for one of two reasons:
The first case, specifying a new peer for a connected UDP socket, differs from the use of connect with a TCP socket: connect can be called only one time for a TCP socket.
The Unix variants seem to differ on exactly how to unconnect a socket, and you may encounter approaches that work on some systems and not others.
For example, calling connect with NULL for the address works only on some systems (and on some, it only works if the third argument, the length, is nonzero)
The POSIX specification and BSD man pages are not much help here, only mentioning that a null address should be used and not mentioning the error return (even on success) at all.
The most portable solution is to zero out an address structure, set the family to AF_UNSPEC as mentioned above, and pass it to connect.
Another area of disagreement is around the local binding of a socket during the unconnect process.
FreeBSD and Linux set the local IP address back to all zeros, even if you previously called bind, but leave the port number intact.
Solaris sets the local IP address back to all zeros if it was assigned by an implicit bind; but if the program called bind explicitly, then the IP address remains unchanged.
Calling sendto for two datagrams on an unconnected UDP socket then involves the.
Another consideration is the number of searches of the routing table.
The first temporary connect searches the routing table for the destination IP address and saves (caches) that information.
When the application knows it will be sending multiple datagrams to the same peer, it is more efficient to connect the socket explicitly.
Calling connect and then calling write two times involves the following steps by the kernel:
In this case, the kernel copies only the socket address structure containing the destination IP address and port one time, versus two times when sendto is called twice.
The changes are the new call to connect and replacing the calls to sendto and recvfrom with calls to write and read.
Our client main function, Figure 8.7 , remains the same.
The first point we notice is that we do not receive the error when we start the client process.
The error occurs only after we send the first datagram to the server.
It is sending this datagram that elicits the ICMP error from the server host.
But when a TCP client calls connect , specifying a server host that is not running the server process, connect returns the error because the call to connect causes the TCP three-way handshake to happen, and the first packet of that handshake elicits an RST from the server TCP (Section 4.3 )
Unfortunately, not all kernels return ICMP messages to a connected UDP socket, as we have shown in this section.
Normally, Berkeley-derived kernels return the error, while System V kernels do not.
For example, if we run the same client on a Solaris 2.4 host and connect to a host that is not running our server, we can watch with tcpdump and verify that the ICMP "port unreachable" error is returned by the server host, but the client's call to read never returns.
UnixWare does not return the error, while AIX, Digital Unix, HP-UX, and Linux all return the error.
We now examine the effect of UDP not having any flow control.
First, we modify our dg_cli function to send a fixed number of datagrams.
We next modify the server to receive datagrams and count the number received.
This server no longer echoes datagrams back to the client.
When we terminate the server with our terminal interrupt key (SIGINT ), it prints the number of received datagrams and terminates.
We now run the server on the host freebsd , a slow SPARCStation.
Additionally, we run netstat s on the server, both before and after, as the statistics that are output tell us how many datagrams were lost.
There is no indication whatsoever to the server application or to the client application that these datagrams were lost.
As we have said, UDP has no flow control and it is unreliable.
It is trivial, as we have shown, for a UDP sender to overrun the receiver.
Unfortunately, the netstat counter of the number dropped due to a full socket buffer is systemwide.
There is no way to determine which applications (e.g., which UDP ports) are affected.
The number of datagrams received by the server in this example is not predictable.
It depends on many factors, such as the network load, the processing load on the client host, and the processing load on the server host.
If we run the same client and server, but this time with the client on the slow Sun and the server on the faster RS/6000, no datagrams are lost.
The number of UDP datagrams that are queued by UDP for a given socket is limited by the size of that socket's receive buffer.
If we increase the size of the socket receive buffer, we expect the server to receive additional datagrams.
While this is slightly better than the earlier example with the default socket receive buffer, it is no panacea.
A connected UDP socket can also be used to determine the outgoing interface that will be used to a particular destination.
This is because of a side effect of the connect function when applied to a UDP socket: The kernel chooses the local IP address (assuming the process has not already called bind to explicitly assign this)
This local IP address is chosen by searching the routing table for the destination IP address, and then using the primary IP address for the resulting interface.
Figure 8.23 shows a simple UDP program that connects to a specified IP address and then calls getsockname, printing the local IP address and port.
Figure 8.23 UDP program that uses connect to determine outgoing interface.
If we run the program on the multihomed host freebsd, we have the following output:
The first time we run the program, the command-line argument is an IP address that follows the default route.
The kernel assigns the local IP address to the primary address of the interface to which the default route points.
The second time, the argument is the IP address of a system connected to a second Ethernet interface, so the kernel assigns the local IP address to the primary address of this second interface.
Calling connect on a UDP socket does not send anything to that host; it is entirely a local operation that saves the peer's IP address and port.
We also see that calling connect on an unbound UDP socket also assigns an ephemeral port to the socket.
Unfortunately, this technique does not work on all implementations, mostly SVR4-derived kernels.
We now combine our concurrent TCP echo server from Chapter 5 with our iterative UDP echo server from this chapter into a single server that uses select to multiplex a TCP and UDP socket.
SO_REUSEADDR socket option in case connections exist on this port.
A UDP socket is also created and bound to the same port.
Even though the same port is used for TCP and UDP sockets, there is no need to set the SO_REUSEADDR socket option before this call to bind, because TCP ports are independent of UDP ports.
A signal handler is established for SIGCHLD because TCP connections will be handled by a child process.
We initialize a descriptor set for select and calculate the maximum of the two descriptors for which we will wait.
Figure 8.24 First half of echo server that handles TCP and UDP using select.
Since our sig_chld handler can interrupt our call to select, we handle an error of EINTR.
We accept a new client connection when the listening TCP socket is readable, fork a child, and call our str_echo function in the child.
Figure 8.25 Second half of echo server that handles TCP and UDP using select.
If the UDP socket is readable, a datagram has arrived.
We read it with recvfrom and send it back to the client with sendto.
Converting our echo client/server to use UDP instead of TCP was simple.
But lots of features provided by TCP are missing: detecting lost packets and retransmitting, verifying responses as being from the correct peer, and the like.
We will return to this topic in Section 22.5 and see what it takes to add some reliability to a UDP application.
Normally, this is not a problem, because many UDP applications are built using a request-reply model, and not for transferring bulk data.
There are still more points to consider when writing UDP applications, but we will save these until Chapter 22, after covering the interface functions, broadcasting, and multicasting.
We will use this program to see the ICMP port unreachable returned by the server host.
Next, run our client from the previous exercise in another window, specifying the IP address of some host that is not running the server.
What about the listening socket; do you think it has its own socket receive buffer?
Put a printf in the client each time a datagram is written to the socket.
Does this change the percentage of received packets? Why? Put a printf in the server each time a datagram is read from the socket.
Modify Figure 8.8 to send one maximum-size UDP datagram, read it back, and print the number of bytes returned by recvfrom.
It was first designed to meet the needs of the growing IP telephony market; in particular, transporting telephony signaling across the Internet.
Since it is a newer transport protocol, it does not have the same ubiquity as TCP or UDP; however, it provides some new features that may simplify certain application designs.
We will discuss the reasons to consider using SCTP instead of TCP in Section 23.12
Although there are some fundamental differences between SCTP and TCP, the one-to-one interface for SCTP provides very nearly the same application interface as TCP.
This allows for trivial porting of applications, but does not permit use of some of SCTP's advanced features.
The one-to-many interface provides full support for these features, but may require significant retooling of existing applications.
The one-to-many interface is recommended for most new applications developed for SCTP.
This chapter describes additional elementary socket functions that can be used with SCTP.
We first describe the two different interface models that are available to the application developer.
We also describe the new functions available for and used exclusively with SCTP.
We look at the shutdown function and how its use with SCTP differs from TCP.
We then briefly cover the use of notifications in SCTP.
Notifications allow an application to be informed of significant protocol events other than the arrival of user data.
We will see an example of how to use notifications in Section 23.4
Since SCTP is a newer protocol, the interface for all its features has not yet completely stabilized.
As of this writing, the interfaces described are believed to be stable, but are not yet as ubiquitous as the rest of the sockets API.
Users of applications designed to use SCTP exclusively may need to be prepared to install kernel patches or otherwise upgrade their operating system, and applications which need to be ubiquitous need to be able to use TCP if SCTP is not available on the system they are running on.
There are two types of SCTP sockets: a one-to-one socket and a one-to-many socket.
Recall from Section 2.5 that an SCTP association is a connection between two systems, but may involve more than two IP addresses due to multihoming.
This mapping is similar to the relationship between a TCP socket and a TCP connection.
With a one-to-many socket, several SCTP associations can be active on a given socket simultaneously.
This mapping is similar to the manner in which a UDP socket bound to a particular port can receive interleaved datagrams from several remote UDP endpoints that are all simultaneously sending data.
When deciding which style of interface to use, the application needs to consider several factors, including:
What type of server is being written, iterative or concurrent?
How many socket descriptors does the server wish to manage?
Is it important to optimize the association setup to enable data on the third (and possibly fourth) packet of the four-way handshake?
How much connection state does the application wish to maintain?
When the sockets API for SCTP was under development, different terminology was used for the two styles of sockets, and readers may sometimes encounter these older terms in documentation or source code.
The original term for the one-to-one socket was a "TCP-style" socket, and the original term for a one-to-many socket was a "UDPstyle" socket.
These style terms were later dropped because they tended to cause confusion by creating expectations that SCTP would behave more like TCP or UDP, depending on which style of socket was used.
In fact, these terms referred to only one aspect of the differences between TCP and UDP sockets (i.e., whether a socket supports multiple concurrent transport-layer associations)
The current terminology ("one-to-one" versus "one-to-many") focuses our attention on the key difference between the two socket styles.
Finally, note that some writers use the term "many-to-one" instead of "one-tomany"; the terms are interchangeable.
The one-to-one style was developed to ease the porting of existing TCP applications to SCTP.
There are some differences one should be aware of, especially when porting existing TCP applications to SCTP using this style.
Any socket options must be converted to the SCTP equivalent.
For example, an application protocol based on TCP might do a write() system call to write a two-byte message length field, x, followed by a write() system call that writes x bytes of data.
However, if this is done with SCTP, the receiving SCTP will receive two separate messages (i.e., the read call will return twice: once with a two-byte message, and then again with an x byte message)
Some TCP applications use a half-close to signal the end of input to the other side.
To port such applications to SCTP, the application-layer protocol will need to be rewritten so that the application signals the end of input in the application data stream.
The send function can be used in the normal fashion.
For the sendto and sendmsg functions, any address information included is treated as an override of the primary destination address (see Section 2.8)
A typical user of the one-to-one style will follow the timeline shown in Figure 9.1
When the server is started, it opens a socket, binds to an address, and waits for a client connection with the accept system call.
Sometime later, the client is started, it opens a socket, and initiates an association with the server.
We assume the client sends a request to the server, the server processes the request, and the server sends back a reply to the client.
This cycle continues until the client initiates a shutdown of the association.
This action closes the association, whereupon the server either exits or waits for a new association.
As can be seen by comparison to a typical TCP exchange, an SCTP one-to-one socket exchange proceeds in a fashion similar to that shown in Figure 4.1
The one-to-many style provides an application writer the ability to write a server without managing a large number of socket descriptors.
A single socket descriptor will represent multiple associations, much the same way that a UDP socket can receive messages from multiple clients.
An association identifier is used to identify a single association on a one-to-many-style socket.
This association identifier is a value of type sctp_assoc_t; it is normally an integer.
It is an opaque value; an application should not use an association identifier that it has not previously been given by the kernel.
Users of the one-to-many style should keep the following issues in mind:
When the client closes the association, the server side will automatically close as well, thus1
Using the one-to-many style is the only method that can be used to cause data to be piggybacked on the third or fourth packet of the four-way handshake (see Exercise 9.3)
Any sendto, sendmsg, or sctp_sendmsg to an address for which an association does not yet exist will cause an active open to be attempted, thus creating (if successful) a new association with that address.
This behavior occurs even if the application doing the send has called the listen function to request a passive open.
The user must use the sendto, sendmsg, or sctp_sendmsg functions, and may not use the send or write function.
If the sctp_peeloff function is used to create a one-to-one-style socket, send or write may be used on it.
To supply this, the caller needs to use the sendmsg function with ancillary data, or the sctp_sendmsg function.
This default setting applies to both the one-to-one and one-to-many style.
When the SCTP sockets API was first developed, the one-to-many-style interface was defined to have the association notification turned on by default as well.
Later versions of the API document have since disabled all notifications except the sctp_data_io_event for both the one-to-one- and one-to-many-style interface.
It is always good practice for an application writer to explicitly disable (or enable) the notifications that are unwanted (or desired)
This explicit approach assures the developer that the expected behavior will result no matter which OS the code is ported to.
A typical one-to-many style timeline is depicted in Figure 9.2
First, the server is started, creates a socket, binds to an address, calls listen to enable client associations, and calls sctp_recvmsg, which blocks waiting for the first message to arrive.
A client opens a socket and calls sctp_sendto, which implicitly sets up the association and piggybacks the data request to the server on the third packet of the four-way handshake.
The server receives the request, and processes and sends back a reply.
The client receives the reply and closes the socket, thus closing the association.
This example shows an iterative server, where (possibly interleaved) messages from many associations (i.e., many clients) can be processed by a single thread of control.
The sctp_peeloff function can be used to peel off a particular association (for example, a long-running session) from a one-to-many socket into its own one-to-one socket.
The one-to-one socket of the extracted association can then be dispatched to its own thread or forked process (as in the concurrent model)
Meanwhile, the main thread continues to handle messages from any remaining associations in an iterative fashion on the original socket.
An SCTP server may wish to bind a subset of IP addresses associated with the host system.
Traditionally, a TCP or UDP server can bind one or all addresses on a host, but they cannot bind a subset of addresses.
The sctp_bindx function provides more flexibility by allowing an SCTP socket to bind a particular subset of addresses.
The sockfd is a socket descriptor returned by the socket function.
The second argument, addrs, is a pointer to a packed list of addresses.
Each socket address structure is placed in the buffer immediately following the preceding socket address structure, with no intervening padding.
The number of addresses being passed to sctp_bindx is specified by the addrcnt parameter.
The sctp_bindx call can be used on a bound or unbound socket.
For an unbound socket, a call to sctp_bindx will bind the given set of addresses to the socket descriptor.
If sctp_bindx is performed on a listening socket, future associations will use the new address configuration; the change does not affect any existing associations.
The port number in all the socket address structures must be the same and must match any port number that is already bound; if it doesn't, then sctp_bindx will fail, returning the error code EINVAL.
Since adding and removing addresses from a connected association is optional functionality, implementations that do not support this functionality will return EOPNOTSUPP.
Note that both ends of an association must support this feature for proper operation.
The sctp_connectx function is used to connect to a multihomed peer.
We specify addrcnt addresses, all belonging to the same peer, in the addrs parameter.
The addrs parameter is a packed list of addresses, as in Figure 9.4
The SCTP stack uses one or more of the given addresses for establishing the association.
All the addresses listed in addrs are considered to be valid, confirmed addresses.
The getpeername function was not designed with the concept of a multihoming-aware transport protocol; when using SCTP, it only returns the primary address.
When all the addresses are required, the sctp_getpaddrs function provides a mechanism for an application to retrieve all the addresses of a peer.
Returns: the number of peer addresses stored in addrs, –1 on error.
The sockfd parameter is the socket descriptor returned by the socket function.
The id is the association identification for a one-to-many-style socket.
If the socket is using the one-to-one style, the id field is ignored.
The sctp_getladdrs function can be used to retrieve the local addresses that are part of an association.
This function is often necessary when a local endpoint wishes to know exactly which local addresses are in use (which may be a proper subset of the system's addresses)
Returns: the number of local addresses stored in addrs, –1 on error.
The sockfd is the socket descriptor returned by the socket function.
If the socket is using the one-to-one style, the id field is ignored.
The addrs parameter is an address of a pointer that sctp_getladdrs will fill in with a locally allocated, packed list of addresses.
An application can control various features of SCTP by using the sendmsg function along with ancillary data (described in Chapter 14)
However, because the use of ancillary data may be inconvenient, many SCTP implementations provide an auxiliary library call (possibly implemented as a system call) that eases an application's use of SCTP's advanced features.
The user of sctp_sendmsg has a greatly simplified sending method at the cost of more arguments.
The sockfd field holds the socket descriptor returned from a socket system call.
The msg field points to a buffer of msgsz bytes to be sent to the peer endpoint to.
The tolen field holds the length of the address stored in to.
The ppid field holds the pay-load protocol identifier that will be passed with the data chunk.
The flags field will be passed to the SCTP stack to identify any SCTP options; valid values for this field may be found in Figure 7.16
A caller specifies an SCTP stream number by filling in the stream.
The caller may specify the lifetime of the message in milliseconds in the lifetime field, where 0 represents an infinite lifetime.
A user context, if any, may be specified in context.
This approach is much easier than allocating the necessary ancillary data and setting up the appropriate structures in the msghdr structure.
The function also allows the user to retrieve the sctp_sndrcvinfo structure that accompanies the message that was read into the message buffer.
On return from this call, msg is filled with up to msgsz bytes of data.
The message sender's address is contained in from, with the address size filled in the fromlen argument.
Any message flags will be contained in the msg_flags argument.
The sctp_opt_info function is provided for implementations that cannot use the getsockopt functions for SCTP.
This inability to use the getsockopt function is because some of the SCTP socket options, for example, SCTP_STATUS, need an in-out variable to pass the association identification.
For systems that cannot provide an in-out variable to the getsockopt function, the user will need to use sctp_opt_info.
For systems like FreeBSD that do allow in-out variables in the socket option call, the sctp_opt_info call is a library call that repackages the arguments into the appropriate getsockopt call.
As previously mentioned, it is possible to extract an association contained by a one-to-many socket into an individual one-to-one-style socket.
The semantics are much like the accept function call with an additional argument.
The caller passes the sockfd of the one-to-many socket and the association identification id that is being extracted.
At the completion of the call, a new socket descriptor is returned.
This new descriptor will be a one-to-one-style socket descriptor with the requested association.
Returns: a new socket descriptor on success, –1 on error.
The shutdown function that we discussed in Section 6.6 can be used with an SCTP endpoint using the one-to-one-style interface.
Because SCTP's design does not provide a half-closed state, an SCTP endpoint reacts to a shutdown call differently than a TCP endpoint.
When an SCTP endpoint initiates a shutdown sequence, both endpoints must complete transmission of any data currently in the queue and close the association.
The endpoint that initiated the active open may wish to invoke shutdown instead of close so that the endpoint can be used to connect to a new peer.
Unlike TCP, a close followed by the opening of a new socket is not required.
Note that the new connection will fail if the endpoint does not wait until the SCTP shutdown sequence completes.
Figure 9.5 shows the typical function calls in this scenario.
If the user had not subscribed to receive these events, then a read of length 0 would have been returned.
The effects of the shutdown function for TCP were described in Section 6.6
The shutdown function howto holds the following semantics for SCTP:
SHUT_WR Disables further send operations and initiates the SCTP shutdown procedures, which will terminate the association.
Note that this option does not provide a half-closed state, but does allow the local endpoint to read any queued data that the peer may have sent prior to receiving the SCTP SHUTDOWN message.
SHUT_RDWR Disables all read and write operations, and initiates the SCTP shutdown procedure.
Any queued data that was in transit to the local endpoint will be acknowledged and then silently discarded.
The SCTP user can track the state of its association(s) via these notifications.
Notifications communicate transportlevel events, including network status change, association startups, remote operational errors, and undeliverable messages.
For both the one-to-one and the one-to-many styles, all events are disabled by default with the exception of sctp_data_io_event.
We will see an example of using notifications in Section 23.7
Eight events can be subscribed to using the SCTP_EVENTS socket option.
Seven of these events generate additional data—termed a notification—that a user will receive via the normal socket descriptor.
The notifications are added to the socket descriptor inline with data as the events that generate them occur.
When reading from a socket with notification subscriptions, user data and notifications will be interleaved on the socket buffer.
To differentiate between peer data and a notification, the user uses either the recvmsg function or the sctp_recvmsg function.
This flag tells the application that the message just read is not data from the peer, but a notification from the local SCTP stack.
Each type of notification is in tag-length-value form, where the first eight bytes of the message identify what type of notification has arrived and its total length.
This information is normally received in ancillary data using the recvmsg call.
Note that the sn_header field is used to interpret the type value, to decode the actual message being sent.
Each notification has its own structure that gives further information about the event that has occurred on the transport.
This notification informs an application that a change has occurred to an association; either a new association has begun or an existing association has ended.
The information provided with this event is defined as follows:
The sac_state describes the type of event that has occurred on the association, and will take one of the following values:
SCTP_COMM_UP This state indicates that a new association has just been started.
The inbound and outbound streams fields indicate how many streams are available in each direction.
The association identification is filled with a unique value that can be used to communicate with the local SCTP stack regarding this association.
Any user-specific information will be found in the sac_info field of the notification.
The most likely cause of this notification is a peer crash and restart.
The application should verify the number of streams in each direction, since these values may change during a restart.
For the one-to-one style, after receiving this notification, the socket descriptor can be used again to connect to a different peer.
The sac_error field holds any SCTP protocol error cause code that may have caused an association change.
For example, if an association was aborted by the peer with a user-defined error, that error would be found in this field.
This notification indicates that one of the peer's addresses has experienced a change of state.
This change may either be a failure, such as the destination is not responding when sent to, or a recovery, such as a destination that was in a failed state has recovered.
The structure that accompanies an address change is as follows:
The spc_aaddr field holds the address of the peer affected by this event.
A remote peer may send an operational error message to the local endpoint.
These messages can indicate a variety of error conditions for the association.
The entire error chunk will be passed to the application in wire format when this notification is enabled.
When a message cannot be delivered to a peer, the message is sent back to the user through this notification.
This notification is usually soon followed by an association failure notification.
In most cases, the only way a message will not be delivered is if the association has failed.
The only time a message failure will occur without an association failure is when the partial reliability extension of SCTP is being used.
When an error notification is sent, the following format will be read by the application:
SCTP_DATA_UNSENT, which indicates that the message could never be transmitted to the peer (e.g., flow control prevented the message from being sent before its lifetime expired), so the peer never received it.
SCTP_DATA_SENT, which indicates that the data was transmitted to the peer at least once, but was never acknowledged.
In this case, the peer may have received the message, but it was unable to acknowledge it.
This distinction may be important to a transaction protocol, which might perform different actions to recover from a broken connection based on whether or not a given message might have been received.
This notification is passed to an application when a peer sends a SHUTDOWN chunk to the local endpoint.
This notification informs the application that no new data will be accepted on the socket.
All currently queued data will be transmitted, and at the completion of that transmission, the association will be shut down.
This parameter is exchanged in the INIT and INIT-ACK to inform each peer what type of application adaption is being performed.
The sai_assoc_id identifies of association that this adaption layer notification.
The partial delivery application interface is used to transport large messages to the user via the socket buffer.
A message of this size would tax or exhaust system resources.
An SCTP implementation would fail to handle such a message unless the implementation had a mechanism to begin delivering the message before all of it arrived.
When an implementation does this form of delivery, it is termed "the partial delivery API." The partial delivery API is invoked by the SCTP implementation sending data with the msg_flags field remaining clear until the last piece of the message is ready to be delivered.
In some instances, the partial delivery API will need to communicate a status to the application.
The pdapi_assoc_id field identifies the association upon which the partial delivery API event has occurred.
The sctp_peeloff function provides a method of extracting an association from one style to the other.
These events can aid an application in better managing the associations it maintains.
Since SCTP is multihomed, not all the standard sockets functions introduced in Chapter 4 are adequate.
Our simple example is similar to the echo server presented in Chapter 5, and performs the following steps:
A client reads a line of text from standard input and sends the line to the server.
The line follows the form [#] text, where the number in brackets is the SCTP stream number on which the text message should be sent.
The server receives the text message from the network, increases the stream number on which the message arrived by one, and sends the text message back to the client on this new stream number.
The client reads the echoed line and prints it on its standard output, displaying the stream number, stream sequence number, and text string.
Figure 10.1 depicts this simple client/server along with the functions used for input and output.
We show two arrows between the client and server depicting two unidirectional streams being used, even though the overall association is full-duplex.
The fgets and fputs functions are from the standard I/O library.
We do not use the writen and readline functions defined in Section 3.9 since they are unnecessary.
Simply making this change, however, would not take advantage of any of the additional features provided by SCTP except multihoming.
Using the one-to-many style allows us to exercise all of SCTP's features.
By default, our server responds using the next higher stream than the one on which the message was received.
If an integer argument is passed on the command line, the server interprets the argument as the value of stream_increment, that is, it decides whether or not to increment the stream number of incoming messages.
We will use this option in our discussion of head-of-line blocking in Section 10.5
Binding the wildcard address tells the system that this SCTP endpoint will use all available local addresses in any association that is set up.
For multihomed hosts, this binding means that a remote endpoint will be able to make associations with and send packets to any of the local host's routeable addresses.
Our choice of the SCTP port number is based on Figure 2.10
Note that the server makes the same considerations that were made earlier in our previous example found in Section 5.2
The server changes its notification subscription for the one-to-many SCTP socket.
From this structure, the server can determine the stream number on which the message arrived.
The server initializes the size of the client socket address structure, then blocks while waiting for a message from any remote peer.
When a message arrives, the server checks the stream_increment flag to see if it should increment the stream number.
If the flag is set (no arguments were passed on the command line), the server increments the stream number of the message.
The server sends back the message using the payload protocol ID, flags, and the possibly modified stream number from the sri structure.
Notice that this server does not want association notification, so it disables all events that would pass messages up the socket buffer.
The server relies on the information in the sctp_sndrcvinfo structure and the returned address found in cliaddr to locate the peer association and return the echo.
This program runs forever until the user shuts it down with an external signal.
First, the client verifies that the caller provided a host to send messages to.
It then checks if the "echo to all" option is being enabled (we will see this used in Section 10.5 )
The client translates the server address, passed on the command line, using the inet_pton function.
It combines that with the server's well-known port number and uses the resulting address as the destination for the requests.
The client explicitly sets the notification subscription provided by our one-to-many SCTP socket.
Therefore, the client turns these off (as was done in the server) and only enables the receipt of the sctp_sndrcvinfo structure.
We will discuss this function in Section 10.5 as we explore uses for SCTP streams.
On return from processing, the client closes the SCTP socket, which shuts down any SCTP associations using the socket.
The client then returns from main with a return code of 0, indicating that the program ran successfully.
The client then enters a loop that reads from the fp passed by our caller with a blocking call to fgets.
The main program passes stdin to this function, so user input is read and processed in the loop until the terminal EOF character (Control-D) is typed by the user.
This user action ends the function and causes a return to the caller.
The client examines the user input to make sure it is of the form [#] text.
If the format is invalid, the client prints an error message and re-enters the blocking call to the fgets function.
The client translates the user requested stream found in the input into the sinfo_stream field in the sri structure.
After initializing the appropriate lengths of the address and the size of the actual user data, the client sends the message using the sctp_sendmsg function.
The client now blocks and waits for the echoed message from the server.
The client displays the returned message echoed to it displaying the stream number, stream sequence number, as well as the text message.
After displaying the message, the client loops back to get another request from the user.
A user starts the SCTP echo server with no arguments on a FreeBSD machine.
The client is started with just the address of our server.
This behavior is expected from our server with no arguments.
Also notice that the stream sequence number incremented on the second message received on stream 5, as expected.
Our simple server provides a method to send text messages to any of a number of streams.
A stream in SCTP is not a stream of bytes (as in TCP), but a sequence of messages that is ordered within the association.
These sub-ordered streams are used to avoid the head-of-line blocking found in TCP.
Head-of-line blocking occurs when a TCP segment is lost and a subsequent TCP segment arrives out of order.
That subsequent segment is held until the first TCP segment is retransmitted and arrives at the receiver.
Delaying delivery of the subsequent segment assures that the receiving application sees all data in the order in which the sending application sent it.
This delay to achieve complete ordering is quite useful, but it has a downside.
Assume that semantically independent messages are being sent over a single TCP connection.
For example, a server may send three different pictures for a Web browser to display.
To make the pictures appear on the user's screen in parallel, a server sends a piece from the first picture, then a piece from the second picture, and finally a piece from the third picture.
The server repeats this process until all three pictures are successfully transmitted to the browser.
But what happens if a TCP packet holding a piece of the first picture is lost? The client will hold all data until that missing piece is retransmitted and arrives successfully, delaying all data for the second and third pictures, as well as data for the first picture.
These multiplexing protocols have been proposed to avoid the harmful behavior of multiple parallel TCP connections that do not share state [Touch 1997]
Although creating one TCP connection per picture (as HTTP clients normally do) avoids the head-of-line blocking problem, each connection has to discover the RTT and available bandwidth independently; a loss on one connection (a signal of congestion on the path) does not necessarily cause the other connections to slow down.
This blocking is not really what the application would like to occur.
Ideally, only later pieces of the first picture would be delayed while pieces of the second and third pictures that arrive in order would be delivered immediately to the user.
In Figure 10.6, we see the same three pictures being sent.
This time, the server uses streams so that head-of-line blocking only occurs where it is desired, allowing delivery of the second and third pictures but holding the partially received first picture until in-order delivery is possible.
This function is similar to our previous sctpstr_cli function except the client no longer expects a stream number in brackets preceding each message.
Instead, the function sends the user message to all SERV_MAX_SCTP_STRM streams.
After sending the messages, the client waits for all the responses to arrive from the server.
In running the code, we also pass an additional argument to the server so that the server responds on the same stream on which a message was received.
This way, the user can better track the responses sent and their order of arrival.
As before, the client initializes the sri structure used to set up the stream it will be sending and receiving from.
In addition, the client zeros out the data buffer from which it will collect user input.
Then, the client enters the main loop, once again blocking on user input.
The client sets up the message size and then deletes the newline character that is at the end of the buffer (if any)
Before sending the message, it appends the string ".msg." and the stream number so that we can observe the order of the arriving messages.
In this way, we can compare the arrival order to the order in which the client sent the actual messages.
Note also the client sends the messages to a set number of streams without regard to how many were actually set up.
It is possible that one or more of the sends may fail if the peer negotiates the number of streams downward.
This code has the potential to fail if the send or receive windows are too small.
If the peer's receive window is too small, it is possible that the client will block.
Since the client does not read any information until all of its sends are complete, the server could also potentially block while waiting for the client to finish reading the responses the server already sent.
The result of such a scenario would be a deadlock of the two endpoints.
This code is not meant to be scalable, but instead to illustrate streams and head-of-line blocking in a simple, straightforward manner.
We now block, reading all the response messages from our server and displaying each as we did before.
After the last message is read, the client loops back for more user input.
We execute the client and server on two separate FreeBSD machines, separated by a configurable router, as illustrated in Figure 10.8
The router can be configured to insert both delay and loss.
We execute the program first with no loss inserted by the router.
We start the server with an additional argument of "0", forcing the server to not increment the stream number on its replies.
Next, we start the client, passing it the address of the echo server and an additional argument so that it will send a message to each stream.
With no loss, the client sees the responses arrive back in the order in which the client sent them.
We now change the parameters of our router to lose 10% of all packets traveling in both directions and restart our client.
We can verify that the messages within a stream are properly being held for reordering by having the client send two messages to each stream.
We also modify the client to add a suffix to its message number to help us identify each message duplicate.
The modifications to the server are shown in Figure 10.9
The client adds an additional message number, "1", to help us track which message is being sent.
Then the client sends the message using the sctp_sendmsg function.
Here the code requires only one small change: We double the number of messages the client expects to receive back from the echo server.
We start our server and modified client, as before, and obtain the following output from the client:
As we can see from the output, messages are lost, and yet only the messages in a particular stream are delayed.
We have seen how SCTP streams can be used, but how can we control the number of streams an endpoint requests at association initialization? Our previous examples used the system default for the number of outbound streams.
For the FreeBSD KAME implementation of SCTP, this default is set to 10 streams.
Note that this change must be made on the socket before an association is created.
As before, the server sets up the flags based on additional arguments and opens the socket.
These lines contain the new code we have added to our server.
This change assures that the setsockopt call will not unintentionally change any other values.
The server then sets the sinit_max_ostreams field to the number of streams it would like to request.
Next, it sets the socket option with the initial message parameters.
An alternative to setting a socket option would be to use the sendmsg function and provide ancillary data to request different stream parameters from the default.
This type of ancillary data is only effective on the one-to-many-style socket interface.
In our examples, we have depended on the client closing the socket to shut down the association.
But the client application may not always wish to close the socket.
For that matter, our server may not want to keep the association open after sending the reply message.
In these cases, we need to look at two alternative mechanisms for shutting down an association.
For the one-tomany-style interface, two possible methods are available to the application: one is graceful, while the other is disruptive.
This flag forces an association to shut down after the message being sent is acknowledged.
This flag will force an immediate termination of the association with an ABORT chunk.
An ABORT chunk is similar to a TCP RST segment, terminating any association without delay.
Note that any data not yet transfered will be discarded.
However, closing an SCTP session with an ABORT chunk does not have any negative side effects like preventing TCP's TIME_WAIT state; the ABORT chunk causes a "graceful" abortive close.
Figure 10.11 shows the modifications needed to our echo server to initiate a graceful shutdown when the response message is sent to the peer.
Figure 10.12 shows a modified client that sends an ABORT chunk before closing the socket.
This flag value causes our server to shut down the association after the reply message is successfully acknowledged.
In these lines, the client prepares a message that is included with the abort as a user error cause.
This flag sends an ABORT chunk, which immediately terminates the association.
The ABORT chunk includes the user-initiated error cause with the message ("goodbye") in the upper layer reason field.
Even though the association has been aborted, we still need to close the socket descriptor to free the system resources associated with it.
We have looked at a simple SCTP client and server spanning about 150 lines of code.
Both the client and server used the one-to-many-style SCTP interface.
The server was constructed in an iterative style, common when using the one-to-many-style interface, receiving each message and responding on either the same stream the message was sent on or on one stream higher.
We modified our client to emphasize the problem and to show how SCTP streams can be used to avoid this problem.
We looked at how the number of streams can be manipulated using one of the many socket options available to control SCTP behavior.
Finally, we again modified our server and client so that they could be made to either abort an association including a user upper layer reason code, or in our server's case, shut down the association gracefully after sending a message.
Why do you think we do this? Is there a better way to find a more optimal size to set this to?
Figure 10.7? Would turning off the Nagle algorithm be better for this program? Build the client and server code, then modify both of them to disable the Nagle algorithm.
What happens if the application changes the number of streams afterwards?
Will this cause any problems? Is it a good design decision?
We should, however, use names instead of numbers for numerous reasons: Names are easier to remember; the numeric address can change but the name can remain the same; and with the move to IPv6, numeric addresses become much longer, making it much more error-prone to enter an address by hand.
This chapter describes the functions that convert between names and numeric values: gethostbyname and gethostbyaddr to convert between hostnames and IPv4 addresses, and getservbyname and getservbyport to convert between service names and port numbers.
Technically, an FQDN is also called an absolute name and must end with a period, but users often omit the ending period.
The trailing period tells the resolver that this name is fully qualified and it doesn't need to search its list of possible domains.
In this section, we will cover only the basics of the DNS that we need for network programming.
Entries in the DNS are known as resource records (RRs)
There are only a few types of RRs that we are interested in.
For example, here are the four DNS records for the host freebsd in the unpbook.com domain, the first of which is an A record:
There will be a transition period during which both zones will be populated.
When multiple MX records exist, they are used in order of preference, starting with the smallest value.
We do not use MX records in this text, but we mention them because they are used extensively in the real world.
If people use these service names instead of the actual hostnames, it is transparent when a service is moved to another host.
For example, the following could be CNAMEs for our host linux:
In our example earlier in this section, we specified both an A record and a AAAA record for our host freebsd.
All the records for another of our hosts are then.
This gives us additional control over the protocol chosen by some applications, as we will see in the next chapter.
Organizations run one or more name servers, often the program known as BIND (Berkeley Internet Name Domain)
Applications such as the clients and servers that we are writing in this text contact a DNS server by calling functions in a library known as the resolver.
The common resolver functions are gethostbyname and gethostbyaddr, both of which are described in this chapter.
The former maps a hostname into its IPv4 addresses, and the latter does the reverse.
Figure 11.1 shows a typical arrangement of applications, resolvers, and name servers.
On some systems, the resolver code is contained in a system library and is link-edited into the application when the application is built.
On others, there is a centralized resolver daemon that all applications share, and the system library code performs RPCs to this daemon.
In either case, application code calls the resolver code using normal function calls, typically calling the functions gethostbyname and gethostbyaddr.
The resolver code reads its system-dependent configuration files to determine the location of the organization's name servers.
We use the plural "name servers" because most organizations run multiple name servers, even though we show only one local server in the figure.
Multiple name servers are absolutely required for reliability and redundancy.
The file /etc/resolv.conf normally contains the IP addresses of the local name servers.
It might be nice to use the names of the name servers in the /etc/resolv.conf file, since the names are easier to remember and configure, but this introduces a chicken-and-egg problem of where to go to do the name-to-address conversion for the server that will do the name and address conversion!
The resolver sends the query to the local name server using UDP.
If the local name server does not know the answer, it will normally query other name servers across the Internet, also using UDP.
If the answers are too large to fit in a UDP packet, the resolver will automatically switch to TCP.
It is possible to obtain name and address information without using the DNS.
Common alternatives are static host files (normally the file /etc/hosts, as we describe in Figure 11.21), the Network Information System (NIS) or Lightweight Directory Access Protocol (LDAP)
If a name server is to be used for hostname lookups, then all these systems use the file /etc/resolv.conf to specify the IP addresses of the name servers.
Fortunately, these differences are normally hidden to the application programmer, so we just call the resolver functions such as gethostbyname and gethostbyaddr.
All the examples that we have shown so far in this book have intentionally used IP addresses instead of names, so we know exactly what goes into the socket address structures for functions such as connect and sendto , and what is returned by functions such as accept and recvfrom.
The example AAAA record and ip6.arpa PTR record in the previous section should make this obvious.
The most basic function that looks up a hostname is gethostbyname.
If successful, it returns a pointer to a hostent structure that contains all the IPv4 addresses for the host.
However, it is limited in that it can only return IPv4 addresses.
The POSIX specification cautions that gethostbyname may be withdrawn in a future version of the spec.
It is unlikely that gethostbyname implementations will actually disappear until the whole Internet is using IPv6, which will be far in the future.
However, withdrawing the function from the POSIX specification is a way to assert that it should not be used in new code.
We encourage the use of getaddrinfo (Section 11.6 ) in new programs.
Returns: non-null pointer if OK,NULL on error with h_errno set.
The non-null pointer returned by this function points to the following hostent structure:
In terms of the DNS, gethostbyname performs a query for an A record.
The returned h_name is called the canonical name of the host.
Some versions of gethostbyname allow the hostname argument to be a dotted-decimal string.
This code was added because the Rlogin client accepts only a hostname, calling gethostbyname , and will not accept a dotted-decimal string [Vixie 1996]
The POSIX specification permits, but does not require, this behavior, so a portable application cannot depend on it.
The NO_DATA error means the specified name is valid, but it does not have an A record.
An example of this is a hostname with only an MX record.
We show some examples of the strings returned by this function in the next example.
Figure 11.3 shows a simple program that calls gethostbyname for any number of command-line arguments and prints all the returned information.
The official hostname is output followed by a list of alias names.
For each address, we call inet_ntop and print the returned string.
We first execute the program with the name of our host aix , which has just one IPv4 address.
Next is a name that we showed in Section 11.2 as having a CNAME record.
As expected, the official hostname differs from our command-line argument.
To see the error strings returned by the hstrerror function, we first specify a non-existent hostname, and then a name that has only an MX record.
The function gethostbyaddr takes a binary IPv4 address and tries to find the hostname corresponding to that address.
Returns: non-null pointer if OK, NULL on error with h_errno set.
This function returns a pointer to the same hostent structure that we described with gethostbyname.
The field of interest in this structure is normally h_name, the canonical hostname.
In terms of the DNS, gethostbyaddr queries a name server for a PTR record in the inaddr.arpa domain.
If we refer to a service by its name in our code, instead of by its port number, and if the mapping from the name to port number is contained in a file (normally /etc/services ), then if the port number changes, all we need to modify is one line in the /etc/services file instead of having to recompile the applications.
The next function, getservbyname , looks up a service given its name.
The canonical list of port numbers assigned to services is maintained by the IANA at http://www.iana.org/assignments/port-numbers (Section 2.9 )
A given /etc/services file is likely to contain a subset of the IANA assignments.
If a protocol is also specified (protoname is a nonnull pointer), then the entry must also have a matching protocol.
Some Internet services are provided using either TCP or UDP (for example, the DNS and all the services in Figure 2.18 ), while others support only a single protocol (e.g., FTP requires TCP)
Normally this does not matter, because services that support multiple protocols often use the same TCP and UDP port number, but this is not guaranteed.
The main field of interest in the servent structure is the port number.
Since the port number is returned in network byte order, we must not call htons when storing this into a socket address structure.
Since FTP supports only TCP, the second and third calls are the same, and the fourth call will fail.
The next function, getservbyport , looks up a service given its port number and an optional protocol.
The last call fails because there is no service that uses port 21 with UDP.
Be aware that a few port numbers are used with TCP for one service, but the same port number is used with UDP for a totally different service.
We can now modify our TCP daytime client from Figure 1.5 to use gethostbyname and getservbyname and take two command-line arguments: a hostname and a service name.
This program also shows the desired behavior of attempting to connect to all the IP addresses for a multihomed server, until one succeeds or all the addresses have been tried.
Figure 11.4 Our daytime client that uses gethostbyname and getservbyname.
The first command-line argument is a hostname, which we pass as an argument to gethostbyname , and the second is a service name, which we pass as an argument to getservbyname.
Our code assumes TCP, and that is what we use as the second argument to getservbyname.
If it was, we construct a single-element list consisting of the corresponding address.
We now code the calls to socket and connect in a loop that is executed for every server address until a connect succeeds or the list of IP addresses is exhausted.
After calling socket , we fill in an Internet socket address structure with the IP address and port of the server.
While we could move the call to bzero and the subsequent two assignments out of the loop, for efficiency, the code is easier to read as shown.
Establishing the connection with the server is rarely a performance bottleneck for a network client.
If the connection establishment fails, we print an error and close the socket.
Recall that a descriptor that fails a call to connect must be closed and is no longer usable.
If the loop terminates because no call to connect succeeded, the program terminates.
Otherwise, we read the server's response, terminating when the server closes the connection.
If we run this program specifying one of our hosts that is running the daytime server, we get the expected output.
What is more interesting is to run the program to a multihomed system that is not running the daytime server.
The getaddrinfo function handles both name-to-address and serviceto-port translation, and returns sockaddr structures instead of a list of addresses.
These sockaddr structures can then be used by the socket functions directly.
In this way, the getaddrinfo function hides all the protocol dependencies in the library function, which is where they belong.
The application deals only with the socket address structures that are filled in by getaddrinfo.
The POSIX definition of this function comes from an earlier proposal by Keith Sklower for a function named getconninfo.
The observation that specifying a hostname and a service name would suffice for connecting to a service independent of protocol details was made by Marshall Rose in a proposal to X/Open.
This function returns through the result pointer a pointer to a linked list of addrinfo structures, which is defined by including <netdb.h>
The service is either a service name or a decimal port number string.
See also Exercise 11.4, where we want to allow an address string for the host or a port number string for the service.
The members of the hints structure that can be set by the caller are:
The possible values for the ai_flags member and their meanings are:
AI_PASSIVE The caller will use the socket for a passive open.
AI_CANONNAME Tells the function to return the canonical name of the host.
AI_NUMERICHOST Prevents any kind of name-to-address mapping; the hostname argument.
AI_NUMERICSERV Prevents any kind of name-to-service mapping; the service argument must be a decimal port number string.
AI_ADDRCONFIG Only looks up addresses for a given IP version if there is one or more interface that is not a loopback interface configured with an IP address of that version.
There are two ways that multiple structures can be returned:
If there are multiple addresses associated with the hostname, one structure is returned for each address that is usable with the requested address family (the ai_family hint, if specified)
If the service is provided for multiple socket types, one structure can be returned for each socket type, depending on the ai_socktype hint.
For example, if no hints are provided and if the domain service is looked up for a host with two IP addresses, four addrinfo structures are returned:
One for the first IP address and a socket type of SOCK_STREAM.
One for the first IP address and a socket type of SOCK_DGRAM.
One for the second IP address and a socket type of SOCK_STREAM.
One for the second IP address and a socket type of SOCK_DGRAM.
There is no guaranteed order of the structures when multiple items are returned; that is, we cannot assume that TCP services will be returned before UDP services.
Although not guaranteed, an implementation should return the IP addresses in the same order as they are returned by the DNS.
Some resolvers allow the administrator to specify an address sorting order in the /etc/resolv.conf file.
The information returned in the addrinfo structures is ready for a call to socket and then either a call to connect or sendto (for a client), or bind (for a server)
In terms of the DNS, this is normally the FQDN.
Programs like telnet commonly use this flag to be able to print the canonical hostname of the system to which they are connecting, so that if the user supplied a shortcut or an alias, he or she knows what got looked up.
Figure 11.5 shows the returned information if we execute the following:
In this figure, everything except the res variable is dynamically allocated memory (e.g., from malloc)
This port number will be in network byte order in the socket address structures.
It is safest for getaddrinfo to always return the specific protocol.
Multiple addrinfo structures are returned for each IP address only when no ai_socktype hint is provided and the service name is supported by multiple transport protocols (as indicated in the /etc/services file)
If we were to enumerate all 64 possible inputs to getaddrinfo (there are six input variables), many would be invalid and some would make little sense.
On return, a TCP client loops through all returned IP addresses, calling socket and connect for each one, until the connection succeeds or until all addresses have been tried.
For a UDP client, the socket address structure filled in by getaddrinfo would be used in a call to sendto or connect.
If the client can tell that the first address doesn't appear to work (either by receiving an error on a connected UDP socket or by experiencing a timeout on an unconnected socket), additional addresses can be tried.
A typical server specifies the service but not the hostname, and specifies the AI_PASSIVE flag in the hints structure.
If the server wants to malloc another socket address structure to obtain the client's address from accept, the returned ai_addrlen value specifies this size.
A UDP server would call socket, bind, and then recvfrom.
If the server wants to malloc another socket address structure to obtain the client's address from recvfrom, the returned ai_addrlen value specifies this size.
This avoids having multiple structures returned, possibly with the wrong ai_socktype value.
The TCP servers that we have shown so far create one listening socket, and the UDP servers create one datagram socket.
An alternate server design is for the server to handle multiple sockets using select or poll.
In this scenario, the server would go through the entire list of structures returned by getaddrinfo, create one socket per structure, and use select or poll.
But, these two protocols are not completely independent, as we will see in Section 12.2
The problem is that we must allocate a hints structure, initialize it to 0, fill in the desired fields, call getaddrinfo, and then traverse a linked list trying each one.
In the next sections, we will provide some simpler interfaces for the typical TCP and UDP clients and servers that we will write in the remainder of this text.
In Section 11.17, we will describe the reverse function, get nameinfo, which converts socket address structures into hostnames and service names.
The nonzero error return values from getaddrinfo have the names and meanings shown in Figure 11.7
The function gai_strerror takes one of these values as an argument and returns a pointer to the corresponding error string.
All the structures in the linked list are freed, along with any dynamic storage pointed to by those structures (e.g., socket address structures and canonical hostnames)
Assume we call getaddrinfo, traverse the linked list of addrinfo structures, and find the desired structure.
If we then try to save a copy of the information by copying just the addrinfo structure and calling freeaddrinfo, we have a lurking bug.
The reason is that the addrinfo structure itself points to dynamically allocated memory (for the socket address structure and possibly the canonical name), and memory pointed to by our saved structure is returned to the system when freeaddrinfo is called and can be used for something else.
Making a copy of just the addrinfo structure and not the structures that it in turn points to is called a shallow copy.
Copying the addrinfo structure and all the structures that it points to is called a deep copy.
We note the following points before summarizing these return values in Figure 11.8
The address family in the hints structure provided by the caller specifies the type of socket address structure that the caller expects to be returned.
The validity of this string depends on the address family specified by the caller.
But, if AF_UNSPEC is specified, either is acceptable and the appropriate type of socket address structure is returned.
But, another way to obtain this result is to prefix the dotted-decimal string with 0::ffff:
The "Result" column is what we want returned to the caller, given the variables in the first three columns.
The actual number of addrinfo structures returned to the caller also depends on the socket type specified and the service name, as summarized earlier in Figure 11.6
We do not show this test program, as it is about 350 lines of uninteresting code.
It is provided with the source code for the book, as described in the Preface.
The test program outputs information on the variable number of addrinfo structures that are returned, showing the arguments for a call to socket and the address in each socket address structure.
We first show the same example as in Figure 11.5
The -f inet option specifies the address family, -c says to return the canonical name, -h bsdi specifies the hostname, and -s domain specifies the service name.
The common client scenario is to specify the address family, socket type (the -t option), hostname, and service name.
The following example shows this for a multihomed host with three IPv4 addresses:
Next, we specify our host aix, which has both a AAAA record and an A record.
We do not specify the address family, but we provide a service name of ftp, which is provided by TCP only.
Our first interface to getaddrinfo does not require the caller to allocate a hints structure and fill it in.
Instead, the two fields of interest, the address family and the socket type, are arguments to our host_serv function.
Returns: pointer to addrinfo structure if OK, NULL on error.
The function initializes a hints structure, calls getaddrinfo, and returns a null pointer if an error occurs.
We will call this function from Figure 16.17 when we want to use getaddrinfo to obtain the host and service information, but we want to establish the connection ourself.
We will now write two functions that use getaddrinfo to handle most scenarios for the TCP clients and servers that we write.
The first function, tcp_connect, performs the normal client steps: create a TCP socket and connect to a server.
Returns: connected socket descriptor if OK, no return on error.
Try each addrinfo structure until success or end of list.
If connect succeeds, a break is made out of the loop.
Otherwise, when all the addresses have been tried, the loop also terminates.
This function (and our other functions that provide a simpler interface to getaddrinfo in the following sections) terminates if either getaddrinfo fails or no call to connect succeeds.
It would be hard to return an error code (one of the EAI_xxx constants) without adding another argument.
Nevertheless, we still call our wrapper function instead of tcp_connect, to maintain consistency with the remainder of the text.
The problem with the return value is that descriptors are non-negative, but we do not know whether the EAI_xxx values are positive or negative.
If these values were positive, we could return the negative of these values if getaddrinfo fails, but we also have to return some other negative value to indicate that all the structures were tried without success.
We now require a second command-line argument to specify either the service name or the port number, which allows our program to connect to other ports.
All the socket code for this client is now performed by tcp_connect.
We call getpeername to fetch the server's protocol address and print it.
We do this to verify the protocol being used in the examples we are about to show.
Note that tcp_connect does not return the size of the socket address structure that was used for the connect.
We could have added a pointer argument to return this value, but one design goal for this function was to reduce the number of arguments compared to getaddrinfo.
What we do instead is use a sockaddr_storage socket address structure, which is large enough to hold and fulfills the alignment constraints of any socket address type the system supports.
We first specify the name of a host that supports only IPv4
Our next function, tcp_listen , performs the normal TCP server steps: create a TCP socket, bind the server's well-known port, and allow incoming connection requests to be accepted.
Returns: connected socket descriptor if OK, no return on error.
If all the calls to socket and bind fail, we print an error and terminate.
As with our tcp_connect function in the previous section, we do not try to return an error from this function.
The socket is turned into a listening socket by listen.
If the addrlenp argument is non-null, we return the size of the protocol addresses through this pointer.
This allows the caller to allocate memory for a socket address structure to obtain the client's protocol address from accept.
We require a command-line argument to specify either the service name or port number.
This makes it easier to test our server, since binding port 13 for the daytime server requires superuser privileges.
We pass a NULL pointer as the third argument because we don't care what size address structure the address family uses; we will use sockaddr_storage.
We could use the function getnameinfo (Section 11.17 ) to try to obtain the hostname of the client, but that involves a PTR query in the DNS, which can take some time, especially if the PTR query fails.
Since we do not want a server (especially an iterative server) to wait seconds for a PTR query, we just print the IP address and port.
Clients do not have this problem since the client must always specify either an IP address or a hostname.
Client applications normally allow the user to enter this as a command-line argument.
The following calls to inet_pton either fail or succeed, as indicated:
Therefore, if we change our servers to accept an optional argument, and if we enter.
Figure 11.14 shows this final version of our daytime server.
The only change from Figure 11.13 is the handling of the command-line arguments, allowing the user to specify either a hostname or an IP address for the server to bind, in addition to a service name or port.
We first start this server with an IPv4 socket and then connect to the server from clients on two other hosts on the local subnet.
Our functions that provide a simpler interface to getaddrinfo change with UDP because we provide one client function that creates an unconnected UDP socket, and another in the next section that creates a connected UDP socket.
Returns: unconnected socket descriptor if OK, no return on error.
This function creates an unconnected UDP socket, returning three items.
Second, saptr is the address of a pointer (declared by the caller) to a socket address structure (allocated dynamically by udp_client), and in that structure, the function stores the destination IP address and port for future calls to sendto.
The size of the socket address structure is returned in the variable pointed to by lenp.
This final argument cannot be a null pointer (as we allowed for the final argument to tcp_listen) because the length of the socket address structure is required in any calls to sendto and recvfrom.
Memory is allocated for one socket address structure, and the socket address structure corresponding to the socket that was created is copied into the memory.
We call our udp_client function and then print the IP address and port of the server to which we will send the UDP datagram.
We send a one-byte datagram and then read and print the reply.
We need to send only a zero-byte UDP datagram, as what triggers the daytime server's response is just the arrival of a datagram, regardless of its length and contents.
But, many SVR4 implementations do not allow a zero-length UDP datagram.
We run our client specifying a hostname that has a AAAA record and an A record.
Since the structure with the AAAA record is returned first by getaddrinfo, an IPv6 socket is created.
Next, we specify the dotted-decimal address of the same host, resulting in an IPv4 socket.
Returns: connected socket descriptor if OK, no return on error.
With a connected UDP socket, the final two arguments required by udp_client are no longer needed.
The caller can call write instead of sendto, so our function need not return a socket address structure and its length.
One difference, however, is that the call to connect with a UDP socket does not send anything to the peer.
If something is wrong (the peer is unreachable or there is no server at the specified port), the caller does not discover that until it sends a datagram to the peer.
Our final UDP function that provides a simpler interface to getaddrinfo is udp_server.
Returns: unconnected socket descriptor if OK, no return on error.
The arguments are the same as for tcp_listen: an optional hostname, a required service (so its port number can be bound), and an optional pointer to a variable in which the size of the socket address structure is returned.
This function is nearly identical to tcp_listen, but without the call to listen.
Since there is nothing like TCP's TIME_WAIT state for a UDP socket, there is no need to set this socket option when the server is started.
This function is the complement of getaddrinfo: It takes a socket address and returns a character string describing the host and another character string describing the service.
This structure and its length are normally returned by accept, recvfrom, getsockname, or getpeername.
The caller allocates space for the two human-readable strings: host and hostlen specify the host string, and serv and servlen specify the service string.
If the caller does not want the host string returned, a hostlen of 0 is specified.
Similarly, a servlen of 0 specifies not to return information on the service.
The difference between sock_ntop and getnameinfo is that the former does not involve the DNS and just returns a printable version of the IP address and port number.
The latter normally tries to obtain a name for both the host and service.
Figure 11.20 shows the six flags that can be specified to change the operation of getnameinfo.
NI_DGRAM should be specified when the caller knows it is dealing with a datagram socket.
The reason is that given only the IP address and port number in the socket address structure, getnameinfo cannot determine the protocol (TCP or UDP)
There are a few port numbers that are used for one service with TCP and a completely different service with UDP.
An example is port 514, which is the rsh service with TCP, but the syslog service with UDP.
NI_NAMEREQD causes an error to be returned if the hostname cannot be resolved using the DNS.
This can be used by servers that require the client's IP address to be mapped into a hostname.
These servers then take this returned hostname and call getaddrinfo, and then verify that one of the returned addresses is the address in the socket address structure.
NI_NOFQDN causes the returned hostname to be truncated at the first period.
But if this flag was specified to getnameinfo, it would return the hostname as just aix.
NI_NUMERICHOST tells getnameinfo not to call the DNS (which can take time)
Instead, the numeric representation of the IP address is returned as a string, probably by calling inet_ntop.
Servers should normally specify NI_NUMERICSERV because the client port numbers typically have no associated service name—they are ephemeral ports.
The gethostbyname function from Section 11.3 presents an interesting problem that we have not yet examined in the text: It is not re-entrant.
We will encounter this problem in general when we deal with threads in Chapter 26, but it is interesting to examine the problem now (without having to deal with the concept of threads) and to see how to fix it.
If we look at its source code (which is easy since the source code for the entire BIND release is publicly available), we see that one file contains both gethostbyname and gethostbyaddr, and the file has the following general outline:
We highlight the static storage class specifier of the result structure because that is the basic problem.
The fact that these three functions share a single host variable presents yet another.
It has since been deprecated; see Section 11.20 for more detail.
We will ignore the fact that gethostbyname2 is involved when we call gethostbyname, as that doesn't affect this discussion.
The re-entrancy problem can occur in a normal Unix process that calls gethostbyname or gethostbyaddr from both the main flow of control and from a signal handler.
When the signal handler is called (say it is a SIGALRM signal that is generated once per second), the main flow of control of the process is temporarily stopped and the signal handling function is called.
If the main flow of control is in the middle of gethostbyname when it is temporarily stopped (say the function has filled in the host variable and is about to return), and the signal handler then calls gethostbyname, since only one copy of the variable host exists in the process, it is reused.
This overwrites the values that were calculated for the call from the main flow of control with the values calculated for the call from the signal handler.
Historically, gethostbyname, gethostbyaddr, getservbyname, and get servbyport are not re-entrant because all return a pointer to a static structure.
Historically, inet_ntoa is not re-entrant, but some implementations that support threads provide a re-entrant version that uses thread-specific data.
One reason that all the memory for the results is dynamically allocated is to allow it to be re-entrant.
Notice that both result strings (for the hostname and the service name) are allocated by the caller to allow this reentrancy.
Historically, there has been a single copy of this integer variable per process.
If a process makes a system call that returns an error, an integer error code is stored in this variable.
For example, when the function named close in the standard C library is called, it might execute something like the following pseudocode:
Put the argument to the system call (an integer descriptor) into a register.
Put a value in another register indicating the close system call is being called.
Invoke the system call (switch to the kernel with a special instruction)
Test the value of a register to see if an error occurred.
First, notice that if an error does not occur, the value of errno is not changed.
That is why we cannot look at the value of errno unless we know that an error has occurred (normally indicated by the function returning -1)
Assume a program tests the return value of the close function and then prints the value of errno if an error occurred, as in the following:
There is a small window of time between the storing of the error code into errno when the system call returns and the printing of this value by the program, during which another thread of execution within this process (i.e., a signal handler) can change the value of errno.
For example, if, when the signal handler is called, the main flow of control is between close and fprintf and the signal handler calls some other system call that returns an error (say write), then the errno value stored from the write system call overwrites the value stored by the close system call.
In looking at these two problems with regard to signal handlers, one solution to the problem with gethostbyname (returning a pointer to a static variable) is to not call nonre-entrant functions from a signal handler.
The problem with errno (a single global variable that can be changed by the signal handler) can be avoided by coding the signal handler to save and restore the value of errno in the signal handler as follows:
In this example code, we also call fprintf, a standard I/O function, from the signal handler.
This is yet another re-entrancy problem because many versions of the standard I/O library are nonreentrant: Standard I/O functions should not be called from signal handlers.
We will revisit this problem of re-entrancy in Chapter 26 and we will see how threads handle the problem of the errno variable.
The next section describes some reentrant versions of the hostname functions.
There are two ways to make a nonre-entrant function such as gethostbyname re-entrant.
Instead of filling in and returning a static structure, the caller allocates the structure and the re-entrant function fills in the caller's structure.
This is the technique used in going from the nonre-entrant gethostbyname to the re-entrant gethostbyname_r.
But, this solution gets more complicated because not only must the caller provide the hostent structure to fill in, but this structure also points to other information: the canonical name, the array of alias pointers, the alias strings, the array of address pointers, and the addresses (e.g., Figure 11.2)
The caller must provide one large buffer that is used for this additional information and the hostent structure that is filled in then contains numerous pointers into this other buffer.
This adds at least three arguments to the function: a pointer to the hostent structure to fill in, a pointer to the buffer to use for all the other information, and the size of this buffer.
A fourth additional argument is also required: a pointer to an integer in which an error code can be stored, since the global integer h_errno can no longer be used.
The global integer h_errno presents the same re-entrancy problem that we described with errno.
The re-entrant function calls malloc and dynamically allocates the memory.
The problem with this approach is that the application calling this function must also call freeaddrinfo to free the dynamic memory.
If the free function is not called, a memory leak occurs: Each time the process calls the function that allocates the memory, the memory use of the process increases.
If the process runs for a long time (a common trait of network servers), the memory usage just grows and grows over time.
We will now discuss the Solaris 2.x re-entrant functions for name-to-address and address-toname resolution.
On success, this pointer is also the return value of the function.
So, a buffer size of 8192 bytes should be adequate.
Unfortunately, this problem of re-entrancy is even worse than it appears.
First, there is no standard regarding re-entrancy and gethostbyname and gethostbyaddr.
The POSIX specification says that gethostbyname and gethostbyaddr need not be re-entrant.
Unix 98 just says that these two functions need not be thread-safe.
Linux provides similar _r functions, except that instead of returning the hostent as the return value of the function, the hostent is returned using a value-result parameter as the next to last function argument.
It returns the success of the lookup as the return value from the function as well as in the h_errno argument.
Lastly, while a re-entrant version of gethostbyname may provide safety from different threads calling it at the same time, this says nothing about the re-entrancy of the underlying resolver functions.
This section briefly describes some of the old API to assist in the conversion of programs using the old API.
This API was not very portable since systems that used a different internal resolver interface had to mimic the BIND resolver interface to provide it.
Enabling RES_USE_INET6 caused gethostbyname to look up AAAA records first, and only look up A records if a name had no AAAA records.
The gethostbyname2 function adds an address family argument to gethostbyname.
Returns: non-null pointer if OK, NULL on error with h_errno set.
It introduced the getipnodebyname function to solve some of these problems.
Returns: non-null pointer if OK, NULL on error with error_num set.
This function returns a pointer to the same hostent structure that we described with gethostbyname.
For thread safety, the return value is dynamically allocated, so it must be freed with the freehostent function.
All four types of information can be stored in a file and three functions are defined for each of the four types:
A getXXXent function that reads the next entry in the file, opening the file if necessary.1
A setXXXent function that opens (if not already open) and rewinds the file.2
Each of the four types of information defines its own structure, and the following definitions are provided by including the <netdb.h> header: the hostent, netent, protoent, and servent structures.
In addition to the three get, set, and end functions, which allow sequential processing of the file, each of the four types of information provides some keyed lookup functions.
These functions go through the file sequentially (calling the getXXXent function to read each line), but instead of returning each line to the caller, these functions look for an entry that matches an argument.
These keyed lookup functions have names of the form getXXXbyYYY.
For example, the two keyed lookup functions for the host information are gethostbyname (look for an entry that matches a hostname) and gethostbyaddr (look for an entry that matches an IP address)
How does this apply when the DNS is being used? First, only the host and network information is available through the DNS.
The protocol and service information is always read from the corresponding file.
We mentioned earlier in this chapter (with Figure 11.1) that different implementations employ different ways for the administrator to specify whether to use the DNS or a file for the host and network information.
Second, if the DNS is being used for the host and network information, then only the keyed lookup functions make sense.
You cannot, for example, use gethostent and expect to sequence through all entries in the DNS! If gethostent is called, it reads only the /etc/hosts file and avoids the DNS.
Although the network information can be made available through the DNS, few people set.
Typically, however, administrators build and maintain an /etc/networks file and it is used instead of the DNS.
The netstat program with the -i option uses this file, if present, and prints the name for each network.
However, classless addressing (Appendix A) makes these functions fairly useless, and these functions do not support IPv6 at all, so new applications should avoid using network names.
The set of functions that an application calls to convert a hostname into an IP address and vice versa is called the resolver.
The two functions gethostbyname and gethostbyaddr are the historical entry points.
The commonly used function dealing with service names and port numbers is getservbyname, which takes a service name and returns a structure containing the port number.
Additional functions exist to map protocol names into protocol numbers and network names into network numbers, but these are rarely used.
Another alternative that we have not mentioned is calling the resolver functions directly, instead of using gethostbyname and gethostbyaddr.
One program that invokes the DNS this way is sendmail, which searches for an MX record, something that the gethostby XXX functions cannot do.
First run the program specifying a hostname with just one IP address and then run the program specifying a hostname that has more than one IP address.
Does your resolver allow this? Modify Figure 11.4 to allow a dotted-decimal IP address as the hostname and a decimal port number string as the service name.
In testing the IP address for either a dotted-decimal string or a hostname, in what order should these two tests be performed?
That is, call gethostbyaddr using the IP address returned by recvfrom, followed by gethostbyname to find all the IP addresses for the host.
If the caller does not do this (i.e., passes a null pointer as the final argument), how can the caller still obtain the actual size of the protocol's addresses?
Start one instance of the server in one window, binding the wildcard address and some port of your choosing.
Start a client in another window and verify that this server is handling the client (note the printf in the server)
Next, start another instance of the server in another window, this time binding one of the host's unicast addresses and the same port as the first server.
What problem do you immediately encounter? Fix this problem and restart this second server.
Start a client, send a datagram, and verify that the second server has stolen the port from the first server.
If possible, start the second server again from a different login account on the first server to see if the stealing still succeeds.
Some vendors will not allow the second bind unless the user ID is the same as that of the process that has already bound the port.
Knowing that a client goes through the two steps gethostbyname and connect, which lines output by the client indicate which steps?
Write a new function named getnameinfo_timeo that takes an additional integer argument specifying the maximum number of seconds to wait for a reply.
We will see how this is done in this chapter.
Hosts and routers will probably run like this for many years into the transition to IPv6
At some point, many systems will be able to turn off their IPv4 stack, but only time will tell when (and if) that will occur.
We will not say much more about the two scenarios where the client and server use the same protocol.
The interesting cases are when the client and server use different protocols.
The server on the right is written using IPv6 and it is running on a dual-stack host.
We assume the clients and server are on the same Ethernet.
We assume both clients send SYN segments to establish a connection with the server.
Appendix A talks more about the formats and contents of these headers.
The receiving datalink looks at the Ethernet type field and passes each frame to the appropriate IP module.
The IPv4 client calls gethostbyname and finds an A record for the server.
The server host will have both an A record and a AAAA record since it supports both protocols, but the IPv4 client asks for only an A record.
Therefore, all communication between this client and server takes place using IPv4 datagrams.
This will work until all the IPv4 addresses are taken.
The scenario is similar for an IPv6 UDP server, but the address format can change for each datagram.
This address format tells the kernel to send an IPv4 datagram to the client.
If the server responds, the kernel will generate an IPv6 datagram.
These are the two arrows labeled "IPv4" in the figure: one to TCP and one to UDP.
These are the two arrows labeled "IPv6" in the figure: one to TCP and one to UDP.
Most dual-stack hosts should use the following rules in dealing with listening sockets:
We now swap the protocols used by the client and server from the example in the previous section.
First consider an IPv6 TCP client running on a dual-stack host.
The kernel detects the mapped address and automatically sends an IPv4 SYN to the server.
Processing of client requests, depending on address type and socket type.
These are the two arrows labeled "IPv4" in the figure.
These are the two arrows labeled "IPv6" in the figure.
Figure 12.5 summarizes this section and the previous section, plus the combinations of clients and servers.
The third column on the final row is marked with an asterisk because interoperability depends on the address chosen by the client.
Choosing the AAAA record and sending an IPv6 datagram will not work.
If we therefore remove the second row and the second column, all of the "(no)" entries disappear and the only problem is the entry with the asterisk.
All return: nonzero if IPv6 address is of specified type, zero otherwise.
The first seven macros test the basic type of IPv6 address.
IPv4-compatible addresses are used by a transition mechanism that has fallen out of favor.
You're not likely to actually see this type of address or need to test for it.
As an example of an application that needs this macro, consider FTP and its PORT command.
If we start an FTP client, log in to an FTP server, and issue an FTP dir command, the FTP client sends a PORT command to the FTP server across the control connection.
This tells the server the client's IP address and port, to which the server then creates a data connection.
Many of the changes that we showed could be done automatically using some editing scripts.
Programs that are more dependent on IPv4, using features such as multicasting, IP options, or raw sockets, will take more work to convert.
The problem with this approach is that the code becomes littered with #ifdefs very quickly, and is harder to follow and maintain.
The first step is to remove calls to gethostbyname and gethostbyaddr and use the getaddrinfo and getnameinfo functions that we described in the previous chapter.
This lets us deal with socket address structures as opaque objects, referenced by a pointer and size, which is exactly what the basic socket functions do: bind, connect, recvfrom, and so on.
We handle this in the helper functions described in the previous chapter by ignoring the error from socket and trying the next address on the list returned by the name server.
Assuming the peer has an A record, and that the name server returns the A record in addition to any AAAA records, the creation of an IPv4 socket will succeed.
This is the type of functionality that belongs in a library function, and not in the source code of every application.
However, the semantics were never completely described, and it was only useful in very specific cases, so it was removed in the next revision of the API.
IPv4 FTP server, make sure the client is in "active" mode (perhaps issuing the passive command to turn off "passive" mode), issue the debug command, and then the dir command.
Next, perform the same operations, but to an IPv6 server, and compare the PORT commands issued as a result of the dir commands.
Start the IPv4 program, specifying the wildcard address as the argument.
A daemon is a process that runs in the background and is not associated with a controlling terminal.
The lack of a controlling terminal is typically a side effect of being started by a system initialization script (e.g., at boot-time)
But if a daemon is started by a user typing to a shell prompt, it is important for the daemon to disassociate itself from the controlling terminal to avoid any unwanted interraction with job control, terminal session management, or simply to avoid unexpected output to the terminal from the daemon as it runs in the background.
During system startup, many daemons are started by the system initialization scripts.
A few network servers are often started from these scripts: the inetd superserver (covered later in this chapter), a Web server, and a mail server (often sendmail)
The syslogd daemon that we will describe in Section 13.2 is normally started by one of these scripts.
The execution of programs on a regular basis is performed by the cron daemon, and programs that it invokes run as daemons.
The cron daemon itself is started in Step 1 during system startup.
The execution of a program at one time in the future is specified by the at command.
The cron daemon normally initiates these programs when their time arrives, so these programs run as daemons.
Daemons can be started from user terminals, either in the foreground or in the background.
This is often done when testing a daemon, or restarting a daemon that was terminated for some reason.
Since a daemon does not have a controlling terminal, it needs some way to output messages when something happens, either normal informational messages or emergency messages that need to be handled by an administrator.
The syslog function is the standard way to output these messages, and it sends the messages to the syslogd daemon.
Unix systems normally start a daemon named syslogd from one of the system initializations scripts, and it runs as long as the system is up.
Berkeley-derived implementations of syslogd perform the following actions on startup:
These messages can be appended to a file (a special case of which is the file /dev/console, which writes the message to the console), written to a specific user (if that user is logged in), or forwarded to the syslogd daemon on another host.
A Unix domain socket is created and bound to the pathname /var/run/log (/dev/log on some systems)
Any error messages from within the kernel appear as input on this device.
If the daemon receives the SIGHUP signal, it rereads its configuration file.
We could send log messages to the syslogd daemon from our daemons by creating a Unix domain datagram socket and sending our messages to the pathname that the daemon has bound, but an easier interface is the syslog function that we will describe in the next section.
Newer implementations disable the creation of the UDP socket, unless specified by the administrator, as allowing anyone to send UDP datagrams to this port opens the system up to denial-of-service attacks, where someone could fill up the filesystem (e.g., by filling up log files) or cause log messages to be dropped (e.g., by overflowing syslog's socket receive buffer)
For example, Unix domain sockets are used by Berkeley-derived implementations, but System V implementations use a STREAMS log driver.
Different Berkeley-derived implementations use different pathnames for the Unix domain socket.
We can ignore all these details if we use the syslog function.
Since a daemon does not have a controlling terminal, it cannot just fprintf to stderr.
The common technique for logging messages from a daemon is to call the syslog function.
Although this function was originally developed for BSD systems, it is provided by virtually all Unix vendors today.
The description of syslog in the POSIX specification is consistent with what we describe here.
The message is like a format string to printf, with the addition of a %m specification, which is replaced with the error message corresponding to the current value of errno.
A newline can appear at the end of the message, but is not mandatory.
If no level is specified by the sender, LOG_NOTICE is the default.
Log messages also contain a facility to identify the type of process sending the message.
For example, the following call could be issued by a daemon when a call to the rename function unexpectedly fails:
The purpose of facility and level is to allow all messages from a given facility to be handled the same in the /etc/syslog.conf file, or to allow all messages of a given level to be handled the same.
When the application calls syslog the first time, it creates a Unix domain datagram socket and then calls connect to the well-known pathname of the socket created by the syslogd daemon (e.g., /var/run/log)
The options argument is formed as the logical OR of one or more of the constants in Figure 13.3
Normally the Unix domain socket is not created when openlog is called.
Instead, it is opened during the first call to syslog.
The LOG_NDELAY option causes the socket to be created when openlog is called.
The facility argument to openlog specifies a default facility for any subsequent calls to syslog that do not specify a facility.
Some daemons call openlog and specify the facility (which normally does not change for a given daemon)
They then specify only the level in each call to syslog (since the level can change depending on the error)
Log messages can also be generated by the logger command.
This can be used from within shell scripts, for example, to send messages to syslogd.
This function should be suitable for use on all variants of Unix, but some offer a C library function called daemon that provides similar features.
We first call fork and then the parent terminates, and the child continues.
If the process was started as a shell command in the foreground, when the parent terminates, the shell thinks the command is done.
Also, the child inherits the process group ID from the parent but gets its own process ID.
This guarantees that the child is not a process group leader, which is required for the next call to setsid.
Chapter 9 of APUE talks about process relationships and sessions in detail.
The process becomes the session leader of the new session, becomes the process group leader of a new process group, and has no controlling terminal.
When this function returns, the parent is really the first child and it terminates, leaving the second child running.
The purpose of this second fork is to guarantee that the daemon cannot automatically acquire a controlling terminal should it open a terminal device in the future.
When a session leader without a controlling terminal opens a terminal device (that is not currently some other session's controlling terminal), the terminal becomes the controlling terminal of the session leader.
But by calling fork a second time, we guarantee that the second child is no longer a session leader, so it cannot acquire a controlling terminal.
We must ignore SIGHUP because when the session leader terminates (the first child), all processes in the session (our second child) receive the SIGHUP signal.
This saves us from having to go through all our code and call one of our error functions if the server is not being run as a daemon (i.e., when we are testing the server), but call syslog if it is being run as a daemon.
For example, a printer daemon might change to the printer's spool directory, where it does all its work.
Should the daemon ever generate a core file, that file is generated in the current working directory.
Another reason to change the working directory is that the daemon could have been started in any filesystem, and if it remains there, that filesystem cannot be unmounted (at least not without using some potentially destructive, forceful measures)
We close any open descriptors that are inherited from the process that executed the daemon (normally a shell)
The problem is determining the highest descriptor in use: There is no Unix function that provides this value.
Our solution is to close the first 64 descriptors, even though most of these are probably not open.
Solaris provides a function called closefrom for use by daemons to solve this problem.
We open/dev/null for standard input, standard output, and standard error.
This guarantees that these common descriptors are open, and a read from any of these descriptors returns 0 (EOF), and the kernel just discards anything written to them.
The reason for opening these descriptors is so that any library function called by the daemon that assumes it can read from standard input or write to either standard output or standard error will not fail.
If the daemon ends up opening a socket to a client, that socket descriptor ends up as stdout or stderr and some erroneous call to something like perror then sends unexpected data to a client.
The first argument is from the caller and is normally the name of the program (e.g., argv[0])
We specify that the process ID should be added to each log message.
We note that since a daemon runs without a controlling terminal, it should never receive the SIGHUP signal from the kernel.
Therefore, many daemons use this signal as a notification from the administrator that the daemon's configuration file has changed, and the daemon should reread the file.
Two other signals that a daemon should never receive are SIGINT and SIGWINCH, so daemons can safely use these signals as another way for administrators to indicate some change that the daemon should react to.
Indeed, if we want our programs to be able to run as a daemon, we must avoid calling the printf and fprintf functions and use our err_msg function instead.
Note how we check argc and issue the appropriate usage message before calling daemon_init.
This allows the user starting the daemon to get immediate feedback if the command has the incorrect number of arguments.
After calling daemon_init, all subsequent error messages go to syslog.
If we run this program on our Linux host linux and then check the /var/log/messages file (where we send all LOG_USER messages) after connecting from the same machine (e.g., localhost), we have.
The date, time, and hostname are prefixed automatically by the syslogd daemon.
On a typical Unix system, there could be many servers in existence, just waiting for a client request to arrive.
With systems before 4.3BSD, each of these services had a process associated with it.
This process was started at boottime from the file /etc/rc, and each process did nearly identical startup tasks: create a socket, bind the server's well-known port to the socket, wait for a connection (if TCP) or a datagram (if UDP), and then fork.
The child process serviced the client and the parent waited for the next client request.
All these daemons contained nearly identical startup code, first with respect to socket creation, and also with respect to becoming a daemon process (similar to our daemon_init function)
Each daemon took a slot in the process table, but each daemon was asleep most of the time.
The 4.3BSD release simplified this by providing an Internet superserver: the inetd daemon.
This daemon can be used by servers that use either TCP or UDP.
It does not handle other protocols, such as Unix domain sockets.
It simplifies writing daemon processes since most of the startup details are handled by inetd.
This obviates the need for each server to call our daemon_init function.
It allows a single process (inetd) to be waiting for incoming client requests for multiple services, instead of one process for each service.
This reduces the total number of processes in the system.
The inetd process establishes itself as a daemon using the techniques that we described with our daemon_init function.
It then reads and processes its configuration file, typically /etc/inetd.conf.
This file specifies the services that the superserver is to handle, and what to do when a service request arrives.
The actual name of the server is always passed as the first argument to a program when it is execed.
Examples are the ability to handle RPC servers, in addition to TCP and UDP servers, and the ability to handle protocols other than TCP and UDP.
Also, the pathname to exec and the command-line arguments to the server obviously depend on the implementation.
In general, it specifies whether the daemon started by inetd intends to take over the listening socket associated with the service.
The interaction of IPv6 with /etc/inetd.conf depends on the vendor and special attention to detail is required to get what you want.
These special protocol names do not typically appear in the /etc/protocols file.
A picture of what the inetd daemon does is shown in Figure 13.7
On startup, it reads the /etc/inetd.conf file and creates a socket of the appropriate type (stream or datagram) for all the services specified in the file.
The maximum number of servers that inetd can handle depends on the maximum number of descriptors that inetd can create.
Each new socket is added to a descriptor set that will be used in a call to select.
This TCP or UDP port number is obtained by calling getservbyname with the service-name and protocol fields from the configuration file as arguments.
For TCP sockets, listen is called so that incoming connection requests are accepted.
After all the sockets are created, select is called to wait for any of the sockets to become readable.
Recall from Section 6.3 that a listening TCP socket becomes readable when a new connection is ready to be accepted and a UDP socket becomes readable when a datagram arrives.
When select returns that a socket is readable, if the socket is a TCP socket and the nowait flag is given, accept is called to accept the new connection.
The inetd daemon forks and the child process handles the service request.
This is similar to a standard concurrent server (Section 4.8)
The child closes all descriptors except the socket descriptor it is handling: the new connected socket returned by accept for a TCP server or the original UDP socket.
If the child reads from standard input, it is reading from the socket and anything it writes to standard output or standard error is written to the socket.
The child calls getpwnam to get the password file entry for the login-name specified in the configuration file.
If this field is not root, then the child becomes the specified user by executing the setgid and setuid function calls.
Since the inetd process is executing with a user ID of 0, the child process inherits this user ID across the fork, and is able to become any user that it chooses.
The child process now does an exec to execute the appropriate server-program to handle the request, passing the arguments specified in the configuration file.
If the socket is a stream socket, the parent process must close the connected socket (like our standard concurrent server)
The parent calls select again, waiting for the next socket to become readable.
If we look in more detail at the descriptor handling that is taking place, Figure 13.8 shows the descriptors in inetd when a new connection request arrives from an FTP client.
The connection request is directed to TCP port 21, but a new connected socket is created by.
Figure 13.9 shows the descriptors in the child, after the call to fork, after the child has closed all the descriptors except the connected socket.
These should be the only descriptors open in the server.
The scenario we have described handles the case where the configuration file specifies nowait for the server.
This is typical for all TCP services and it means that inetd need not wait for its child to terminate before accepting another connection for that service.
If another connection request arrives for the same service, it is returned to the parent process as soon as it calls select again.
Specifying the wait flag for a datagram service changes the steps done by the parent process.
This flag says that inetd must wait for its child to terminate before selecting on this socket again.
When fork returns in the parent, the parent saves the process ID of the child.
This allows the parent to know when this specific child process terminates, by looking at the value returned by waitpid.
The parent disables the socket from future selects by using the FD_CLR macro to turn off the bit in its descriptor set.
This means that the child process takes over the socket until it terminates.
When the child terminates, the parent is notified by a SIGCHLD signal, and the parent's signal handler obtains the process ID of the terminating child.
It reenables select for the corresponding socket by turning on the bit in its descriptor set for this socket.
The reason that a datagram server must take over the socket until it terminates, preventing inetd from selecting on that socket for readability (awaiting another client datagram), is because there is only one socket for a datagram server, unlike a TCP server that has a listening socket and one connected socket per client.
If inetd did not turn off readability for the datagram socket, and if the parent (inetd) executed before the child, then the datagram from the client would still be in the socket receive buffer, causing select to return readable again, causing inetd to fork another (unneeded) child.
The way that inetd knows when that child is finished with the socket is by receiving SIGCHLD, indicating that the child has terminated.
We will show an example of this in Section 22.7
Since inetd is the process that calls accept for a TCP server, the actual server that is invoked by inetd normally calls getpeername to obtain the IP address and port number of the client.
Recall Figure 4.18 where we showed that after a fork and an exec (which is what inetd does), the only way for the actual server to obtain the identify of the client is to call getpeername.
In this mode, the process control cost for each client connection is just a fork, while the cost for a TCP server invoked by inetd is a fork and an exec.
It is now common to find an extended Internet services daemon, called xinetd, on Linux and other systems.
Those features include options for logging, accepting or rejecting connections based on the client's address, configuring services one-per-file instead of a single monolithic configuration, and many more.
It is not described further here since the basic superserver idea behind them both is the same.
This function is trivial compared to daemon_init , because all of the daemonization steps are performed by inetd when it starts.
First, all the socket creation code is gone: the calls to tcp_listen and to accept.
Those steps are done by inetd and we reference the TCP connection using descriptor 0 (standard input)
Second, the infinite for loop is gone because we are invoked once per client connection.
Since we do not call tcp_listen , we do not know the size of the socket address structure it returns, and since we do not call accept , we do not know the client's protocol address.
To run this example on our Solaris system, we first assign the service a name and port, adding the following line to /etc/services :
We place the executable in the specified location and send the SIGHUP signal to inetd , telling it to reread its configuration file.
The /var/adm/messages file (where we have directed the LOG_USER facility messages to be logged in our /etc/syslog.conf file) contains the following entry:
Daemons are processes that run in the background independent of control from all terminals.
All output from a daemon is normally sent to the syslogd daemon by calling the syslog function.
The administrator then has complete control over what happens to these messages, based on the daemon that sent the message and the severity of the message.
To start an arbitrary program and have it run as a daemon requires a few steps: Call fork to run in the background, call setsid to create a new POSIX session and become the session leader, fork again to avoid obtaining a new controlling terminal, change the working directory and the file mode creation mask, and close all unneeded files.
It handles all the required daemonization steps, and when the actual server is started, the socket is open on standard input, standard output, and standard error.
This lets us omit calls to socket, bind, listen, and accept, since all these steps are handled by inetd.
This man page also says that inetd cannot do this for a UDP service.
Is there a way around this limitation for UDP services?
This chapter covers a variety of functions and techniques that we lump into the category of "advanced I/O." First is setting a timeout on an I/O operation, which can be done in three different ways.
Next are three more variations on the read and write functions: recv and send, which allow a fourth argument that contains flags from the process to the kernel, readv and writev, which let us specify a vector of buffers to input into or output from, and recvmsg and sendmsg, which combine all the features from the other I/O functions along with the new capability of receiving and sending ancillary data.
We also consider how to determine the amount of data in the socket receive buffer, how to use the C standard I/O library with sockets, and we discuss some advanced ways to wait for events.
There are three ways to place a timeout on an I/O operation involving a socket:
Call alarm, which generates the SIGALRM signal when the specified time has expired.
This involves signal handling, which can differ from one implementation to the next, and it may interfere with other existing calls to alarm in the process.
Block waiting for I/O in select, which has a time limit built-in, instead of blocking in a call to read or write.
The problem with this approach is that not all implementations support these two socket options.
All three techniques work with input and output operations (e.g., read, write, and other variations such as recvfrom and sendto), but we would also like a technique that we can use with connect, since a TCP connect can take a long time to time out (typically 75 seconds)
We also note that the first two techniques work with any descriptor, while the third technique works only with socket descriptors.
The first three arguments are the three required by connect and the fourth argument is the number of seconds to wait.
The current signal handler (if any) is saved, so we can restore it at the end of the function.
The alarm clock for the process is set to the number of seconds specified by the caller.
The return value from alarm is the number of seconds currently remaining in the alarm clock for the process (if one has already been set by the process) or 0 (if there is no current alarm)
In the former case we print a warning message since we are wiping out that previously set alarm (see Exercise 14.2)
The socket is closed to prevent the three-way handshake from continuing.
The alarm is turned off by setting it to 0 and the previous signal handler (if any) is restored.
The signal handler just returns, assuming this return will interrupt the pending connect, causing connect to return an error of EINTR.
One point to make with this example is that we can always reduce the timeout period for a connect using this technique, but we cannot extend the kernel's existing timeout.
Berkeley-derived kernel the timeout for a connect is normally 75 seconds.
Another point with this example is that we use the interruptibility of the system call (connect) to return before the kernel's time limit expires.
This is fine when we perform the system call and can handle the EINTR error return.
But in Section 29.7, we will encounter a library function that performs the system call, and the library function reissues the system call when EINTR is returned.
We can still use SIGALRM in this scenario, but we will see in Figure 29.10 that we also have to use sigsetjmp and siglongjmp to get around the library's ignoring of EINTR.
Although this example is fairly simple, signals are quite difficult to use correctly with multithreaded programs (see Chapter 26)
So, the technique shown here is only recommended for single-threaded programs.
We establish a signal handler for SIGALRM and then call alarm for a five-second timeout before each call to recvfrom.
If recvfrom is interrupted by our signal handler, we print a message and continue.
If a line is read from the server, we turn off the pending alarm and print the reply.
Our signal handler just returns, to interrupt the blocked recvfrom.
This example works correctly because we are reading only one reply each time we establish an alarm.
In Section 20.4, we will use the same technique, but since we are reading multiple replies for a given alarm, a race condition exists that we must handle.
We demonstrate the second technique for setting a timeout (using select) in Figure 14.3
It shows our function named readable_timeo which waits up to a specified number of seconds for a descriptor to become readable.
The bit corresponding to the descriptor is turned on in the read descriptor set.
A timeval structure is set to the number of seconds that the caller wants to wait.
This function does not perform the read operation; it just waits for the descriptor to be ready for reading.
Therefore, this function can be used with any type of socket, TCP or UDP.
It is trivial to create a similar function named writable_timeo that waits for a descriptor to become writable.
This new version calls recvfrom only when our readable_timeo function returns a positive value.
We do not call recvfrom until the function readable_timeo tells us that the descriptor is readable.
We set this option once for a descriptor, specifying the timeout value, and this timeout then applies to all read operations on that descriptor.
The nice thing about this method is that we set the option only once, compared to the previous two methods, which required doing something before every operation on which we wanted to place a time limit.
But this socket option applies only to read operations, and the similar option SO_SNDTIMEO applies only to write operations; neither socket option can be used to set a timeout for a connect.
The fourth argument to setsockopt is a pointer to a timeval structure that is filled in with the desired timeout.
If the I/O operation times out, the function (recvfrom, in this case) returns EWOULDBLOCK.
These two functions are similar to the standard read and write functions, but one additional argument is required.
Both return: number of bytes read or written if OK, –1 on error.
The first three arguments to recv and send are the same as the first three arguments to read and write.
MSG_DONTROUTE This flag tells the kernel that the destination is on a locally attached network and not to perform a lookup of the routing table.
This feature can be enabled for a single output operation with the MSG_DONTROUTE flag, or enabled for all output operations for a given socket using the socket option.
MSG_DONTWAIT This flag specifies nonblocking for a single I/O operation, without having to turn on the nonblocking flag for the socket, perform the I/O operation, and then turn off the nonblocking flag.
We will describe nonblocking I/O in Chapter 16, along with turning the nonblocking flag on and off for all I/O operations on a socket.
This flag is newer than the others and might not be supported on all systems.
MSG_OOB With send, this flag specifies that out-of-band data is being sent.
With recv, this flag specifies that out-of-band data is to be read instead of normal data.
MSG_PEEK This flag lets us look at the data that is available to be read, without having the system discard the data after the recv or recvfrom returns.
It tells the kernel not to return from a read operation until the requested number of bytes have been read.
Even if we specify MSG_WAITALL, the function can still return fewer than the requested number of bytes if (i) a signal is caught, (ii) the connection is terminated, or (iii) an error is pending for the socket.
There are additional flags used by other protocols, but not TCP/IP.
For example, the OSI transport layer is record-based (not a byte stream such as TCP) and supports the MSG_EOR flag for output operations to specify the end of a logical record.
There is a fundamental design problem with the flags argument: It is passed by value; it is not a value-result argument.
Therefore, it can be used only to pass flags from the process to the kernel.
This is not a problem with TCP/IP, because it is rare to need to pass flags back to the process from the kernel.
Thus, the decision was made with 4.3BSD Reno to leave the arguments to the commonly used input functions (recv and recvfrom) as-is and change the msghdr structure that is used with recvmsg and sendmsg.
This also means that if a process needs to have the flags updated by the kernel, the process must call recvmsg instead of either recv or recvfrom.
These two functions are similar to read and write, but readv and writev let us read into or write from one or more buffers with a single function call.
These operations are called scatter read (since the input data is scattered into multiple application buffers) and gather write (since multiple buffers are gathered for a single output operation)
Both return: number of bytes read or written, –1 on error.
The second argument to both functions is a pointer to an array of iovec structures, which is defined by including the <sys/uio.h> header.
The datatypes shown for the members of the iovec structure are those specified by POSIX.
There is some limit to the number of elements in the array of iovec structures that an implementation allows.
The readv and writev functions can be used with any descriptor, not just sockets.
For a record-based protocol such as UDP, one call to writev generates a single UDP datagram.
These two functions are the most general of all the I/O functions.
Indeed, we could replace all calls to read, readv, recv, and recvfrom with calls to recvmsg.
Similarly all calls to the various output functions could be replaced with calls to sendmsg.
Both return: number of bytes read or written if OK, –1 on error.
The msghdr structure that we show is the one specified in POSIX.
Some systems still use an older msghdr structure that originated with 4.2BSD.
The newer form of the msghdr structure is often available using conditional compilation flags.
The only form of ancillary data supported by the older structure is the passing of file descriptors (called access rights)
They are similar to the fifth and sixth arguments to recvfrom and sendto: msg_name points to a socket address structure in which the caller stores the destination's protocol address for sendmsg, or in which recvmsg stores the sender's protocol address.
If a protocol address does not need to be specified (e.g., a TCP socket or a connected UDP socket), msg_name should be set to a null pointer.
With recvmsg and sendmsg, we must distinguish between two flag variables: the flags argument, which is passed by value, and the msg_flags member of the msghdr structure, which is passed by reference (since the address of the structure is passed to the function)
This value is then updated based on the result of recvmsg.
The msg_flags member is ignored by sendmsg because this function uses the flags argument to drive its output processing.
There is no column for sendmsg msg_flags because, as we mentioned, it is not used.
Summary of input and output flags by various I/O functions.
The first four flags are only examined and never returned; the next two are both examined and returned; and the last four are only returned.
The following comments apply to the six flags returned by recvmsg:
MSG_BCAST This flag is relatively new, supported by at least BSD, and is returned if the datagram was received as a link-layer broadcast or with a destination IP address that is a broadcast address.
This flag is a better way of determining if a UDP datagram was sent to a broadcast address, compared to the IP_RECVDSTADDR socket option.
MSG_MCAST This flag is also a fairly recent addition, supported by at least BSD, and is returned if the datagram was received as a link-layer multicast.
MSG_EOR This flag is cleared if the returned data does not end a logical record; the flag is turned on if the returned data ends a logical record.
MSG_OOB This flag is never returned for TCP out-of-band data.
This flag is returned by other protocol suites (e.g., the OSI protocols)
MSG_NOTIFICATON This flag is returned for SCTP receivers to indicate that the message read is an event notification, not a data message.
Figure 14.8 shows a msghdr structure and the various information it points to.
We assume in this figure that the process is about to call recvmsg for a UDP socket.
Data structures when recvmsg is called for a UDP socket.
Sixteen bytes are allocated for the protocol address and 20 bytes are allocated for the ancillary data.
We also assume that the IP_RECVDSTADDR socket option has been set for the socket, to receive the destination IP address from the UDP datagram.
Figure 14.9 shows all the information in the msghdr structure when recvmsg returns.
The buffer pointed to by msg_name has been filled in as an Internet socket address structure, containing the source IP address and source UDP port from the received datagram.
The last 70 bytes of the final buffer are not modified.
The buffer pointed to by msg_control is filled in as a cmsghdr structure.
The msg_controllen member is updated with the actual amount of ancillary data that was stored.
The msg_flags member is updated by recvmsg, but there are no flags to return to the process.
Figure 14.10 summarizes the differences among the five groups of I/O functions we described.
In this section, we will describe the concept and show the structure and macros used to build and process ancillary data, but we will save the code examples for later chapters that describe the actual uses of ancillary data.
Figure 14.11 is a summary of the various uses of ancillary data we cover in this text.
The OSI protocol suite also uses ancillary data for various purposes we do not discuss in this text.
Ancillary data consists of one or more ancillary data objects, each one beginning with a cmsghdr structure, defined by including <sys/socket.h>
The ancillary data pointed to by msg_control must be suitably aligned for a cmsghdr structure.
We will show one way to do this in Figure 15.11
Figure 14.12 shows an example of two ancillary data objects in the control buffer.
Each object is preceded by a cmsghdr structure that describes the object.
There can be padding between the cmsg_type member and the actual data, and there can also be.
The five CMSG_xxx macros we describe shortly account for this possible padding.
Not all implementations support multiple ancillary data objects in the control buffer.
In this figure, we assume each of the three members of the cmsghdr structure occupies four bytes and there is no padding between the cmsghdr structure and the actual data.
When descriptors are passed, the contents of the cmsg_data array are the actual descriptor values.
In this figure, we show only one descriptor being passed, but in general, more than one can be.
Since the ancillary data returned by recvmsg can contain any number of ancillary data objects, and to hide the possible padding from the application, the following five macros are defined by including the <sys/socket.h> header to simplify the processing of the ancillary data:
Returns: pointer to first cmsghdr structure or NULL if no ancillary data.
Returns: pointer to next cmsghdr structure or NULL if no more ancillary data objects.
Returns: pointer to first byte of data associated with cmsghdr structure.
Returns: value to store in cmsg_len given the amount of data.
Returns: total size of an ancillary data object given the amount of data.
CMSG_NXTHDR returns a null pointer when there is not another ancillary data object in the control buffer.
If the goal is not to block in the kernel because we have something else to do when nothing is ready to be read, nonblocking I/O can be used.
If we want to do this, but we are not sure that something is ready to be read, we can use this flag with a nonblocking socket or combine this flag with the MSG_DONTWAIT flag.
Be aware that the amount of data on the receive queue can change between two successive calls to recv for a stream socket.
In the case of a UDP socket with a datagram on the receive queue, if we call recvfrom specifying MSG_PEEK, followed by another call without specifying MSG_PEEK, the return values from both calls (the datagram size, its contents, and the sender's address) will be the same, even if more datagrams are added to the socket receive buffer between the two calls.
We are assuming, of course, that some other process is not sharing the same descriptor and reading from this socket at the same time.
This value is the total number of bytes queued, which for a UDP socket includes all queued datagrams.
These functions work with descriptors and are normally implemented as system calls within the Unix kernel.
Another method of performing I/O is the standard I/O library.
It is specified by the ANSI C standard and is intended to be portable to non-Unix systems that support ANSI C.
The standard I/O library handles some of the details that we must worry about ourselves when using the Unix I/O functions, such as automatically buffering the input and output streams.
Unfortunately, its handling of a stream's buffering can present a new set of problems we must worry about.
The standard I/O library can be used with sockets, but there are a few items to consider:
A standard I/O stream can be created from any descriptor by calling the fdopen function.
Similarly, given a standard I/O stream, we can obtain the corresponding descriptor by calling fileno.
Our first encounter with fileno was in Figure 6.9 when we wanted to call select on a standard I/O stream.
Standard I/O streams can also be full-duplex: we just open the stream with a type of r+, which means read-write.
But on such a stream, an output function cannot be followed by an input function without an intervening call to fflush, fseek, fsetpos, or rewind.
Similarly, an input function cannot be followed by an output function without an intervening call to fseek, fsetpos, or rewind, unless the input function encounters an EOF.
The problem with these latter three functions is that they all call lseek, which fails on a socket.
The easiest way to handle this read-write problem is to open two standard I/O streams for a given socket: one for reading and one for writing.
We now show an alternate version of our TCP echo server (Figure 5.3), which uses standard I/O instead of read and writen.
This version has a problem that we will describe shortly.
Two standard I/O streams are created by fdopen: one for input and one for output.
The calls to read and writen are replaced with calls to fgets and fputs.
If we run our server with this version of str_echo and then run our client, we see the following:
There is a buffering problem here because nothing is echoed by the server until we enter our EOF character.
We type the first line of input and it is sent to the server.
The server reads the line with fgets and echoes it with fputs.
The server's standard I/O stream is fully buffered by the standard I/O library.
This means the library copies the echoed line into its standard I/O buffer for this stream, but does not write the buffer to the descriptor, because the buffer is not full.
We type the second line of input and it is sent to the server.
The server reads the line with fgets and echoes it with fputs.
Again, the server's standard I/O library just copies the line into its buffer, but does not write the buffer because it is still not full.
The same scenario happens with the third line of input that we enter.
The server TCP receives the FIN, which fgets reads, causing fgets to return a null pointer.
The output buffer that was partially filled by our calls to fputs is now output.
The server child process terminates, causing its connected socket to be closed, sending a FIN to the client, completing the TCP four-packet termination sequence.
The three echoed lines are received by our str_cli function and output.
The problem here is the buffering performed automatically by the standard I/O library on the server.
There are three types of buffering performed by the standard I/O library:
Fully buffered means that I/O takes place only when the buffer is full, the process explicitly calls fflush, or the process terminates by calling exit.
A common size for the standard I/O buffer is 8,192 bytes.
Line buffered means that I/O takes place when a newline is encountered, when the process calls fflush, or when the process terminates by calling exit.
Unbuffered means that I/O takes place each time a standard I/O output function is called.3
Most Unix implementations of the standard I/O library use the following rules:
Standard input and standard output are fully buffered, unless they refer to a terminal device, in which case, they are line buffered.
All other streams are fully buffered unless they refer to a terminal device, in which case, they are line buffered.
One way around this is to force the output stream to be line buffered by calling setvbuf.
Another is to force each echoed line to be output by calling fflush after each call to fputs.
But in practice, either of these solutions is still error-prone and may interact badly with the Nagle algorithm described in Section 7.9
In most cases, the best solution is to avoid using the standard I/O library altogether for sockets and operate on buffers instead of lines, as described in Section 3.9
Using standard I/O on sockets may make sense when the convenience of standard I/O streams outweighs the concerns about bugs due to buffering, but these are rare cases.
This can be a problem with network servers that handle lots of descriptors.
Check the definition of the FILE structure in your <stdio.h> header to see what type of variable holds the descriptor.
Earlier in this chapter, we discussed several ways to set a time limit on a socket operation.
Many operating systems now offer another alternative, and provide the features of select and poll we described in Chapter 6 as well.
Since none of these methods have been adopted by POSIX yet, and each implementation seems to be slightly different, code that uses these mechanisms should be considered nonportable.
We'll describe two mechanisms here; other available mechanisms are similar.
Solaris provides a special file called /dev/poll , which provides a more scalable way to poll large numbers of file descriptors.
The problem with select and poll is that the file descriptors of interest must be passed in with each call.
The poll device maintains state between calls so that a program can set up the list of descriptors to poll and then loop, waiting for events, without setting up the list again each time around the loop.
After opening /dev/poll , the polling program must initialize an array of pollfd structures (the same structure used by the poll function, but the revents field is unused in this case)
The array is then passed to the kernel by calling write to write the structured directly to the /dev/poll device.
The program then uses an ioctl call, DO_POLL , to block, waiting for events.
The field dp_fds points to a buffer that is used to hold an array of pollfd structures returned from the ioctl call.
The field dp_nfds field specifies the size of the buffer.
The ioctl call blocks until there are interesting events on any of the polled file descriptors, or until dp_timeout milliseconds have passed.
Using a value of zero for dp_timeout will cause the ioctl to return immediately, which provides a nonblocking way to use this interface.
Passing in -1 for the timeout indicates that no timeout is desired.
After filling in an array of pollfd structures, we pass them to /dev/poll.
Our example only requires two file descriptors, so we use a static array of structures.
In practice, programs that use /dev/poll need to monitor hundreds or even thousands of file descriptors, so the array would likely be allocated dynamically.
Rather than calling select , this program blocks, waiting for work, in the ioctl call.
The return is the number of file descriptors that are ready.
The code in our example is simplified since we know the ready file descriptors will be sockfd , the input file descriptor, or both.
In a large-scale program, this loop would be more complex, perhaps even dispatching the work to threads.
This interface allows a process to register an "event filter" that describes the kqueue events it is interested in.
Events include file I/O and timeouts like select , but also adds asynchronous I/O, file modification notification (e.g., notification when a file is removed or modified), process tracking (e.g., notification when a given process exits or calls fork ), and signal handling.
The kqueue interface includes the following two functions and macro:
The kqueue function returns a new kqueue descriptor, which can be used with future calls to kevent.
The kevent function is used to both register events of interest and determine if any events have occurred.
The changelist and nchanges parameters describe the changes to be made to the events of interest, or are NULL and 0 , respectively, if no changes are to be made.
Any filters whose conditions have triggered, including those that may have just been added in the changelist , are returned through the eventlist parameter, which points to an array of nevents struct kevents.
The kevent function returns the number of events that are returned, or zero if a timeout has occurred.
The timeout argument holds the timeout, which is handled just like select: NULL to block, a nonzero timespec to specify an explicit timeout, and a zero timespec to perform a nonblocking check for events.
Note that the timeout parameter is a struct timespec , which is different from select's struct timeval in that it has nanosecond instead of microsecond resolution.
The kevent structure is defined by including the <sys/event.h> header.
The actions for changing a filter and the flag return values are shown in Figure 14.16
The behavior of kqueue on EOF is different depending on whether the file descriptor is associated with a file, a pipe, or a terminal, so we use the fstat call to determine if it is a file.
We call kqueue to get a kqueue descriptor, set the timeout to zero to allow a nonblocking call to kevent , and call kevent with our array of kevents as the change request.
We pass a NULL change list, since we are only interested in the events we have already registered, and a NULL timeout to block forever.
We check each event that was returned and process it individually.
This code is exactly the same as in Figure 6.13
This code is similar to Figure 6.13 , but is structured slightly differently to handle how kqueue reports an EOF.
On pipes and terminals, kqueue returns a readable indication that an EOF is pending, just like select.
However, on files, kqueue simply returns the number of bytes left in the file in the data member of the struct kevent and assumes that the application will know when it reaches the end.
Therefore, we restructure the loop to write the data to the network if a nonzero number of bytes were read.
Next, we check our modified EOF condition: if we have read zero bytes or if it's a file and we've read as many bytes as are left in the file.
Care should be taken with these newly evolved interfaces to read the documentation specific to the OS release.
These interfaces often change in subtle ways between releases while the vendors work through the details of how they should work.
While writing nonportable code is, in general, something to avoid, it is quite common to use any means possible to optimize a very heavily used network application for the specific server it runs on.
There are three main ways to set a time limit on a socket operation:
The first is easy to use, but involves signal handling, and as we will see in Section 20.5, can lead to race conditions.
Using select means that we block in this function with its provided time limit instead of blocking in a call to read, write, or connect.
The third alternative, to use the new socket options, is also easy, but is not provided by all implementations.
They combine the ability to specify an MSG_xxx flag (from recv and send), plus employ the ability to return or specify the peer's protocol address (from recvfrom and sendto), with the ability to use multiple buffers (from readv and writev), along with two new features: returning flags to the application and receiving or sending ancillary data.
We describe ten different forms of ancillary data in the text, six of which are new with IPv6
Ancillary data consists of one or more ancillary data objects, each object preceded by a cmsghdr structure specifying its length, protocol level, and type of data.
Five functions beginning with CMSG_ are used to build and parse ancillary data.
Sockets can be used with the C standard I/O library, but doing this adds another level of buffering to that already being performed by TCP.
Indeed, a lack of understanding of the buffering performed by the standard I/O library is the most common problem with the library.
Since a socket is not a terminal device, the common solution to this potential problem is to set the standard I/O stream to unbuffered, or to simply avoid standard I/O on sockets completely.
Many vendors provide advanced ways to poll for many events without the overhead required by select and poll.
While writing nonportable code should be avoided whenever possible, sometimes the benefits of performance improvements outweigh the risk of nonportability.
Modify the function to reset this alarm for the process after the connect, before the function returns.
When this returns, call ioctl with a command of FIONREAD and print the number of bytes queued on the socket's receive buffer.
The Unix domain protocols are not an actual protocol suite, but a way of performing client/server communication on a single host using the same API that is used for clients and servers on different hosts.
The Unix domain protocols are an alternative to the interprocess communication (IPC) methods described in Volume 2 of this series, when the client and server are on the same host.
Two types of sockets are provided in the Unix domain: stream sockets (similar to TCP) and datagram sockets (similar to UDP)
Even though a raw socket is also provided, its semantics have never been documented, it is not used by any program that the authors are aware of, and it is not defined by POSIX.
One application takes advantage of this: the X Window System.
If the server is on the same host as the client, the client opens a Unix domain stream connection to the server; otherwise the client opens a TCP connection to the server.
Unix domain sockets are used when passing descriptors between processes on the same host.
We will provide a complete example of this in Section 15.7
Newer implementations of Unix domain sockets provide the client's credentials (user ID and group IDs) to the server, which can provide additional security checking.
The protocol addresses used to identify clients and servers in the Unix domain are pathnames within the normal filesystem.
These pathnames are not normal Unix files: We cannot read from or write to these files except from a program that has associated the pathname with a Unix domain socket.
The POSIX specification does not define the length of the sun_path array and it specifically warns that applications should not assume a particular length.
Use the sizeof operator to find the length at run-time and to verify that a pathname fits into the array.
The pathname stored in the sun_path array must be null-terminated.
Nevertheless, we still use the term "Unix domain" as that has become its de facto name, regardless of the underlying OS.
Also, even with POSIX attempting to make these OS-independent, the socket address structure still retains the _un suffix!
The program in Figure 15.2 creates a Unix domain socket, binds a pathname to it, and then calls getsockname and prints the bound pathname.
Figure 15.2 bind of a pathname to a Unix domain socket.
The pathname that we bind to the socket is the command-line argument.
But the bind will fail if the pathname already exists in the filesystem.
Therefore, we call unlink to delete the pathname, in case it already exists.
If it does not exist, unlink returns an error, which we ignore.
We copy the command-line argument using strncpy, to avoid overflowing the structure if the pathname is too long.
Since we initialize the structure to zero and then subtract one from the size of the sun_path array, we know the pathname is null-terminated.
We then call getsockname to fetch the name that was just bound and print the result.
If we run this program under Solaris, we obtain the following results:
We first print our umask value because POSIX specifies that the file access permissions of the resulting pathname should be modified by this value.
Our value of 22 turns off the group-write and other-write bits.
This is an example of a value-result argument whose result when the function returns differs from its value when the function was called.
We can output the pathname using the %s format of printf because the pathname is null-terminated in the sun_path member.
We then run the program again, to verify that calling unlink removes the pathname.
We run ls -l to see the file permissions and file type.
Under Solaris (and most Unix variants), the file type is a socket, which is printed as s.
We also notice that the permission bits were modified as appropriate by the umask value.
Finally, we run ls again, with the -F option, which causes Solaris to append an equals sign to the pathname.
Historically, the umask value did not apply to the creation of Unix domain sockets, but over time, most Unix vendors have fixed this so the permissions fit expectations.
Systems still exist where the file permission bits may show either all permissions or no permissions (regardless of the umask setting)
In addition, some systems show the file as a FIFO, which is printed as p, and not all systems show the equals sign with ls -F.
The socketpair function creates two sockets that are then connected together.
This function is similar to the Unix pipe function: Two descriptors are returned, and each descriptor is connected to the other.
The two created sockets are unnamed; that is, there is no implicit bind involved.
The result of socketpair with a type of SOCK_STREAM is called a stream pipe.
It is similar to a regular Unix pipe (created by the pipe function), but a stream pipe is full-duplex; that is, both descriptors can be read and written.
We show a picture of a stream pipe created by socketpair in Figure 15.7
There are several differences and restrictions in the socket functions when using Unix domain sockets.
We list the POSIX requirements when applicable, and note that not all implementations are currently at this level.
The default file access permissions for a pathname created by bind should be 0777 (read, write, and execute by user, group, and other), modified by the current umask value.
The pathname associated with a Unix domain socket should be an absolute pathname, not a relative pathname.
The reason to avoid the latter is that its resolution depends on the current working directory of the caller.
That is, if the server binds a relative pathname, then the client must be in the same directory as the server (or must know this directory) for the client's call to either connect or sendto to succeed.
The pathname specified in a call to connect must be a pathname that is currently bound to an open Unix domain socket of the same type (stream or datagram)
Errors occur if: (i) the pathname exists but is not a socket; (ii) the pathname exists and is a socket, but no open socket descriptor is associated with the pathname; or (iii) the pathname exists and is an open socket, but is of the wrong type (that is, a Unix domain stream socket cannot connect to a pathname associated with a Unix domain datagram socket, and vice versa)
The permission testing associated with the connect of a Unix domain socket is the same as if open had been called for write-only access to the pathname.
Unix domain stream sockets are similar to TCP sockets: They provide a byte stream interface to the process with no record boundaries.
If a call to connect for a Unix domain stream socket finds that the listening socket's queue is full (Section 4.5), ECONNREFUSED is returned immediately.
This differs from TCP: The TCP listener ignores an arriving SYN if the socket's queue is full, and the TCP connector retries by sending the SYN several times.
Unix domain datagram sockets are similar to UDP sockets: They provide an unreliable datagram service that preserves record boundaries.
Unlike UDP sockets, sending a datagram on an unbound Unix domain datagram socket does not bind a pathname to the socket.
Recall that sending a UDP datagram on an unbound UDP socket causes an ephemeral port to be bound to the socket.
This means the receiver of the datagram will be unable to send a reply unless the sender has bound a pathname to its socket.
Similarly, unlike TCP and UDP, calling connect for a Unix domain datagram socket does not bind a pathname to the socket.
We now recode our TCP echo client/server from Chapter 5 to use Unix domain sockets.
The datatype of the two socket address structures is now sockaddr_un.
The first argument to socket is AF_LOCAL, to create a Unix domain stream socket.
The constant UNIXSTR_PATH is defined in unp.h to be /tmp/unix.str.
We first unlink the pathname, in case it exists from an earlier run of the server, and then initialize the socket address structure before calling bind.
Notice that this call to bind differs from the call in Figure 15.2
Here, we specify the size of the socket address structure (the third argument) as the total size of the sockaddr_un structure, not just the number of bytes occupied by the pathname.
Both lengths are valid since the pathname must be null-terminated.
The remainder of the function is the same as Figure 5.12
Figure 15.4 is the Unix domain stream protocol echo client.
The socket address structure to contain the server's address is now a sockaddr_un structure.
The datatype of the two socket address structures is now sockaddr_un.
The first argument to socket is AF_LOCAL, to create a Unix domain datagram socket.
The constant UNIXDG_PATH is defined in unp.h to be /tmp/unix.dg.
We first unlink the pathname, in case it exists from an earlier run of the server, and then initialize the socket address structure before calling bind.
Figure 15.6 is the Unix domain datagram protocol echo client.
The socket address structure to contain the server's address is now a sockaddr_un structure.
We also allocate one of these structures to contain the client's address, which we will talk about shortly.
Unlike our UDP client, when using the Unix domain datagram protocol, we must explicitly bind a pathname to our socket so that the server has a pathname to which it can send its reply.
We call tmpnam to assign a unique pathname that we then bind to our socket.
Recall from Section 15.4 that sending a datagram on an unbound Unix domain datagram socket does not implicitly bind a pathname to the socket.
Therefore, if we omit this step, the server's call to recvfrom in the dg_echo function returns a null pathname, which then causes an error when the server calls sendto.
The code to fill in the socket address structure with the server's well-known pathname is identical to the code shown earlier for the server.
When we think of passing an open descriptor from one process to another, we normally think of either.
A child sharing all the open descriptors with the parent after a call to fork.
In the first example, the process opens a descriptor, calls fork, and then the parent closes the descriptor, letting the child handle the descriptor.
This passes an open descriptor from the parent to the child.
But, we would also like the ability for the child to open a descriptor and pass it back to the parent.
Current Unix systems provide a way to pass any open descriptor from one process to any other process.
That is, there is no need for the processes to be related, such as a parent and its child.
The technique requires us to first establish a Unix domain socket between the two processes and then use sendmsg to send a special message across the Unix domain socket.
This message is handled specially by the kernel, passing the open descriptor from the sender to the receiver.
But, the process can still access this kernel feature using a Unix domain socket.
The steps involved in passing a descriptor between two processes are then as follows:
Create a Unix domain socket, either a stream socket or a datagram socket.
If the goal is to fork a child and have the child open the descriptor and pass the descriptor back to the parent, the parent can call socketpair to create a stream pipe that can be used to exchange the descriptor.
If the processes are unrelated, the server must create a Unix domain stream socket and bind a pathname to it, allowing the client to connect to that socket.
The client can then send a request to the server to open some descriptor and the server can pass back the descriptor across the Unix domain socket.
Alternately, a Unix domain datagram socket can also be used between the client and server, but there is little advantage in doing this, and the possibility exists for a datagram to be discarded.
We will use a stream socket between the client and server in an example presented later in this section.
One process opens a descriptor by calling any of the Unix functions that returns a descriptor: open, pipe, mkfifo, socket, or accept, for example.
Any type of descriptor can be passed from one process to another, which is why we call the technique "descriptor passing" and not "file descriptor passing."
The sending process builds a msghdr structure (Section 14.5) containing the descriptor to be passed.
At this point, we say that the descriptor is "in flight." Even if the sending process closes the descriptor after calling sendmsg, but before the receiving process calls recvmsg (in the next step), the descriptor remains open for the receiving process.
Sending a descriptor increments the descriptor's reference count by one.
It is normal for the descriptor number in the receiving process to differ from the descriptor number in the sending process.
Passing a descriptor is not passing a descriptor number, but involves creating a new descriptor in the receiving process that refers to the same file table entry within the kernel as the descriptor that was sent by the sending process.
The client and server must have some application protocol so that the receiver of the descriptor knows when to expect it.
Also, the MSG_PEEK flag should be avoided with recvmsg if a descriptor is expected, as the result is unpredictable.
We will write a program named mycat that takes a pathname as a command-line argument, opens the file, and copies it to standard output.
But instead of calling the normal Unix open function, we call our own function named my_open.
This function creates a stream pipe and calls fork and exec to initiate another program that opens the desired file.
This program must then pass the open descriptor back to the parent across the stream pipe.
Figure 15.7 shows the first step: our mycat program after creating a stream pipe by calling socketpair.
The process then calls fork and the child calls exec to execute the openfile program.
The parent must pass three pieces of information to the openfile program: (i) the pathname of the file to open, (ii) the open mode (read-only, read–write, or write-only), and (iii) the descriptor number corresponding to its end of the stream pipe (what we show as [1])
We choose to pass these three items as command-line arguments in the call to exec.
An alternative method is to send these three items as data across the stream pipe.
The openfile program sends back the open descriptor across the stream pipe and terminates.
The exit status of the program tells the parent whether the file could be opened, and if not, what type of error occurred.
The advantage in executing another program to open the file is that the program could be a "setuser-ID" binary, which executes with root privileges, allowing it to open files that we normally do not have permission to open.
This program could extend the concept of normal Unix permissions (user, group, and other) to any form of access checking it desires.
We begin with the mycat program, shown in Figure 15.9
Figure 15.9 mycat program: copies a file to standard output.
If we replace the call to my_open with a call to open, this simple program just copies a file to standard output.
It takes two arguments, a pathname and an open mode (such as O_RDONLY to mean read-only), opens the file, and returns a descriptor.
The descriptor number of the other end of the stream pipe is formatted into the argsockfd array and the open mode is formatted into the argmode array.
We call snprintf because the arguments to exec must be character strings.
The execl function should not return unless it encounters an error.
On success, the main function of the openfile program starts executing.
The parent closes the other end of the stream pipe and calls waitpid to wait for the child to terminate.
The termination status of the child is returned in the variable status, and we first verify that the program terminated normally (i.e., it was not terminated by a signal)
We will see shortly that if the openfile program encounters an error opening the requested file, it terminates with the corresponding errno value as its exit status.
Our function read_fd, shown next, receives the descriptor on the stream pipe.
In addition to the descriptor, we read one byte of data, but do nothing with it.
When sending and receiving a descriptor across a stream pipe, we always send at least one byte of data, even if the receiver does nothing with the data.
The first three arguments to this function are the same as for the read function, with a fourth argument being a pointer to an integer that will contain the received descriptor on return.
The msg_control buffer must be suitably aligned for a cmsghdr structure.
Here we declare a union of a cmsghdr structure with the character array, which guarantees that the array is suitably aligned.
Another technique is to call malloc, but that would require freeing the memory before the function returns.
If ancillary data is returned, the format is as shown in Figure 14.13
We verify that the length, level, and type are correct, then fetch the newly created descriptor and return it through the caller's recvfd pointer.
We cast this to an int pointer and fetch the integer descriptor that is pointed to.
If the older msg_accrights member is supported, the length should be the size of an integer and the newly created descriptor is returned through the caller's recvfd pointer.
It takes the three command-line arguments that must be passed and calls the normal open function.
Figure 15.12 openfile function: opens a file and passes back the descriptor.
Since two of the three command-line arguments were formatted into character strings by my_open, two are converted back into integers using atoi.
If an error is encountered, the errno value corresponding to the open error is returned as the exit status of the process.
The descriptor is passed back by write_fd, which we show next.
But, recall that earlier in the chapter, we said that it was acceptable for the sending process to close the descriptor that was passed (which happens when we call exit), because the kernel knows that the descriptor is in flight, and keeps it open for the receiving process.
An alternate technique that doesn't require the errno values to be less than 256 would be to pass back an error indication as normal data in the call to sendmsg.
As with read_fd, this function must deal with either ancillary data or older access rights.
In either case, the msghdr structure is initialized and then sendmsg is called.
We will show an example of descriptor passing in Section 28.7 that involves unrelated processes.
Additionally, we will show an example in Section 30.9 that involves related processes.
In Figure 14.13 , we showed another type of data that can be passed along a Unix domain socket as ancillary data: user credentials.
Exactly how credentials are packaged up and sent as ancillary data tends to be OS-specific.
We describe FreeBSD here, and other Unix variants are similar (usually the challenge is determining which structure to use for the credentials)
We describe this feature, even though it is not uniform across systems, because it is an important, yet simple, addition to the Unix domain protocols.
When a client and server communicate using these protocols, the server often needs a way to know exactly who the client is, to validate that the client has permission to ask for the service being requested.
FreeBSD passes credentials in a cmsgcred structure, which is defined by including the <sys/socket.h> header.
This information is always available on a Unix domain socket, although there are often special arrangments the sender must make to have the information included when sending, and there are often special arrangements (e.g., socket options) the receiver must make to get the credentials.
On our FreeBSD system, the receiver doesn't have to do anything special other than call recvmsg with an ancillary buffer large enough to hold the credentials, as we show in Figure 15.14
The sender, however, must include a cmsgcred structure when sending data using sendmsg.
It is important to note that although FreeBSD requires the sender to include the structure, the contents are filled in by the kernel and cannot be forged by the sender.
This makes the passing of credentials over a Unix domain socket a reliable way to verify the client's identity.
As an example of credential passing, we modify our Unix domain stream server to ask for the client's credentials.
If credentials were returned, the length, level, and type of the ancillary data are verified, and the resulting structure is copied back to the caller.
The main function for our echo server, Figure 15.3 , is unchanged.
This function is called by the child after the parent has accepted a new client connection and called fork.
This code reads buffers from the client and writes them back to the client.
Our client from Figure 15.4 is only changed minimally to pass an empty cmsgcred structure that will be filled in when it calls sendmsg.
Before running the client, we can see our current credentials using the id command.
Starting the server and then running the client one time in another window produces the following output from the server:
This information is output only after the client has sent data to the server.
We see that the information matches what we saw with the id command.
Unix domain sockets are an alternative to IPC when the client and server are on the same host.
The advantage in using Unix domain sockets over some form of IPC is that the API is nearly identical to a networked client/server.
The advantage in using Unix domain sockets over TCP, when the client and server are on the same host, is the increased performance of Unix domain sockets over TCP on many implementations.
We modified our TCP and UDP echo clients and servers to use the Unix domain protocols and the only major difference was having to bind a pathname to the UDP client's socket, so that the UDP server had somewhere to send the replies.
Descriptor passing is a powerful technique between clients and servers on the same host and it takes place across a Unix domain socket.
We showed an example in Section 15.7 that passed a descriptor from a child back to the parent.
Start with Figure 11.14 and modify it to call write for each byte of the result that is sent to the client.
We discussed similar modifications in the solution to Exercise 1.5
Run the client and server on the same host using TCP.
Run the client and server on the same host using a Unix domain socket.
Now call send instead of write in the server and specify the MSG_EOR flag.
Run the client and server on the same host using a Unix domain socket.
One approach is to create a stream pipe and then fork into a parent and child.
Each time through the loop, the parent first writes the value of the backlog to the stream pipe.
The child reads this value, creates a listening socket bound to the loopback address, and sets the backlog to that value.
The child then writes to the stream pipe, just to tell the parent it is ready.
The parent then attempts as many connections as possible, detecting when it has hit the backlog limit because the connect blocks.
The parent may use an alarm set at two seconds to detect the blocking connect.
The child never calls accept to let the kernel queue the connections from the parent.
When the parent's alarm expires, it knows from the loop counter which connect hit the backlog limit.
The parent then closes its sockets and writes the next new backlog value to the stream pipe for the child.
When the child reads this next value, it closes its listening socket and creates a new listening socket, starting the procedure again.
This means that when we issue a socket call that cannot be completed immediately, our process is put to sleep, waiting for the condition to be true.
We can divide the socket calls that may block into four categories:
Input operations— These include the read, readv, recv, recvfrom, and recvmsg functions.
If we call one of these input functions for a blocking TCP socket (the default), and there is no data available in the socket receive buffer, we are put to sleep until some data arrives.
Since TCP is a byte stream, we will be awakened when "some" data arrives: It could be a single byte of data, or it could be a full TCP segment of data.
Since UDP is a datagram protocol, if the socket receive buffer is empty for a blocking UDP socket, we are put to sleep until a UDP datagram arrives.
With a nonblocking socket, if the input operation cannot be satisfied (at least one byte of data for a TCP socket or a complete datagram for a UDP socket), we see an immediate return with an error of EWOULDBLOCK.
Output operations— These include the write, writev, send, sendto, and sendmsg functions.
For a TCP socket, we said in Section 2.11 that the kernel copies data from the application's buffer into the socket send buffer.
If there is no room in the socket send buffer for a blocking socket, the process is put to sleep until there is room.
With a nonblocking TCP socket, if there is no room at all in the socket send buffer, we return immediately with an error of EWOULDBLOCK.
If there is some room in the socket send buffer, the return value will be the number of bytes the kernel was able to copy into the buffer.
We also said in Section 2.11 that there is no actual UDP socket send buffer.
The kernel just copies the application data and moves it down the stack, prepending the UDP and IP headers.
Therefore, an output operation on a blocking UDP socket (the default) will not block for the same reason as a TCP socket, but it is possible for output operations to block on some systems due to the buffering and flow control that happens within the networking code in the kernel.
If accept is called for a blocking socket and a new connection is not available, the process is put to sleep.
If accept is called for a nonblocking socket and a new connection is not available, the error EWOULDBLOCK is returned instead.
Initiating outgoing connections— This is the connect function for TCP.
Recall that connect can be used with UDP, but it does not cause a "real" connection to be established; it just causes the kernel to store the peer's IP address and port number.
We showed in Section 2.6 that the establishment of a TCP connection involves a three-way handshake and the connect function does not return until the client receives the ACK of its SYN.
This means that a TCP connect always blocks the calling process for at least the RTT to the server.
If connect is called for a nonblocking TCP socket and the connection cannot be established immediately, the connection establishment is initiated (e.g., the first packet of TCP's threeway handshake is sent), but the error EINPROGRESS is returned.
Notice that this error differs from the error returned in the previous three scenarios.
Also notice that some connections can be established immediately, normally when the server is on the same host as the client.
So, even with a nonblocking connect, we must be prepared for connect to return successfully.
We will show an example of a nonblocking connect in Section 16.3
Traditionally, System V has returned the error EAGAIN for a nonblocking I/O operation that cannot be satisfied, while Berkeley-derived implementations have returned the error EWOULDBLOCK.
Because of this history, the POSIX specification says either may be returned for this case.
Fortunately, most current systems define these two error codes to be the same (check your system's <sys/errno.h> header), so it doesn't matter which one we use.
Section 6.2 summarized the different models available for I/O and compared nonblocking I/O to other models.
In this chapter, we will provide examples of all four types of operations and develop a new type of client, similar to a Web client, that initiates multiple TCP connections at the same time using a nonblocking connect.
The latter version, which uses select, still uses blocking I/O.
For example, if a line is available on standard input, we read it with read and then send it to the server with writen.
But the call to writen can block if the socket send buffer is full.
While we are blocked in the call to writen, data could be available for reading from the socket receive buffer.
Similarly, if a line of input is available from the socket, we can block in the subsequent call to write, if standard output is slower than the network.
Our goal in this section is to develop a version of this function that uses nonblocking I/O.
This prevents us from blocking while we could be doing something productive.
Unfortunately, the addition of nonblocking I/O complicates the function's buffer management noticeably, so we will present the function in pieces.
As we discussed in Chapter 6, using standard I/O with sockets can be difficult, and that is very much the case with nonblocking I/O.
So we continue to avoid standard I/O in this example.
We maintain two buffers: to contains data going from standard input to the server, and fr contains data arriving from the server going to standard output.
Figure 16.1 shows the arrangement of the to buffer and the pointers into the buffer.
Buffer containing data from standard input going to the socket.
The pointer toiptr points to the next byte into which data can be read from standard input.
There are toiptr minus tooptr bytes to be written to the socket.
The number of bytes that can be read from standard input is &to [MAXLINE] minus toiptr.
As soon as tooptr reaches toiptr, both pointers are reset to the beginning of the buffer.
Figure 16.2 shows the corresponding arrangement of the fr buffer.
Buffer containing data from the socket going to standard output.
All three descriptors are set to nonblocking using fcntl: the socket to and from the server, standard input, and standard output.
The pointers into the two buffers are initialized and the maximum descriptor plus one is calculated, which will be used as the first argument for select.
As with the previous version of this function, Figure 6.13, the main loop of the function is a call to select followed by individual tests of the various conditions we are interested in.
If we have not yet read an EOF on standard input, and there is room for at least one byte of data in the to buffer, the bit corresponding to standard input is turned on in the read set.
If there is room for at least one byte of data in the fr buffer, the bit corresponding to the socket is turned on in the read set.
If there is data to write to the socket in the to buffer, the bit corresponding to the socket is turned on in the write set.
Finally, if there is data in the fr buffer to send to standard output, the bit corresponding to standard output is turned on in the write set.
The next part of the function is shown in Figure 16.4
This code contains the first two tests (of four) that are made after select returns.
The third argument is the amount of available space in the to buffer.
If an error occurs and it is EWOULDBLOCK, nothing happens.
Normally this condition "should not happen," that is, select telling us that the descriptor is readable and read returning.
If read returns 0, we are finished with the standard input.
If there is no more data in the to buffer to send (tooptr equals toiptr), shutdown sends a FIN to the server.
If there is still data in the to buffer to send, the FIN cannot be sent until the buffer is written to the socket.
We output a line to standard error noting the EOF, along with the current time, and we show how we use this output after describing this function.
We also turn on the bit corresponding to the socket in the write set, to cause the test for this bit to be true later in the loop, thus causing a write to be attempted to the socket.
This is one of the hard design decisions when writing code.
Instead of setting the bit in the write set, we could do nothing, in which case, select will test for writability of the socket the next time it is called.
But this requires another loop around and another call to select when we already know that we have data to write to the socket.
Another choice is to duplicate the code that writes to the socket here, but this seems wasteful and a potential source for error (in case there is a bug in that piece of duplicated code, and we fix it in one location but not the other)
Lastly, we could create a function that writes to the socket and call that function instead of duplicating the code, but that function needs to share three of the local variables with str_cli, which would necessitate making these variables global.
The choice made is the authors' view on which alternative is best.
These lines of code are similar to the if statement we just described when standard input is readable.
If we encounter an EOF from the server, this is okay if we have already encountered an EOF on the standard input, but it is not expected otherwise.
If read returns some data, friptr is incremented and the bit for standard output is turned on in the write descriptor set, to try to write the data in the next part of the function.
If standard output is writable and the number of bytes to write is greater than 0, write is called.
Notice that this condition is entirely possible because the code at the end of the previous part of this function turns on the bit for standard output in the write set, without knowing whether the write will succeed or not.
If the write is successful, froptr is incremented by the number of bytes written.
If the output pointer has caught up with the input pointer, both pointers are reset to point to the beginning of the buffer.
This section of code is similar to the code we just described for writing to the standard output.
The one difference is that when the output pointer catches up with the input pointer, not only are both pointers reset to the beginning of the buffer, but if we encountered an EOF on standard input, the FIN can be sent to the server.
We now examine the operation of this function and the overlapping of the nonblocking I/O.
This function returns a string containing the current time, including microseconds, in the following format:
This is intentionally in the same format as the timestamps output by tcpdump.
Also notice that all the calls to fprintf in our str_cli function write to standard error, allowing us to separate standard output (the lines echoed by the server) from our diagnostic output.
We can then run our client and tcpdump and take this diagnostic output along with the tcpdump output and sort the two outputs together, ordered by the time.
This lets us see what happens in our program and correlate it with the corresponding TCP action.
For example, we first run tcpdump on our host solaris, capturing only TCP segments to or from port 7 (the echo server), saving the output in the file named tcpd.
We then run our TCP client on this host, specifying the server on the host linux.
Standard output is sent to the file out, and standard error is sent to the file diag.
Finally, we terminate tcpdump with our interrupt key and then print the tcpdump records, sorting these records with the diagnostic output from the client.
We wrapped the long lines containing the SYNs and we also removed the don't fragment (DF) notations from the Solaris segments, denoting that the DF bit is set (path MTU discovery)
Using this output, we can draw a timeline of what's happening.
We show this in Figure 16.8, with time increasing down the page.
In this figure, we do not show the ACK segments.
Also realize that when the program outputs "wrote N bytes to stdout," the write has returned, possibly causing TCP to send one or more segments of data.
What we can see from this timeline are the dynamics of a client/server exchange.
Using nonblocking I/O lets the program take advantage of these dynamics, reading or writing when the operation can take place.
We let the kernel tell us when an I/O operation can occur by using the select function.
But, is it worth the effort to code an application using nonblocking I/O, given the complexity of the resulting code? The answer is no.
Whenever we find the need to use nonblocking I/O, it will usually be simpler to split the application into either processes (using fork) or threads (Chapter 26)
The function immediately calls fork to split into a parent and child.
The child copies lines from the server to standard output and the parent copies lines from standard input to the server, as shown in Figure 16.9
We explicitly note that the TCP connection is full-duplex and that the parent and child are sharing the same socket descriptor: The parent writes to the socket and the child reads from the socket.
There is only one socket, one socket receive buffer, and one socket send buffer, but this socket is referenced by two descriptors: one in the parent and one in the child.
Normal termination occurs when the EOF on standard input is encountered.
The parent reads this EOF and calls shutdown to send a FIN.
But when this happens, the child needs to continue copying from the server to the standard output, until it reads an EOF on the socket.
The parent calls pause when it has finished copying, which puts it to sleep until a signal is caught.
Even though our parent does not catch any signals, this puts the parent to sleep until it receives the SIGTERM signal from the child.
The default action of this signal is to terminate the process, which is fine for this example.
The reason we make the parent wait for the child is to measure an accurate clock time for this version of str_cli.
Normally, the child finishes after the parent, but since we measure the clock time using the shell's time command, the measurement ends when the parent terminates.
Notice the simplicity of this version compared to the nonblocking I/O version shown earlier in this section.
Our nonblocking version managed four different I/O streams at the same time, and since all four were nonblocking, we had to concern ourselves with partial reads and writes for all four streams.
But in the fork version, each process handles only two I/O streams, copying from one to the other.
There is no need for nonblocking I/O because if there is no data to read from the input stream, there is nothing to write to the corresponding output stream.
We have now shown four different versions of the str_cli function.
Our nonblocking I/O version is almost twice as fast as our version using blocking I/O with select.
Our simple version using fork is slower than our nonblocking I/O version.
Nevertheless, given the complexity of the nonblocking I/O code versus the fork code, we recommend the simple approach.
When a TCP socket is set to nonblocking and then connect is called, connect returns immediately with an error of EINPROGRESS but the TCP three-way handshake continues.
We then check for either a successful or unsuccessful completion of the connection's establishment using select.
A connect takes one RTT to complete (Section 2.6) and this can be anywhere from a few milliseconds on a LAN to hundreds of milliseconds or a few seconds on a WAN.
There might be other processing we wish to perform during this time.
We can establish multiple connections at the same time using this technique.
This has become popular with Web browsers, and we will show an example of this in Section 16.5
Since we wait for the connection to be established using select, we can specify a time limit for select, allowing us to shorten the timeout for the connect.
Many implementations have a timeout for connect that is between 75 seconds and several minutes.
There are times when an application wants a shorter timeout, and using a nonblocking connect is one way to accomplish this.
Section 14.2 talks about other ways to place timeouts on socket operations.
As simple as the nonblocking connect sounds, there are other details we must handle:
Even though the socket is nonblocking, if the server to which we are connecting is on the same host, the connection is normally established immediately when we call connect.
Berkeley-derived implementations (and POSIX) have the following two rules regarding select and nonblocking connects:
These two rules regarding select fall out from our rules in Section 6.3 about the conditions that make a descriptor ready.
A TCP socket is writable if there is available space in the send buffer (which will always be the case for a connecting socket since we have not yet written anything to the socket) and the socket is connected (which occurs only when the three-way handshake completes)
A pending error causes a socket to be both readable and writable.
There are many portability problems with nonblocking connects that we mention in the examples that follow.
We replace the call to connect in Figure 1.5 with.
The first three arguments are the normal arguments to connect, and the fourth argument is the number of seconds to wait for the connection to complete.
A value of 0 implies no timeout on the select; hence, the kernel will use its normal TCP connection establishment timeout.
At this point, we can do whatever we want while we wait for the connection to complete.
If the nonblocking connect returns 0, the connection is complete.
As we have said, this can occur when the server is on the same host as the client.
We call select and wait for the socket to be ready for either reading or writing.
We zero out rset, turn on the bit corresponding to sockfd in this descriptor set, and then copy rset into wset.
This assignment is probably a structure assignment since descriptor sets are normally represented as structures.
We also initialize the timeval structure and then call select.
If select returns 0, the timer expired, and we return ETIMEDOUT to the caller.
We also close the socket, to prevent the three-way handshake from proceeding any further.
Do whatever we want while the connect is taking place.
If the descriptor is readable or writable, we call getsockopt to fetch the socket's pending error (SO_ERROR)
If an error occurred, Berkeley-derived implementations of getsockopt return 0 with the pending error returned in our variable error.
But Solaris causes getsockopt itself to return –1 with errno set to the pending error.
If our error variable is nonzero from getsockopt, that value is stored in errno and the function returns –1
As we said earlier, there are portability problems with various socket implementations and nonblocking connects.
First, it is possible for a connection to complete and for data to arrive from a peer before select is called.
In this case, the socket will be both readable and writable on success, the same as if the connection had failed.
Our code in Figure 16.11 handles this scenario by calling getsockopt and checking the pending error for the socket.
Next is determining whether the connection completed successfully or not, if we cannot assume that writability is the only way success is returned.
These would replace our call to getsockopt in Figure 16.11
If this fails with ENOTCONN, the connection failed and we must then call getsockopt with SO_ERROR to fetch the pending error for the socket.
If the read fails, the connect failed and the errno from read indicates the reason for the connection failure.
It should fail, and if the error is EISCONN, the socket is already connected and the first connection succeeded.
Unfortunately, nonblocking connects are one of the most nonportable areas of network programming.
A simpler technique is to create a thread (Chapter 26) to handle a connection.
What happens if our call to connect on a normal blocking socket is interrupted, say, by a caught signal, before TCP's three-way handshake completes? Assuming the connect is not automatically restarted, it returns EINTR.
But, we cannot call connect again to wait for the connection to complete.
What we must do in this scenario is call select, just as we have done in this section for a nonblocking connect.
The client establishes an HTTP connection with a Web server and fetches a home page.
Often, that page will have numerous references to other Web pages.
Instead of fetching these other pages serially, one at a time, the client can fetch more than one at the same time using nonblocking connects.
Figure 16.12 shows an example of establishing multiple connections in parallel.
In the middle scenario, we perform two connections in parallel.
At time 0, the first two connections are started, and when the first of these finishes, we start the third.
If the parallel connections are sharing a common link (say the client is behind a dialup modem link to the Internet), each can compete against each other for the limited resources and all the individual connection times might get longer.
Nevertheless, the total time would be 21, still shorter than the serial scenario.
In the third scenario, we perform three connections in parallel, and we again assume there is no interference between the three connections (the ideal case)
But, the total time is the same (15 units) as the second scenario given the example times that we choose.
When dealing with Web clients, the first connection is done by itself, followed by multiple connections for the references found in the data from that first connection.
To further optimize this sequence, the client can start parsing the data that is returned for the first connection before the first connection completes and initiate additional connections as soon as it knows that additional connections are needed.
Our program will read up to 20 files from a Web server.
We specify as command-line arguments the maximum number of parallel connections, the server's hostname, and each of the filenames to fetch from the server.
The command-line arguments specify three simultaneous connections: the server's hostname, the filename for the home page (/, the server's root page), and seven files to then read (which in this example are all GIF images)
These seven files would normally be referenced on the home page, and a Web client would read the home page and parse the HTML to obtain these filenames.
We do not want to complicate this example with HTML parsing, so we just specify the filenames on the command line.
This is a larger example, so we will show it in pieces.
The program reads up to MAXFILES files from the Web server.
We maintain a file structure with information about each file: its name (copied from the command-line argument), the hostname or IP address of the server to read the file from, the socket descriptor being used for the file, and a set of flags to specify what we are doing with this file (connecting, reading, or done)
We define the global variables and function prototypes for the functions that we will describe shortly.
Figure 16.15 shows the first part of the main program.
Figure 16.15 First part of simultaneous connect: globals and start of main.
The file structures are filled in with the relevant information from the command-line arguments.
The function home_page, which we will show next, creates a TCP connection, sends a command to the server, and then reads the home page.
This is the first connection, which is done by itself, before we start establishing multiple connections in parallel.
Two descriptor sets, one for reading and one for writing, are initialized.
An HTTP GET command is issued for the home page (often named /)
The reply is read (we do not do anything with the reply) and the connection is closed.
A TCP socket is created and the socket is set to nonblocking.
The nonblocking connect is initiated and the file's flag is set to F_CONNECTING.
The socket descriptor is turned on in both the read set and the write set since select will wait for either condition as an indication that the connection has finished.
If connect returns successfully, the connection is already complete and the function write_get_cmd (shown next) sends a command to the server.
We set the socket to nonblocking for the connect, but never reset it to its default blocking mode.
This is fine because we write only a small amount of data to the socket (the GET command in the next function) and we assume that this command is much smaller than the socket send buffer.
Even if write returns a short count because of the nonblocking flag, our writen function handles this.
Leaving the socket as nonblocking has no effect on the subsequent reads that are performed because we always call select to wait for the socket to become readable.
Figure 16.18 Send an HTTP GET command to the server.
This indicates to the main loop that this descriptor is ready for input.
The descriptor is also turned on in the read set and maxfd is updated, if necessary.
This is the main loop of the program: As long as there are more files to process (nlefttoread is greater than 0), start another connection if possible and then use select on all active descriptors, handling both nonblocking connection completions and the arrival of data.
The number of active connections is incremented (nconn) and the number of connections remaining to be established is decremented (nlefttoconn)
Descriptors that have a nonblocking connect in progress will be enabled in both sets, while descriptors with a completed connection that are waiting for data from the server will be enabled in just the read set.
We now process each element in the array of file structures to determine which descriptors need processing.
If the F_CONNECTING flag is set and the descriptor is on in either the read set or the write set, the nonblocking connect is finished.
As we described with Figure 16.11, we call getsockopt to fetch the pending error for the socket.
In that case, we turn off the descriptor in the write set and call write_get_cmd to send the HTTP request to the server.
If the F_READING flag is set and the descriptor is ready for reading, we call read.
If the connection was closed by the other end, we close the socket, set the F_DONE flag, turn off the descriptor in the read set, and decrement the number of active connections and the total number of connections to be processed.
There are two optimizations that we do not perform in this example (to avoid complicating it even more)
First, we could terminate the for loop in Figure 16.19 when we finish processing the number of descriptors that select said were ready.
Next, we could decrease the value of maxfd when possible, to save select from examining descriptor bits that are no longer set.
Since the number of descriptors this code deals with at any one time is probably less than 10, and not in the thousands, it is doubtful that either of these optimizations is worth the additional complications.
What is the performance gain in establishing multiple connections at the same time? Figure 16.20 shows the clock time required to fetch a Web server's home page, followed by nine image files from that server.
We also include in this figure, for comparison, values for a version of this program that we will develop in Section 26.9 using threads.
Most of the improvement is obtained with three simultaneous connections (the clock time is halved), and the performance increase is much less with four or more simultaneous connections.
We provide this example using simultaneous connects because it is a nice example using nonblocking I/O and one whose performance impact can be measured.
It is also a feature used by a popular Web application, the Netscape browser.
There are pitfalls in this technique if there is any congestion in the network.
When multiple connections are established from a client to a server, there is no communication between the connections at the TCP layer.
That is, if one connection encounters a packet loss, the other connections to the same server are not notified, and it is highly probable that the other connections will soon encounter packet loss unless they slow down.
These additional connections are sending more packets into an already congested network.
This technique also increases the load at any given time on the server.
We stated in Chapter 6 that a listening socket is returned as readable by select when a completed connection is ready to be accepted.
Therefore, if we are using select to wait for incoming connections, we should not need to set the listening socket to nonblocking because if select tells us that the connection is ready, accept should not block.
Unfortunately, there is a timing problem that can trip us up here [Gierth 1996]
To see this problem, we modify our TCP echo client (Figure 5.4) to establish the connection and then send an RST to the server.
Figure 16.21 TCP echo client that creates connection and sends an RST.
As stated in Section 7.5, this causes an RST to be sent on a TCP socket when the connection is closed.
In the following code from the beginning of Figure 6.22, the two lines preceded by a plus sign are new:
What we are simulating here is a busy server that cannot call accept as soon as select returns that the listening socket is readable.
Normally, this slowness on the part of the server is not a problem (indeed, this is why a queue of completed connections is maintained), but when combined with the RST from the client, after the connection is established, we can have a problem.
In Section 5.11, we noted that when the client aborts the connection before the server calls accept, Berkeley-derived implementations do not return the aborted connection to the server, while other implementations should return ECONNABORTED but often return EPROTO instead.
The client establishes the connection and then aborts it as in Figure 16.21
Between the server's return from select and its calling accept, the RST is received from the client.
The completed connection is removed from the queue and we assume that no other completed connections exist.
The server calls accept, but since there are no completed connections, it blocks.
The server will remain blocked in the call to accept until some other client establishes a connection.
But in the meantime, assuming a server like Figure 6.22, the server is blocked in the call to accept and will not handle any other ready descriptors.
This problem is somewhat similar to the denial-of-service attack described in Section 6.8, but with this new bug, the server breaks out of the blocked accept as soon as another client establishes a connection.
Always set a listening socket to nonblocking when you use select to indicate when a connection is ready to be accepted.
Ignore the following errors on the subsequent call to accept: EWOULDBLOCK (for Berkeleyderived implementations, when the client aborts the connection), ECONNABORTED (for POSIX implementations, when the client aborts the connection), EPROTO (for SVR4 implementations, when the client aborts the connection), and EINTR (if signals are being caught)
This version of our client is the fastest version that we show, although the code modifications are nontrivial.
We then showed that it is simpler to divide the client into two pieces using fork; we will employ the same technique using threads in Figure 26.2
Nonblocking connects let us do other processing while TCP's three-way handshake takes place, instead of being blocked in the call to connect.
Unfortunately, these are also nonportable, with different implementations having different ways of indicating that the connection completed successfully or encountered an error.
We used nonblocking connects to develop a new client, which is similar to a Web client that opens multiple TCP connections at the same time to reduce the clock time required to fetch numerous files from a server.
The ioctl function has traditionally been the system interface used for everything that didn't fit into some other nicely defined category.
For example, the Unix terminal interface was traditionally accessed using ioctl, but POSIX created 12 new functions for terminals: tcgetattr to get the terminal attributes, tcflush to flush pending input or output, and so on.
In a similar vein, POSIX has replaced one network ioctl: the new sockatmark function (Section 24.3) replaces the SIOCATMARK ioctl.
A common use of ioctl by network programs (typically servers) is to obtain information on all the host's interfaces when the program starts: the interface addresses, whether the interface supports broadcasting, whether the interface supports multicasting, and so on.
This function affects an open file referenced by the fd argument.
The third argument is always a pointer, but the type of pointer depends on the request.
As long as the prototype is in scope (i.e., the program using ioctl has included <unistd.h>), the correct type for the system will be used.
Some implementations specify the third argument as a void * pointer instead of the ANSI C ellipsis notation.
There is no standard for the header to include to define the function prototype for ioctl since it is not standardized by POSIX.
We can divide the requests related to networking into six categories:
Recall from Figure 7.20 that not only do some of the ioctl operations overlap some of the fcntl operations (e.g., setting a socket to nonblocking), but there are also some operations that can be specified more than one way using ioctl (e.g., setting the process group ownership of a socket)
Figure 17.1 lists the requests, along with the datatype of what the arg address must point to.
All three require that the third argument to ioctl be a pointer to an integer.
The next group of requests begin with FIO and may apply to certain types of files, in addition to sockets.
The following five requests all require that the third argument to ioctl point to an integer:
One of the first steps employed by many programs that deal with the network interfaces on a system is to obtain from the kernel all the interfaces configured on the system.
This is done with the SIOCGIFCONF request, which uses the ifconf structure, which in turn uses the ifreq structure, both of which are shown in Figure 17.2
Figure 17.2 ifconf and ifreq structures used with various interface ioctl requests.
Before calling ioctl , we allocate a buffer and an ifconf structure and then initialize the latter.
The third argument to ioctl is a pointer to our ifconf structure.
The buffer has been filled in with the two structures and the ifc_len member of the ifconf structure has been updated to reflect the amount of information stored in the buffer.
We assume in this figure that each ifreq structure occupies 32 bytes.
Notice that each ifreq structure contains a union and there are numerous #defines to hide the fact that these fields are members of a union.
All references to individual members are made using defined names.
Since many programs need to know all the interfaces on a system, we will develop a function of our own named get_ifi_info that returns a linked list of structures, one for each interface that is currently "up." In this section, we will implement this function using the SIOCGIFCONF ioctl , and in Chapter 18 , we will develop a version using routing sockets.
Our own header for the programs that need interface configuration info.
A linked list of these structures is returned by our function, each structure's ifi_next member pointing to the next one.
We return in this structure just the information that a typical application is probably interested in: the interface name, the interface index, the MTU, the hardware address (e.g., an Ethernet address), the interface flags (to let the application determine if the interface supports broadcasting or multicasting, or is a point-to-point interface), the interface address, the broadcast address, and the destination address for a point-to-point link.
All the memory used to hold the ifi_info structures, along with the socket address structures contained within, are obtained dynamically.
Therefore, we also provide a free_ifi_info function to free all this memory.
Before showing the implementation of our get_ifi_info function, we show a simple program that calls this function and then outputs all the information.
This program is a miniature version of the ifconfig program and is shown in Figure 17.6
If the length of the hardware address is greater than 0, it is printed as hexadecimal numbers.
The MTU and three IP addresses are printed, if returned.
If we run this program on our host macosx (Figure 1.16 ), we have the following output:
Note that under MacOS X, the hardware address of the Ethernet interface is not available using this method.
We now show our implementation of get_ifi_info that uses the SIOCGIFCONF ioctl.
Figure 17.7 shows the first part of the function, which obtains the interface configuration from the kernel.
We create a UDP socket that will be used with ioctls.
A fundamental problem with the SIOCGIFCONF request is that some implementations do not return an error if the buffer is not large enough to hold the result.
Instead, the result is truncated and success is returned (a return value of 0 from ioctl )
This means the only way we know that our buffer is large enough is to issue the request, save the return length, issue the request again with a larger buffer, and compare the length with the saved value.
Only if the two lengths are the same is our buffer large enough.
Some implementations provide a SIOCGIFNUM request that returns the number of interfaces.
This allows the application to then allocate a buffer of sufficient size before issuing the SIOCGIFCONF request, but this new request is not widespread.
Allocating a fixed-sized buffer for the result from the SIOCGIFCONF request has become a problem with the growth of the Web, because large Web servers are allocating many alias addresses to a single interface.
Sites with numerous aliases discovered that programs with fixed-size buffers for interface information started failing.
Even though Solaris returns an error if a buffer is too small, these programs allocate their fixed-size buffer, issue the ioctl , but then die if an error is returned.
We dynamically allocate a buffer, starting with room for 100 ifreq structures.
If an error of EINVAL is returned by ioctl , and we have not yet had a successful return (i.e., lastlen is still 0), we have not yet allocated a buffer large enough and continue through the loop.
If ioctl returns success, and if the returned length equals lastlen , the length has not changed (our buffer is large enough) and we break out of the loop since we have all the information.
Each time around the loop, we increase the buffer size to hold 10 more ifreq structures.
Since we will be returning a pointer to the head of a linked list of ifi_info structures, we use the two variables ifihead and ifipnext to hold pointers to the list as we build it.
As we loop through all the ifreq structures, ifr points to each structure and we then increment ptr to point to the next one.
But, we must deal with newer systems that provide a.
Even though the declaration in Figure 17.2 declares the socket address structure contained within the ifreq structure as a generic socket address structure, on newer systems, this can be any type of socket address structure.
Therefore, if the length member is supported, we must use its value to update our pointer to the next socket address structure.
Otherwise, we use a length based on the address family, using the size of the generic socket address structure (16 bytes) as the default.
We put in a case for IPv6, for newer systems, just in case.
This is not a problem on systems that have the sa_len field in the sockaddr since they can indicate variable-sized sockaddr structures easily.
We ignore any addresses from families except those desired by the caller.
We must detect any aliases that may exist for the interface, that is, additional addresses that have been assigned to the interface.
To handle both cases, we save the last interface name in lastname and only compare up to a colon, if present.
If a colon is not present, we still ignore this interface if the name is equivalent to the last interface we processed.
We issue an ioctl of SIOCGIFFLAGS (Section 17.5 ) to fetch the interface flags.
The third argument to ioctl is a pointer to an ifreq structure that must contain the name of the interface for which we want the flags.
We make a copy of the ifreq structure before issuing the ioctl , because if we didn't, this request would overwrite the IP address of the interface since both are members of the same union in Figure 17.2
At this point, we know that we will return this interface to the caller.
We allocate memory for our ifi_info structure and add it to the end of the linked list we are building.
We copy the interface flags, MTU, and name into the structure.
We copy the saved interface index and hardware length; if the length is nonzero, we also copy the saved hardware address.
We copy the IP address that was returned from our original SIOCGIFCONF request in the structure we are building.
We allocate memory for the socket address structure containing this address and add it to the ifi_info structure we are building.
Similarly, if the interface is a point-to-point interface, the SIOCGIFDSTADDR returns the IP address of the other end of the link.
As we showed in the previous section, the SIOCGIFCONF request returns the name and a socket address structure for each interface that is configured.
There are a multitude of other requests that we can then issue to set or get all the other characteristics of the interface.
The get version of these requests (SIOCGxxx) is often issued by the netstat program, and the set version (SIOCSxxx) is often issued by the ifconfig program.
Any user can get the interface information, while it takes superuser privileges to set the information.
These requests take or return an ifreq structure whose address is specified as the third argument to ioctl.
Many of these requests use a socket address structure to specify or return an IP address or address mask with the application.
The interface metric is added to the hop count (to make an interface less favorable)
On some systems, the ARP cache is also manipulated with the ioctl function.
Systems that use routing sockets (Chapter 18) usually use routing sockets instead of ioctl to access the ARP cache.
Figure 17.12 arpreq structure used with ioctl requests for ARP cache.
The third argument to ioctl must point to one of these structures.
The caller specifies the Internet address for the entry to be deleted.
The caller specifies the Internet address, and the corresponding Ethernet address is returned along with the flags.
These three requests are normally issued by the arp program.
These ARP-related ioctl requests are not supported on some newer systems, which use routing sockets for these ARP operations.
Notice that there is no way with ioctl to list all the entries in the ARP cache.
On many systems, the arp command, when invoked with the -a flag (list all entries in the ARP cache), reads the kernel's memory (/dev/kmem) to obtain the current contents of the ARP cache.
We will see an easier (and better) way to do this using sysctl, which only works on some systems (Section 18.4)
We now use our get_ifi_info function to return all of a host's IP addresses, followed by an ioctl of SIOCGARP for each IP address to obtain and print the hardware addresses.
We call get_ifi_info to obtain the host's IP addresses and then loop through each address.
On some systems, two ioctl requests are provided to operate on the routing table.
These two requests require that the third argument to ioctl be a pointer to an rtentry structure, which is defined by including the <net/route.h> header.
On systems with routing sockets (Chapter 18), these requests use routing sockets instead of ioctl.
There is no way with ioctl to list all the entries in the routing table.
This operation is usually performed by the netstat program when invoked with the -r flag.
This program obtains the routing table by reading the kernel's memory (/dev/kmem)
As with listing the ARP cache, we will see an easier (and better) way to do this using sysctl in Section 18.4
The ioctl commands that are used in network programs can be divided into six categories:
We will use the socket and file operations, and obtaining the interface list is such a common operation that we developed our own function to do this.
We will use this function numerous times in the remainder of the text.
Only a few specialized programs use the ioctl operations with the ARP cache and routing table.
Next, put some statements in the loop to print the buffer size each time the request is issued, whether or not ioctl returns an error, and upon success print the returned buffer length.
Run the prifinfo program and see how your system handles this request when the buffer size is too small.
Also print the address family for any returned structures whose address family is not the desired value to see what other structures are returned by your system.
Traditionally, the Unix routing table within the kernel has been accessed using ioctl commands.
In Section 17.9, we described the two commands that are provided, SIOCADDRT and SIOCDELRT, to add or delete a route.
We also mentioned that no command exists to dump the entire routing table, and instead programs such as netstat read the kernel memory to obtain the contents of the routing table.
One additional piece to this hodgepodge is that routing daemons such as gated need to monitor ICMP redirect messages that are received by the kernel, and they often do this by creating a raw ICMP socket (Chapter 28) and listening on this socket to all received ICMP messages.
The only type of socket supported in the route domain is a raw socket.
Three types of operations are supported on a routing socket:
A process can send a message to the kernel by writing to a routing socket.
For example, this is how routes are added and deleted.
A process can read a message from the kernel on a routing socket.
This is how the kernel notifies a process that an ICMP redirect has been received and processed, or how it requests a route resolution from an external routing process.
For example, the process sends a message to the kernel on a routing socket asking for all the information on a given route, and the process reads back the response from the kernel on the routing socket.
A process can use the sysctl function (Section 18.4) to either dump the routing table or list all configured interfaces.
The first two operations require superuser privileges on most systems, while the last operation can be performed by any process.
Some newer releases have relaxed the superuser requirement for opening a routing socket and instead restrict only routing socket messages that change the table.
This allows any process to use, for instance, RTM_GET to look up a route without being the superuser.
Technically, the third operation is not performed using a routing socket but invokes the generic sysctl function.
We will see that one of the input parameters is the address family, which is AF_ROUTE for the operations we describe in this chapter, and the information returned is in the same format as the information returned by the kernel on a routing socket.
Unfortunately, not all implementations that support routing sockets provide sysctl.
We will encounter datalink socket address structures as return values contained in some of the messages returned on a routing socket.
This header defines the following macro to return the pointer to the link-layer address:
After a process creates a routing socket, it can send commands to the kernel by writing to the socket and read information from the kernel by reading from the socket.
The first three members of each structure are the same: length, version, and type of message.
The type is one of the constants from the first column in Figure 18.2
The length member allows an application to skip over message types it does not understand.
Constants used to refer to socket address structures in routing messages.
When multiple socket address structures are present, they are always in the order shown in the table.
The next-hop router is this system's gateway to the Internet.
Before showing the source code, we show what we write to the routing socket in Figure 18.5 along with what is returned by the kernel.
Data exchanged with kernel across routing socket for RTM_GET command.
We build a buffer containing an rt_msghdr structure, followed by a socket address structure containing the destination address for the kernel to look up.
This command can be used with any protocol family (that provides a routing table), because the family of the address to look up is contained in the socket address structure.
Our unproute.h header includes some files that are needed and then includes our unp.h file.
The constant BUFLEN is the size of the buffer that we allocate to hold our message to the kernel, along with the kernel's reply.
We need room for one rt_msghdr structure and possibly eight socket address structures (the maximum number that is ever returned on a routing socket)
We store our process ID and a sequence number of our choosing in the structure.
We will compare these values in the responses that we read, looking for the correct reply.
All we set are the address length, the address family, and the address.
We write the message to the kernel and read back the reply.
Since other processes may have routing sockets open, and since the kernel passes a copy of all routing messages to all routing sockets, we must check the message type, sequence number, and process ID to verify that the message received is the one we are waiting for.
The last half of this program is shown in Figure 18.7
Our program then goes through the rti_info array, doing what it wants with all the non-null pointers in the array.
Each of the four possible addresses are printed, if present.
If the bit is set, the corresponding element in the rti_info array is set to the pointer to the socket address structure; otherwise, the array element is set to a null pointer.
The socket address structures are variable-length, but this code assumes that each has an sa_len field specifying its length.
This value represents a mask of all zero bits, which we printed as 0.0.0.0 for the network mask of the default route in our earlier example.
Figure 18.9 Build array of pointers to socket address structures in routing message.
Figure 18.10 Convert a mask value to its presentation format.
In this example, we want to read the kernel's reply because the reply contains the information we are looking for.
But in general, the return value from our write to the routing socket tells us if the command succeeded or not.
If that is all the information we need, we can call shutdown with a second argument of SHUT_RD immediately after opening the socket to prevent a reply from being sent.
Similarly, an error return of EEXIST from write when adding a route means the entry already exists.
In our example in Figure 18.6 , if the routing table entry does not exist (say our host does not have a default route), then write returns an error of ESRCH.
Our main interest in routing sockets is the use of the sysctl function to examine both the routing table and interface list.
Whereas the creation of a routing socket (a raw socket in the AF_ROUTE domain) requires superuser privileges, any process can examine the routing table and interface list using sysctl.
This function uses names that look like SNMP management information base (MIB) names.
The name argument is an array of integers specifying the name, and namelen specifies the number of elements in the array.
The first element in the array specifies which subsystem of the kernel the request is directed to.
The second element specifies some part of that subsystem, and so on.
Figure 18.11 shows the hierarchical arrangement, with some of the constants used at the first three levels.
To fetch a value, oldp points to a buffer into which the kernel stores the value.
If the buffer is not large enough, ENOMEM is returned.
As a special case, oldp can be a null pointer and oldlenp a non-null pointer, and the kernel can determine how much data the call would have returned and returns this size through oldlenp.
To set a new value, newp points to a buffer of size newlen.
The sysctl man page details all the various system information that can be obtained with this function: information on the filesystems, virtual memory, kernel limits, hardware, and so on.
Our interest is in the networking subsystem, designated by the first element of the name array being.
The next level specifies the protocol using one of the IPPROTO_xxx constants.
We will show an example of this use of sysctl at the end of this section.
AF_LINK—Get or set link-layer information such as the number of PPP interfaces.
AF_ROUTE—Return information on either the routing table or interface list.
AF_UNSPEC—Get or set some socket-layer variables such as the maximum size of a socket send or receive buffer.
The information returned by these four operations is returned through the oldp pointer in the call to sysctl.
If this address family is 0, the routing tables for all address families are returned.
The routing table is returned as a variable number of RTM_GET messages, with each message followed by up to four socket address structures: the destination, gateway, network mask, and cloning mask of the routing table entry.
All that changes with this sysctl operation is that one or more of these messages are returned by the kernel.
All ARP cache entries in the routing table have the RTF_LLINFO flag bit set.
The information is returned in the same format as the previous item.
We will say more about interface indexes in Section 18.6
All the addresses assigned to each interface are also returned, and if name [3] is nonzero, only addresses for that address family are returned.
We now provide a simple example of sysctl with the Internet protocols to check whether UDP checksums are enabled.
Some UDP applications (e.g., BIND) check whether UDP checksums are enabled when they start, and if not, they try to enable them.
Naturally, it takes superuser privileges to enable a feature such as this, but all we do now is check whether the feature is enabled or not.
We allocate an integer array with four elements and store the constants that correspond to the hierarchy shown in Figure 18.11
Since we are only fetching a variable and not setting a new value, we specify a null pointer for the newp argument to sysctl and a value of 0 for the newlen argument.
This function calls sysctl with the NET_RT_IFLIST command to return the interface list for a specified address family.
The array mib is initialized as shown in Figure 18.12 to return the interface list and all configured addresses of the specified family.
In the first call, the third argument is null, which returns the buffer size required to hold all the interface information in the variable pointed to by lenp.
Space is then allocated for the buffer and sysctl is called again, this time with a non-null third argument.
This time, the variable pointed to by lenp will return with the amount of.
A pointer to the buffer is also returned to the caller.
We declare the local variables and then call our net_rt_iflist function.
The for loop steps through each routing message in the buffer filled in by sysctl.
Recall that the first three members of all three structures are identical, so it doesn't matter which of the three structures we use to look at the type member.
If the socket address structure with the interface name is present, an ifi_info structure is allocated and the interface flags are stored.
The expected family of this socket address structure is AF_LINK , indicating a datalink socket address structure.
Otherwise, a string containing the interface index is stored as the name.
An RTM_NEWADDR message is returned by sysctl for each address associated with the interface: the primary address and all aliases.
If we have already filled in the IP address for this interface, then we are dealing with an alias.
In that case, if the caller wants the alias address, we must allocate memory for another ifi_info structure, copy the fields that have been filled in, and then fill in the addresses that have been returned.
If the interface supports broadcasting, the broadcast address is returned, and if the interface is a point-to-point interface, the destination address is returned.
These four functions are used in many places where it is necessary to describe an interface.
The basic concept is that each interface has a unique name and a unique positive index (0 is never used as an index)
Returns: pointer to interface name if OK, NULL on error.
The memory for this array, along with the names pointed to by the array members, is dynamically obtained and is returned by calling if_freenameindex.
We now provide an implementation of these four functions using routing sockets.
This function is nearly identical to the previous function, but instead of looking for an interface name, we compare the interface index against the caller's argument.
Also, the second argument to our net_rt_iflist function is the desired index, so the result should contain the information for only the desired interface.
When a match is found, the interface name is returned and it is also null-terminated.
We call our net_rt_iflist function to return the interface list.
We also use the returned size as the size of the buffer that we allocate to contain the array of if_nameindex structures we return.
This is an overestimate, but it is simpler than making two passes through the interface list: one to count the number of interfaces and the total sizes of the names and another to fill in the information.
We create the if_nameindex array at the beginning of this buffer and store the interface names starting at the end of the buffer.
We process all the messages looking for RTM_IFINFO messages and the datalink socket address structures that follow.
The interface name and index are stored in the array we are building.
This function is trivial because we stored both the array of structures and the names in the same buffer.
If we had called malloc for each name, to free the memory, we would have to go through the entire array, free the memory for each name, and then free the array.
The last socket address structure that we encounter in this text is the sockaddr_dl structure, the variable-length datalink socket address structure.
Berkeley-derived kernels associate these with interfaces, returning the interface index, name, and hardware address in one of these structures.
Five types of messages can be written to a routing socket by a process and 15 different messages can be returned by the kernel asynchronously on a routing socket.
We showed an example where the process asks the kernel for information on a routing table entry and the kernel responds with all the details.
These kernel responses contain up to eight socket address structures and we have to parse this message to obtain each piece of information.
The sysctl function is a general way to fetch and store OS parameters.
The changes required by IPv6 to the sockets API include four functions to map between interface names and their indexes.
Berkeley-derived implementations already associate an index with each interface, so we are easily able to implement these functions using sysctl.
As with routing sockets, the only type of socket supported in the key domain is a raw socket.
On systems where privileges are segmented, there must be an individual privilege for opening key management sockets.
On regular UNIX systems, opening a key management socket is limited to the superuser.
IPsec provides security services to packets based on security associations, or SAs.
An SA describes a combination of source and destination addresses (and optionally, transport protocol and ports), mechanism (e.g., authentication), and keying material.
More than one SA (e.g., authentication and encryption) can apply to a single stream of traffic.
The set of security associations stored for use on a system is called the security association database, or SADB.
For this reason, PF_KEY sockets are not specific to IPsec.
The security policy database describes requirements for traffic; for example, traffic between host A and host B must be authorized using IPsec AH, and any that is not must be dropped.
The SADB describes how to perform the required security steps, such as, if traffic between host A and host B is using IPsec AH, then the SADB contains the algorithm and key to use.
Unfortunately, there is no standard mechanism to maintain the SPDB.
PF_KEY allows maintenance of the SADB, but not the SPDB.
KAME's IPsec implementation uses PF_KEY extensions for SPDB maintenance, but there is no standard for this.
Three types of operations are supported on key management sockets:
A process can send a message to the kernel and all other processes with open key management sockets by writing to a key management socket.
This is how SADB entries are added and deleted, and how processes that do their own security like OSPFv2 can request a key from a key management daemon.
A process can read a message from the kernel (or another process) on a key management socket.
The kernel can use this facility to request that a key management daemon install a security association for a new TCP session that policy requires be protected.
A process can send a dump request message to the kernel, and the kernel will reply with a dump of the current SADB.
This is a debugging feature that may not be available on all systems.
All messages on a key management socket have the same basic header, shown in Figure 19.1
Each message may be followed by various extensions, depending on what additional information is available or required.
Each sadb_msg header will be followed by zero or more extensions.
Most message types have required and optional extensions; we will describe these as we describe each message type.
We now show several examples and the messages and extensions involved in several common operations on key management sockets.
Not all types of security associations are supported by all implementations.
When requesting a specific type whose table is empty, the errno ENOENT is returned.
Our program to dump the SADB follows in Figure 19.5
This is our first encounter with the POSIX getopt function.
The third argument is a character string specifying the characters that we allow as command-line arguments, just t in this example.
It is followed by a colon, indicating that the option takes an argument.
This function works with four global variables that are defined by including <unistd.h>
Before calling getopt, we set opterr to 0 to prevent the function from writing error messages to standard error in case of an error, because we want to handle these.
This requires system privileges, as described earlier, since this allows access to sensitive keying material.
We first zero out the sadb_msg struct so that we can skip initializing the fields that we wish to remain zero.
We fill in each remaining field in the sadb_msg struct individually.
We set the length to the length of the base header with no extensions since the dump message does not take extensions.
Finally, we set the process ID (PID) to our own PID since all messages must be identified by the PID of the sender.
We don't show this routine since it is long and uninteresting, but it is included in the freely available source code.
This routine accepts a message that is being written to or has been received from a key management socket and prints all the information from the message in a human-readable form.
We loop, reading replies and printing them using our print_sadb_msg function.
The last message in the dump sequence has a message sequence number of zero, so we use this as our "end-of-file" indication.
This program takes a single optional argument, which is the type of SA to dump.
By specifying a command-line argument, the user can select which type of SAs to dump.
This program uses our getsatypebyname function, which returns the type value for a text string.
Finally, we call the sadb_dump function we defined above to do all the work.
The following is a sample run of the dump program on a system with two static SAs.
Although manual specification of keying material does not lead easily to key changes, which are crucial to avoid cryptanalysis attacks, it is quite easy to configure: Alice and Bob agree on a key and algorithms to use out-of-band, and proceed to use them.
We show the steps needed to create and send an SADB_ADD message.
The SADB_ADD message requires three extensions: SA, address and key.
It can also optionally contain other extensions: lifetime, identity, and sensitivity.
This value, combined with the destination address and protocol in use (e.g., IPsec AH), uniquely identifies an SA.
When receiving a packet, this value is used to look up the SA for that packet; when sending a packet, this value is inserted into the packet for the other end to use.
It has no other meaning, so these values can be allocated sequentially, randomly, or using any method the destination system prefers.
The sadb_sa_reply field specifies the window size for replay protection.
Since static keying prevents replay protection, we will set this to zero.
Possible values for these fields are listed in Figure 19.8
This flag requests perfect forward security , that is, the value of this key must not be dependent on any previous keys or some master key.
This flag value is used when requesting keys from a key management application and is not used when adding static associations.
The next required extensions for an SADB_ADD command are the addresses.
This permits an SA to match more than one address.
We show our program to add a static SADB entry in Figure 19.11
As before, we open a PF_KEY socket and save our PID for later.
We build the common message header for the SADB_ADD message.
We don't set the sadb_msg_len element until just before writing the message since it must reflect the entire length of the message.
The len variable keeps a running length of the message, and the p pointer always points to the first unused byte in the buffer.
Next, we add the required SA extension (Figure 19.6 )
The sadb_sa_spi field must be in network byte order, so we call htonl on the host order value that was passed to the function.
We set the authentication algorithm to the algorithm value specified on the command line, and specify no encryption with SADB_EALG_NONE.
We set the protocol to 0, meaning that this association applies to all protocols.
We add the authentication key to the message as an SADB_EXT_KEY_AUTH extension.
We calculate the length field the same way as for the addresses, to add the required padding for the variable-length key.
We set the number of bits and copy the key data to follow the extension header.
We print out the message with our print_sadb_msg function, and write it to the socket.
We read messages from the socket until we receive one that is addressed to our PID and is an SADB_ADD message.
We then print that message with the print_sadb_msg function and exit.
Note that the reply echoes the request without the key.
After adding the SA to the database, we ping 127.0.0.1 to cause the SA to be used, then dump the database to see what was added.
This is an artifact of this implementation, not a general property of PF_KEY sockets.
The kernel returns an extension (numbered 19) that our dump program doesn't understand.
A lifetime extension (Figure 19.12 ) is returned containing the current lifetime information of the SA.
The kernel sends an SADB_EXPIRE message when the soft lifetime has been reached; the SA will not be used after its hard lifetime has been reached.
As of this writing, the IETF IPsec working group is working on a replacement for IKE.
If a daemon can handle multiple SA types, it sends multiple SADB_REGISTER messages, each registering a single type.
In its SADB_REGISTER reply message, the kernel includes a supported algorithms extension, indicating what encryption and/or authentication mechanisms are supported with what key lengths.
Our first example program, shown in Figure 19.15, simply registers with the kernel for a given mechanism and prints the supported algorithms reply.
Since messages will be addressed to us using our PID, we store it for comparison later.
We zero out the message and then fill in the individual fields needed.
We display the message that we're sending using our print_sadb_msg function, and send the message to the socket.
We read messages from the socket and wait for the reply to our register message.
The reply is addressed to our PID and is a SADB_REGISTER message.
It contains a list of supported algorithms, which we print with our print_sadb_msg function.
When the kernel needs to communicate with a peer and policy says that an SA is required but one is not available, the kernel sends an SADB_ACQUIRE message to key management sockets that have registered the SA type required, containing a proposal extension describing the kernel's proposed algorithms and key lengths.
The proposal may be a combination of what is supported by the system and preconfigured policy that limits what is permitted for this communication.
The proposal is a list of algorithms, key lengths, and lifetimes, in order of preference.
When a key management daemon receives an SADB_ACQUIRE message, it performs the acts required to choose a key that fits one of the kernel's proposed combinations, and installs this key in the kernel.
It uses the SADB_GETSPI message to ask the kernel to select an SPI from a desired range.
The kernel's response to the SADB_GETSPI message includes creating an SA in the larval state.
The daemon then negotiates security parameters with the remote end using the SPI supplied by the kernel, and uses the SADB_UPDATE message to complete the SA and cause it to enter the mature state.
Dynamically created SAs generally have both a soft and a hard lifetime associated with them.
When either lifetime expires, the kernel sends an SADB_EXPIRE message, indicating whether the soft or hard lifetime has expired.
If the soft lifetime has expired, the SA has entered.
If the hard lifetime has expired, the SA has entered the dead state, in which it is no longer used for security purposes and will be removed from the SADB.
Key management sockets are used to communicate SAs to the kernel, key management daemons, and to other security consumers such as routing daemons.
SAs can be installed statically or dynamically via a key negotiation protocol.
Dynamic keys can have associated lifetimes; when the soft lifetime is reached, the key management daemon is informed.
If an SA is not replaced before the hard lifetime is reached, the SA can no longer be used.
Ten messages are exchanged between the process and kernel on key management sockets.
Each message type has associated extensions, some required and some optional.
Each message that is sent by a process is echoed to all other open key management sockets, removing any extensions containing sensitive data.
In this chapter, we will describe broadcasting and in the next chapter, we will describe multicasting.
All the examples in the text so far have dealt with unicasting: a process talking to exactly one other process.
Indeed, TCP works with only unicast addresses, although UDP and raw IP support other paradigms.
Figure 20.1 shows a comparison of the different types of addressing.
Anycasting allows addressing one (usually the "closest" by some metric) system out of a set of systems that usually provides identical services.
However, RFC 3513's anycasting only permits routers to have anycast addresses; hosts may not provide anycasting services.
As of this writing, there is no API defined for using anycast addresses.
There is work in progress to refine the IPv6 anycast architecture, and hosts may be able to dynamically provide anycasting services in the future.
Broadcasting and multicasting require datagram transport such as UDP or raw IP; they cannot work with TCP.
One use for broadcasting is to locate a server on the local subnet when the server is assumed to be on the local subnet but its unicast IP address is not known.
Another use is to minimize the network traffic on a LAN when there are multiple clients communicating with a single server.
There are numerous examples of Internet applications that use broadcasting for this purpose.
ARP— Although this is a protocol that lies underneath IPv4, and not a user application, ARP broadcasts a request on the local subnet that says, "Will the system with an IP address of a.b.c.d please identify yourself and tell me your hardware address?" ARP uses link-layer broadcast, not IP-layer, but is an example of a use of broadcasting.
DHCP— The client assumes a server or relay is on the local subnet and sends its request to.
Network Time Protocol (NTP)— In one common scenario, an NTP client is configured with the IP address of one or more servers to use, and the client polls the servers at some frequency (every 64 seconds or longer)
The client updates its clock using sophisticated algorithms based on the time-of-day returned by the servers and the RTT to the servers.
But on a broadcast LAN, instead of making each of the clients poll a single server, the server can broadcast the current time every 64 seconds for all the clients on the local subnet, reducing the amount of network traffic.
Routing daemons— The oldest routing daemon, routed, which implements RIP version 1, broadcasts its routing table on a LAN.
This allows all other routers attached to the LAN to receive these routing announcements, without each router having to be configured with the IP addresses of all its neighboring routers.
This feature can also be used by hosts on the LAN listening to these routing announcements and updating their routing tables accordingly.
We must note that multicasting can replace both uses of broadcasting (resource discovery and reducing network traffic) and we will describe the problems with broadcasting later in this chapter and the next chapter.
The router normally does not forward the datagram on to the 192.168.42/24 subnet.
Some systems have a configuration option that allows subnet-directed broadcasts to be forwarded (Appendix E of TCPv1)
Forwarding subnet-directed broadcasts enables a class of denial-of-service attacks called "amplification" attacks; for instance, sending an ICMP echo request to a subnetdirected broadcast address can cause multiple replies to be sent for a single request.
Combined with a forged IP source address, this results in a bandwidth utilization attack against the victim system, so it's advisable to leave this configuration option off.
For this reason, it's inadvisable to design an application that relies on forwarding of subnet-directed broadcasts except in a controlled environment, where you know it's safe to turn them on.
Other systems send one copy of the datagram out from each broadcastcapable interface.
For portability, however, if an application needs to send a broadcast out from all broadcast-capable interfaces, it should obtain the interface configuration (Section 17.6) and do one sendto for each broadcast-capable interface with the destination set to that interface's broadcast address.
Before looking at broadcasting, let's make certain we understand the steps that take place when a UDP datagram is sent to a unicast address.
The UDP layer prepends a UDP header and passes the UDP datagram to the IP layer.
The packet is then sent as an Ethernet frame with that 48-bit address as the destination Ethernet address.
Since they are not equal, the interface ignores the frame.
With a unicast frame, there is no overhead whatsoever to this host.
The Ethernet interface on the host on the right also sees the frame pass by, and when it compares the destination Ethernet address with its own Ethernet address, they are equal.
This interface reads in the entire frame, probably generates a hardware interrupt when the frame is complete, and the device driver reads the frame from the interface memory.
Since the frame type is 0x0800, the packet is placed on the IP input queue.
Also recall our discussion of the strong end system model and the weak end system model in Section 8.8
Since the destination address is one of the host's own IP addresses, the packet is accepted.
The UDP layer looks at the destination port (and possibly the source port, too, if the UDP socket is connected), and in our example, places the datagram onto the appropriate socket receive queue.
The process is awakened, if necessary, to read the newly received datagram.
The key point in this example is that a unicast IP datagram is received by only the one host specified by the destination IP address.
When the host on the left sends the datagram, it notices that the destination IP address is the subnet-directed broadcast address and maps this into the Ethernet address of 48 one bits: ff:ff:ff:ff:ff:ff.
This causes every Ethernet interface on the subnet to receive the frame.
The two hosts on the right of this figure that are running IPv4 will both receive the frame.
Since the Ethernet frame type is 0x0800, both hosts pass the packet to the IP layer.
Since the destination IP address matches the broadcast address for each of the two hosts, and since the protocol field is 17 (UDP), both hosts pass the packet up to UDP.
Nothing special needs to be done by an application to receive a broadcast UDP datagram: It just creates a UDP socket and binds the application's port number to the socket.
This host must not send an ICMP "port unreachable," as doing so could generate a broadcast storm: a condition where lots of hosts on the subnet generate a response at about the same time, leading to the network being unusable for a period of time.
In addition, it's not clear what the sending host would do with an ICMP error: What if some receivers report errors and others don't?
In this example, we also show the datagram that is output by the host on the left being delivered to itself.
We also assume that the sending application has bound the port that it is sending to (520), so it will receive a copy of each broadcast datagram it sends.
In general, however, there is no requirement that a process bind a UDP port to which it sends datagrams.
A network could use a physical loopback, but this can lead to problems in the case of network faults (such as an unterminated Ethernet)
This example shows the fundamental problem with broadcasting: Every IPv4 host on the subnet that is not participating in the application must completely process the broadcast UDP datagram all the way up the protocol stack, through and including the UDP layer, before discarding the datagram.
For applications that generate IP datagrams at a high rate (audio or video, for example), this unnecessary processing can severely affect these other hosts on the subnet.
We will see in the next chapter how multicasting gets around this problem to some extent.
This is the port used by the routed daemon to exchange RIP packets.
The command-line argument is the subnet-directed broadcast address for the secondary Ethernet.
We type a line of input, the program calls sendto, and the error EACCES is returned.
The reason we receive the error is that we are not allowed to send a datagram to a broadcast destination address unless we explicitly tell the kernel that we will be broadcasting.
Solaris 2.5, on the other hand, accepts the datagram destined for the broadcast address even if we do not specify the socket option.
The POSIX specification requires the SO_BROADCAST socket option to be set to send a broadcast packet.
This option was added to 4.3BSD and any process was allowed to set the option.
This version sets the SO_BROADCAST socket option and prints all the replies received within five seconds.
SO_BROADCAST socket option is set and a signal handler is installed for SIGALRM.
The next two steps, fgets and sendto, are similar to previous versions of this function.
But since we are sending a broadcast datagram, we can receive multiple replies.
We call recvfrom in a loop and print all the replies received within five seconds.
After five seconds, SIGALRM is generated, our signal handler is called, and recvfrom returns the error EINTR.
Each time we must type a line of input to generate the output UDP datagram.
Each time we receive three replies, and this includes the sending host.
As we said earlier, the destination of a broadcast datagram is all the hosts on the attached network, including the sender.
Each reply is unicast because the source address of the request, which is used by each server as the destination address of the reply, is a unicast address.
All the systems report the same time because all run NTP.
Berkeley-derived kernels do not allow a broadcast datagram to be fragmented.
This is a policy decision that has existed since 4.2BSD.
There is nothing that prevents a kernel from fragmenting a broadcast datagram, but the feeling is that broadcasting puts enough load on the network as it is, so there is no need to multiply this load by the number of fragments.
We can see this scenario with our program in Figure 20.5
We redirect standard input from a file containing a 2,000-byte line, which will require fragmentation on an Ethernet.
For portability, however, an application that needs to broadcast should determine the MTU of the outgoing interface using the SIOCGIFMTU ioctl, and then subtract the IP and transport header lengths to determine the maximum payload size.
Alternately, it can pick a common MTU, like Ethernet's 1500, and use it as a constant.
A race condition is usually when multiple processes are accessing data that is shared among them, but the correct outcome depends on the execution order of the processes.
Since the execution order of processes on typical Unix systems depends on many factors that may vary between executions, sometimes the outcome is correct, but sometimes the outcome is wrong.
The hardest type of race conditions to debug are those in which the outcome is normally correct and only occasionally is the outcome wrong.
We will talk more about these types of race conditions in Chapter 26, when we discuss mutual exclusion variables and condition variables.
Race conditions are always a concern with threads programming since so much data is shared among all the threads (e.g., all the global variables)
Race conditions of a different type often exist when dealing with signals.
The problem occurs because a signal can normally be delivered at anytime while our program is executing.
An example is an easy way to see this problem.
A race condition exists in Figure 20.5; take a few minutes and see if you can find it.
When we make these changes to the function and then type the first line of input, the line is sent as a broadcast and we set the alarm for one second in the future.
We block in the call to recvfrom, and the first reply then arrives for our socket, probably within a few milliseconds.
The reply is returned by recvfrom, but we then go to sleep for one second.
Additional replies are received, and they are placed into our socket's receive buffer.
But while we are asleep, the alarm timer expires and the SIGALRM signal is generated: Our signal handler is called, and it just returns and interrupts the sleep in which we are blocked.
We then loop around and read the queued replies with a one-second pause each time we print a reply.
When we have read all the replies, we block again in the call to recvfrom, but the timer is not running.
The fundamental problem is that our intent is for our signal handler to interrupt a blocked recvfrom, but the signal can be delivered at any time, and we can be executing anywhere in the infinite for loop when the signal is delivered.
We now examine four different solutions to this problem: one incorrect solution and three different correct solutions.
Our first (incorrect) solution reduces the window of error by blocking the signal from being delivered while we are executing the remainder of the for loop.
Figure 20.6 Block signals while executing within the for loop (incorrect solution)
We declare a signal set, initialize it to the empty set (sigemptyset), and then turn on the bit corresponding to SIGALRM (sigaddset)
Before calling recvfrom, we unblock the signal (so that it can be delivered while we are blocked) and then block it as soon as recvfrom returns.
If the signal is generated (i.e., the timer expires) while it is blocked, the kernel remembers this fact, but cannot deliver the signal (i.e., call our signal handler) until it is unblocked.
This is the fundamental difference between the generation of a signal and its delivery.
Chapter 10 of APUE provides additional details on all these facets of POSIX signal handling.
If we compile and run this program, it appears to work fine, but then most programs with a race condition work most of the time! There is still a problem: The unblocking of the signal, the call to recvfrom, and the blocking of the signal are all independent system calls.
Assume recvfrom returns with the final datagram reply and the signal is delivered between the recvfrom and the blocking of the signal.
We have reduced the window, but the problem still exists.
A variation of this solution is to have the signal handler set a global flag when the signal is delivered.
The flag is initialized to 0 each time alarm is called.
Our dg_cli function checks this flag before calling recvfrom and does not call it if the flag is nonzero.
If the signal was generated during the time it was blocked (after the previous return from recvfrom), and when the signal is unblocked in this piece of code, it will be delivered before sigprocmask returns, setting our flag.
But there is still a small window of time between the testing of the flag and the call to recvfrom when the signal can be generated and delivered, and if this happens, the call to recvfrom will block forever (assuming, of course, no additional replies are received)
The final argument to pselect is a pointer to our sigset_empty variable, which is a signal set with no signals blocked, that is, all signals are unblocked.
Before returning, the signal mask of the process is reset to its value when pselect was called.
The key to pselect is that the setting of the signal mask, the testing of the descriptors, and the resetting of the signal mask are atomic operations with regard to the calling process.
If our socket is readable, we call recvfrom, knowing it will not block.
Our reason for showing this incorrect implementation is to show the three steps involved: setting the signal mask to the value specified by the caller along with saving the current mask, testing the descriptors, and resetting the signal mask.
Another correct way to solve our problem is not to use the ability of a signal handler to interrupt a blocked system call, but to call siglongjmp from the signal handler instead.
This is called a nonlocal goto because we can use it to jump from one function back to another.
Figure 20.9 Use of sigsetjmp and siglongjmp from signal handler.
We allocate a jump buffer that will be used by our function and its signal handler.
This will cause the for loop in dg_cli to terminate.
Using sigsetjmp and siglongjmp in this fashion guarantees that we will not block forever in recvfrom because of a signal delivered at an inopportune time.
However, this introduces another potential problem: If the signal is delivered while printf is in the middle of its output, we will effectively jump out of the middle of printf and back to our sigsetjmp.
This may leave printf with inconsistent private data structures, for example.
To prevent this, we should combine the signal blocking and unblocking from Figure 20.6 with the nonlocal goto.
This makes this solution unwieldy, as the signal blocking has to occur around any function that may behave poorly as a result of being interrupted in the middle.
There is yet another correct way to solve our problem.
Instead of having the signal handler just return and hopefully interrupt a blocked recvfrom, we have the signal handler use IPC to notify our dg_cli function that the timer has expired.
This is somewhat similar to the proposal we made earlier for the signal handler to set the global had_alarm when the timer expired, because that global variable was being used as a form of IPC (shared memory between our function and the signal handler)
The problem with that solution, however, was our function had to test this variable, and this led to timing problems if the signal was delivered at about the same time.
What makes this such a nice solution is that the testing for the pipe being readable is done using select.
We test for either the socket being readable or the pipe being readable.
We create a normal Unix pipe and two descriptors are returned.
We could also use socketpair and get a full-duplex pipe.
On some systems, notably SVR4, a normal Unix pipe is always full-duplex and we can read from either end and write to either end.
We select on both sockfd, the socket, and pipefd[0], the read end of the pipe.
When SIGALRM is delivered, our signal handler writes one byte to the pipe, making the read end readable.
Therefore, if select returns EINTR, we ignore the error, knowing that the read end of the pipe will also be readable, and that will terminate the for loop.
When the read end of the pipe is readable, we read the null byte that the signal handler wrote and ignore it.
But this tells us that the timer expired, so we break out of the infinite for loop.
Figure 20.10 Using a pipe as IPC from signal handler to our function.
Broadcasting sends a datagram that all hosts on the attached subnet receive.
The disadvantage in broadcasting is that every host on the subnet must process the datagram, up through the UDP layer in the case of a UDP datagram, even if the host is not participating in the application.
For high data rate applications, such as audio or video, this can place an excessive processing load on these hosts.
We will see in the next chapter that multicasting solves this problem because only the hosts that are interested in the application receive the datagram.
Using a version of our UDP echo client that sends a broadcast to the daytime server and then prints all the replies that are received within five seconds, we looked at race conditions with the SIGALRM signal.
Since the use of the alarm function and the SIGALRM signal is a common way to place a timeout on a read operation, this subtle error is common in networking applications.
We showed one incorrect way to solve the problem, and three correct ways:
Using IPC (typically a pipe) from the signal handler to the main loop.
How many replies do you receive? Are the replies always in the same order? Do the hosts on your network have synchronized clocks?
When the alarm expires, does your system return EINTR or readability on the pipe?
As shown in Figure 20.1, a unicast address identifies a single IP interface, a broadcast address identifies all IP interfaces on the subnet, and a multicast address identifies a set of IP interfaces.
Unicasting and broadcasting are the extremes of the addressing spectrum (one or all) and the intent of multicasting is to allow addressing something in between.
A multicast datagram should be received only by those interfaces interested in the datagram, that is, by the interfaces on the hosts running applications wishing to participate in the multicast group.
Also, broadcasting is normally limited to a LAN, whereas multicasting can be used on a LAN or across a WAN.
Indeed, applications multicast across a subset of the Internet on a daily basis.
The additions to the sockets API to support multicasting are simple; they comprise nine socket options: three that affect the sending of UDP datagrams to a multicast address and six that affect the host's reception of multicast datagrams.
Figure 21.1 shows how IP multicast addresses are mapped into Ethernet multicast addresses.
We also show the mapping for IPv6 multicast addresses to allow easy comparison of the resulting Ethernet addresses.
The high-order 5 bits of the group address are ignored in the mapping.
This means that 32 multicast addresses map to a single Ethernet address: The mapping is not one-to-one.
The low-order 2 bits of the first byte of the Ethernet address identify the address as a universally administered group address.
Universally administered means the high-order 24 bits have been assigned by the IEEE and group addresses are recognized and handled specially by receiving interfaces.
We will talk about what it means to join a multicast group shortly.
All multicast-capable routers on a subnet must join this group on all multicast-capable interfaces.
These addresses are reserved for low-level topology discovery or maintenance protocols, and datagrams destined to any of these addresses are never forwarded by a multicast router.
The high-order byte of an IPv6 multicast address has the value ff.
The low-order two bits of the first byte of the Ethernet address specify the address as a locally administered group address.
Locally administered means there is no guarantee that the address is unique to IPv6
There could be other protocol suites besides IPv6 sharing the network and using the same high-order two bytes of the Ethernet address.
As we mentioned earlier, group addresses are recognized and handled specially by receiving interfaces.
The group was renamed in IPv6 to make it clear that it is intended to address routers, printers, and any other IP devices on the subnet as well as hosts.
All routers on a subnet must join these groups on all multicast-capable interfaces.
IPv6 packets also have a hop limit field that limits the number of times the packet is forwarded by a router.
The following values have been assigned to the scope field:
An interface-local datagram must not be output by an interface and a link-local datagram must never be forwarded by a router.
What defines an admin region, a site, or an organization is up to the administrators of the multicast routers at that site or organization.
IPv6 multicast addresses that differ only in scope represent different groups.
IPv4 does not have a separate scope field for multicast packets.
Although use of the IPv4 TTL field for scoping is accepted and recommended practice, administrative scoping is preferred when possible.
This is the high end of the multicast address space.
Addresses in this range are assigned locally by an organization, but are not guaranteed to be unique across organizational boundaries.
An organization must configure its boundary routers (multicast routers at the boundary of the organization) not to forward multicast packets destined to any of these addresses.
For example, an audio/video teleconference may comprise two sessions; one for audio and one for video.
These sessions almost always use different ports and sometimes also use different groups for flexibility in choice when receiving.
For example, one client may choose to receive only the audio session, and one client may choose to receive both the audio and the video session.
If the sessions used the same group address, this choice would not be possible.
We will see shortly that this "join" operation is done by calling setsockopt.
This is the Ethernet address corresponding to the multicast address that the application has just joined using the mapping we showed in Figure 21.1
Nothing special is required to send a multicast datagram: The application does not have to join the multicast group.
The sending host converts the IP address into the corresponding Ethernet destination address and the frame is sent.
Notice that the frame contains both the destination Ethernet address (which is examined by the interfaces) and the destination IP address (which is examined by the IP layers)
This host ignores the frame completely because: (i) the destination Ethernet address does not match the address of the interface; (ii) the destination Ethernet address is not the Ethernet broadcast address; and (iii) the interface has not been told to receive.
The frame is received by the datalink on the right based on what we call imperfect filtering, which is done by the interface using the Ethernet destination address.
We say this is imperfect because it is normally the case that when the interface is told to receive frames destined to one specific Ethernet multicast address, it can receive frames destined to other Ethernet multicast addresses, too.
One of 512 bits in an array is then turned ON.
If the corresponding bit in the array is ON, the frame is received; otherwise, it is ignored.
Over time, as more and more applications use multicasting, this size will probably increase even more.
Some interface cards today already have perfect filtering (the ability to filter out datagrams addressed to all but the desired multicast addresses)
Other interface cards have no multicast filtering at all, and when told to receive a specific multicast address, must receive all multicast frames (sometimes called multicast promiscuous mode)
Another does perfect filtering for 80 multicast addresses, but then has to enter multicast promiscuous mode.
Even if the interface performs perfect filtering, perfect software filtering at the IP layer is still required because the mapping from the IP multicast address to the hardware address is not one-to-one.
Assuming that the datalink on the right receives the frame, since the Ethernet frame type is IPv4, the packet is passed to the IP layer.
Since the received packet was destined to a multicast IP address, the IP layer compares this address against all the multicast addresses that applications on this host have joined.
There are three scenarios that we do not show in Figure 21.4:
A host running an application that has joined the multicast address 225.0.1.1
Since the upper five bits of the group address are ignored in the mapping to the Ethernet address, this host's interface will also be receiving frames with a destination Ethernet address of IP layer.
A host running an application that has joined some multicast group whose corresponding Ethernet address just happens to be one that the interface receives when it is programmed to receive 01:00:5e:00:01:01
This frame will be discarded either by the datalink layer or by the IP layer.
This demonstrates that for a process to receive a multicast datagram, the process must join the group and bind the port.
Multicasting on a single LAN, as discussed in the previous section, is simple.
One host sends a multicast packet and any interested host receives the packet.
The benefit of multicasting over broadcasting is reducing the load on all the hosts not interested in the multicast packets.
Consider the WAN shown in Figure 21.5, which shows five LANs connected with five multicast routers.
Next, assume that some program is started on five of the hosts (say a program that listens to a multicast audio session) and those five programs join a given multicast group.
Each of the five hosts then joins that multicast group.
We also assume that the multicast routers are all communicating with their neighbor multicast router using a multicast routing protocol, which we designate as just MRP.
When a process on a host joins a multicast group, that host sends an IGMP message to any attached multicast routers telling them that the host has just joined that group.
The multicast routers then exchange this information using the MRP so that each multicast router knows what to do if it receives a packet destined to the multicast address.
Multicast routing is still a research topic and could easily consume a book on its own.
We now assume that a process on the host at the top left starts sending packets destined to the multicast address.
Say this process is sending the audio packets that the multicast receivers are waiting to receive.
We can follow the steps taken as the multicast packets go from the sender to all the receivers:
The packets are multicast on the top left LAN by the sender.
It also makes a copy of the packet and sends it to MR3
Making a copy of the packet, as MR2 does here, is something unique to multicast forwarding.
A unicast packet is never duplicated as it is forwarded by routers.
Two less desirable alternatives to multicasting on a WAN are broadcast flooding and sending individual copies to each receiver.
In the first case, the packets would be broadcast by the sender, and each router would broadcast the packets out each of its interfaces, except the arriving interface.
It should be obvious that this increases the number of uninterested hosts and routers that must deal with the packet.
In the second case, the sender must know the IP address of all the receivers and send each one a copy.
Multicasting on a WAN has been difficult to deploy for several reasons.
The biggest problem is that the MRP, described in Section 21.4, needs to get the data from all the senders, which may be located anywhere in the network, to all the receivers, which may similarly be located anywhere.
Another large problem is multicast address allocation: There are not enough IPv4 multicast addresses to statically assign them to everyone who wants one, as is done with unicast addresses.
To send wide-area multicast and not conflict with other multicast senders, you need a unique address, but there is not yet a global multicast address allocation mechanism.
Source-specific multicast, or SSM [Holbrook and Cheriton 1999], provides a pragmatic solution to these problems.
It combines the group address with a system's source address, which solves the problems as follows:
The receivers supply the sender's source address to the routers as part of joining the group.
This removes the rendezvous problem from the network, as the network now knows exactly where the sender is.
However, it retains the scaling properties of not requiring the sender to know who all the receivers are.
This means that the source may pick any multicast address since it becomes the (source, destination) combination that must be unique, and the source already makes it unique.
An SSM session is the combination of source, destination, and port.
Spoofing is still possible, of course, but is much harder.
The API support for traditional multicasting requires only five new socket options.
Source-filtering support, which is required for SSM, adds four more.
A pointer to a variable of the datatype shown is the fourth argument to getsockopt and setsockopt.
All nine of these options are valid with setsockopt, but the six that join and leave a multicast group or source are not allowed with getsockopt.
The change with IPv6 makes them more consistent with other options.
We now describe each of these nine socket options in more detail.
Join an any-source multicast group on a specified local interface.
The following three structures are used when joining or leaving a group:
We say that a host belongs to a given multicast group on a given interface if one or more processes currently belongs to that group on that interface.
More than one join is allowed on a given socket, but each join must be for a different multicast address, or for the same multicast address but on a different interface from previous joins for that address on this socket.
This can be used on a multihomed host where, for example, one socket is created and then for each interface, a join is performed for a given multicast address.
As we noted, IPv6 multicast addresses that differ only in scope represent different groups.
Most implementations have a limit on the number of joins that are allowed per socket.
Leave an any-source multicast group on a specified local interface.
The same structures that we just showed for joining a group are used with this socket option.
If a process joins a group but never explicitly leaves the group, when the socket is closed (either explicitly or on process termination), the membership is dropped automatically.
It is possible for multiple sockets on a host to each join the same group, in which case, the host remains a member of that group until the last socket leaves the group.
Block receipt of traffic on this socket from a source given an existing any-source group membership on a specified local interface.
If all joined sockets have blocked the same source, the system can inform routers that this traffic is unwanted, possibly affecting multicast routing in the network.
It can be used to ignore traffic from rogue senders, for example.
The following two structures are used when blocking or unblocking a source:
The same structures that we just showed for blocking or unblocking sources are used with this socket option.
The same structures that we just showed for joining a source-specific group are used with this socket option.
If a process joins a source-specific group but never explicitly leaves the group, when the socket is closed (either explicitly or on process termination), the membership is dropped automatically.
It is possible for multiple processes on a host to each join the same sourcespecific group, in which case, the host remains a member of that group until the last process leaves the group.
Specify the interface for outgoing multicast datagrams sent on this socket.
Be careful to distinguish between the local interface specified (or chosen) when a process joins a group (the interface on which arriving multicast datagrams will be received) and the local interface specified (or chosen) when a multicast datagram is output.
Berkeley-derived kernels choose the default interface for an outgoing multicast datagram by searching the normal IP routing table for a route to the destination multicast address, and the corresponding interface is used.
This is the same technique used to choose the receiving interface if the process does not specify one when joining a group.
The assumption is that if a route exists for a given multicast address (perhaps the default route in the routing table), then the resulting interface should be used for input and output.
If this is not specified, both will default to 1, which restricts the datagram to the local subnet.
By default, loopback is enabled: A copy of each multicast datagram sent by a process on the host will also be looped back and processed as a received datagram by that host, if the host belongs to that multicast group on the outgoing interface.
This is similar to broadcasting, where we saw that broadcasts sent on a host are also processed as a received datagram on that host (Figure 20.4)
With broadcasting, there is no way to disable this loopback.
This means that if a process belongs to the multicast group to which it is sending datagrams, it will receive its own transmissions.
The loopback that is being described here is an internal loopback performed at the IP layer or higher.
This RFC also states that the loopback option defaults to ON as "a performance optimization for upper-layer protocols that restrict the membership of a group to one process per host (such as a routing protocol)."
We mentioned earlier that nothing special is required to send a multicast datagram.
If no multicast socket option is specified before sending a multicast datagram, the interface for the outgoing datagram will be chosen by the kernel, the TTL or hop limit will be 1, and a copy will be looped back.
To receive a multicast datagram, a process must join the multicast group and it must also bind a UDP socket to the port number that will be used as the destination port number for datagrams.
Joining the group tells the host's IP layer and datalink layer to receive multicast datagrams sent to that group.
Binding the port is how the application specifies to UDP that it wants to receive datagrams sent to that port.
Some applications also bind the multicast address to the socket, in addition to the port.
This prevents any other datagrams that might be received for that port to other unicast, broadcast, or multicast addresses from being delivered to the socket.
Historically, the multicast service interface only required that some socket on the host join the multicast group, not necessarily the socket that binds the port and then receives the multicast datagrams.
There is the potential, however, with these implementations for multicast datagrams to be delivered to applications that are not multicast-aware.
Newer multicast kernels now require that the process bind the port and set any multicast socket option for the socket, the latter being an indication that the application is multicast-aware.
The most common multicast socket option to set is a join of the group.
Solaris differs slightly and only delivers received multicast datagrams to a socket that has both joined the group and bound the port.
For portability, all multicast applications should join the group and bind the port.
The newer multicast service interface requires that the IP layer only deliver multicast packets to a socket if that socket has joined the applicable group and/or source.
This reinforces the requirement to join the group, but relaxes the requirement to bind to the group address.
However, for maximum portability to both the old and new multicast service interfaces, applications should both join the group and bind to the group address.
Some older multicast-capable hosts do not allow the bind of a multicast address to a socket.
A better solution is to hide the differences within the following eight functions:
Returns: current TTL or hop limit if OK, –1 on error.
We can specify the interface on which to join the group by either the interface name (a non-null ifname ) or a.
If neither is specified, the kernel chooses the interface on which the group is joined.
Recall that with IPv6, the interface is specified to the socket option by its index.
With the IPv4 socket option, the interface is specified by its unicast IP address.
If a name is specified for an IPv4 socket, we call ioctl with a request of SIOCGIFADDR to obtain the unicast IP address for the interface.
Note that mcast_leave does not take an interface specification; it always deletes the first matching membership.
This simplifies the library API, but means that programs that require direct control of per-interface membership need to use the setsockopt API directly.
The src, srclen, grp , and grplen arguments must be the same as a previous call to mcast_block_source.
We can specify the interface on which to join the group by either the interface name (a non-null ifname ) or a nonzero interface index (ifindex )
If neither is specified, the kernel chooses the interface on which the group is joined.
If ifindex is greater than 0, then it specifies the interface index; otherwise, if ifname is nonnull, then it specifies the interface name.
If the caller supplied an index, then we just use it directly.
Otherwise, if the caller supplied an interface name, the index is obtained by calling if_nametoindex.
Otherwise, the interface is set to 0, telling the kernel to choose the interface.
The caller's socket address is copied directly into the request's group field.
Recall that the group field is a sockaddr_storage , so it is big enough to handle any socket address type the system supports.
However, to guard against buffer overruns caused by sloppy coding, we check the sockaddr size and return EINVAL if it is too large.
The level argument to setsockopt is determined using the family of the group address and our family_to_level function.
We do not show this trivial function, but the source code is freely available (see the Preface)
If an index was specified, if_indextoname is called, storing the name into our ifreq structure.
If this succeeds, we branch ahead to issue the ioctl.
The caller's name is copied into an ifreq structure, and an ioctl of SIOCGIFADDR returns the unicast address associated with this name.
If an index was not specified and a name was not specified, the interface is set to the wildcard address, telling the kernel to choose the interface.
Since the argument is a socket descriptor and not a socket address structure, we call our sockfd_to_family function to obtain the address family of the socket.
We do not show the source code for all remaining mcast_ XXX functions, but it is freely available (see the Preface)
As we said earlier, none of the multicast socket options needs to be set to send a multicast datagram if the default settings for the outgoing interface, TTL, and loopback option are acceptable.
We run a modified UDP echo server that joins the allhosts group, then run our program specifying the allhosts group as the destination address.
We get a response from both systems on the subnet.
Each reply is unicast because the source address of the request, which is used by each server as the destination address of the reply, is a unicast address.
We mentioned at the end of Section 20.4 that most systems do not allow the fragmentation of a broadcast datagram as a policy decision.
Fragmentation is fine to use with multicasting, as we can easily verify using the same file with a 2,000-byte line.
Announcements The IP multicast infrastructure is the portion of the Internet with inter-domain multicast enabled.
Multicast is widely deployed within enterprises, but being part of the interdomain IP multicast infrastructure is less common.
To receive a multimedia conference on the IP multicast infrastructure, a site needs to know only the multicast address of the conference and the UDP ports for the conference's data streams (audio and video, for example)
A site wishing to announce a session on the IP multicast infrastructure periodically sends a multicast packet containing a description of the session to a well-known multicast group and UDP port.
Sites on the IP multicast infrastructure run a program named sdr to receive these announcements.
This program does a lot: Not only does it receive session announcements, but it also provides an interactive user interface that displays the information and lets the user send announcements.
In this section, we will develop a simple program that only receives these session announcements to show an example of a simple multicast receiving program.
Our goal is to show the simplicity of a multicast receiver, not to delve into the details of this one application.
Figure 21.14 shows our main program that receives periodic SAP/SDP announcements.
All the well-known multicast addresses (see http://www.iana.org/assignments/multicast-addresses) appear in the DNS under the mcast.net hierarchy.
We call our udp_client function to look up the name and port, and it fills in the appropriate socket address structure.
We use the defaults if no command-line arguments are specified; otherwise, we take the multicast address, port, and interface name from the commandline arguments.
We set the SO_REUSEADDR socket option to allow multiple instances of this program to run on a host, and bind the port to the socket.
By binding the multicast address to the socket, we prevent the socket from receiving any other UDP datagrams that may be received for the port.
Binding this multicast address is not required, but it provides filtering by the kernel of packets in which we are not interested.
If the interface name was specified as a command-line argument, it is passed to our function; otherwise, we let the kernel choose the interface on which the group is joined.
We call our loop function, shown in Figure 21.15, to read and print all the announcements.
When one arrives, we place a null byte at the end of the buffer, fix the byte order of the header field, and print the source of the packet and SAP hash.
We check the SAP header to see if it is a type that we handle.
We don't handle SAP packets with IPv6 addresses in the header, or compressed or encrypted packets.
We skip over any authentication data that may be present, skip over the packet content type if it's present, and then print out the contents of the packet.
This announcement describes the NASA coverage on the IP Multicast Infrastructure of a space shuttle mission.
The SDP session description consists of numerous lines of the form.
The value is a structured text string that depends on the type.
The five-tuple consisting of the username, session ID, network type, address type, and address form a globally unique identifier for the session.
The a= lines are attributes; either of the session, if they appear before any m= lines, or of the media, if they appear after a m= line.
Although these are separated by a slash, like the CIDR prefix format, they are not meant to represent a prefix and a mask.
The next m= line specifies that the audio is on port 31954 and may be in any of a number of RTP/AVP payload types, some of which are standard and some of which are specified below using a=rtpmap: attributes.
The IP multicast infrastructure session announcement program in the previous section only received multicast datagrams.
We will now develop a simple program that sends and receives multicast datagrams.
The first part sends a multicast datagram to a specific group every five seconds and the datagram contains the sender's hostname and process ID.
The second part is an infinite loop that joins the multicast group to which the first part is sending and prints every received datagram (containing the hostname and process ID of the sender)
This allows us to start the program on multiple hosts on a LAN and easily see which host is receiving datagrams from which senders.
Figure 21.17 Create sockets, fork, and start sender and receiver.
We create two sockets, one for sending and one for receiving.
We then want the receiving socket to join the multicast group.
Therefore, we must create two sockets: one for sending and one for receiving.
Our udp_client function creates the sending socket, processing the two command-line arguments that specify the multicast address and port number.
This function also returns a socket address structure that is ready for calls to sendto along with the length of this socket address structure.
We create the receiving socket using the same address family that was used for the sending socket.
We set the SO_REUSEADDR socket option to allow multiple instances of this program to run at the same time on a host.
We then allocate room for a socket address structure for this socket, copy its contents from the sending socket address structure (whose address and port were taken from the command-line arguments), and bind the multicast address and port to the receiving socket.
For the join, we specify the interface name as a null pointer and the interface index as 0, telling the kernel to choose the interface.
We fork and then the child is the receive loop and the parent is the send loop.
The main function passes as arguments the socket descriptor, a pointer to a socket address structure containing the multicast destination and port, and the structure's length.
We obtain the hostname from the uname function and build the output line containing it and the process ID.
We send a datagram and then sleep for five seconds.
A socket address structure is allocated to receive the sender's protocol address for each call to recvfrom.
Figure 21.19 Receive all multicast datagrams for a group we have joined.
We run this program on our two systems, freebsd4 and macosx.
We see that each system sees the packets that the other is sending.
It is common for a few hosts on a LAN to synchronize their clocks across the Internet to other NTP hosts and then redistribute this time on the LAN using either broadcasting or multicasting.
In this section, we will develop an SNTP client that listens for NTP broadcasts or multicasts on all attached networks and then prints the time difference between the NTP packet and the host's current time-of-day.
We do not try to adjust the time-of-day, as that takes superuser privileges.
The file ntp.h, shown in Figure 21.20, contains some basic definitions of the NTP packet format.
When the program is executed, the user must specify the multicast address to join as the command-line argument.
If this program is run on a host that does not support multicasting, any IP address can be specified, as only the address family and port are used from this structure.
Note that our udp_client function does not bind the address to the socket; it just creates the socket and fills in the socket address structure.
We allocate space for another socket address structure and fill it in by copying the structure that was filled in by udp_client.
We call our sock_set_wild function to set the IP address to the wildcard and then call bind.
Our get_ifi_info function returns information on all the interfaces and addresses.
The address family that we ask for is taken from the socket address structure that was filled in by udp_client based on the command-line argument.
We call our mcast_join function to join the multicast group specified by the command-line argument for each multicast-capable interface.
All these joins are done on the one socket that this program uses.
Another socket address structure is allocated to hold the address returned by recvfrom and the program enters an infinite loop, reading all the NTP packets that the host receives and calling our sntp_proc function (described next) to process the packet.
Since the socket was bound to the wildcard address, and since the multicast group was joined on all multicast-capable interfaces, the socket should receive any unicast, broadcast, or multicast NTP packet that the host receives.
We first check the size of the packet and then print the version, mode, and server stratum.
If the mode is MODE_CLIENT, the packet is a client request, not a server reply, and we ignore it.
The field in the NTP packet that we are interested in is xmt, the transmit timestamp, which is the 64-bit fixed-point time at which the packet was sent by the server.
We then calculate and print the difference between the host's time-of-day and the NTP server's time-of-day, in microseconds.
One thing that our program does not take into account is the network delay between the server and the client.
But we assume that the NTP packets are normally received as a broadcast or multicast on a LAN, in which case, the network delay should be only a few milliseconds.
To run our program, we first terminated the normal NTP server running on this host, so when our program starts, the time is very close to the server's time.
A multicast application starts by joining the multicast group assigned to the application.
This tells the IP layer to join the group, which in turns tells the datalink layer to receive multicast frames that are sent to the corresponding hardware layer multicast address.
Multicasting takes advantage of the hardware filtering present on most interface cards, and the better the filtering, the fewer the number of undesired packets received.
Using this hardware filtering reduces the load on all the other hosts that are not participating in the application.
Multicasting on a WAN requires multicast-capable routers and a multicast routing protocol.
Until all the routers on the Internet are multicast-capable, multicast is only available to a subset of Internet users.
We use the term "IP multicast infrastructure" to describe the set of all multicastcapable systems on the Internet.
The first six are for receiving, and the last three are for sending.
Are you allowed to bind a multicast address to the socket? If you have a tool such as tcpdump, watch the packets on the network.
What is the source IP address of the datagram you send?
This chapter is a collection of various topics that affect applications using UDP sockets.
First is determining the destination address of a UDP datagram and the interface on which the datagram was received, because a socket bound to a UDP port and the wildcard address can receive unicast, broadcast, and multicast datagrams on any interface.
With UDP, however, each input operation corresponds to a UDP datagram (a record), so a problem arises of what happens when the received datagram is larger than the application's input buffer.
We will discuss the factors affecting when UDP can be used instead of TCP.
In these UDP applications, we must include some features to make up for UDP's unreliability: a timeout and retransmission, to handle lost datagrams, and sequence numbers, to match the replies to the requests.
We develop a set of functions that we can call from our UDP applications to handle these details.
If the implementation does not support the IP_RECVDSTADDR socket option, then one way to determine the destination IP address of a UDP datagram is to bind all the interface addresses and use select.
Most UDP servers are iterative, but there are applications that exchange multiple UDP datagrams between the client and server requiring some form of concurrency.
The final topic is the per-packet information that can be specified as ancillary data for an IPv6 datagram: the source IP address, the sending interface, the outgoing hop limit, and the next-hop address.
Similar information can be returned with an IPv6 datagram: the destination IP address, received interface, and received hop limit.
Interface Index Historically, sendmsg and recvmsg have been used only to pass descriptors across Unix domain sockets (Section 15.7 ), and even this was rare.
But the use of these two functions is increasing for two reasons:
Ancillary data is being used to pass more and more information between the application and the kernel.
As an example of recvmsg , we will write a function named recvfrom_flags , which is similar to recvfrom but also returns the following:
The destination address of the received datagram (from the IP_RECVDSTADDR socket option)
The index of the interface on which the datagram was received (the IP_RECVIF socket option)
To return the last two items, we define the following structure in our unp.h header:
This function is intended to be used with a UDP socket.
The function arguments are similar to recvfrom , except the fourth argument is now a pointer to an integer flag (so that we can return the flags returned by recvmsg ) and the seventh argument is new: It is a pointer to an unp_in_pktinfo structure that will contain the destination IPv4 address of the received datagram and the interface index on which the datagram was received.
When dealing with the msghdr structure and the various MSG_ xxx constants, we encounter lots of differences between various implementations.
Our way of handling these differences is to use C's conditional inclusion feature (#ifdef )
A msghdr structure is filled in and recvmsg is called.
If the destination IP address was returned as control information (Figure 14.9 ), it is returned to the caller.
If the index of the received interface was returned as control information, it is returned to the caller.
Figure 22.3 shows the contents of the ancillary data object that is returned.
The data returned in the ancillary data object is one of these structures, but the three lengths are 0 (name length, address length, and selector length)
We do this to see what happens when we receive a UDP datagram that is larger than the buffer that we pass to the input function (recvmsg in this case)
If the IP_RECVDSTADDR socket option is defined, it is turned on.
The source IP address and port of the server's reply are converted to presentation format by sock_ntop.
We test four additional flags and print a message if any are on.
All Berkeley-derived implementations that support the msghdr structure with the msg_flags member provide this notification.
This is an example of a flag that must be returned from the kernel to the process.
We mentioned in Section 14.3 that one design problem with the recv and recvfrom functions is that their flags argument is an integer, which allows flags to be passed from the process to the kernel, but not vice versa.
Discard the excess bytes and return the MSG_TRUNC flag to the application.
This requires that the application call recvmsg to receive the flag.
Discard the excess bytes, but do not tell the application.2
Keep the excess bytes and return them in subsequent read operations on the socket.
The POSIX specification specifies the first type of behavior: discarding the excess bytes and setting the MSG_TRUNC flag.
Early releases of SVR4 exhibited the third type of behavior.
Since there are such variations in how implementations handle datagrams that are larger than the application's receive buffer, one way to detect the problem is to always allocate an application buffer that is one byte greater than the largest datagram the application should ever receive.
If a datagram is ever received whose length equals this buffer, consider it an error.
Given that TCP is reliable while UDP is not, the question arises: When should we use UDP instead of TCP, and why? We first list the advantages of UDP:
As we show in Figure 20.1, UDP supports broadcasting and multicasting.
Indeed, UDP must be used if the application uses broadcasting or multicasting.
With regard to Figure 2.5, UDP requires only two packets to exchange a request and a reply (assuming the size of each is less than the minimum MTU between the two end-systems)
Also important in this number-of-packet analysis is the number of packet round trips required to obtain the reply.
This becomes important if the latency exceeds the bandwidth, as described in Appendix A of TCPv3
That text shows that the minimum transaction time for a UDP request-reply is RTT + server processing time (SPT)
With TCP, however, if a new TCP connection is used for the request-reply, the minimum transaction time is 2 x RTT + SPT, one RTT greater than the UDP time.
It should be obvious with regard to the second point that if a TCP connection is used for multiple request-reply exchanges, then the cost of the connection's establishment and teardown is amortized across all the requests and replies, and this is normally a better design than using a new connection for each request-reply.
Nevertheless, there are applications that use a new TCP connection for each request-reply (e.g., the older versions of HTTP), and there are applications in which the client and server exchange one request-reply (e.g., the DNS) and then might not talk to each other for hours or days.
We now list the features of TCP that are not provided by UDP, which means that an application must provide these features itself, if they are necessary to the application.
We use the qualifier "necessary" because not all features are needed by all applications.
For example, dropped segments might not need to be retransmitted for a real-time audio application, if the receiver can interpolate the missing data.
Also, for simple request-reply transactions, windowed flow control might not be needed if the two ends agree ahead of time on the size of the largest request and reply.
Positive acknowledgments, retransmission of lost packets, duplicate detection, and sequencing of packets reordered by the network—TCP acknowledges all data, allowing lost packets to be detected.
The implementation of these two features requires that every TCP data segment contain a sequence number that can then be acknowledged.
It also requires that TCP estimate a retransmission timeout value for the connection and that this value be updated continually as network traffic between the two end-systems changes.
Windowed flow control—A receiving TCP tells the sender how much buffer space it has allocated for receiving data, and the sender cannot exceed this.
That is, the amount of unacknowledged data at the sender can never exceed the receiver's advertised window.
Slow start and congestion avoidance—This is a form of flow control imposed by the sender to.
Any form of desired error control must be added to the clients and servers, but applications often use broadcasting or multicasting when some (assumed small) amount of error is acceptable (such as lost packets for audio or video)
Multicast applications requiring reliable delivery have been built (e.g., multicast file transfer), but we must decide whether the performance gain in using multicasting (sending one packet to N destinations versus sending N copies of the packet across N TCP connections) outweighs the added complexity required within the application to provide reliable communications.
Flow control is often not an issue for reasonably sized requests and responses.
The reason is that windowed flow control, congestion avoidance, and slow-start must all be built into the application, along with the features from the previous bullet point, which means we are reinventing TCP within the application.
We should let the vendors focus on better TCP performance and concentrate our efforts on the application itself.
There are exceptions to these rules, especially in existing applications.
But this requires that TFTP include its own sequence number field for acknowledgments, along with a timeout and retransmission capability.
This is partly historical, because in the mid-1980s when it was designed, UDP implementations were faster than TCP, and NFS was used only on LANs, where packet loss is often orders of magnitude less than on WANs.
Similar reasoning (UDP being faster than TCP in the mid-1980s along with a predominance of LANs over WANs) led the precursor of the DCE RPC package (the Apollo NCS package) to also choose UDP over TCP, although current implementations support both UDP and TCP.
We might be tempted to say that UDP usage is decreasing compared to TCP, with good TCP implementations being as fast as the network today, and with fewer application designers wanting to reinvent TCP within their UDP application.
But the predicted increase in multimedia applications over the next decade will see an increase in UDP usage, since multimedia usually implies multicasting, which requires UDP.
If we are going to use UDP for a request-reply application, as mentioned in the previous section, then we must add two features to our client:
Sequence numbers so the client can verify that a reply is for the appropriate request2
These two features are part of most existing UDP applications that use the simple request-reply paradigm: DNS resolvers, SNMP agents, TFTP, and RPC, for example.
We are not trying to use UDP for bulk data transfer; our intent is for an application that sends a request and waits for a reply.
By definition, a datagram is unreliable; therefore, we purposely do not call this a "reliable datagram service." Indeed, the term "reliable datagram" is an oxymoron.
What we are showing is an application that adds reliability on top of an unreliable datagram service (UDP)
The client prepends a sequence number to each request and the server must echo this number back to the client in the reply.
This lets the client verify that a given reply is for the request that was issued.
The old-fashioned method for handling timeout and retransmission was to send a request and wait for N seconds.
If no response was received, retransmit and wait another N seconds.
After this had happened some number of times, the application gave up.
The problem with this technique is that the amount of time required for a datagram to make a round trip on a network can vary from fractions of a second on a LAN to many seconds on a WAN.
Factors affecting the RTT are distance, network speed, and congestion.
Additionally, the RTT between a client and server can change rapidly with time, as network conditions change.
We must use a timeout and retransmission algorithm that take into account the actual RTTs that we measure along with the changes in the RTT over time.
Much work has been focused on this area, mostly relating to TCP, but the same ideas apply to any network application.
We want to calculate the RTO to use for every packet that we send.
To calculate this, we measure the RTT: the actual round-trip time for a packet.
Every time we measure an RTT, we update two statistical estimators: srtt is the smoothed RTT estimator and rttvar is the smoothed mean deviation estimator.
The latter is a good approximation of the standard deviation, but easier to compute since it does not involve a square root.
Given these two estimators, the RTO to use is srtt plus four times rttvar.
Another point made in [Jacobson 1988] is that when the retransmission timer expires, an exponential backoff must be used for the next RTO.
Jacobson's algorithms tell us how to calculate the RTO each time we measure an RTT and how to increase the RTO when we retransmit.
But, a problem arises when we have to retransmit a packet and then receive a reply.
Figure 22.5 shows the following three possible scenarios when our retransmission timer expires:
When the client receives a reply to a request that was retransmitted, it cannot tell to which request the reply corresponds.
In the example on the right, the reply corresponds to the original request, while in the two other examples, the reply corresponds to the retransmitted request.
Karn's algorithm [Karn and Partridge 1991] handles this scenario with the following rules that apply whenever a reply is received for a request that was retransmitted:
If an RTT was measured, do not use it to update the estimators since we do not know to which request the reply corresponds.
Since this reply arrived before our retransmission timer expired, reuse this RTO for the next packet.
Only when we receive a reply to a request that is not retransmitted will we update.
It is not hard to take Karn's algorithm into account when coding our RTT functions, but it turns out that an even better and more elegant solution exists.
In addition to prepending a sequence number to each request, which the server must echo back, we also prepend a timestamp that the server must also echo.
Each time we send a request, we store the current time in the timestamp.
When a reply is received, we calculate the RTT of that packet as the current time minus the timestamp that was echoed by the server in its reply.
Since every request carries a timestamp that is echoed by the server, we can calculate the RTT of every reply we receive.
Furthermore, since all the server does is echo the client's timestamp, the client can use any units desired for the timestamps and there is no requirement at all that the client and server have synchronized clocks.
We will now put all of this together in an example.
Figure 22.7 Outline of RTT functions and when they are called.
When a reply is received but the sequence number is not the one expected, we call recvfrom again, but we do not retransmit the request and we do not restart the retransmission timer that is running.
Notice in the rightmost example in Figure 22.5 that the final reply from the retransmitted request will be in the socket receive buffer the next time the client sends a new request.
That is fine as the client will read this reply, notice that the sequence number is not the one expected, discard the reply, and call recvfrom again.
We call sigsetjmp and siglongjmp to avoid the race condition with the SIGALRM signal we described in Section 20.5
We define one of these structures and numerous other variables.
We want to hide the fact from the caller that we prepend a sequence number and a timestamp to each packet.
The easiest way to do this is to use writev , writing our header (the hdr structure), followed by the caller's data, as a single UDP datagram.
Recall that the output for writev on a datagram socket is a single datagram.
This is easier than forcing the caller to allocate room at the front of its buffer for our use and is also faster than copying our header and the caller's data into one buffer (that we would have to allocate) for a single sendto.
But since we are using UDP and have to specify a destination address, we must use the iovec capability of sendmsg and recvmsg , instead of sendto and recvfrom.
Recall from Section 14.5 that some systems have a newer msghdr structure with ancillary data, while older systems still have the access rights members at the end of the structure.
To avoid complicating the code with #ifdefs to handle these differences, we declare two msghdr structures as static , forcing their initialization to all zero bits by C and then just ignore the unused members at the end of the structures.
The first time we are called, we call the rtt_init function.
We fill in the two msghdr structures that are used for output and input.
We increment the sending sequence number for this packet, but do not set the sending timestamp until we send the packet (since it might be retransmitted, and each retransmission needs the current timestamp)
The current timestamp is obtained by rtt_ts and stored in the hdr structure prepended to the user's data.
We establish a jump buffer for our signal handler with sigsetjmp.
We wait for the next datagram to arrive by calling recvmsg.
We discussed the use of sigsetjmp and siglongjmp along with SIGALRM with Figure 20.9
If we give up, we set errno to ETIMEDOUT and return to the caller.
We wait for a datagram to arrive by calling recvmsg.
When it returns, the datagram's length must be at least the size of our hdr structure and its sequence number must equal the sequence number that was sent.
When the expected reply is received, the pending alarm is turned off and rtt_stop updates the RTT estimators.
We now look at the various RTT functions that were called by dg_send_recv.
This structure contains the variables necessary to time the packets between a client and server.
The first four variables are from the equations given near the beginning of this section.
These constants define the minimum and maximum retransmission timeouts and the maximum number of times we retransmitted.
Figure 22.11 shows a macro and the first two of our RTT functions.
The RTT_RTOCALC macro calculates the RTO as the RTT estimator plus four times the mean deviation estimator.
We convert this to milliseconds and also convert the microsecond value returned by gettimeofday into milliseconds.
The timestamp is then the sum of these two values in milliseconds.
The difference between two calls to rtt_ts is the number of milliseconds between the two calls.
But, we store the millisecond timestamps in an unsigned 32-bit integer instead of a timeval structure.
This function should be called whenever a new packet is sent for the first time.
The return value can then be used as the argument to alarm.
Update our estimators of RTT and mean deviation of RTT.
The second argument is the measured RTT, obtained by the caller by subtracting the received timestamp in the reply from the current timestamp (rtt_ts )
The current RTO is doubled: This is the exponential backoff.
As an example, our client was run twice to two different echo servers across the Internet in the morning on a weekday.
Eight packets were lost to the first server and 16 packets were lost to the second server.
Of the 16 lost to the second server, one packet was lost twice in a row: that is, the packet had to be retransmitted two times before a reply was received.
All other lost packets were handled with a single retransmission.
We could verify that these packets were really lost by printing the sequence number of each received packet.
If a packet is just delayed and not lost, after the retransmission, two replies will be received by the client: one corresponding to the original transmission that was delayed and one corresponding to the retransmission.
Notice we are unable to tell when we retransmit whether it was the client's request or the server's reply that was discarded.
For the first edition of this book, the author wrote a UDP server that randomly discarded packets to test this client.
That is no longer needed; all we have to do is run the client to a server across the Internet and we are almost guaranteed of some packet loss!
One common use for our get_ifi_info function is with UDP applications that need to monitor all interfaces on a host to know when a datagram arrives, and on which interface it arrives.
This allows the receiving program to know the destination address of the UDP datagram, since that address is what determines the socket to which a datagram is delivered, even if the host does not support the IP_RECVDSTADDR socket option.
If the host employs the common weak end system model, the destination IP address may differ from the IP address of the receiving interface.
In this case, all we can determine is the destination address of the datagram, which does not need to be an address assigned to the receiving interface.
Figure 22.15 is the first part of a simple example of this technique with a UDP server that binds all the unicast addresses, all the broadcast addresses, and finally the wildcard address.
Figure 22.15 First part of UDP server that binds all addresses.
Not all implementations require that this socket option be set.
Berkeley-derived implementations, for example, do not require the option and allow a new bind of an already bound port if the new IP address being bound: (i) is not the wildcard, and (ii) differs from all the IP addresses that are already bound to the port.
A child is forked and the function mydg_echo is called for the child.
This function waits for any datagram to arrive on this socket and echoes it back to the sender.
Figure 22.16 shows the next part of the main function, which handles broadcast addresses.
If the interface supports broadcasting, a UDP socket is created and the broadcast address is bound to it.
This time, we allow the bind to fail with an error of EADDRINUSE because if an interface has multiple addresses (aliases) on the same subnet, then each of the different unicast addresses will have the same broadcast address.
In this scenario, we expect only the first bind to succeed.
Figure 22.16 Second part of UDP server that binds all addresses.
A child is spawned and it calls the function mydg_echo.
The final part of the main function is shown in Figure 22.17
This code binds the wildcard address to handle any destination addresses except the unicast and broadcast addresses we have already bound.
A UDP socket is created, the SO_REUSEADDR socket option is set, and the wildcard IP address is bound.
The main function terminates, and the server continues executing all the children that were spawned.
Figure 22.17 Final part of UDP server that binds all addresses.
The fourth argument to this function is the IP address that was bound to the socket.
This socket should receive only datagrams destined to that IP address.
If the IP address is the wildcard, then the socket should receive only datagrams that are not matched by some other socket bound to the same port.
The datagram is read with recvfrom and sent back to the client with sendto.
This function also prints the client's IP address and the IP address that was bound to the socket.
We now run this program on our host solaris after establishing an alias address for the hme0 Ethernet interface.
We can check that all these sockets are bound to the indicated IP address and port using netstat.
We should note that our design of one child process per socket is for simplicity and other designs are possible.
For example, to reduce the number of processes, the program could manage all the descriptors itself using select, never calling fork.
The problem with this design is the added code complexity.
While it is easy to use select for all the descriptors, we would have to maintain some type of mapping of each descriptor to its bound IP address (probably an array of structures) so we could print the destination IP address when a datagram was read from a socket.
It is often simpler to use a single process or a single thread for one operation or descriptor instead of having a single process multiplex many different operations or descriptors.
Most UDP servers are iterative: The server waits for a client request, reads the request, processes the request, sends back the reply, and then waits for the next client request.
But when the processing of the client request takes a long time, some form of concurrency is desired.
The definition of a "long time" is whatever is considered too much time for another client to wait while the current client is being serviced.
With TCP, it is simple to just fork a new child (or create a new thread, as we will see in Chapter 26) and let the child handle the new client.
What simplifies this server concurrency when TCP is being used is that every client connection is unique: The TCP socket pair is unique for every connection.
But with UDP, we must deal with two different types of servers:
First is a simple UDP server that reads a client request, sends a reply, and is then finished with the client.
In this scenario, the server that reads the client request can fork a child and let it handle the request.
The "request," that is, the contents of the datagram and the socket address structure containing the client's protocol address, are passed to the child in its memory image from fork.
The child then sends its reply directly to the client.
Second is a UDP server that exchanges multiple datagrams with the client.
The problem is that the only port number the client knows for the server is its wellknown port.
The client sends the first datagram of its request to that port, but how does the server distinguish between subsequent datagrams from that client and new requests? The typical solution to this problem is for the server to create a new socket for each client, bind an ephemeral port to that socket, and use that socket for all its replies.
This requires that the client look at the port number of the server's first reply and send subsequent datagrams for this request to that port.
An example of the second type of UDP server is TFTP.
To transfer a file using TFTP normally requires many datagrams (hundreds or thousands, depending on the file size), because the protocol sends only 512 bytes per datagram.
The client sends a datagram to the server's wellknown port (69), specifying the file to send or receive.
The server reads the request, but sends its reply from another socket that it creates and bind to an ephemeral port.
All subsequent datagrams between the client and server for this file use the new socket.
This allows the main TFTP server to continue to handle other client requests, which arrive at port 69, while this file transfer takes place (perhaps over seconds, or even minutes)
If we assume a standalone TFTP server (i.e., not invoked by inetd), we have the scenario shown in Figure 22.19
If inetd is used, the scenario involves one more step.
Recall from Figure 13.6 that most UDP servers specify the wait-flag as wait.
In our description following Figure 13.10, we said that this causes inetd to stop selecting on the socket until its child terminates, allowing its child to read the datagram that has arrived on the socket.
The TFTP server that is the child of inetd calls recvfrom and reads the client request.
It then forks a child of its own, and that child will process the client request.
IPv6 allows an application to specify up to five pieces of information for an outgoing datagram:
Four similar pieces of information can be returned for a received packet, and they are returned as ancillary data with recvmsg:
Figure 22.21 summarizes the contents of the ancillary data, which we will discuss shortly.
To specify this information for a given packet, just specify the control information as ancillary data for sendmsg.
This information is returned as ancillary data by recvmsg only if the application has the IPV6_RECVPKTINFO socket option enabled.
If the application specifies an outgoing interface for a multicast packet, the interface specified by the ancillary data overrides any interface specified by the IPV6_MULTICAST_IF socket option for this datagram only.
The source IPv6 address is normally specified by calling bind.
Supplying the source address together with the data may require less overhead.
The kernel will verify that the requested source address is indeed a unicast address assigned to the node.
Specifying the hop limit as ancillary data lets us override either the kernel's default or a previously specified value, for either a unicast destination or a multicast destination, for a single output operation.
The received hop limit is returned as ancillary data by recvmsg only if the application has enabled the IPV6_RECVHOPLIMIT socket option.
Realize that the value returned as ancillary data is the actual value from the received datagram, while the value returned by a getsockopt of the IPV6_UNICAST_HOPS option is the default value the kernel will use for outgoing datagrams on the socket.
To control the outgoing hop limit for a given packet, just specify the control information as ancillary data for sendmsg.
To do this, the application can enable just the IPV6_RECVPKTINFO socket option and then use the received control information from recvmsg as the outgoing control information for sendmsg.
The application need not examine or modify the in6_pktinfo structure at all.
The IPV6_NEXTHOP ancillary data object specifies the next hop for the datagram as a socket address structure.
In this case, the node identified by that address must be a neighbor of the sending host.
The IPV6_TCLASS ancillary data object specifies the traffic class for the datagram.
As described in Section A.3, the traffic class is made up of the DSCP and ECN fields.
The kernel may mask or ignore the user-specified value if it needs to control the value (e.g., if the kernel implements ECN, it may set the ECN bits to its own desired value, ignoring the two bits specified with the IPV6_TCLASS option)
To specify the traffic class for a given packet, include the ancillary data with that packet.
The received traffic class is returned as ancillary data by recvmsg only if the application has the IPV6_RECVTCLASS socket option enabled.
The defaults are appropriate for the vast majority of applications, but special-purpose programs may want to modify the path MTU discovery behavior.
When performing path MTU discovery, packets are normally fragmented using the MTU of the outgoing interface or the path MTU, whichever is smaller.
Fragmenting to this minimum MTU wastes opportunities for sending larger packets (which is more efficient), but avoids the drawbacks of path MTU discovery (dropped packets and delay while the MTU is being discovered)
Two classes of applications may want to use the minimum MTU: those that use multicast (to avoid an implosion of ICMP "packet too big" messages) and those that perform brief transactions to lots of destinations (such as the DNS)
Learning the MTU for a multicast session may not be important enough to pay the cost of receiving and processing millions of ICMP "packet too big" messages, and applications such as the DNS generally don't talk to the same server often enough to make it worthwhile to risk the cost of dropped packets.
The use of the minimum MTU is controlled with the IPV6_USE_MIN_MTU socket option.
To receive change notifications in the path MTU, an application can enable the IPV6_RECVPATHMTU socket option.
This flag enables the reception of the path MTU as ancillary data anytime it changes.
This structure contains the destination for which the path MTU has changed and the new path MTU value in bytes.
This is a getonly option, which returns an ip6_mtuinfo structure (see above) containing the current path MTU.
If no path MTU has been determined, it returns the MTU of the outgoing interface.
By default, the IPv6 stack will fragment outgoing packets to the path MTU.
An application such as traceroute may not want this automatic fragmentation, to discover the path MTU on its own.
When automatic fragmentation is off, a send call providing a packet that requires fragmentation may return EMSGSIZE; however, the implementation is not required to provide this.
The only way to determine whether a packet requires fragmentation is to use the IPV6_RECVPATHMTU option, which was already described.
There are applications that want to know the destination IP address and the received interface for a UDP datagram.
Despite all the features provided by TCP that are not provided by UDP, there are times to use UDP.
We added reliability to our UDP client in Section 22.5 by detecting lost packets using a timeout and retransmission.
We modified our retransmission timeout dynamically by adding a timestamp to each packet and kept track of two estimators: the RTT and its mean deviation.
We also added a sequence number to verify that a given reply was the one expected.
Our client still employed a simple stop-and-wait protocol, but that is the type of application for which UDP can be used.
Then call udp_client just to obtain the address family, port number, and length of the socket address structure.
Also, modify the dg_send_recv function to print each received sequence number.
Plot the resulting RTTs along with the estimators for the RTT and its mean deviation.
In this chapter, we will dig a bit deeper into SCTP, examining more of the features and socket options that SCTP provides to its users.
We will discuss a number of topics, including control of failure detection, unordered data, and notifications.
Throughout this chapter, we will provide examples of code so that the reader can see how to use some of SCTP's advanced features.
Partial messages will only be delivered if the application chooses to send large messages (e.g., larger than half the socket buffer size) to its peer.
When partial messages are delivered, SCTP will never mix two partial messages together.
An application will either receive a whole message in one receive operation or it will receive a message in several consecutive receive operations.
We will illustrate a method of dealing with this partial delivery mechanism through an example utility function.
This method allows the construction of a server that is both iterative and concurrent.
The server depends on the client to close the association, thereby removing the association state.
But depending on the client to close the association leaves a weakness: What happens if a client opens an association and never sends any data? Resources would be allocated to a client that never uses them.
This dependency could introduce an accidental denial-of-service attack to our SCTP implementation from lazy clients.
To avoid this problem, an autoclose feature was added to SCTP.
Autoclose lets an SCTP endpoint specify a maximum number of seconds an association may remain idle.
An association is considered idle when it is not transmitting user data in either direction.
If an association is idle for more than this maximum time, the association is automatically closed by the SCTP implementation.
When using this option, care should be taken in choosing a value for autoclose.
The server should not pick too small a value, otherwise it may find itself needing to send data on an association that has been closed.
There would be extra overhead in re-opening the association to send back the data to the client, and it is unlikely that the client would have performed a listen to enable inbound associations.
Figure 23.1 revisits our server code and inserts the necessary calls to make our server resistant to stale idle associations.
Next, the server calls the socket option that configures the autoclose time.
Now, SCTP will automatically close associations that remain idle for more than two minutes.
By forcing the association to close automatically, we reduce the amount of server resources consumed by lazy clients.
Partial delivery will be used by the SCTP implementation any time a "large" message is being received, where "large" means the SCTP stack deems that it does not have the resources to dedicate to the message.
The following considerations will be made by the receiving SCTP implementation before starting this API:
The amount of buffer space being consumed by the message must meet or exceed some threshold.
The stack can only deliver from the beginning of the message sequentially up to the first missing piece.
Once invoked, no other messages may be made available for the user until the current message has been completely received and passed to the user.
This means that the large message blocks all other messages that would normally be deliverable, including those in other streams.
The KAME implementation of SCTP uses a threshold of one-half the socket receive buffer.
At this writing, the default receive buffer for the stack is 131,072 bytes.
We then create a modified server to use our new function.
Figure 23.2 shows our wrapper function to handle the partial delivery API.
If the function's static buffer has not been allocated, allocate it and set up the state associated with it.
If sctp_recvmsg returns an error or an EOF, we pass it directly back to the caller.
While the message flags show that the function has not received a complete message, collect more data.
The function starts by calculating how much is left in the static buffer.
Whenever the function no longer has a minimum amount of room left in its receive buffer, it must grow the buffer.
We do this using the realloc function to allocate a new buffer of the current size, plus an increment amount, and copy the old data.
If for some reason the function cannot grow its buffer any more, it exits with an error.
The function increments the buffer index and goes back to test if it has read all of the message.
When the loop terminates, the function copies the number of bytes read into the pointer provided by the caller and returns a pointer to the allocated buffer.
We next modify our server in Figure 23.3 so that it uses the new function.
Here the server calls the new partial delivery utility function.
The server calls this after nulling out any old data that may have been hanging around in the sri variable.
Note that now the server must test for NULL to see if the read was successful.
As we discussed in Section 9.14, an application can subscribe to seven notifications.
Up to now, our application has ignored all events that may occur other than the receipt of new data.
The examples in this section give an overview of how to receive and interpret SCTP's notifications of additional transport-layer events.
Figure 23.4 shows a function that will display any notification that arrives from the transport.
We will also modify our server to enable all events and call this new function when a notification is received.
Note that our server is not really using the notification for any specific purpose.
The function casts the incoming buffer to the overall union type.
If the function finds an association change notification in the buffer, it prints the type of association change that occurred.
If it was a peer address notification, the function prints the address event (after decoding) and the address.
If the function finds a remote error, it displays this fact and the association ID on which it occurred.
The function does not bother to decode and display the actual error reported by the remote peer.
If the function decodes a send failed notification, it knows that a message was not sent to the peer.
This means that either: (i) the association is coming down, and an association notification will soon follow (if it has not already arrived), or (ii) the server is using the partial reliability extension and a message was not successfully sent (due to constraints placed on the transfer)
The data actually sent is available to the function in the ssf_data field (which our function does not examine)
If the function decodes an adaption layer indicator, it displays the 32-bit value passed in the setup message (INIT or INIT-ACK)
If a partial delivery notification arrives, the function announces it.
The only event defined as of this writing is that the partial delivery is aborted.
If the function decodes this notification, it knows that the peer has issued a graceful shutdown.
This notification is usually soon followed by an association change notification when the shutdown sequence completes.
The modification to the server to use our new function can be seen in Figure 23.5
Here the server changes the event settings so that it will receive all notifications.
We start the client and send one message as follows:
When receiving the connection, message, and connection termination, our modified server displays each event as it occurs.
As you can see, the server now announces the events as they occur on the transport.
A message with the MSG_UNORDERED flag is sent with no order constraints and is made deliverable as soon as it arrives.
Figure 23.6 shows the changes needed to our client program to send the request to the echo server with the unordered data service.
Normally, all data within a given stream is ordered with sequence numbers.
The MSG_UNORDERED flag causes the data sent with this flag to be sent unordered, with no sequence number, and can be delivered as soon as it arrives, even if other unordered data that was sent earlier on the same stream has not yet arrived.
Some applications may want to bind a proper subset of the IP addresses of a machine to a single socket.
In TCP and UDP, traditionally, it was not possible to bind a subset of addresses.
The bind system call allows an application to bind a single address or the wildcard address.
Due to this restriction, the new function call sctp_bindx is provided to allow an application to bind to more than one address.
Note that all the addresses must use the same port number, and if bind was called, the port number must be the same as that provided to bind.
The sctp_bindx call will fail if a different port is provided.
Figure 23.7 shows a utility we will add to our server that will bind an argument list.
Our sctp_bind_arg_list function starts off by allocating space for the bind arguments.
This results in some memory waste but is simpler than calculating the exact space required by processing the argument list twice.
We set up the portbuf to be an ASCII representation of the port number, to prepare to call our getaddrinfo wrapper function, host_serv.
We copy the first sockaddr that is returned and ignore any others.
Since the arguments to this function are meant to be literal address strings, as opposed to names that could have multiple addresses associated with them, this is safe.
We free the return value from getaddrinfo, increment our count of addresses, and move the pointer to the next element in our packed array of sockaddr structures.
The function now resets its pointer to the top of the bind buffer and calls sctp_bindx with the subset of addresses decoded earlier.
If the function reaches here, we are successful, so clean up and return.
Figure 23.8 illustrates our modified echo server that now binds a list of addresses passed on the command line.
Note that we have modified the server slightly so it always returns any echoed message on the stream on which it arrived.
Here we see the server we have been working on throughout this chapter, but with a slight modification.
The server calls the new sctp_bind_arg_list function, passing the argument list to it for processing.
Because SCTP is a multihomed protocol, different mechanisms are needed to find out what addresses are in use at both the remote as well as the local endpoints of an association.
In this section, we will modify our client to receive the communication up notification.
Our client will then use this notification to display the addresses of both the local and remote sides of the association.
We see a slight change to our client's main routine.
We now look at the modifications needed to sctpstr_cli so that it will use our new notification processing routine.
Here the client sets up the address length variable and calls the receive function to get the echoed message from the server.
The client now checks to see if the message it just read is a notification.
If it is, the client calls our notification processing routine shown in Figure 23.11
If the message read was a notification, keep looping until we read actual data.
Next, the client displays the message and goes back to the top of its processing loop, waiting for user input.
The function casts the receive buffer to our generic notification pointer to find the notification type.
We call sctp_getpaddrs to gather a list of remote addresses.
We call sctp_getladdrs to gather a list of local addresses, plus print the number of addresses and the addresses themselves.
The function loops through each address based on the number of addresses our caller specified.
Recall that this prints any socket address structure format the system supports.
The list of addresses is a packed list, not a simple array of sockaddr_storage structures.
This is because the sockaddr_storage structure is quite large and it is too wasteful to use in passing addresses back and forth between the kernel and user space.
On systems on which the sockaddr structure contains its own length, this is trivial: just extract the length from the current sockaddr_storage structure.
On other systems, we choose the length based on the address family and quit with an error if the address family is unknown.
The function now adds the size of the address to the base pointer to move forward through the list of addresses.
We run our modified client against the server as follows:
In the recent changes we made to our client in Section 23.7, the client used the association notification to trigger retrieving the list of addresses.
This notification was quite convenient since it held the association's identification in the sac_assoc_id field.
But, if the application is not tracking association identifications and only has an address of a peer, how can it find an association's identification? In Figure 23.13, we illustrate a simple function that translates a peer's address into an association ID.
The server will use this function later in Section 23.10
We copy the address, using the passed length, into the sctp_paddrparams structure.
This call will return the current heartbeat interval, the maximum number of retransmissions before the SCTP implementation considers the peer address to have failed, and most importantly, the association ID.
Note that if the call fails, the earlier clearing of the structure will assure our caller of getting a 0 as the returned association ID.
An association ID of 0 is not allowed and is used to indicate no association by the SCTP implementation as well.
In the case of SCTP, however, the option is enabled by default.
The application can control the heartbeat and set the error threshold for an address by using the same socket option we saw in Section 23.8
The error threshold is the number of missed heartbeats or retransmission timeouts that must occur before a destination address is considered unreachable.
When the destination address becomes reachable again, detected by heartbeats, the address becomes active.
The application can disable heartbeats, but without heartbeats, SCTP has no way to detect if a failed peer address has become reachable again.
Such addresses cannot come back to an active state without user intervention.
This value, added to the current retransmission timer value plus a random jitter, will become the amount of time between heartbeats.
In Figure 23.14 we show a small function that will either set the heartbeat delay, request an on-demand heartbeat, or disable the heartbeat for the specified destination.
The function sets up the address and copies it into the sctp_paddrparams structure so that the SCTP implementation will know the address to which we wish to send a heartbeat.
Finally, the function issues the socket option call to cause the action the user has requested.
We have been focusing on the one-to-many-style interface provided by SCTP.
This interface has several advantages over the more classic one-to-one style:
It lets an application send data on the third and fourth packet of the four-way handshake by using sendmsg or sctp_sendmsg to implicitly establish the connection.
In other words, the application just does a receive call on the socket descriptor and does not need to do any of the traditional connect or accept function calls before receiving messages.
It makes it difficult to build a concurrent server (either using threads or by forking children)
This drawback has brought about the addition of the sctp_peeloff function.
The original socket remains open, and any other associations represented by the one-to-many socket are left unaffected.
This socket can then be handed off to either a thread or a child process to execute a concurrent server.
We use the address of the received message to call our function that gets us the association ID (Section 23.8)
After forking the child, our server loops back to process the next message.
The server receives and processes the first message a client sends.
The server next uses our function from Figure 23.13 to translate the address to an association ID.
If for some reason the server cannot get an association ID, it skips this attempt to fork a child and instead will try with the next message.
The server extracts the association into its own socket descriptor with sctp_peeloff.
This results in a one-to-one socket that can be passed to our earlier TCP version of str_echo.
The server forks a child and lets the child perform all future work on this new socket descriptor.
All of these advanced controls are accessed via socket options we discussed in Section 7.10
In this section, we will highlight some of the specific controls that influence how long an SCTP endpoint will take to declare either an association or destination failure.
There are seven specific controls that dictate failure detection time in SCTP (Figure 23.16)
Each of these parameters influences how quickly SCTP will detect failure or attempt retransmission.
We can think of these as control knobs that either shorten or lengthen the time it takes for an endpoint to detect failure.
An SCTP endpoint tries to open an association to a peer that has been disconnected from the network.
Two multihomed SCTP endpoints are exchanging data, and one of them is powered down during the middle of the communication.
No ICMP messages are being received due to filtering by a firewall.
After a timeout, it would retransmit the INIT message and double the RTO timer to 6,000 ms.
There are a number of knobs and combinations of knobs we can tune to shorten or lengthen this time.
First, let's focus on the influence of two specific parameters we can use to shorten the time from 270 seconds.
One change we can make is to decrease the number of retransmissions by changing sinit_max_attempts.
But this method has a drawback: We may experience a case where our peer is available, but due to loss in the network, or perhaps overload at the peer, we declare the peer to be unreachable.
This decreases our failure detection time to 121 seconds, less than one-half of the original value, but this change also.
If we pick a value that is too low, it is possible that excessive delay in the network would cause us to send many more INIT messages than needed.
Now let's turn our attention to Scenario 2, in which there are two multihomed peers communicating with each other.
One endpoint has the addresses IP-A and IP-B, the other IP-X and IP-Y.
We again focus on two parameters we can use to affect these timeouts and the resulting failure detection.
The same concerns we mentioned before apply to this scenario as well: A brief, survivable network problem or remote system overload could cause a working connection to be torn down.
Among the many alternatives, we do not recommend lowering the minimum RTO (srto_min)
When communicating across the Internet, lowering this value could have dire consequences in that we would retransmit much more rapidly, straining the Internet's infrastructure.
In a private network, it may be acceptable to tune this value lower, but for most applications, this value should not be decreased.
Each application, when turning these timing knobs, must take into consideration several factors before making adjustments:
Will the application be run in private networks where the conditions on the overall end-toend path are well-known and less varying than the Internet?
Only after carefully answering these questions will an application be able to properly tune the timing parameters of SCTP.
However, during its development, its scope was expanded beyond that into a general-purpose transport protocol.
It provides most of the features of TCP, and adds to those a wide range of new transport-layer services.
There are few applications that could not benefit by the use of SCTP.
So when should we use SCTP? Let's start by listing the benefits of SCTP:
An endpoint can take advantage of multiple networks on a host to gain additional reliability.
An added bonus is that the application does not need to take any action, other than moving to SCTP, to automatically take advantage of SCTP's multihomed service.
An application can use a single SCTP association and transport multiple data elements in parallel.
A loss in one stream of information will not influence any of the other parallel streams of information flowing through the association (we discussed this concept in Section 10.5)
Many applications do not send streams of bytes; instead, they send messages.
No longer is it necessary to mark message boundaries within a stream of bytes and provide special handling code to deal with reconstructing messages from the information flow at the receiver.
In the past, such an application may have used TCP for its reliability with the drawback that all data, even though unordered, would have to be delivered in order.
Any loss would cause head-of-line blocking for all subsequent messages flowing through the connection.
With SCTP, an unordered service is available that avoids this issue and allows an application to match its needs directly to the transport.
A partially reliable service is available in some SCTP implementations.
When both endpoints support this feature, time-sensitive data can be discarded by the transport instead of the application, even if it has been transmitted and lost, thus optimizing data transport in the face of congestion.
An easy migration path from TCP is provided by SCTP with its one-to-one-style interface.
This interface duplicates a typical TCP interface so that with one or two slight changes, a TCP application can be migrated to SCTP.
Many of the features of TCP, such as positive acknowledgment, retransmission of lost data, resequencing of data, windowed flow control, slow-start and congestion avoidance, and selective acknowledgments, are included in SCTP with two notable exceptions (the halfclosed state and urgent data)
This state is entered when an application closes its half of the connection but still allows the peer to send data to it (we discussed this in Section 6.6)
An application enters the half-closed state to signal to the peer that it is finished transmitting data.
Very few applications use this feature, so during SCTP development, it was considered not worth adding to the protocol.
Applications that need this feature and want to move to SCTP will need to change their application-layer protocol to provide this signal in the application data stream.
Another TCP feature that SCTP does not provide is urgent data.
Using a separate SCTP stream for urgent data has somewhat similar semantics, but cannot replicate the feature exactly.
Another type of application that may not benefit from SCTP is one that is truly byte streamoriented, like telnet, rlogin, rsh, and ssh.
For such an application, TCP can segment the stream of bytes into IP packets more efficiently than SCTP.
In summary, many applications could consider using SCTP as it becomes available on their Unix platform.
However, it takes an eye toward SCTP's special features to truly benefit from them; until SCTP is ubiquitous, it could be advantageous to simply stick with TCP.
In this chapter, we looked at the SCTP autoclose facility, exploring how it can be used to limit idle associations in a one-to-many socket.
We built a simple utility that an application can use to receive large messages with the partial delivery API.
We examined how an application can decode events that occur on the transport with a simple utility that displays notifications.
We briefly looked at how a user can send unordered data and bind a subset of addresses.
We saw how to acquire the addresses of both the peer end of an association as well as the local end.
We also examined a simple method an application can use to translate an address into an association ID.
Heartbeats (termed keep-alives in TCP) are exchanged by default on an SCTP association.
We examined how to control this feature through a small utility we built.
We looked at how to extract an association with the sctp_peeloff system call, and illustrated an example server that was both iterative and concurrent using this call.
We also discussed considerations an application needs to make before tuning the SCTP timing parameters.
We concluded with a look at when an application should consider using SCTP.
Also assume that all of your servers and clients are running on multihomed hosts.
What parameters do you need to adjust to assure that you detect failure in two seconds or less?
Many transport layers have the concept of out-of-band data, which is sometimes called expedited data.
The idea is that something important occurs at one end of a connection and that end wants to tell its peer quickly.
By "quickly" we mean that this notification should be sent before any "normal" (sometimes called "in-band") data that is already queued to be sent, and should be sent regardless of any flow control or blocking issues.
That is, out-of-band data is considered higher priority than normal data.
Instead of using two connections between the client and server, out-ofband data is mapped onto the existing connection.
Unfortunately, once we get beyond the general concepts and down to the real world, almost every transport layer has a different implementation of out-of-band data.
As an extreme example, UDP has no implementation of out-of-band data.
In this chapter, we will focus on TCP's model of outof-band data, provide numerous small examples of how it is handled by the sockets API, and describe how it is used by applications like telnet, rlogin, and FTP.
Other than remote interactive applications like these, it is rare to find any use for out-of-band data.
Assume a process has written N bytes of data to a TCP socket and that data is queued by TCP in the socket send buffer, waiting to be sent to the peer.
The process now writes a single byte of out-of-band data, containing the ASCII character a, using the send function and the MSG_OOB flag.
We show this in Figure 24.2 and have labeled the out-of-band byte "OOB."
Socket send buffer after one byte of out-of-band data is written by application.
TCP's urgent pointer has a sequence number one greater than the byte of data that is written with the MSG_OOB flag.
As long as the sending TCP and the receiving TCP agree on the interpretation of TCP's urgent pointer, all is fine.
Given the state of the TCP socket send buffer shown in Figure 24.2, the next segment sent by TCP will have its URG flag set in the TCP header and the urgent offset field in the TCP header will point to the byte following the out-of-band byte.
But this segment may or may not contain the byte that we have labeled as OOB.
Whether the OOB byte is sent depends on the number of bytes ahead of it in the socket send buffer, the segment size that TCP is sending to the peer, and the current window advertised by the peer.
We have used the terms urgent pointer and urgent offset.
From a programming perspective, we need not worry about this detail and just refer to TCP's urgent pointer.
This is an important characteristic of TCP's urgent mode: The TCP header indicates that the sender has entered urgent mode (i.e., the URG flag is set along with the urgent offset), but the actual byte of data referred to by the urgent pointer need not be sent.
This is one reason why applications use TCP's urgent mode (i.e., out-of-band data): The urgent notification is always sent to the peer TCP, even if the flow of data is stopped by TCP's flow control.
What happens if we send multiple bytes of out-of-band data, as in.
In this example, TCP's urgent pointer points one beyond the final byte; that is, the final byte (the c) is considered the out-of-band byte.
Now that we have covered the sending of out-of-band data, let's look at it from the receiver's side:
When TCP receives a segment with the URG flag set, the urgent pointer is examined to see whether it refers to new out-of-band data, that is, whether this is the first time TCP's urgent mode has referenced this particular byte in the stream of data from the sender to the receiver.
It is common for the sending TCP to send multiple segments (typically over a short period of time) containing the URG flag, but with the urgent pointer pointing to the same byte of data.
Only the first of these segments causes the receiving process to be notified that new out-of-band data has arrived.
The receiving process is notified when a new urgent pointer arrives.
First the SIGURG signal is sent to the owner of the socket, assuming either fcntl or ioctl has been called to establish an owner for the socket (Figure 7.20)
Second, if the process is blocked in a call to select waiting for this socket descriptor to have an exception condition, select returns.
These two potential notifications to the receiving process take place when a new urgent pointer arrives, regardless of whether the actual byte of data pointed to by the urgent.
There is only one OOB mark; if a new OOB byte arrives before the old OOB byte was read, the old byte is discarded.
When the actual byte of data pointed to by the urgent pointer arrives at the receiving TCP, the data byte can be pulled out-of-band or left inline.
By default, the SO_OOBINLINE socket option is not set for a socket, so the single byte of data is not placed into the socket receive buffer.
The only way for the process to read from this special one-byte buffer is to call recv, recvfrom, or recvmsg and specify the MSG_OOB flag.
If a new OOB byte arrives before the old byte is read, the previous value in this buffer is discarded.
If, however, the process sets the SO_OOBINLINE socket option, then the single byte of data referred to by TCP's urgent pointer is left in the normal socket receive buffer.
The process cannot specify the MSG_OOB flag to read the data byte in this case.
The process will know when it reaches this byte of data by checking the out-of-band mark for this connection, as we will describe in Section 24.3
If the process asks for out-of-band data (e.g., specifying the MSG_OOB flag), but the peer has not sent any, EINVAL is returned.
If the process has been notified that the peer has sent an out-of-band byte (e.g., by SIGURG or select), and the process tries to read it but that byte has not yet arrived, EWOULDBLOCK is returned.
All the process can do at this point is read from the socket receive buffer (possibly discarding the data if it has no room to store the data), to make space in the buffer so that the peer TCP can send the out-of-band byte.
If the process tries to read the same out-of-band byte multiple times, EINVAL is returned.3
We now show a trivial example of sending and receiving out-of-band data.
Nine bytes are sent, with a one-second sleep between each output operation.
The purpose of the pause is to let each write or send be transmitted as a single TCP segment and received as such by the other end.
We'll talk later about some of the timing considerations with out-of-band data.
When we run this program, we see the expected output.
The signal handler for SIGURG is established, and fcntl sets the owner of the connected socket.
Notice that we do not establish the signal handler until accept returns.
But if we established the signal handler before calling accept and also set the owner of the listening socket (which carries over to the connected socket), then if out-of-band data arrives before accept returns, our signal handler won't yet have a value for connfd.
If this scenario is important for the application, it should initialize connfd to –1, check for this value in the signal handler, and if true, just set a flag for the main loop to check after accept returns.
Alternately, it could block the signal around the call to accept, but this is subject to all the signal race conditions we discussed in Section 20.5
The process reads from the socket, printing each string that is returned by read.
When the sender terminates the connection, the receiver then terminates.
Our signal handler calls printf, reads the out-of-band byte by specifying the MSG_OOB flag, and then prints the returned data.
As stated earlier, calling the unsafe printf function from a signal handler is not recommended.
We do it just to see what's happening with our programs.
Here is the output when we run the receiving program, and then run the sending program from Figure 24.3:
Each sending of out-of-band data by the sender generates SIGURG for the receiver, which then reads the single out-of-band byte.
We now redo our out-of-band receiver to use select instead of the SIGURG signal.
The process calls select while waiting for either normal data (the read set, rset) or outof-band data (the exception set, xset)
When we run this program and then run the same sending program as earlier (Figure 24.3), we encounter the following error:
We cannot read the out-of-band data more than once because after we read it the first time, the kernel clears the one-byte out-of-band buffer.
When we call recv specifying the MSG_OOB flag the second time, it returns EINVAL.
Figure 24.5 Receiving program that (incorrectly) uses select to be notified of out-of-band data.
The solution is to select for an exception condition only after reading normal data.
We declare a new variable named justreadoob, which indicates whether we just read out-ofband data or not.
This flag determines whether or not to select for an exception condition.
When we set the justreadoob flag, we must also clear the bit for this descriptor in the exception set.
Whenever out-of-band data is received, there is an associated out-of-band mark.
This is the position in the normal stream of data at the sender when the sending process sent the out-ofband byte.
The receiving process determines whether or not it is at the out-of-band mark by calling the sockatmark function while it reads from the socket.
Figure 24.7 shows an implementation of this function using the commonly found SIOCATMARK ioctl.
One common use of the out-of-band mark is for the receiver to treat all the data as special until the mark is passed.
We now show a simple example to illustrate the following two features of the out-of-band mark:
The out-of-band mark always points one beyond the final byte of normal data.
This means that, if the out-of-band data is received inline, sockatmark returns true if the next byte to be read is the byte that was sent with the MSG_OOB flag.
This forced stop at the mark is to allow the process to call sockatmark to determine if the buffer pointer is at the mark.
It sends three bytes of normal data, one byte of out-of-band data, followed by another byte of normal data.
This program does not use the SIGURG signal or select.
Instead, it calls sockatmark to determine when the out-of-band byte is encountered.
We want to receive the out-of-band data inline, so we must set the SO_OOBINLINE socket option.
But if we wait until accept returns and set the option on the connected socket, the threeway handshake is complete and out-of-band data may have already arrived.
Therefore, we must set this option for the listening socket, knowing that all socket options carry over from the listening socket to the connected socket (Section 7.4)
The receiver sleeps after the connection is accepted to let all the data from the sender be received.
This allows us to demonstrate that a read stops at the out-of-band mark, even though additional data is in the socket receive buffer.
The program calls read in a loop, printing the received data.
But before calling read, sockatmark checks if the buffer pointer is at the out-of-band mark.
When we run this program, we get the following output:
Even though all the data has been received by the receiving TCP when read is called the first time (because the receiving process calls sleep), only three bytes are returned because the mark is encountered.
The next byte read is the out-of-band byte (with a value of 4), because we told the kernel to place the out-of-band data inline.
We now show another simple example to illustrate two additional features of out-of-band data, both of which we mentioned earlier.
A receiving process can be notified that the sender has sent out-of-band data (with the SIGURG signal or by select) before the out-of-band data arrives.
If the process then calls recv specifying MSG_OOB and the out-of-band data has not arrived, an error of EWOULDBLOCK is returned.
We will see shortly that the receiver sets the size of its socket receive buffer to 4,096, so these operations by the sender guarantee that the sending TCP fills the receiver's socket receive buffer.
The receiving process sets the size of the listening socket's receive buffer to 4,096
This size will carry over to the connected socket after the connection is established.
The process then accepts the connection, establishes a signal handler for SIGURG, and establishes the owner of the socket.
The signal handler calls recv to read the out-of-band data.
When we start the receiver and then the sender, here is the output from the sender:
As expected, all the data fits into the sender's socket send buffer, and then it terminates.
The error string printed by our err_sys function corresponds to EAGAIN, which is the same as EWOULDBLOCK in FreeBSD.
The solution is for the receiver to make room in its socket receive buffer by reading the normal data that is available.
This will cause its TCP to advertise a nonzero window to the sender, which will eventually let the sender transmit the out-of-band byte.
First, even if the socket send buffer is full, an out-of-band byte is always accepted by the.
Second, when the process sends an out-ofband byte, a TCP segment is immediately sent that contains the urgent notification.
Our next example demonstrates that there is only a single out-of-band mark for a given TCP connection, and if new out-of-band data arrives before the receiving process reads some existing out-of-band data, the previous mark is lost.
There are no pauses in the sending, allowing all the data to be sent to the receiving TCP quickly.
The receiving program is identical to Figure 24.9, which sleeps for five seconds after accepting the connection to allow the data to arrive at its TCP.
As we said, there is at most one out-of-band mark per TCP connection.
All our examples using out-of-band data so far have been trivial.
Unfortunately, out-of-band data gets messy when we consider the timing problems that may arise.
The first point to consider is that the concept of out-of-band data really conveys three different pieces of information to the receiver:
The receiving process can be notified of this with either the SIGURG signal or with select.
This notification is transmitted immediately after the sender sends the out-of-band byte, because we saw in Figure 24.11 that TCP sends the notification even if it is stopped by flow control from sending any data to the receiver.
This notification might cause the receiver to go into some special mode of processing for any subsequent data that it receives.
The position of the out-of-band byte, that is, where it was sent with regard to the rest of data from the sender: the out-of-band mark.
Since TCP is a byte stream protocol that does not interpret the data sent by the application, this can be any 8-bit value.
With TCP's urgent mode, we can think of the URG flag as being the notification, the urgent pointer as being the mark, and the byte of data as itself.
The problems with this concept of out-of-band data are that: (i) there is only one TCP urgent pointer per connection; (ii) there is only one out-of-band mark per connection; and (iii) there is only a single one-byte out-of-band buffer per connection (which is an issue only if the data is not being read inline)
We saw with Figure 24.12 that an arriving mark overwrites any previous mark that the process has not yet encountered.
If the data is being read inline, previous out-of-band bytes are not lost when new out-of-band data arrives, but the mark is lost.
The server needs to tell the client to discard all queued output because up to one window's worth of output may be queued to send from the server to the client.
The server sends a special byte to the client, telling it to flush all output, and this byte is sent as out-of-band data.
When the client receives the SIGURG signal, it just reads from the socket until it encounters the mark, discarding everything up through the mark.
In this scenario, if the server sent multiple out-of-band bytes in quick succession, it wouldn't affect the client, as the client just reads up through the final mark, discarding all the data.
In summary, the usefulness of out-of-band data depends on why it is being used by the application.
If the purpose is to tell the peer to discard the normal data up through the mark, then losing an intermediate out-of-band byte and its corresponding mark is of no consequence.
But if it is important that no out-of-band bytes be lost, then the data must be received inline.
Furthermore, the data bytes that are sent as out-of-band data should be differentiated from normal data since intermediate marks can be overwritten when a new mark is received, effectively mixing out-of-band bytes with the normal data.
This lets it differentiate its commands from normal user data, but requires that the client and server.
It provides an urgent pointer that is sent in the TCP header to the peer as soon as the sender goes into urgent mode.
The receipt of this pointer by the other end of the connection tells that process that the sender has gone into urgent mode, and the pointer points to the final byte of urgent data.
But all the data is still subject to TCP's normal flow control.
The sockets API maps TCP's urgent mode into what it calls out-of-band data.
The sender goes into urgent mode by specifying the MSG_OOB flag in a call to send.
The final byte of data in this call is considered the out-of-band byte.
The receiver is notified when its TCP receives a new urgent pointer by either the SIGURG signal, or by an indication from select that the socket has an exception condition pending.
By default, TCP takes the out-of-band byte out of the normal stream of data and places it into its own one-byte out-of-band buffer that the process reads by calling recv with the MSG_OOB flag.
Alternately, the receiver can set the SO_OOBINLINE socket option, in which case, the out-of-band byte is left in the normal stream of data.
Regardless of which method is used by the receiver, the socket layer maintains an out-of-band mark in the data stream and will not read through the mark with a single input operation.
The receiver determines if it has reached the mark by calling the sockatmark function.
When using signal-driven I/O, the kernel notifies us with a signal when something happens on a descriptor.
Historically, this has been called asynchronous I/O, but the signal-driven I/O that we will describe is not true asynchronous I/O.
The latter is normally defined as the process performing the I/O operation (say a read or write), with the kernel returning immediately after the kernel initiates the I/O operation.
Some form of notification is then provided to the process when the operation is complete or encounters an error.
We compared the various types of I/O that are normally available in Section 6.2 and showed the difference between signal-driven I/O and asynchronous I/O.
The nonblocking I/O we described in Chapter 16 is not asynchronous I/O either.
With nonblocking I/O, the kernel does not return after initiating the I/O operation; the kernel returns immediately only if the operation cannot be completed without putting the process to sleep.
These functions let the process specify whether or not a signal is generated when the I/O completes, and which signal to generate.
Berkeley-derived implementations support signal-driven I/O for sockets and terminal devices using the SIGIO signal.
SVR4 supports signal-driven I/O for STREAMS devices using the SIGPOLL signal, which is then equated to SIGIO.
To use signal-driven I/O with a socket (SIGIO) requires the process to perform the following three steps:
A signal handler must be established for the SIGIO signal.1
The O_ASYNC flag is a relatively late addition to the POSIX specification.
In Figure 25.4, we will enable signaldriven I/O with the FIOASYNC ioctl instead.
Notice the bad choice of names by POSIX: The name O_SIGIO would have been a better choice for the new flag.
We should establish the signal handler before setting the owner of the socket.
Under Berkeley-derived implementations, the order of the two function calls does not matter because the default action is to ignore SIGIO.
Therefore, if we were to reverse the order of the two function calls, there is a small chance that a signal could be generated after the call to fcntl but before the call to signal; if that happens, the signal is just discarded.
Therefore, under SVR4, we want to be certain the signal handler is installed before setting the owner of the socket.
Although setting a socket for signal-driven I/O is easy, the hard part is determining what conditions cause SIGIO to be generated for the socket owner.
Hence, when we catch SIGIO for a UDP socket, we call recvfrom to either read the datagram that arrived or to obtain the asynchronous error.
We talked about asynchronous errors with regard to UDP sockets in Section 8.9
Recall that these are generated only if the UDP socket is connected.
Unfortunately, signal-driven I/O is next to useless with a TCP socket.
The problem is that the signal is generated too often, and the occurrence of the signal does not tell us what happened.
Data has been sent from a socket (i.e., the output buffer has free space)
For example, if one is both reading from and writing to a TCP socket, SIGIO is generated when new data arrives and when data previously written is acknowledged, and there is no way to distinguish between the two in the signal handler.
If SIGIO is used in this scenario, the TCP socket should be set to nonblocking to prevent a read or write from blocking.
We should consider using SIGIO only with a listening TCP socket, because the only condition that generates SIGIO for a listening socket is the completion of a new connection.
The only real-world use of signal-driven I/O with sockets that the authors were able to find is the NTP server, which uses UDP.
The main loop of the server receives a datagram from a client and sends a response.
But, there is a non-negligible amount of processing to do for each client's request (more than our trivial echo server)
It is important for the server to record accurate timestamps for each received datagram, since that value is returned to the client and then used by the client to calculate the RTT to the server.
Figure 25.1 shows two ways to build such a UDP server.
Most UDP servers (including our echo server from Chapter 8) are designed as shown at the left of this figure.
But the NTP server uses the technique shown on the right side: When a new datagram arrives, it is read by the SIGIO handler, which also records the time at which the datagram arrived.
The datagram is then placed on another queue within the process from which it will be removed by and processed by the main server loop.
Although this complicates the server code, it provides accurate timestamps of arriving datagrams.
One could argue that two additional pieces of information that should also be returned for a received UDP datagram are an indication of the received interface (which can differ from the destination address, if the host employs the common weak end system model) and the time at which the datagram arrived.
FreeBSD also provides the SO_TIMESTAMP socket option, which returns the time at which the datagram was received as ancillary data in a timeval structure.
Linux provides an SIOCGSTAMP ioctl that returns a timeval structure containing the time at which the datagram was received.
We now provide an example similar to the right side of Figure 25.1 : a UDP server that uses the SIGIO signal to receive arriving datagrams.
This example also illustrates the use of POSIX reliable signals.
The only changes that we make are to the dg_echo function, which we show in the next four figures.
Each structure contains a pointer to the received datagram, its length, a pointer to a socket address structure containing the protocol address of the client, and the size of the protocol address.
We also allocate a diagnostic counter, cntread , that we will examine shortly.
Figure 25.3 shows the array of structures, assuming the first entry points.
Data structures used to hold received datagrams and their socket address structures.
The socket descriptor is saved in a global variable since the signal handler needs it.
The socket owner is set using fcntl and the signal-driven and non-blocking I/O flags are set using ioctl.
We mentioned earlier that the O_ASYNC flag with fcntl is the POSIX way to specify signaldriven I/O, but since most systems do not yet support it, we use ioctl instead.
While most systems do support the O_NONBLOCK flag to set nonblocking, we show the ioctl method here.
Three signal sets are initialized: zeromask (which never changes), oldmask (which contains the old signal mask when we block SIGIO ), and newmask.
We then enter the for loop and test the nqueue counter.
As long as this counter is 0, there is nothing to do and we can call sigsuspend.
This POSIX function saves the current signal mask internally and then sets the current signal mask to the argument (zeromask )
Since zeromask is an empty signal set, this enables all signals.
It is an unusual function because it always returns an error, EINTR.
Before returning, sigsuspend always sets the signal mask to its value when the function was called, which in this case is the value of newmask , so we are guaranteed that when sigsuspend returns, SIGIO is blocked.
That is why we can test the counter nqueue , knowing that while we are testing it, a SIGIO signal cannot be delivered.
Consider what would happen if SIGIO was not blocked while we tested the variable nqueue , which is shared between the main loop and the signal handler.
We then call sigsuspend and go to sleep, effectively missing the signal.
We are never awakened from the call to sigsuspend unless another signal occurs.
This is similar to the race condition we described in Section 20.5
We unblock SIGIO by calling sigprocmask to set the signal mask of the process to the value that was saved earlier (oldmask )
Notice that we do not need SIGIO blocked while modifying iget , because this index is used only by the main loop; it is never modified by the signal handler.
We must block the signal while modifying this variable since it is shared between the main loop and the signal handler.
Also, we need SIGIO blocked when we test nqueue at the top of the loop.
An alternate technique is to remove both calls to sigprocmask that are within the for loop, which avoids unblocking the signal and then blocking it later.
The problem, however, is that this executes the entire loop with the signal blocked, which decreases the responsiveness of the signal handler.
Datagrams should not get lost because of this change (assuming the socket receive buffer is large enough), but the delivery of the signal to the process will be delayed the entire time that the signal is blocked.
One goal when coding applications that perform signal handling should be to block the signal for the minimum amount of time.
The problem that we encounter when coding this signal handler is that POSIX signals are normally not queued.
This means that, if we are in the signal handler, which guarantees that the signal is blocked, and the signal occurs two more times, the signal is delivered only one more time.
Consider the following scenario: A datagram arrives and the signal is delivered.
The signal handler reads the datagram and places it onto the queue for the main loop.
But while the signal handler is executing, two more datagrams arrive, causing the signal to be generated two more times.
Since the signal is blocked, when the signal handler returns, it is called only one more time.
The second time the signal handler executes, it reads the second datagram, but the third datagram is left on the socket receive queue.
This third datagram will be read only if and when a fourth datagram arrives.
When a fourth datagram arrives, it is the third datagram that is read and placed on the queue for the main loop, not the fourth one.
Because signals are not queued, the descriptor that is set for signal-driven I/O is normally set to nonblocking also.
We then code our SIGIO handler to read in a loop, terminating only when the read returns EWOULDBLOCK.
There are other ways to handle this (e.g., additional buffers could be allocated), but for our simple example, we just terminate.
The array entry indexed by iput is where the datagram is stored.
If there are no datagrams to read, break jumps out of the for loop.
Before the signal handler returns, it increments the counter corresponding to the number of datagrams read per signal.
We print this array in Figure 25.6 when the SIGHUP signal is delivered as diagnostic information.
The final function (Figure 25.6 ) is the SIGHUP signal handler, which prints the cntread array.
To illustrate that signals are not queued and that we must set the socket to nonblocking in addition to setting the signal-driven I/O flag, we will run this server with six clients simultaneously.
Each client sends 3,645 lines for the server to echo, and each client is started from a shell script in the background so that all clients are started at about the same time.
When all the clients have terminated, we send the SIGHUP signal to the server, causing it to print its cntread array.
Most of the time, the signal handler reads only one datagram, but there are times when more than one is ready.
The nonzero counter for cntread[0] is when the signal is generated while the signal handler is executing, but before the signal handler returns, it reads all pending datagrams.
When the signal handler is called again, there are no datagrams left to read.
Signal-driven I/O has the kernel notify us with the SIGIO signal when "something" happens on a socket.
With a connected TCP socket, numerous conditions can cause this notification, making this feature of little use.
With a listening TCP socket, this notification occurs when a new connection is ready to be accepted.
With UDP, this notification means either a datagram has arrived or an asynchronous error has arrived; in both cases, we call recvfrom.
We modified our UDP echo server to use signal-driven I/O, using a technique similar to that used by NTP: read a datagram as soon as possible after it arrives to obtain an accurate timestamp for its arrival and then queue it for later processing.
In the traditional Unix model, when a process needs something performed by another entity, it forks a child process and lets the child perform the processing.
Most network servers under Unix are written this way, as we have seen in our concurrent server examples: The parent accepts the connection, forks a child, and the child handles the client.
While this paradigm has served well for many years, there are problems with fork:
Memory is copied from the parent to the child, all descriptors are duplicated in the child, and so on.
Current implementations use a technique called copy-onwrite, which avoids a copy of the parent's data space to the child until the child needs its own copy.
Passing information from the parent to the child before the fork is easy, since the child starts with a copy of the parent's data space and with a copy of all the parent's descriptors.
But, returning information from the child to the parent takes more work.
Threads are sometimes called lightweight processes since a thread is "lighter weight" than a process.
That is, thread creation can be 10–100 times faster than process creation.
All threads within a process share the same global memory.
This makes the sharing of information easy between the threads, but along with this simplicity comes the problem of synchronization.
One analogy is to think of signal handlers as a type of thread as we discussed in Section 11.18
That is, in the traditional Unix model, we have the main flow of execution (one thread) and a signal handler (another thread)
If the main flow is in the middle of updating a linked list when a signal occurs, and the signal handler also tries to update the linked list, havoc normally results.
The main flow and signal handler share the same global variables, but each has its own stack.
In this text, we cover POSIX threads, also called Pthreads.
We will see that all the Pthread functions begin with pthread_
This chapter is an introduction to threads, so that we can use threads in our network programs.
In this section, we will cover five basic thread functions and then use these in the next two sections to recode our TCP client/server using threads instead of fork.
When a program is started by exec, a single thread is created, called the initial thread or main thread.
Each thread within a process is identified by a thread ID, whose datatype is pthread_t (often an unsigned int)
On successful creation of a new thread, its ID is returned through the pointer tid.
Each thread has numerous attributes: its priority, its initial stack size, whether it should be a daemon thread or not, and so on.
When a thread is created, we can specify these attributes by initializing a pthread_attr_t variable that overrides the default.
We normally take the default, in which case, we specify the attr argument as a null pointer.
Finally, when we create a thread, we specify a function for it to execute.
The thread starts by calling this function and then terminates either explicitly (by calling pthread_exit) or implicitly (by letting the function return)
The address of the function is specified as the func argument, and this function is called with a single pointer argument, arg.
If we need multiple arguments to the function, we must package them into a structure and then pass the address of this structure as the single argument to the start function.
This lets us pass one pointer (to anything we want) to the thread, and lets the thread return one pointer (again, to anything we want)
The return value from the Pthread functions is normally 0 if successful or nonzero on an error.
But unlike the socket functions, and most system calls, which return –1 on an error and set errno to a positive value, the Pthread functions return the positive error indication as the function's return value.
For example, if pthread_create cannot create a new thread because of exceeding some system limit on the number of threads, the function return value is EAGAIN.
A value of 0 is never assigned to one of the Exxx names.
We can wait for a given thread to terminate by calling pthread_join.
We must specify the tid of the thread that we want to wait for.
Unfortunately, there is no way to wait for any of our threads (similar to waitpid with a process ID argument of –1)
We will return to this problem when we discuss Figure 26.14
If the status pointer is non-null, the return value from the thread (a pointer to some object) is stored in the location pointed to by status.
Each thread has an ID that identifies it within a given process.
Comparing threads to Unix processes, pthread_self is similar to getpid.
When a joinable thread terminates, its thread ID and exit status are retained until another thread calls pthread_join.
But a detached thread is like a daemon process: When it terminates, all its resources are released and we cannot wait for it to terminate.
If one thread needs to know when another thread terminates, it is best to leave the thread as joinable.
The pthread_detach function changes the specified thread so that it is detached.
This function is commonly called by the thread that wants to detach itself, as in.
One way for a thread to terminate is to call pthread_exit.
If the thread is not detached, its thread ID and exit status are retained for a later pthread_join by some other thread in the calling process.
The pointer status must not point to an object that is local to the calling thread since that object disappears when the thread terminates.
There are two other ways for a thread to terminate:
The function that started the thread (the third argument to pthread_create) can return.
Since this function must be declared as returning a void pointer, that return value is the exit status of the thread.
If the main function of the process returns or if any thread calls exit, the process terminates, including any threads.
This is the first time we have encountered the unpthread.h header.
The thread that we are about to create needs the values of the two arguments to str_cli: fp, the standard I/O FILE pointer for the input file, and sockfd, the TCP socket connected to the server.
For simplicity, we store these two values in external variables.
An alternative technique is to put the two values into a structure and then pass a pointer to the structure as the argument to the thread we are about to create.
The thread is created and the new thread ID is saved in tid.
The main thread calls readline and fputs, copying from the socket to the standard output.
When this happens, all threads in the process are terminated.
Normally, the copyto thread will have already terminated by the time the server's main function completes.
But in the case where the server terminates prematurely (Section 5.12), calling exit when the server's main function completes will terminate the copyto thread, which is what we want.
This thread just copies from standard input to the socket.
When it reads an EOF on standard input, a FIN is sent across the socket by shutdown and the thread returns.
The return from this function (which started the thread) terminates the thread.
The threads version we just presented took 8.5 seconds, which is slightly faster than the version using fork (which we expect), but slower than the nonblocking I/O version.
Nevertheless, comparing the complexity of the nonblocking I/O version (Section 16.2) versus the simplicity of the threads version, we still recommend using threads instead of nonblocking I/O.
We now redo our TCP echo server from Figure 5.2 using one thread per client instead of one child process per client.
The single argument that we pass to the doit function is the connected socket descriptor, connfd.
We cast the integer descriptor connfd to be a void pointer.
It works only on systems on which the size of an integer is less than or equal to the size of a pointer.
The thread detaches itself since there is no reason for the main thread to wait for each thread it creates.
When this function returns, we must close the connected socket since the thread shares all descriptors with the main thread.
With fork, the child did not need to close the connected socket because when the child terminated, all open descriptors were closed on process termination (see Exercise 26.2)
Also notice that the main thread does not close the connected socket, which we always did with a concurrent server that calls fork.
This is because all threads within a process share the descriptors, so if the main thread called close, it would terminate the connection.
Creating a new thread does not affect the reference counts for open descriptors, which is different from fork.
There is a subtle error in this program, which we will describe in detail in Section 26.5
We mentioned that in Figure 26.3, we cast the integer variable connfd to be a void pointer, but this is not guaranteed to work on all systems.
First, notice that we cannot just pass the address of connfd to the new thread.
From an ANSI C perspective this is acceptable: We are guaranteed that we can cast the integer pointer to be a void * and then cast this pointer back to an integer pointer.
There is one integer variable, connfd in the main thread, and each call to accept overwrites this variable with a new value (the connected descriptor)
The pointer to connfd (not its contents) is the final argument to pthread_create.
A thread is created and the doit function is scheduled to start executing.
Another connection is ready and the main thread runs again (before the newly created thread)
The problem is that multiple threads are accessing a shared variable (the integer value in connfd) with no synchronization.
This is fine, given the way that C passes integer values to a called function (a copy of the value is pushed onto the stack for the called function)
Figure 26.4 TCP echo server using threads with more portable argument passing.
Each time we call accept, we first call malloc and allocate space for an integer variable, the connected descriptor.
This gives each thread its own copy of the connected descriptor.
The thread fetches the value of the connected descriptor and then calls free to release the memory.
That is, calling either function from a signal handler while the main thread is in the middle of one of these two functions has been a recipe for disaster, because of static data structures that are manipulated by these two functions.
How can we call these two functions in Figure 26.4? POSIX requires that these two functions, along with many others, be thread-safe.
This is normally done by some form of synchronization performed within the library functions that is transparent to us.
Unfortunately, POSIX says nothing about thread safety with regard to the networking API functions.
We talked about the nonre-entrant property of gethostbyname and gethostbyaddr in Section 11.18
We mentioned that even though some vendors have defined thread-safe versions whose names end in _r, there is no standard for these functions and they should be avoided.
All of the nonre-entrant getXXX functions were summarized in Figure 11.21
Two of the functions are thread-safe only if the caller allocates space for the result and passes that pointer as the argument to the function.
When converting existing functions to run in a threads environment, a common problem encountered is due to static variables.
A function that keeps state in a private buffer, or one that returns a result in the form of a pointer to a static buffer, is not thread-safe because multiple threads cannot use the buffer to hold different things at the same time.
This is nontrivial and then converts the function into one that works only on systems with threads support.
The advantage to this approach is that the calling sequence does not change and all the changes go into the library function and not the applications that call the function.
We show a version of readline that is thread-safe by using thread-specific data later in this section.
Change the calling sequence so that the caller packages all the arguments into a structure, and also store in that structure the static variables from Figure 3.18
This was also done, and Figure 26.6 shows the new structure and new function prototypes.
Figure 26.6 Data structure and function prototype for re-entrant version of readline.
These new functions can be used on threaded and nonthreaded systems, but all applications that call readline must change.
Restructure the interface to avoid any static variables so that the function is thread-safe.
Since we said the older version was "painfully slow," taking this option is not always viable.
Thread-specific data is a common technique for making an existing function thread-safe.
Before describing the Pthread functions that work with thread-specific data, we describe the concept and a possible implementation, because the functions appear more complicated than they really are.
Part of the complication in many texts on using threads is that their descriptions of threadspecific data read like the Pthreads standard, talking about key-value pairs and keys being opaque objects.
We describe thread-specific data in terms of indexes and pointers because common implementations use a small integer index for the key, and the value associated with the index is just a pointer to a region that the thread mallocs.
Each system supports a limited number of thread-specific data items.
The system (probably the threads library) maintains one array of structures per process, which we call key structures, as we show in Figure 26.7
The flag in the Key structure indicates whether this array element is currently in use, and all the flags are initialized to be "not in use." When a thread calls pthread_key_create to create a new thread-specific data item, the system searches through its array of Key structures and finds the first one not in use.
We will talk about the "destructor pointer," the other member of the Key structure, shortly.
In addition to the process-wide array of Key structures, the system maintains numerous pieces of information about each thread within a process.
We call this a Pthread structure and part of this information is a 128-element array of pointers, which we call the pkey array.
All entries in the pkey array are initialized to null pointers.
Each thread can then store a value (pointer) for the key, and each thread normally obtains the pointer by calling malloc.
Part of the confusion with thread-specific data is that the pointer is the value in the key-value pair, but the real thread-specific data is whatever this pointer points to.
We now go through an example of how thread-specific data is used, assuming that our readline function uses thread-specific data to maintain the per-thread state across successive calls to the function.
Shortly we will show the code for this, modifying our readline function to follow these steps:
One of the threads will be the first to call readline, and it in turn calls pthread_key_create.
Therefore, readline calls malloc to allocate the memory that it needs to keep the per-thread information across successive calls to readline for this thread.
In this figure, we note that the Pthread structure is maintained by the system (probably the thread library), but the actual thread-specific data that we malloc is maintained by our function (readline, in this case)
Another thread, say thread n, calls readline, perhaps while thread 0 is still executing within readline.
Thread n continues executing in readline, using and modifying its own thread-specific data.6
One item we have not addressed is: What happens when a thread terminates? If the thread has called our readline function, that function has allocated a region of memory that needs to be freed.
This is where the "destructor pointer" from Figure 26.7 is used.
When a thread terminates, the system goes through that thread's pkey array, calling the corresponding destructor function for each non-null pkey pointer.
What we mean by "corresponding destructor" is the function pointer stored in the Key array in Figure 26.7
This is how the thread-specific data is freed when a thread terminates.
Both return: 0 if OK, positive Exxx value on error.
The key is returned through the keyptr pointer, and the destructor function, if the argument is a non-null pointer, will be called by each thread on termination if that thread has stored a value for this key.
Typical usage of these two functions (ignoring error returns) is as follows:
This value is what we called the "pointer" in Figure 26.8
What this pointer points to is up to the application, but normally, it points to dynamically allocated memory.
Notice that the argument to pthread_key_create is a pointer to the key (because this function stores the value assigned to the key), while the arguments to the get and set functions are the key itself (probably a small integer index as discussed earlier)
We now show a complete example of thread-specific data by converting the optimized version of our readline function from Figure 3.18 to be thread-safe, without changing the calling sequence.
Our destructor function just frees the memory that was allocated for this thread.
We will see that our one-time function is called one time by pthread_once, and it just creates the key that is used by readline.
Our Rline structure contains the three variables that caused the problem by being declared static in Figure 3.18
One of these structures will be dynamically allocated per thread and then released by our destructor function.
The first argument to the function is now a pointer to the Rline structure that was allocated for this thread (the actual thread-specific data)
But if this is the first time this thread has called readline, the return value is a null pointer.
With threads, we can leave the sockets in their default blocking mode and create one thread per connection.
Each thread can block in its call to connect, as the kernel will just run some other thread that is ready.
Figure 26.13 shows the first part of the program, the globals, and the start of the main function.
Solaris threads in addition to Pthreads, as we will describe shortly.
We have added one member to the file structure: f_tid, the thread ID.
The remainder of this code is similar to Figure 16.15
With this threads version, we do not use select and therefore do not need any descriptor sets or the variable maxfd.
Figure 26.14 shows the main processing loop of the main thread.
If we are allowed to create another thread (nconn is less than maxnconn), we do so.
The function that each new thread executes is do_get_read and the argument is the pointer to the file structure.
Unfortunately, Pthreads does not provide a way to wait for any one of our threads to terminate; the pthread_join function makes us specify exactly which thread we want to wait for.
We will see in Section 26.9 that the Pthreads solution for this problem is more complicated, requiring us to use a condition variable for the terminating thread to notify the main thread when it is done.
The solution that we show, using the Solaris thread thr_join function, is not portable to all environments.
Nevertheless, we want to show this version of our Web client example using threads without having to complicate the discussion with condition variables and mutexes.
Fortunately, we can mix Pthreads with Solaris threads under Solaris.
This function establishes the TCP connection, sends an HTTP GET command to the server, and reads the server's reply.
A TCP socket is created and a connection is established by our tcp_connect function.
The socket is a normal blocking socket, so the thread will block in the call to connect until the connection is established.
When the connection is closed by the server, the F_DONE flag is set and the function returns, terminating the thread.
We will return to this example, replacing the Solaris thr_join function with the more portable Pthreads solution, but we must first discuss mutexes and condition variables.
Notice in Figure 26.14 that when a thread terminates, the main loop decrements both nconn and nlefttoread.
We could have placed these two decrements in the function do_get_read, letting each thread decrement these two counters immediately before the thread terminates.
But this would be a subtle, yet significant, concurrent programming error.
The problem with placing the code in the function that each thread executes is that these two variables are global, not thread-specific.
If one thread is in the middle of decrementing a variable, that thread is suspended, and if another thread executes and decrements the same variable, an error can result.
For example, assume that the C compiler turns the decrement operator into three instructions: load from memory into a register, decrement the register, and store from the register into memory.
Sometime later, the system switches threads from B to A.
A's registers are restored and A continues where it left off, at the second machine instruction in the three-instruction sequence.
These types of concurrent programming errors are hard to find for numerous reasons.
Nevertheless, it is an error and it will fail (Murphy's Law)
Second, the error is hard to duplicate since it depends on the nondeterministic timing of many events.
Lastly, on some systems, the hardware instructions might be atomic; that is, there might be a hardware instruction to decrement an integer in memory (instead of the three-instruction sequence we assumed above) and the hardware cannot be interrupted during this instruction.
But, this is not guaranteed by all systems, so the code works on one system but not on another.
We call threads programming concurrent programming, or parallel programming, since multiple threads can be running concurrently (in parallel), accessing the same variables.
While the error scenario we just discussed assumes a single-CPU system, the potential for error also exists if threads A and B are running at the same time on different CPUs on a multiprocessor system.
With normal Unix programming, we do not encounter these concurrent programming problems because with fork, nothing besides descriptors is shared between the parent and child.
We will, however, encounter this same type of problem when we discuss shared memory between processes.
We exacerbate the potential for a problem by fetching the current value of counter, printing the new value, and then storing the new value.
If we run this program, we have the output shown in.
Figure 26.17 Two threads that increment a global variable incorrectly.
Each thread fetches, prints, and increments the counter NLOOP times.
This happens numerous times through the 10,000 lines of output.
The nondeterministic nature of this type of problem is also evident if we run the program a few times: Each time, the end result is different from the previous run of the program.
Also, if we redirect the output to a disk file, sometimes the error does not occur since the program runs faster, providing fewer opportunities to switch between the threads.
The greatest number of errors occurs when we run the program interactively, writing the output to the (slow) terminal, but saving the output in a file using the Unix script program (discussed in detail in Chapter 19 of APUE)
The problem we just discussed, multiple threads updating a shared variable, is the simplest problem.
The solution is to protect the shared variable with a mutex (which stands for "mutual exclusion") and access the variable only when we hold the mutex.
In terms of Pthreads, a mutex is a variable of type pthread_mutex_t.
We lock and unlock a mutex using the following two functions:
Both return: 0 if OK, positive Exxx value on error.
If we try to lock a mutex that is already locked by some other thread, we are blocked until the mutex is unlocked.
But there is no guarantee that this is acceptable and other systems (e.g., Digital Unix) define the initializer to be nonzero.
Each thread fetches, prints, and increments the counter NLOOP times.
We declare a mutex named counter_mutex and this mutex must be locked by the thread before the thread manipulates the counter variable.
When we run this program, the output is always correct: The value is incremented monotonically and the final value printed is always 10,000
The difference in CPU time from the incorrect version with no mutex to the correct version that used a mutex was 10%
This tells us that mutex locking is not a large overhead.
A mutex is fine to prevent simultaneous access to a shared variable, but we need something else to let us go to sleep waiting for some condition to occur.
But, we cannot call the Pthread function until we know that a thread has terminated.
We first declare a global variable that counts the number of terminated threads and protect it with a mutex.
We then require that each thread increment this counter when it terminates, being careful to use the associated mutex.
This is fine, but how do we code the main loop? It needs to lock the mutex continually and check if any threads have terminated.
While this is okay, it means the main loop never goes to sleep; it just loops, checking ndone every time around the loop.
This is called polling and is considered a waste of CPU time.
We want a method for the main loop to go to sleep until one of its threads notifies it that something is ready.
A condition variable, in conjunction with a mutex, provides this facility.
The mutex provides mutual exclusion and the condition variable provides a signaling mechanism.
In terms of Pthreads, a condition variable is a variable of type pthread_cond_t.
Both return: 0 if OK, positive Exxx value on error.
The term "signal" in the second function's name does not refer to a Unix SIGxxx signal.
An example is the easiest way to explain these functions.
Returning to our Web client example, the counter ndone is now associated with both a condition variable and a mutex.
A thread notifies the main loop that it is terminating by incrementing the counter while its mutex lock is held and by signaling the condition variable.
The main loop then blocks in a call to pthread_cond_wait, waiting to be signaled by a terminating thread.
Notice that the variable ndone is still checked only while the mutex is held.
Then, if there is nothing to do, pthread_cond_wait is called.
This puts the calling thread to sleep and releases the mutex lock it holds.
Furthermore, when the thread returns from pthread_cond_wait (after some other thread has signaled it), the thread again holds the mutex.
Why is a mutex always associated with a condition variable? The "condition" is normally the value of some variable that is shared between the threads.
The mutex is required to allow this variable to be set and tested by the different threads.
For example, if we did not have the mutex in the example code just shown, the main loop would test it as follows:
If this happens, this last "signal" is lost and the main loop would block forever, waiting for something that will never occur again.
This is the same reason that pthread_cond_wait must be called with the associated mutex locked, and why this function unlocks the mutex and puts the calling thread to sleep as a single, atomic operation.
If this function did not unlock the mutex and then lock it again when it returns, the thread would have to unlock and lock the mutex and the code would look like the following:
Both return: 0 if OK, positive Exxx value on error.
This time value is an absolute time; it is not a time delta.
This differs from both select and pselect, which specify the number of seconds and microseconds (nanoseconds for pselect) until some time in the future when the function should.
The advantage in using an absolute time instead of a delta time is if the function prematurely returns (perhaps because of a caught signal), the function can be called again, without having to change the contents of the timespec structure.
The disadvantage, however, is having to call gettimeofday before the function can be called the first time.
The POSIX specification defines a clock_gettime function that returns the current time as a timespec structure.
As discussed in that section, we now must specify exactly which thread we are waiting for.
To do this we will use a condition variable, as described in Section 26.8
The only change to the globals (Figure 26.13 ) is to add one new flag and the condition variable.
To wait for one of the threads to terminate, we wait for ndone to be nonzero.
As discussed in Section 26.8 , the test must be done while the mutex is locked.
Figure 16.20 shows the timing for this version, along with the timing of the version using nonblocking connect s.
The creation of a new thread is normally faster than the creation of a new process with fork.
This alone can be an advantage in heavily used network servers.
Threads programming, however, is a new paradigm that requires more discipline.
All threads in a process share global variables and descriptors, allowing this information to be shared between different threads.
But this sharing introduces synchronization problems and the Pthread synchronization primitives that we must use are mutexes and condition variables.
Synchronization of shared data is a required part of almost every threaded application.
When writing functions that can be called by threaded applications, these functions must be thread-safe.
Thread-specific data is one technique that helps with this, and we showed an example with our readline function.
We return to the threads model in Chapter 30 with another server design in which the server creates a pool of threads when it starts.
An available thread from this pool handles the next client request.
Modify Figure 26.2 to print this message too, when appropriate.
Then, build the TCP echo client from Figure 6.13 that works in a batch mode correctly.
Find a large text file on your system and start the client three times in a batch mode, reading from the large text file and writing its output to a temporary file.
If possible, run the clients on a different host from the server.
If the three clients terminate correctly (often they hang), look at their temporary output files and compare them to the input file.
Now build a version of the server using the correct readline function from Section 26.5
Rerun the test with three clients; all three clients should now work.
This shows that the key is created only one time, but the memory is allocated for every thread, and that the destructor function is called for every thread.
Although 10 different options are defined, the most commonly used is the source route option.
Access to these options is through the IP_OPTIONS socket option and we will demonstrate this with an example that uses source routing.
NOP: no-operation—A one-byte option typically used for padding to make a later option fall on a four-byte boundary.
Since the total size of the IP options must be a multiple of four bytes, EOL bytes follow the final option.
This option is included in IP datagrams that should be examined by all routers that forward the datagram.
The fourth argument to getsockopt and setsockopt is a pointer to a buffer (whose size is 44 bytes or less), and the fifth argument is the size of this buffer.
The reason that the size of this buffer for getsockopt can be four bytes larger than the maximum size of the options is because of the way the source route option is handled, as we will describe shortly.
Other than the two source route options, the format of what goes into the buffer is the format of the options when placed into the IP datagram.
When the IP options are set using setsockopt, the specified options will then be sent on all IP datagrams on that socket.
To clear these options, call setsockopt and specify either a null pointer as the fourth argument or a value of 0 as the fifth argument (the length)
Setting the IP options for a raw IP socket does not work on all implementations if the IP_HDRINCL socket option (which we will describe in the next chapter) is also set.
The source route is automatically reversed by TCP because the source route specified by the client was from the client to the server, but the server needs to use the reverse of this route in datagrams it sends to the client.
For all other TCP sockets and for all UDP sockets and raw IP sockets, calling getsockopt to fetch the IP options just returns a copy of whatever IP options have been set by setsockopt for the socket.
Note that for a raw IP socket, the received IP header, including any IP options, is always returned by the input functions, so the received IP options are always available.
Berkeley-derived kernels have never returned a received source route, or any other IP options, for a UDP socket.
This makes it impossible for a UDP application to use the reverse of a received route for datagrams back to the sender.
A source route is a list of IP addresses specified by the sender of the IP datagram.
If the source route is strict, then the datagram must pass through each listed node and only the listed nodes.
That is, all the nodes listed in the source route must be neighbors.
But if the source route is loose, the datagram must pass through each listed node, but can also pass through other nodes that do not appear in the source route.
While it can be very useful for network debugging, it can be used for "source address spoofing" and other types of attacks.
One legitimate use for source routing is to detect asymmetric routes using the traceroute program, as demonstrated on pp.
Nevertheless, specifying and receiving source routes is part of the sockets API and needs to be described.
IPv4 source routes are called source and record routes (LSRR for the loose option and SSRR for the strict option), because as a datagram passes through all the listed nodes, each one replaces its listed address with the address of the outgoing interface.
This allows the receiver to take this new list and reverse it to follow the reverse path back to the sender.
This is the format of the buffer that we will pass to setsockopt.
We place an NOP before the source route option, which causes all the IP addresses to be aligned on a four-byte boundary.
This is not required, but takes no additional space (the IP options are always padded to be a multiple of four bytes) and aligns the addresses.
In this figure, we show up to 10 IP addresses in the route, but the first listed address is removed from the source route option and becomes the destination address of the IP datagram when it leaves the source host.
The len that we specify is the size of the option in bytes, including the three-byte header, and including the extra destination address at the end.
The NOP is not part of the option and is not included in the len field, but is included in the size of the buffer that we specify to setsockopt.
The value of this field increases by four as the datagram is processed by each listed node.
We now develop three functions to initialize, create, and process a source route option.
While it is possible to combine a source route with other IP options (such as router alert), such a combination is rare.
The value of the EOL option is 0, so this initializes the entire option to EOL bytes.
As shown in Figure 27.1, we first use an NOP for alignment, then the type of source route (loose or strict), the length, and the pointer.
We save a pointer to the len field and will store this value as each address is added to the list.
The pointer to the option is returned to the caller and will be passed as the fourth argument to setsockopt.
The argument points to either a hostname or a dotted-decimal IP address.
We check that too many addresses are not specified and then initialize if this is the first address.
Our host_serv function handles either a hostname or a dotted-decimal string and we store the resulting binary address in the list.
We update the len field and return the total size of the buffer (including the NOP) that the caller must pass to setsockopt.
When a received source route is returned to the application by getsockopt, the format is different from Figure 27.1
First, the order of the addresses has been reversed by the kernel from the ordering in the received source route.
What we mean by "reversed" is that if the received source route contains.
Since the NOP is always present, the length returned by getsockopt will always be a multiple of 4 bytes.
In Figure 27.5, we find it just as easy to parse the data ourselves, instead of using this structure.
This returned format differs from the format that we pass to setsockopt.
Fortunately, we do not have to do this, as Berkeley-derived implementations automatically use the reverse of a received source route for a TCP socket.
That is, the information shown in Figure 27.4 is returned by getsockopt for our information only.
We do not have to call setsockopt to tell the kernel to use this route for IP datagrams sent on the TCP connection; the kernel does that automatically.
We will see an example of this shortly with our TCP server.
The next of our source route functions takes a received source route, in the format shown in Figure 27.4, and prints the information.
The first IP address in the buffer is saved and any NOPs that follow are skipped.
We only print the information for a source route, and from the three-byte header, we check the code, fetch the len, and skip over the ptr.
We then print all the IP addresses that follow the three-byte header, except the destination IP address.
We now modify our TCP echo client to specify a source route and our TCP echo server to print a received source route.
We call our inet_srcrt_init function to initialize the source route, with the type of route specified by either the -g option (loose) or the -G option (strict)
If the ptr pointer is set, a source route option was specified and we add all the specified intermediate hops to the source route that we allocated above with our inet_srcrt_add function.
If ptr is not set, but there is more than one argument remaining on the command line, the user specified a route without specifying whether it is loose or strict, so we exit with an error.
Figure 27.6 TCP echo client that specifies a source route.
The final command-line argument is the hostname or dotted-decimal address of the server and our host_serv function processes it.
We are not able to call our tcp_connect function because we must specify the source route between the calls to socket and connect.
The latter initiates the three-way handshake and we want the initial SYN and all subsequent packets to use this source route.
Our TCP server is almost identical to the code shown in Figure 5.12, with the following changes.
We then fetch the IP options after the call to accept, but before the call to fork.
If the received SYN from the client does not contain any IP options, the len variable will contain 0 on return from getsockopt (it is a value-result argument)
All we are doing by calling getsockopt is obtaining a copy of the reversed source route.
If we do not want TCP to use this route, we call setsockopt after accept returns, specifying a fifth argument (the length) of 0, and this removes any IP options currently in use.
The source route has already been used by TCP for the second segment of the three-way handshake (Figure 2.5), but if we remove the options, IP will use whatever route it calculates for future packets to this client.
We now show an example of our client/server when we specify a source route.
We run our client on the host freebsd as follows:
The two systems, freebsd4 and macosx, must forward and accept source-routed datagrams for this example to work.
When the connection is established at the server, it outputs the following:
If we watch the client/server exchange using tcpdump, we can see the source route option on every datagram in both directions.
Unfortunately, the operation of the IP_OPTIONS socket option has never been documented, so you may encounter variations on systems that are not derived from the Berkeley source code.
Also, Solaris 2.5 precedes all source route options with four NOPs, limiting the option to eight IP addresses instead of the real limit of nine.
Unfortunately, source routes present a security hole to programs that perform authentication using only IP addresses (now known to be inadequate)
If a hacker sends packets with a trusted address as the source, but his or her own address in the source route, the return packets using the reverse source route will get to the hacker without involving the system listed as the original source at all.
If a connection arrives with any IP options (the value of optsize returned by getsockopt is nonzero), a message is logged using syslog and setsockopt is called to clear the options.
This prevents any future TCP segments sent on this connection from using the reverse of the received source route.
This technique is now known to be inadequate, because by the time the application receives the connection, the TCP three-way handshake is complete, and the second segment (the server's SYN-ACK in Figure 2.5) has already followed the reverse of the source route back to the client (or at least to one of the intermediate hops listed in the source route, which is where the hacker is located)
Since the hacker has seen TCP's sequence numbers in both directions, even if no more packets are sent with the source route, the hacker can still send packets to the server with the correct sequence number.
The only solution for this potential problem is to forbid all TCP connections that arrive with a source route when you are using the source IP address for some form of validation (as do rlogind and rshd)
Replace the call to setsockopt in the code fragment just shown with a closing of the just-accepted connection and a termination of the newly spawned server.
This way, the second segment of the three-way handshake has already been sent, but the connection should not be left open.
There are no hop-byhop options currently defined that are usable by an application.
No destination options are currently defined that are usable by an application.2
The fragmentation header is automatically generated by a host that fragments an IPv6 datagram and then processed by the final destination when it reassembles the fragments.
This leaves the first three options, which we will discuss in the next two sections.
The hop-by-hop and destination options have a similar format, shown in Figure 27.7
The 8-bit next header field identifies the next header that follows this extension header.
The hop-by-hop options header and destination options header each hold any number of individual options, which have the format shown in Figure 27.8
This is called TLV coding because each option appears with its type, length, and value.
Additionally, the two high-order bits specify what an IPv6 node does with this option if it does not understand the option:
This error is sent only if the packet's destination is not a multicast address.
The next high-order bit specifies whether or not the option data changes en route:
However, option value assignments are made to keep the low-order 5 bits unique for as long as possible.
The 8-bit length field specifies the length of the option data in bytes.
The type field and this length field are not included in this length.
Other options are also defined, for instance, for Mobile-IPv6, but we do not show them here.
The pad1 byte is the only option without a length and value.
The padN option is used when 2 or more bytes of padding are required.
The router alert option indicates that this packet should be intercepted by certain routers along the path; the value in the router alert option indicates what routers should be interested.
We show the padding options because each hop-by-hop and destination option also has an associated alignment requirement, written as xn + y.
This means that the option must appear at an integer multiple of x bytes from the start of the header, plus y bytes.
The hop-by-hop and destination options are normally specified as ancillary data with sendmsg and returned as ancillary data by recvmsg.
Nothing special needs to be done by the application to send either or both of these options; just specify them in a call to sendmsg.
Figure 27.10 shows the format of the ancillary data objects used to send and receive the hop-byhop and destination options.
To reduce code duplication, seven functions are defined to create and process these data sections.
Returns: number of bytes required to hold empty extension header, -1 on error.
Returns: updated length of overall extension header after adding option, -1 on error.
Returns: updated length of finished extension header, –1 on error.
If the extbuf argument is not NULL, it initializes the extension header.
If the extbuf argument is not NULL, it also initializes the option and inserts any necessary padding.
It fails and returns -1 if the new option does not fit in the supplied buffer.
The type and len arguments are the type and length of the option, and are copied directly into the option header.
The align argument specifies the alignment requirement, that is, x from the function xn + y.
The value of y is derived from align and len, so it does not need to be explicitly specified.
The databufp argument is the address to a pointer that will be filled in with the location of the option value; the caller can then copy the option value into this location using the inet6_opt_set_val function or any other method.
As before, if the extbuf argument is non-NULL, the padding is actually inserted into the buffer; otherwise, the function simply computes the updated length.
The val and vallen arguments specify the value to copy into the option value buffer.
The expected use of these functions is to make two passes through the list of options you intend to insert: the first to calculate the desired length, and the second to actually build the option into an appropriately sized buffer.
We then dynamically allocate the option buffer using the size returned by inet6_opt_finish, and we will pass this buffer as the extbuf argument during the second pass.
Alternately, we can pre-allocate a buffer that should be large enough for our.
Returns: offset of next option, -1 on end of options or error.
Returns: offset of next option, -1 on end of options or error.
As with inet6_opt_append, offset is a running offset into the buffer.
The first two bytes of the routing header are the same as we showed in Figure 27.7: a next header field followed by a header extension length.
The next two bytes specify the routing type and the number of segments left (i.e., how many listed nodes are still to be visited)
An unlimited number of addresses can appear in the routing header (limited only by packet length) and segments left must be equal to or less than the number of addresses in the header.
The routing header is normally specified as ancillary data with sendmsg and returned as ancillary data by recvmsg.
Nothing special needs to be done by the application to send the header: just specify it as ancillary data in a call to sendmsg.
To receive the routing header, the IPV6_RECVRTHDR socket option must be enabled, as in.
Figure 27.12 shows the format of the ancillary data object used to send and receive the routing header.
Six functions are defined to create and process the routing header.
Returns: positive number of bytes if OK, 0 on error.
The return value is the pointer to the buffer, and this pointer is then used as an argument to the next function.
When successful, the segleft member of the routing header is updated to account for the new address.
The following three functions deal with a received routing header:
Returns: number of segments in routing header if OK, -1 on error.
The reversal can occur in place; that is, the in and out pointers can point to the same buffer.
To demonstrate these options, we create a UDP client and server.
The client, shown in Figure 27.6; the server prints the received source route and reverses it to send back to the client.
If more than one argument was supplied, all but the final argument form the source route.
We first determine how much space the route header will require with inet6_rth_space, then allocate the necessary space with malloc.
This is very similar to our IPv4 TCP client, except that instead of our own helper functions, these library functions are provided by the system.
We use host_serv to look up the destination, and create a socket to use.
As we will see in Section 27.7, instead of sending the same ancillary data with every packet, we can use setsockopt to apply the routing header to every packet in the session.
We only set this option if ptr is non-NULL, meaning that we allocated a route header earlier.
Our server is the same simple UDP server as before: open a socket and call dg_echo.
The setup is trivial, so we do not show it.
To receive the incoming source route, we must set the IPV6_RECVRTHDR socket option.
We must also use recvmsg, so we set up the unchanging fields of a msghdr structure.
We set the length fields to the appropriate sizes and call recvmsg.
Even though we are only expecting one piece of ancillary data, it is good practice to loop like this.
We then reverse the route with inet6_rth_reverse so that we can use it to return the packet along the same path.
In this case, inet6_rth_reverse works on the route in place, so that we can use the same ancillary data to send the return packet.
We set the length of the data to send, and use sendmsg to return the packet.
We first use inet6_rth_segments to determine the number of segments present in the route.
Our client and server that handle IPv6 source routes do not need to know how the source route is formatted in the packet.
The library functions the API provides hide the details of the packet format from us, yet give us all the flexibility we had when we built the option from scratch in IPv4
We have described the use of ancillary data with sendmsg and recvmsg to send and receive seven different ancillary data objects:
The outgoing hop limit or received hop limit (Figure 22.21)2
When the same value will be used for all packets sent on a socket, instead of sending these options in every call to sendmsg, we can set the corresponding socket options instead.
But, these sticky options can be overridden on a per-packet basis for a UDP socket or for a raw IPv6 socket by specifying ancillary data in a call to sendmsg.
If any ancillary data is specified in a call to sendmsg, the corresponding sticky options are not sent with that datagram.
The concept of sticky options can also be used with TCP because ancillary data is never sent or received by sendmsg or recvmsg on a TCP socket.
Instead, a TCP application can set the corresponding socket option and specify any of the seven option types mentioned at the beginning of this section.
These objects then affect all packets sent on this socket.
However, retransmission of packets that were originally sent when other (or no) sticky options were set may use either the original or the new sticky options.
There is no way to retrieve options received via TCP since there is no relationship between received packets and user receive operations.
These functions dealt with struct cmsghdr objects directly, assuming that all options were contained in ancillary data.
These functions also operate directly on struct cmsghdr ancillary data objects.
In this API, sticky options were set with the IPV6_PKTOPTIONS socket option.
The ancillary data objects that would have been passed to sendmsg were instead set as the data portion of the IPV6_PKTOPTIONS socket option.
Access to IPv6 extension headers is through a functional interface, obviating the need to understand their actual format in the packet.
These extension headers are written as ancillary data with sendmsg and returned as ancillary data with recvmsg.
What would we do if we did not place an NOP at the beginning of the buffer, as shown in Figure 27.1?
Raw sockets provide three features not provided by normal TCP and UDP sockets:
The ping program, for example, sends ICMP echo requests and receives ICMP echo replies.
We will develop our own version of the ping program in Section 28.5
The multicast routing daemon, mrouted, sends and receives IGMPv4 packets.
This capability also allows applications that are built using ICMP or IGMP to be handled entirely as user processes, instead of putting more code into the kernel.
It processes two ICMP messages (router advertisement and router solicitation) that the kernel knows nothing about.
But many other values are defined for the protocol field: The IANA's "Protocol Numbers" registry lists all the values.
The gated program that implements OSPF must use a raw socket to read and write these IP datagrams since they contain a protocol field the kernel knows nothing about.
This can be used, for example, to build UDP and TCP packets, and we will show an example of this in Section 29.7
The steps involved in creating a raw socket are as follows:
The socket function creates a raw socket when the second argument is SOCK_RAW.
For example, to create an IPv4 raw socket we would write.
This prevents normal users from writing their own IP datagrams to the network.
We will describe the effect of this socket option in the next section.
This function sets only the local address: There is no concept of a port number with a raw socket.
With regard to output, calling bind sets the source IP address that will be used for datagrams sent on the raw socket (but only if the IP_HDRINCL socket option is not set)
This function sets only the foreign address: Again, there is no concept of a port number with a raw socket.
With regard to output, calling connect lets us call write or send instead of sendto, since the destination IP address is already specified.
Output on a raw socket is governed by the following rules:
Normal output is performed by calling sendto or sendmsg and specifying the destination IP address.
If the IP_HDRINCL option is not set, the starting address of the data for the kernel to send specifies the first byte following the IP header because the kernel will build the IP header and prepend it to the data from the process.
The kernel sets the protocol field of the IPv4 header that it builds to the third argument from the call to socket.
If the IP_HDRINCL option is set, the starting address of the data for the kernel to send specifies the first byte of the IP header.
The amount of data to write must include the size of the caller's IP header.
The kernel fragments raw packets that exceed the outgoing interface MTU.
Unfortunately, this means that certain pieces of the API are dependent on the OS kernel, specifically with regard to the byte ordering of the fields in the IP header.
On Linux and OpenBSD, however, all the fields must be in network byte order.
Before this, the only way for an application to specify its own IP header in packets sent on a raw IP socket was to apply a kernel patch that was introduced in 1988 by Van Jacobson to support traceroute.
The functions that perform input and output on raw sockets are some of the simplest in the kernel.
Earlier versions, such as Net/2, filled in more fields in the IP header when this option was set.
All fields in the protocol headers sent or received on a raw IPv6 socket are in network byte order.
Checksums on raw IPv6 sockets are handled differently, as will be described shortly.
One of the fields in this pseudoheader is the source IPv6 address, and normally the application lets the kernel choose this value.
To prevent the application from having to try to choose this address just to calculate the checksum, it is easier to let the kernel calculate the checksum.
By default, this option is disabled, and it is enabled by setting the option value to a nonnegative value, as in.
To disable the option, it must be set to -1
When enabled, the kernel will calculate and store the checksum for outgoing packets sent on the socket and also verify the checksums for packets received on the socket.
The first question that we must answer regarding raw socket input is: Which received IP datagrams does the kernel pass to raw sockets? The following rules apply:
Received UDP packets and received TCP packets are never passed to a raw socket.
Most ICMP packets are passed to a raw socket after the kernel has finished processing the ICMP message.
These three ICMP messages are processed entirely by the kernel.
All IGMP packets are passed to a raw socket after the kernel has finished processing the IGMP message.
All IP datagrams with a protocol field that the kernel does not understand are passed to a raw socket.
If the datagram arrives in fragments, nothing is passed to a raw socket until all fragments have arrived and have been reassembled.
When the kernel has an IP datagram to pass to the raw sockets, all raw sockets for all processes are examined, looking for all matching sockets.
A copy of the IP datagram is delivered to each matching socket.
The following tests are performed for each raw socket and only if all three tests are true is the datagram delivered to the socket:
If a nonzero protocol is specified when the raw socket is created (the third argument to socket), then the received datagram's protocol field must match this value or the datagram is not delivered to this socket.
If a local IP address is bound to the raw socket by bind, then the destination IP address of the received datagram must match this bound address or the datagram is not delivered to this socket.
If a foreign IP address was specified for the raw socket by connect, then the source IP address of the received datagram must match this connected address or the datagram is not delivered to this socket.
Notice that if a raw socket is created with a protocol of 0, and neither bind nor connect is called, then that socket receives a copy of every raw datagram the kernel passes to raw sockets.
Whenever a received datagram is passed to a raw IPv4 socket, the entire datagram, including the IP header, is passed to the process.
Under Linux, all fields are left in network byte order.
As previously mentioned, the raw socket interface is defined to provide an identical interface to the one a protocol would have if it was resident in the kernel, so the contents of the fields are dependent on the OS kernel.
We mentioned in the previous section that all fields in a datagram received on a raw IPv6 socket are left in network byte order.
But most applications using a raw socket are interested in only a small subset of all ICMP messages.
The filt argument to all the macros is a pointer to an icmp6_filter variable that is modified by the first four macros and examined by the final two macros.
The SETPASSALL macro specifies that all message types are to be passed to the application, while the SETBLOCKALL macros specifies that no message types are to be passed.
The SETPASS macro enables one specific message type to be passed to the application while the SETBLOCK macro blocks one specific message type.
As an example, consider the following application, which wants to receive only ICMPv6 router advertisements:
We first block all message types (since the default is to pass all message types) and then pass only router advertisements.
We will develop our own program instead of presenting the publicly available source code for two reasons.
First, the publicly available ping program suffers from a common programming disease known as creeping featurism : It supports a dozen different options.
Our goal in examining a ping program is to understand the network programming concepts and techniques without being distracted by all these options.
Our version of ping supports only one option and is about five times smaller than the public version.
The operation of ping is extremely simple: An ICMP echo request is sent to some IP address and that node responds with an ICMP echo reply.
We will see that we set the identifier to the PID of the ping process and we increment the sequence number by one for each packet we send.
We store the 8-byte timestamp of when the packet is sent as the optional data.
The rules of ICMP require that the identifier, sequence number , and any optional data be returned in the echo reply.
Storing the timestamp in the packet lets us calculate the RTT when the reply is received.
Note that we made our ping program set-user-ID, as it takes superuser privileges to create a raw socket.
Figure 28.3 is an overview of the functions that comprise our ping program.
The program operates in two parts: One half reads everything received on a raw socket, printing the ICMP echo replies, and the other half sends an ICMP echo request once per second.
The second half is driven by a SIGALRM signal once per second.
Figure 28.4 shows our ping.h header that is included by all our program files.
This structure contains two function pointers, two pointers to socket address structures, the size of the socket address structures, and the protocol value for ICMP.
We set the amount of optional data that gets sent with the ICMP echo request to 56 bytes.
Any data that accompanies an echo request must be sent back in the echo reply.
We will store the time at which we send an echo request in the first 8 bytes of this data area and then use this to calculate and print the RTT when the echo reply is received.
We do not print echo replies belonging to another copy of ping that is running.
A signal handler is established for SIGALRM , and we will see that this signal is generated once per second and causes an ICMP echo request to be sent.
A hostname or IP address string is a required argument and it is processed by our host_serv function.
We initialize the pr global to the correct proto structure.
The socket address structure that has already been allocated by the getaddrinfo function is used as the one for sending, and another socket address structure of the same size is allocated for receiving.
The call to setuid sets our effective user ID to our real user ID, in case the program was set-user-ID instead of being run by root.
The program must have superuser privileges to create the raw socket, but now that the socket is.
It is always best to give up an extra privilege when it is no longer needed, just in case the program has a latent bug that someone could exploit.
If the protocol specified an initialization function, we call it.
We do this in case the user pings either the IPv4 broadcast address or a multicast address, either of which can generate lots of replies.
By making the buffer larger, there is a smaller chance that the socket receive buffer will overflow.
We call our signal handler, which we will see sends a packet and schedules a SIGALRM for one second in the future.
It is not common to see a signal handler called directly, as we do here, but it is acceptable.
A signal handler is just a C function, even though it is normally called asynchronously.
We set up the unchanging fields in the msghdr and iovec structs that we will pass to recvmsg.
Also realize that when the ICMPv4 message is received by the process on the raw socket, the kernel has already verified.
This lets us set icmp to point to the beginning of the ICMP header.
We make sure that the IP protocol is ICMP and that there is.
Figure 28.9 shows the various headers, pointers, and lengths used by the code.
If the message is an ICMP echo reply, then we must check the identifier field to see if this reply is in response to a request our process sent.
If the ping program is running multiple times on this host, each process gets a copy of all received ICMP messages.
We calculate the RTT by subtracting the time the message was sent (contained in the optional data portion of the ICMP reply) from the current time (pointed to by the tvrecv function argument)
The RTT is converted from microseconds to milliseconds and printed, along with the sequence number field and the received TTL.
The sequence number field lets the user see if packets were dropped, reordered, or duplicated, and the TTL gives an indication of the number of hops between the two hosts.
If the user specified the -v command-line option, we print the type and code fields from all other received ICMP messages.
If the -v command-line option was not specified, install a filter that blocks all ICMP message types except for the expected echo reply.
This reduces the number of packets received on the socket.
The API to request reception of the hop limit with incoming packets has changed over time.
We prefer the newer API: setting the IPV6_RECVHOPLIMIT socket option.
However, if the constant for this option is not defined, we can try the older API: setting IPV6_HOPLIMIT as an option.
The ICMPv6 header is the data returned by the receive operation.
Recall that the IPv6 header and extension headers, if any, are never returned as normal data, but as ancillary data.
Figure 28.11 shows the various headers, pointers, and lengths used by the code.
If the ICMP message type is an echo reply, we check the identifier field to see if the reply is for us.
If so, we calculate the RTT and then print it along with the sequence number and IPv6 hop limit.
We obtain the hop limit from the IPV6_HOPLIMIT ancillary data.
If the user specified the -v command-line option, we print the type and code fields from all other received ICMP messages.
We saw in Figure 28.6 that our readloop function calls this signal handler once at the beginning to send the first packet.
The identifier field is set to our PID and the sequence number field is set to the global nsent , which is then incremented for the next packet.
We store a pattern of 0xa5 in the data portion of the ICMP message.
The current time-of-day is then stored in the beginning of the data portion.
The Internet checksum is the one's complement of the one's complement sum of the 16-bit values to be checksummed.
The first while loop calculates the sum of all the 16-bit values.
If the length is odd, then the final byte is added in with the sum.
The algorithm we show in Figure 28.15 is the simple algorithm.
The kernel often has a specially optimized checksum algorithm due to the high volume of checksum computations performed by the kernel.
This function is taken from the public domain version of ping by Mike Muuss.
In this section, we will develop our own version of the traceroute program.
Like the ping program we developed in the previous section, we will develop and present our own version, instead of presenting the publicly available version.
This datagram causes the first-hop router to return an ICMP "time exceeded in transit" error.
The TTL is then increased by one and another UDP datagram is sent, which locates the next router in the path.
When the UDP datagram reaches the final destination, the goal is to have that host return an ICMP "port unreachable" error.
This is done by sending the UDP datagram to a random port that is (hopefully) not in use on that host.
Current systems, however, provide an IP_TTL socket option that lets us specify the TTL to use for outgoing datagrams.
This socket option was introduced with the 4.3BSD Reno release.
Figure 28.17 shows our trace.h header, which all our program files include.
The rec structure defines the data portion of the UDP datagram that we send, but we will see that we never need to examine this data.
The maximum TTL or hop limit that the program uses defaults to 30, although we provide the -m command-line option to let the user change this.
For each TTL, we send three probe packets, but this could be changed with another command-line option.
The initial destination port is 32768+666, which will be incremented by one each time we send a UDP datagram.
We hope that these ports are not in use on the destination host when the datagrams finally reach the destination, but there is no guarantee.
The -v command-line option causes most received ICMP messages to be printed.
The destination hostname or IP address is processed by our host_serv function, returning a pointer to an addrinfo structure.
After creating the raw socket, we reset our effective user ID to our real user ID since we no longer require superuser privileges.
If we are tracing the route to an IPv6 address and the -v command-line option was not specified, install a filter that blocks all ICMP message types except for the ones we expect: "time exceeded" or "destination unreachable." This reduces the number of packets received on the socket.
Since it is possible for multiple copies of the traceroute program to be running at any given time, we need a way to determine if a received ICMP message was generated in response to one of our datagrams, or in response to a datagram sent by another copy of the program.
We use the source port in the UDP header to identify the sending process because the returned ICMP message is required to include the UDP header from the datagram that caused the ICMP error.
We establish our function sig_alrm as the signal handler for SIGALRM because each time we send a UDP datagram, we wait three seconds for an ICMP message before sending the next probe.
Main loop; set TTL or hop limit and send three probes.
The main loop of the function is a double nested for loop.
This structure will be compared to the socket address structure returned by recvfrom when the ICMP message is read, and if the two structures are different, the IP address from the new structure will be printed.
Using this technique, the IP address corresponding to the first probe for each TTL is printed, and should the IP address change for a given value of the TTL (say a route changes while we are running the program), the new IP address will then be printed.
Each time a probe packet is sent, the destination port in the sasend socket address structure is changed by calling our sock_set_port function.
The reason for changing the port for each probe is that when we reach the final destination, all three probes are sent to a different port, and hopefully at least one of the ports is not in use.
As we mentioned earlier, if this is the first reply for a given TTL, or if the IP address of the node sending the ICMP message has changed for this TTL, we print the hostname and IP address, or just the IP address (if the call to getnameinfo doesn't return the hostname)
The RTT is calculated as the time difference from when we sent the probe to the time when the ICMP message was returned and printed.
An alarm is set for three seconds in the future and the function enters a loop that calls recvfrom , reading each ICMPv4 message returned on the raw socket.
This function avoids the race condition we described in Section 20.5 by using a global flag.
Figure 28.21 shows the various headers, pointers, and lengths used by the code.
If the ICMP message is a "time exceeded in transit" message, it is possibly a reply to one of our probes.
If the ICMP message was generated by a UDP datagram and if the source and destination ports of that datagram are the values we sent, then this is a reply to our probe from an intermediate router.
If the ICMP message is "destination unreachable," then we look at the UDP header returned in the ICMP message to see if the message is a response to our probe.
If so, and if the ICMP code is "port unreachable," we return -1 as we have reached the final destination.
If the ICMP message is from one of our probes but it is not a "port unreachable," then that ICMP code value is returned.
A common example of this is a firewall returning some other unreachable code for the destination host we are probing.
All other ICMP messages are printed if the -v flag was specified.
This function is nearly identical to recv_v4 except for the different constant names and the different structure member names.
Figure 28.22 shows the various headers, pointers, and lengths used by the code.
We have wrapped the long lines for a more readable output.
We have wrapped the long lines for a more readable output.
Receiving asynchronous ICMP errors on a UDP socket has been, and continues to be, a problem.
In the sockets API, we have seen that it requires connecting the UDP socket to one IP address to receive these errors (Section 8.11 )
The reason for this limitation is that the only error returned from recvfrom is an integer errno code, and if the application sends datagrams to multiple destinations and then calls recvfrom , this function cannot tell the application which datagram encountered an error.
In this section, we will provide a solution that does not require any kernel changes.
It also creates a Unix domain stream socket, binds it to the pathname /tmp/icmpd , and listens for incoming client connects to this pathname.
A UDP application (which is a client to the daemon) first creates its UDP socket, the socket for which it wants to receive asynchronous errors.
The application must bind an ephemeral port to this socket, for reasons we will discuss later.
It then creates a Unix domain socket and connects to this daemon's well-known pathname.
Application creates its UDP socket and a Unix domain connection to the daemon.
The application next "passes" its UDP socket to the daemon across the Unix domain connection using descriptor passing , as we described in Section 15.7
This gives the daemon a copy of the socket so that it can call getsockname and obtain the port number bound to the socket.
We will show this passing of the socket in Figure 28.28
After the daemon obtains the port number bound to the UDP socket, it closes its copy of the socket, taking us back to the arrangement shown in Figure 28.27
If the host supports credential passing (Section 15.8 ), the application could also send its credentials to the daemon.
The daemon could then check whether this user should be allowed access to this facility.
From this point on, any ICMP errors the daemon receives in response to UDP datagrams sent from the port bound to the application's UDP socket cause the daemon to send a message (which we will describe shortly) across the Unix domain socket to the application.
The application must therefore use select or poll , awaiting data on either the UDP socket or the Unix domain socket.
We now look at the source code for an application using this daemon, and then the daemon itself.
We start with Figure 28.29 , our header that is included by both the application and the daemon.
We define the server's well-known pathname and the icmpd_err structure that is passed from the server to the application whenever an ICMP message is received that should be passed to this application.
Figure 28.30 shows the ICMP messages that are handled, plus their mapping into an errno value.
Currently, there is no API defined to allow a UDP application to perform path MTU discovery.
What often happens on kernels that support path MTU discovery for UDP is that the receipt of this ICMP error causes the kernel to record the new path MTU value in the kernel's routing table, but the UDP application that sent the datagram that got discarded is not notified.
Instead, the application must time out and retransmit the datagram, in which case, the kernel will find the new (and smaller) MTU in its routing table, and the kernel will then fragment the datagram.
Passing this error back to the application lets the application retransmit sooner, and perhaps lets the application reduce the size of the datagrams it sends.
This often indicates a routing loop, which might be a transient error.
They indicate that a packet has been discarded, and we therefore treat them like a "destination unreachable" message.
Note that IPv6 does not have a "source quench" error.
All other destination unreachable messages indicate that a packet has been discarded.
The icmpd_dest member is a socket address structure containing the destination IP address and port of the datagram that generated the ICMP error.
If the application is sending datagrams to multiple destinations, it probably has one socket address structure per destination.
By returning this information in a socket address structure, the application can compare it against its own structures to find the one that caused the error.
It is a sockaddr_storage to allow storage of any sockaddr type the system supports.
We now modify our UDP echo client, the dg_cli function, to use our icmpd daemon.
The function arguments are the same as all previous versions of this function.
We call our sock_bind_wild function to bind the wildcard IP address and an ephemeral port to the UDP socket.
We do this so that the copy of this socket that we pass to the daemon has bound a port, as the daemon needs to know this port.
The daemon could also do this bind if a local port has not already been bound to the socket that it receives, but this does not work in all environments.
The easiest solution is to require the application to bind the local port before passing the socket to the daemon.
We create an AF_LOCAL socket and connect to the daemon's well-known pathname.
We also send a single byte of data, the character "1", because some implementations do not like passing a descriptor without any data.
The daemon sends back a single byte of data, consisting of the character "1" to indicate success.
We initialize a descriptor set and calculate the first argument for select (the maximum of the two descriptors, plus one)
The last half of our client is shown in Figure 28.32
This is the loop that reads a line from standard input, sends the line to the server, reads back the server's reply, and writes the reply to standard output.
Since we are calling select , we can easily place a timeout on our wait for the echo server's reply.
We set this to five seconds, enable both descriptors for readability, and call select.
If a timeout occurs, we print a message and go back to the top of the loop.
If a datagram is returned by the server, we print it to standard output.
If our Unix domain connection to the icmpd daemon is readable, we try to read an icmpd_err structure.
If this succeeds, we print the relevant information the daemon returns.
First, ANSI C says nothing about an error return from the function.
The Solaris man page says that the function returns a null pointer if the argument is out of range.
But the FreeBSD implementation, along with all the source code implementations the authors could find, handle an invalid argument by returning a pointer to a string such as "Unknown error." This makes sense and means the code above is fine.
But POSIX changes this and says that because no return value is reserved to indicate an error, if the argument is out of range, the function sets errno to EINVAL.
This means that completely conforming code must set errno to 0, call strerror , test whether errno equals EINVAL , and print some other message in case of an error.
We now show some examples of this client before looking at the daemon source code.
We first send datagrams to an IP address that is not connected to the Internet.
We assume icmpd is running and expect ICMP "host unreachable" errors to be returned by some router, but none are received.
We show this to reiterate that a timeout is still required and the generation of ICMP messages such as "host unreachable" may not occur.
Our next example sends a datagram to the standard echo server on a host that is not running the server.
We start the description of our icmpd daemon with the icmpd.h header, shown in Figure 28.33
Since the daemon can handle any number of clients, we use an array of client structures to keep the information about each client.
This is similar to the data structures we used in Section 6.8
We also declare the function prototypes and the globals shared by these functions.
Figure 28.34 shows the first half of the main function.
The client array is initialized by setting the connected socket member to –1
We unlink any previously existing instance of the Unix domain socket, bind its well-known pathname to the socket, and call listen.
The maximum descriptor is also calculated for select and a socket address structure is allocated for calls to accept.
Figure 28.35 shows the second half of the main function, which is an infinite loop that calls select , waiting for any of the daemon's descriptors to be readable.
Figure 28.35 Second half of main function: handles readable descriptor.
The listening Unix domain socket is tested first and if ready, readable_listen is called.
The variable nready , the number of descriptors that select returns as readable, is a global variable.
Each of our readable _XXX function decrements this variable and returns its new value as the return value of the function.
When this value reaches 0, all the readable descriptors have been processed and select is called again.
We next check whether any of the connected Unix domain sockets are readable.
Readability on any of these sockets means that the client has sent a descriptor, or that the client has terminated.
The connection is accepted and the first available entry in the client array is used.
The code in this function was copied from the beginning of Figure 6.22
If an entry couldn't be found in the client array, we simply closed the new client connection and remained to serve our current clients.
Its argument is the index of this client in the client array.
If the return value is 0, the client has closed its end of the connection, possibly by terminating.
One design decision was whether to use a Unix domain stream socket between the application and the daemon, or a Unix domain datagram socket.
The application's UDP socket can be passed over either type of Unix domain socket.
The reason why we used a stream socket was to detect when a client terminated.
When a client terminates, all its descriptors are automatically closed, including its Unix domain connection to the daemon, which tells the daemon to remove this client from the client array.
Had we used a datagram socket, we would not know when the client terminated.
If the client has not closed the connection, then we expect a descriptor.
Since we do not know what size buffer to allocate for the socket address structure, we use a sockaddr_storage structure, which is large enough and appropriately aligned to store any socket address structure the system supports.
The address family of the socket is stored in the client structure, along with the port number.
Figure 28.38 Get port number that client has bound to its UDP socket.
One byte consisting of the character "1" is sent back to the client.
We are finished with the client's UDP socket and close it.
This descriptor was passed to us by the client and is therefore a copy; hence, the UDP socket is still open in the client.
If an error occurs, a byte of "0" is written back to the client.
When the client terminates, our end of the Unix domain connection is closed, and the descriptor is removed from the set of descriptors for select.
The connfd member of the client structure is set to –1, indicating it is available.
This function prints some information about every received ICMPv4 message.
This was done for debugging when developing this daemon and could be output based on a command-line argument.
This is the IP header of the datagram that elicited the ICMP error.
We verify that this IP datagram is a UDP datagram and then fetch the source UDP port number from the UDP header following the IP header.
A search is made of all the client structures for a matching address family and port.
If a match is found, an IPv4 socket address structure is built containing the destination IP address and port from the UDP datagram that caused the error.
An icmpd_err structure is built that is sent to the client across the Unix domain connection to this client.
We can read and write IP datagrams with a protocol field that the kernel does not handle.
We can build our own IPv4 header, normally used for diagnostic purposes (or by hackers, unfortunately)
We also developed our own icmpd daemon that provides access to ICMP errors for a UDP socket.
This example also provided an example of descriptor passing across a Unix domain socket between an unrelated client and server.
What information in an IPv6 datagram is not available to an application?
Unix domain connection to the icmpd daemon and lots of ICMP errors arrive for the client? What is the easiest solution?
That is, a broadcast ICMP echo request is sent as a link-layer broadcast, even though we do not set the SO_BROADCAST socket option.
Providing access to the datalink layer for an application is a powerful feature that is available with most current operating systems.
The ability to watch the packets received by the datalink layer, allowing programs such as tcpdump to be run on normal computer systems (as opposed to dedicated hardware devices to watch packets)
When combined with the capability of the network interface to go into a promiscuous mode, this allows an application to watch all the packets on the local cable, not just the packets destined for the host on which the program is running.
This ability is less useful in switched networks, which have become quite common.
This is because the switch only passes traffic to a port if it is addressed to the device or devices attached to that port (unicast, multicast, or broadcast)
To monitor traffic carried on other ports of the switch, the switch port must be configured to receive other traffic, often called monitor mode or port mirroring.
The ability to run certain programs as normal applications instead of as part of the kernel.
For example, most Unix versions of an RARP server are normal applications that read RARP requests from the datalink (RARP requests are not IP datagrams) and then write the reply back to the datalink.
We present an overview of these three, but then describe libpcap, the publicly available packet capture library.
This library works with all three and using this library makes our programs independent of the actual datalink access provided by the OS.
We describe this library by developing a program that sends DNS queries to a name server (we build our own UDP datagrams and write them to a raw socket) and reading the reply using libpcap to determine if the name server enables UDP checksums.
Each datalink driver calls BPF right before a packet is transmitted and right after a packet is received, as shown in Figure 29.1
The reason for calling BPF as soon as possible after reception and as late as possible before transmission is to provide accurate timestamps.
While it is not hard to provide a tap into the datalink to catch all packets, the power of BPF is in its filtering capability.
Each application that opens a BPF device can load its own filter, which is then applied by BPF to each packet.
While some filters are simple (the filter " udp or tcp" receives only UDP or TCP packets), others can examine fields in the packet headers for certain values.
While one can write filter programs in the machine language of this pseudomachine (which is described on the BPF man page), the simplest interface is to compile ASCII strings (such as the one beginning with tcp that we just showed) into this machine language using the pcap_compile function that we will describe in Section 29.7
Three techniques are used by BPF to reduce its overhead:
The BPF filtering is within the kernel, which minimizes the amount of data copied from BPF to the application.
This copy, from kernel space to user space, is expensive.
If every packet was copied, BPF could have trouble keeping up with fast datalinks.
Only a portion of each packet is passed by BPF to the application.
Most applications need only the packet headers, not the packet data.
This also reduces the amount of data copied by BPF to the application.
But, to print additional information for other protocols (e.g., DNS and NFS) requires the user to increase this value when tcpdump is run.
The purpose of the buffering is to reduce the number of system calls.
The same number of packets are still copied between BPF and the application, but each system call has an overhead, and reducing the number of system calls always reduces the overhead.
Although we show only a single buffer in Figure 29.1, BPF maintains two buffers for each application and fills one while the other is being copied to the application.
In Figure 29.1, we show only the BPF reception of packets: packets received by the datalink from below (the network) and packets received by the datalink from above (IP)
The application can also write to BPF, causing packets to be sent out the datalink, but most applications only read from BPF.
There is no reason to write to BPF to send IP datagrams because the IP_HDRINCL socket option allows us to write any type of IP datagram desired, including the IP header.
The only reason to write to BPF is to send our own network packets that are not IP datagrams.
The RARP daemon does this, for example, to send its RARP replies, which are not IP datagrams.
To access BPF, we must open a BPF device that is not currently open.
Once a device is opened, about a dozen ioctl commands set the characteristics of the device: load the filter, set the read timeout, set the buffer size, attach a datalink to the BPF device, enable promiscuous mode, and so on.
Access to DLPI is by sending and receiving STREAMS messages.
In one style, there is a single device to open, and the desired interface is specified using a DLPI DL_ATTACH_REQ request.
In the other style, the application simply opens the device (e.g., le0)
But for efficient operation, two additional STREAMS modules are normally pushed onto the stream: pfmod, which performs packet filtering within the kernel, and bufmod, which buffers the data destined for the application.
Conceptually, this is similar to what we described in the previous section for BPF: pfmod supports filtering within the kernel using a pseudomachine and bufmod reduces the amount of data and number of system calls by supporting a snapshot length and a read timeout.
One interesting difference, however, is the type of pseudomachine supported by the BPF and pfmod filters.
The BPF filter is a directed acyclic control flow graph (CFG), while pfmod uses a Boolean expression tree.
The former maps naturally into code for a register machine while the latter maps naturally into code for a stack machine [McCanne and Jacobson 1993]
Another difference is that BPF always makes the filtering decision before copying the packet, in order to not copy packets that the filter will discard.
Depending on the DLPI implementation, the packet may be copied to give it to pfmod, which may then discard it.
There are two methods of receiving packets from the datalink layer under Linux.
The original method, which is more widely available but less flexible, is to create a socket of type SOCK_PACKET.
The newer method, which introduces more filtering and performance features, is to create a socket of family PF_PACKET.
To do either, we must have sufficient privileges (similar to creating a raw socket), and the third argument to socket must be a nonzero value specifying the Ethernet frame type.
For example, to receive all frames from the datalink, we write.
This would return frames for all protocols that the datalink receives.
If we wanted only IPv4 frames, the call would be.
Specifying a protocol of ETH_P_xxx tells the datalink which frame types to pass to the socket for the frames the datalink receives.
If the datalink supports a promiscuous mode (e.g., an Ethernet), then the device must also be put into a promiscuous mode, if desired.
On older systems, this is done instead by an ioctl of SIOCGIFFLAGS to fetch the flags, setting the IFF_PROMISC flag, and then storing the flags with SIOCSIFFLAGS.
Unfortunately, with this method, multiple promiscuous listeners can interfere with each other and a buggy program can leave promiscuous mode on even after it exits.
Some differences are evident when comparing this Linux feature to BPF and DLPI:
The Linux feature provides no kernel buffering and kernel filtering is only available on newer systems (via the SO_ATTACH_FILTER socket option)
There is a normal socket receive buffer, but multiple frames cannot be buffered together and passed to the application with a single read.
This increases the overhead involved in copying the potentially voluminous amounts of data from the kernel to the application.
PF_PACKET sockets can be linked to a device by calling bind.
The application must then discard data from any device in which it is not interested.
The problem again is too much data can be returned to the application, which can get in the way when monitoring a high-speed network.
Currently, it supports only the reading of packets (although adding a few lines of code to the library lets one write datalink packets too on some systems)
See the next section for a description of another library that supports not only writing datalink packets, but also constructing arbitrary packets.
About 25 functions comprise the library, but rather than just describe the functions, we will show the actual use of the common functions in a complete example in a later section.
The pcap man page describes these functions in more detail.
The library hides many of the details of crafting the IP and UDP or TCP headers, and provides simple and portable access to writing datalink and raw packets.
As with libpcap, the library is made up of quite a number of functions.
We will show how to use a small group of the functions for accessing raw sockets in the example in the following section, as well as the code required to use raw sockets directly for comparison.
All the library functions begin with the libnet_ prefix; the libnet man page and online manual describe these functions in more detail.
We will now develop an example that sends a UDP datagram containing a DNS query to a name server and reads the reply using the packet capture library.
The goal of the example is to determine whether the name server computes a UDP checksum or not.
With IPv4, the computation of a UDP checksum is optional.
Most current systems enable these checksums by default, but unfortunately, older systems, notably SunOS 4.1.x, disable these checksums by default.
All systems today, and especially a system running a name server, should always run with UDP checksums enabled, as corrupted datagrams can corrupt the server's database.
Enabling or disabling UDP checksums is normally done on a systemwide basis, as described in Appendix E of TCPv1
We will build our own UDP datagram (the DNS query) and write it to a raw socket.
We could use a normal UDP socket to send the query, but we want to show how to use the IP_HDRINCL socket option to build a complete IP datagram.
We can never obtain the UDP checksum when reading from a normal UDP socket, and we can never read UDP or TCP packets using a raw socket (Section 28.4 )
Therefore, we must use the packet capture facility to obtain the entire UDP datagram containing the name server's reply.
We will also examine the UDP checksum field in the UDP header.
If it is 0, the server does not have UDP checksums enabled.
Our application to check if a name server has UDP checksums enabled.
We write our own UDP datagrams to the raw socket and read back the replies using libpcap.
Notice that UDP also receives the name server reply, and it will respond with an ICMP "port unreachable" because it knows nothing about the source port number that our application chooses.
We also note that it is harder to write a test program of this form that uses TCP, even though we are easily able to write our own TCP segments because any reply to the TCP segments we generate will normally cause our TCP to respond with an RST to whomever we sent the segment.
One way around this is to send TCP segments with a source IP address that belongs to the attached subnet but is not currently assigned to some other node.
Add an ARP entry to the sending host for this new IP address so that the sending host will answer ARP requests for this new address, but do not configure the new IP address as an alias.
This will cause the IP stack on the sending host to discard packets received for this new IP address, assuming the sending host is not acting as a router.
Figure 29.4 is a summary of the functions that comprise our program.
Figure 29.5 shows the header udpcksum.h , which includes our basic unp.h header along with the various system headers that are needed to access the structure definitions for the IP and UDP packet headers.
Additional Internet headers are required to deal with the IP and UDP header fields.
We define some global variables and prototypes for our own functions that we will show shortly.
The first part of the main function is shown in Figure 29.6
The next part of the main function, shown in Figure 29.7 , processes the command-line arguments.
The -i option lets us specify the interface on which to receive the server's reply.
If this is not specified, the packet capture library chooses one, which might not be correct on a multihomed host.
This is one way that reading from a packet capture device differs from reading from a normal socket: With a socket, we can wildcard the local address, allowing us to receive packets arriving on any interface, but with a packet capture device, we receive arriving packets on only one interface.
We note that the Linux SOCK_PACKET feature does not limit its datalink capture to a single device.
Nevertheless, libpcap provides this filtering based on either its default or on our -i option.
The -l option lets us specify the source IP address and port number.
The port (or a service name) is taken as the string following the final period, and the source IP address is taken as everything before the final period.
Figure 29.8 main function: converts hostnames and service names; creates socket.
The last part of the main function is shown in Figure 29.8
We verify that exactly two command-line arguments remain: the destination host-name and service name.
We call host_serv to convert these into a socket address structure, the pointer to which we save in dest.
If specified on the command line, we then do the same conversion of the local host-name and port, saving the pointer to the socket address structure in local.
Otherwise, we determine the local IP address to use by connecting a datagram socket to the destination and storing the resulting local address in local.
Since we will be building our own IP and UDP headers, we must know the source IP address when we write the UDP datagram.
We cannot leave it as 0 and let IP choose the address, because the address is part of the UDP pseudoheader (which we describe shortly) that we must use for the UDP checksum computation.
The function open_output prepares the output method, whether raw sockets or libnet.
The function open_pcap opens the packet capture device; we will show this function next.
We normally need superuser privileges to open the packet capture device, but this depends on the implementation.
For example, with BPF, the administrator can set the permissions of the /dev/bpf devices to whatever is desired for that system.
We now give up these additional permissions, assuming the program file is set-user-ID.
If the process has superuser privileges, calling setuid sets our real user ID, effective user ID, and saved set-user-ID to our real user ID (getuid )
We establish signal handlers in case the user terminates the program before it is done.
If the packet capture device was not specified (the -i command-line option), then pcap_lookupdev chooses a device.
It issues the SIOCGIFCONF ioctl and chooses the lowest numbered device that is up, but not the loopback.
Many of the pcap library functions fill in an error string if an error occurs.
The sole argument to this function is an array that is filled in with an error string.
The term "live" refers to an actual device being opened, instead of a save file containing previously saved packets.
If the promiscuous flag is set, the interface is placed into promiscuous mode, causing it to receive all packets passing by on the wire.
For our example, however, the DNS server replies will be sent to our host.
Instead of having the device return a packet to the process every time a packet is received (which could be inefficient, invoking lots of copies of individual packets from the kernel to the process), a packet is returned only when either the device's read buffer is full or when the read timeout expires.
If the read timeout is set to 0, every packet is returned as soon as it is received.
We must specify the subnet mask in the call to pcap_compile that follows, because the packet filter needs this to determine if an IP address is a subnet-directed broadcast address.
This will select the packets that we want to receive.
This initiates the capturing of the packets we selected with the filter.
We need this when receiving packets to determine the size of the datalink header that will be at the beginning of each packet we read (Figure 29.15 )
This function sends a DNS query and reads the server's reply.
We want the two automatic variables, nsent and timeout , to retain their values after a siglongjmp from the signal handler back to this function.
A signal handler is established for SIGALRM and sigsetjmp establishes a jump buffer for siglongjmp.
These two functions are described in detail in Section 10.15 of APUE.
The second argument of 1 to sigsetjmp tells it to save the current signal mask since we will call siglongjmp from our signal handler.
This code is executed only when siglongjmp is called from our signal handler.
This indicates that a timeout occurred: We sent a request and never received a reply.
Since we do not want to modify the library functions to return this error, our only solution is to catch the SIGALRM signal and perform a nonlocal goto, returning control to our code instead of the library code.
We call alarm to prevent the read from blocking forever.
If the specified timeout period (in seconds) expires, SIGALRM is generated and our signal handler calls siglongjmp.
If the received UDP checksum is 0, the server did not calculate and send a checksum.
The flag canjump was set in Figure 29.10 after the jump buffer was initialized by sigsetjmp.
To understand the details of the UDP datagram built by this function requires an understanding of the DNS message format.
We call our function udp_write to build the UDP and IP headers and write the IP datagram to our raw socket.
We declare a global variable in which to hold the descriptor for the raw socket.
We create a raw socket and enable the IP_HDRINCL socket option.
This option lets us write complete IP datagrams, including the IP header.
We explicitly set the header area to zeros, to avoid checksumming any leftover data that might be in the buffer.
Previous versions of this code explicitly set every element of the struct udpiphdr to zero; however, this struct contains some implementation details so it may be different from system to system.
This is a typical portability problem when building headers explicitly.
When the UDP checksum is calculated, it includes not only the UDP header and UDP data, but also fields from the IP header.
These additional fields from the IP header form what is called the pseudoheader.
The inclusion of the pseudoheader provides additional verification that if the checksum is correct, then the datagram was delivered to the correct host and to the correct protocol code.
These statements initialize the fields in the IP header that form the pseudoheader.
In one's-complement arithmetic, the two values are the same, but UDP sets the checksum to 0 to indicate that the sender did not store a UDP checksum.
The kernel calculates the checksum and we must set the ui_sum field to the UDP length.
Since we have set the IP_HDRINCL socket option, we must fill in most fields in the IP header.
Section 28.3 discusses these writes to a raw socket when this socket option is set.
Note that we set the ip_len field in either host or network byte order, depending on the OS we're using.
This is a typical portability problem when using raw sockets.
Since the datalink headers differ depending on the actual device type, we branch based on the value returned by the pcap_datalink function.
We call the library function pcap_next , which returns the next packet or NULL if a timeout occurs.
If the timeout occurs, we simply loop and call pcap_next again.
A pointer to the packet is the return value of the function and the second argument points to a pcap_pkthdr structure, which is also filled in on return.
The timestamp is when the packet capture device read the packet, as opposed to the actual delivery of the packet to the process, which could be sometime later.
The purpose of the packet capture facility is to capture the packet headers and not all the data in each packet.
The captured length is returned through the pointer argument and the return value of the function is the pointer to the packet.
If we look at the implementation of pcap_next in the library, it shows the division of labor between the different functions.
For example, we show that the BPF implementation calls read , while the DLPI implementation calls getmsg and the Linux implementation calls recvfrom.
Arrangement of function calls to read from packet capture library.
Our function udp_check verifies numerous fields in the IP and UDP headers.
We must do these verifications because when the packet is passed to us by the packet capture device, the IP layer has not yet seen the packet.
The packet length must include at least the IP and UDP headers.
The IP version is verified along with the IP header length and the IP header checksum.
If the protocol field indicates a UDP datagram, the function returns the pointer to the combined IP/UDP header.
We first run our program with the -0 command-line option to verify that the name server responds to datagrams that arrive with no checksum.
Next, we run our program to a local name server (our system freebsd4 ) that does not have UDP checksums enabled.
Note that it's increasingly rare to find a name server without UDP checksums enabled.
As we will see, libnet takes care of many details for us, including the portability problems with checksums and IP header byte order that we mentioned.
In this way, it is similar to both socket and pcap descriptors.
We then call the libnet_build_dnsv4 function, which accepts each field in the DNS packet as a separate function argument.
We only need to know the layout of the query portion; the details of how to put together the DNS packet header are taken care of for us.
Fill in UDP header and arrange for UDP checksum calculation.
Similarly, we build the UDP header by calling libnet_build_udp function.
This also accepts each header field as a separate function argument.
When passing a checksum field in as 0, libnet automatically calculates the checksum for that field.
If the user requested that the checksum not be calculated, we must specifically turn checksum calculation off.
As with other libnet_build functions, we supply only the field contents and libnet puts the header together for us.
Note that libnet automatically takes care of whether or not the ip_len field is in network.
This is a sample of a portability improvement gained by using libnet.
We call the function libnet_write to write the assembled datagram to the network.
With raw sockets, we have the capability to read and write IP datagrams that the kernel does not understand, and with access to the datalink layer, we can extend that capability to read and write any type of datalink frame, not just IP datagrams.
Different operating systems have different ways of accessing the datalink layer.
But we can ignore all their differences and still write portable code using the freely available packet capture library, libpcap.
The freely available libnet library hides these differences and provides an interface to output both via raw sockets and directly on the datalink.
We have several choices for the type of process control to use when writing a Unix server:
Our first server, Figure 1.9, was an iterative server, but there are a limited number of scenarios where this is recommended because the server cannot process a pending client until it has completely serviced the current client.
Figure 5.2 was our first concurrent server and it called fork to spawn a child process for every client.
In Section 6.8, we developed a different version of our TCP server consisting of a single process using select to handle any number of clients.
In Figure 26.3, we modified our concurrent server to create one thread per client instead of one process per client.
There are two other modifications to the concurrent server design that we will look at in this chapter:
Preforking has the server call fork when it starts, creating a pool of child processes.
One process from the currently available pool handles each client request.
Prethreading has the server create a pool of available threads when it starts, and one thread from the pool handles each client.
There are numerous details with preforking and prethreading that we will examine in this chapter: What if there are not enough processes or threads in the pool? What if there are too many processes or threads in the pool? How can the parent and its children or threads synchronize with each other?
Clients are typically easier to write than servers because there is less process control in a client.
Nevertheless, we have already examined various ways to write our simple echo client and we summarize these in Section 30.2
In this chapter, we will look at nine different server designs and we will run each server against the same client.
Our client/server scenario is typical of the Web: The client sends a small request to the server and the server responds with data back to the client.
Some of the servers we have already discussed in detail (e.g., the concurrent server with one fork per client), while the preforked and prethreaded servers are new and therefore discussed in detail in this chapter.
We will run multiple instances of a client against each server, measuring the CPU time required to service a fixed number of client requests.
Instead of scattering all our CPU timings throughout the chapter, we summarize them in Figure 30.1 and refer to this figure throughout the chapter.
We note that the times in this figure measure the CPU time required only for process control and the iterative server is our baseline we subtract from actual CPU time because an iterative server has no process control overhead.
We include the baseline time of 0.0 in this figure to reiterate this point.
We use the term process control CPU time in this chapter to denote this difference from the baseline for a given system.
Timing comparisons of the various servers discussed in this chapter.
All these server timings were obtained by running the client shown in Figure 30.3 on two different hosts on the same subnet as the server.
For all tests, both clients spawned five children to create five simultaneous connections to the server, for a maximum of 10 simultaneous connections at the server at any time.
Each client requested 4,000 bytes from the server across the connection.
Some server designs involve creating a pool of child processes or a pool of threads.
An item to consider in these cases is the distribution of the client requests to the available pool.
Figure 30.2 summarizes these distributions and we will discuss each column in the appropriate section.
Number of clients or threads serviced by each of the 15 children or threads.
We have already examined various client designs, but it is worth summarizing their strengths and weaknesses:
First, while it is blocked awaiting user input, it does not see network events such as the peer closing the connection.
Additionally, it operates in a stop-and-wait mode, making it inefficient for batch processing.
Figure 6.9 was the next iteration, and by using select, the client was notified of network events while waiting for user input.
Figure 6.13 corrected this problem by using the shutdown function.
Figure 16.3 began the presentation of our client using nonblocking I/O.
The first of our clients that went beyond the single-process, single-thread design was Figure 16.10, which used fork with one process handling the client-to-server data and the other process handling the server-to-client data.
At the end of Section 16.2, we summarized the timing differences between these various versions.
As we noted there, although the nonblocking I/O version was the fastest, the code was more complex and using either two processes or two threads simplifies the code.
Figure 30.3 shows the client that we will use to test all the variations of our server.
Each time we run the client, we specify the hostname or IP address of the server, the server's port, the number of children for the client to fork (allowing us to initiate multiple connections to the same server concurrently), the number of requests each child should send to the server, and the number of bytes to request the server to return each time.
The parent calls fork for each child, and each child establishes the specified number of connections with the server.
On each connection, the child sends a line specifying the number of bytes for the server to return, and then the child reads that amount of data from the server.
The parent just waits for all the children to terminate.
Notice that the client closes each TCP connection, so TCP's TIME_WAIT state occurs on the client, not on the server.
This is a difference between our client/server and normal HTTP connections.
When we measure the various servers in this chapter, we execute the client as.
However, we do not need anything this sophisticated to make some general comparisons of the various server design alternatives that we will examine in this chapter.
Figure 30.3 TCP client program for testing our various servers.
An iterative TCP server processes each client's request completely before moving on to the next client.
Iterative TCP servers are rare, but we showed one in Figure 1.9: a simple daytime server.
We do, however, have a use for an iterative server in comparing the various servers in this chapter.
But since the server is iterative, there is no process control whatsoever performed by the server.
This gives us a baseline measurement of the CPU time required to handle this number of clients that we can then subtract from all the other server measurements.
From a process control perspective, the iterative server is the fastest possible because it performs no process control.
We then compare the differences from this baseline in Figure 30.1
We do not show our iterative server as it is a trivial modification to the concurrent server that we will present in the next section.
Traditionally, a concurrent TCP server calls fork to spawn a child to handle each client.
This allows the server to handle numerous clients at the same time, one client per process.
The only limit on the number of clients is the OS limit on the number of child processes for the user ID under which the server is running.
Figure 5.12 is an example of a concurrent server and most TCP servers are written in this fashion.
The problem with these concurrent servers is the amount of CPU time it takes to fork a child for each client.
Years ago (the late 1980s), when a busy server handled hundreds or perhaps even a few thousand clients per day, this was acceptable.
But the explosion of the Web has changed this attitude.
Busy Web servers measure the number of TCP connections per day in the millions.
This is for an individual host, and the busiest sites run multiple hosts, distributing the load among the hosts.
Figure 30.4 shows the main function for our concurrent TCP server.
This function is similar to Figure 5.12: It calls fork for each client connection and handles the SIGCHLD signals from the terminating children.
We also catch the SIGINT signal, generated when we type our terminal interrupt key.
We type this key after the client completes, to print the CPU time required for the program.
This is an example of a signal handler that does not return.
The values printed are the total user time (CPU time spent in the user process) and total system time (CPU time spent within the kernel, executing on behalf of the calling process)
After the client establishes the connection with the server, the client writes a single line specifying the number of bytes the server must return to the client.
This is some-what similar to HTTP: The client sends a small request and the server responds with the desired information (often an HTML file or a GIF image, for example)
In the case of HTTP, the server normally closes the connection after sending back the requested data, although newer versions are using persistent connections, holding the TCP connection open for additional client requests.
When compared to the subsequent lines in this figure, we see that the concurrent server requires the most CPU time, which is what we expect with one fork per client.
One server design that we do not measure in this chapter is one invoked by inetd, which we covered in Section 13.5
Our first of the "enhanced" TCP servers uses a technique called preforking.
Instead of generating one fork per client, the server preforks some number of children when it starts, and then the children are ready to service the clients as each client connection arrives.
Figure 30.8 shows a scenario where the parent has preforked N children and two clients are currently connected.
The advantage of this technique is that new clients can be handled without the cost of a fork by the parent.
The disadvantage is that the parent must guess how many children to prefork when it starts.
If the number of clients at any time ever equals the number of children, additional clients are ignored until a child is available.
But recall from Section 4.5 that the clients are not completely ignored.
The kernel will complete the three-way handshake for any additional clients, up to the listen backlog for this socket, and then pass the completed connections to the server when it calls accept.
But, the client application can notice a degradation in response time because even though its connect might return immediately, its first request might not be handled by the server for some time.
With some extra coding, the server can always handle the client load.
What the parent must do is continually monitor the number of available children, and if this value drops below some threshold, the parent must fork additional children.
Also, if the number of available children exceeds another threshold, the parent can terminate some of the excess children, because as we'll see later in this chapter, having too many available children can degrade performance, too.
But before worrying about these enhancements, let's examine the basic structure of this type of server.
Figure 30.9 shows the main function for the first version of our preforked server.
An additional command-line argument is the number of children to prefork.
An array is allocated to hold the PIDs of the children, which we need when the program terminates to allow the main function to terminate all the children.
We do this by sending SIGTERM to each child, and then we wait for all the children.
The child continues in this loop until terminated by the parent.
If you have never seen this type of arrangement (multiple processes calling accept on the same listening descriptor), you probably wonder how it can even work.
It's worth a short digression on how this is implemented in Berkeley-derived kernels (e.g., as presented in TCPv2)
The parent creates the listening socket before spawning any children, and if you recall, all descriptors are duplicated in each child each time fork is called.
Figure 30.13 shows the arrangement of the proc structures (one per process), the one file structure for the listening descriptor, and the one socket structure.
Descriptors are just an index in an array in the proc structure that reference a file structure.
One of the properties of the duplication of descriptors in the child that occurs with fork is that a given descriptor in the child references the same file structure as that same descriptor in the parent.
Each file structure has a reference count that starts at one when the file or socket is opened and is incremented by one each time fork is called or each time the descriptor is duped.
In our example with N children, the reference count in the file structure would be N + 1 (don't forget the parent that still has the listening descriptor open, even though the parent never calls accept)
When the first client connection arrives, all N children are awakened.
This is because all N have gone to sleep on the same "wait channel," the so_timeo member of the socket structure, because all N share the same listening descriptor, which points to the same socket structure.
This is sometimes called the thundering herd problem because all N are awakened even though only one will obtain the connection.
Nevertheless, the code works, with the performance side effect of waking up too many processes each time a connection is ready to be accepted.
We can measure the effect of the thundering herd problem by just increasing the number of children for the same maximum number of clients (10)
We don't show the results of increasing the number of children because the individual test results aren't that interesting.
Since any number greater than 10 introduces superfluous children, the thundering herd problem worsens and the timing results increase.
The next thing to examine is the distribution of the client connections to the pool of available children that are blocked in the call to accept.
To collect this information, we modify the main function to allocate an array of long integer counters in shared memory, one counter per child.
Since the array is created by mmap before the children are spawned, the array is then shared between this process (the parent) and all its children created later by fork.
Figure 30.14 meter function to allocate an array in shared memory.
When the available children are blocked in the call to accept, the kernel's scheduling algorithm distributes the connections uniformly to all the children.
While looking at this example under 4.4BSD, we can also examine another poorly understood, but rare phenomenon.
A collision occurs when multiple processes call select on the same descriptor, because room is allocated in the socket structure for only one process ID to be awakened when the descriptor is ready.
If multiple processes are waiting for the same descriptor, the kernel must wake up all processes that are blocked in a call to select since it doesn't know which processes are affected by the descriptor that just became ready.
We can force select collisions with our example by preceding the call to accept in Figure 30.12 with a call to select, waiting for readability on the listening socket.
The children will spend their time blocked in this call to select instead of in the call to accept.
Part of this increase is probably because of the additional system call (since we are calling select and accept instead of just accept), and another part is probably because of the kernel overhead in handling the collisions.
The lesson to be learned from this discussion is when multiple processes are blocking on the same descriptor, it is better to block in a function such as accept instead of blocking in select.
The implementation that we just described for 4.4BSD, which allows multiple processes to call accept on the same listening descriptor, works only with Berkeley-derived kernels that implement accept within the kernel.
System V kernels, which implement accept as a library function, may not allow this.
Indeed, if we run the server from the previous section on such a system, soon after the clients start connecting to the server, a call to accept in one of the children returns EPROTO, which means a protocol error.
Solaris fixes this, but the problem still exists in most other SVR4 implementations.
The solution is for the application to place a lock of some form around the call to accept, so that only one process at a time is blocked in the call to accept.
The remaining children will be blocked trying to obtain the lock.
There are various ways to provide this locking around the call to accept, as we described in the second volume of this series.
In this section, we will use POSIX file locking with the fcntl function.
The caller specifies a pathname template as the argument to my_lock_init, and the mktemp function creates a unique pathname based on this template.
A file is then created with this pathname and immediately unlinked.
By removing the pathname from the directory, if the program crashes, the file completely disappears.
But as long as one or more processes have the file open (i.e., the file's reference count is greater than 0), the file itself is not removed.
This is the fundamental difference between removing a pathname from a directory and closing an open file.
Two flock structures are initialized: one to lock the file and one to unlock the file.
We never write anything to the file (its length is always 0), but that is fine.
The advisory lock is still handled correctly by the kernel.
But more importantly, there is no guarantee by POSIX as to the order of the members in the structure.
The l_type member may be the first one in the structure, but not on all systems.
All POSIX guarantees is that the members that POSIX requires are present in the structure.
Therefore, initializing a structure to anything other than all zeros should always be done by actual C code, and not by an initializer when the structure is allocated.
An exception to this rule is when the structure initializer is provided by the implementation.
For example, when initializing a Pthread mutex lock in Chapter 26, we wrote.
The pthread_mutex_t datatype is often a structure, but the initializer is provided by the implementation and can differ from one implementation to the next.
Figure 30.17 shows the two functions that lock and unlock the file.
These are just calls to fcntl, using the structures that were initialized in Figure 30.16
This new version of our preforked server now works on SVR4 systems by assuring that only one child process at a time is blocked in the call to accept.
The Apache Web server, http://www.apache.org, preforks its children and then uses either the technique in the previous section (all children blocked in the call to accept), if the implementation allows this, or file locking around the accept.
We can check this version to see if the same thundering herd problem exists, which we described in the previous section.
We check by increasing the number of (unneeded) children and noticing that the timing results get worse proportionally.
We can examine the distribution of the clients to the pool of available children by using the function we described with Figure 30.14
The OS distributes the file locks uniformly to the waiting processes (and this behavior was uniform across several operating systems we tested)
As we mentioned, there are various ways to implement locking between processes.
The POSIX file locking in the previous section is portable to all POSIX-compliant systems, but it involves filesystem operations, which can take time.
In this section, we will use thread locking, taking advantage of the fact that this can be used not only for locking between the threads within a given process, but also for locking between different processes.
The only thing that changes is our three locking functions.
To use thread locking between different processes requires that: (i) the mutex variable must be stored in memory that is shared between all the processes; and (ii) the thread library must be told that the mutex is shared among different processes.
There are various ways to share memory between different processes, as we described in the second volume of this series.
In our example, we will use the mmap function with the /dev/zero device, which works under Solaris and other SVR4 kernels.
The descriptor is then closed, which is fine, because the memory mapped to /dev/zero will remain mapped.
But with a mutex in shared memory, we must call some Pthread library functions to tell the library that the mutex is in shared memory and that it will be used for locking between different processes.
Each is now just a call to a Pthread function to lock or unlock the mutex.
The final modification to our preforked server is to have only the parent call accept and then "pass" the connected socket to one child.
This gets around the possible need for locking around the call to accept in all the children, but requires some form of descriptor passing from the parent to the children.
This technique also complicates the code somewhat because the parent must keep track of which children are busy and which are free to pass a new socket to a free child.
In the previous preforked examples, the process never cared which child received a client connection.
The OS handled this detail, giving one of the children the first call to accept , or giving one of the children the file lock or the mutex lock.
The first two columns of Figure 30.2 also show that the OS that we are measuring does this in a fair, round-robin fashion.
With this example, we need to maintain a structure of information about each child.
We show our child.h header that defines our Child structure in Figure 30.20
We store the child's PID, the parent's stream pipe descriptor that is connected to the child, the child's status, and a count of the number of clients the child has handled.
We will print this counter in our SIGINT handler to see the distribution of the client requests among the children.
We create a stream pipe, a Unix domain stream socket (Chapter 15 ), before calling fork.
Furthermore, the child duplicates its end of the stream pipe (sockfd[1] ) onto standard error, so that each child just reads and writes to standard error to communicate with the parent.
Stream pipe after parent and child both close one end.
After all the children are created, we have the arrangement shown in Figure 30.23
We close the listening socket in each child, as only the parent calls accept.
We show that the parent must handle the listening socket along with all the stream sockets.
The changes from previous versions of this function are that descriptor sets are allocated and the bits corresponding to the listening socket along with the stream pipe to each child are turned on in the set.
The main loop is driven by a call to select.
The counter navail keeps track of the number of available children.
If this counter is 0, the listening socket is turned off in the descriptor set for select.
This prevents us from accepting a new connection for which there is no available child.
The kernel still queues these incoming connections, up to the listen backlog, but we do not want to accept them until we have a child ready to process the client.
If the listening socket is readable, a new connection is ready to accept.
We write one byte along with the descriptor, but the recipient does not look at the contents of this byte.
We always start looking for an available child with the first entry in the array of Child structures.
This means the first children in the array always receive new connections to process before later elements in the array.
If we didn't want this bias toward earlier children, we could remember which child received the most recent connection and start our search one element past that each time, circling back to the first element when we reach the end.
There is no advantage in doing this (it really doesn't matter which child handles a client request if multiple children are available), unless the OS scheduling algorithm penalizes processes with longer total CPU times.
Spreading the load more evenly among all the children would tend to average out their total CPU times.
We will see that our child_main function writes a single byte back to the parent across the stream pipe when the child has finished with a client.
That makes the parent's end of the stream pipe readable.
We read the single byte (ignoring its value) and then mark the child as available.
We catch this and terminate, but a better approach is to log the error and spawn a new child to replace the one that terminated.
This function differs from the ones in the previous two sections because our child no longer calls accept.
Instead, the child blocks in a call to read_fd , waiting for the parent to pass it a connected socket descriptor to process.
When we have finished with the client, we write one byte across the stream pipe to tell the parent we are available.
Passing a descriptor across the stream pipe to each child and writing a byte back across the stream pipe from the child takes more time than locking and unlocking either a mutex in shared memory or a file lock.
The earlier children do handle more clients, as we discussed with Figure 30.24
The last five sections have focused on one process per client, both one fork per client and preforking some number of children.
If the server supports threads, we can use threads instead of child processes.
It is a modification of Figure 30.4 that creates one thread per client, instead of one process per client.
The main thread blocks in a call to accept and each time a client connection is returned, a new thread is created by pthread_create.
The function executed by the new thread is doit and its argument is the connected socket.
We note from Figure 30.1 that this simple threaded version is faster than even the fastest of the preforked versions.
In Section 26.5 we noted three alternatives for converting a function that is not thread-safe into one that is thread-safe.
Therefore, for simplicity we use the less efficient version from Figure 3.17 for the threaded server examples in this chapter.
We found earlier in this chapter that it is faster to prefork a pool of children than to create one child for every client.
On a system that supports threads, it is reasonable to expect a similar speedup by creating a pool of threads when the server starts, instead of creating a new thread for every client.
The basic design of this server is to create a pool of threads and then let each thread call accept.
Instead of having each thread block in the call to accept, we will use a mutex lock (similar to Section 30.8) that allows only one thread at a time to call accept.
There is no reason to use file locking to protect the call to accept from all the threads, because with multiple threads in a single process, we know that a mutex lock can be used.
We also declare a few globals, such as the listening socket descriptor and a mutex variable that all the threads need to share.
The only argument is the index number of the thread.
We expect this, since we create the pool of threads only once, when the server starts, instead of creating one thread per client.
Indeed, this version of our server is the fastest on these two hosts.
The uniformity of this distribution is caused by the thread scheduling algorithm that appears to cycle through all the threads in order when choosing which thread receives the mutex lock.
On a Berkeley-derived kernel, we do not need any locking around the call to accept and can make a version of Figure 30.29 without any mutex locking and unlocking.
If we look at the two components of the CPU time, the user time and the system time, without any locking, the user time decreases (because the locking is done in the threads library, which executes in user space), but the system time increases (the kernel's thundering herd as all threads blocked in accept are awakened when a connection arrives)
Since some form of mutual exclusion is required to return each connection to a single thread, it is faster for the threads to do this themselves than for the kernel.
Our final server design using threads has the main thread create a pool of threads when it starts, and then only the main thread calls accept and passes each client connection to one of the available threads in the pool.
This is similar to the descriptor passing version in Section 30.9
The design problem is how does the main thread "pass" the connected socket to one of the available threads in the pool? There are various ways to implement this.
We could use descriptor passing, as we did earlier, but there's no need to pass a descriptor from one thread to another since all the threads and all the descriptors are in the same process.
All the receiving thread needs to know is the descriptor number.
We also define a clifd array in which the main thread will store the connected socket descriptors.
The available threads in the pool take one of these connected sockets and service the corresponding client.
Naturally, this data structure that is shared between all the threads must be protected and we use a mutex along with a condition variable.
The main thread blocks in the call to accept, waiting for each client connection to arrive.
When one arrives, the connected socket is stored in the next entry in the clifd array, after obtaining the mutex lock on the array.
We also check that the iput index has not caught up with the iget index, which indicates that our array is not big enough.
The condition variable is signaled and the mutex is released, allowing one of the threads in the pool to service this client.
The former is identical to the version in Figure 30.29
Each thread in the pool tries to obtain a lock on the mutex that protects the clifd array.
When the lock is obtained, there is nothing to do if the iget and iput indexes are equal.
In that case, the thread goes to sleep by calling pthread_cond_wait.
The times in Figure 30.1 show that this server is slower than the one in the previous section, in which each thread called accept after obtaining a mutex lock.
The reason is that this section's example requires both a mutex and a condition variable, compared to just a mutex in Figure 30.29
If we examine the histogram of the number of clients serviced by each thread in the pool, it is similar to the final column in Figure 30.2
In this chapter, we looked at nine different server designs and ran them all against the same Web-style client, comparing the amount of CPU time spent performing process control:
First, if the server is not heavily used, the traditional concurrent server model, with one fork per client, is fine.
This can even be combined with inetd, letting it handle the accepting of each connection.
The remainder of our comments are meant for heavily used servers, such as Web servers.
The coding is not complicated, but what is required, above and beyond the examples that we have shown, is monitoring the number of free children and increasing or decreasing this number as the number of clients being served changes dynamically.
Some implementations allow multiple children or threads to block in a call to accept, while on other implementations, we must place some type of lock around the call to accept.
Either file locking or Pthread mutex locking can be used.
Having all the children or threads call accept is normally simpler and faster than having the main thread call accept and then pass the descriptor to the child or thread.
Having all the children or threads block in a call to accept is preferable over blocking in a call to select because of the potential for select collisions.
For example, if the server that accepts the client's connection calls fork and exec, it can be faster to fork a single threaded process than to fork a multithreaded process.
In this chapter, we will provide an overview of the STREAMS system and the functions used by an application to access a stream.
Our goal is to understand the implementation of networking protocols within the STREAMS framework.
We will also develop a simple TCP client using the Transport Provider Interface (TPI), the interface into the transport layer that sockets normally use on a system based on STREAMS.
Additional information on STREAMS, including information on writing kernel routines that utilize STREAMS, can be found in [Rago 1993]
The POSIX specification defines STREAMS as an option group, which means a system may not implement STREAMS, but if it does, the implementation must comply with the POSIX specification.
Any system derived from System V should provide POSIX, but the various 4.xBSD releases do not provide POSIX.
Be careful to distinguish between STREAMS, the stream I/O system that we are describing in this chapter, versus "standard I/O streams." The latter term is used when talking about the standard I/O library (e.g., functions such as fopen, fgets, printf, and the like)
Although we describe the bottom box as a driver, this does not need to be associated with a hardware device; it can also be a pseudo-device driver (e.g., a software driver)
The stream head consists of the kernel routines that are invoked when the application makes a system call for a STREAMS descriptor (e.g., read, putmsg, ioctl, and the like)
A process can dynamically add and remove intermediate processing modules between the stream head and the driver.
A module performs some type of filtering on the messages going up and down a stream.
Any number of modules can be pushed onto a stream.
When we say "push," we mean that each new module gets inserted just below the stream head.
A special type of pseudo-device driver is a multiplexor, which accepts data from multiple sources.
When a socket is created, the module sockmod is pushed onto the stream by the sockets library.
It is the combination of the sockets library and the sockmod STREAMS module that provides the sockets API to the process.
When an XTI endpoint is created, the module timod is pushed onto the stream by the XTI library.
It is the combination of the XTI library and the timod STREAMS module that provides the X/Open Transport Interface (XTI) API to the process.
This is one of the few places where we mention XTI.
An earlier edition of this book described the XTI API in great detail, but it fell out of common use and even the POSIX specification no longer covers it, so we dropped the coverage from this book.
Figure 31.3 shows where the XTI implemention typically lives and we touch on it briefly in this chapter, but we stop short of providing any detail since there's rarely a reason to use XTI anymore.
The STREAMS module tirdwr must normally be pushed onto a stream to use read and write with an XTI endpoint.
The middle process using TCP in Figure 31.3 has done this.
This process has probably abandoned the use of XTI by doing this, so we have not shown the XTI.
Various service interfaces define the format of the networking messages exchanged up and down a stream.
The Network Provider Interface (NPI) [Unix International 1992a] defines the interface provided by a network-layer provider (e.g., IP)
An alternate reference for TPI and DLPI, which contains sample C code, is [Rago 1993]
Each component in a stream—the stream head, all processing modules, and the driver—contains at least one pair of queues: a write queue and a read queue.
Each component in a stream has at least one pair of queues.
The priority of a STREAMS message is used for both queueing and flow control.
Figure 31.5 shows the ordering of the messages on a given queue.
Ordering of STREAMS messages on a queue, based on priority.
TCP's out-of-band data is not considered true expedited data by TPI.
Indeed, TCP uses band 0 for both normal data and its out-of-band data.
The use of band 1 for expedited data is for protocols in which the expedited data (not just the urgent pointer, as in TCP) is sent ahead of normal data.
In releases before SVR4, there were no priority bands; there were just normal messages and priority messages.
SVR4 implemented priority bands, requiring the getpmsg and putpmsg functions, which we will describe shortly.
Common terminology [Rago 1993] refers to everything other than high-priority messages as normal-priority messages and then subdivides these normal-priority messages into priority bands.
Although we talk about normal-priority messages and high-priority messages, there are about a dozen normal-priority message types and around 18 high-priority message types.
Figure 31.6 shows how these three different message types are generated by the write and putmsg functions.
The data transferred up and down a stream consists of messages, and each message contains control, data, or both.
If we use read and write on a stream, these transfer only data.
To allow a process to read and write both data and control information, two new functions were added.
Both return: non-negative value if OK (see text), –1 on error.
Both the control and data portions of the message are described by the following strbuf structure:
Note the similarity between the strbuf structure and the netbuf structure.
The names of the three elements in each structure are identical.
But the two lengths in the netbuf structure are unsigned integers, while the two lengths in the strbuf structure are signed integers.
The reason why is that some of the STREAMS functions use a len or maxlen value of –1 to indicate something special.
We can send only control information, only data, or both using putmsg.
If the integer pointed to by flagsp is 0 when the function is called, the first message on the stream is returned (which can be normal- or high-priority)
If the integer value is RS_HIPRI when the function is called, the function waits for a high-priority message to arrive at the stream head.
But, getmsg returns 0 only if the entire message was returned to the caller.
If the control buffer is too small for all the control information, the return value is MORECTL (which is guaranteed to be non-negative)
Similarly, if the data buffer is too small, MOREDATA can be returned.
If both are too small, the logical OR of these two flags is returned.
When support for different priority bands was added to STREAMS with SVR4, the following two variants of getmsg and putmsg were added:
If the flags argument is MSG_BAND, then a message is generated in the specified priority band.
Note that this flag is named differently from the RS_HIPRI flag for putmsg.
The two integers pointed to by bandp and flagsp are value-result arguments for getpmsg.
The only change from the function prototype shown in Section 17.2 is the headers that must be included when dealing with STREAMS.
There are about 30 requests that affect a stream head.
Each request begins with I_ and they are normally documented on the streamio man page.
Both sockets and XTI use this interface in a STREAMS environment.
In Figure 31.3, it is a combination of the sockets library and sockmod, along with a combination of the XTI library and timod, that exchange TPI messages with TCP and UDP.
It defines the messages that are exchanged up and down a stream between the application (e.g., the sockets library) and the transport layer: the format of these messages and what operation each message performs.
In many instances, the application sends a request to the provider (such as "bind this local address") and the provider sends back a response ("OK" or "error")
Some events occur asynchronously at the provider (the arrival of a connection request for a server), causing a message or a signal to be sent up the stream.
We are able to bypass both sockets and XTI and use TPI directly.
In this section, we will rewrite our simple daytime client using TPI instead of sockets (Figure 1.5)
Using programming languages as an analogy, using sockets is like programming in a high-level language such as C or Pascal, while using TPI directly is like programming in assembly language.
We are not advocating the use of TPI directly in real applications.
But examining how TPI works and developing this example give us a better understanding of how the sockets library works in a STREAMS environment.
We need to include one additional STREAMS header along with <sys/tihdr.h>, which contains the definitions of the structures for all TPI messages.
Figure 31.8 is the main function for our daytime client.
Figure 31.8 main function for our daytime client written to TPI.
We open the device corresponding to the transport provider (normally /dev/tcp)
We call our own function tpi_bind (shown shortly) to do the bind.
We fill in another Internet socket address structure with the server's IP address (taken from the command line) and port (13)
As in our other daytime clients, we just copy data from the connection to standard output, stopping when we receive the EOF from the server (e.g., the FIN)
We then call our tpi_close function to close our endpoint.
All TPI requests are defined as a structure that begins with a long integer type field.
Since the reply is a highpriority message, it will bypass any normal-priority messages on the stream.
All these messages begin with the type, so we can read the reply assuming it is a T_BIND_ACK message, look at the type, and process the message accordingly.
We do not expect any data from the provider, so we specify a null pointer as the third argument to getmsg.
When we verify that the amount of control information returned is at least the size of a long integer, we must be careful to cast the sizeof value to an integer.
The sizeof operator returns an unsigned integer value, but it is possible for the returned len field to be –1
But since the less-than comparison is comparing a signed value on the left to an unsigned value on the right, the compiler casts the signed value to an unsigned value.
If the reply is T_BIND_ACK, the bind was successful and we return.
The actual address that was bound to the endpoint is returned in the addr member of our bind_ack structure, which we ignore.
If the reply is T_ERROR_ACK, we verify that the entire message was received and then print the three return values in the structure.
In this simple program, we terminate when an error occurs; we do not return to the caller.
The error TACCES has the value 3 on this system.
If we change the port to a value greater than.
The error TADDRBUSY has the value 23 on this system.
Since we do not know what type of message we will receive, a union named T_primitives is defined as the union of all the possible requests and replies, and we allocate one of these that we use as the input buffer for the control information when we call getmsg.
The successful T_OK_ACK message that was just received only tells us that the connection establishment was started.
We must now wait for a T_CONN_CON message to tell us that the other.
We can see the different errors that are returned by the provider.
We first specify the IP address of a host that is not running the daytime server.
Next, we specify an IP address that is not connected to the Internet.
But if we run our program again, specifying the same IP address, we get a different error.
The difference in the last two results is that the first time, no ICMP "host unreachable" errors were returned, while the next time, this error was returned.
This time, we call getmsg to read both control information and data.
The strbuf structure for the data points to the caller's buffer.
The data was copied into the caller's buffer by getmsg, and we just return the length of this data as the return value of the function.
If this message is returned, we ignore the MORE_flag member (it will never be set for a stream protocol such as TCP) and just return the length of the data that was copied into the caller's buffer by getmsg.
A T_ORDREL_IND message is returned if all the data has been consumed and the next item is a FIN.
We just return 0, indicating to the caller that the EOF has been encountered on the connection.
A T_DISCON_IND message is returned if a disconnect has been received.
The application sends messages down a stream to the provider (requests) and the provider sends messages up the stream (replies)
Some exchanges follow a simple request-reply scenario (binding a local address), while others may take a while (establishing a connection), allowing us to do something while we wait for the reply.
Our choice of writing a TCP client using TPI was done for simplicity; writing a TCP server and handling connections are much harder.
We can compare the number of system calls required for the network operations that we have seen in this chapter when using TPI versus a kernel that implements sockets within the kernel.
Four new functions are provided to access the STREAMS subsystem: getmsg, getpmsg, putmsg, and putpmsg, plus the existing ioctl function is heavily used by the STREAMS subsystem also.
It is used by both sockets and XTI, as shown in Figure 31.3
We developed a version of our daytime client using TPI directly as an example to show the message-based interface that TPI uses.
What happens if our orderly release request is lost by the STREAMS subsystem when the stream is closed?
Any desired reliability, ordering, and duplicate suppression must be added by the upper layers.
In the case of a TCP or SCTP application, this is performed by the transport layer.
In the case of a UDP application, this must be done by the application since UDP is unreliable; we show an example of this in Section 22.5
One of the most important functions of the IP layer is routing.
This has been the version of IP in use since the early 1980s.
The header length field is the length of the entire IP header, including any options, in whole 32-bit words.
This field is required because some datalinks pad the frame to some minimum length (e.g., Ethernet) and it is possible for the size of a valid IP datagram to be less than the datalink minimum.
The value must be unique for the packet's source, destination, and protocol, for the length of time that the datagram could be in transit.
If there is no chance that the packet will be fragmented, for instance, the DF bit is set, there is no need to set this field.
The DF (don't fragment) bit, the MF (more fragments) bit, and the 13-bit fragment offset field are also used with fragmentation and reassembly.
The DF bit is also used with path MTU discovery (Section 2.11)
This limits the lifetime of any IP datagram to 255 hops.
The 8-bit protocol field specifies the next layer protocol contained in the IP datagram.
These values are specified in the IANA's "Protocol Numbers" registry [IANA]
The 16-bit header checksum is calculated over just the IP header (including any options)
The 20-bit flow label field can be chosen by the application or kernel for a given socket.
A flow is a sequence of packets from a particular source to a particular destination for which the source desires special handling by intervening routers.
For a given flow, once the flow label is chosen by the source, it does not change.
The flow label does not change while flowing through the network.
The interface for the flow label is yet to be completely defined.
That is why the field is called the "next header" and not the "protocol."
Early specifications of IPv4 had routers decrement the TTL by either one or the number of seconds that the router held the datagram, whichever was greater.
Hence the name "time-to-live." In reality, however, the field was always decremented by one.
Another change is simplifying the IPv6 header as follows, to facilitate faster processing as a datagram traverses the network:
There are no fragmentation fields in the IPv6 header because there is a separate fragmentation header for this purpose.
This design decision was made because fragmentation is the exception, and exceptions should not slow down normal processing.
By omitting the checksum from the header, routers that forward the.
Again, speed of forwarding by routers is the key point.
The case of sending to all systems on a subnet is handled with the all-nodes multicast group.
Fragmentation is performed only by the originating host with IPv6
Section 22.9 describes socket options to control path MTU discovery behavior.
These are known as classless addresses, so called because the mask is explicitly specified instead of being implied by the address class.
IPv4 network addresses are normally written as a dotted-decimal number, followed by a slash, followed by the prefix length.
Discontiguous subnet masks were never ruled out by any RFC, but they are confusing and cannot be represented in prefix notation.
BGP4, the Internet interdomain routing protocol, cannot represent discontiguous masks.
IPv6 also requires that all address masks be contiguous starting at the leftmost bit.
All routes in CIDR must be accompanied by a mask or a prefix length.
The class of the address no longer implies the mask.
The boundary between the network ID and the subnet ID is fixed by the prefix length of the assigned network address.
This prefix length is normally assigned by the organization's Internet service provider (ISP)
But, the boundary between the subnet ID and the host ID is chosen by the site.
All the hosts on a given subnet share a common subnet mask, and this mask specifies the boundary between the subnet ID and the host ID.
This division results in the subnets shown in Figure A.5
Most systems today support these two forms of subnet IDs.
The highest host ID (31, in this case) is reserved for the broadcast address.
In general, network programs need not care about specific subnet or host IDs and should treat IP addresses as opaque values.
By convention, the address 127.0.0.1 is assigned to the loopback interface.
Anything sent to this IP address loops around and becomes IP input without ever leaving the machine.
We often use this address when testing a client and server on the same host.
In an IPv4 packet, it is only permitted to appear as the source address in packets sent by a node that is bootstrapping before the node learns its IP address.
In the sockets API, this address is called the wildcard address and is normally known by the name INADDR_ANY.
Also, specifying it in the sockets API, for example, to bind for a listening TCP socket, indicates that the socket will accept client connections destined to any of the node's IPv4 addresses.
These addresses must never appear on the Internet; they are reserved for use in private networks.
Many small sites use these private addresses and NAT to a single public IP address visible to the Internet.
Traditionally, the definition of a multihomed host has been a host with multiple interfaces: two Ethernets, for example, or an Ethernet and a point-to-point link.
When counting interfaces to determine if a host is multihomed, the loopback interface does not count.
A router, by definition, is multihomed since it forwards packets that arrive on one interface out another interface.
But, a multihomed host is not a router unless it forwards packets.
Indeed, a multihomed host must not assume it is a router just because the host has multiple interfaces; it must not act as a router unless it has been configured to do so (typically by the administrator enabling a configuration option)
A host with multiple interfaces is multihomed and each interface must in general have its own IP address.
Newer hosts have the capability of assigning multiple IP addresses to a given physical interface.
Each additional IP address, after the first (primary), is called an alias or logical interface.
Often, aliased IP addresses share the same subnet address as the primary address but have different host IDs.
But, it is also possible for aliases to have a completely different network address or subnet addresses from the primary.
We show an example of aliased addresses in Section 17.6
Hence, the definition of a multihomed host is one with multiple interfaces visible to the IP layer, regardless of whether those interfaces are physical or logical.
It is common to give a high-usage server multiple connections to the same Ethernet switch, and to aggregate these connections to appear as one higher bandwidth interface.
Although such a system has multiple physical interfaces, it is not considered to be multihomed since only one logical interface is visible to IP.
A network that has multiple connections to the Internet is also called multihomed.
For example, some sites have two connections to the Internet instead of one, providing a backup capability.
The SCTP transport protocol can potentially take advantage of these multiple connections by communicating that the site is multihomed to its peer.
Figure A.7 shows the different values of the high-order bits and what type of address these bits imply.
These addresses will be used where IPv4 unicast addresses are used today.
Figure A.8 illustrates the format of a global unicast address.
The interface ID must be constructed in modified EUI-64 format.
This identifier should be automatically assigned for an interface based on its hardware MAC address when possible.
These addresses are considered temporary, and nodes using these addresses will have to renumber when aggregatable global unicast addresses are assigned.
These assignments are meant to reflect how IPv6 addresses would be assigned in realworld environments.
The subnet ID and interface ID are used as above for subnet and node identification.
These addresses are not stored in any DNS data files; they are created when needed by a resolver.
When writing an IPv6 address, a consecutive string of zeros can be abbreviated with two colons.
Also, the embedded IPv4 address is written using dotted-decimal notation.
However, deployment concerns have reduced the usage of this feature.
In the sockets API, this address is called the wildcard address.
Specifying it, for example, to bind for a listening TCP socket, indicates that the socket will accept client connections destined to any of the node's addresses.
A link-local address is used on a single link when it is known that the datagram will not be forwarded beyond the local network.
Example uses are automatic address configuration at bootstrap time and neighbor discovery (similar to IPv4's ARP)
An IPv6 router must not forward a datagram with a link-local source or destination address to another link.
As of this writing, the IETF IPv6 working group has decided to deprecate site-local addresses in their current form.
The forthcoming replacement may or may not finally use the same address range as was originally defined for site-local addresses (fec0/10)
Site-local addresses were meant to be used for addressing within a site without the need for a global prefix.
Figure A.13 shows the originally defined format of these addresses.
An IPv6 router must not forward a datagram with a site-local source or destination address outside of that site.
The ping and traceroute applications (Chapter 28), for example, both use ICMP.
From a network programming perspective, we need to understand which ICMP messages can be returned to an application, what causes an error, and how an error is returned to the application.
The third column indicates the errno value returned by those messages that trigger an error to be returned to the application.
When using TCP, the error is noted but is not immediately returned.
If TCP later gives up on the connection due to a timeout, any ICMP error indication is then returned.
When using UDP, the next send or receive operation receives the error, but only when using a connected socket, as described in Section 8.9
For example, TCP sends an RST message so it does not need the "port unreachable" message.
The notation "user process" means that the kernel does not process the message and it is up to a user process with a raw socket to handle the message.
We must also note that different implementations may handle certain messages differently.
For example, although Unix systems normally handle router solicitations and router advertisements in a user process, other implementations might handle these messages in the kernel.
These RFC 1323 changes, for example, are slowly appearing in host implementations of TCP, and when a new TCP connection is established, each end can determine if the other end supports the new feature.
If both hosts support the feature, it can be used.
But, what if people want to start using the new features without having to wait for all the systems to be upgraded? To do this, a virtual network is established on top of the existing IPv4 Internet using tunnels.
If two or more hosts on a LAN support multicasting, multicast applications can be run on all these hosts and communicate with each other.
To connect this LAN to some other LAN that also has multicast-capable hosts, a tunnel is configured between one host on each of the LANs, as shown in Figure B.1
We show this as a UDP datagram, since most multicast applications use UDP.
The datagram is received by all the multicast-capable hosts on the LAN, including MR2
We note that MR2 is also functioning as a multicast router, running the mrouted program, which performs multicast routing.
This unicast address is configured by the administrator of MR2 and is read by the mrouted program when it starts up.
The datagram is sent to the next-hop router, UR3, which we explicitly denote as a unicast router.
All the multicast-capable hosts on the lower LAN receive the multicast datagram.8
The result is that the multicast datagram sent on the top LAN also gets transmitted as a multicast datagram on the lower LAN.
This occurs even though the two routers that we show attached to these two LANs, and all the Internet routers between these two routers, are not multicastcapable.
In this example, we show the multicast routing function being performed by the mrouted program running on one host on each LAN.
But around 1996, multicast routing functionality started appearing in the routers from most major router vendors.
In fact, the MBone is virtually nonexistent at this point, having been replaced with native multicast in this manner.
There are probably still tunnels present in the Internet's multicast infrastructure, but they are commonly between native multicast routers inside a service provider's network and are invisible to the end-user.
We cover the 6bone here because the examples still demonstrate configured tunnels.
We will expand the example to include dynamic tunnels in Section B.4
The configured tunnel counts as an interface, even though it is a virtual interface and not a physical interface.
This allows for simpler configuration and a central location to enforce security policy.
It also permits colocation of 6to4 functionality with the common NAT/firewall function that is often at the edge of a network (e.g., a small NAT/firewall device at the customer's end of a DSL or cable-modem connection)
This leaves two bytes for the subnet ID before the 64-bit interface ID.
Such routers can be local to a site, regional, or global, depending on the scope of their route advertisements.
Debugging Techniques This appendix contains some hints and techniques for debugging network applications.
No single technique is the answer for everyone; instead, there are various tools that we should be familiar with, and then use whatever works in our environment.
Working at this level, we need to differentiate between a system call and a function.
The former is an entry point into the kernel, and that is what we are able to trace with the tools we will look at in this section.
For example, on a Berkeley-derived kernel, socket is a system call even though it appears to be a normal C function to the application programmer.
But under SVR4, we will see shortly that it is a library function in the sockets library that issues calls to putmsg and getmsg, these latter two being actual system calls.
In this section, we will examine the system calls involved in running our daytime client.
We start with FreeBSD, a Berkeley-derived kernel in which all the socket functions are system calls.
The ktrace program is provided by FreeBSD to run a program and trace the system calls that are executed.
This writes the trace information to a file (whose default name is ktrace.out), which we print with kdump.
We then execute kdump to output the trace information to standard output.
We see the calls to socket and connect, followed by the call to read that returns 26 bytes.
Our client writes these bytes to standard output and the next call to read returns 0 (EOF)
To provide additional compatibility, versions starting with Solaris 2.6 changed the implementation technique and implemented sockets using a sockfs filesystem.
This provides kernel sockets, as we can verify using truss on our sockets client.
After the normal library linking, the first system call we see is to so_socket, a system call invoked by our call to socket.
The first three arguments to so_socket are our three arguments to socket.
We see that connect is a system call, and truss, when invoked with the -v connect flag, prints the contents of the socket address structure pointed to by the second argument (the IP address and port number)
The only system calls that we have replaced with ellipses are a few dealing with standard input and standard output.
We have used the daytime service many times for testing our clients.
The discard service is a convenient port to which we can send data.
The echo service is similar to the echo server we have used throughout this text.
Nevertheless, you can hopefully use these services within your own network.
The handy thing about the program is that it generates so many different scenarios, saving us from having to write special test programs.
We do not show the source code for the program in this text (it is over 2,000 lines of C), but the source code is freely available (see the Preface)
The program operates in one of four modes, and each mode can use either TCP or UDP:
In the client mode, everything read from standard input is written to the network, and everything received from the network is written to standard output.
The server's IP address and port must be specified, and in the case of TCP, an active open is performed.
Standard input, standard output server—This mode is similar to the previous mode, except the program binds a well-known port to its socket, and in the case of TCP, performs a passive open.
The program performs a fixed number of writes to a network of some specified size.
The program performs a fixed number of reads from a network.
These four operating modes correspond to the following four commands:
In the two server modes, the wildcard address is bound, unless the optional hostname is specified.
About 40 command-line options can also be specified, and these drive the optional features of the program.
We will not detail these options here, but many of the socket options described in Chapter 7 can be set.
Executing the program without any arguments prints a summary of the options.
C.4 Small Test Programs Another useful debugging technique, one that the authors use all the time, is writing small test programs to see how one specific feature works in a carefully constructed test case.
It helps when writing small test programs to have a set of library wrapper functions and some simple error functions, such as the ones we have used throughout this text.
This reduces the amount of code that we have to write, but still provides the required testing for errors.
C.5 tcpdump Program An invaluable tool when dealing with network programming is a tool like tcpdump.
This program reads packets from a network and prints lots of information about the packets.
It also has the capability of printing only those packets that match some criteria that we specify.
Appendix A of TCPv1 details the operation of this program in more detail.
This program is available from http://www.tcpdump.org/ and works under many different flavors of Unix.
It was originally written by Van Jacobson, Craig Leres, and Steven McCanne at LBL, and is now maintained by a team at tcpdump.org.
Some vendors supply a program of their own with similar functionality.
The advantage of tcpdump is that it works under so many versions of Unix, and using a single tool in a heterogeneous environment, instead of a.
C.6 netstat Program We have used the netstat program many times throughout the text.
We showed this in Section 5.6, when we followed the status of our endpoint as we started our client and server.
It shows the multicast groups that a host belongs to on each interface.
The -ia flags are the normal way to show this, or the -g flag under Solaris 2.x.
We showed this in Section 8.13, when looking at the lack of flow control with UDP.
It displays the routing table with the -r option and the interface information with the -i option.
We showed this in Section 1.9, where we used netstat to discover the topology of our network.
There are other uses of netstat and most vendors have added their own features.
C.7 lsof Program The name lsof stands for "list open files." Like tcpdump , it is a publicly available tool that is handy for debugging and has been ported to many versions of Unix.
One common use for lsof with networking is to find which process has a socket open on a specified IP address or port.
For example, to find out which process provides the daytime server, we execute the following:
One common use for this program is when we start a server that binds its well-known port and get the error that the address is already in use.
We then use lsof to find the process that is using the port.
Since lsof reports on open files, it cannot report on network endpoints that are not associated with an open file: TCP endpoints in the TIME_WAIT state.
Some vendors supply their own utility that does similar things.
The advantage in lsof is that it works under so many versions of Unix, and using a single tool in a heterogeneous environment, instead of a different tool for each environment, is a big advantage.
This header includes all the standard system headers that most network programs need, along with some general system headers.
It also defines constants such as MAXLINE , ANSI C function prototypes for the functions we define in the text (e.g., readline ), and all the wrapper functions we use.
Define bzero() as a macro if it's not in standard C library.
This is the standard value, but there's no guarantee it is -1
D.2 config.h Header The GNU autoconf tool was used to aid in the portability of all the source code in this text.
This tool generates a shell script named configure that you must run after downloading the software onto your system.
This script determines the features provided by your Unix system: Do socket address structures have a length field? Is multicasting supported? Are datalink socket address structures supported? and so on, generating a header named config.h.
This header is the first header included by our unp.h header in the previous section.
The lines that are commented out and contain #undef are features that the system does not provide.
D.3 Standard Error Functions We define our own set of error functions that are used throughout the text to handle error conditions.
The reason for using our own error functions is to let us write our error handling with a single line of C code, as in.
Our error functions use the variable-length argument list facility from ANSI C.
Figure D.3 lists the differences between the various error functions.
If the global integer daemon_proc is nonzero, the message is passed to syslog with the indicated level; otherwise, the error is output to standard error.
To find more information on this error, we first use grep to search for the string Protocol not supported in the <sys/errno.h> header.
Most man pages give additional, albeit terse, information toward the end under a heading of the form "Errors."
The results vary, depending on the client host and server host.
But, one combination of client and server may produce two packets, and another combination 26 packets.
Our discussion of the Nagle algorithm in Section 7.9 explains one reason for this.
The purpose of this example is to reiterate that different TCPs do different things with the data and our application must be prepared to read the data as a stream of bytes until the end of the data stream is encountered.
To start, fetch the current RFC index, normally the file rfc-index.txt, also available in an HTML version at http://www.rfc-editor.org/rfcindex.html.
Whenever looking for information that might be covered by an RFC, the RFC index should be searched.
The host on the Ethernet can send packets with up to 4,096 bytes of data, but it will not exceed the MTU of the outgoing interface (the Ethernet) to avoid fragmentation.
Only a cumulative acknowledgment says that the data up to and including the sequence number in the cumulative acknowledgment message was received.
When freeing data from the send buffer based on a selective acknowledgment, the system may only free the exact data that was acknowledged, and not any before or after the selective acknowledgment.
For a called function to modify a value passed by the caller requires that the caller pass a pointer to the value to be modified.
Notice that the value-result argument for getsockname (len) must be initialized before the call to the size of the variable pointed to by the second argument.
The most common programming error with value-result arguments is to forget this initialization.
Later, when the parent calls close, the reference count is decremented to 0 and the FIN is sent.
In this example, it will read the first three characters and then terminate the string with a null byte.
One byte is sent to the server, but the server blocks in its call to readline, waiting for a newline character.
This is called a deadlock: Both processes are blocked waiting for something that will never arrive from the other one.
The problem here is that fgets signifies the end of the data that it returns with a null byte, so the data that it reads cannot contain any null bytes.
Our client adds only a newline, which is actually a linefeed character.
Nevertheless, we can use the Telnet client to communicate with our server as our server echoes back every character, including the CR that precedes each newline.
When the client sends the data to the server, after we kill the server child (the "another line"), the server TCP responds with an RST.
The RST aborts the connection and also prevents the server end of the connection (the end that did the active close) from passing through the TIME_WAIT state.
What we send in Step 3 is a data segment destined for an ESTABLISHED TCP connection.
Our server with the listening socket never sees this data segment, and the server TCP still responds to it with an RST.
The initial sleep of two seconds is to let the daytime server send its reply and close its end of the connection.
Our first write sends a data segment to the server, which responds with an RST (since the daytime server has completely closed its socket)
Note that our TCP allows us to write to a socket that has received a FIN.
The second sleep lets the server's RST be received, and our second write generates SIGPIPE.
Since our signal handler returns, write returns an error of EPIPE.
That is, the server host will accept an incoming IP datagram (which contains a TCP segment in this case) arriving on the leftmost datalink, even though the destination IP address is the address of the rightmost datalink.
After the connection is established, if we run netstat on the server, we see that the local IP address is the destination IP address from the client's SYN, not the IP address of the datalink on which the SYN arrived (as we mentioned in Section 4.4)
This big-endian value is sent across the socket to the client where it is interpreted as the.
The second 64-bit value remains in the server's socket receive buffer.
The primary IP address of the outgoing interface is used as the source IP address, assuming the socket has not already bound a local IP address.
Read operations on a blocking socket will always return a short count if some data is available, but write operations on a blocking socket will block until all the data can be accepted by the kernel.
Therefore, when using select to test for writability, we must set the socket to nonblocking to avoid blocking.
But this does not break the client; it just makes it less efficient.
That is, if select returns with both descriptors readable, the first if is true, causing a readline from the socket followed by an fputs to standard output.
The next if is skipped (because of the else we prepended), but select is then called again and immediately finds standard input readable and returns immediately.
The key concept here is that what clears the condition of "standard input being readable" is not select returning, but reading from the descriptor.
Notice that we handle this condition in Figure 6.26, although even that code is inadequate.
Consider what happens if connectivity is lost between the client and server and one of the server's responses eventually times out.
In general, a server should not abort for errors like these.
It should log the error, close the socket, and continue servicing other clients.
But if the server was a child handling just one client, then having that one child abort would not affect the parent (which we assume handles all new connections and spawns the children), or any of the other children that are servicing other clients.
We have removed the printing of the data string returned by the server as that value is not needed.
Figure E.5 Print socket receive buffer size and MSS before and after connection establishment.
On a local Ethernet, for example, the value after connect could be 1,460
After a connect to a server on a remote network, however, the MSS may be similar to the default, unless your system supports path MTU discovery.
If possible, run a tool like tcpdump (Section C.5) while the program is running to see the actual MSS option on the SYN segment from the peer.
With regard to the socket receive buffer size, many implementations round this value up after the connection is established to a multiple of the MSS.
Another way to see the socket receive buffer size after the connection is established is to watch the packets using a tool like tcpdump and look at TCP's advertised window.
Allocate a linger structure named ling and initialize it as follows:
This should cause the client TCP to terminate the connection with an RST instead of the normal four-segment exchange.
The server child's call to readline returns an error of ECONNRESET and the message printed is as follows:
The client socket should not go through the TIME_WAIT state, even though the client did the active close.
The first client calls setsockopt , bind , and connect.
But between the first client's calls to bind and connect , if the second client calls bind , EADDRINUSE is returned.
But as soon as the first client connects to the peer, the second client's bind will work, since the first client's socket is then connected.
The only way to handle this is for the second client to try calling bind multiple times if EADDRINUSE is returned, and not give up the first time the error is returned.
We run the program on a host with multicast support (MacOS X 10.2.6)
On this system, we do not need to specify SO_REUSEADDR for the first bind, only for the second.
We first try SO_REUSEADDR for both servers, but this does not work.
Next we try SO_REUSEPORT , but only for the second server, not for the first.
This does not work since a completely duplicate binding requires the option for all sockets that share the binding.
Finally we specify SO_REUSEPORT for both servers, and this works.
This does nothing because ping uses an ICMP socket and the SO_DEBUG socket option affects only TCP sockets.
The description for the SO_DEBUG socket option has always been something generic such as "this option enables debugging in the respective protocol layer," but the only protocol layer to implement the option has been TCP.
Setting the TCP_NODELAY socket option causes the data from the second write to be sent immediately, even though the connection has a small packet outstanding.
The total time in this example is just over 150 ms.
The advantage to this solution is reducing the number of packets, as we show in Figure E.8
The keep-alive option has no effect on a listening socket so the parent is not affected should the client host crash.
The child's read will return an error of ETIMEDOUT , sometime around two hours after the last data exchange across the connection.
The client in Figure 5.5 spends most of its time blocked in the call to fgets , which in turn is blocked in some type of read operation on standard input within the standard I/O library.
When the keep-alive timer expires around two hours after the last data exchange across the connection, and all the keep-alive probes fail to elicit a response from the server, the socket's pending error is set to ETIMEDOUT.
But the client is blocked in the call to fgets on standard input and will not see this error until it performs a read or write on the socket.
This client spends most of its time blocked in the call to select , which will return the socket as readable as soon as the pending error is set to ETIMEDOUT (as we described in the previous solution)
There is a very low probability that the two systems will have timers that are exactly synchronized; hence, one end's keep-alive timer will expire shortly before the other's.
The first one to expire sends the keep-alive probe, causing the other end to ACK this probe.
But the receipt of the keep-alive probe causes the keep-alive timer on the host with the (slightly) slower clock to be reset for two hours in the future.
The original sockets API did not have a listen function.
Instead, the fourth argument to socket contained socket options, and SO_ACCEPTCON was used to specify a listening socket.
A recvfrom on a datagram socket never returns more than one datagram, regardless of how much the application asks for.
We will see in Chapter 15 that this is acceptable with Unix domain socket address structures, but the correct way to code the function is to use the actual length returned by recvfrom as the length for sendto.
We reduce the number of packets sent from the normal one per second just to reduce the output.
Note that not all ping clients print received ICMP errors, even with the -v flag.
Most implementations do not preallocate memory for socket send buffers or socket receive buffers.
The local IP address is the Internet-side interface in Figure 1.16, but the datagram must go out the other interface to get to the destination.
Watching the network with tcpdump shows that the source IP address is the one that was bound by the client, not the outgoing interface address.
Putting a printf in the server should cause the server to lose more datagrams.
If you forget to set the send buffer size, Berkeley-derived kernels return an error of EMSGSIZE from sendto, since the size of the socket send buffer is normally less than required for a maximum-sized UDP datagram (be sure to do Exercise 7.1)
But if we set the client's socket buffer sizes as shown in Figure E.9 and run the client program, nothing is returned by the server.
We can verify that the client's datagram is sent to the server by running tcpdump, but if we put a printf in the server, its call to recvfrom does not return the datagram.
The problem is that the server's UDP socket receive buffer is smaller than the datagram we are sending, so the datagram is discarded and not delivered to the socket.
On a FreeBSD system, we can verify this by running netstat -s and looking at the "dropped due to full socket buffers" counter before and after our big datagram is received.
The final solution is to modify the server, setting its socket send and receive buffer sizes.
Therefore, you may encounter hosts that will not receive the maximum-sized datagrams sent in this exercise.
An example application that might use this function is a traditional UDP-like server that responds to requests such as small transactions, but occasionally is requested to do a long-term audit.
In most cases, you only need to send one or two small messages and no more; but when a audit request arrives, a long-term conversation is invoked, sending audit information.
In this situation, you would peel off the audit into its own thread or process to do the audit.
In summary, any application that has mainly small requests but on occasion needs to have a long-term conversation can take advantage of sctp_peeloff.
This is because SCTP does not support the half-closed state, so when the client calls close, the shutdown sequence will flush any pending data the server had queued for the client and complete the shutdown, closing the association.
For the one-to-many style, an application can send data to a peer to set up an association.
This means that when the COOKIE is sent, the DATA is available to send to the peer.
This would occur if each side was using the one-to-many style and each side did a send to implicitly set up the association.
In particular, when addresses that an application has bound contain both private and public IP addresses, only the public addresses may be shared with a peer endpoint.
Another example is found in IPv6, where link-local addresses cannot necessarily be shared with a peer.
A way to fix this is to check the error return codes, and if an error occurs on sending, the client should NOT do the receive, but instead should report an error.
If the sctp_recvmsg function returns an error, no message will arrive and the server will still attempt to send a message, possibly setting up an association.
To avoid this, the error code should be checked, and depending on the error, you may wish to report the error and close the socket, letting the server also then receive an error; or, if the error is transient, you could retry the sctp_recvmsg call.
A method that can be used by the client to detect this is to enable association events.
This will allow the client application to receive a message when the server exits, telling the client that the association is now gone.
This would allow the client to then take a recovery action such as contacting a different server.
An alternative method the client could use is to set up a timer and abort after some time period.
A better way would be to get or set the SCTP_MAXSEG socket option to determine the size that will fit in one chunk.
So as long as we send a size that forces SCTP to send immediately, no harm will occur.
However, choosing a smaller size for out_sz would skew the results, holding some transmissions awaiting SACKs from the remote endpoint.
So if a smaller size is to be used, turning off the Nagle algorithm (i.e., turning on the SCTP_NODELAY socket option) would be a good idea.
This is because changing the number of streams only affects new associations, not existing ones.
To use ancillary data to change the setup of an association, you need to use the sendmsg call to provide the data before the association is set up.
This program works fine for a host with a single IP address.
If we run the program in Figure 11.3 for a host with four IP addresses, we get the following:
But if we run the program in Figure E.10 for the same host, only the first IP address is output as follows:
The problem is that the two functions, gethostbyname and gethostbyaddr , share the same hostent structure, as we show at the beginning of Section 11.18
When our new program calls gethostbyaddr , it reuses this structure, along with the storage that the structure points to (i.e., the h_addr_list array of pointers), wiping out the remaining three IP addresses returned by gethostbyname.
The chargen server sends data to the client until the client closes the connection (i.e., until you abort the client)
This is a feature of some resolvers, but you cannot rely on it in a portable program because POSIX leaves the behavior unspecified.
The order of the tests on the hostname string is important.
We call inet_pton first, as it is a fast, in-memory test for whether or not the string is a valid dotted-decimal IP address.
Only if this fails do we call gethostbyname , which typically involves some network resources and some time.
If the string is a valid dotted-decimal IP address, we make our own array of pointers (addrs ) to the single IP address, allowing the loop using pptr to remain the same.
Since the address has already been converted to binary in the socket address structure, we change the call to memcpy in Figure 11.4 to call memmove instead, because when a dotted-decimal IP address is entered, the source and destination fields are the same in this call.
Figure E.11 Allow dotted-decimal IP address or hostname, port number, or service name.
We use the h_addrtype value returned by gethostbyname to determine the type of address.
First, we must handle all the differences, looking at h_addrtype and then setting sa and salen appropriately.
A better solution is to have a library function that not only looks up the hostname and service name, but also fills in the entire socket address structure (e.g., getaddrinfo in Section 11.6 )
Second, this program compiles only on hosts that support IPv6
We return to the concept of protocol independence in Chapter 11 and see better ways to accomplish it.
Allocate a big buffer (larger than any socket address structure) and call getsockname.
The third argument is a value-result argument that returns the actual size of the protocol's addresses.
We first allocate arrays to hold the hostname and service name as follows:
After accept returns, we call getnameinfo instead of sock_ntop as follows:
The first problem is that the second server cannot bind the same port as the first server because the SO_REUSEADDR socket option is not set.
Any client pause before this is the time taken by the resolver to look up the hostname.
Any pause between these two lines of output is the time taken by connect to establish the connection.
The other two TCP servers, time and daytime, do not require a fork because their service is trivial to implement (get the current time and date, format it, write it, and close the connection), so these two are handled directly by inetd.
All five UDP services are handled without a fork because each generates at most a single datagram in response to the client datagram that triggers the service.
This is echoed and sends another datagram to the chargen server.
One solution, implemented in FreeBSD, is to refuse datagrams to any of the internal servers if the source port of the incoming datagram belongs to any of the internal servers.
Another solution is to disable these internal services, either through inetd on each host or at an organization's router to the Internet.
The reason inetd does not do this for a UDP socket is because the recvfrom to read the datagram is performed by the actual server that is execed, not by inetd itself.
Hence, exit is called, plus the standard I/O cleanup routine is called.
The server's listening socket is not affected, but no clients will be able to connect after the unlink.
The client cannot connect to the server even if the pathname still exists, because for the connect to succeed, a Unix domain socket must be currently open and bound to that pathname (Section 15.4 )
When the server prints the client's protocol address by calling sock_ntop , the output is "datagram from (no pathname bound)" because no pathname is bound to the client's socket by default.
This puts the protocol dependency in the library function where it belongs, not in our application.
This is just to confirm (again) that TCP is a byte stream with no inherent record markers.
To use the Unix domain protocols, we start the client and server with the two command-line arguments /local (or /unix ) and /tmp/daytime (or any other temporary pathname you wish to use)
Nothing changes: 26 bytes are returned by read each time the client runs.
What is happening here is that Berkeley-derived implementations support the MSG_EOR flag by default.
This is undocumented, however, and should not be used in production code.
We use it here as an example of the difference between a byte stream and a record-oriented protocol.
From an implementation perspective, each output operation goes into a memory buffer (mbuf) and the MSG_EOR flag is retained by the kernel with the mbuf as the mbuf goes from the sending socket to the receiving socket's receive buffer.
Figure E.13 Determine actual number of queued connections for different backlog values.
The next write after this will send SIGPIPE to the parent as we discussed in Section 5.12
The child will try to send the signal to the init process, but will not have adequate permission.
But if there is a chance that this client could run with superuser privileges, allowing it to send this signal to init, then the return value of getppid should be tested before sending the signal.
But select will return immediately because with the connection established, the socket is writable.
This test and goto are to avoid the unnecessary call to select.
But, disabling the option prevents replies from being sent to the sender.
The sending host, however, is normally the first reply since the datagrams to and from it loop back internally and do not appear on the actual network.
It is called again and returns readability on the pipe.
To prevent accidental reception of multicast datagrams that a server is not expecting, the kernel does not deliver multicast groups to a socket that has never performed any multicast operations (e.g., joining a group)
What is happening here is that the destination address of the UDP datagram is 224.0.0.1, the all-hosts group that all multicast-capable nodes must join.
The UDP datagram is sent as a multicast Ethernet frame and all the multicast-capable nodes receive the datagram since they all belong to the group.
However, the kernel drops the received datagram since the process bound to the daytime port has not set any multicast options.
Figure E.14 UDP client main function that binds a multicast address.
Both systems on the right-hand Ethernet in Figure 1.16 respond.
To prevent certain denial-of-service attacks, some systems do not respond to broadcast or multicast pings by default.
To get freebsd to respond, we had to configure it with.
If we call it twice as arguments in a call to printf , the second call overwrites the result of the first call.
Yes, if the reply contains 0 bytes of user data (i.e., just an hdr structure)
Since select does not modify the timeval structure that specifies its time limit, you need to note the time when the first packet is sent (this is already returned in units of milliseconds by rtt_ts )
If select returns with the socket being readable, note the current time, and if recvmsg is called again, calculate the new timeout for select.
The common technique is to create one socket per interface address, as we did in Section 22.6 , and send the reply from the same socket on which the request arrived.
But in the second example (the two function calls), first the a is sent with an urgent pointer that points just beyond it, and this is followed by another TCP segment containing the b with a different urgent pointer that points just beyond it.
The problem is that nqueue is decremented before the array entry dg[iget] is processed, allowing the signal handler to read a new datagram into this array element.
In the threaded server, however, there are 101 descriptors in the single process.
Each thread (including the main thread) is handling one descriptor.
The main thread checks this variable, and if 0, prints the error message.
Since only one thread sets the variable, there is no need for any synchronization.
This bug was probably fixed when ANSI C prototypes were first used.
The payload length field is available as either an argument to one of the output functions or as the return value from one of the input functions.
But, if a jumbo payload option is required, that actual option itself is not available to an application.
The fragment header is also not available to an application.
We do not want this to happen, as that stops the daemon from handling any more data on any of its sockets.
The easiest solution is for the daemon to set its end of the Unix domain connection to the client to nonblocking.
The daemon must then call write instead of the wrapper function Write and just ignore an error of EWOULDBLOCK.
The SO_BROADCAST socket option needs to be specified only for UDP sockets.
Therefore, the kernel chooses the outgoing interface, probably by searching the routing table for 224.0.0.1
While the flag may seem superfluous, there is a chance that the signal can be delivered after the signal handler is established, but before the call to sigsetjmp.
Even if the program doesn't cause the signal to be generated, signals can be generated in other ways, such as with the kill command.
With a datagram socket, the parent does not receive an EOF on its end of the stream pipe when a child terminates prematurely, but the parent could use SIGCHLD for this purpose.
One difference in this scenario, where SIGCHLD can be used versus our icmpd daemon in Section 28.7, is that in the latter, there was no parent/child relationship between the client and server so the EOF on the stream pipe was the only way for the server to detect the disappearance of a client.
Bibliography All RFCs are available at no charge through electronic mail, anonymous FTP, or the World Wide Web.
Items marked "Internet Draft" are works in progress of the IETF.
The appropriate version of the draft may change after this book is published, or the draft may be published as an RFC.
They are available at no charge via the Internet, similar to the RFCs.
We include the filename portion of the URL for each Internet Draft, since the filename contains the version number.
Whenever an electronic copy was found of a paper or report referenced in this bibliography, its URL is included.
Be aware that these URLs can change over time, and readers are encouraged to check the Errata for this text on the book's home page for any changes (http://www.unpbook.com/) A terrific online database of papers can be found at http://citeseer.nj.nec.com/cs.
Entering the title of a paper or report will not only find other papers that refer to the one entered, but will also point to known online versions.
This is a revision of [Conta and Deering 1998] and is expected to eventually replace it.
Ordering information on IEEE standards and draft standards is available at http://www.ieee.org.
A classic paper describing the slow start and congestion avoidance algorithms for TCP.
Describes the window scale option, the timestamp option, and the PAWS algorithm, along with the reasons why these modifications were needed.
As of this writing, this RFC is being updated by the IETF IPsec Working Group (see [Kent 2003a])
As of this writing, this RFC is being updated by the IETF IPsec Working Group (see [Kent 2003b])
This manual also has appendices describing the use of XTI with Net-BIOS, the OSI protocols, SNA, and the Netware IPX and SPX protocols.
Three appendices cover the use of both sockets and XTI with ATM.
The database referred to in this RFC is [IANA 2003]
This RFC is the last in the series of "Assigned Numbers" RFCs.
Since the information changed so often, it was decided to simply keep the directory online.
A newer version of this specification is available online from The Open Group at http://www.rdg.opengroup.org/pubs/catalog/web.htm.
A newer version of this specification is available online from The Open Group at http://www.rdg.opengroup.org/pubs/catalog/web.htm.
The implementation of the Internet protocols in the 4.4BSD-Lite operating system.
