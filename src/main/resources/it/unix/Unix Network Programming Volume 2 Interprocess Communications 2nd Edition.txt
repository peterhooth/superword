Interprocess Commun ica t i ons  I Well-implemented interprocess communications (IPC) are  key to the performance of virtually every non-trivial UNIX program.
Richard Stevens presents a comprehensive guide to every form of IPC.
Stevens heglns with a basic: introduction to IPC and the problems it is intended to solve Slep-by-step you'll learn how to maximize both System V IPC and the new Posix standards.
The book contains extensive new source code--all caqfully optimized and available on the Web.
You'll even find a complete guide to mesuring IPC performance with message passing bandwidth and laterfcy progrMns, and thread and process synchronization programs.
Prentice Hall books are widely used by corporations and government agencies for training, marketing, and resale.
The publisher offers discounts on this book when ordered in bulk quantities.
All products or services mentioned in this book are the trademarks or service marks of their respective companies or organizations.
No part of this book may be reproduced, in any form or by any means, without permission in writing from the publisher.
Most nontrivial programs involve some form of IPC or Interprocess Communication.
This is a natural effect of the design principle that the better approach is to design an application as a group of small pieces that communicate with each other, instead of designing one huge monolithic program.
The various pieces of the program can be implemented as functions that exchange information as function parameters, function return values, and global variables.
Multiple programs that communicate with each other using some form of IPC.
Many of the standard Unix tools were designed in this fashion, using shell pipelines (a form of IPC) to pass information from one program to the next.
One program comprised of multiple threads that communicate with each other using some type of IPC.
The term IPC describes this communication even though it is between threads and not between processes.
Combinations of the second two forms of design are also possible: multiple processes, each consisting of one or more threads, involving communication between the threads within a given process and between the different processes.
What I have described is distributing the work involved in performing a given application between multiple processes and perhaps among the threads within a process.
On a system containing multiple processors (CPUs), multiple processes might be.
Therefore, distributing an application among multiple processes or threads might reduce the amount of time required for an application to perform a given task.
This book describes four different forms of IPC in detail:
This book does not cover the writing of programs that communicate across a computer network.
One could argue that single-host or nonnetworked IPC (the subject of this volume) should not be used and instead all applications should be written as distributed applications that run on various hosts across a network.
Practically, however, single-host IPC is often much faster and sometimes simpler than communicating across a network.
Techniques such as shared memory and synchronization are normally available only on a single host, and may not be used across a network.
Experience and history have shown a need for both nonnetworked IPC (this volume) and IPC across a network (Volume 1 of this series)
This current volume builds on the foundation of Volume 1 and my other four books, which are abbreviated throughout this text as follows:
Although covering IPC in a text with "network programming" in the title might seem odd, IPC is often used in networked applications.
As stated in the Preface of the 1990 edition of UNIX Network Programming, "A requisite for understanding how to develop software for a network is an understanding of interprocess communication (IPC)."
Based on a word count, the material has expanded by a factor of five.
The following are the major changes with this new edition:
In addition to the three forms of "System V IPC" (message queues, semaphores, and shared memory), the newer Posix functions that implement these three types of IPC are also covered.
I say more about the Posix family of standards in Section 1.7
In the coming years, I expect a movement to the Posix IPC functions, which have several advantages over their System V counterparts.
The Posix functions for synchronization are covered: mutex locks, condition variables, and read-write locks.
These can be used to synchronize either threads or processes and are often used when accessing shared memory.
This volume assumes a Posix threads environment (called "Pthreads"), and many of the examples are built using multiple threads instead of multiple processes.
The coverage of pipes, FIFOs, and record locking focuses on their Posix definitions.
In addition to describing the IPC facilities and showing how to use them, I also develop implementations of Posix message queues, read-write locks, and Posix semaphores (all of which can be implemented as user libraries)
Understanding an implementation of a certain feature often leads to a greater knowledge of how to use that feature.
I precede this with a description of the new Solaris doors API, which is similar to RPC but on a single host.
This provides an introduction to many of the features that we need to worry about when calling procedures in another process, without having to worry about any networking details.
This text can be used either as a tutorial on IPC, or as a reference for experienced programmers.
The two chapters on RPC are also independent of the other IPC techniques.
To aid in the use as a reference, a thorough index is provided, along with summaries on the end papers of where to find detailed descriptions of all the functions and structures.
To help those reading topics in a random order, numerous references to related topics are provided throughout the text.
The source code for all the examples that appear in this book is available from the author's home page (listed at the end of this Preface)
The best way to learn the IPC techniques described in this book is to take these programs, modify them, and enhance them.
Actually writing code of this form is the only way to reinforce the concepts and techniques.
Numerous exercises are also provided at the end of each chapter, and most answers are provided in Appendix D.
A current errata for this book is also available from the author's home page.
Although the author's name is the only one to appear on the cover, the combined effort of many people is required to produce a quality text book.
First and foremost is the author's family, who put up with the long and weird hours that go into writing a book.
A special thanks to Larry Rafsky at GSquared, for lots of things.
A subset of the code in this book was tested on other Unix systems: my thanks to Michael Johnson of Red Hat Software for providing the latest releases of Red Hat Linux, and to Dave Marquardt and Jessie Haug of IBM Austin for an RS/6000 system and access to the latest releases of AIX.
My thanks to the wonderful staff at Prentice Hall-my editor Mary Franz, along with Noreen Regina, Sophie Papanikolaou, and Patti Guerrieri-for all their help, especially in bringing everything together on a tight schedule.
I produced camera-ready copy of the book (Postscript), which was then typeset for the final book.
The formatting system used was James Clark's wonderful groff package, on a SparcStation running Solaris 2.6
Dave Hanson's loom program, the GNU indent program, and some scripts by Gary Wright were used to include the 8,046 lines of C source code in the book.
I welcome email from any readers with comments, suggestions, or bug fixes.
Traditionally the term describes different ways of message passing between different processes that are running on some operating system.
This text also describes numerous forms of synchronization, because newer forms of communication, such as shared memory, require some form of synchronization to operate.
In the evolution of the Unix operating system over the past 30 years, message passing has evolved through the following stages:
Pipes (Chapter 4) were the first widely used form of IPC, available both within programs and from the shell.
The problem with pipes is that they are usable only between processes that have a common ancestor (i.e., a parent-child relationship), but this was fixed with the introduction of named pipes or FIFOs (Chapter 4)
These can be used between related or unrelated processes on a given host.
Although these are still referred to with the "System V" prefix, most versions of Unix today support them, regardless of whether their heritage is System V or not.
When describing Unix processes, the term related means the processes have some ancestor in common.
This is another way of saying that these related processes were generated.
A common example is when a process calls fork  twice, generating two child processes.
With regard to IPC, the parent can establish some form of IPC before calling fork  (a pipe or message queue, for example), knowing that the two children will inherit this IPC object across the fork.
We talk more about the inheritance of the various IPC objects with Figure 1.6
We must also note that all Unix processes are theoretically related to the init process, which starts everything going when a system is bootstrapped.
Practically speaking, however, process relationships start with a login shell (called a session) and all the processes generated by that shell.
Chapter 9 of APUE talks about sessions and process relationships in more detail.
Throughout the text, we use indented, parenthetical notes such as this one to describe implementation details, historical points, and minutiae.
These can be used between related or unrelated processes on a given host.
Since information is normally passed between the client and server (the arguments and return values of the function that is called), and since RPC can be used between a client and server on the same host, W C  can be considered as another form of message passing.
Looking at the evolution of the various forms of synchronization provided by Unix is also interesting.
Early programs that needed some form of synchronization (often to prevent multiple processes from modifying the same file at the same time) used quirks of the filesystem, some of which we talk about in Section 9.8
Although these are often used for synchronization between threads, they can also provide synchronization between different processes.
Read-write locks (Chapter 8) are an additional form of synchronization.
These have not yet been standardized by Posix, but probably will be soon.
In the traditional Unix programming model, we have multiple processes running on a system, with each process having its own address space.
Information can be shared between Unix processes in various ways.
The two processes on the left are sharing some information that resides in a file in the filesystem.
To access this data, each process must go through the kernel (e.g., read, wri te ,  lseek, and the like)
Some form of synchronization is required when a file is being updated, both to protect multiple writers from each other, and to protect one or more readers from a writer.
The two processes in the middle are sharing some information that resides within the kernel.
A pipe is an example of this type of sharing, as are System V message queues and System V semaphores.
Each operation to access the shared information now involves a system call into the kernel.
The two processes on the right have a region of shared memory that each process can reference.
Once the shared memory is set up by each process, the processes can access the data in the shared memory without involving the kernel at all.
Some form of synchronization is required by the processes that are sharing the memory.
Note that nothing restricts any of the IPC techniques that we describe to only two processes.
Any of the techniques that we describe work with any number of processes.
We show only two processes in Figure 1.1 for simplicity.
Although the concept of a process within the Unix system has been used for a long time, the concept of multiple threads within a given process is relatively new.
What we must worry about, however, is synchronizing access to this global data among the various threads.
Indeed, synchronization, though not explicitly a form of IPC, is used with many forms of IPC to control access to some shared data.
In this text, we describe IPC between processes and IPC between threads.
But on a system that supports threads, only the thread that calls read on an empty pipe is blocked, and the remaining threads in the process can continue to execute.
Writing data to this empty pipe can be done by another thread in the same process or by some thread in another process.
Appendix B summarizes some of the characteristics of threads and the five basic Pthread functions that are used throughout this text.
We can define the persistence of any type of IPC as how long an object of that type remains in existence.
A process-persistent IPC object remains in existence until the last process that holds the object open closes the object.
A kernel-persistent IPC object remains in existence until the kernel reboots or until the object is explicitly deleted.
Examples are System V message queues, semaphores, and shared memory.
The object retains its value even if the kernel reboots.
Posix message queues, semaphores, and shared memory have this property, if they are implemented using mapped files (not a requirement)
We must be careful when defining the persistence of an IPC object because it is not always as it seems.
For example, the data within a pipe is maintained within the kernel, but pipes have process persistence and not kernel persistence-after the last process that has the pipe open for reading closes the pipe, the kernel discards all the data and removes the pipe.
Similarly, even though FIFOs have names within the filesystem, they also have process persistence because all the data in a FIFO is discarded after the last process that has the FIFO open closes the FIFO.
Figure 1.3 summarizes the persistence of the IPC objects that we describe in this text.
Persistence process process process process process process kernel kernel process kernel kernel kernel kernel process process process.
Note that no type of IPC has filesystem persistence, but we have mentioned that the three types of Posix IPC may, depending on the implementation.
Obviously, writing data to a file provides filesystem persistence, but this is normally not used as a form of IPC.
Most forms of IPC are not intended to survive a system reboot, because the processes do not survive the reboot.
Requiring filesystem persistence would probably degrade the performance for a given form of IPC, and a common design goal for IPC is high performance.
When two unrelated processes use some type of IPC to exchange information between themselves; the IPC object must have a name or identifier of some form so that one.
Pipes do not have names (and therefore cannot be used between unrelated processes), but FIFOs have a Unix pathname in the filesystem as their identifier (and can therefore be used between unrelated processes)
As we move to other forms of IPC in the following chapters, we use additional naming conventions.
The set of possible names for a given type of IPC is called its name space.
The name space is important, because with all forms of IPC other than plain pipes, the name is how the client and server connect with each other to exchange messages.
Figure 1.4 summarizes the naming conventions used by the different forms of IPC.
Figure 1.4 Name spaces for the various forms of IPC.
For comparison purposes, we include three types of sockets, which are described in detail in UNPvl.
Note that the sockets API (application program interface) is being standardized by the Posix.lg working group and should eventually become part of a future Posix.1 standard.
Even though Posix.1 standardizes semaphores, they are an optional feature.
For the optional features, we specify the name of the constant (e.g., -POSIX-THREADS) that is defined (normally in the <unistd.h> header) if the feature is supported.
System V semaphore (not defined) I mandatory System V shared memory (not defined) mandatow.
We need to understand the effect of the fork, exec, and -exit  functions on the various forms of IPC that we discuss.
The latter is called by the e x i t  function.
Most of these features are described later in the text, but we need to make a few points.
First, the calling of fork from a multithreaded process becomes messy with regard to unnamed synchronization variables (mutexes, condition variables, read-write locks, and memory-based semaphores)
We simply note in the table that if these variables reside in shared memory and are created with the process-shared attribute, then they remain accessible to any thread of any process with access to that shared memory.
Second, the three forms of System V IPC have no notion of being open or closed.
So these three forms of IPC are available to any process that knows the identifier, although some special handling is indicated for semaphores and shared memory.
Figure 1.6 Effect of calling fork, exec, and -exit on IPC.
In any real-world program, we must check every function call for an error return.
Since terminating on an error is the common case, we can shorten our programs by defining a wrapper function that performs the actual function call, tests the return value, and terminates on an error.
The convention we use is to capitalize the name of the function, as in.
Sern jos t  ( p t r )  ; Our wrapper function is shown in Figure 1.7
Whenever you encounter a function name in the text that begins with a capital letter, that is a wrapper function of our own.
It calls a function whose name is the same but begins with the lowercase letter.
The wrapper function always terminates with an error message if an error is encountered.
When describing the source code that is presented in the text, we always refer to the lowest-level function being called (e.g., semjost) and not the wrapper function (e.g., Semjost)
Similarly the index always refers to the lowest level function being called, and not the wrapper functions.
The format of the source code just shown is used throughout the text.
The text describing portions of the code begins with the starting and ending line numbers in the left margin.
Sometimes the paragraph is preceded by a short descriptive bold heading, providing a summary statement of the code being described.
The horizontal rules at the beginning and end of the code fragment specify the source code filename: the file wrapunix.
Since the source code for all the examples in the text is freely available (see the Preface), you can locate the appropriate source file.
Compiling, running, and especially modifying these programs while reading this text is an excellent way to learn the concepts of interprocess communications.
Although these wrapper functions might not seem like a big savings, when we discuss threads in Chapter 7, we will find that the thread functions do not set the standard Unix errno variable when an error occurs; instead the errno value is the return value of the function.
This means that every time we call one of the pthread functions, we must allocate a variable, save the return value in that variable, and then set errno to this value before calling our err-sys function (Figure C.4)
To avoid cluttering the code with braces, we can use C's comma operator to combine the assignment into errno and the call of err-sys into a single statement, as in the following:
Alternately, we could define a new error function that takes the system's error number as an argument.
But we can make this piece of code much easier to read as just.
With careful C coding, we could use macros instead of functions, providing a little run-time efficiency, but these wrapper functions are rarely, if ever, the performance bottleneck of a program.
Our choice of capitalizing the first character of the function name is a compromise.
Our style seems the least distracting while still providing a visual indication that some other function is really being called.
Throughout the rest of this book, we use these wrapper functions unless we need to check for an explicit error and handle it in some form other than terminating the process.
We do not show the source code for all our wrapper functions, but the code is freely available (see the Preface)
When an error occurs in a Unix function, the global variable errno is set to a positive value, indicating the type of error, and the function normally returns -1
Our err-sys function looks at the value of errno and prints the corresponding error message string (e.g., "Resource temporarily unavailable" if errno equals EAGAIN)
The value of errno is set by a function only if an error occurs.
Its value is undefined if the function does not return an error.
All the positive error values are constants with an all-uppercase name beginning with E and are normally defined in the.
Most activity these days with regard to Unix standardization is being done by Posix and The Open Group.
Posix is an acronym for "Portable Operating System Interface." Posix is not a single standard, but a family of standards being developed by the Institute for Electrical and Electronics Engineers, Inc., normally called the IEEE.
It specified the C language interface into a Unix-like kernel covering the following areas: process primitives (fork, exec, signals, timers), the environment of a process (user IDS, process groups), files and directories (all the 1 / 0  functions), terminal I/O, the system databases (password file and group file), and the t a r  and cpio archive formats.
The first Posix standard was a trial use version in 1986 known as "IEEEIX." The name Posix was suggested by Richard Stallman.
Appended to the title was "Part 1: System Application Program Interface (API) [C Language]" indicating that this standard was the C language API.
Throughout this text, we refer to this standard as Posix.2
Three chapters on threads were added, along with additional sections on thread synchronization (mutexes and condition variables), thread scheduling, and synchronization scheduling.
Throughout this text, we refer to this standard as Posix.1
Over one-quarter of the 743 pages are an appendix titled "Rationale and Notes." This rationale contains historical information and reasons why certain features were included or omitted.
Often the rationale is as informative as the official standard.
Unfortunately, the IEEE standards are not freely available on the Internet.
Note that semaphores were defined in the realtime standard, separately from mutexes and condition variables (which were defined in the Pthreads standard), which accounts for some of the differences that we see in their APIs.
Finally, note that read-write locks are not (yet) part of any Posix standard.
Work on all of the Posix standards continues and it is a moving target for any book.
The current status of the various Posix standards is available fromhttp://www.pasc.org/standing/sdll.html.
It is an international consortium of vendors and end-user customers from industry, govemment, and academia.
The latest name for this set of specifications is the "X/Open Single Unix Specification," although it is also called "Unix 95."
Products conforming to this specification can be called "Unix 98," which is how we refer to this specification throughout this text.
Much of the Single Unix Specification is freely available on the Internet from this URL.
Historically, most Unix systems show either a Berkeley heritage or a System V heritage, but these differences are slowly disappearing as most vendors adopt the Posix standards.
The main differences still existing deal with system administration, one area that no Posix standard currently addresses.
Three patterns of interaction are used predominantly throughout the text to illustrate various features:
File server: a client-server application in which the client sends the server a pathname and the server returns the contents of that file to the client.
Producer<onsumer: one or more threads or processes (producers) place data into a shared buffer, and one or more threads or processes (consumers) operate on the data in the shared buffer.
Sometimes the sequence number is in a shared file, and sometimes it is in shared memory.
The first example illustrates the various forms of message passing, whereas the other two examples illustrate the various types of synchronization and shared memory.
Various solutions have been implemented, none of which are perfect.
We consider IPC between multiple threads in a single process, and between multiple processes.
When choosing the type of IPC to use for a given application, we must be aware of the persistence of that IPC object.
Another feature of each type of IPC is its name space: how IPC objects are identified by the processes and threads that use the IPC object.
Typically, a server creates an IPC object with some name and the clients use that name to access the IPC object.
Throughout the source code in the text, we use the wrapper functions described in Section 1.6 to reduce the size of our code, yet still check every function call for an error return.
Figure 1.9 Different versions of the file server client-server example.
Description Uses two pipes, parent-child Uses popen and c a t Uses two FIFOs, parent-child Uses two FIFOs, stand-alone server, unrelated client Uses FIFOs, stand-alone iterative server, multiple clients Uses pipe or FIFO: builds records on top of byte stream Uses two System V message queues Uses one System V message queue, multiple clients Uses one System V message queue per client, multiple clients Uses descriptor passing across a door.
Posix memory-based semaphores, one producer, one consumer Posix memory-based semaphores, multiple producers, one consumer Posix memory-based semaphores, multiple producers, multiple consumers Posix memory-based semaphores, one producer, one consumer: multiple buffers.
This chapter describes all these common properties: the pathnames used for identification, the flags specified when opening or creating, and the access permissions.
A summary of their functions is shown in Figure 2.1
In Figure 1.4, we noted that the three types of Posix IPC use "Posix IPC names" for their identification.
The first argument to the three functions mq_open, sem-open, and s h x o p e n  is such a name, which may or may not be a real pathname in a filesystem.
It must conform to existing rules for pathnames (must consist of at most PATH-MAX bytes, including a terminating null byte)
If it begins with a slash, then different calls to these functions all reference the same queue.
If it does not begin with a slash, the effect is implementation dependent.
Functions to create, open, or delete meopen m ~ c l o s e mq-unlink.
The interpretation of additional slashes in the name is implementation defined.
So, for portability, these names must begin with a slash and must not contain any other slashes.
Unfortunately, these rules are inadequate and lead to portability problems.
Solaris 2.6 requires the initial slash but forbids any additional slashes.
Assuming a message queue, it then creates three files in / trnp that begin with.
Digital Unix 4.OB, on the other hand, creates the specified pathname in the filesystem.
The portability problem occurs if we specify a name with only one slash (as the first character): we must have write permission in that directory, the root directory.
If we specify a name of /tmp/test .1234, this will succeed on all systems that create an actual file with that name (assuming that the / tmp directory exists and that we have write permission in that directory, which is normal for most Unix systems), but fails under Solaris.
To avoid these portability problems we should always #define the name in a header that is easy to change if we move our application to another system.
This case is one in which the standard tries to be so general (in this case, the realtime standard was trying to allow message queue, semaphore, and shared memory implementations all within existing Unix kernels and as stand-alone diskless systems) that the standard's solution is nonportable.
Within Posix, this is called "a standard way of being nonstandard."
These three macros evaluate to a nonzero value if the specified IPC object (message queue, semaphore, or shared memory object) is implemented as a distinct file type and the stat structure references such a file type.
Unfortunately, these macros are of little use, since there is no guarantee that these three types of IPC are implemented using a distinct file type.
All the other macros that test for a given file type have names beginning with S-1s and their single argument is the st-mode member of a s t a t  structure.
Since these three new macros have a different argument, their names were changed to begin with S-TYPEIS.
Another solution to this portability problem is to define our own function named px-ipc-name that prefixes the correct directory for the location of Posix IPC names.
This is the notation we use for functions of our own throughout this book that are not standard system functions: the box around the function prototype and return value is dashed.
The header that is included at the beginning is usually our unpipc.
For example, the call px-ipc-name ( " t e s t l "  )
The memory for the result string is dynamically allocated and is returned by calling free.
Additionally, the environment variable PX-IPC-NAME can override the default directory.
This may be your first encounter with snpr in t f.
Lots of existing code calls s p r i n t f  instead, but s p r i n t f  cannot check for overflow of the destination buffer.
Providing input that intentionally overflows a program's sp r in t f buffer has been used for many years by hackers breaking into systems.
Nevertheless, many vendors are providing it as part of the standard C library.
We use snpr in t f  throughout the text, providing our own version that just calls s p r i n t f  when it is not provided.
The three functions that create or open an IPC object, mq_open, sem-open, and s-open, all take a second argument named oflag that specifies how to open the requested object.
This is similar to the second argument to the standard open function.
The various constants that can be combined to form this argument are shown in Figure 2.3
Figure 23 Various constants when opening or creating a Posix IPC object.
The first three rows specify how the object is being opened: read-only, write-only, or read-write.
A message queue can be opened in any of the three modes, whereas none.
When creating a new message queue, semaphore, or shared memory object at least one additional argument is required, called mode.
This argument specifies the permission bits and is formed as the bitwise-OR of the constants shown in Figure 2.4
Figure 2.4 mode constants when a new IPC object is created.
As with a newly created file, when a new message queue, semaphore, or shared memory object is created, the user ID is set to the effective user ID of the process.
The group ID of a semaphore or shared memory object is set to the effective group ID of the process or to a system default group ID.
The group ID of a new message queue is set to the effective group ID of the process.
Pages 77-78 of APUE talk more about the user and group IDS.
This difference in the setting of the group ID between the three types of Posix IPC is strange.
The group ID of a new file created by open is either the effective group ID of the process or the group ID of the directory in which the file is created, but the IPC functions cannot assume that a pathname in the filesystem is created for an IPC object.
The check for the existence of the message queue, semaphore, or shared memory object and its creation (if it does not already exist) must be atomic with regard to other processes.
We will see two similar flags for System V IPC in Section 3.4
We talk about this more with the m c r e c e i v e  and m c s e n d  functions in Section 5.4
O-TRUNC If an existing shared memory object is opened read-write, this flag specifies that the object be truncated to 0 length.
Figure 2.5 shows the actual logic flow for opening an IPC object.
We describe what we mean by the test of the access permissions in Section 2.4
Figure 25 Logic for opening or creating an IPC object.
Figure 2.6 Logic for creating or opening an IPC object.
Object does not exist error, e r rno = ENOENT OK, creates new object OK, creates new object.
Object already exists OK, references existing object OK, references existing object.
A new message queue, named semaphore, or shared memory object is created by mcopen, sem-open, or shm-open when the oflag argument contains the 0-CREAT flag.
As noted in Figure 2.4, permission bits are associated with each of these forms of IPC, similar to the permission bits associated with a Unix file.
The tests performed by most Unix kernels are as follows:
If the effective user ID of the process is 0 (the superuser), access is allowed.
By appropriate access permission bit, we mean if the process is opening the IPC object for reading, the user-read bit must be on.
If the process is opening the IPC object for writing, the user-write bit must be on.
If the effective group ID of the process or one of the supplementary group IDS of the process equals the group ID of the IPC object: if the appropriate group access permission bit is set, access is allowed, else permission is denied.
If the appropriate other access permission bit is set, access is allowed, else permission is denied.
These four steps are tried in sequence in the order listed.
Therefore, if the process owns the IPC object (step 2), then access is granted or denied based only on the user access permissions-the group permissions are never considered.
Similarly if the process does not own the IPC object, but the process belongs to an appropriate group, then access is granted or denied based only on the group access permissions-the other permissions are not considered.
The three types of Posix IPC-message queues, semaphores, and shared memory-are identified by pathnames.
But these may or may not be real pathnames in the filesystem, and this discrepancy can be a portability problem.
The solution that we employ throughout the text is to use our own px-ipc-name function.
When an IPC object is created or opened, we specify a set of flags that are similar to those for the open function.
When a new IPC object is created, we must specify the permissions for the new object, using the same s-xxx constants that are used with open (Figure 2.4)
When an existing IPC object is opened, the permission testing that is performed is the same as when an existing file is opened.
They share many similarities in the functions that access them, and in the information that the kernel maintains on them.
A summary of their functions is shown in Figure 3.1
Function to create or open Function for control operations Functions for IPC operations.
Information on the design and development of the System V IPC functions is hard to find.
Laboratories in Columbus, Ohio, for an internal version of Unix called (not surprisingly) "Columbus Unix" or just "CB Unix." This version of Unix was used for "Operation Support Systems," transaction processing systems that automated telephone company administration and recordkeeping.
In Figure 1.4, the three types of System V IPC are noted as using key-t values for their names.
These integer values are normally assigned by the f tok function.
The function f tok converts an existing pathname and an integer identifier into a key-t value (called an IPC key)
This function assumes that for a given application using System V IPC, the server and clients all agree on a single pathname that has some meaning to the application.
It could be the pathname of the server daemon, the pathname of a common data file used by the server, or some other pathname on the system.
If the client and server need only a single IPC channel between them, an id of one, say can be used.
If multiple IPC channels are needed, say one from the client to the server and another from the server to the client, then one channel can use an id of one, and the other an id of two, for example.
Once the pathname and id are agreed on by the client and server, then both can call the f tok function to convert these into the same IPC key.
Typical implementations of f tok call the stat function and then combine.
The combination of these three values normally produces a 32-bit key.
No guarantee exists that two different pathnames combined with the same, id generate different keys, because the number of bits of information in the three items just listed (filesystem identifier, i-node, and id) can be greater than the number of bits in an integer.
If the pathname does not exist, or is not accessible to the calling process, f t o k returns -1
Be aware that the file whose pathname is used to generate the key must not be a file that is created and deleted by the server during its existence, since each time it is created, it can assume a new i-node number that can change the key returned by f tok  to the next caller.
The program in Figure 3.2 takes a pathname as a command-line argument, calls stat, calls ftok, and then prints the st-dev and st- ino members of the stat  structure, and the resulting IPC key.
These three values are printed in hexadecimal, so we can easily see how the IPC key is constructed from these two values and our id of 0x57
Our purpose in showing this example is not to let us count on this combination of information to form the IPC key but to let us see how one implementation combines the pathname and id.
Note that the mapping done by f tok is one-way, since some bits from st-dev and st- ino are not used.
That is, given a key, we cannot determine the pathname that was used to create the key.
The kernel maintains a structure of information for each IPC object, similar to the information it maintains for files.
We talk about all the members of this structure in this chapter.
The three getXXX functions that create or open an IPC object (Figure 3.1) all take an IPC key value, whose type is key-t, and return an integer identifier.
This identifier is not the same as the id argument to the f tok  function, as we see shortly.
An application has two choices for the key value that is the first argument to the three getXXX functions:
All three getXXX functions (Figure 3.1) also take an oflag argument that specifies the read-write permission bits (the mode member of the i p c j e r m  structure) for the IPC object, and whether a new IPC object is being created or an existing one is being referenced.
The rules for whether a new IPC object is created or whether an existing one is referenced are as follows:
Specifying a key of IPC-PRIVATE guarantees that a unique IPC object is created.
No combinations of pathname and id exist that cause f t o k  to generate a key value of IPC-PRIVATE.
Setting the IPC-CREAT bit of the oflag argument creates a new entry for the specified key, if it does not already exist.
If an existing entry is found, that entry is returned.
Setting both the IPC-CREAT and IPC-EXCL bits of the oflag argument creates a new entry for the specified key, only if the entry does not already exist.
If an existing entry is found, an error of EEXIST is returned, since the IPC object already exists.
Setting the I PC-EXCL bit, without setting the I PC-CREAT bit, has no meaning.
The actual logic flow for opening an IPC object is shown in Figure 3.4
Note that in the middle line of Figure 3.5, the IPC-CREAT flag without IPC-EXCL, we do not get an indication whether a new entry has been created or whether we are referencing an existing entry.
In most applications, the server creates the IPC object and specifies either IPC-CREAT (if it does not care whether the object already exists) or IPC-CREAT I IPC-EXCL (if it needs to check whether the object already exists)
The clients specify neither flag (assuming that the server has already created the object)
The System V IPC functions define their own IPC-xxx constants, instead of using the Posix IPC functions (Figure 2.3)
Also note that the System V IPC functions combine their IPC-xxx constants with the perrnission bits (which we describe in the next section) into a single oflag argument.
The open function along with the Posix IPC functions have one argument named oflag that specifies the various 0-xxx flags, and another argument named mode that specifies the permission bits.
Figure 3.4 Logic for creating or opening an IPC object.
Figure 3.5 Logic for creating or opening an IPC channel.
Whenever a new IPC object is created using one of the getXXX functions with the IPC-CREAT flag, the following information is saved in the i pc se rm structure (Section 3.3):
Some of the bits in the oflag argument initialize the mode member of the ipcaerm structure.
Figure 3.6 shows the permission bits for the three different IPC mechanisms.
The two members c u i d  and cgid are set to the effective user ID and effective group ID of the calling process, respectively.
The two members u i d  and gid in the i p c x e r m  structure are also set to the effective user ID and effective group ID of the calling process.
The creator IDS never change, although a process can change the owner IDS by calling the c t l X X X  function for the IPC mechanism with a command of IPC-SET.
The three c t l X X X  functions also allow a process to change the permission bits of the mode member for the IPC object.
The three getXXX functions do not use the normal Unix file mode creation mask.
The permissions of the message queue, semaphore, or shared memory segment are set to exactly what the function specifies.
Posix IPC does not let the creator of an IPC object change the owner.
But if the Posix IPC name is stored in the filesystem, then the superuser can change the owner using the chown command.
Two levels of checking are done whenever an IPC object is accessed by any process, once when the IPC object is opened (the g e t X X X  function) and then each time the IPC object is used:
Whenever a process establishes access to an existing IPC object with one of the g e t X X X  functions, an initial check is made that the caller's oflag argument does not specify any access bits that are not in the mode member of the i p c j e r m structure.
For example, a server process can set the mode member for its input message queue so that the group-read and other-read permission bits are off.
Any process that tries to specify an oflag argument that includes these bits gets an error return from the m s g g e t  function.
But this test that is done by the g e t X X X  functions is of little use.
If the creator specifically turns off certain permission bits, and if the caller specifies these bits, the error is detected by the getXXX function.
Any process, however, can totally bypass this check by just specifying an oflag argument of 0 if it knows that the IPC object already exists.
Every IPC operation does a permission test for the process using the operation.
For example, every time a process tries to put a message onto a message queue with the msgsnd function, the following tests are performed in the order listed.
As soon as a test grants access, no further tests are performed.
If the effective user ID equals either the uid value or the cuid value for the.
By "appropriate access bit," we mean the read-bit must be set if the caller wants to do a read operation or, the IPC object (receiving a message from a message queue, for example), or the write-bit must be set for a write operation.
If the effective group ID equals either the gid value or the cgid value for the IPC object, and if the appropriate access bit is on in the mode member for the IPC object, permission is granted.
If none of the above tests are true, the appropriate "other" access bit must be on in the mode member for the IPC object, for permission to be allowed.
The ipcjerm structure (Section 3.3) also contains a variable named seq, which is a slot usage sequence number.
This is a counter that is maintained by the kernel for every potential IPC object in the system.
Every time an IPC object is removed, the kernel increments the slot number, cycling it back to zero when it overflows.
What we are describing in this section is the common SVR4 implementation.
First, consider the file descriptors maintained by the kernel for open files.
They are small integers, but have meaning only within a single process-they are process-specific values.
If we try to read from file descriptor 4, say, in a process, this approach works only if that process has a file open on this descriptor.
It has no meaning whatsoever for a file that might be open on file descriptor 4 in some other unrelated process.
System V IPC identifiers, however, are systemwide and not process-specific.
We obtain an IPC identifier (similar to a file descriptor) from one of the get functions: msgget, semget, and shmget.
These identifiers are also integers, but their meaning applies to all processes.
If two unrelated processes, a client and server, for example, use a single message queue, the message queue identifier returned by the.
This feature means that a rogue process could try to read a message from some other application's message queue by trying different small integer identifiers, hoping to find one that is currently in use that allows world read access.
To avoid this problem, the designers of these IPC facilities decided to increase the possible range of identifier values to include all integers, not just small integers.
This increase is implemented by incrementing the identifier value that is returned to the calling process, by the number of IPC table entries, each time a table entry is reused.
For example, if the system is configured for a maximum of 50 message queues, then the first time the first message queue table entry in the kernel is used, the identifier returned to the process is zero.
The next time, the identifier is 100, and so on.
A second reason for incrementing the slot usage sequence number is to avoid short term reuse of the System V IPC identifiers.
This helps ensure that a server that prematurely terminates and is then restarted, does not reuse an identifier.
Each time around the loop msgget creates a message queue, and then msgctl with a command of IPC-WID deletes the queue.
If we run the program again, we see that this slot usage sequence number is a kernel variable that persists between processes.
Since the three types of System V IPC are not identified by pathnarnes in the filesystem, we cannot look at them or remove them using the standard 1s and r m  programs.
Instead, two special programs are provided by any system that implements these types of IPC: ipcs, which prints various pieces of information about the System V IPC features, and ipcrm, which removes a System V message queue, semaphore set, or shared memory segment.
The former supports about a dozen command-line options, which affect which of the three types of IPC is reported and what information is output, and the latter supports six command-line options.
Consult your manual pages for the details of all these options.
Since System V IPC is not part of Posix, these two commands are not standardized by Posix.2
Most implementations of System V IPC have inherent kernel limits, such as the maximum number of message queues and the maximum number of semaphores per semaphore set.
These limits are often derived from the original System V implementation.
Fortunately, most systems allow the administrator to change some or all of these default limits, but the required steps are different for each flavor of Unix.
Most require rebooting the running kernel after changing the values.
Unfortunately, some implementations still use 16-bit integers for some of the limits, providing a hard limit that cannot be exceeded.
Their current values are printed by the sysdef command, although the values are printed as 0 if the corresponding kernel module has not been loaded (i.e., the facility has not yet been used)
These may be changed by placing any of the following statements in the / e t c / system file, which is read when the kernel bootstraps.
With Digital Unix 4.OB, the sysconf i g  program can query or modify many kernel parameters and limits.
Here is the output of this program with the -q option, which queries the kernel for the current limits, for the ipc subsystem.
We have omitted some lines unrelated to the System V IPC facility.
Different defaults for these parameters can be specified in the / e t c  / sysconf i g t a b file, which should be maintained using the sysconf igdb program.
The first argument to the three functions, msgget, semget, and shmget, is a System V IPC key.
These keys are normally created from a pathname using the system's f tok function.
The key can also be the special value of IPC-PRIVATE.
These three functions create a new IPC object or open an existing IPC object and return a System V IPC identifier: an integer that is then used to identify the object to the remaining IPC functions.
These integers are not per-process identifiers (like descriptors) but are systemwide identifiers.
These identifiers are also reused by the kernel after some time.
Associated with every System V IPC object is an i p c s e r m  structure that contains information such as the owner's user ID, group ID, read-write permissions, and so on.
One difference between Posix IPC and System V IPC is that this information is always available for a System V IPC object (by calling one of the three XXXc t l  functions with an argument of IPC-STAT), but access to this information for a Posix IPC object depends on the implementation.
If the Posix IPC object is stored in the filesystem, and if we know its name in the filesystem, then we can access this same information using the existing filesystem tools.
When a new System V IPC object is created or an existing object is opened, two flags are specified to the getXXX function (IPC-CREAT and IPC-EXCL), combined with nine permission bits.
Undoubtedly, the biggest problem in using System V IPC is that most implementations have artificial kernel limits on the sizes of these objects, and these limits date back to their original implementation.
These mean that most applications that make heavy use of System V IPC require that the system administrator modify these kernel limits, and accomplishing this change differs for each flavor of Unix.
Immediately after running the program in Figure 3.7, we run a program that creates two message queues.
Assuming no other message queues have been used by any other applications since the kernel was booted, what two values are returned by the kernel as the message queue identifiers? We noted in Section 3.5 that the System V IPC getXXX functions do not use the file mode creation mask.
Compare the permissions of the resulting FIFO and message queue.
Make certain your shell umask value is nonzero before running this program.
A server wants to create a unique message queue for its clients.
Which is preferable-using some constant pathname (say the server's executable file) as an argument to f tok, or using IPC-PRIVATE? Modify Figure 3.2 to print just the IPC key and pathname.
Run the find program to print all the pathnames on your system and run the output through the program just modified.
How many pathnames map to the same key? If your system supports the sar program ("system activity reporter"), run the command.
Although useful for many operations, their fundamental limitation is that they have no name, and can therefore be used only by related processes.
Both pipes and FIFOs are accessed using the normal read and w r i t e  functions.
But for practical purposes, pipes are normally used between processes that have a common ancestor.
This chapter describes the creation and use of pipes and FIFOs.
We use a simple file server example and also look at some client-server design issues: how many IPC channels are needed, iterative versus concurrent servers, and byte streams versus message interfaces.
The client reads a pathname from the standard input and writes it to the P C  channel.
The server reads this pathname from the IPC channel and tries to open the file for reading.
If the server can open the file, the server responds by reading the file and writing it to the IPC channel; otherwise, the server responds with an error message.
If the file cannot be read by the server, the client reads an error message from the IPC channel.
The two dashed lines between the client and server in Figure 4.1 are the IPC channel.
A pipe is created by the p ipe  function and provides a one-way (unidirectional) flow of data.
Some versions of Unix, notably SVR4, provide full-duplex pipes, in which case, both ends are available for reading and writing.
Another way to create a full-duplex IPC channel is with the socketpair function, described in Section 14.3 of UNPvl, and this works on most current Unix systems.
The most common use of pipes, however, is with the various shells, in which case, a half-duplex pipe is adequate.
The S-ISFIFO macro can be used to determine if a descriptor or file is either a pipe or a FIFO.
Its single argument is the s t-mode member of the s ta t  structure and the macro evaluates to true (nonzero) or false (0)
For a pipe, this structure is filled in by the f s t a t  function.
For a FIFO, this struc- is filled in by the f s ta t ,  1s t a t ,  or s ta t functions.
Figure 4.2 shows how a pipe looks in a single process.
Although a pipe is created by one process, it is rarely used within a single process.
We show an example of a pipe within a single process in Figure 5.14
Pipes are typically used to communicate between two different processes (a parent and child) in the following way.
First, a process (which will be the parent) creates a pipe and then forks to create a copy of itself, as shown in Figure 4.3
Figure 4.3 Pipe in a single process, immediately after fork.
Next, the parent process closes the read end of one pipe, and the child process closes the write end of that same pipe.
This provides a one-way flow of data between the two p r e cesses, as shown in Figure 4.4
When we enter a command such as who I sort I lp.
The shell also duplicates the read end of each pipe to standard input and the write end of each pipe to standard output.
Figure 4.5 Pipes between three processes in a shell pipeline.
All the pipes shown so far have been half-duplex or unidirectional, providing a oneway flow of data only.
When a two-way flow of data is desired, we must create two pipes and use one for each direction.
We show the code for these steps in Figure 4.8
Figure 4.6 Two pipes to provide a bidirectional flow of data.
Let us now implement the client-server example described in Section 4.2 using pipes.
The main function creates two pipes and forks a child.
The client then runs in the parent process and the server runs in the child process.
The first pipe is used to send the pathname from the client to the server, and the second pipe is used to send the contents of that file (or an error message) from the server to the client.
This setup gives us the arrangement shown in Figure 4.7
Realize that in this figure we show the two pipes connecting the two processes, but each pipe goes through the kernel, as shown previously in Figure 4.6
Therefore, each byte of data from the client to the server, and vice versa, crosses the user-kernel interface twice: once when written to the pipe, and again when read from the pipe.
The server (the child) terminates first, when it calls exit after writing the final data to the pipe.
It then becomes a zombie: a process that has terminated, but whose parent is still running but has not yet waited for the child.
When the child terminates, the kernel also generates a SIGCHLD signal for the parent, but the parent does not catch this signal, and the default action of this signal is to be ignored.
Shortly thereafter, the parent's client function returns after reading the final data from the pipe.
The parent then calls waitpid to fetch the termination status of the terminated child (the zombie)
If the parent did not call waitpid, but just terminated, the child would be inherited by the init process, and another SIGCHLD signal would be sent to the init process, which would then fetch the termination status of the zombie.
Normally this is the contents of the file, but i f  the specified pathname cannot be opened, what the server returns is an error message.
Read pathname from pipe The pathname written by the client is read from the pipe and null terminated.
The file is opened for reading, and if an error occurs, an error message string is returned to the client across the pipe.
We call the strerror function to return the error message string corresponding to errno.
Pages 690-691 of UNPvl talk more about the strerror function.
If the open succeeds, the contents of the file are copied to the pipe.
We can see the output from the program when the pathname is OK, and when an error occurs.
We mentioned in the previous section that some systems provide full-duplex pipes: SVR4's pipe function and the socketpair function provided by many kernels.
A full-duplex pipe could be implemented as shown in Figure 4.12
This implies that only one buffer exists for the pipe and everything written to the pipe (on either descriptor) gets appended to the buffer and any read from the pipe (on either descriptor) just takes data from the front of the buffer.
Figure 4.12 One possible (incorrect) implementation of a full-duplex pipe.
The problem with this implementation becomes apparent in a program such as Figure A.29
We want two-way communication but we need two independent data streams, one in each direction.
Otherwise, when a process writes data to the full-duplex pipe and then turns around and issues a read on that pipe, it could read back what it just wrote.
Figure 4.13 shows the actual implementation of a full-duplex pipe.
Here, the full-duplex pipe is constructed from two half-duplex pipes.
The parent writes the character p to the pipe, and then reads a character from the pipe.
The child sleeps for 3 seconds, reads a character from the pipe, and then writes the character c to the pipe.
The purpose of the sleep in the child is to allow the parent to call read before the child can call read, to see whether the parent reads back what it wrote.
If we run this program under Solaris 2.6, which provides full-duplex pipes, we observe the desired behavior.
The parent does not read back what it wrote (the character p)
The error returned by read is EBADF, which means that the descriptor is not open for reading.
Similarly, w r i t e  returns the same error if its descriptor is not open for writing.
It is processed by the s h  program (normally a Bourne shell), so the PATH environment variable is used to locate the command.
A pipe is created between the calling process and the specified command.
The value returned by popen is a standard I/O F I L E  pointer that is used for either input or output, depending on the character string type.
If type is r, the calling process reads the standard output of the command.
If type is w, the calling process writes to the standard input of the command.
Section 14.3 of APUE provides an implementation of popen and pclose.
Figure 4.15 shows another solution to our client-server example using the popen function and the Unix c a t  program.
The pathname is read from standard input, as in Figure 4.9
The output from either the shell or the cat program is copied to standard output.
One difference between this implementation and the implementation in Figure 4.8 is that now we are dependent on the error message generated by the system's cat program, which is often inadequate.
For example, under Solaris 2.6, we get the following error when trying to read a file that we do not have permission to read:
But under BSD/OS 3.1, we get a more descriptive error when trying to read a similar file:
Also realize that the call to popen succeeds in such a case, but fgets just returns an end-of-file the first time it is called.
The cat program writes its error message to standard error, and popen does nothing special with it-only standard output is redirected to the pipe that it creates.
Pipes have no names, and their biggest disadvantage is that they can be used only between processes that have a parent process in common.
Two unrelated processes cannot create a pipe between them and use it for IPC (ignoring descriptor passing)
But unlike pipes, a FIFO has a pathname associated with it, allowing unrelated processes to access a single FIFO.
A FIFO is created by the mkf i f o function.
The pathname is a normal Unix pathname, and this is the name of the FIFO.
The mode argument specifies the file permission bits, similar to the second argument.
That is, it creates a new FIFO or returns an error of EEXIST if the named FIFO already exists.
If the creation of a new FIFO is not desired, call open instead of mkf i f o.
To open an existing FIFO or create a new FIFO if it does not already exist, call mkf if o, check for an error of EEXIST, and if this occurs, call open instead.
The mkf i f o command also creates a FIFO.
This can be used from shell scripts or from the command line.
It must not be opened for read-write, because a FIFO is half-duplex.
A write to a pipe or FIFO always appends the data, and a read always returns what' is at the beginning of the pipe or FIFO.
If lseek is called for a pipe or FIFO, the error ESPIPE is returned.
We now redo our client-server from Figure 4.8 to use two FIFOs instead of two pipes.
Our client and server functions remain the same; all that changes is the main function, which we show in Figure 4.16
Create two FlFOs 10-16 TWO FIFOs are created in the / tmp filesystem.
These permission bits are modified by the file mode creation mask of the process.
Before executing these calls, the parent opens the first FIFO for writing and the second FIFO for reading, and the child opens the first FIFO for reading and the second FIFO for writing.
This is similar to our pipe example, and Figure 4.17 shows this arrangement.
The changes from our pipe example to this FIFO example are as follows:
To create and open a pipe requires one call to pipe.
To create and open a FIFO requires one call to mkf i f o followed by a call to open.
A FIFO's name is deleted from the filesystem only by calling unlink.
The benefit in the extra calls required for the FIFO is that a FIFO has a name in the filesystem allowing one process to create a FIFO and another unrelated process to open the FIFO.
Subtle problems can occur with programs that do not use FIFOs correctly.
Consider Figure 4.16: if we swap the order of the two calls to open in the parent, the program does not work.
The reason is that the open of a FIFO for reading blocks if no process currently has the FIFO open for writing.
If we swap the order of these two opens in the parent, both the parent and the child are opening a FIFO for reading when no process has the FIFO open for writing, so both block.
In Figure 4.16, the client and server are still related processes.
But we can redo this example with the client and server unrelated.
This program is nearly identical to the server portion of Figure 4.16
Notice that the client, not the server, deletes the FIFOs when done, because the client performs the last operation on the FIFOs.
In the case of a pipe or FIFO, where the kernel keeps a reference count of the number of open descriptors that refer to the pipe or FIFO, either the client or server could call unlink without a problem.
Even though this function removes the pathname from the filesystem, this does not affect open descriptors that had previously opened the pathname.
But for other forms of IPC, such as System V message queues, no counter exists and if the server were to delete the queue after writing its final message to the queue, the queue could be gone when the client tries to read the final message.
To run this client and server, start the server in the background % server-fifo &
Alternately, we could start only the client and have it invoke the server by calling fo rk  and then exec.
The client could also pass the names of the two FIFOs to the server as command-line arguments through the exec function, instead of coding them into a header.
But this scenario would make the server a child of the client, in which case, a pipe could just as easily be used.
We need to describe in more detail some properties of pipes and FIFOs with regard to their opening, reading, and writing.
First, a descriptor can be set nonblocking in two ways.
The 0-NONBLOCK flag can be specified when open is called.
For example, the first call to open in Figure 4.20 could be.
If a descriptor is already open, f cntl can be called to enable the 0-NONBLOCK flag.
This technique must be used with a pipe, since open is not called for a pipe, and no way exists to specify the 0-NONBLOCK flag in the call to pipe.
When using f cntl, we first fetch the current file status flags with the F-GETFL command, bitwise-OR the 0-NONBLOCK flag, and then store the file status flags with the F-SETFL command:
Beware of code that you may encounter that simply sets the desired flag, because this also clears all the other possible file status flags:
Note a few additional rules regarding the reading and writing of a pipe or FIFO.
If we ask to read more data than is currently available in the pipe or FIFO, only the available data is returned.
We must be prepared to handle a return value from read that is less than the requested amount.
If the number of bytes to w r i t e  is less than or equal to PIPE-BUF (a Posix limit that we say more about in Section 4.11), the w r i t e  is guaranteed to be atomic.
This means that if two processes each write to the same pipe or FIFO at about the same time, either all the data from the first process is written, followed by all the data from the second process, or vice versa.
The system does not intermix the data from the two processes.
If, however, the number of bytes to w r i t e  is greater than PIPE-BUF, there is no guarantee that the w r i t e  operation is atomic.
We show a program in Section 4.11 that prints this value.
The setting of the 0-NONBLOCK flag has no effect on the atomicity of wr i tes  to a pipe or FIFO-atomicity is determined solely by whether the requested number of bytes is less than or equal to PIPE-BUF.
But when a pipe or FIFO is set nonblocking, the return value from w r i t e  depends on the number of bytes to write.
If the number of bytes to w r i t e  is less than or equal to PIPE-BUF: a.
If there is room in the pipe or FIFO for the requested number of bytes, all the.
If there is not enough room in the pipe or FIFO for the requested number of.
Since the 0-NONBLOCK flag is set, the process does not want to be put to sleep.
But the kernel cannot accept part of the data and still guarantee an atomic wri te ,  so the kernel must return an error and tell the process to try again later.
If the number of bytes to w r i t e  is greater than PIPE-BUF: a.
If there is room for at least 1 byte in the pipe or FIFO, the kernel transfers.
If the pipe or FIFO is full, return is made immediately with an error of EAGAIN.
If we w r i t e  to a pipe or FIFO that is not open for reading, the SIGPIPE signal is generated: a.
If the process does not catch or ignore SIGPIPE, the default action of terminating the process is taken.
If the process ignores the SIGPIPE signal, or if it catches the signal and.
But the easiest way to handle this signal is to ignore it (set its disposition to SIG-IGN) and let write return an error of EPIPE.
An application should always detect an error return from write, but detecting the termination of a process by SIGPIPE is harder.
If the signal is not caught, we must look at the termination status of the process from the shell to determine that the process was killed by a signal, and which signal.
The real advantage of a FIFO is when the server is a long-running process (e.g., a daemon, as described in Chapter 12 of UNPvl) that is unrelated to the client.
The daemon creates a FIFO with a well-known pathname, opens the FIFO for reading, and the client then starts at some later time, opens the FIFO for writing, and sends its commands or whatever to the daemon through the FIFO.
One-way communication of this form (client to server) is easy with a FIFO, but it becomes harder if the daemon needs to send something back to the client.
Figure 4.22 shows the technique that we use with our example.
The server creates a FIFO with a well-known pathname, / tmp/ f i f  o.
Each client creates its own FIFO when it starts, with a pathname containing its process ID.
The second open (with the 0-WRONLY flag) then returns immediately, because the FIFO is already open for reading.
Each client request is a single line consisting of the process ID, one space, and then the pathname.
The strchr function returns a pointer to the first blank in the line, and ptr is incremented to point to the first character of the pathname that follows.
The pathname of the client's FIFO is constructed from the process ID, and the FIFO is opened for write-only by the server.
The remainder of the server is similar to our server function from Figure 4.10
The file is opened and if this fails, an error message is returned to the client across the FIFO.
If the open succeeds, the file is copied to the client's FIFO.
When done, we must close the server's end of the client's FIFO, which causes the client's read to return 0 (end-of-file)
The server does not delete the client's FIFO; the client must do so after it reads the end-of-file from the server.
The client's FIFO is created with the process ID as the final part of the pathname.
The client's request consists of its process ID, one blank, the pathname for the server to send to the client, and a newline.
This line is built in the array buff, reading the pathname from the standard input.
The server's FIFO is opened and the request is written to the FIFO.
If this client is the first to open this FIFO since the server was started, then this open unblocks the server from its call to open (with the 0-RDONLY flag)
The server's reply is read from the FIFO and written to standard output.
We can start our server in one window and run the client in another window, and it works as expected.
We can also interact with the server from the shell, because FIFOs have names in the filesystem.
We send our process ID and pathname to the server with one shell command (echo) and read the server's reply with another (cat)
Any amount of time can occur between these two commands.
Therefore, the server appears to write the file to the FIFO, and the client later executes cat to read the data from the FIFO, which might make us think.
Indeed, the rule is that when the final c lose  of a pipe or FIFO occurs, any remaining data in the pipe or FIFO is discarded.
What is happening in our shell example is that after the server reads the request line from the client, the server blocks in its call to open on the client's FIFO, because the client (our shell) has not yet opened the FIFO for reading (recall Figure 4.21)
Only when we execute c a t  sometime later, which opens the client FIFO for reading, does the server's call to open for this FIFO return.
This timing also leads to a denial-of-service attack, which we discuss in the next section.
Using the shell also allows simple testing of the server's error handling.
We can easily send a line to the server without a process ID, and we can also send a line to the server specifying a process ID that does not correspond to a FIFO in the / tmp directory.
For example, if we invoke the client and enter the following lines.
Our simple client-server also lets us see why the atomicity property of wri tes  to pipes and FIFOs is important.
Assume that two clients send requests at about the same time to the server.
The data in the FIFO will not be something like.
FIFOs are a form of IPC that can be used on a single host.
Although FIFOs have names in the filesystem, they can be used only on local filesystems, and not on NFS-mounted filesystems.
In this example, the filesystem /nf s /bsdi/usr is the /usr filesystem on the host bsdi.
Some systems (e.g., BSD/OS) do allow FIFOs to be created on an NFSmounted filesystem, but data cannot be passed between the two systems through one of these FIFOs.
In this scenario, the FIFO would be used only as a rendezvous point in the filesystem between clients and servers on the same host.
A process on one host cannot send data to a process on another host through a FIFO, even though both processes may be able to open a FIFO that is accessible to both hosts through NFS.
The server in our simple example from the preceding section is an iterative server.
It iterates through the client requests, completely handling each client's request before proceeding to the next client.
The new child handles the client request to completion, and the multiprogramming features of Unix provide the concurrency of all the different processes.
But there are other techniques that are discussed in detail in Chapter 27 of UNPvl:
Although the discussion in UNPvl is for network servers, the same techniques apply to IPC servers whose clients are on the same host.
We have already mentioned one problem with an iterative server-some clients must wait longer than expected because they are in Line following other clients with longer requests-but another problem exists.
This means that a malicious client could tie up the server by sending it a request line, but never opening its FIFO for reading.
To avoid this, we must be careful when coding the iterative portion of any server, to note where the server might block, and for how long it might block.
One way to handle the problem is to place a timeout on certain operations, but it is usually simpler to code the server as a concurrent server, instead of as an iterative server, in which case, this type of denial-of-service attack affects only one child, and not the main server.
Even with a concurrent server, denial-of-service attacks can still occur: a malicious client could send lots of independent requests, causing the server to reach its limit of child processes, causing subsequent forks to fail.
The examples shown so far, for pipes and FIFOs, have used the stream I/O model, which is natural for Unix.
No record boundaries exist-reads and writes do not examine the data at all.
The data is a byte stream with no interpretation done by the system.
If any interpretation is desired, the writing process and the reading process must agree to it a priori and do it themselves.
Sometimes an application wants to impose some structure on the data being transferred.
This can happen when the data consists of variable-length messages and the reader must know where the message boundaries are so that it knows when a single message has been read.
Special termination sequence in-band: many Unix applications use the newline character to delineate each message.
The writing process appends a newline to each message, and the reading process reads one line at a time.
In general, this requires that any occurrence of the delimiter in the data must be escaped (that is, somehow flagged as data and not as a delimiter)
Many Internet applications (ETP, SMTP, HTTP, NNTP) use the 2-character sequence of a carriage return followed by a liiefeed (CR/LF) to delineate text records.
This technique is also used by Sun RPC when used with TCP.
One advantage to this technique is that escaping a delimiter that appears in the data is unnecessary, because the receiver does not need to scan all the data, looking for the end of each record.
This requires a new connection for every record, but is used with HTTP 1.0
Since a FIFO has a name, it can be opened using the standard I/O f open function.
More structured messages can also be built, and this capability is provided by both Posix message queues and System V message queues.
We will see that each message has a length and a priority (System V calls the latter a "type")
The length and priority are specified by the sender, and after the message is read, both are returned to the reader.
Each message is a record, similar to UDP datagrams (UNPvl)
We can also add more structure to either a pipe or FIFO ourselves.
Our own "messages" to use with pipes, FIFOs, and message queues.
We ignore the type field for now, but return to it in Chapter 6, when we describe System V message queues.
Each message also has a length, and we allow the length to be zero.
What we are doing with the mymesg structure is to precede each message with its length, instead of using newlines to separate the messages.
Earlier, we mentioned two benefits of this design: the receiver need not scan each received byte looking for the end of the message, and there is no need to escape the delimiter (a newline) if it appears in the message.
Figure 4.26 shows a picture of the mymesg structure, and how we use it with pipes, FIFOs, and System V message queues.
I second argument for msgsnd and msgrcv I I mesg-len I mesg-type / mesg-data I.
It now takes two reads for each message, one to read the length, and another to read the actual message (if the length is greater than 0)
Careful readers may note that mesg-recv checks for all possible errors and terminates if one occurs.
Nevertheless, we still define a wrapper function named Mesg-recv and call it from our programs, for consistency.
We now change our client and server functions to use the mesg-send and mesg-recv functions.
The client calls mesg-recv in a loop, reading everything that the server sends back.
By convention, when mesg-recv returns a length of 0, this indicates the end of data from the server.
If the call to f open succeeds, the file is read using f gets and sent to the client, one line per message.
A message with a length of 0 indicates the end of the file.
When using either pipes or FIFOs, we could also close the IPC channel to notify the peer that the end of the input file was encountered.
We send back a message with a length of 0, however, because we will encounter other types of IPC that do not have the concept of an end-of-file.
The main functions that call our client and server functions do not change at all.
OPEN-MAX the maximum number of descriptors open at any time by a process (Posix requires that this be at least 16), and.
The value of OPEN-MAX can be queried by calling the sysconf function, as we show shortly.
It can normally be changed from the shell by executing the ul imi t command (Bourne shell and KornShell, as we show shortly) or the limit command (C shell)
It can also be changed from a process by calling the setrlimi t function (described in detail in Section 7.11 of APUE)
The value of PIPE-BUF is often defined in the <limits.
This means that its value can differ, depending on the pathname that is specified (for a FIFO, since pipes do not have names), because different pathnames can end up on different filesystems, and these filesystems might have different characteristics.
The value can therefore be obtained at run time by calling either pathconf or fpathconf.
Figure 4.31 shows an example that prints these two limits.
Figure 4.31 Determine values of PIPE-BUF and OPEN-W at run time.
We now show how to change the value of OPEN-mX under Solaris, using the KornShell.
Although the value of PIPE-BUF can change for a FIFO, depending on the underlying filesystem in which the pathname is stored, this should be extremely rare.
Chapter 2 of APUE describes the fpathconf, pathconf, and sysconf functions, which provide run-time information on certain kernel limits.
The getconf command is defined by Posix.2, and it prints the value of most of these implementation limits.
Pipes and FIFOs are fundamental building blocks for many applications.
Pipes are commonly used with the shells, but also used from within programs, often to pass information from a child back to a parent.
Some of the code involved in using a pipe (pipe, fork, close, exec, and waitpid) can be avoided by using popen and pclose, which handle all the details and invoke a shell.
FIFOs are similar to pipes, but are created by m k f  i f  o and then opened by open.
We must be careful when opening a FIFO, because numerous rules (Figure 4.21) govern whether an open blocks or not.
Using pipes and FIFOs, we looked at some client-server designs: one server with multiple clients, and iterative versus concurrent servers.
An iterative server handles one client request at a time, in a serial fashion, and these types of servers are normally open to denial-of-service attacks.
A concurrent server has another process or thread handle each client request.
One characteristic of pipes and FIFOs is that their data is a byte stream, similar to a TCP connection.
Any delineation of this byte stream into records is left to the application.
We will see in the next two chapters that message queues provide record boundaries, similar to UDP datagrams.
What can happen if the logic is changed, calling open first and then mkf i f  o if the FIFO does not exist? What happens in the call to popen in Figure 4.15 if the shell encounters an error? Remove the open of the server's FIFO in Figure 4.23 and verify that this causes the server to terminate when no more clients exist.
In Figure 4.23, we noted that when the server starts, it blocks in its first call to open until the first client opens this FIFO for writing.
How can we get around this, causing both opens to return immediately, and block instead in the first call to readl ine? What happens to the client in Figure 4.24 if it swaps the order of its two calls to open? Why is a signal generated for the writer of a pipe or FIFO after the reader disappears, but not for the reader of a pipe or FIFO after its writer disappears? Write a small test program to determine whether f s t a t  returns the number of bytes of data currently in a FIFO as the s t- s ize  member of the s t a t  structure.
Write a small test program to determine what s e l e c t  returns when you select for writability on a pipe descriptor whose read end has been closed.
A message queue can be thought of as a linked list of messages.
Threads with adequate permission can put messages onto the queue, and threads with adequate permission can remove messages from the queue.
Each message is a record (recall our discussion of streams versus messages in Section 4.10), and each message is assigned a priority by the sender.
No requirement exists that someone be waiting for a message to arrive on a queue before some process writes a message to that queue.
This is in contrast to both pipes and FIFOs, for which it having a writer makes no sense unless a reader also exists.
A process can write some messages to a queue, terminate, and have the messages read by another process at a later time.
We say that message queues have kernel persistence (Section 1.3)
We said in Chapter 4 that any data remaining in a pipe or FIFO when the last close of the pipe or FIFO takes place, is discarded.
This chapter looks at Posix message queues and Chapter 6 looks at System V message queues.
Many similarities exist between the two sets of functions, with the main differences being:
A read on a Posix message queue always returns the oldest message of the highest priority, whereas a read on a System V message queue can return a message of any desired priority.
Posix message queues allow the generation of a signal or the initiation of a thread when a message is placed onto an empty queue, whereas nothing similar is provided by System V message queues.
Every message on a queue has the following attributes: an unsigned integer priority (Posix) or a long integer type (System V), the length of the data portion of the message (which can be O), and the data itself (if the length is greater than 0)
The latter two are byte streams with no message boundaries, and no type associated with each message.
We discussed this in Section 4.10 and added our own message interface to pipes and FIFOs.
Figure 5.1 shows one possible arrangement of a message queue.
Figure 5.1 Possible arrangement of a Posix message queue containing three messages.
We are assuming a linked list, and the head of the list contains the two attributes of the queue: the maximum number of messages allowed on the queue, and the maximum size of a message.
In this chapter, we use a technique that we use in later chapters when looking at message queues, semaphores, and shared memory.
Since all of these IPC objects have at least kernel persistence (recall Section 1.3), we can write small programs that use these techniques, to let us experiment with them and learn more about their operation.
For example, we can write a program that creates a Posix message queue, write another program that adds a message to a Posix message queue, and write another that reads from one of these queues.
By writing messages with different priorities, we can see how these messages are returned by the m ~ r e c e i v e  function.
The m L o p e n  function creates a new message queue or opens an existing message queue.
I Returns: message queue descriptor if OK, -1 on error.
We describe the rules about the name argument in Section 2.2
When a new queue is created (0-CREAT is specified and the message queue does not already exist), the mode and attr arguments are required.
The attr argument lets us specify some attributes for the queue.
If this argument is a null pointer, the default attributes apply.
The return value from mLopen is called a message queue descriptor, but it need not be (and probably is not) a small integer like a file descriptor or a socket descriptor.
This value is used as the first argument to the remaining seven message queue functions.
In our sample implementation in Section 5.8, these descriptors are pointers to a structure.
An open message queue is closed by mLc 10s e.
The functionality is similar to the c lose  of an open file: the calling process can no longer use the descriptor, but the message queue is not removed from the system.
If the process terminates, all open message queues are closed, as if mLclose were called.
To remove a name that was used as an argument to mLopen from the system, m ~ u n l i n k  must be called.
Posix message queues have at least kernel persistence (recall Section 1.3)
We will see that if these message queues are implemented using memory-mapped files (Section 12.2), then they can have filesystem persistence, but this is not required and cannot be counted on.
Since Posix message queues have at least kernel persistence, we can write a set of small programs to manipulate these queues, providing an easy way to experiment with them.
The program in Figure 5.2 creates a message queue whose name is specified as the command-line argument.
Figure 5.2 Create a message queue with the exclusive-create flags specified.
We allow a -e option that specifies an exclusive create.
We say more about the getopt function and our Getopt wrapper with Figure 5.5
Upon return, getopt stores in optind the index of the next argument to be processed.
We call mcopen with the IPC name from the command-line, without calling our px-ipc-name function (Section 2.2)
This lets us see exactly how the implementation handles these Posix IPC names.
We do this with all our simple test programs throughout this book.
We call this version of our program mqcreatel, because we enhance it in Figure 5.5 after describing attributes.
The third file has the permissions that we specify with our FILE-MODE constant (read-write for the user, read-only for the group and other), but the other two files have different permissions.
We guess that the filename containing D contains the data, the filename containing L is some type of lock, and the filename containing P specifies the permissions.
Under Digital Unix 4.OB, we can see the actual pathname that is created.
Figure 5.3 is our mqunlink program, which removes a message queue from the system.
We can remove the message queue that was created by our mqcreate program.
All three files in the / tmp directory that were shown earlier are removed.
Each message queue has four attributes, all of which are returned by m ~ g e t a t t r  and one of which is set by m L s e t a t t r.
A pointer to one of these structures can be passed as the fourth argument to mLopen, allowing us to set both mLmaxmsg and m ~ m s g s i z e  when the queue is created.
The other two members of this structure are ignored by mLopen.
The other three members of the structure are ignored: the maximum number of messages per queue and the maximum number of bytes per message can be set only when the queue is created, and the number of messages currently on the queue can be fetched but not set.
The program in Figure 5.4 opens a specified message queue and prints its attributes.
Figure 5.4 Fetch and print the attributes of a message queue.
We can create a message queue and print its default attributes.
We can modify our program from Figure 5.2, allowing us to specify the maximum number of messages for the queue and the maximum size of each message.
We cannot specify one and not the other; both must be specified (but see Exercise 5.1)
To specify that a command-line option requires an argument, we specify a colon following the option character for the m and z options in the call to getopt.
When processing the option character, optarg points to the argument.
Our Getopt wrapper function calls the standard library's ge topt  function and terminates the process if ge topt  detects an error: encountering an option letter not included in the third argument, or an option letter without a required argument (indicated by an option letter followed by a colon)
In either case, ge topt  writes an error message to standard error and returns an error, which causes our Getopt wrapper to terminate.
For example, the following two errors are detected by getopt:
If neither of the two new options are specified, we must pass a null pointer as the final argument to mLopen, else we pass a pointer to our a t t r  structure.
These two functions place a message onto a queue and take a message off a queue.
Every message has a priority, which is an unsigned integer less than MQ-PRIO-MAX.
We show how to obtain these values with Figure 5.8
This operation of mq-receive differs from that of the System V msgrcv (Section 6.4)
System V messages have a type field, which is similar to the priority, but with msgrcv, we can specify three different scenarios as to which message is returned: the oldest message on the queue, the oldest message with a specific type, or the oldest message whose type is less than or equal to some value.
The first three arguments to both functions are similar to the first three arguments for w r i t e  and read, respectively.
Declaring the pointer argument to the buffer as a char* looks like a mistake.
The value of the len argument for m ~ r e c e i v e  must be at least as big as the maximum size of any message that can be added to this queue, the m q - m s g s i z e  member of the m ~ a t t r  structure for this queue.
If lm is smaller than this value, EMSGSIZE is returned immediately.
This means that most applications that use Posix message queues must call mq-getattr after opening the queue, to determine the maximum message size, and then allocate one or more read buffers of that size.
By requiring that the buffer always be large enough for any message on the queue, m ~ r e c e i v e  does not need to return a notification if the message is larger than the buffer.
If priop is a nonnull pointer for mareceive, the priority of the returned message is stored through this pointer.
This instance is one in which what is important is not what is said in the standard (i.e., Posix.l), but what is not said: nowhere is a 0-byte message forbidden.
One feature is missing from both Posix message queues and System V message queues: accurately identifying the sender of each message to the receiver.
Unfortunately, most IPC messaging mechanisms do not identify the sender.
In Section 15.5, we describe how doors provide this identity.
Section 14.8 of UNPvl describes how BSD/OS provides this identity when a Unix domain socket is used.
We cannot have the sender include its identity (e.g., its effective user ID) with the message, as we cannot trust the sender to tell the truth.
Although the access permissions on a message queue determine whether the sender is allowed to place a message onto the queue, this still does not identify the sender.
The possibility exists to create one queue per sender (which we talk about with regard to System V message queues in Section 6.8), but this does not scale well for large applications.
Lastly, realize that if the message queue functions are implemented entirely as user functions (as we show in Section 5.8), and not within the kernel, then we could not trust any sender identity that accompanied the message, as it would be easy to forge.
Figure 5.6 shows our program that adds a message to a queue.
Both the size of the message and its priority must be specified as command-line arguments.
The program in Figure 5.7 reads the next message from a queue.
We open the queue and then get its attributes by calling mcgetattr.
We need to determine the maximum message size, because we must allocate a buffer of this size for the call to m~receive.
We print the size of the message that is read and its priority.
Since n is a size-t datatype and we do not know whether this is an int or a long, we cast the value to be a long integer and use the %Id format string.
We can use these two programs to see how the priority field is used.
We can see that m ~ r e c e i v e  returns the oldest message of the highest priority.
We have already encountered two limits for any given queue, both of which are established when the queue is created:
No inherent limits exist on either value, although for the two implementations that we have looked at, room in the filesystem must exist for a file whose size is the product of these two numbers, plus some small amount of overhead.
Virtual memory requirements may also exist based on the size of the queue (see Exercise 5.5)
MQ-PRIO-MAX the maximum value plus one for the priority of any message (Posix requires that this be at least 32)
These two constants are often defined in the <unistd.h> header and can also be obtained at run time by calling the sysconf function, as we show next.
One problem that we will see with System V message queues in Chapter 6 is their inability to notify a process when a message is placed onto a queue.
We can block in a call to msgrcv, but that prevents us from doing anything else while we are waiting.
If we specify the nonblocking flag for msgrcv (IPC-NOWAIT), we do not block, but we must continually call this function to determine when a message arrives.
We said this is called polling and is a waste of CPU time.
We want a way for the system to tell us when a message is placed onto a queue that was previously empty.
This section and the remaining sections of this chapter contain advanced topics that you may want to skip on a first reading.
Posix message queues allow for an asynchronous event notification when a message is placed onto an empty message queue.
We establish this notification by calling mcnot i f y.
The sigevent structure is new with the Posix.1 realtime signals, which we say more about in the next section.
This structure and all of the new signal-related constants introduced in this chapter are defined by <signal.
We will show some examples shortly of the different ways to use this notification, but a few rules apply in general for this function.
If the notification argument is nonnull, then the process wants to be notified when a message arrives for the specified queue and the queue is empty.
We say that "the process is registered for notification for the queue."
If the notification argument is a null pointer and if the process is currently regis tered for notification for the queue, the existing registration is removed.
Only one process at any given time can be registered for notification for a given queue.
When a message arrives for a queue that was previously empty and a process is registered for notification for the queue, the notification is sent only if no thread is blocked in a call to m c r e c e i v e  for that queue.
That is, blocking in a call to m c r e c e i v e  takes precedence over any registration for notification.
When the notification is sent to the registered process, the registration is removed.
The process must reregister (if desired) by calling m c n o t  i f y again.
One of the original problems with Unix signals was that a signal's action was reset to its default each time the signal was generated (Section 10.4 of APUE)
Usually the first function called by a signal handler was s ignal ,  to reestablish the handler.
This provided a small window of time, between the signal's generation and the process reestablishing its signal handler, during which another occurrence of that signal could terminate the process.
At first glance, we seem to have a similar problem with m ~ n o t i f y ,  since the process must reregister each time the notification occurs.
But message queues are different from signals, because the notification cannot occur again until the queue is empty.
Therefore, we must be careful to reregister before reading the message from the queue.
Before getting into the details of Posix realtime signals or threads, we can write a simple program that causes SIGUSRl to be generated when a message is placed onto an empty queue.
We show this program in Figure 5.9 and note that this program contains an error that we talk about in detail shortly.
Figure 5.9 Generate SIGUSRl when message placed onto an empty queue (incorrect version)
We open the message queue, obtain its attributes, and allocate a read buffer.
We fill in the sigev-notify member of the sigevent structure with the SIGEV-SIGNAL constant, which says that.
We set the sigev-signo member to the signal that we want generated and call mcnot if y.
Our main function is then an infinite loop that goes to sleep in the pause function, which returns -1 each time a signal is caught.
Our signal handler calls m c n o  t i f y, to reregister for the next event, reads the message, and prints its length.
The re tu rn  statement at the end of sig-usrl is not needed, since there is no return value and falling off the end of the function is an implicit return to the caller.
Nevertheless, the author always codes an explicit r e tu rn  at the end of a signal handler to reiterate that the return from this function is special.
It might cause the premature return (with an error of EINTR) of a function call in the thread that handles the signal.
As expected, our mqnotifysigl program outputs SIGUSRl received, read 50 bytes.
We can verify that only one process at a time can be registered for the notification, by starting another copy of our program from another window:
The problem with Figure 5.9 is that it calls mcnotify, mcreceive, and printf from the signal handler.
None of these functions may be called from a signal handler.
Posix uses the term async-signal-safe to describe the functions that may be called from a signal handler.
Functions not listed may not be called from a signal handler.
Note that none of the standard I/O functions are listed and none of the pthread-XXX functions are listed.
Of all the IPC functions covered in this text, only semsost, read, and write are listed (we are assuming the latter two would be used with pipes and FIFOs)
One way to avoid calling any function from a signal handler is to have the handler just set a global flag that some thread examines to determine when a message has been received.
Figure 5.11 shows this technique, although it contains a different error, which we describe shortly.
Since the only operation performed by our signal handler is to set mqf lag nonzero, the global variables from Figure 5.9 need not be global.
Reducing the number of global variables is always a good technique, especially when threads are being used.
We open the message queue, obtain its attributes, and allocate a receive buffer.
We initialize three signal sets and turn on the bit for SIGUSRl in the set newmask.
We establish a signal handler for SIGUSRl, fill in our sigevent structure, and call mq-notify.
Figure 5.11 Signal handler just sets a flag for main thread (incorrect version)
We then test the global mqf lag in a loop, waiting for the signal handler to set it nonzero.
As long as it is 0, we call sigsuspend, which atomically puts the calling thread to sleep and resets its signal mask to zeromask (no signals are blocked)
When mqf lag is nonzero, we reregister and then read the message from the queue.
We then unblock SIGUSRl and go back to the top of the for loop.
We mentioned that a problem still exists with this solution.
Consider what happens if two messages arrive for the queue before the first message is read.
We can simulate this by adding a sleep before the call to mcnot i f y.
The fundamental problem is that the notification is sent only when a message is placed onto an empty queue.
If two messages arrive for a queue before we can read the first, only one notification is sent: we read the first message and then call sigsuspend waiting for another message, which may never be sent.
In the meantime, another message is already sitting on the queue waiting to be read that we are ignoring.
The correction to the problem just noted is to always read a message queue in a nonblocking mode when mcnotify is being used to generate a signal.
The first change is to specify 0-NONBLOCK when the message queue is opened.
The other change is to call m~receive in a loop, processing each message on the queue.
An error return of EAGAIN is OK and just means that no more messages exist.
Although the previous example is correct, it could be more efficient.
Our program blocks, waiting for a message to arrive, by calling sigsuspend.
When a message is placed onto an empty queue, the signal is generated, the main thread is stopped, the signal handler executes and sets the mqf lag variable, the main thread executes again, finds m c f  lag nonzero, and reads the message.
An easier approach (and probably more efficient) would be to block in a function just waiting for the signal to be delivered, without having the kernel execute a signal handler just to set a flag.
Figure 5.12 Using a signal notification to read a Posix message queue.
Before calling sigwait ,  we block some set of signals.
We specify this set of signals as the set argument.
This is called "synchronously waiting for an asynchronous event": we are using a signal but without an asynchronous signal handler.
Figure 5.13 shows the use of m a n o t i f y  with sigwait.
One signal set is initialized to contain just SIGUSR1, and this signal is then blocked by sigprocmask.
We now block, waiting for the signal, in a call to s igwait.
When SIGUSRl is delivered, we reregister the notification and read all available messages.
Indeed, looking at its function prototype, we see that its return value is 0 or one of the EXXX errors, which is the same as most of the Pthread functions.
But sigprocrnask cannot be used with a multithreaded process; instead, pthread-sigmask must be called, and it changes the signal mask of just the calling thread.
The arguments for pthread-sigmas k are identical to those for sigprocmask.
Two more variants of s igwai t  exist: s igwai t info  also returns a siginfo-t  structure (which we define in the next section) and is intended for use with reliable signals.
A message queue descriptor (an mqd-t variable) is not a "normal" descriptor and cannot be used with either s e l e c t  or p o l l  (Chapter 6 of UNPvl)
Nevertheless, we can use them along with a pipe and the m a n o t  i f y  function.
We show a similar technique in Section 6.9 with System V message queues, which involves a child process and a pipe.
First, notice from Figure 5.10 that the w r i t e  function is async-signal-safe, so we can call it from a signal handler.
This is an example of a pipe being used within a single process.
We initialize the descriptor set r s e t  and each time around the loop turn on the bit corresponding to p ipefd  [ 0 ] (the read end of the pipe)
We then call s e l e c t  waiting for only this descriptor, although in a typical application, this is where input or output on multiple descriptors would be multiplexed.
When the read end of the pipe is readable, we reregister the message queue notification and read all available messages.
Our signal handler just wr i tes  1 byte to the pipe.
Another alternative is to set sigev-notify to SIGEV-THREAD, which causes a new thread to be created.
The function specified by the s igev-no t i f y-f unc t i on is called with the parameter of sigev-value.
We specify a null pointer for the new thread's argument (sigev-value), so nothing is passed to the thread start function.
We could pass a pointer to the message queue descriptor as the argument, instead of declaring it as a global, but the new thread still needs the message queue attributes and the s igev  structure (to reregister)
We specify a null pointer for the new thread's attributes, so system defaults are used.
Unix signals have gone through numerous evolutionary changes over the past years.
Signals could get lost, and it was hard for a process to turn off selected signals while executing critical sections of code.
This work originated from the Posixlb realtime extensions (which was called Posix.4)
Almost every Unix system today provides Posix reliable signals, and newer systems are providing the Posix realtime signals.
We need to say more about the realtime signals, as we have already encountered some of the structures defined by this extension in the previous section (the s i gval and s igevent structures)
The realtime signals whose values are between SIGRTMIN and SIGRTMAX, inclusive.
Both implementations define SIGRTMIN and SIGRTMAX as macros that call sysconf, to allow their values to change in the future.
Next we note whether or not the new SA-SIGINFO flag is specified in the call to sigaction by the process that receives the signal.
These differences lead to the four possible scenarios shown in Figure 5.16
Figure 5.16 Realtime behavior of Posix signals, depending on SA-SIGINFO.
What we mean in the three boxes labeled "realtime behavior unspecified" is that some implementations may provide realtime behavior and some may not.
If we want realtime behavior, we must use the new realtime signals between SIGRTMIN and SIGRTMAX, and we must specify the SA-SIGINFO flag to sigaction when the signal handler is installed.
That is, if the signal is generated three times, it is delivered three times.
Furthermore, multiple occurrences of a given signal are queued in a first-in, first-out (FIFO) order.
For signals that are not queued, a signal that is generated three times can be delivered only once.
When multiple, unblocked signals in the range SIGRTMIN through SIGRTMAX are queued, lower-numbered signals are delivered before higher-numbered signals.
That is, SIGRTMIN is a "higher priority" than the signal numbered SIGRTMIN+l, which is a "higher priority" than the signal numbered SIGRTMIN+2, and so on.
When a nonrealtime signal is delivered, the only argument to the signal handler is the signal number.
The signal handler for a realtime signal that is installed with the SA-SIGINFO flag set is declared as.
Technically a nonrealtime Posix signal handler is called with just one argument.
Many Unix systems have an older, three-argument convention for signal handlers that predates the Posix realtime standard.
Some new functions are defined to work with the realtime signals.
For example, the sigqueue function is used instead of the kill function, to send a signal to some process, and the new function allows the sender to pass a sigval union with the signal.
The realtime signals are generated by the following Posix.1 features, identified by the si-code value contained in the siginf o-t structure that is passed to the signal handler.
SI-MESGQ The signal was generated when a message was placed onto an empty message queue, as we described in Section 5.6
SI-TIMER The signal was generated by the expiration of a timer that was set by the t imer-set time function, which we do not describe.
SI-USER The signal was sent by the ki 11 function.
If the signal was generated by some other event si-code will be set to some value other than the ones just shown.
The contents of the si-value member of the siginf o-t structure are valid only when si-code is SI-ASYNCIO, SI-MESGQ, SI-QUEUE, Or S I-TIMER.
Figure 5.17 is a simple program that demonstrates realtime signals.
The program calls fork, the child blocks three realtime signals, the parent then sends nine signals (three occurrences each of three realtime signals), and the child then unblocks the signals and we see how many occurrences of each signal are delivered and the order in which the signals are delivered.
We print the minimum and maximum realtime signal numbers, to see how many realtime signals the implementation supports.
We cast the two constants to an integer, because some implementations define these two constants to be macros that call sysconf, as in.
We call our s igna l - r t  function (which we show in Figure 5.18) to establish our function s ig - r t  as the handler for the three realtime signals.
This function sets the SA-SIGINFO flag, and since these three signals are realtime signals, we expect realtime behavior.
Wait for parent to generate the signals, then unblock the signals.
We wait 6 seconds to allow the parent to generate the nine signals.
We then call sigprocmask to unblock the three realtime signals.
This should allow all the queued signals to be delivered.
We pause for another 3 seconds, to let the signal handler call p r i n t  f nine times, and then the child terminates.
The parent pauses for 3 seconds to let the child block all signals.
We purposely generate the signals starting with the highest signal number, because we expect them to be delivered starting with the lowest signal number.
We also send a different integer value ( s iva l - in t )  with each signal, to verify that the three occurrences of a given signal are generated in FIFO order.
We first run the program under Solaris 2.6, but the output is not what is expected.
The nine signals are queued, but the three signals are generated starting with the highest signal number (we expect the lowest signal number to be generated first)
Then for a given signal, the queued signals appear to be delivered in LIFO, not FIFO, order.
We now run the program under Digital Unix 4.08 and see different results.
But for a given signal, the three occurrences are delivered in FIFO order.
We call this new function signal-rt and show it in Figure 5.18
We said earlier in this section that this is the function prototype for a signal handler installed with the SA-SIGINFO flag set.
The sigaction structure changed when realtime signal support was added, with the addition of the new sa-s igac t ion member.
If the SA-SIGINFO flag is set in the sa-flags member, then the sa-s igac t i on member specifies the address of the signal-handling function.
If the SA-SIGINFO flag is not set in the sa-flags member, then the sa-handler member specifies the address of the signal-handling function.
To specify the default action for a signal or to ignore a signal, set sa-handler to either SIG-DFL or SIG-IGN, and do not set SA-SIGINFO.
We now provide an implementation of Posix message queues using memory-mapped I/O, along with Posix mutexes and condition variables.
You may wish to skip this section until you have read those chapters.
Figure 5.19 shows a layout of the data structures that we use to implement Posix message queues.
In this figure, we assume that the message queue was created to hold up to four messages of 7 bytes each.
Our message queue descriptor is just a pointer to an mcinf o structure.
Each call to mcopen allocates one of these structures, and the pointer to this structure is what gets returned to the caller.
This reiterates that a message queue descriptor need not be a small integer, like a file descriptor-the only Posix requirement is that this datatype cannot be an array type.
Figure 5.19 Layout of data structures to implement Posix message queues using a memory-mapped file.
The mq_f lags member of the mqh-attr structure is not used, because the flags (the nonblocking flag is the only one defined) must be maintained on a per-open basis, not on a per-queue basis.
The flags are maintained in the m c i n f o structure.
We describe the remaining members of this structure as we use them in the various functions.
Note now that everything that we refer to as an index (the mqh-head and mqh-f ree members of this structure, and the msg-next member of the next structure) contains byte indexes from the beginning of the mapped file.
These indexes are used to maintain two linked lists in the mapped file: one list (mqh-head) contains all the messages currently on the queue, and the other (mqh-f ree) contains all the free messages on the queue.
We cannot use actual memory pointers (addresses) for these list pointers, because the mapped file can start at different memory addresses in each process that maps the file (as we show in Figure 13.6)
All messages are either on the message list or on the free list, and the msg-next member contains the index of the next message on the list (or 0 if this message is the end of the list)
One of these structures is dynamically allocated by meopen when a queue is opened, and freed by mec lose.
A pointer to this structure is the fundamental mqd-t datatype of our implementation, and this pointer is the return value from mq-open.
The mqi-magic member contains MQI-MAGIC, once this structure has been initialized and is checked by each function that is passed an mqd-t pointer, to make certain that the pointer really points to an m c i n f  o structure.
For alignment purposes, we want each message in the mapped file to start on a long integer boundary.
Figure 5.21 shows the first part of our meopen function, which creates a new message queue or opens an existing message queue.
The problem we encounter is on BSD/OS, which defines this datatype as an unsigned s h o r t  integer (occupying 16 bits)
Therefore, we must define our own datatype, va-mode-t, that is an integer under BSD/OS, or of type mode-t under other systems.
We turn off the user-execute bit in the mode variable (s-IXUSR) for reasons that we describe shortly.
A regular file is created with the name specified by the caller, and the user-execute bit is turned on.
If we were to just open the file, memory map its contents, and initialize the mapped file (as described shortly) when the 0-CREAT flag is specified by the caller, we would have a race condition.
A message queue is initialized by mcopen only if 0-CREAT is specified by the caller and the message queue does not already exist.
That means we need some method of detecting whether the message queue already exists.
To do so, we always specify 0-EXCL when we open the file that will be memory-mapped.
But an error return of EEXIST from open becomes an error from mcopen, only if the caller specified 0-EXCL.
The possible race condition is because our use of a memory-mapped file to represent a message queue requires two steps to initialize a new message queue: first, the file must be created by open, and second, the contents of the file (described shortly) must be initialized.
The problem occurs if two threads (in the same or different processes) call mcopen at about the same time.
One thread can create the file, and then the system switches to the second thread before the first thread completes the initialization.
This second thread detects that the file already exists (using the 0-EXCL flag to open) and immediately tries to use the message queue.
But the message queue cannot be used until the first thread initializes the message queue.
We use the user-execute bit of the file to indicate that the message queue ha~&een initialized.
This bit is enabled only by the thread that actually creates the file (using the 0-EXCL flag to detect which thread creates the file), and that thread initializes the message queue and then turns off the user-execute bit.
If the caller specifies the attributes, we verify that mq-maxmsg and rn~msgsize are positive.
Figure 5.22 Second part of mcopen function: complete initialization of new queue.
Set the file size a - 5 s  We calculate the size of each message, rounding up to the next multiple of the size.
To calculate the file size, we also allocate room for the mchdr structure at the beginning of the file and the msg-hdr structure at the beginning of each message (Figure 5.19)
Just calling f truncate (Section 13.3) would be easier, but we are not guaranteed that this works to increase the size of a file.
We allocate one mq-inf o structure for each call to mcopen.
Once the message queue is initialized, we turn off the user-execute bit.
We also close the file, since it has been memory mapped and there is no need to keep it open (taking up a descriptor)
Figure 5.23 shows the final part of our mcopen function, which opens an existing queue.
The return value is a pointer to the m c i n f  o structure that was allocated.
We are careful that the functions called to clean up after the error is detected do not affect the errno returned by this function.
Get pointers to structures ; The argument is validated, and pointers are then obtained to the memory-mapped region (mqhdr) and the attributes (in the m c h d r  structure)
I We call m c n o t  i f y to unregister the calling process for this queue.
If the process is registered, it will be unregistered, but if it is not registered, no error is returned.
Note that if the process terminates without calling m c c l o s e ,  the same operations take place on process termination: the memory-mapped file is unmapped and the memory is freed.
Figure 5.26 shows our m c r g e t a t t r  function, which returns the current attributes of the specified queue.
I We must acquire the message queue's mutex lock before fetching the attributes, in case some other thread is in the middle of changing them.
If the third argument is a nonnull pointer, we return the previous attributes and current status before changing anything.
The only attribute that can be changed with this function is m q _ f  lags, which we store in the m c i n f  o structure.
The mcnoti f y function shown in Figure 5.28 registers or unregisters the calling process for the queue.
We keep track of the process currently registered for a queue by storing its process ID in the mqh_pid member of the mchdr structure.
Only one process at a time can be registered for a given queue.
When a process registers itself, we also save its specified sigevent structure in the mqh-event structure.
Strangely, no error is specified if the calling process is not registered for this queue.
This performs the normal error checking, but does not send a signal and returns an error of ESRCH if the process does not exist.
An error of EBUSY is returned if the previously registered process still exists.
Otherwise, the process ID is saved, along with the caller's s igeven t  structure.
Our test for whether the previously registered process exists is not perfect.
This process can terminate and then have its process ID reused at some later time.
Figure 5.29 shows the first half of our m e s e n d  function.
Pointers are obtained to the structures that we will use, and the mutex lock for the queue is obtained.
A check is made that the size of the message does not exceed the maximum message size for this queue.
If we are placing a message onto an empty queue, we check whether any process is registered for this queue and whether any thread is blocked in a call to mcrece ive.
For the latter check, we will see that our m c r e c e i v e  function keeps a count (mqh-nwait) of the number of threads blocked on the empty queue.
If this counter is nonzero, we do not send any notification to the registered process.
We handle a notification of SIGEV-SIGNAL and call s igqueue to send the signal.
Calling sigqueue to send the signal results in an si-code of SI-QUEUE being passed to the signal handler in the siginfo-t  structure (Section 5.7), which is incorrect.
Generating the correct si-code of SI-MESGQ from a user process is implementation dependent.
Otherwise, we wait on the condition variable mqh-wait, which we will see is signaled by our m ~ r e c e i v e  function when a message is read from a full queue.
Our implementation is simplistic with regard to returning an error of EINTR if this call to -send is interrupted by a signal that is caught by the calling process.
The problem is that pthread-cond-wait does not return an error when the signal handler returns: it can either return a value of 0 (which appears as a spurious wakeup) or it need not return at all.
Figure 5.30 shows the second half of our m e s e n d  function.
At this point, we know the queue has room for the new message.
The priority and length are stored in its m s g- h d r  structure, and then the contents of the message are copied from the caller.
When a new message is added to the queue and one or more messages of the same priority are already on the queue, the new message is added after the last message with its priority.
Using this ordering, m c r e c e i v e always returns the first message on the linked list (which is the oldest message of the highest priority on the queue)
As we step through the linked list, pmsghdr contains the address of the previous message in the list, because its msg-next value will contain the index of the new message.
Our design can be slow when lots of messages are on the queue, forcing a traversal of a large number of l i t  entries each time a message is written to the queue.
A separate index could be maintained that remembers the location of the last message for each possible priority.
The number of messages currently on the queue, mccurmsgs, is incremented.
Figure 5.31 shows the first half of our m e r e c e i v e  function, which sets up the pointers that it needs, obtains the mutex lock, and verifies that the caller's buffer is large enough for the largest possible message.
If the queue is empty and the 0-NONBLOCK flag is set, an error of EAGAIN is returned.
Otherwise, we increment the queue's mqh-nwait counter, which was examined by our m e s e n d  function in Figure 5.29, if the queue was empty and someone was registered for notification.
We then wait on the condition variable, which is signaled by m c s e n d  in Figure 5.29
As with our implementation of m c s e n d ,  our implementation of m ~ r e c e i v e  is simplistic with regard to returning an error of EINTR if this call is interrupted by a signal that is caught by the calling process.
Figure 5.32 shows the second half of our m c r e c e i v e  function.
At this point, we know that a message is on the queue to return to the caller.
The space occupied by this message becomes the new head of the free list.
Messages are placed onto a queue with mcsend  and read with m ~ r e c e i v e.
Attributes of the queue can be queried and set with m ~ g e t a t t r  and m L s e t a t t r ,  and the function m ~ n o t i f y  lets us register a signal to be sent, or a thread to be invoked, when a message is placed onto an empty queue.
Small integer priorities are assigned to each message on the queue, and m ~ r e c e i v e  always returns the oldest message of the highest priority each time it is called.
Using m a n o t i f y  introduced us  to the Posix realtime signals, named SIGRTMIN through SIGRTMAX.
Finally, w e  implemented most of the Posix message queue features in about 500 lines of C code, using memory-mapped I/O, along with a Posix mutex and a Posix condition variable.
This implementation showed a race condition dealing with the creation of a new queue; we  will encounter this same race condition in Chapter 10 when implementing Posix semaphores.
Exercises With Figure 5.5, we said that if the attr argument to m e o p e n  is nonnull when a new queue is created, both of the members m c m a x m s g  and m c m s g s i z e  must be specified.
Then send two messages to the queue and verify that the signal is not generated for the second message.
Why? Modify Figure 5.9 so that it does not read the message from the queue when the signal is delivered.
Instead, just call m ~ n o t i f y  and print that the signal was received.
Then send two messages to the queue and verify that the signal is not generated for the second message.
After m e o p e n  returns, print another message, sleep for 30 seconds, and then call m e c l o s e.
Compile and run the program, specifying a large number of messages (a few hundred thousand) and a maximum message size of (say) 10 bytes.
The goal is to create a large message queue (megabytes) and then see whether the implementation uses memory-mapped files.
During the first 30-second pause, run a program such as ps and look at the memory size of the program.
Do this again, after m e o p e n  has returned.
How many message queues are needed for two-way communication between a parent and child? In Figure 5.24, why don't we destroy the mutex and condition variable? Posix says that a message queue descriptor cannot be an array type.
Why? Where does the m a i n  function in Figure 5.14 spend most of its time? What happens every time a signal is delivered? How do we handle this scenario?
System V message queues are identified by a message queue identifier.
Any process with adequate privileges (Section 3.5) can place a message onto a given queue, and any process with adequate privileges can read a message from a given queue.
As with Posix message queues, there is no requirement that some process be waiting for a message to arrive on a queue before some process writes a message to that queue.
For every message queue in the system, the kernel maintains the following structure of information, defined by including <sys /msg.
Unix 98 does not require the msg-f irst, msg-last, or msg-cbytes members.
Nevertheless, these three members are found in the common System V derived implementations.
Naturally, no requirement exists that the messages on a queue be maintained as a linked list, as implied by the msg-f irst and msg-last members.
If these two pointers are present, they point to kernel memory and are largely useless to an application.
We can picture a particular message queue in the kernel as a linked list of messages, as shown in Figure 6.1
In this chapter, we look at the functions for manipulating System V message queues and implement our file server example from Section 4.2 using message queues.
A new message queue is created, or an existing message queue is accessed with the msgget function.
I int msgget (key-t key, int oflag) ; I I Returns: nonnegative identifier if OK, -1 on error I The return value is an integer identifier that is used in the other three msg functions to refer to this queue, based on the specified key, which can be a value returned by f tok  or the constant IPC-PRIVATE, as shown in Figure 3.3
This can be bitwise-ORed with either IPC-CREAT or IPC-CREAT I IPC-EXCL, as discussed with Figure 3.4
When a new message queue is created, the following members of the msqid-ds structure are initialized:
The uid and cuid members of the msgserm structure are set to the effective user ID of the process, and the gid and cgid members are set to the effective group ID of the process.
The read-write permission bits in oflag are stored in msgserm .mode.
Once a message queue is opened by msgget, we put a message onto the queue using msgsnd.
The message type must be greater than 0, since nonpositive message types are used as a special indicator to the msgrcv function, which we describe in the next section.
The name mtext in the msgbuf structure definition is a misnomer; the data portion of the message is not restricted to text.
Any form of data is allowed, binary data or text.
The kernel does not interpret the contents of the message data at all.
We use the term "template" to describe this structure, because what ptr points to is just a long integer containing the message type, immediately followed by the message itself (if the length of the message is greater than 0 bytes)
But most applications do not use this definition of the msgbuf structure, since the amount of data (1 byte) is normally inadequate.
No compile-time limit exists to the amount of data in a message (this limit can often be changed by the system administrator), so rather than declare a structure with a huge amount of data (more data than a given implementation may support), this template is defined instead.
Most applications then define their own message structure, with the data portion defined by the needs of the application.
The length argument to msgsnd specifies the length of the message in bytes.
This is the length of the user-defined data that follows the long integer message type.
In the example just shown, the length could be passed as sizeof (Message) - sizeof (long)
This flag makes the call to msgsnd nonblocking: the function returns immediately if no room is available for the new message.
If one of these two conditions exists and if IPC-NOWAIT is specified, msgsnd returns an error of EAGAIN.
If one of these two conditions exists and if IPC-NOWAIT is not specified, then the thread is put to sleep until.
A message is read from a message queue using the msgrcv function.
Returns: number of bytes of data read into buffer if OK, -1 on error.
The ptr argument specifies where the received message is to be stored.
As with msgsnd, this pointer points to the long integer type field (Figure 4.26) that is returned immediately before the actual message data.
This is the maximum amount of data that is returned by the function.
If type is 0, the first message on the queue is returned.
Since each message queue is maintained as a FIFO list (first-in, first-out), a type of 0 specifies that the oldest message on the queue is to be returned.
If type is greater than 0, the first message whose type equals type is returned.
If type is less than 0, the first message with the lowest type that is less than or equal to the absolute value of the type argument is returned.
Consider the message queue example shown in Figure 6.1, which contains three messages:
Figure 6.2 shows the message returned for different values of type.
Figure 6.2 Messages returned by msgrcv for different values of type.
The flag argument specifies what to do if a message of the requested type is not on the queue.
If the IPC-NOWAIT bit is set and no message is available, the msgrcv function returns immediately with an error of ENOMSG.
Otherwise, the caller is blocked until one of the following occurs:
An additional bit in the flag argument can be specified: MSG-NOERROR.
When set, this specifies that if the actual data portion of the received message is greater than the length argument, just truncate the data portion and return without an error.
Not specifying the MSG-NOERROR flag causes an error return of E2BIG if length is not large enough to receive the entire message.
On successful return, msgrcv returns the number of bytes of data in the received message.
This does not include the bytes needed for the long integer message type that is also returned through the ptr argument.
The msgct 1 function provides a variety of control operations on a message queue.
Remove the message queue specified by msqid from the system.
We have already seen an example of this operation in Figure 3.7
The third argument to the function is ignored for this command.
Set the following four members of the msqid-ds structure for the message queue from the corresponding members in the structure pointed to by the buff argument: msgserm.
Return to the caller (through the buff argument) the current msqid-ds structure for the specified message queue.
On this system there is a limit of 4096 bytes per message queue.
Since System V message queues are kernel-persistent, we can write a small set of programs to manipulate these queues, and see what happens.
Figure 6.4 shows our msgcreate program, which creates a message queue.
We allow a command-line option of -e to specify the IPC-EXCL flag.
The pathname that is required as a command-line argument is passed as an argument to f tok.
The resulting key is converted into an identifier by msgget.
Our msgsnd program is shown in Figure 6.5, and it places one message of a specified length and type onto a queue.
We allocate a pointer to a generic msgbuf structure but then allocate the actual structure (e.g., the output buffer) by calling calloc, based on the size of the message.
Figure 6.6 shows our msgrcv function, which reads a message from a queue.
An optional -n argument specifies nonblocking, and an optional - t argument specifies the type argument for msgrcv.
Figure 6.6 Read a message from a System V message queue.
No simple way exists to determine the maximum size of a message (we talk about this and other limits in Section 6.10), so we define our own constant for this limit.
To remove a message queue, we call msgctl with a command of IPc-RMID, as shown in Figure 6.7
We now use the four programs that we have just shown.
We first create a message queue and write three messages to the queue.
We first try to create a message queue using a pathname that does not exist.
This demonstrates that the pathname argument for f tok must exist.
We then create the file /tmp/testl and create a message queue using this pathname.
We next demonstrate the use of the type argument to msgrcv in reading the messages in an order other than FIFO.
The last execution of our msgrcv program shows the IPC-NOWAIT flag.
What happens if we specify a positive type argument to msgrcv but no message with that type exists on the queue?
When we ask for a message of type 999, the program blocks (in the call to msgrcv), waiting for a message of that type to be placed onto the queue.
We interrupt this by terminating the program with our interrupt key.
We then specify the -n flag to prevent blocking, and see that the error ENOMSG is returned in this scenario.
We then remove the queue from the system with our msgrmid program.
We could have removed the queue using the system-provided command.
We now demonstrate that to access a System V message queue, we need not call msgget: all we need to know is the message queue identifier (easily obtained with ipcs) and read permission for the queue.
Instead, the caller specifies the message queue identifier on the command line.
We obtain the identifier of 150 from ipcs, and this is the command-line argument to our msgrcvid program.
We now code our client-server example from Section 4.2 to use two message queues.
One queue is for messages from the client to the server, and the other queue is for messages in the other direction.
We include our standard header and define the keys for each message queue.
The main function for the server is shown in Figure 6.10
Both message queues are created and if either already exists, it is OK, because we do not specify the IPC-EXCL flag.
The server function is the one shown in Figure 4.30 that calls our mesg-send and mesg-recv functions, versions of which we show shortly.
The two message queues are opened and our client function from Figure 4.29 is called.
This function calls our mesg-send and mesg-recv functions, which we show next.
Both the client and server functions use the message format shown in Figure 4.25
These two functions also call our mesg-send and mesg-recv functions.
Notice that the arguments to these two functions do not change from the versions that called write and read, because the first integer argument can contain either an integer descriptor (for a pipe or FIFO) or an integer message queue identifier.
Two features are provided by the type field that is associated with each message on a queue:
The type field can be used to identify the messages, allowing multiple processes to multiplex messages onto a single queue.
One value of the type field is used for messages from the clients to the server, and a different value that is unique for each client is used for messages from the server to the clients.
Naturally, the process ID of the client can be used as the type field that is unique for each client.
The type field can be used as a priority field.
This lets the receiver read the messages in an order other than first-in, first-out (FIFO)
With pipes and FIFOs, the data must be read in the order in which it was written.
With System V message queues, we can read the messages in any order that is consistent with the values we associate with the message types.
Furthermore, we can call msgrcv with the IPC-NOWAIT flag to read any messages of a given type from the queue, but return immediately if no messages of the specified type exist.
Recall our simple example of a server process and a single client process.
With either pipes or FIFOs, two IPC channels are required to exchange data in both directions, since these types of IPC are unidirectional.
With a message queue, a single queue can be used, having the type of each message signify whether the message is from the client to the server, or vice versa.
Here we can use a type of 1, say, to indicate a message from any client to the server.
If the client passes its process ID as part of the message, the server can send its messages to the client processes, using the client's process ID as the message type.
Each client then specifies its process ID as the type argument to msgrcv.
Figure 6.14 shows how a single message queue can be used to multiplex these messages between multiple clients and one server.
Figure 6.14 Multiplexing messages between multiple clients and one server.
A potential for deadlock always exists when one IPC channel is used by both the clients and the server.
Clients can fill up the message queue (in this example), preventing the server from sending a reply.
The clients are then blocked in rnsgsnd, as is the server.
One convention that can detect this deadlock is for the server to always use a nonblocking write to the message queue.
We now redo our client-server example using a single message queue with different message types for messages in each direction.
These programs use the convention that messages with a type of 1 are from the client to the server, and all other messages have a type equal to the process ID of the client.
This client-server requires that the client request contain the client's process ID along with the pathname, similar to what we did in Section 4.8
Only one message queue is created, and if it already exists, it is OK.
The same message queue identifier is used for both arguments to the server function.
The server function does all the server processing, and is shown in Figure 6.16
Notice that the process ID sent by the client is used as the message type for all messages sent by the server to the client.
Also, this server is an infinite loop that is called once and never returns, reading each client request and sending back the replies.
Our server is an iterative server, as we discussed in Section 4.9
The client opens the message queue, which the server must have already created.
The client function shown in Figure 6.18 does all of the processing for our client.
Note that the type of messages requested from mesg-recv equals the process ID of the client.
We now modify the previous example to use one queue for all the client requests to the server and one queue per client for that client's responses.
Figure 6.19 One queue per server and one queue per client.
The server's queue has a key that is well-known to the clients, but each client creates its own queue with a key of IPC-PRIVATE.
Instead of passing its process ID with the request, each client passes the identifier of its private queue to the server, and the server sends its reply to the client's queue.
We also write this server as a concurrent server, with one fork  per client.
One potential problem with this design occurs if a client dies, in which case, messages may be left in its private queue forever (or at least until the kernel reboots or someone explicitly deletes the queue)
The following headers and functions do not change from previous versions: mesg.
We open the server's well-known queue (MQ- KEY^) and then create our own queue with a key of IPC-PRIVATE.
The two queue identifiers become the arguments to the client function (Figure 6.21)
When the client is done, its private queue is removed.
Figure 6.21 is the c l i e n t  function.
This function is nearly identical to Figure 6.18, but instead of passing the client's process ID as part of the request, the identifier of the client's private queue is passed instead.
The message type in the mesg structure is also left as 1, because that is the type used for messages in both directions.
Figure 6.23 is the s e r v e r  function.
The main change from Figure 6.16 is writing this function as an infinite loop that calls f o r k  for each client request.
Since we are spawning a child for each client, we must worry about zombie processes.
Here we establish a signal handler for the SIGCHLD signal, and our function sig-chld (Figure 6.22) is called when a child terminates.
The server parent blocks in the call to mesg-recv waiting for the next client message to arrive.
We purposely put the call to fopen in the child, instead of the parent, just in case the file is on a remote filesystem, in which case, the opening of the file could take some time if any network problems occur.
Our handler for the SIGCHLD function is shown in Figure 6.22
Each time our signal handler is called, it calls wa i tp id  in a loop, fetching the termination status of any children that have terminated.
This can create a problem, because the parent process spends most of its time blocked in a call to msgrcv in the function mesg-recv (Figure 6.13)
When our signal handler returns, this call to msgrcv is interrupted.
We must handle this interrupted system call, and Figure 6.24 shows the new version of our Mesg-recv wrapper function.
We allow an error of EINTR from mesg-recv (which just calls msgrcv), and when this happens, we just call mesg-recv again.
Message Queues with select and poll One problem with System V message queues is that they are known by their own identifiers, and not by descriptors.
Actually, one version of Unix, IBM's AIX, extends s e l e c t  to handle System V message queues in addition to descriptors.
This missing feature is often uncovered when someone wants to write a server that handles both network connections and IPC connections.
Network communications using either the sockets API or the XTI API (UNPvl) use descriptors, allowing either select or p o l l  to be used.
Pipes and FIFOs also work with these two functions, because they too are identified by descriptors.
One solution to this problem is for the server to create a pipe and then spawn a child, with the child blocking in a call to msgrcv.
When a message is ready to be processed, msgrcv returns, and the child reads the message from the queue and writes the message to the pipe.
The server parent can then select  on the pipe, in addition to some network connections.
The downside is that these messages are then processed three times: once when read by the child using msgrcv, again when written to the pipe by the child, and again when read from the pipe by the parent.
To avoid this extra processing, the parent could create a shared memory segment that is shared between itself and the child, and then use the pipe as a flag between the parent and child (Exercise 12.5)
In Figure 5.14 we showed a solution using Posix message queues that did not require a fork.
We can use a single process with Posix message queues, because they provide a notification capability that generates a signal when a message arrives for an empty queue.
System V message queues do not provide this capability, so we must fork  a child and have the child block in a call to msgrcv.
If such a facility were provided, then the parent-child scenario just described (to get around the select problem) could be made more efficient by having the child specify the peek flag to msgrcv and just write 1 byte to the pipe when a message was ready, and let the parent read the message.
As we noted in Section 3.8, certain system limits often exist on message queues.
The first column is the traditional System V name for the kernel variable that contains this limit.
Figure 6.25 Typical system limits for System V message queues.
The intent of this section is to show some typical values, to aid in planning for portability.
When a system runs applications that make heavy use of message queues, kernel tuning of these (or similar) parameters is normally required (which we described in Section 3.8)
How many messages of varying size can be put onto a queue?
Next we start with 8-byte messages and see how many can be placed onto a given queue.
Once we determine this limit, we delete the queue (discarding all these messages) and try again with 16-byte messages.
We keep doing so until we pass the maximum message size that was determined in the first step.
We expect smaller messages to encounter a limit on the total number of messages per queue and larger messages to encounter a limit on the total number of bytes per queue.
Normally a system limit exists on the maximum number of message queue identifiers that can be open at any time.
We determine this by just creating queues until msgget fails.
System V message queues are similar to Posix message queues.
New applications should consider using Posix message queues, but lots of existing code uses System V message queues.
Nevertheless, recoding an application to use Posix message queues, instead of System V message queues, should not be hard.
The main feature missing from Posix message queues is the ability to read messages of a specified priority from the queue.
Neither form of message queue uses real descriptors, making it hard to use either select or p o l l  with a message queue.
What changes must then be made to the remaining programs in Section 6.6?
This chapter begins our discussion of synchronization: how to synchronize the actions of multiple threads or multiple processes.
Synchronization is normally needed to allow the sharing of data between threads or processes.
Mutexes and condition variables are the building blocks of synchronization.
Mutexes and condition variables are h m  the Posix.1 threads standard, and can always be used to synchronize the various threads within a process.
Posix also allows a mutex or condition variable to be used for synchronization between multiple processes, if the mutex or condition variable is stored in memory that is shared between the processes.
In this chapter, we introduce the classic producer-consumer problem and use mutexes and condition variables in our solution of this problem.
We use multiple threads for this example, instead of multiple processes, because having multiple threads share the common data buffer that is assumed in this problem is trivial, whereas sharing a common data buffer between multiple processes requires some form of shared memory (which we do not describe until Part 4)
We provide additional solutions to this problem in Chapter 10 using semaphores.
A mutex, which stands for mutual exclusion, is the most basic form of synchronization.
A mutex is used to protect a critical region, to make certain that only one thread at a time.
The normal outline of code to protect a critical region looks like.
Since only one thread at a time can lock a given mutex, this guarantees that only one thread at a time can be executing the instructions within the critical region.
If we try to lock a mutex that is already locked by some other thread, pthread-mutex-lock blocks until the mutex is unlocked.
If multiple threads are blocked waiting for a mutex, which thread runs when the mutex is unlocked? One of the features added by the 1003.1b-1993 standard is an option for priority scheduling.
We do not cover this area, but suffice it to say that different threads can be assigned different priorities, and the synchronization functions (mutexes, read-write locks, and semaphores) will wake up the highest priority thread that is blocked.
Although we talk of a critical region being protected by a mutex, what is really p r c tected is the data being manipulated within the critical region.
That is, a mutex is normally used to protect shared data that is being shared between multiple threads or between multiple processes.
That is, if the shared data is a linked list (for example), then all the threads that manipulate the linked list must obtain the mutex lock before manipulating the l i t.
Nothing can prevent one thread from manipulating the linked list without first obtaining the mutex.
One of the classic problems in synchronization is called the producer-consumer problem, also known as the bounded buffer problem.
One or more producers (threads or processes) are creating data items that are then processed by one or more consumers (threads or processes)
The data items are passed between the producers and consumers using some type of IPC.
We deal with this problem all the time with Unix pipes.
A Unix pipe is used as the form of IPC.
The required synchronization between the producer and consumer is handled by the kernel in the way in which it handles the wr i t e s  by the producer and the reads by the consumer.
If the producer gets ahead of the consumer (i.e., the pipe fills up), the kernel puts the producer to sleep when it calls wri te , until more room is in the pipe.
If the consumer gets ahead of the producer (i.e., the pipe is empty), the kernel puts the consumer to sleep when it calls read, until some data is in the pipe.
This type of synchronization is implicit; that is, the producer and consumer are not even aware that it is being performed by the kernel.
If we were to use a Posix or System V message queue as the form of IPC between the producer and consumer, the kernel would again handle the synchronization.
When shared memory is being used as the form of IPC between the producer and the consumer, however, some type of explicit synchronization must be performed by the producers and consumers.
The example that we use is shown in Figure 7.1
Figure 7.1 Producer-consumer example: multiple producer threads, one consumer thread.
We have multiple producer threads and a single consumer thread, in a single process.
The integer array buff contains the items being produced and consumed (i.e., the shared data)
The consumer just goes through this array and verifies that each entry is correct.
In this first example, we concern ourselves only with synchronization between the multiple producer threads.
We do not start the consumer thread until all the producers are done.
Globals shared between the threads -12 These variables are shared between the threads.
We allocate this structure and initialize the mutex that is used for synchronization between the producer threads.
We will always try to collect shared data with their synchronization variables (mutex, condition variable, or semaphore) into a structure as we have done here, as a good programming technique.
In many cases, however, the shared data is dynamically allocated, say as a linked list.
We might be able to store the head of the linked list in a structure with the synchronization variables (as we did with our mchdr  structure in Figure 5.20), but other shared data (the rest of the list) is not in the structure.
Command-line arguments -22 The first command-line argument specifies the number of items for the producers to.
Under Solaris 2.6, this is just a call to thr-setconcurrency and is required if we want the multiple producer threads to each have a chance to execute.
If we omit this call under Solaris, only the first producer thread runs.
Under Digital Unix 4.OB, our set-concurrency function does nothing (because all the threads within a process compete for the processor by default)
This function is needed with threads implementations that multiplex user threads (what we create with pthread-create) onto a smaller set of kernel execution entities (e.g., kernel threads)
These are commonly referred to as many-to-few, two-level, or M-to-N implernentations.
Create producer threads -2s The producer threads are created, and each executes the function produce.
The argument to each producer thread is a pointer to an element of the count array.
We first initialize the counter to 0, and each thread then increments this counter each time it stores an item in the buffer.
We print this array of counters when we are done, to see how many items were stored by each producer thread.
We wait for all the producer threads to terminate, also printing each thread's counter, and only then start a single consumer thread.
This is how (for the time being) we avoid any synchronization issues between the producers and consumer.
We wait for the consumer to finish and then terminate the process.
Figure 7.3 shows the produce and consume functions for our example.
We protect this region with a mutex lock, being certain to unlock the mutex when we are done.
Notice that the increment of the count element (through the pointer arg) is not part of the critical region because each thread has its own counter (the count array in the main function)
Therefore, we do not include this line of code within the region locked by the mutex, because as a general programming principle, we should always strive to minimize the amount of code that is locked by a mutex.
If we run the program just described, specifying one million items and five producer threads, we have.
If we remove the mutex locking from this example, it fails, as expected.
We can also verify that the removal of the mutex locking has no effect if only one producer thread is run.
We now demonstrate that mutexes are for locking and cannot be used for waiting.
We modify our producer-consumer example from the previous section to start the consumer thread right after all the producer threads have been started.
This lets the consumer thread process the data as it is being generated by the producer threads, unlike Figure 7.2, in which we did not start the consumer until all the producer threads were finished.
But we must now synchronize the consumer with the producers to make certain that the consumer processes only data items that have already been stored by the producers.
All the lines prior to the declaration of main have not changed from Figure 7.2
Figure 7.4 main function: start consumer immediately after starting producers.
We increase the concurrency level by one, to account for the additional consumer thread.
We create the consumer thread immediately after creating the producer threads.
We show in Figure 7.5 the consume function, which calls our new consume-wait function.
Wait for producers -64 Our consume-wait function must wait until the producers have generated the ith.
To check this condition, the producer's mutex is locked and i is compared to the producer's nput index.
We must acquire the mutex lock before looking at nput, since this variable may be in the process of being updated by one of the producer threads.
The fundamental problem is: what can we do when the desired item is not ready? All we do in Figure 7.5 is loop around again, unlocking and locking the mutex each time.
This is calling spinning or polling and is a waste of CPU time.
We could also sleep for a short amount of time, but we do not know how long to sleep.
What is needed is another type of synchronization that lets a thread (or process) sleep until some event occurs.
A mutex is for locking and a condition variable is for waiting.
These are two different types of synchronization and both are needed.
A condition variable is a variable of type pthread-cond-t, and the following two functions are used with these variables.
Both return: 0 if OK, positive Exxx value on error.
The term "signal" in the second function's name does not refer to a Unix SIGxxx signal.
We choose what defines the "condition" to wait for and be notified of: we test this.
We explain the use of condition variables by recoding the example from the previous section.
The two variables nput and nval are associated with the mutex, and we put all three variables into a structure named put.
The next structure, nready, contains a counter, a condition variable, and a mutex.
The produce and consume functions do change, and we show them in Figure 7.7
We now use the mutex put  .mutex to lock the critical section when the producer places a new item into the array.
We increment the counter nready .nready, which counts the number of items ready for the consumer to process.
We can now see the inter action of the mutex and condition variable associated with this counter.
The counter is shared between the producers and the consumer, so access to it must be when the associated mutex (nready.mutex) is locked.
Since this counter is shared among all the producers and the consumer, we can test its value only while we have its associated mutex locked.
If, while we have the mutex locked, the value is 0, we call pthread-cond-wait to go to sleep.
Notice that when pthread-cond-wait returns, we always test the condition again, because spurious wakeups can occur: a wakeup when the desired condition is still not true.
Implementations try to minimize the number of these spurious wakeups, but they can still occur.
In our example, the variable that maintains the condition was an integer counter, and setting the condition was just incrementing the counter.
The code that tests the condition and goes to sleep waiting for the condition to be true normally looks like the following:
In a worst-case scenario, we could imagine the system immediately scheduling the thread that is signaled; that thread runs and then immediately stops, because it cannot acquire the mutex.
An alternative to our code in Figure 7.7 would be.
Here we do not signal the condition variable until we release the mutex.
When a writer is finished with a lock, it wants to awaken all queued readers, because multiple readers are allowed at the same time.
An alternate (and safer) way of thinking about a signal versus a broadcast is that you can always use a broadcast.
A signal is an optimization for the cases in which you know that all the waiters are properly coded, only one waiter needs to be awakened, and which waiter is awakened does not matter.
This structure specifies the system time when the function must return, even if the condition variable has not been signaled yet.
This time value is an absolute time; it is not a time delta.
This differs from se lec t ,  pse lec t ,  and p o l l  (Chapter 6 of UNPvl), which all specify some number of fractional seconds in the future when the function should return.
The advantage in using an absolute time, instead of a delta time, is if the function prematurely returns (perhaps because of a caught signal): the function can be called again, without having to change the contents of the timespec structure.
Our examples in this chapter of mutexes and condition variables have stored them as globals in a process in which they are used for synchronization between the threads within that process.
Mutexes and condition variables initialized in this fashion assume the default attributes, but we can initialize these with other than the default attributes.
First, a mutex or condition variable is initialized or destroyed with the following functions:
All four return: 0 if OK, positive Exxx value on error.
Considering a mutex, mptr must point to a pthread-mutex-t variable that has been allocated, and pthread-mutex-init initializes that mutex.
The pthread-mut exat  t r-t value, pointed to by the second argument to pthread-mutex-init (attr), specifies the attributes.
If this argument is a null pointer, the default attributes are used.
Once a mutex attribute or a condition variable attribute has been initialized, separate functions are called to enable or disable certain attributes.
For example, one attribute that we will use in later chapters specifies that the mutex or condition variable is to be shared between different processes, not just between different threads within a single process.
This attribute is fetched or stored with the following functions.
All four return: 0 if OK, positive Exxx value on error.
The two g e t  functions return the current value of this attribute in the integer pointed to by valptr and the two set functions set the current value of this attribute, depending on value.
The latter is also referred to as the process-shared attribute.
The following code fragment shows how to initialize a mutex so that it can be shared between processes:
The amount of shared memory that must be allocated for the mutex is s i zeo f  (pthread-mutex-t)
We showed examples of these process-shared mutexes and condition variables in Figure 5.22
When a mutex is shared between processes, there is always a chance that the process can terminate (perhaps involuntarily) while holding the mutex lock.
There is no way to have the system automatically release held locks upon process termination.
We will see that read-write locks and Posix semaphores share this property.
The only type of synchronization locks that the kernel always cleans up automatically upon process termination is f c n t l  record locks (Chapter 9)
When using System V semaphores, the application chooses whether a semaphore lock is automatically cleaned up or not by the kernel upon process termination (the SEM-UNDO feature that we talk about in Section 11.3)
A thread can also terminate while holding a mutex lock, by being canceled by another thread, or by calling pthread-exit.
The latter should be of no concern, because the thread should know that it holds a mutex lock if it voluntarily terminates by calling pthread-exit.
In the case of cancellation, the thread can install cleanup handlers that are called upon cancellation, which we demonstrate in Section 8.5
Fatal conditions for a thread normally result in termination of the entire process.
For example, if a thread makes an invalid pointer reference, generating SIGSEGV, this terminates the entire process if the signal is not caught, and we are back to the previous condition dealing with the termination of the process.
Even if the system were to release a lock automatically when a process terminates, this may not solve the problem.
The lock was protecting a critical region probably while some data was being updated.
If the process terminates while it is in the middle of this critical region, what is the state of the data? A good chance exists that the data has some inconsistencies: for example, a new item may have been only partially entered into a linked list.
If the kernel were to just unlock the mutex when the process terminates, the next process to use the linked list could find it corrupted.
In some examples, however, having the kernel clean up a lock (or a counter in the case of a semaphore) when the process terminates is OK.
For example, a server might use a System V semaphore (with the SEM-UNDO feature) to count the number of clients currently being serviced.
Each time a child is forked, it increments this semaphore, and when the child terminates, it decrements this semaphore.
If the child terminates abnormally, the kernel will still decrement the semaphore.
An example of when it is OK for the kernel to release a lock (not a counter as we just described) is shown in Section 9.7
The daemon obtains a write Lock on one of its data files and holds this lock as long as it is running.
Should someone try to start another copy of the daemon, the new copy will terminate when it cannot get the write lock, guaranteeing that only one copy of the daemon is ever running.
But should the daemon terminate abnormally, the kernel releases the write lock, allowing another copy to be started.
Mutexes are used to protect critical regions of code, so that only one thread at a time is executing within the critical region.
When this happens, the thread waits on a condition variable.
The pthread-cond-wait function that puts the thread to sleep unlocks the mutex before putting the thread to sleep and relocks the mutex before waking up the thread at some later time.
Mutexes and condition variables can be statically allocated and statically initialized.
They can also be dynamically allocated, which requires that they be dynamically initialized.
Dynamic initialization allows us to specify the process-shared attribute, allowing the mutex or condition variable to be shared between different processes, assuming that the mutex or condition variable is stored in memory that is shared between the different processes.
Exercises Remove the mutex locking from Figure 7.3 and verify that the example fails if more than one producer thread is run.
Watch the memory usage of the process, using a program such as ps.
A mutex lock blocks all other threads from entering what we call a critical region.
This critical region usually involves accessing or updating one or more pieces of data that are shared between the threads.
But sometimes, we can distinguish between reading a piece of data and modifying a piece of data.
We now describe a read-write lock and distinguish between obtaining the read-write lock for reading and obtaining the read-write lock for writing.
Any number of threads can hold a given read-write lock for reading as long as no thread holds the read-write lock for writing.
A read-write lock can be allocated for writing only if no thread holds the read-write lock for reading or writing.
Stated another way, any number of threads can have read access to a given piece of data as long as no thread is reading or modifying that piece of data.
A piece of data can be modified only if no other thread is reading the data.
In some applications, the data is read more often than the data is modified, and these applications can benefit from using read-write locks instead of mutex locks.
Allowing multiple readers at any given time can provide more concurrency, while still protecting the data while it is modified from any other readers or writers.
This sharing of access to a given resource is also known as shared~xclusive locking, because obtaining a read-write lock for reading is called a shared lock, and obtaining a read-write lock for writing is called an exclusive lock.
Other terms for this type of problem (multiple readers and one writer) are the readers and writers problem and.
In the last term, "readers" is intentionally plural, and "writer" is intentionally singular, emphasizing the multiple-readers but single-writer nature of the problem.
A common analogy for a read-write lock is accessing bank accounts.
Multiple threads can be reading the balance of an account at the same time, but as soon as one thread wants to update a given balance, that thread must wait for all readers to finish reading that balance, and then only the updating thread should be allowed to modify the balance.
No readers should be allowed to read the balance until the update is complete.
A Posix working group (1003.lj) is currently developing a set of Pthreads extensions that includes read-write locks, which will hopefully be thesame as described in this chapter.
Both return: 0 if OK, positive Exxx value on error.
When initializing a read-write lock, if attr is a null pointer, the default attributes are used.
To assign other than these defaults, the following two functions are provided:
Both return: 0 if OK, positive Exxx value on error I The first function returns the current value of this attribute in the integer pointed to by valptr.
Read-write locks can be implemented using just mutexes and condition variables.
This section and the remaining sections of this chapter contain advanced topics that you may want to skip on a first reading.
The implementation shown in this section is from Doug Schmidt's ACE package, http: / /www.
Our pthread-rwlock-t datatype contains one mutex, two condition variables, one flag, and three counters.
We will see the use of all these in the functions that follow.
Whenever we examine or manipulate this structure, we must hold the rw-mutex.
When the structure is successfully initialized, the rw-magic member is set to RW-MAGIC.
This member is then tested by all the functions to check that the caller is passing a pointer to an initialized lock, and then set to 0 when the lock is destroyed.
Our first fundion, pthread-rwlock-ini t, dynamically initializes a read-write lock and is shown in Figure 8.2
We do not support assigning attributes with this function, so we check that the attr argument is a null pointer.
We initialize the mutex and two condition variables that are in our structure.
All three counters are set to 0 and rw-magic is set to the value that indicates that the structure is initialized.
If the initialization of the mutex or condition variables fails, we are careful to destroy the initialized objects and return an error.
We first check that the lock is not in use and then call the appropriate destroy functions for the mutex and two condition variables.
Whenever we manipulate the pthread-rwlock-t structure, we must lock the rw-mutex member.
If either of these conditions is true, we increment rw-nwaitreaders and call pthread-cond-wait on the rw-condreaders condition variable.
We will see shortly that when a read-write lock is unlocked, a check is first made for any waiting writers, and if none exist, then a check is made for any waiting readers.
If readers are waiting, the rw-condreaders condition variable is broadcast.
When we get the read lock, we increment rw-ref count.
A problem exists in this function: if the calling thread blocks in the call to pthread-cond-wait and the thread is then canceled, the thread terminates while it holds the mutex lock, and the counter rw-nwaitreaders is wrong.
If a writer currently holds the lock, or if threads are waiting for a write lock, EBUSY is returned.
To do so, we increment rw-nwai twri t ers and call pthread-cond-wai t on the rw-condwri t ers condition variable.
We will see that this condition variable is signaled when the read-write lock is unlocked and writers are waiting.
When we obtain the write lock, we set rw-ref count to -1
Otherwise, we obtain the write lock and rw-ref count is set to -1
If rw-ref count is currently greater than 0, then a reader is releasing a read lock.
If -ref count is currently -1, then a writer is releasing a write lock.
If a writer is waiting, the rw-condwriters condition variable is signaled if the lock is available (i.e., if the reference count is 0)
Notice that we do not grant any additional read locks as soon as a writer is waiting; otherwise, a stream of continual read requests could block a waiting writer forever.
For this reason, we need two separate i f  tests, and cannot write.
We could also omit the test of rw->rw-refcount, but that can result in calls to pthread-cond-s igna l  when read locks are still allocated, which is less efficient.
We alluded to a problem with Figure 8.4 if the calling thread gets blocked in the call to pthread-cond-wait and the thread is then canceled.
A thread may be canceled by any other thread in the same process when the other thread calls pthread-cancel, a fundion whose only argument is the thread ID to cancel.
Cancellation can be used, for example, if multiple threads are started to work on a given task (say finding a record in a database) and the first thread that completes the task then cancels the other tasks.
Another example is when multiple threads start on a task and one thread finds an error, necessitating that it and the other threads stop.
To handle the possibility of being canceled, any thread can install (push) and remove (pop) cleanup handlers.
These handlers are just fundions that are called when the thread is canceled (by some thread calling pthread-cancel), or when the thread voluntarily terminates (either by calling pthread-exit or returning from its thread start function)
The cleanup handlers can restore any state that needs to be restored, such as unlocking any mutexes or semaphores that the thread currently holds.
We encounter thread cancellation again with Figure 15.31 when we see that a doors server is canceled if the client terminates while a procedure call is in progress.
An example is the easiest way to demonstrate the problem with our implementation in the previous section.
We sleep for a second after creating the first thread, to allow it to obtain a read lock.
We wait for the second thread first, and verify that its status is PTHREAD-CANCEL.
We then wait for the first thread to terminate and verify that its status is a null pointer.
We then print the three counters in the pthread-rwlock-t structure and destroy the lock.
The first thread then calls pthread-cancel to cancel the second thread, sleeps another 3 seconds, releases its read lock, and terminates.
The second thread tries to obtain a write lock (which it cannot get, since the first thread has already obtained a read lock)
If we run this program using the functions from the previous section, we get solaris % testcancel threadl ( )  got a read lock thread20 trying to obtain a write lock.
The s l e e p  ( 3 ) in the first thread returns, and pthread-cancel is called.
We have not installed any cancellation cleanup handlers yet, but the mutex is still reacquired before the thread is canceled.
Therefore, when the second thread is canceled, it holds the mutex lock for the read-write lock, and the value of rw-nwaitwriters in Figure 8.6 has been incremented.
If pthread-cond-wait returns, our second new line of code removes the cleanup handler.
If this argument is nonzero, the cleanup handler is first called and then removed.
If the thread is canceled while it is blocked in its call to pthread-cond-wait, no return is made from this function.
Instead, the cleanup handlers are called (after reacquiring the associated mutex, which we mentioned in step 3 earlier)
First we add two new lines around the call to pthread-cond-wai t:
If we run our test program from Figure 8.10 with these new functions, the results are now correct.
Read-write locks can provide more concurrency than a plain mutex lock when the data being protected is read more often than it is written.
The read-write lock functions defined by Unix 98, which is what we have described in this chapter, or something similar, should appear in a future Posix standard.
Read-write locks can be implemented easily using just mutexes and condition variables, and we have shown a sample implementation.
Our implementation gives priority to waiting writers, but some implementations give priority to waiting readers.
Threads may be canceled while they are blocked in a call to pthread-cond-wait, and our implementation allowed us to see this occur.
We provided a fix for this problem, using cancellation cleanup handlers.
The read-write locks described in the previous chapter are allocated in memory as variables of datatype pthread-rwlock-t.
This chapter describes an extended type of read-write lock that can be used by related or unrelated processes to share the reading and writing of a file.
The file that is being locked is referenced through its descriptor, and the function that performs the locking is f cntl.
These types of locks are normally maintained within the kernel, and the owner of a lock is identified by its process ID.
This means that these locks are for locking between different processes and not for locking between the different threads within one process.
Consider the following scenario, which comes from the Unix print spoolers (the BSD lpr command and the System V lp command)
The process that adds a job to the print queue (to be printed at a later time by another process) must assign a unique sequence number to each print job.
The process ID, which is unique while the process is running, cannot be used as the sequence number, because a print job can exist long enough for a given process ID to be reused.
A given process can also add multiple print jobs to a queue, and each job needs a unique number.
The technique used by the print spoolers is to have a file for each printer that contains the next sequence number to be used.
The file is just a single line containing the sequence number in ASCII.
Each process that needs to assign a sequence number goes through three steps:
The problem is that in the time a single process takes to execute these three steps, another process can perform the same three steps.
Chaos can result, as we will see in some examples that follow.
What we have just described is a mutual exclusion problem.
What differs with this problem, however, is that we assume the processes are unrelated, which makes using these techniques harder.
We could have the unrelated processes share memory (as we describe in Part 4) and then use some type of synchronization variable in that shared memory, but for unrelated processes, f c n t l  record locking is often easier to use.
Another factor is that the problem we described with the line printer spoolers predates the availability of mutexes, condition variables, and read-write locks by many years.
Record locking was added to Unix in the early 1980s, before shared memory and threads.
What is needed is for a process to be able to set a lock to say that no other process can access the file until the first process is done.
Figure 9.2 shows a simple program that does these three steps.
The functions my-lock and my-unlock are called to lock the file at the beginning and unlock the file when the process is done with the sequence number.
Printing a process ID requires that we cast the variable of type pid-t to a long and then print it with the %Id  format string.
The problem is that the pid-t type is an integer type, but we do not know its size ( i n t  or long), so we must assume the largest.
If we assumed an i n t  and used a format string of %d, but the type was actually a long, the code would be wrong.
To show the results when locking is not used, the functions shown in Figure 9.1 provide no locking at all.
If the sequence number in the file is initialized to one, and a single copy of the program is run, we get the following output:
Notice that the main function (Figure 9.2) is in a file named lockmain.
This is because we will provide other implementations of the two functions mlock and my-unlock that use other locking techniques, so we name the executable based on the type of locking that we use.
When the sequence number is again initialized to one, and the program is run twice in the background, we have the following output:
The first thing we notice is that the shell's prompt is output before the first line of output from the program.
This is OK and is common when running programs in the background.
But a problem occurs with the first line of output from the other instance of the program (process ID 15499): it prints a sequence number of 1, indicating that it probably was started first by the kernel, it read the sequence number file (with a value of I), and the kernel then switched to the other process.
This process only ran again when the other process terminated, and it continued executing with the value of 1 that it had read before the kernel switched processes.
What we need is some way to allow a process to prevent other processes from accessing the sequence number file while the three steps are being performed.
That is, we need these three steps to be performed as an atomic operation with regard to other processes.
When we run two instances of the program in the background as just shown, the output is nondeterministic.
There is no guarantee that each time we run the two programs we get the same output.
But this is not OK if the three steps are not handled atomically, often generating an ending value less than 40, which is an error.
Whether the three steps are performed atomically is what makes the program correct or incorrect.
Being nondeterministic, however, usually makes debugging these types of programs harder.
The Unix kernel has no notion whatsoever of records within a file.
Any interpretation of records is up to the applications that read and write the file.
Nevertheless, the term record locking is used to describe the locking features that are provided.
But the application specifies a byte range within the file to lock or unlock.
Whether this byte range has any relationship to one or more logical records within the file is left to the application.
Our remaining discussion concerns record locking, with file locking just one special case.
The term granularity is used to denote the size of the object that can be locked.
With , Posix record locking, this granularity is a single byte.
Normally the smaller the granularity, the greater the number of simultaneous users allowed.
For example, assume five processes access a given file at about the same time, three readers and two writers.
Also assume that all five are accessing different records in the file and that each of the five requests takes about the same amount of time, say 1 second.
If the locking is done at the file level (the coarsest granularity possible), then all three readers can access their records at the same time, but both writers must wait until the readers are done.
Then one writer can modify its record, followed by the other writer.
We are ignoring lots of details in these timing assumptions, of course.
But if the locking granularity is the record (the finest granularity possible), then all five accesses can proceed simultaneously, since all five are working on different records.
Berkeley-derived implementations of Unix support file locking to lock or unlock an entire file, with no capabilities to lock or unlock a range of bytes within the file.
Various techniques have been employed for file and record locking under Unix over the years.
Early programs such as UUCP and line printer daemons used various tricks that exploited characteristics of the filesystem implementation.
We describe three of these filesystem techniques in Section 9.8
These are slow, however, and better techniques were needed for the database systems that were being implemented in the early 1980s.
This provided mandatory record locking and was picked up by many versions of System I11 and Xenix.
We describe the differences between mandatory and advisory locking, and between record locking and file locking later in this chapter.
The 1984 /usr/group Standard (one of the predecessors to X/Open) defined the lockf function, which provided only exclusive locks (write locks), not shared locks (read locks)
The lockf function was also provided, but it was just a library function that called fcntl.
Many current systems still provide this implementation of lockf using fcntl.
The Posix interface for record locking is the f cnt 1 function.
Three values of the crnd argument are used with record locking.
These three commands require that the third argument, arg, be a pointer to an flock structure:
If the lock cannot be granted to the process, the function returns immediately (it does not block) with an error of EACCES or EAGAIN.
F-SETLKW This command is similar to the previous command; however, if the lock cannot be granted to the process, the thread blocks until the lock can be granted.
F-GETLK Examine the lock pointed to by arg to see whether an existing lock would prevent this new lock from being granted.
If no lock currently exists that would prevent the new lock from being granted, the 1-type member of the flock structure pointed to by arg is set to F-UNLCK.
Otherwise, information about the existing lock, including the process ID of the process holding the lock, is returned in the flock structure pointed to by arg (i.e., the contents of the structure are overwritten by this function)
Realize that issuing an F-GETLK followed by an F-SETLK is not an atomic operation.
That is, if we call F-GETLK and it sets the 1-type member to F-uNLCK on return, this does not guarantee that an immediate issue of the F-SETLK will return success.
Another process could run between these two calls and obtain the lock that we want.
The reason that the F-GETLK command is provided is to return information about a lock when F-SETLK returns an error, allowing us to determine who has the region locked, and how (a read lock or a write lock)
But even in this scenario, we must be prepared for the F-GETLK command to return.
The flock structure describes the type of lock (a read lock or a write lock) and the byte range of the file to lock.
The 1-len member specifies the number of consecutive bytes starting at that offset.
A length of 0 means "from the starting offset to the largest possible value of the file offset." Therefore, two ways to lock the entire file are.
The first of these two ways is most common, since it requires a single function call (f cntl) instead of two function calls.
A lock can be for reading or writing, and at most, one type of lock (read or write) can exist for any byte of a file.
Furthermore, a given byte can have multiple read locks but only a single write lock.
This corresponds to the read-write locks that we described in the previous chapter.
Naturally an error occurs if we request a read lock when the descriptor was not opened for reading, or request a write lock when the descriptor was not opened for writing.
All locks associated with a file for a given process are removed when a descriptor for that file is closed by that process, or when the process holding the descriptor terminates.
Locks are not inherited by a child across a fork.
This cleanup of existing locks by the kernel when the process terminates is provided only by f cntl record locking and as an option with System V semaphores.
The other synchronization techniques that we describe (mutexes, condition variables, read-write locks, and Pmix semaphores) do not perform this cleanup on process termination.
We talked about this at the end of Section 7.7
Record locking should not be used with the standard I/O library, because of the internal buffering performed by the library.
When a file is being locked, read and write should be used with the file to avoid problems.
Notice that we must specify a write lock, to guarantee only one process at a time updates the sequence number.
We also specify a command of F-SETLKW when obtaining the lock, because if the lock is not available, we want to block until it is available.
Given the definition of the flock structure shown earlier, we might think we could initialize our structure in my-lock as.
Posix defines only the required members that must be in a structure, such as flock.
We do not show the output, but it appears correct.
Realize that running our simple program from Figure 9.2 does not let us state that our program works.
If the output is wrong, as we have seen, we can say that our program is not correct, but running two copies of the program, each looping 20 times is not an adequate test.
If no switch occurs between the two processes, we might never see the error.
A better test is to run the functions from Figure 9.3 with a m a i n  function that increments the sequence number say, ten thousand times, without printing the value each time through the loop.
In Figure 9.3, to request or release a lock takes six lines of code.
We must allocate a structure, fill in the structure, and then call fcntl.
We can simplify our programs by defining the following seven macros, which are from Section 12.3 of APUE:
When using these macros, we need not worry about the structure or the function that is actually called.
The first three arguments to these macros are purposely the same as the first three arguments to the lseek function.
We also define two wrapper functions, Lock-reg and Lock-test, which terminate with an error upon an f cntl error, along with seven macros whose names also begin with a capital letter that call these two wrapper functions.
Using these macros, our my-lock and my-unlock functions from Figure 9.3 become.
This means the kernel maintains correct knowledge of all files that have been locked by each process, but it does not prevent a process from writing to a file that is read-locked by another process.
Similarly, the kernel does not prevent a process from reading from a file that is write-locked by another process.
A process can ignore an advisory lock and write to a file that is read-locked, or read from a file that is write-locked, assuming the process has adequate permissions to read or write the file.
The programming of daemons used by network programming is an example of cooperative processes-the programs that access a shared resource, such as the sequence number file, are all under control of the system administrator.
As long as the actual file containing the sequence number is not writable by any process, some random process cannot write to the file while it is locked.
This new program reads the sequence number value of 11 before our lockf cntl program writes it back to the file.
The advisory record lock held by the lockf cntl program has no effect on our 1 ocknone program.
Some systems provide another type of record locking, called mandatory locking.
With a mandatory lock, the kernel checks every read and write request to verify that the operation does not interfere with a lock held by a process.
For a normal blocking descriptor, the read or write that conflicts with a mandatory lock puts the process to.
With a nonblocking descriptor, issuing a read or write that conflicts with a mandatory lock causes an error return of EAGAIN.
Many implementations derived from System V, however, provide both advisory and mandatory locking.
To enable mandatory locking for a particular file, the group-execute bit must be off, and the set-group-ID bit must be on.
Note that having the set-user-ID bit on for a file without having the user-execute bit on also makes no sense, and similarly for the set-group-ID bit and the group-execute bit.
Therefore, mandatory locking was added in this way, without affecting any existing user software.
Similarly, the chmod command accepts a specification of 1 to enable mandatory locking for a file.
On a first glance, using mandatory locking should solve the problem of an uncooperating process, since any reads or writes by the uncooperating process on the locked file will block that process until the lock is released.
Unfortunately, the timing problems are more complex, as we can easily demonstrate.
To change our example using f cnt 1 to use mandatory locking, all we do is change the permission bits of the seqno file.
We also run a different version of the main function that takes the for loop limit from the first command-line argument (instead of using the constant 20) and does not call print f each time around the loop.
We now start two programs in the background: loopf cntl uses f cntl locking, and loopnone does no locking.
We specify a command-line argument of 10,000, which is the number of times that each program reads, increments, and writes the sequence number.
If the locking worked as desired, the ending value would always be 20,001
To see where the error occurs, we need to draw a time line of the individual steps, which we show in Figure 9.6
Figure 9.6 Time line of l oop fcn t l  and loopnone programs.
We assume that the loopfcntl program starts first and executes the first eight steps shown in the figure.
The kernel then switches processes while loopf cntl has a record lock on the sequence number file.
This behavior is the type that we expect: the kernel blocks the read from the uncooperating process, because the file it is trying to read is locked by another process.
But the second process writes a value of 6 to the file, which is wrong.
If multiple processes are updating a file, all the processes must cooperate using some form of locking.
Priorities of Readers and Writers In our implementation of read-write locks in Section 8.4, we gave priority to waiting writers over waiting readers.
We now look at some details of the solution to the readers and writer problem provided by f c n t l  record locking.
What we want to look at is how pending lock requests are handled when a region is already locked, something that is not specified by Posix.
The first question we ask is: if a resource is read-locked with a write lock queued, is another read lock allowed? Some solutions to the readers and writers problem do not allow another reader if a writer is already waiting, because if new read requests are continually allowed, a possibility exists that the already pending write request will never be allowed.
To test how f c n t l  record locking handles this scenario, we write a test program that obtains a read lock on an entire file and then forks two children.
The first child tries to obtain a write lock (and will block, since the parent holds a read lock on the entire file), followed in time by the second child, which tries to obtain a read lock.
The parent opens the file and obtains a read lock on the entire file.
Notice that we call read-lock (which does not block but returns an error if the lock cannot be granted) and not readw-lock (which can wait), because we expect this lock to be granted immediately.
Figure 9.7 Determine whether another read lock is allowed while a write lock is pending.
When the write lock is granted, this first child holds the lock for 2 seconds, releases the lock, and terminates.
The second child is created, and it sleeps for 3 seconds to allow the first child's write lock to be pending, and then tries to obtain a read lock of the entire file.
We can tell by the time on the message printed when readw-lock returns whether this read lock is queued or granted immediately.
The parent holds the read lock for 5 seconds, releases the lock, and terminates.
That is, the read lock requested by the second child is granted even though a write lock is already pending from the first child.
This allows for potential starvation of write locks as long as read locks are continually issued.
Here is the output with some blank lines added between the major time events for readability:
The next question we ask is: do pending writers have a priority over pending readers? Some solutions to the readers and writers problem build in this priority.
The parent creates the file and obtains a write lock on the entire file.
The first child is created, and it sleeps for 1 second and then requests a write lock on the entire file.
We know this will block, since the parent has a write lock on the entire file and holds this lock for 5 seconds, but we want this request queued when the parent's lock is released.
The second child is created, and it sleeps for 3 seconds and then requests a read lock on the entire file.
This too will be queued when the parent releases its write lock.
But this doesn't tell us that write locks have a priority over read locks, because the reason could be that the kernel grants the lock requests in FIFO order, regardless whether they are read locks or write locks.
These two programs show that Solaris and Digital Unix handle lock requests in a FIFO order, regardless of the type of lock request.
These two programs also show that BSD/OS 3.1 gives priority to read requests.
Figure 9.9 Test whether writers have a priority over readers.
Figure 9.10 Test whether writers have a priority over readers.
A common use for record locking is to make certain that only one copy of a program (such as a daemon) is running at a time.
The code fragment shown in Figure 9.11 would be executed when a daemon starts.
Figure 9.11 Make certain only one copy of a program is running.
If the lock is not granted, then we know that another copy of the program is running, and we print an error and terminate.
Many Unix systems have their daemons write their process ID to a file.
Solaris 2.6 stores some of these files in the /etc directory.
Digital Unix and BSD/OS both store these files in the /var / run directory.
If we just wrote the line, without truncating the file, the contents would be 123\n6\n.
While the first line would still contain the process ID, it is cleaner and less confusing to avoid the possibility of a second line in the file.
Other ways exist for a daemon to prevent another copy of itself from being started.
The advantages in the method shown in this section are that many daemons already write their process ID to a file, and should the daemon prematurely crash, the record lock is automatically released by the kernel.
Furthermore, the check for the existence of the file and the creation of the file (if it does not already exist) must be atomic with regard to other processes.
We can therefore use the file created with this technique as a lock.
We are guaranteed that only one process at a time can create the file (i.e., obtain the lock), and to release the lock, we just unlink the file.
Figure 9.12 shows a version of our locking functions using this technique.
If the open succeeds, we have the lock, and the my-lock function returns.
We close the file because we do not need its descriptor: the lock is the existence of the file, regardless of whether the file is open or not.
If open returns an error of EEXIST, then the file exists and we try the open again.
If the process that currently holds the lock terminates without releasing the lock, the filename is not removed.
There are ad hoc techniques to deal with this-check the last-access time of the file and assume it has been orphaned if it is older than some amount of time-but none are perfect.
Another technique is to write the process ID of the process holding the lock into the lock file, so that other processes can read this process ID and check whether that process is still running.
This is imperfect because process IDS are reused after some time.
This scenario is not a problem with f cntl record locking, because when a process terminates, any record locks held by that process are automatically released.
If some other process currently has the file open, we just call open again, in an infinite loop.
This is called polling and is a waste of CPU time.
This is not a problem with f c n t l  record locking, assuming that the process that wants the lock specifies the FSETLKW command.
The kernel puts the process to sleep until the lock is available and then awakens the process.
Creating and deleting a second file by calling open and unlink involves the filesystem and normally takes much longer than calling f c n t l  twice (once to obtain the lock and once to release the lock)
Two other quirks of the Unix filesystem have also been used to provide ad hoc locking.
The first is that the l i n k  function fails if the name of the new link already exists.
To obtain a lock, a unique temporary file is first created whose pathname contains the process ID (or some combination of the process ID and thread ID, if locking is needed between threads in different processes and between threads within the same process)
The l i n k  function is then called to create a link to this file under the well-known pathname of the lock file.
If this succeeds, then the temporary pathname can be unlinked.
When the thread is finished with the lock, it just unlinks the well-known pathname.
If the l i n k  fails with an error of EEXIST, the thread must try again (similar to what we did in Figure 9.12)
One requirement of this technique is that the temporary file and the.
The second quirk is based on open returning an error if the file exists, if O-TRUNC is specified, and if write permission is denied.
If this succeeds, we have the lock and we just unlink the pathname when we are done.
If open fails with an error of EACCES, the thread must try again (similar to what we did in Figure 9.12)
One caveat is that this trick does not work if the calling thread has superuser privileges.
The lesson from these examples is to use f cntl record locking.
Nevertheless, you may encounter code that uses these older types of locking, often in programs written before the widespread implementation of f cntl locking.
Unix systems normally support NFS record locking with two additional daemons: lockd and statd.
When a process calls f cntl to obtain a lock, and the kernel detects that the descriptor refers to a file that is on an NFS-mounted filesystem, the local lockd sends the request to the server's lockd.
The s tatd daemon keeps track of the clients holding locks and interacts with lockd to provide crash and recovery functions for NFS locking.
We should expect record locking for an NFS file to take longer than record locking for a local file, since network communication is required to obtain and release each lock.
To test NFS record locking, all we need to change is the filename specified by SEQFILE in Figure 9.2
Also realize that when the sequence number file is on an NFS-mounted filesystem, network communication is involved for both the record locking and for the reading and writing of the sequence number.
Caveat emptor: NFS record locking has been a problem for many years, and most of the problems have been caused by poor implementations.
Despite the fact that the major Unix vendors have finally cleaned up their implementations, using f cntl record locking over NFS is still a religious issue for many.
We will not take sides on this issue but will just note that f cntl record locking is supposed to work over NFS, but your success depends on the quality of the implementations, both client and server.
These locks are for locking between different processes and not for locking between the different threads within one process.
A better term is "range locking," because we  speclfy a range of bytes within the file to lock or unlock.
Almost all uses of this type of record locking are advisory between cooperating processes, because even mandatory locking can lead to inconsistent data, as we showed.
With f c n t l  record locking, there is no  guarantee as  to the priority of pending readers versus pending writers, which is what w e  saw in Chapter 8 with read-write locks.
Verify that the program does not work without any locking, and that the results are nondeterministic.
Modify Figure 9.2 so that the standard output is unbuffered.
What effect does this have? Continue the previous exercise by also calling putchar for every character that is output to standard output, instead of calling printf.
What effect does this have? Change the lock in the my-lock function in Figure 9.3 to be a read lock instead of a write lock.
What happens? Change the call to open in the loopmain.
Build the loopfcntlnonb program and run two instances of it at the same time.
Does anything change? Why? Continue the previous exercise by using the nonblocking version of loopmain.
Enable the s e w 0  file for mandatory locking.
Run one instance of this program and another instance of the loopf cntlnonb program from the previous exercise at the same time.
What happens? Build the loopf cntl program and run it 10 times in the background from a shell script.
First, time the shell script when advisory locking is used, and then change the permissions of the s e w 0 file to enable mandatory locking.
Why don't we just specify the 0-TRUNC flag for open instead? If we are writing a threaded application that uses fcntl record locking, should we use SEEK-SET, SEEK-CUR, or SEEK-END when specifying the starting byte offset to lock, and whv?
A semaphore is a primitive used to provide synchronization between various processes or between the various threads in a given process.
We look at three types of semaphores in this text.
Posix named semaphores are identified by Posix IPC names (Section 2.2) and can be used to synchronize processes or threads.
Posix memory-based semaphores are stored in shared memory and can be used to synchronize processes or threads.
System V semaphores (Chapter 11) are maintained in the kernel and can be used to synchronize processes or threads.
For now, we concern ourselves with synchronization between different processes.
Also, Posix semaphores are identified by names that might correspond to pathnames in the filesystem.
Therefore, Figure 10.2 is a more realistic picture of what is termed a Posix named semaphore.
Figure 10.2 A Posix named binary semaphore being used by two processes.
We must make one qualification with regard to Figure 10.2: although Posix named semaphores are identified by names that might correspond to pathnames in the filesystem, nothing requires that they actually be stored in a file in the filesystem.
An embedded realtime system, for example, could use the name to identify the semaphore, but keep the actual semaphore value somewhere in the kernel.
The fundamental requirement here is that the test of the value in the w h i l e statement, and its subsequent decrement (if its value was greater than O), must be done as an atomic operation with respect to other threads or processes accessing this semaphore.
That is one reason System V semaphores were implemented in the mid-1980s within the kernel.
Since the semaphore operations were system calls within the kernel, guaranteeing this atomicity with regard to other processes was easy.
There are other common names for this operation: originally it was called P by Edsger Dijkstra, for the Dutch word proberen (meaning to try)
This increments the value of the semaphore and can be summarized by the pseudocode.
If any processes are blocked, waiting for this semaphore's value to be greater than 0, one of those processes can now be awoken.
As with the wait code just shown, this post operation must also be atomic with regard to other processes accessing the semaphore.
There are other common names for this operation: originally it was called V for the Dutch word verhogen (meaning to increment)
It is also known as up (since the value of the semaphore is being incremented), unlock, and signal.
Obviously, the actual semaphore code has more details than we show in the pseudocode for the wait and post operations: namely how to queue all the processes that are waiting for a given semaphore and then how to wake up one (of the possibly many processes) that is waiting for a given semaphore to be posted to.
The code works with semaphores that are initialized to any nonnegative value.
These are normally initialized to some value N, which indicates the number of resources (say buffers) available.
We show examples of both binary semaphores and counting semaphores throughout the chapter.
We often differentiate between a binary semaphore and a counting semaphore, and we do so for our own edification.
No difference exists between the two in the system code that implements a semaphore.
A binary semaphore can be used for mutual exclusion, just like a mutex.
Figure 10.3 Comparison of mutex and semaphore to solve mutual exclusion problem.
The call to sem-wai t waits for the value to be greater than 0 and then decrements the value.
Although semaphores can be used like a mutex, semaphores have a feature not provided by mutexes: a mutex must always be unlocked by the thread that locked the.
Figure 10.4 shows a producer that places an item into a shared buffer and a consumer that removes the item.
Figure 10.5 shows the pseudocode for the producer and consumer.
The semaphore put controls whether the producer can place an item into the shared buffer, and the semaphore get controls whether the consumer can remove an item from the shared buffer.
Since a thread is blocked on this semaphore (the consumer), waiting for its value to become positive, that thread is marked as ready-to-run.
The producer must wait until the consumer empties the buffer.
Since a thread is blocked on this semaphore (the producer), waiting for its value to become positive, that thread is marked as ready-to-run.
The producer returns from its call to sem-wait, places data into the buffer, and this scenario just continues.
We assumed that each time s e n j o s t  was called, even though a process was waiting and was then marked as ready-to-run, the caller continued.
Whether the caller continues or whether the thread that just became ready runs does not matter (you should assume the other scenario and convince yourself of this fact)
We can list three differences among semaphores and mutexes and condition variables.
A mutex must always be unlocked by the thread that locked the mutex, whereas a semaphore post need not be performed by the same thread that did the semaphore wait.
A mutex is either locked or unlocked (a binary state, similar to a binary semaphore)
Since a semaphore has state associated with it (its count), a semaphore post is always remembered.
When a condition variable is signaled, if no thread is waiting for this condition variable, the signal is lost.
As an example of this feature, consider Figure 10.5 but assume that the first time through the producer loop, the consumer has not yet called sen-wait.
The producer can still put the data item into the buffer, call s e m j o s t  on the g e t  semaphore (incrementing its value from 0 to I), and then block in its call to sen-wait on the put semaphore.
Mutexes and condition variables are specified as synchronization mechanisms between threads; these threads always share (some) memory.
Both are synchronization paradigms that have been in widespread use for a number of years.
Even though semaphores are intended for interprocess synchronization and mutexes and condition variables are intended for interthread synchronization, semaphores can be used between threads and mutexes and condition variables can be used between processes.
We should use whichever set of primitives fits the application.
We mentioned that Posix provides two types of semaphores: named semaphores and memory-based (also called unnamed) semaphores.
Figure 10.6 compares the functions used for both types of semaphores.
Figure 10.7 shows a Posix memory-based semaphore within a process that is shared by two threads.
Figure 10.7 Memory-based semaphore shared between two threads within a process.
We show that the shared memory belongs to the address space of both processes.
Figure 10.8 Memory-based semaphore in shared memory, shared by two processes.
In this chapter, we first describe Posix named semaphores and then Posix memorybased semaphores.
We return to the producer-consumer problem from Section 7.3 and expand it to allow multiple producers with one consumer and finally multiple.
We then show that the common I/O technique of multiple buffers is just a special case of the producer-consumer problem.
We show three implementations of Posix named semaphores: the first using FIFOs, the next using memory-mapped I/O with mutexes and condition variables, and the last using System V semaphores.
The function sem-open creates a new named semaphore or opens an existing named semaphore.
A named semaphore can always be used to synchronize either threads or processes.
I Returns: pointer to semaphore if OK, SEM-FAILED on error I We described the rules about the name argument in Section 2.2
Specifying 0-CREAT if the semaphore already exists is not an error.
The return value is a pointer to a sem-t datatype.
This pointer is then used as the argument to sem-close, sem-wait, sem-trywait, sem_post, and sem-getvalue.
The return value of SEM-FAILED to indicate an error is strange.
Earlier drafts that led to the Posix standard specified a return value of -1 to indicate an error, and many implementations define.
Posix.1 says little about the permission bits associated with a semaphore when it is created or opened by sem-open.
The reason is probably that the two semaphore operations-post and wait-both read and change the value of the semaphore.
Not having either read access or write access for an existing semaphore on these two implementations causes the sem-open function to return an error of EACCES ("Permission denied")
A named semaphore that was opened by sem-open is closed by sen-close.
This semaphore close operation also occurs automatically on process termination for any named semaphore that is still open.
This happens whether the process terminates voluntarily (by calling e x i t  or - exit), or involuntarily (by being killed by a signal)
Closing a semaphore does not remove the semaphore from the system.
That is, Posix named semaphores are at least kernel-persistent: they retain their value even if no process currently has the semaphore open.
A named semaphore is removed from the system by sem-unlink.
Semaphores have a reference count of how many times they are currently open (just like files), and this function is similar to the unlink function for a file: the name can be removed from the filesystem while its reference count is greater than 0, but the destruction of the semaphore (versus removing its name from the filesystem) does not take place until the last sem-close occurs.
The sem- wai t  function tests the value of the specified semaphore, and if the value is greater than 0, the value is decremented and the function returns immediately.
We mentioned earlier that the "test and decrement" operation must be atomic with regard to other threads accessing this semaphore.
We now provide some simple programs that operate on Posix named semaphores, to learn more about their functionality and implementation.
Since Posix named semaphores have at least kernel persistence, we can manipulate them across multiple programs.
The final two arguments, however, are used by sem-open only if the semaphore does not already exist.
We call sem-close, although if this call were omitted, the semaphore is still closed (and the system resources released) when the process terminates.
Figure 10.11 is a simple program that opens a named semaphore, fetches its current value, and prints that value.
Figure 10.13 is a program that posts to a named semaphore (i.e., increments its value by one) and then fetches and prints the semaphore's value.
We first create a named semaphore under Digital Unix 4.08 and print its (default) value.
As with Posix message queues, the system creates a file in the filesystem corresponding to the name that we specify for the named semaphore.
We now wait for the semaphore and then abort the program that holds the semaphore lock.
That is, the semaphore's value of 1 is maintained by the kernel from when the semaphore was created in our previous example, even though no program had the semaphore open during this time.
Second, when we abort our semwait program that holds the semaphore lock, the value of the semaphore does not change.
That is, the semaphore is not unlocked by the kernel when a process holding the lock terminates without releasing the lock.
This differs from record locks, which we said in Chapter 9 are automatically released when the process holding the lock terminates without releasing the lock.
We now show that this implementation uses a negative semaphore value to indicate the number of processes waiting for the semaphore to be unlocked.
We now execute the same example under Solaris 2.6 to see the differences in the implementation.
As with Posix message queues, files are created in the /tmp directory containing the specified name as the filename suffixes.
We see that the permissions on the first file correspond to the permissions specified in our call to sem-open, and we guess that the second file is used for locking.
We now verify that the kernel does not automatically post to a semaphore when the process holding the semaphore lock terminates without releasing the lock.
Next we see how this implementation handles the semaphore value when processes are waiting for the semaphore.
One difference in this output compared to the previous output under Digital Unix, is when the semaphore is posted to: it appears that the waiting process runs before the process that posted to the semaphore.
In Section 7.3, we described the producer-consurner problem and showed some solutions in which multiple producer threads filled an array that was processed by one consumer thread.
In our first solution (Section 7.2), the consumer started only after the producers were finished, and we were able to solve this synchronization problem using a single mutex (to synchronize the producers)
This adds another synchronization problem in that the producer must not get ahead of the consumer.
We still assume that the producer and consumer are threads, but they could also be processes, assuming that some way existed to share the buffer between the processes (e.g., shared memory, which we describe in Part 4)
Three conditions must be maintained by the code when the shared buffer is considered as a circular buffer:
The consumer cannot try to remove an item from the buffer when the buffer is empty.
The producer cannot try to place an item into the buffer when the buffer is full.
Our solution using semaphores demonstrates three different types of semaphores:
A binary semaphore named mutex protects the critical regions: inserting a data item into the buffer (for the producer) and removing a data item from the buffer (for the consumer)
Obviously we could use a real mutex for this, instead of a binary semaphore.
A counting semaphore named nempty counts the number of empty slots in the buffer.
This semaphore is initialized to the number of slots in the buffer (NBUFF)
A counting semaphore named n s t o r e d  counts the number of filled slots in the buffer.
This semaphore is initialized to 0, since the buffer is initially empty.
Figure 10.14 shows the status of our buffer and the two counting semaphores when the program has finished its initialization.
Figure 10.14 Buffer and the two counting semaphores after initialization.
The consumer takes these integers from the buffer and verifies that they are correct, printing any errors to standard output.
Figure 10.15 shows the buffer and the counting semaphores after the producer has placed three items into the buffer, but before the consumer has taken any of these items from the buffer.
We next assume that the consumer removes one item from the buffer, and we show this in Figure 10.16
Figure 10.17 is the m a i n  function that creates the three semaphores, creates two threads, waits for both threads to complete, and then removes the semaphores.
The buffer containing NBUFF items is shared between the two threads, as are the three semaphore pointers.
As described in Chapter 7, we collect these into a structure to reiterate that the semaphores are used to synchronize access to the buffer.
Three semaphores are created and their names are passed to our px-ipc-name function.
We specify the 0-EXCL flag because we need to initialize each semaphore to the correct value.
If any of the three semaphores are still lying around from a previous run of this program that aborted, we could handle that by calling s e m- u n l i n k  for each semaphore, ignoring any errors, before creating the semaphores.
Alternately, we could check for an error of EEXIST from sem-open with the 0-EXCL flag, and call s e m- u n l i n k  followed by another call to sem-open, but this is more complicated.
If we need to verify that only one copy of this program is running (which we could do before trying to create any of the semaphores), we would do so as described in Section 9.7
The two threads are created, one as the producer and one as the consumer.
The main thread then waits for both threads to terminate, and removes the three semaphores.
We could also call sem-close for each semaphore, but this happens automatically when the process terminates.
Removing the name of a named semaphore, however, must be done explicitly.
Figure 10.18 shows the p r o d u c e  and consume functions.
The first time this statement is executed, the value of the semaphore will go from NBUFF to NBUFF-1
Producer stores item in buffer as Before storing the new item into the buffer, the producer must obtain the mutex.
In our example, where the producer just stores a value into the array element indexed by i % NBUFF, no shared variables describe the status of the buffer (i.e., we do not use a linked list that we need to update each time we place an item into the buffer)
Therefore, obtaining and releasing the mutex semaphore is not actually required.
Nevertheless, we show it, because in general it is required for this type of problem (updating a buffer that is shared by multiple threads)
After the item is stored in the buffer, the mutex semaphore is released (its value goes from 0 to I), and the nstored semaphore is posted to.
When the nstored semaphore's value is greater than 0, that many items are in the buffer to process.
The consumer takes one item from the buffer and verifies that its value is correct, protecting this buffer access with the mutex semaphore.
The consumer then posts to the nempty semaphore, telling the producer that another slot is empty.
The consumer starts and verifies the first NBUFF items from the buffer.
The consumer then blocks in the call Sem-wait (shared-nstored) after calling Sem-wait (shared.mutex)
The producer is waiting for the mutex semaphore, but the consumer is holding this semaphore and waiting for the nstored semaphore.
But the producer cannot post to the ns tored semaphore until it obtains the mutex semaphore.
This is one of the problems with semaphores: if we make an error in our coding, our program does not work correctly.
We now return to our sequence number problem from Chapter 9 and provide versions of our my-lock and my-unlock functions that use Posix named semaphores.
To obtain the file lock, we call sem-wait, and to release the lock, we call s e m ~ o s t.
Everything so far in this chapter has dealt with the Posix named semaphores.
These semaphores are identified by a name argument that normally references a file in the filesystem.
But Posix also provides memory-based semaphores in which the application allocates the memory for the semaphore (that is, for a sem-t datatype, whatever that happens to be) and then has the system initialize this semaphore.
The sem argument points to the s-t variable that the application must allocate.
If shared is 0, then the semaphore is shared between the threads of a process, else the semaphore is shared between processes.
When shared is nonzero, then the semaphore must be stored in some type of shared memory that is accessible to all the processes that will be using the semaphore.
As with sem-open, the value argument is the initial value of the semaphore.
When we are done with a memory-based semaphore, sem-des t r o y  destroys it.
Notice that there is nothing similar to 0-CREAT for a memory-based semaphore: s-init always initializes the semaphore value.
Therefore, we must be careful to call sem-init only once for a given semaphore.
The results are undefined if sem-ini t is called for a semaphore that has already been initialized.
Make certain you understand a fundamental difference between sem-open and sem-init.
The former returns a pointer to a sem-t variable that the function has allocated and initialized.
The first argument to sem-ini t ,  on the other hand, is a pointer to a sem-t variable that the caller must allocate and that the function then initializes.
Posix.1 warns that for a memory-based semaphore, only the location pointed to by the sem argument to sem-init can be used to refer to the semaphore, and using copies of this sem-t datatype is undefined.
A memory-based semaphore can be used when the name associated with a named semaphore is not needed.
Named semaphores are normally used when different, unrelated processes are using the semaphore.
In Figure 1.3, we say that memory-based semaphores have process persistence, but their persistence really depends on the type of memory in which the semaphore is stored.
A memory-based semaphore remains in existence as long as the memory in which the semaphore is contained is valid.
If a memory-based semaphore is being shared between the threads of a single process (the shared argument to sem-init is O), then the semaphore has process persistence and disappears when the process terminates.
If a memory-based semaphore is being shared between different processes (the shared argument to sem-init is I), then the semaphore must be stored in shared memory and the semaphore remains in existence as long as the shared memory remains in existence.
Recall from Figure 1.3 that Posix shared memory and System V shared memory both have kernel persistence.
This means that a server can create a region of shared memory, initialize a Posix memory-based semaphore in that shared memory, and then terminate.
Sometime later, one or more clients can open the region of shared memory and access the memorybased semaphore stored therein.
Be warned that the following code does not work as planned: sem-t mysem;
The problem here is that the semaphore mysem is not in shared memory-see Section 10.12
Memory is normally not shared between a parent and child across a fork.
The child starts with a copy of the parent's memory, but this is not the same as shared memory.
We talk more about shared memory in Part 4 of this book.
Allocate semaphores 6 Our declarations for the three semaphores are now for three sem-t datatypes themselves, not for pointers to three of these datatypes.
We call sem-init instead of sem-open, and then sem-destroy instead of sem-unlink.
These calls to sem-destroy are really not needed, since the program is about to terminate.
The remaining changes are to pass pointers to the three semaphores in all the calls to sem-wait and semjost.
The producer-consumer solution in Section 10.6 solves the classic one-producer, oneconsumer problem.
An interesting modification is to allow multiple producers with one consumer.
We will start with the solution from Figure 10.20, which used memory-based semaphores.
The global ni tems is the total number of items for all the producers to produce, and nproducers is the number of producer threads.
Two new variables are declared in the shared structure: nput, the index of the next buffer entry to store into (modulo NBUFF), and nputval, the next value to store in the buffer.
These two variables are needed to synchronize the multiple producer threads.
The semaphores are initialized, and all the producer threads and one consumer thread are created.
Figure 10.22 shows the produce function that is executed by each producer thread.
Notice that multiple producer threads can acquire the nempty semaphore at the same time, but only one producer thread at a time can acquire the mutex semaphore.
This protects the variables nput and nputval from being modified by more than one producer thread at a time.
We must carefully handle the termination of the producer threads.
After the last item is produced, each producer thread executes.
But before the thread terminates, it must increment this semaphore, because the thread does not store an item in the buffer during its last time around the loop.
The consume function in Figure 10.23 just verifies that each entry in the buffer is correct, printing a message if an error is detected.
Termination of the single consumer thread is simple-it just counts the number of items consumed.
The next modification to our producer-consumer problem is to allow multiple producers and multiple consumers.
Whether it makes sense to have multiple consumers depends on the application.
The author has seen two applications that use this technique.
A program that converts IP addresses to their corresponding hostnames.
Each consumer takes an IP address, calls gethostbyaddr (Section 9.6 of UNPvI), and appends the hostname to a file.
Since each call to gethostbyaddr can take a variable amount of time, the order of the IP addresses in the buffer will normally not be the same as the order of the hostnames in the file appended by the consumer threads.
The advantage in this scenario is that multiple calls to gethostbyaddr (each of which can take seconds) occur in parallel: one per consumer thread.
This assumes a reentrant version of gethostbyaddr, and not all implementations have this property.
If a reentrant version is not available, an alternative is to store the buffer in shared memory and use multiple processes instead of multiple threads.
A program that reads UDP datagrams, operates on the datagrams, and then writes the result to a database.
One consumer thread processes each datagram, and multiple consumer threads are needed to overlap the potentially long processing of each datagram.
Even though the datagrams are normally written to the database by the consumer threads in an order that differs from the original datagram order, the ordering of the records in the database handles this.
We have added two more variables to our shared  structure: nget, the next item number for any one of the consumer threads to fetch, and ngetval ,  the corresponding value.
The main function, shown in Figure 10.25, is changed to create multiple consumer threads.
A new command-line argument specifies the number of consumer threads to create.
We must also allocate an array (tid-consume) to hold all the consumer thread IDS, and an array (conscount) to hold our diagnostic count of how many items each consumer thread processes.
Multiple producer threads and multiple consumer threads are created and then waited for.
Figure 10.25 main function that creates multiple producers and multiple consumers.
Our producer function contains one new line from Figure 10.22
When the producer threads terminate, the line preceded with the plus sign is new:
We again must be careful when handling the termination of the producer threads and the consumer threads.
After all the items in the buffer have been consumed, each consumer thread blocks in the call.
We have the producer threads increment the ns  t o r e d  semaphore to unblock the consumer threads, letting them see that they are done.
After the last item has been consumed from the buffer, the consumer threads block, waiting for the n s t o r e d  semaphore to be.
Therefore, as each consumer thread terminates, it increments nstored to let another consumer thread terminate.
Many programs that process a text file, for example, read a line of input, process that line, and write a line of output.
For text files, the calls to read and write are often replaced with calls to the standard I/O functions f ge t s and f pu t s.
Figure 10.27 shows one way to depict this operation, in which we identify a function named reader that reads the data from the input file and a function named writer that writes the data to the output file.
Figure 10.27 One process that reads data into a buffer and then writes the buffer out.
We have labeled the time line with numbers on the left, designating some arbitrary units of time.
We can modify this application by dividing the processing into two threads, as shown in Figure 10.29
Here we use two threads, since a global buffer is automatically shared by the threads.
We could also divide the copying into two processes, but that would require shared memory, which we have not yet covered.
Dividing the operation into two threads (or two processes) also requires some form of notification between the threads (or processes)
The reader thread must notify the writer thread when the buffer is ready to be written, and the writer thread must notify the reader thread when the buffer is ready to be filled again.
We assume that the time to process the data in the buffer, along with the notification of the other thread, takes 2 units of time.
The important thing to note is that dividing the reading and writing into two threads does not affect the total amount of time required to do the operation.
We have not gained any speed advantage; we have only distributed the operation into two threads (or processes)
We are ignoring many fine points in these time lines.
For example, most Unix kernels detect sequential reading of a file and do asynchronous read ahead of the next disk block for the reading process.
This can improve the actual amount of time, called "clock time," that it takes to perform this type of operation.
We are also ignoring the effect of other processes on our reading and writing threads, and the effects of the kernel's scheduling algorithms.
The next step in our example is to use two threads (or processes) and two buffers.
This is the classic double buffering solution, and we show it in Figure 10.31
Figure 10.31 File copying divided into two threads using two buffers.
We show the reader thread reading into the first buffer while the writer thread is writing from the second buffer.
The two buffers are then switched between the two threads.
Note that we cannot go any faster than the slowest operation, which in our example is the write.
The total clock time, however, will be almost halved for the double buffered case, compared to the single buffered case, for our hypothetical example.
This can help with some devices, such as tape drives, that operate faster if the data is written to the device as quickly as possible (this is called a streaming mode)
The interesting thing to note about the double buffering problem is that it is just a special case of the producer-consumer problem.
We start with our solution from Figure 10.20 that used memory-based semaphores.
Instead of just a double buffering solution, this solution handles any number of buffers (the NBUFF definition)
Figure 10.33 shows the global variables and the main function.
Our shared structure now contains an array of another structure named buff,  and this new structure contains a buffer and its count.
The command-line argument is the pathname of a file that we will copy to standard output.
But in our example in which we just use the next buffer, with just one producer thread, nothing needs protection from the consumer.
We still show the locking and unlocking of the mutex, to emphasize that this may be needed in other modifications to this code.
Read data and increment nstored semaphore 43-49 Each time the producer obtains an empty buffer, it calls read.
When read returns 0 (end-of-file), the semaphore is incremented and the producer returns.
Consumer thread 7-68 The consumer thread takes the buffers and writes them to standard output.
A buffer containing a Length of 0 indicates the end-of-file.
As with the producer, the critical region protected by the mutex is empty.
In Section 22.3 of UNPvl, we developed an example using multiple buffers.
In that example, the producer was the SIGIO signal handler, and the consumer was the main processing loop (the dg-echo function)
The variable shared between the producer and consumer was the nqueue counter.
The consumer blocked the sIGIo signal from being generated whenever it examined or modified this counter.
With regard to named semaphores, different processes (related or unrelated) can always reference the same named semaphore by having each process call sem-open specifying the same name.
Even though the pointers returned by sem-open might be different in each process that calls sem-open for a given name, the semaphore functions that use this pointer (e.g., semsost and sem-wait) will all reference the same named semaphore.
But what if we call sem-open, which returns a pointer to a sem-t datatype, and then call fork? The description of the fork function in Posix.1 says "any semaphores that are open in the parent process shall also be open in the child process." This means that code of the following form is OK:
The reason that we must be careful about knowing when we can and cannot share a semaphore between different processes is that the state of a semaphore might be contained in the sem-t datatype itself but it might also use other information (e.g., file descriptors)
We will see in the next chapter that the only handle that a process has to describe a System V.
Any process that knows that identifier can then access the semaphore.
All the state information for a System V semaphore is contained in the kernel, and the integer identifier just tells the kernel which semaphore is being referenced.
SEM-NSEMS-MAX the maximum number of semaphores that a process can have open at once (Posix requires that this be at least 256), and.
SEM-VALUE-MAX the maximum value of a semaphore (Posix requires that this be at least 32767)
These two constants are often defined in the <unistd.h> header and can also be obtained at run time by calling the sysconf function, as we show next.
We now provide an implementation of Posix named semaphores using FIFOs.
Each named semaphore is implemented as a FIFO using the same name.
The nonnegative number of bytes in the FIFO is the current value of the semaphore.
The sem-open function creates the FIFO if the 0-CREAT flag is specified, opens it twice (once read-only, once write-only), and if a new FIFO has been created, writes the number of bytes specified by the initial value to the FIFO.
For similarity with pipes, we store both descriptors in a two-element array, with the first descriptor for reading and the second descriptor for writing.
The sem-magic member contains SEM-MAGIC once this structure has been initialized.
This value is checked by each function that is passed a sem-t pointer, to make certain that the pointer really points to an initialized semaphore structure.
This member is set to 0 when the semaphore is closed.
This technique, although not perfect, can help detect some programming errors.
Figure 10.37 shows our sem-open function, which creates a new semaphore or opens an existing semaphore.
We call va-start to initialize the variable ap to point to the last named argument (of lag)
We then use ap and the implementation's va-arg function to obtain the values for the third and fourth arguments.
We described the handling of the variable argument list and our va-mode-t datatype with Figure 5.21
Section 4.6, this function returns an error of EEXIST if the FIFO already exists.
Allocate sem-t datatype and open FlFO for reading and writing.
We allocate space for a sem-t datatype, which will contain two descriptors.
We open the FIFO twice, once read-only and once write-only.
We also specify the 0-NONBLOCK flag when we open the FIFO write-only, but this is to detect overflow (e.g., if we try to write more than PIPE-BUF bytes to the FIFO)
After the FIFO has been opened twice, we turn off the nonblocking flag on the read-only descriptor.
If a new semaphore has been created, we initialize its value by writing value number of bytes to the FIFO.
If the initial value exceeds the implementation's PIPE-BUF limit, the call to write after the FIFO is full will return an error of EAGAIN.
Our sem-unlink function, shown in Figure 10.39, removes the name associated with our semaphore.
Figure 10.40 shows our semsost function, which increments the value of a semaphore.
If the FIFO was empty, this will wake up any processes that are blocked in a call to read on this FIFO, waiting for a byte of data.
The final function is shown in Figure 10.41, sem-wai t.
We read 1 byte from the FIFO, blocking if the FIFO is empty.
We have not implemented the sem-trywait function, but that could be done by enabling the nonblocking flag for the FIFO and calling read.
Some implementations return the number of bytes currently in a pipe or FIFO when the stat or f stat function is called, as the st-size member of the stat structure.
But this is not guaranteed by Posix and is therefore nonportable.
Implementations of these two Posix semaphore functions are shown in the next section.
We now provide an implementation of Posix named semaphores using memorymapped I/O along with Posix mutexes and condition variables.
You may wish to skip this section until you have read those chapters.
Our semaphore data structure contains a mutex, a condition variable, and an unsigned integer containing the current value of the semaphore.
As discussed with Figure 10.36, the sem-magic member contains SEM-MAGIC once this structure has been initialized.
Figure 10.43 shows the first half of our sem-open function, which creates a new semaphore or opens an existing semaphore.
If the caller specifies the 0-CREAT flag, then we know that four arguments are required, not two.
We described the handling of the variable argument list and our va-mode-t datatype with Figure 5.21
We turn off the user-execute bit in the mode variable (s-IXUSR) for reasons that we describe shortly.
A file is created with the name specified by the caller, and the user-execute bit is turned on.
If, when the 0-CREAT flag is specified by the caller, we were to just open the file, memory map its contents, and initialize the three members of the sem-t structure, we would have a race condition.
We described this race condition with Figure 5.21, and the technique that we use is the same as shown there.
We set the size of the newly created file by writing a zero-filled structure to the file.
This file will contain the current value of the sem-t data structure, although since we have memory mapped the file, we just reference it through the pointer returned by mrnap: we never call read or write.
Three nearly identical steps are done for the condition variable.
We are careful to destroy the attributes in the case of an error.
We compare this value to the maximum value allowed, which we obtain by calling sysconf (Section 10.13)
Once the semaphore is initialized, we turn off the user-execute bit.
We c l o s e  the file, since it has been memory mapped and we do not need to keep it open.
Figure 10.44 shows the second half of our sem-open function.
In Figure 5.23, we described a race condition that we handle here using the same technique.
We open the file containing the sem- t datatype for reading and writing, and memory map the file into the address space of the process (mmap)
We can now see why Posix.1 states that "references to copies of the semaphore produce unde fined results." When named semaphores are implemented using memory-mapped I/O, the semaphore (the sem-t datatype) is memory mapped into the address space of all processes that have the semaphore open.
This is performed by sem-open in each process that opens the named semaphore.
Changes made by one process (e.g., to the semaphore's count) are seen by all the other processes through the memory mapping.
If we were to make our own copy of a sem-t data structure, this copy would no longer be shared by all the processes.
Note from Figure 1.6, however, that memory-mapped regions in a parent are retained in the child across a fork, so a copy of a semaphore that is made by the kernel from a parent to a child across a fork is OK.
To do so, we call s t a t  and look at the file's permissions (the st-mode member of the s t a t  structure)
If the user-execute bit is off, the semaphore has been initialized.
Figure 10.45 shows our sem-close function, which just calls munmap for the region that was memory mapped.
Should the caller continue to use the pointer that was returned by sem-open, it should receive a SIGSEGV signal.
Our sem-unlink function shown in Figure 10.46 removes the name associated with our semaphore.
We must acquire the semaphore's mutex lock before manipulating its value.
We must acquire the semaphore's mutex lock before manipulating its value.
Once the value is greater than 0, we decrement the value and release the mutex.
Figure 10.49 shows the sern-trywait function, the nonblocking version of sern-wait.
Otherwise, the return value is -1 with e r r n o  set to EAGAIN.
Figure 10.50 shows our final function, sern-getvalue, which returns the current value of the semaphore.
We acquire the semaphore's mutex lock and return its value.
We can see from this implementation that semaphores are simpler to use than mutexes and condition variables.
We now provide one more implementation of Posix named semaphores using System V semaphores.
Since implementations of the older System V semaphores are more common than the newer Posix semaphores, this implementation can allow applications to start using Posix semaphores, even if not supported by the operating system.
You may wish to skip this section until you have read that chapter.
Our semaphore data structure contains the System V semaphore ID and a magic number (which we discussed with Figure 10.36)
Figure 10.52 shows the first half of our sem-open function, which creates a new semaphore or opens an existing semaphore.
We described the handling of the variable argument list and our va-mode-t datatype with Figure 5.21
Create ancillary file and map pathname into System V IPC key.
A regular file is created with the pathname specified by the caller.
We do so just to have a pathname for f t o k  to identify the semaphore.
This creates the file if it does not already exist and will cause an error return if the file already exists and 0-EXCL is specified.
The descriptor is closed, because the only use of this file is with f tok, which converts the pathname into a System V IPC key (Section 3.2)
IPC-xxx constants and call semget to create a System V semaphore set consisting of one member.
We always specify IPC-EXCL to determine whether the semaphore exists or not.
Initialize semaphore so Section 11.2 describes a fundamental problem with initializing System V.
We are guaranteed that the semaphore's sem-otime value is initialized to 0 by semget and will be set nonzero by the creator's call to semop.
Therefore, any other thread that finds that the semaphore already exists knows that the semaphore has been initialized once the sen-otime value is nonzero.
If the semaphore already exists and the caller does not specify 0-EXCL, this is not an error.
In this situation, the code falls through to open (not create) the existing semaphore.
Figure 10.53 shows the second half of our sem-open function.
Must open semaphore and make certain it has been initialized.
Notice that sem-open does not have a mode argument when 0-CREAT is not specified, but semget requires the equivalent of a mode argument even if an existing semaphore is just being opened.
We then verlfy that the semaphore has been initialized by calling semctl with a command of IPC-STAT, waiting for sem-otime to be nonzero.
When an error occurs, we are careful not to change errno.
A pointer to the sem-t datatype is the return value from the function.
Figure 10.54 shows our sem-close function, which just calls free to return the dynamically allocated memory that was used for the sem-t datatype.
We do so now, in case one of the remaining functions returns an error.
Figure 10.56 shows our semsost function, which increments the value of a semaphore.
We call semop with a single operation that increments the semaphore value by one.
We call semop with a single operation that decrements the semaphore value by one.
Our sem-trywait function, the nonblocking version of sem-wait, is shown in Figure 10.58
The only change from our sem-wait function in Figure 10.57 is specifying sem-f l g  as IPC-NOWAIT.
If the operation cannot be completed without blocking the calling thread, the return value from semop is EAGAIN, which is what sem-trywait must return if the operation cannot be completed without blocking.
The final function is shown in Figure 10.59; it is sem-getvalue, which returns the current value of the semaphore.
The current value of the semaphore is obtained with a command of GETVAL to semctl.
Posix semaphores are counting semaphores, and three basic operations are provided:
Named semaphores can always be shared between different processes, whereas memory-based semaphores must be designated as process-shared when created.
The persistence of these two types of semaphores also differs: named semaphores have at least kernel persistence, whereas memory-based semaphores have process persistence.
The producer-consumer problem is the classic example for demonstrating semaphores.
In this chapter, our first solution had one producer thread and one consumer thread, our next solution allowed multiple producer threads and one consumer thread, and our final solution allowed multiple consumer threads.
We then showed that the classic problem of double buffering is just a special case of the producer-consumer problem, with one producer and one consumer.
The first, using FIFOs, is the simplest because much of the synchronization is handled by the kernel's read and write functions.
The next implementation used memory-mapped I/O, similar to our implementation of Posix message queues in Section 5.8, and used a mutex and condition variable for synchronization.
Our final implementation used System V semaphores, providing a simpler interface to these semaphores.
Exercises Modify the produce and consume functions in Section 10.6 as follows.
First, swap the order of the two calls to  em-wait in the consumer, to generate a deadlock (as we discussed in Section 10.6)
Next, add a call to p r i n t f  before each call to  em-wait, indicating which thread (the producer or the consumer) is waiting for which semaphore.
Add another call to p r i n t f  after the call to  em-wait, indicating that the thread got the semaphore.
Reduce the number of buffers to 2, and then build and run this program to verify that it leads to a deadlock.
Assume that we start four copies of our program that calls our my-lock function from Figure 10.19:
Is this OK? What happens in the previous exercise if one of the four programs terminates after calling my-lock but before calling my-unlock? What could happen in Figure 10.37 if we did not initialize both descriptors to -l? In Figure 10.37, why do we save the value of e r rno  and then restore it, instead of coding the two calls to c l o s e  as.
Yet in the solution to the previous problem, we said that Figure 10.37 does not have a race condition.
Posix.1 makes it optional for sem-wait to detect that it has been interrupted by a caught signal and return EINTR.
Write a test program to determine whether your implementation detects this or not.
When we described the concept of a semaphore in Chapter 10, we first desaibed.
We used these to count resources in our producer-consumer problem, with the value of the semaphore being the number of resources available.
In both types of semaphores, the wait operation waits for the semaphore value to be greater than 0, and then decrements the value.
System V semaphores add another level of detail to semaphores by defining.
When we refer to a "System V semaphore," we are referring to a set of counting semaphores.
For every set of semaphores in the system, the kernel maintains the following structure of information, defined by including <sys / sem.
The ipcserm structure was described in Section 3.3 and contains the access permissions for this particular semaphore.
The sem structure is the internal data structure used by the kernel to maintain the set of values for a given semaphore.
Every member of a semaphore set is described by the following structure:
Note that sem-base contains a pointer to an array of these sem structures: one array element for each semaphore in the set.
In addition to maintaining the actual values for each semaphore in the set, the kernel also maintains three other pieces of information for each semaphore in the set: the process ID of the process that performed the last operation on this value, a count of the number of processes waiting for the value to increase, and a count of the number of processes waiting for the value to become zero.
The name that we show, sem, is from the historical System V implementation.
We can picture a particular semaphore in the kernel as being a semid-ds structure that points to an array of sem structures.
If the semaphore has two members in its set, we would have the picture shown in Figure 21.1
The semget function creates a semaphore set or accesses an existing semaphore set.
Figure 11.1 Kernel data structures for a semaphore set with two values in the set.
The return value is an integer called the semaphore identifier that is used with the semop and semc t 1 functions.
The nsems argument specifies the number of semaphores in the set.
We cannot change the number of semaphores in a set once it is created.
The oflag value is a combination of the SEM-R and SEM-A constants shown in Figure 3.6
When a new semaphore set is created, the following members of the semid-ds structure are initialized:
The uid and cuid members of the semserm structure are set to the effective user ID of the process, and the gid and cgid members are set to the effective group ID of the process.
The read-write permission bits in of 1 ag are stored in s emserm.
The sem structure associated with each semaphore in the set is not initialized.
These structures are initialized when semctl is called with either the SETVAL or SETALL commands.
Although some systems do initialize the semaphore values to 0, this is not guaranteed.
Indeed, older implementations of System V do not initialize the semaphore.
Most manual pages for semget say nothing at all about the initial values of the semaphores when a new set is created.
This requirement of two function calls to create a semaphore set (semget) and then initialize it (semctl) is a fatal flaw in the design of System V semaphores.
A partial solution is to specify IPC-CREAT I IPC-EXCL when calling semget, so that only one process (the first one to call semge t) creates the semaphore.
The other processes receive an error of EEXIST from semget and they then call semget again, without specifying either IPC-CREAT or IPC-EXCL.
Assume that two processes both try to create and initialize a one-member semaphore set at about the same time, both executing the following numbered lines of code:
The first process executes lines 1-3 and is then stopped by the kernel.
Even though the first process to create the semaphore will be the only process to initialize the semaphore, since it takes two steps to do the creation and initialization, the kernel can switch to another process between these two steps.
That other process can then use the semaphore (line 9 in the code fragment), but the semaphore value has not been initialized by the first process.
The semaphore value, when the second process executes line 9, is indeterminate.
We are guaranteed that the sem-otime member of the semid-ds structure is set to 0 when a new semaphore set is created.
This member is set to the current time only by a successful call to semop.
Therefore, the second process in the preceding example must call semctl.
It then waits for sem-otime to be nonzero, at which time it knows that the semaphore has been initialized and that the process that did the initialization has successfully called semop.
This means the process that creates the semaphore must initialize its value and must call semop before any other process can use the semaphore.
Posix named semaphores avoid this problem by having one function (sem-open) create and initialize the semaphore.
Furthermore, even if 0-CREAT is specified, the semaphore is initialized only if it does not already exist.
Whether this potential race condition is a problem also depends on the application.
But in other applications (eg., our file locking example in Figure 10.19), no single process creates and initializes the semaphore: the first process to open the semaphore must create it and initialize it, and the race condition must be avoided.
Once a semaphore set is opened with semget, operations are performed on one or more of the semaphores in the set using the semop function.
The number of elements in the array of sembuf structures pointed to by opsptr is specified by the nops argument.
Each element in this array specifies an operation for one particular semaphore value in the set.
We are guaranteed only that the structure contains the three members shown.
It might contain other members, and we have no guarantee that the members are in the order that we show.
This means that we must not statically initialize this structure, as in.
The array of operations passed to the semop function are guaranteed to be performed atomically by the kernel.
The kernel either does all the operations that are specified, or it does none of them.
Each particular operation is specified by a sem-op value, which can be negative, 0, or positive.
In the discussion that follows shortly, we refer to the following items:
This value is updated only if the SEM-UNDO flag is specified in the sem-f lg member of the sembuf structure for this operation.
This is a conceptual variable that is maintained by the kernel for each process that specifies the SEM-UNDO flag in a semaphore operation; a structure member with the name of s emad j need not exist.
A given semaphore operation is made nonblocking by specifying the IPC-NOWAIT flag in the sem-f lg member of the sembuf structure.
When this flag is specified and the given operation cannot be completed without putting the calling thread to sleep, semop returns an error of EAGAIN.
When a thread is put to sleep waiting for a semaphore operation to complete and that semaphore is removed from the system by some other thread or process, sernop returns an error of EIDRM ("identifier removed")
We now describe the operation of semop, based on the three possible values of each specified sem-op operation: positive, 0, or negative.
If sem-op is positive, the value of sem-op is added to semval.
This corresponds to the release of resources that a semaphore controls.
If the SEM-UNDO flag is specified, the value of sem-op is subtracted from the semaphore's semad j value.
If semval is nonzero, the semaphore's semzcnt value is incremented and the calling thread is blocked until semval becomes 0 (at which time, the semaphore's semzcnt value is decremented)
As mentioned earlier, the thread is not put to sleep if IPC-NOWAIT is specified.
The sleep returns prematurely with an error if a caught signal interrupts the function or if the semaphore is removed.
If sem-op is negative, the caller wants to wait until the semaphore's value becomes greater than or equal to the absolute value of sem-op.
If semval is greater than or equal to the absolute value of sem-op, the absolute value of sem-op is subtracted from semval.
If the SEM-UNDO flag is specified, the absolute value of sem-op is added to the semaphore's semadj value.
If semval is less than the absolute value of sem-op, the semaphore's semncnt value is incremented and the calling thread is blocked until semval becomes greater than or equal to the absolute value of sem-op.
When this change occurs, the thread is unblocked, the absolute value of sem-op is subtracted from semval, and the semaphore's semncnt value is decremented.
If the SEM-UNDO flag is specified, the absolute value of sem-op is added to the semaphore's semadj value.
As mentioned earlier, the thread is not put to sleep if IPC-NOWAIT is specified.
Also, the sleep returns prematurely with an error if a caught signal interrupts the function or if the semaphore is removed.
These more general operations, along with the fact that System V semaphores can have a set of values, is what complicates System V semaphores, compared to the simpler Posix semaphores.
The semc t 1 function performs various control operations on a semaphore.
The first argument semid identifies the semaphore, and semnum identifies the member of the semaphore set (0,1, and so on, up to nsems-I)
The fourth argument is optional, depending on the cmd (see the comments in the union below)
This union does not appear in any system header and must be declared by the application.
That is, the actual value of the union is the argument, not a pointer to the union.
Unfortunately, some systems (FreeBSD and Linux) define this union as a result of including the <sys/sem.
Even though having the system header declare this union makes sense, Unix 98 states that it must be explicitly declared by the application.
Return the current value of semval as the return value of the function.
Since a semaphore value is never negative (semval is declared as an unsigned short), a successful return value is always nonnegative.
If this is successful, the semaphore adjustment value for this semaphore is set to 0 in all processes.
Return the current value of sempid as the return value of the function.
Return the current value of semncnt as the return value of the function.
Return the current value of semzcnt as the return value of the func tion.
Return the values of semval for each member of the semaphore set.
Notice that the caller must allocate an array of unsigned short integers large enough to hold all the values for the set, and then set arg.array to point to this array.
Set the values of sernval for each member of the semaphore set.
Remove the semaphore set specified by semid from the system.
Set the following three members of the semid-ds structure for the semaphore set from the corresponding members in the structure pointed to by the arg.buf argument: semserm.
The sem-ctime member of the semid-ds structure is also set to the current time.
IPC-STAT Return to the caller (through the argbuf argument) the current semid-ds structure for the specified semaphore set.
Notice that the caller must first allocate a semid-ds structure and set arg.buf to point to this structure.
Since System V semaphores have kernel persistence, we can demonstrate their usage by writing a small set of programs to manipulate them and seeing what happens.
The values of the semaphores will be maintained by the kernel from one of our programs to the next.
Our first program shown in Figure 11.2 just creates a System V semaphore set.
The -e command-line option specifies the IPC-EXCL flag, and the number of semaphores in the set must be specified by the final command-line argument.
The next program, shown in Figure 11.3, removes a semaphore set from the system.
A command of IPc-RMID is executed through the semctl function to remove the set.
Our semsetvalues program (Figure 11.4) sets all the values in a semaphore set.
After obtaining the semaphore ID with semget, we issue an IPC-STAT command to semctl to fetch the semid-ds structure for the semaphore.
The sem-nsems member is the number of semaphores in the set.
We allocate memory for an array of unsigned shorts, one per set member, and copy the values from the command-line into the array.
A command of SETALL to semc t 1 sets all the values in the semaphore set.
Figure 11.5 shows our semgetvalues program, which fetches and prints all the values in a semaphore set.
After obtaining the semaphore ID with semget, we issue an IPC-STAT command to semctl to fetch the semid-ds structure for the semaphore.
The sem-nsems member is the number of semaphores in the set.
We allocate memory for an array of unsigned shorts, one per set member, and issue a command of GETALL to semctl to fetch all the values in the semaphore set.
Our semops program, shown in Figure 11.6, executes an array of operations on a semaphore set.
An option of -n specifies the IPC-NOWAIT flag for each operation, and an option of -u specifies the SEM-UNDO flag for each operation.
Note that the semop function allows us to specify a different set of flags for each member of the sembuf structure (that is, for the operation on each member of the set), but for simplicity we have these command-line options specify that flag for all specified operations.
After opening the semaphore set with semget, an array of sembuf structures is allocated, one element for each operation specified on the command line.
Unlike the previous two programs, this program allows the user to specify fewer operations than members of the semaphore set.
We now demonstrate the five programs that we have just shown, looking at some of the features of System V semaphores.
We first create a file named / tmp/r ich  that will be used (by f tok) to identify the semaphore set.
We now demonstrate the atomicity of the set of operations when performed on a semaphore set.
We specify the nonblocking flag (-n) and three operations, each of which decrements a value in the set.
Since the last operation cannot be performed, and since we specified nonblocking, an error of EAGAIN is returned.
Had we not specified the nonblocking flag, our program would have just blocked.
We then verify that none of the values in the set were changed.
The atomicity of semop means that either all of the operations are performed or none of the operations are performed.
We now demonstrate the SEM-UNDO property of System V semaphores.
This causes all three values to become 0, but since we specify the -u flag to our semops program, the SEM-UNDO flag is specified for each of the three operations.
We then execute our semops program again, but without the -u flag, and tlus leaves the three values at 0 when our semops program terminates.
We can provide a version of our my-lock and my-unlock functions from Figure 10.19, implemented using System V semaphores.
If we start multiple processes at about the same time, each of which calls our my-lock function, only one will create the semaphore (assuming it does not already exist), and then that process initializes the semaphore too.
The first call to semget will return an error of EEXIST to the other processes, which then call semge t again, but without the I PC-CREAT I I PC-EXCL flags.
To avoid this, any process that finds that the semaphore already exists must call semctl with a command of IPC-STAT to look at.
Once this value is nonzero, we know that the process that created the semaphore has initialized it, and has called semop (the call to semop is at the end of this function)
We limit the number of times that we try this, to avoid sleeping forever.
Instead, we allocate two of these structures and fill them in at run time, when the process calls my-lock for the first time.
We specify the SEM-UNDO flag, so that if a process terminates while holding the lock, the kernel will release the lock (see Exercise 10.3)
Creating a semaphore on its first use is easy (each process tries to create it but ignores an error if the semaphore already exists), but removing it after all the processes are done is much harder.
In the case of a printer daemon that uses the sequence number file to assign job numbers, the semaphore would remain in existence all the time.
But other applications might want to delete the semaphore when the file is deleted.
In this case, a record lock might be better than a semaphore.
As with System V message queues, there are certain system limits with System V semaphores, most of which arise from their original System V implementation (Section 3.8)
The first column is the traditional System V name for the kernel variable that contains this limit.
Up to this point each set has been created with semmsl * members.
But this just failed, so try recreating this * final set with one fewer member per set, until it works.
Figure 11.9 Determine the system limits on System V semaphores.
The following changes occur when moving from Posix semaphores to System V semaphores:
When specifying a group of semaphore operations to apply to a set, either all of the operations are performed or none of the operations are performed.
Three operations may be applied to each member of a semaphore set: test for the value being 0, add an integer to the value, and subtract an integer from the value (assuming that the value remains nonnegative)
The only operations allowed for a Posix semaphore are to increment by one and to decrement by one (assuming that the value remains nonnegative)
Creating a System V semaphore set is tricky because it requires two operations to create the set and then initialize the values, which can lead to race conditions.
System V semaphores provide an "undo" feature that reverses a semaphore operation upon process termination.
We showed that the identifier is all we need to know to access a System V message queue (assuming we have adequate permission)
Make similar modifications to Figure 11.6 and show that the same feature applies to System V semaphores.
Once the memory is mapped into the address space of the processes that are sharing the memory region, no kernel involvement occurs in passing data between the processes.
What is normally required, however, is some form of synchronization between the processes that are storing and fetching information to and from the shared memory region.
In Part 3, we discussed various forms of synchronization: mutexes, condition variables, read-write locks, record locks, and semaphores.
Obviously, the kernel must establish the memory mappings that allow the processes to share the memory, and then manage this memory over time (handle page faults, and the like)
Consider the normal steps involved in the client-server file copying program that we used as an example for the various types of message passing (Figure 4.1)
The file data is read by the kernel into its memory and then copied from the kernel to the process.
The server writes this data in a message, using a pipe, FIFO, or message queue.
These forms of IPC normally require the data to be copied from the process to the kernel.
But pipes, FIFOs, and System V message queues all involve copying the data from the process to the kernel for a w r i t e  or msgsnd, or copying the data from the kernel to the process for a read or msgrcv.
The client reads the data from the IPC channel, normally requiring the data to be copied from the kernel to the process.
Finally, the data is copied from the client's buffer, the second argument to the w r i t e  function, to the output file.
A total of four copies of the data are normally required.
Additionally, these four copies are done between the kernel and a process, often an expensive copy (more expensive than copying data within the kernel, or copying data within a single process)
Figure 12.1 depicts this movement of the data between the client and server, through the kernel.
Figure 12.1 Flow of file data from server to client.
The problem with these forms of IPC-pipes, FIFOs, and message queues-is that for two processes to exchange information, the information has to go through the kernel.
Shared memory provides a way around this by letting two or more processes share a region of memory.
The processes must, of course, coordinate or synchronize the use of the shared memory among themselves.
Sharing a common piece of memory is similar to sharing a disk file, such as the sequence number file used in all the file locking examples.
Any of the techniques described in Part 3 can be used for this synchronization.
The steps for the client-server example are now as follows:
The server gets access to a shared memory object using (say) a semaphore.
The server reads from the input file into the shared memory object.
The second argument to the read, the address of the data buffer, points into the shared memory object.
When the read is complete, the server notifies the client, using a semaphore.
The client writes the data from the shared memory object to the output file.
In this figure the data is copied only twice-from the input file into shared memory and from shared memory to the output file.
We draw one dashed box enclosing the client and the shared memory object, and another dashed box enclosing the server and the shared memory object, to reinforce that the shared memory object appears in the address space of both the client and the server.
The concepts involved in using shared memory are similar for both the Posix interface and the System V interface.
But we now store the sequence number in memory instead of in a file.
We first reiterate that memory is not shared by default between a parent and child across a fork.
The program in Figure 12.3 has a parent and child increment a global integer named count.
Since this assumption is false, this semaphore is not really needed.
Notice that we remove the semaphore name from the system by calling sem-unlink, but although this removes the pathname, it has no effect on the semaphore that is already open.
We do this so that the pathname is removed from the filesystem even if the program aborts.
We set standard output unbuffered because both the parent and child will be writing to it.
This prevents interleaving of the output from the two processes.
Figure 123 Parent and child both increment the same global.
If we run this program and look only at the output when the system switches between the parent and child, we have the following:
As we can see, both processes have their own copy of the global count.
Each starts with the value of this variable as 0, and each increments its own copy of this variable.
When f o r k  is called, the child starts with its own copy of the parent's data space.
Figure 12.5 shows the two processes after f o r k  returns.
We see that the parent and child each have their own copy of the variable count.
The mmap function maps either a file or a Posix shared memory object into the address space of a process.
Zen is the number of bytes to map into the address space of the process, starting at offset bytes from the beginning of the file.
The protection of the memory-mapped region is specified by the prot argument using the constants in Figure 12.7
The common value for this argument is PROT-READ I PROT-WRITE for read-write access.
The flags are specified by the constants in Figure 12.8
Either the MAP-SHARED or the MAP-PRIVATE flag must be specified, optionally ORed with MAP-FIXED.
If MAP-PRIVATE is specified, then modifications to the mapped data by the calling process are visible only to that process and do not change the underlying object (either a file object or a shared memory object)
If MAP-SHARED is specified, modifications to the mapped data by the calling process are visible to all processes that are sharing the object, and these changes do modify the underlying object.
If it is not specified, but addr is not a null pointer, then it is implementation dependent as to what the implementation does with addr.
The nonnull value of addr is normally taken as a hint about where the memory should be located.
Portable code should specify addr as a null pointer and should not specify MAP-FIXED.
One way to share memory between a parent and child is to call map with MAP-SHARED before calling fork.
Posix.1 then guarantees that memory mappings in the parent are retained in the child.
Furthermore, changes made by the parent are visible to the child and vice versa.
After map  returns success, the fd argument can be closed.
This has no effect on the mapping that was established by rnrnap.
To remove a mapping from the address space of the process, we call munrnap.
Further references to these addresses result in the generation of a SIGSEGV signal to the process (assuming, of course, that a later call to map does not reuse this portion of the address space)
If the mapped region was mapped using MAP-PRIVATE, the changes made are discarded.
In Figure 12.6, the kernel's virtual memory algorithm keeps the memory-mapped file (typically on disk) synchronized with the memory-mapped region in memory, assuming a MAP-SHARED segment.
That is, if we modify a location in memory that is memory-mapped to a file, then at some time later the kernel will update the file accordingly.
But sometimes, we want to make certain that the file on disk corresponds to what is in the memory-mapped region, and we call msync to perform this synchronization.
The addr and len arguments normally refer to the entire memory-mapped region of memory, although subsets of this region can also be specified.
The flags argument is formed from the combination of constants shown in Figure 12.9
One of the two constants MS-ASYNC and MS-SYNC must be specified, but not both.
The difference in these two is that MS-ASYNC returns once the write operations are queued by the kernel, whereas MS-SYNC returns only after the write operations are complete.
If MS-INVALIDATE is also specified, all in-memory copies of the file data that are inconsistent with the file data are invalidated.
Our description of mmap so far has implied a memory-mapped file: some file that we open and then map into our address space by calling mmap.
Beware of some caveats, however, in that not all files can be memory mapped.
Trying to map a descriptor that refers to a terminal or a socket, for example, generates an error return from map.
These types of descriptors must be accessed using read and wr i t e  (or variants thereof )
Another use of m a p  is to provide shared memory between unrelated processes.
In this case, the actual contents of the file become the initial contents of the memory that is shared, and any changes made by the processes to this shared memory are then copied back to the file (providing filesystem persistence)
This assumes that MAP-SHARED is specified, which is required to share the memory between processes.
We now modify Figure 12.3 (which did not work) so that the parent and child share a piece of memory in which the counter is stored.
To do so, we use a memory-mapped file: a file that we open and then m a p  into our address space.
We have a new command-line argument that is the name of a file that will be memory mapped.
We open the file for reading and writing, creating the file if it does not exist, and then write an integer with a value of 0 to the file.
We call m a p  to map the file that was just opened into the memory of this process.
The first argument is a null pointer, telling the system to pick the starting address.
The length is the size of an integer, and we specify read-write access.
By specifying a fourth argument of MAP-SHARED, any changes made by the parent will be seen by the child, and vice versa.
The return value is the starting address of the memory region that will be shared, and we store it in p t r.
The parent and child both increment the integer counter pointed to by p t r.
Memory-mapped files are handled specially by fork, in that memory mappings created by the parent before calling fork  are shared by the child.
Therefore, what we have done by opening the file and calling m a p  with the MAP-SHARED flag is provide a piece of memory that is shared between the parent and child.
Furthermore, since the shared memory is a memory-mapped file, any changes to the shared memory (the piece of memory pointed to by p t r  of size s izeof  ( i n t  ) ) are also reflected in the actual file (whose name was specified by the command-line argument)
Figure 12.10 Parent and child incrementing a counter in shared memory.
If we execute this program, we see that the memory pointed to by ptr is indeed shared between the parent and child.
We show only the values when the kernel switches between the two processes.
Since the file was memory mapped, we can look at the file after the program terminates with the od program and see that the final value of the counter (20,000) is indeed stored in the file.
We show the semaphore as being in the kernel, but as we mentioned with Posix semaphores, this is not a requirement.
Whatever implementation is used, the semaphore must have at least kernel persistence.
The semaphore could be stored as another memory-mapped file, as we demonstrated in Section 10.15
Figure 12.11 Parent and child sharing memory and a semaphore.
We show that the parent and child each have their own copy of the pointer ptr, but each copy points to the same integer in shared memory: the counter that both processes increment.
Figure 12.12 Counter and semaphore are both in shared memory.
Define structure that will be in shared memory 2-5 W e  define a structure containing the integer counter and a semaphore to protect it.
This structure will be stored in the shared memory object.
The second argument must be nonzero, to indicate that the semaphore is being shared between processes.
Figure 12.13 Counter and semaphore are now in shared memory.
When the purpose of calling m a p  is to provide a piece of mapped memory that will be shared across a fork, we can simplify this scenario, depending on the implementation.
Instead, we specify the flags as MAP-SHARED I MAP-ANON and the fd as -1
SVR4 provides /dev/zero, which we open, and we use the resulting descrip tor in the call to map.
This device returns bytes of 0 when read, and anything written to the device is discarded.
The automatic variables f d  and zero are gone, as is the command-line argument that specified the pathname that was created.
The MAP-ANON flag is specified in the call to mmap, and the fifth argument (the descriptor) is -1
We open /dev/ zero, and the descriptor is then used in the call to map.
For example, in Figure 12.12 the file size is set to the size of our shared structure by w r i t e ,  and this value is also the size of the memory mapping.
But these two sizes-the file size and the memorymapped size-can differ.
We will use the program shown in Figure 12.16 to explore the rnrnap function in more detail.
The file being opened is created if it does not exist, or truncated to a size of 0 if it already exists.
The file is memory mapped, using the size specified as the final command-line argument.
The page size of the implementation is obtained using sysconf and printed.
The memory-mapped region is read (the first byte of each page and the last byte of each page), and the values printed.
We expect one of the references to generate a signal eventually, which will terminate the program.
When the for loop terminates, we print the first byte of the next page, expecting this to fail (assuming that the program has not already failed)
The first scenario that we show is when the file size equals the memory-mapped size, but this size is not a multiple of the page size.
The kernel lets us read and write that portion of the final page beyond our mapping (since the kernel's memory protection works with pages), but anything that we write to this extension is not written to the file.
The -b option says to print the bytes in octal, and the -A d option says to print the addresses in decimal.
The results are similar to our earlier example when the file size and the memory map size were the same (both 5000)
This example generates SIGBUS (which the shell prints as "Bus Error"), whereas the previous example generated SIGSEGV.
The difference is that SIGBUS means we have referenced within our memory-mapped region but beyond the size of the underlying object.
The SIGSEGV in the previous example meant we had referenced beyond the end of our memory-mapped region.
What we have shown here is that the kernel knows the size of the underlying object that is mapped (the file f oo in this case), even though we have closed the descriptor for that object.
Figure 12.18 Memory mapping when m a p  size exceeds file size.
It shows a common technique for handling a file that is growing: specify a memory-map size that is larger than the file, keep track of the file's current size (making certain not to reference beyond the current endof-file), and then just let the file's size increase as more data is written to the file.
We open a file, creating it if it does not exist or truncating it if it already exists.
When we run this program, we see that as we increase the size of the file, we are able to reference the new data through our established memory map.
In Exercise 13.1, we modify our two programs to work with Posix shared memory and see the same results.
Shared memory is the fastest form of IPC available, because one copy of the data in the shared memory is available to all the threads or processes that share the memory.
Some form of synchronization is normally required, however, to coordinate the various threads or processes that are sharing the memory.
This chapter has focused on the mmap function and the mapping of regular files into memory, because this is one way to share memory between related or unrelated processes.
Once we have memory mapped a file, we no longer use read, write, or lseek to access the file; instead, we just fetch or store the memory locations that have been mapped to the file by mmap.
When the memory is to be shared across a subsequent fork, this can be simplified by not creating a regular file to map, but using anonymous memory mapping instead.
This involves either a new flag of MAP-ANON (for Berkeley-derived kernels) or mapping / dev/ zero (for SVR4-derived kernels)
Our reason for covering mmap in such detail is both because memory mapping of files is a useful technique and because mmap is used for Posix shared memory, which is the topic of the next chapter.
Also available are four additional functions (that we do not cover) defined by Posix dealing with memory management:
Assume that System V message queues are used and draw a diagram of how the messages go from the sender to the receiver.
Read the manual page for /dev/zero to determine what happens when the kernel writes the changes back to the file.
What are the contents of the file that is memory mapped?
The parent also creates two pipes; one is used by the child to notify the parent that a message is ready in shared memory, and the other pipe is used by the parent to notify the child that the shared memory is now available.
This allows the parent to select on the read end of the pipe, along with any other descriptors on which it wants to select.
Call our my-shun function (Figure A.46) to allocate the anonymous shared memory object.
Use our msgcreate and msgsnd programs from Section 6.6 to create the message queue, and then place records onto the queue.
The parent should just print the size and type of each message that the child reads.
The previous chapter described shared memory in general terms, along with the m a p function.
Examples were shown that used m a p  to provide shared memory between a parent and child:
We now extend the concept of shared memory to include memory that is shared between unrelated processes.
Posix.1 provides two ways to share memory between unrelated processes.
Memory-mapped pes: a file is opened by open, and the resulting descriptor is mapped into the address space of the process by map.
We described this technique in Chapter 12 and showed its use when sharing memory between a parent and child.
What differs is how the descriptor that is an argument to m a p  is obtained: by open or by shm_open.
Figure 13.1 Posix memory objects: memory-mapped files and shared memory objects.
The name argument used with shxopen  is then used by any other processes that want to share this memory.
The reason for this two-step process, instead of a single step that would take a name and return an address within the memory of the calling process, is that m a p  already existed when Posix invented its form of shared memory.
The reason that sh-open returns a descriptor (recall that mq-open returns an mqd-t value and sem-open returns a pointer to a s e m - t  value) is that an open descriptor is what m a p  uses to map the memory object into the address space of the process.
We described the rules about the name argument in Section 2.2
Note that unlike the mcopen and sem-open functions, the mode argument to shm-open must always be specified.
The return value from s h x o p e n  is an integer descriptor that is then used as the fifth argument to m a p.
The s h x u n l i n k  function removes the name of a shared memory object.
As with all the other un l ink  functions (the un l ink  of a pathname in the filesystem, the m c u n l i n k  of a Posix message queue, and the sem-unlink of a Posix named semaphore), unlinking a name has no effect on existing references to the underlying object, until all references to that object are closed.
Unlinking a name just prevents any subsequent call to open, mcopen, or sem-open from succeeding.
When dealing with map ,  the size of either a regular file or a shared memory object can be changed by calling f t runca te.
Posix defines the function slightly differently for regular files versus shared memory objects.
For a regular file: If the size of the file was larger than length, the extra data is discarded.
If the size of the file was smaller than length, whether the file is changed or its size is increased is unspecified.
Fortunately, almost all Unix implementations support extending a file with f t runca te.
For a shared memory object: f t r u n c a t e  sets the size of the object to length.
We call f t r u n c a t e  to specify the size of a newly created shared memory object or to change the size of an existing object.
When we open an existing shared memory object, we can call f s t a t  to obtain information about the object.
A dozen or more members are in the s ta t  structure (Chapter 4 of APUE talks about all the members in detail), but only four contain information when fd refers to a shared memory object.
We show examples of these two function in the next section.
Unfortunately, Posix.1 does not specify the initial contents of a newly created shared memory object.
The description of the shm-open function states that "The shared memory object shall have a size of 0." The description of f t runcate  specifies that for a regular file (not shared memory), "If the file is extended, the extended area shall appear as if it were zero-filled." But nothing is said in the description of f t runcate  about the new contents of a shared memory object that is extended.
The Posix.1 Rationale states that "If the memory object is extended, the contents of the extended areas are zeros" but this is the Rationale, not the official standard.
If a newly extended piece of shared memory is not initialized to some value (i.e., if the contents are left as is), this could be a security hole.
We now develop some simple programs that operate on Posix shared memory.
Our shmcreate program, shown in Figure 13.2, creates a shared memory object with a specified name and length.
If the -e  option is specified, it is an error if the object already exists.
Since Posix shared memory has at least kernel persistence, this does not remove the shared memory object.
Figure 13.2 Create a Posix shared memory object of a specified size.
Figure 13.3 shows our trivial program that calls shm-unlink to remove the name of a shared memory object from the system.
Figure 13.4 Open a shared memory object and fill it with a pattern.
The shared memory object is opened by shxopen, and we fetch its size with f s tat.
We then map it using mmap and c 1 o s e the descriptor.
Our shmread program, shown in Figure 13.5, verifies the pattern that was written by shmwrite.
The shared memory object is opened read-only, its size is obtained by fstat, it is mapped by mmap (for reading only), and the descriptor is closed.
We see that a file with the same name is created in the filesystem.
Next, we run our shmwrite program and use od to verify that the initial contents are as expected.
We verify the shared memory object's contents with shmread and then unlink the name.
If we run our shmcreate program under Solaris 2.6, we see that a file is created in the / tmp directory with the specified size.
We now provide a simple example in Figure 13.6 to demonstrate that a shared memory object can be memory mapped starting at different addresses in different processes.
Figure 13.6 Shared memory can appear at different addresses in different processes.
We create a shared memory segment whose name is the command-line argument, set its size to the size of an integer, and then open the file /etc /motd.
We fork, and both the parent and child call mmap twice, but in a different order.
When we run this program, we see that the shared memory object is memory mapped at different starting addresses in the parent and child.
The pointers ptrl in the parent and child both point to the same shared memory segment, even though the value of each pointer is different in each process.
We now develop an example similar to the one shown in Section 12.3, in which multiple processes increment a counter that is stored in shared memory.
We store the counter in shared memory and use a named semaphore for synchronization, but we no longer need a parent-child relationship.
Since Posix shared memory objects and Posix named semaphores are referenced by names, the various processes that are incrementing the counter can be unrelated, as long as each knows the IPC names and each has adequate permission for the IPC objects (shared memory and semaphore)
Figure 13.7 shows the server that creates the shared memory object, creates and initializes the semaphore, and then terminates.
We call sh~unlink in case the shared memory object still exists, followed by shm-open to create the object.
The size of the object is set to the size of our shmstruct structure by f truncate, and then mmap maps the object into our address space.
It will be used as a mutex by any process that increments the counter in the shared memory object.
Since Posix shared memory has at least kernel persistence, the object remains in existence until all open references are closed (when this process terminates there are no open references) and explicitly unlinked.
Our program must use different names for the shared memory object and the semaphore.
There is no guarantee that the implementation adds anything to the Posix IPC names to differentiate among message queues, semaphores, and shared memory.
Figure 13.7 Program that creates and initializes shared memory and semaphore.
Figure 13.8 shows our client program that increments the counter in shared memory some number of times, obtaining the semaphore each time it increments the counter.
The memory is mapped into the address space of the process by mmap, and the descriptor is then closed.
The counter is incremented the number of times specified by the command-line argument.
We print the old value of the counter each time, along with the process ID, since we will run multiple copies of this program at the same time.
Figure 13.8 Program that increments a counter in shared memory.
We first start the server and then run three copies of the client in the background.
A server is started that creates a shared memory object in which messages are placed by client processes.
We call these other processes clients, because that is how they appear to our server, but they may well be servers of some form to other clients.
For example, a Telnet server is a client of the syslog daemon when it sends log messages to the daemon.
Instead of using one of the message passing techniques that we described in Part 2, shared memory is used to contain the messages.
This, of course, necessitates some form of synchronization between the clients that are storing messages and the server that is retrieving and printing the messages.
Figure 13.9 Multiple clients sending messages to a server through shared memory.
What we have here are multiple producers (the clients) and a single consumer (the server)
The shared memory appears in the address space of the server and in the address space of each client.
Since we have multiple producers, this variable must be in the shared memory and can be referenced only while the mutex is held.
Overflow counter lo The possibility exists that a client wants to send a message but all the message slots.
But if the client is actually a server of some type (perhaps an FTP server or an HTTP server), the client does not want to wait for the server to free up a slot.
Therefore, we will write our clients so that they do not block but increment the noverflow counter when this happens.
Since this overflow counter is also shared among all the clients and the server, it too requires a mutex so that its value is not corrupted.
The array msgof f contains offsets into the msgdata array of where each message begins.
Be sure to understand that we must use offsets such as these when dealing with shared memory, because the shared memory object can get mapped into a different physical address in each process that maps the object.
That is, the return value from mmap can be different for each process that calls m a p  for the same shared memory object.
For this reason, we cannot use pointers within the shared memory object that contain actual addresses of variables within the object.
Figure 13.11 is our server that waits for a message to be placed into shared memory by one of the clients, and then prints the message.
The object is created by shxopen and then mapped into the address space by map.
The array of offsets is initialized to contain the offset of each message.
Initialize semaphores 24 The four memory-based semaphores in the shared memory object are initialized.
The second argument to sem-init is nonzero for each call, since the semaphore is in shared memory and will be shared between processes.
The first half of the for loop is the standard consumer algorithm: wait for nstored to be greater than 0, wait for the mutex, process the data, release the mutex, and increment nemp t y.
Notice that we fetch the current value of the counter while the noverf lowmutex is held, but then release it before comparing and possibly printing it.
This demonstrates the general rule that we should always write our code to perform the minimum number of operations while a mutex is held.
The first command-line argument is the name of the shared memory object, the next is the number of messages to store for the server, and the last one is the number of microseconds to pause between each message.
By starting multiple copies of our client and specifying a small value for this pause, we can force an overflow to occur, and verify that the server handles it correctly.
Our client follows the basic algorithm for the consumer but instead of calling sem-wait (nempty), which is where the consumer blocks if there is no room in the buffer for its message, we call sem-trywait, which will not block.
If the value of the semaphore is 0, an error of EAGAIN is returned.
It sleeps for the specified number of microseconds, and is implemented by calling either select or poll.
While the mut ex semaphore is held we obtain the value of offset and increment nput, but we then release the mutex before copying the message into the shared memory.
We should do only those operations that must be protected while holding the semaphore.
Figure 13.12 Client that stores messages in shared memory for server.
We first start our server in the background and then run our client, specifying 50 messages with no pause between each message.
But if we run our client again, we see some overflows.
Obviously, in this example, we caused the overflow by having the client generate the messages as fast as it can, with no pause between each message, which is not a typical real-world scenario.
The purpose of this example, however, is to demonstrate how to handle situations in which no room is available for the client's message but the client does not want to block.
This is not unique to shared memory-the same scenario can happen with message queues, pipes, and FIFOs.
Overrunning a receiver with data is not unique to this example.
Section 8.13 of UNPvI talks about this with regard to UDP datagrams, and the UDP socket receive buffer.
In Figure 13.12, our client (the sender) knows when the server's buffer has overflowed, so if this code were placed into a general-purpose function for other programs to call, the function could return an error to the caller when the server's buffer overflows.
Posix shared memory is built upon the m a p  function from the previous chapter.
We first call sh-open, specifying a Posix IPC name for the shared memory object, obtain a descriptor, and then memory map the descriptor with map.
The result is similar to a memory-mapped file, but the shared memory object need not be implemented as a file.
Since shared memory objects are represented by descriptors, their size is set with f t runcate,  and information about an existing object (protection bits, user ID, group ID, and size) is returned by f s t a t.
We do not do this for Posix shared memory, because the implementation would be trivial.
If we are willing to memory map a file (as is done by the Solaris and Digital Unix implementations), then s h o p e n  is implemented by calling open, and shm-unlink is implemented by calling unlink.
Would it be preferable to use p t r  [ i] instead?
System V shared memory is similar in concept to Posix shared memory.
Instead of calling shm-open followed by mmap, we call shmget followed by shmat.
For every shared memory segment, the kernel maintains the following structure of information, defined by including <sys / shm.
We described the i p c s e r m  structure in Section 3.3, and it contains the access permissions for the shared memory segment.
A shared memory segment is created, or an existing one is accessed, by the shmget function.
The return value is an integer called the shared memory identifier that is used with the three other shmXXX functions to refer to this segment.
When a new shared memory segment is created, a nonzero value for size must be specified.
This can be bitwise-ORed with either IPC-CREAT or IPC-CREAT I IPC-EXCL, as discussed with Figure 3.4
Note that shmget creates or opens a shared memory segment, but does not provide.
That is the purpose of the shmat function, which we describe next.
After a shared memory segment has been created or opened by shmget, we attach it to our address space by calling shmat.
The return value from shmat is the starting address of the shared memory segment within the calling process.
If shmaddr is a null pointer, the system selects the address for the caller.
If shmaddr is a nonnull pointer, the returned address depends on whether the caller specifies the SHM-rn~ value for the flag argument:
If SHM-RND is not specified, the shared memory segment is attached at the address specified by the shmaddr argument.
If SHM-~D is specified, the shared memory segment is attached at the address specified by the shmaddr argument, rounded down by the constant SHMLBA.
By default, the shared memory segment is attached for both reading and writing by the calling process, if the process has read-write permissions for the segment.
The SHM-RDONLY value can also be specified in the flag argument, specifying read-only access.
When a process is finished with a shared memory segment, it detaches the segment by calling shmd t.
When a process terminates, all shared memory segments currently attached by the process are detached.
Note that this call does not delete the shared memory segment.
Deletion is accomplished by calling shmctl with a command of IPC-MID, which we describe in the next section.
IPC-RMID Remove the shared memory segment identified by shmid from the system and destroy the shared memory segment.
IPC-SET Set the following three members of the shmid-ds structure for the shared memory segment from the corresponding members in the structure pointed to by the buff argument: shm_perm.uid, s-erm.
The s m c  t ime value is also replaced with the current time.
IPC-STAT Return to the caller (through the buff argument) the current shmid-ds structure for the specified shared memory segment.
We now develop some simple programs that operate on System V shared memory.
Our shmget program, shown in Figure 14.1, creates a shared memory segment using a specified pathname and length.
Figure 14.1 Create a System V shared memory segment of a specified size.
The pathname passed as a command-line argument is mapped into a System V IPC key by f tok.
If the -e option is specified, it is an error if the segment already exists.
Since System V shared memory has at least kernel persistence, this does not remove the shared memory segment.
The shared memory segment is opened by shmget and attached by shmat.
We fetch its size by calling shmc t 1 with a command of I PC-STAT.
Our s h m r e a d  program, shown in Figure 14.4, verifies the pattern that was written by shmwr i t e.
Figure 14.4 Open a shared memory segment and verify its data pattern.
Its size is obtained by calling shmc  t 1 with a command of I PC-STAT.
The pattern written by shmwr i t e is verified.
The pathname used to identify the segment (e.g., the pathname passed to f t o k )  is the pathname of our smet executable.
Using the pathname of a server's executable file often provides a unique identifier for a given application.
We run the ipcs program to verify that the segment has been created.
We notice that the number of attaches (which is stored in the s-nattch member of the shmid-ds structure) is 0, as we expect.
Next, we run our shmwri te program to set the contents of the shared memory segment to the pattern.
We verify the shared memory segment's contents with shmread and then remove the identifier.
We run ipcs to verify that the shared memory segment has been removed.
We have been able to use a relative pathname for the examples in this section because all of the programs have been run from the directory containing the server executable.
Realize that f tok uses the i-node of the file to form the IPC identifier (e.g., Figure 3.2), and whether a given file is referenced by an absolute pathname or by a relative pathname has no effect on the i-node.
As with System V message queues and System V semaphores, certain system limits exist on System V shared memory (Section 3.8)
The first column is the traditional System V name for the kernel variable that contains this limit.
Figure 14.5 Typical system limits for System V shared memory.
System V shared memory is similar in concept to Posix shared memory.
We showed that the identifier is all we need to know to access a System V message queue (assuming we have adequate permission)
Make similar modifications to Figure 14.4 and show that the same feature applies to System V shared memory.
When discussing client-server scenarios and procedure calls, there are three different types of procedure calls, which we show in Figure 15.1
A local procedure call is what we are familiar with from our everyday C programming: the procedure (function) being called and the calling procedure are both in the same process.
Typically, some machine instruction is executed that transfers control to the new procedure, and the called procedure saves machine registers and allocates space on the stack for its local variables.
A remote procedure call (RPC) is when the procedure being called and the calling procedure are in different processes.
We normally refer to the caller as the client and the procedure being called as the server.
In the middle scenario in Figure 15.1, we show the client and server executing on the same host.
This is a frequently occurring special case of the bottom scenario in this figure, and this is what doors provide us: the ability for a process to call a procedure (function) in another process on the same host., One process (a server) makes a procedure available within that process for other processes (clients) to call by creating a door for that procedure.
We can also think of doors as a special type of IPC, since information, in the form function arguments and return values, is exchanged between the client and server.
Historically, doors were developed for the Spring distributed operating system, details of which are available at h t t p  : / /www.
Doors then appeared in Solaris 2.5, although the only manual page contained just a warning that doors were an experimental interface used only by some Sun applications.
With Solaris 2.6, the interface was documented in eight manual pages, but these manual pages list the stability of the interface as "evolving." Expect that changes might occur to the API that we describe in this chapter with future releases of Solaris.
A preliminary version of doors for Linux is being developed: h t t p  : / /www.
The implementation of doors in Solaris 2.6 involves a library (containing the door-XXX functions that we describe in this chapter), which is linked with the user's application (-ldoor), and a kernel filesystem ( /kernel /  sys/doorf s)
Even though doors are a Solaris-only feature, we describe them in detail because they provide a nice introduction to remote procedure calls, without having to deal with any networking details.
We will also see in Appendix A that they are as fast, if not faster, than all other forms of message passing.
Local procedure calls are synchronous: the caller does not regain control until the called procedure returns.
Threads can be thought of as providing a form of asynchronous procedure call: a function is called (the third argument to pthread-create), and both that function and the caller appear to execute at the same.
The caller can wait for the new thread to finish by calling pthread-join.
Remote procedure calls can be either synchronous or asynchronous, but we will see that door calls are synchronous.
Within a process (client or server), doors are identified by descriptors.
Externally, doors may be identified by pathnames in the filesystem.
A server creates a door by calling door-create, whose argument is a pointer to the procedure that will be associated with this door, and whose return value is a descriptor for the newly created door.
The server then associates a pathname with the door descriptor by calling fattach.
A client opens a door by calling open, whose argument is the pathname that the server associated with the door, and whose return value is the client's descriptor for this door.
The client then calls the server procedure by calling door-call.
Naturally, a server for one door could be a client for another door.
We said that door calls are synchronous: when the client calls door-call, this function does not return until the server procedure returns (or some error occurs)
The Solaris implementation of doors is also tied to threads.
Each time a client calls a server procedure, a thread in the server process handles this client's call.
Thread management is normally done automatically by the doors library, creating new threads as they are needed, but we will see how a server process can manage these threads itself, if desired.
This also means that a given server can be servicing multiple client calls of the same server procedure at the same time, with one thread per client.
Since multiple instances of a given server procedure can be executing at the same time (each instance as one thread), the server procedures must be thread safe.
When a server procedure is called, both data and descriptors can be passed from the client to the server.
Both data and descriptors can also be passed back from the server to the client.
Furthermore, since doors are identified by descriptors, this allows a process to pass a door to some other process.
We begin our description of doors with a simple example: the client passes a long integer to the server, and the server returns the square of that value as the long integer result.
We gloss over many details in this example, all of which we cover later in the chapter.
The door is specified by the pathname on the command line, and it is opened by calling open.
The returned descriptor is called the door descriptor, but sometimes we just call it the door.
The arg structure contains a pointer to the arguments and a pointer to the results.
The two members descstr and desc-num deal with the passing of descriptors, which we describe in Section 15.8
Figure 15.2 Client that sends a long integer to the server to be squared.
It consists of a server procedure named servproc and a main function.
The server procedure is called with five arguments, but the only one we use is da tap t  r, which points to the first byte of the arguments.
The long integer argument is fetched through this pointer and squared.
Control is passed back to the client, along with the result, by door- return.
The first argument points to the result, the second is the size of the result, and the remaining two deal with the returning of descriptors.
The first argument is a pointer to the function that will be called for this door (servproc)
After this descriptor is obtained, it must be associated with a pathname in the filesystem, because this pathname is how the client identifies the door.
The main server thread then blocks in a call to pause.
All the work is done by the servproc  function, which will be executed as another thread in the server process each time a client request arrives.
To run this client and server, we first start the server in one window solaris % serverl /tm/serverl.
Figure 15.4 shows a diagram of what appears to be happening with this example.
It appears that door-call calls the server procedure, which then returns.
Figure 15.5 shows what is actually going on when we call a procedure in a different process on the same host.
Figure 15.4 Apparent procedure call from one process to another.
The server process starts first, calls door-create to create a door descriptor referring to the function sewroc, and then attaches this descriptor to a pathname in the filesystem.
The door-call library function performs a system call into the kernel.
The target procedure is identified and control is passed to some doors library function in the target process.
The actual server procedure (named servproc in our example) is called.
The server procedure does whatever it needs to do to handle the client request and calls door-return when it is done.
The client is identified and control is passed back to the client.
The remaining sections describe the doors API in more detail looking at many examples.
In Appendix A, we will see that doors provide the fastest form of IPC, in terms of latency.
The door-call function is called by a client, and it calls a server procedure that is executing in the address space of the server process.
The descriptor fd is normally returned by open (e.g., Figure 15.2)
The pathname opened by the client identifies the server procedure that is called by door-call when this descriptor is the first argument.
The second argument argp points to a structure describing the arguments and the buffer to be used to hold the return values:
All six members of this structure can change on return, as we now describe.
The use of char * for the two pointers is strange and necessitates explicit casts in our code to avoid compiler warnings.
We will see the same use of char * with the first argument to door- return.
Solaris 2.7 will probably change the datatype of desc-num to be an unsigned i n t ,  and the final argument to door-return would change accordingly.
Two types of arguments and two types of results exist: data and descriptors.
The data arguments are a sequence of da t a-s i z e bytes pointed to by da tastr.
The client and server must somehow "know" the format of these arguments (and the results)
For example, no special coding tells the server the datatypes of the arguments.
One way to encapsulate this information (for someone reading the code years later) is to put all the arguments into one structure, all the results into another structure, and define both structures in a header that the client and server include.
Since the client and server deal with binary arguments and results that are packed into an argument buffer and a result buffer, the implication is that the client and server must be compiled with the same compiler.
Sometimes different compilers, on the same system, pack structures differently.
The descriptor arguments are an array of door-desc-t structures, each one containing one descriptor that is passed from the client to the server procedure.
We describe this structure and what it means to "pass a descriptor" in Section 15.8
Upon return, da t a j  t r points to the data results, and da ta-s i z e specifies the size of these results.
If there are no data results, data-size will be 0, and we should ignore da t a s  t r.
Upon return, there can also be descriptor results: d e s c ~ t r  points to an array of door-desc-t structures, each one containing one descriptor that was passed by the server procedure to the client.
The number of door-desc-t structures returned is contained in desc-num.
If there are no descriptor results, desc-num will be 0, and we should ignore descstr.
Using the same buffer for the arguments and results is OK.
That is, datastr and descstr can point into the buffer specified by rbuf when door-call is called.
Before calling door-call, the client sets rbuf to point to a buffer where the results will be stored, and rsize is the buffer size.
Normally upon return, datastr and descstr both point into this result buffer.
If this buffer is too small to hold the server's results, the doors library automatically allocates a new buffer in the caller's address space using mmap (Section 12.2) and updates rbuf and rsize accordingly.
It is the caller's responsibility to notice that rbuf has changed and at some later time to return this buffer to the system by calling munmap with rbuf and rsize as the arguments to munmap.
A server process establishes a server procedure by calling door-create.
In this declaration, we have added our own typedef, which simplifies the function prototype.
This typedef says that door server procedures (e.g., servproc in Figure 15.3) are called with five arguments and return nothing.
When door-create is called by a server, the first argument proc is the address of the server procedure that will be associated with the door descriptor that is the return value of this function.
When this server procedure is called, its first argument cookie is the value that was passed as the second argument to door-create.
This provides a way for the server to cause some pointer to be passed to this procedure every time that procedure is called by a client.
The next four arguments to the server procedure, dataptr, datasize, descptr, and ndesc, describe the data arguments and the descriptor arguments from the client: the information described by the first four members of the door-arg-t structure that we described in the previous section.
The final argument to door-create, attr, describes special attributes of this server procedure, and is either 0 or the bitwise-OR of the following two constants:
DOOR-PRIVATE The doors library automatically creates new threads in the server process as needed to call the server procedures as client requests arrive.
By default, these threads are placed into a process-wide thread pool and can be used to service a client request for any door in the server process.
Specifying the DOOR-PRIVATE attribute tells the library that this door is to have its own pool of server threads, separate from the process-wide pool.
DOOR-UNREF When the number of descriptors referring to this door goes from two to one, the server procedure is called with a second argument (dataptr) of DOOR-UNREF-DATA.
We show some examples of this attribute starting with Figure 15.16
The return value from a server procedure is declared as void because a server procedure never returns by calling return or by falling off the end of the function.
Instead, the server procedure calls door-return, which we describe in the next section.
We saw in Figure 15.3 that after obtaining a door descriptor from door-create, the server normally calls fat tach to associate that descriptor with a pathname in the filesystem.
The client opens that pathname to obtain its door descriptor for its call to door-call.
Also, a function named f detach undoes this association, and a command named f detach just invokes this function.
Door descriptors created by door-create have the FD-CLOEXEC bit set in the descriptor's file descriptor flags.
This means the descriptor will be closed by the kernel if this process calls any of the exec functions.
With regard to fork, even though all descriptors open in the parent are then shared by the child, only the parent will receive door invocations from clients; none are delivered to the child, even though the descriptor returned by door-create is open in the child.
If we consider that a door is identified by a process ID and the address of a server procedure to call (which we will see in the door-inf o-t structure in Section 15.6), then these two rules regarding fork and exec make sense.
A child will never get any door invocations, because the process ID associated with the door is the process ID of the parent that called door-create.
A door descriptor must be closed upon an exec, because even though the process ID does not change, the address of the server procedure associated with the door has 1 no meaning in the newly invoked program that runs after exec.
When a server procedure is done it returns by calling door-return.
This causes the associated door-call in the client to return.
One nice feature of doors is that the server procedure can obtain the client's credentials on every call.
The door-cred-t structure that is pointed to by cred contains the client's credentials on return.
Notice that there is no descriptor argument to this function.
It returns information about the client of the current door invocation, and must therefore be called by the server procedure or some function called by the server procedure.
The door-cred function that we just described provides information for the server about the client.
The client can find information about the server by calling the door-inf o function.
The door-inf o-t structure that is pointed to by info contains information about the server on return.
The cookie pointer that is passed as the first argument to the server procedure is returned as di-data.
The current attributes of the door are contained in d i -a t t r ibutes ,  and we described two of these in Section 25.3: DOOR-PRIVATE and DOOR-UNREF.
Two new attributes are DOOR-LOCAL (the procedure is local to this process) and DOOR-REVOKE (the server has revoked the procedure associated with this door by calling the door-revoke function)
Each door is assigned a systemwide unique number when created, and this is returned as di-uniquif i e r.
This function is normally called by the client, to obtain information about the server.
But it can also be issued by a server procedure with a first argument of DOOR-QUERY: this returns information about the calling thread.
In this scenario, the address of the server procedure ( d i s r o c )  and the cookie (di-data) might be of interest.
We now show some examples of the five functions that we have described.
Figure 15.6 shows a program that opens a door, then calls door-inf o, and prints information about the door.
We open the specified pathname and first verify that it is a door.
The st-mode member of the stat structure for a door will contain a value so that the S-ISDOOR macro is true.
We first run the program specifying a pathname that is not a door, and then run it on the two doors that are used by Solaris 2.6
We use the ps command to see what program is running with the process ID returned by door-inf o.
When describing the door-call function, we mentioned that if the result buffer is too small for the server's results, a new buffer is automatically allocated.
In this version of our program, we print the address of our oval variable, the contents of data_ptr, which points to the result on return from door-call, and the address and size of the result buffer (rbuf and rsize)
When we execute this new client program, we see that a new result buffer has been allocated and d a t a s t r  points to this new buffer.
We can see from this example that we should always reference the server's result.
This new buffer is allocated by mmap and can be returned to the system using munmap.
The client can also just keep using this buffer for subsequent calls to door-call.
This time, we make one change to our servproc function from Figure 15.3: we call the door-cred function to obtain the client credentials.
We first run the client and will see that the effective user ID equals the real user ID, as we expect.
We then become the superuser, change the owner of the executable file to root, enable the set-user-ID bit, and run the client again.
If we look at the server output, we can see the change in the effective user ID the second time we ran the client.
To see the thread management performed by the server, we have the server procedure print its thread ID when the procedure starts executing, and then we have it sleep for 5 seconds, to simulate a long running server procedure.
The sleep lets us start multiple clients while an existing client is being serviced.
Figure 15.9 Server procedure that prints thread ID and sleeps.
It has one argument (a pointer to a thread ID or a null pointer to use the calling thread's ID) and returns a long integer identifier for this thread (often a small integer)
A process can always be identified by an integer value, its process ID.
Even though we do not know whether the process ID is an int or a long, we just cast the return value from getpid to a long and print the value (Figure 9.2)
But the identifier for a thread is a pthread-t datatype (called a thread ID), and this need not be an integer.
Indeed, Solaris 2.6 uses small integers as the thread ID, whereas Digital Unix uses pointers.
Often, however, we want to print a small integer identifier for a thread (as in this example) for debugging purposes.
Our library function, shown in Figure 15.10, handles this problem.
Figure 15.10 pr-thread-id function: return small integer identifier for calling thread.
If the implementation does not provide a small integer identifier for a thread, the function could be more sophisticated, mapping the pthread-t values to small integers and remembering this mapping (in an array or linked l i t )  for future calls.
Returning to Figure 15.9, we run the client three times in a row.
Since we wait for the shell prompt before starting the next client, we know that the 5-second wait is complete at the server each time.
The server output shows that two new threads are created to handle the second and third invocations of the server procedure:
What we can see with this example is that the server process (i.e., the doors library that is linked with our server code) automatically creates saver threads as they are needed.
If an application wants to handle the thread management itself, it can, using the functions that we describe in Section 15.9
We have also verified that the server procedure is a concurrent server: multiple instances of the same server procedure can be running at the same time, as separate threads, servicing different clients.
Another way we know that the server is concurrent is that when we run three clients at the same time, all three results are printed 5 seconds later.
The previous example had only one server procedure in the server process.
Our next question is whether multiple server procedures in the same process can use the same thread pool.
To test this, we add another server procedure to the server process and also recode this example to show a better style for handling the arguments and results between different processes.
Our new procedure takes a long integer input value and returns a double containing the square root of the input.
We define the pathname, input structure, and output structure in our sq r tp roc.
It just calls the two procedures, one after the other, and prints the result.
This program is similar to the other client programs that we have shown in this chapter.
Each prints its thread ID and argument, sleeps for 5 seconds, computes the result, and returns.
The main function, shown in Figure 15.15, opens two door descriptors and associates each one with one of the two server procedures.
Figure 15.13 Client program that calls our square and square root procedures.
If we run the client, it takes 10 seconds to print the results (as we expect)
If we look at the server output, we see that the same thread in the server process handles both client requests.
This tells us that any thread in the pool of server threads for a given process can handle a client request for any server procedure.
We mentioned in Section 15.3 that the DOOR- REF attribute can be specified to door-create as an attribute of a newly created door.
The manual page says that when the number of descriptors referring to the door drops to one (that is, the reference count goes from two to one), a special invocation is made of the door's server procedure.
What is special is that the second argument to the server procedure (the pointer to the data arguments) is the constant DOOR- REF-DATA.
We will demonstrate three ways in which the door is referenced.
The descriptor returned by door-create in the server counts as one reference.
In fact, the reason that the trigger for an unreferenced procedure is the transition of the reference count from two to one, and not from one to 0, is that the server process normally keeps this descriptor open for the duration of the process.
The descriptor returned by open in the client counts as an open reference until the descriptor is closed, either explicitly by calling close or implicitly by the termination of the client process.
In all the client processes that we have shown in this chapter, this close is implicit.
Our first example shows that if the server closes its door descriptor after calling f attach, an unreferenced invocation of the server procedure occurs immediately.
Figure 15.16 shows our server procedure and the server main function.
Our server procedure recognizes the special invocation and prints a message.
When we start the server, we notice that the unreferenced invocation occurs immediately:
If we follow the reference count for this door, it becomes one after door-create returns and then two after f a t t a c h  returns.
The server's call to c l o s e  reduces the count from two to one, triggering the unreferenced invocation.
The only reference left for this door is its pathname in the filesystem, and that is what the client needs to refer to this door.
Furthermore, no further unreferenced invocations of the server procedure occur.
Indeed, only one unreferenced invocation is delivered for a given door.
We now change our server back to the common scenario in which it does not c lose its door descriptor.
We show the server procedure and the server main function in Figure 15.17
We leave in the 6-second sleep and also print when the server procedure returns.
We start the server in one window, and then from another window we verify that the door's pathname exists in the filesystem and then remove the pathname with rm:
As soon at the pathname is removed, the unreferenced invocation is made of the server procedure:
If we follow the reference count for this door, it becomes one after door-create returns and then two after f a t t a c h  returns.
When we r m  the pathname, this command reduces the count from two to one, triggering the unreferenced invocation.
In our final example of this attribute, we again remove the pathname from the filesystem, but onlj~ after starting three client invocations of the door.
What we show is that each client invocation increases the reference count, and only when all three clients.
Figure 15.17 Server that does not close its door descriptor.
If we follow the reference count for this door, it becomes one after door-create returns and then two after f a t t a c h  returns.
As each client calls open, the reference count is incremented, going from two to three, from three to four, and then from four to five.
When we r m  the pathname, the count reduces from five to four.
Then as each client terminates, the count goes from four to three, then three to two, then two to one, and this final decrement triggers the unreferenced invocation.
What we have shown with these examples is that even though the description of the DOOR-UNREF attribute is simple ("the unreferenced invocation occurs when the reference count goes from two to one"), we must understand this reference count to use this feature.
When we think of passing an open descriptor from one process to another, we normally think of either.
In the first example, the process opens a descriptor, calls fork, and then the parent closes the descriptor, letting the child handle the descriptor.
This passes an open descriptor from the parent to the child.
Current Unix systems extend this notion of descriptor passing and provide the ability to pass any open descriptor from one process to any other process, related or unrelated.
Doors provide one API for the passing of descriptors from the client to the server, and from the server to the client.
We described descriptor passing using Unix domain sockets in Section 14.7 of UNPvl.
But an SVR4 process can still access this kernel feature using a Unix domain socket.
Be sure to understand what we mean by passing a descriptor.
In Figure 4.7, the server opens the file and then copies the entire file across the bottom pipe.
But if the server passes a descriptor back to the client, instead of the file.
The client then takes this descriptor and reads the file, writing its contents to standard output.
All the file reading takes place in the client, and the server only opens the file.
Realize that the server cannot just write the descriptor number across the bottom pipe in Figure 4.7, as in.
Suppose the value of f d is 4 in the server.
Even if this descriptor is open in the client, it almost certainly does not refer to the same file as descriptor 4 in the server process.
The only time descriptor numbers mean something from one process to another is across a fork or across an exec.
Some kernel black magic is involved in descriptor passing, but APIs like doors and Unix domain sockets hide all these internal details, allowing processes to pass descriptors easily from one process to another.
Descriptors are passed across a door from the client to server by setting the d e s c q t r  member of the door-arg-t structure to point to an array of door-desc-t structures, and setting door-num to the number of these structures.
Descriptors are passed from the server to the client by setting the third argument of door- return to point to an array of door-desc-t structures, and setting the fourth argument to the number of descriptors being passed.
This structure contains a union, and the first member of the structure is a tag that identifies what is contained in the union.
But currently only one member of the union is defined (a d-desc structure that describes a descriptor), and the tag (d-attributes) must be set to DOOR-DESCRIPTOR.
We modify our file server example (recall Figure 1.9) so that the server opens the file, passes the open descriptor to the client, and the client then copies the file to standard output.
Figure 15.18 File server example with server passing back open descriptor.
The pathname associated with the door is a command-line argument and the door is opened.
The filename that the client wants opened is read from standard input and the trailing newline is deleted.
We add one to the size of the pathname to allow the server to null terminate the pathname.
We call the server procedure and then check that the result is what we expect: no data and one descriptor.
We will see shortly that the server returns data (containing an error message) only if it cannot open the file, in which case, our call to err-quit prints that error.
The descriptor is fetched from the door-desc-t structure, and the file is copied to standard output.
The server main function has not changed from Figure 15.3
We null terminate the client's pathname and try to open the file.
If an error occurs, the data result is a string containing the error message.
Figure 15.20 Server procedure that opens a file and passes back its descriptor.
We start the server and specify its door pathname as / tmpl f dl and then run the client:
The first two times, we specify a pathname that causes an error return, and the third time, the server returns the descriptor for a 2-line file.
There is a problem with descriptor passing across a door.
To see the problem in our example, just add a printf to the server procedure after a successful open.
You will see that each descriptor value is one greater than the previous descriptor value.
The problem is that the server is not closing the descriptors after it passes them to the client.
The logical place to perform the close would be after door-return returns, once the descriptor has been sent to the client, but door-return does not return! If we had been.
But the doors paradigm for passing descriptors is different from these two techniques, since no return occurs from the function that passes the descriptor.
The only way around this problem is for the server procedure to somehow remember that it has a descriptor open and close it at some later time, which becomes very messy.
This problem should be fixed in Solaris 2.7 with the addition of a new DOOR-RELEASE attribute.
The sender sets d -a t t r i bu t e s  to DOOR-DESCRIPTOR I DOOR-RELEASE, which tells the system to close the descriptor after passing it to the receiver.
We showed with Figure 15.9 that the doors library automatically creates new threads as needed to handle the client requests as they arrive.
These are created by the library as detached threads, with the default thread stack size, with thread cancellation disabled, and with a signal mask and scheduling class that are initially inherited from the thread that called door-create.
If we want to change any of these features or if we want to manage the pool of server threads ourselves, we call door-server-create and specify our own server creation procedure.
As with our declaration of door-create in Section 15.3, we use C's typedef to simplify the function prototype for the library function.
Our new datatype defines a server creation procedure as taking a single argument (a pointer to a door-inf o-t structure), and returning nothing (void)
Our server creation procedure is called whenever a new thread is needed to service a client request.
Information on which server procedure needs the thread is in the door-info-t structure whose address is passed to the creation procedure.
The disroc member contains the address of the server procedure, and di-data contains the cookie pointer that is passed to the server procedure each time it is called.
An example is the easiest way to see what is happening.
In our server, we add two new functions in addition to our server procedure function and our server main function.
Figure 15.21 shows an overview of the four functions in our server process, when some are registered, and when they are all called.
Figure 15.21 Overview of the four functions in our server process.
Figure 15.22 main function for example of thread pool management.
This tells the library that this door will have its own pool of threads, called a private server pool.
Specifying a private server pool with DOOR-PRIVATE and specifying a server creation procedure with door-server-create are independent.
Default: no private server pools and no server aeation procedure.
The system creates threads as needed, and they all go into the process-wide thread pool.
The system creates threads as needed, and they go into the process-wide pool for doors created without DOOR-PRIVATE or into a door's private server pool for doors created with DOOR-PRIVATE.
The server creation procedure is called whenever a new thread is needed, and these threads all go into the process-wide thread pool.
The server creation procedure is called whenever a new thread is needed.
When a thread is created, it should call door-bind to assign itself to the appropriate private server pool, or the thread will be assigned to the process-wide pool.
Figure 15.23 shows our two new functions: my-create is our server creation procedure, and it calls my-thread as the function that is executed by each thread that it creates.
The thread is created and starts executing the my-thread function.
The argument to this function is a pointer to the door-inf o-t structure.
If we have a server with multiple doors and we specify a server creation procedure, this one server creation procedure is called when a new thread is needed for any of the doors.
The only way for this server creation procedure and the thread start function that it specifies to pthread-create to differentiate between the different server procedures is to look at the diaroc pointer in the door-inf o-t structure.
The latter will not work with doors, because the doors library requires that the kernel lightweight process performing the door-return be the same lightweight process that originated the invocation.
The reason for requiring that the thread be created as a detached thread is to prevent the system from saving any information about the thread when it terminates, because no one will be calling pthread-j oin.
Thread start function 15-20 my-thread is the thread start function specified by the call to pthread-create.
The argument is the pointer to the door-info-t structure that was passed to my-create.
The only server procedure that we have in this process is servproc, and we just verify that the argument references this procedure.
This call is issued from within the doors library before door-create returns.
But the variable f d will not contain the door descriptor until door-create returns.
Since we know that my-thread is running as a separate thread from the main thread that calls door-create, our solution to this timing problem is to use the mutex fdlock as follows: the main thread locks the mutex before calling door-create and unlocks the mutex when door-create returns and a value has been stored into f d (Figure 15.22)
Our my-thread function just locks the mutex (probably blocking until the main thread has unlocked the mutex) and then unlocks it.
We could have added a condition variable that the main thread signals, but we don't need it here, since we know the sequence of calls that will occur.
When a new Posix thread is created by pthread-create, thread cancellation is enabled by default.
When cancellation is enabled, and a client aborts a door-call that is in progress (which we will demonstrate in Figure 15.31), the thread cancellation handlers (if any) are called, and the thread is then terminated.
When cancellation is disabled (as we are doing here), and a client aborts a door-call that is in progress, the server procedure complctcs (the thread is not terminated), and the results from door-return are just discarded.
Since the server thread is terminated when cancellation is enabled, and since the server procedure may be in the middle of an operation for the client (it may hold some locks or semaphores), the doors library disables thread cancellation for all the threads that it creates.
If a server procedure wants to be canceled when a client terminates prematurely, that thread must enable cancellation and must be prepared to deal with it.
But the cancellation mode can be set only by the thread itself once it is running.
Indeed, even though we just disable cancellation, a thread can enable and disable cancellation whenever it wants.
Since we need the door descriptor for this call, we made f d a global variable for this version of our server.
This version is identical to the one in Figure 15.9
As soon as the server starts and door-create is called, our server creation procedure is called the first time, even though we have not even started the client.
This creates the first thread, which will wait for the first client call.
We then run the client three times in a row:
The doors library appears to always keep one extra thread ready.
We then execute the client three times, all at about the same time in the background.
It binds the calling thread to the private server pool associated with the door whose descriptor is fd.
If the calling thread is already bound to some other door, an implicit unbind is performed.
A door descriptor can be revoked only by the process that created the descriptor.
Any door invocation that is in progress when this function is called is allowed to complete normally.
All our examples so far have assumed that nothing abnormal happens to either the client or server.
We now consider what happens when errors occur at either the client or server.
Realize that when the client and server are part of the same process (the local procedure call in Figure 15.1), the client does not need to worry about the server crashing and vice versa, because if either crashes the entire process crashes.
But when the client and server are distributed to two processes, we must consider what happens if one of the two crashes and how the peer is notified of this failure.
This is something we must worry about regardless of whether the client and server are on the same host or on different hosts.
While the client is blocked in a call to door-call, waiting for results, it needs to know if the server thread terminates for some reason.
This terminates just this thread, not the entire server process.
When we run our client, we see that an error of EINTR is returned by door-call if the server procedure terminates before returning.
The door-call manual page warns that this function is not a restartable system call.
The door-call function in the doors library invokes a system call of the same name.
Therefore, about 2 seconds after the client parent calls door-call, the parent catches SIGCHLD and the signal handler returns, interrupting the door-call system call.
The client sees the same error as if the server procedure terminated prematurely: EINTR.
This means we must block any signals that might be generated during a call to door- call from being delivered to the process, because those signals will interrupt door-call.
What if we know that we just caught a signal, detect the error of EINTR from door-call, and call the server procedure again, since we know that the error is from our caught signal and not from the server procedure terminating prematurely? This can lead to problems, as we will show.
Figure 15.28 Server procedure that prints its thread ID when called and when returning.
We declare the global caught-sigchld and set this to one when the SIGCHLD signal is caught.
This second time, the server procedure proceeds to completion and the expected result is returned.
But looking at the server output, we see that the server procedure is called twice.
Figure 15.29 Client that calls door-call again after receiving EINTR.
When the client calls door-call the second time, after the first call is interrupted by the caught signal, this starts another thread that calls the server procedure a second time.
But if the server procedure is not idempotent, this is a problem.
The term idempotent, when describing a procedure, means the procedure can be called any number of times without harm.
Our server procedure, which calculates the square of a number, is idempotent: we get the correct result whether we call it once or twice.
Another example is a procedure that returns the current time and date.
The classic example of a nonidempotent procedure is one that subtracts some amount from a bank account: the end result is wrong unless this procedure is called only once.
We now see how a server procedure is notified if the client terminates after calling door-call but before the server returns.
This function schedules a SIGALRM signal for 3 seconds in the future, but since we do not catch this signal, its default action terminates the process.
This will cause the client to terminate before door-call returns, because we will put a 6-second sleep in the server procedure.
Figure 15.31 shows our server procedure and its thread cancellation handler.
Figure 15.31 Server procedure that detects premature termination of client.
When the system detects that the client is terminating with a door-call in progress, the server thread handling that call is sent a cancellation request.
If the server thread has cancellation disabled, nothing happens, the thread executes to completion (when it calls door-return), and the results are then discarded.
If cancellation is enabled for the server thread, any cleanup handlers are called, and the thread is then terminated.
In our server procedure, we first call p thread-s e t cancel s t a t e to enable cancellation, because when the doors library creates new threads, it disables thread cancellation.
This function also saves the current cancellation state in the variable oldstate, and we restore this state at the end of the function.
All our function does is print that the thread has been canceled, but this is where a server procedure can do whatever must be done to clean up after the terminated client: release mutexes, write a log file record, or whatever.
We also put a 6-second sleep in our server procedure, to allow the client to abort while its door- call is in progress.
When we run our client twice, we see that the shell prints "Alarm clock" when our process is killed by a SIGALRM signal.
If we look at the corresponding server output, we see that each time the client terminates prematurely, the server thread is indeed canceled and our cleanup handler is called.
The reason we ran our client twice is to show that after the thread with an ID of 4 is canceled, a new thread is created by the doors library to handle the second client invocation.
Doors provide the ability to call a procedure in another process on the same host.
In the next chapter we extend this concept of remote procedure calls by describing the calling of a procedure in another process on another host.
A server calls door-create to create a door and associate it with a server procedure, and then calls f a t t a c h  to attach the door to a pathname in the filesystem.
The client calls open on this pathname and then door-call to call the server procedure in the server process.
Normally, the only permission testing performed for a door is that done by open when it creates the door, based on the client's user IDS and group IDS, along with the permission bits and owner IDS of the pathname.
One nice feature of doors that we have not seen with the other forms of IPC in this text is the ability of the server to determine the clienvs credentials: the client's effective and real user IDS, and effective and real group IDS.
These can be used by the server to determine whether it wants to service this client's request.
Doors allow the passing of descriptors from the client to the server and vice versa.
This is a powerful technique, because so much in Unix is represented by a descriptor:
When calling procedures in  another process, we  must worry about premature termination of the peer, something w e  d o  not need to worry about with local procedure calls.
A doors client is notified if the server thread terminates prematurely by an error return of EINTR from door-call.
A doors server thread is notified if its client terminates while the client is blocked in a call to door-call by the receipt of a cancellation request for the server thread.
The server thread must decide whether to handle this cancellation or  not.
Exercises How many bytes of information are passed as arguments by door-call from the client to the server? In Figure 15.6, do we need to call f stat to first verify that the descriptor is a door? Remove this call and see what happens.
When we build an application, our first choice is whether to.
If we choose the second option, the next choice is whether to.
Most of this text has focused on case (2a): IPC between processes on the same host, using message passing, shared memory, and possibly some form of synchronization.
When we require network communications among the various pieces of the application, most applications are written using explicit network programming, that is, direct calls to either the sockets API or the XTI API, as described in UNPv1
Using the sockets API, clients call socket, connect, read, and w r i t e ,  whereas servers call socket, bind, l i s t e n ,  accept,  read, and wri te.
An alternative way to write a distributed application is to use implicit network programming.
We code our application using the familiar procedure call, but the calling process (the client) and the process containing the procedure being called (the server) can be executing on different hasts.
The fact that the client and server are running on different hosts, and that network 110 is involved in the procedure call, is for the most part transparent.
Indeed, one metric by which to measure any RPC package is how transparent it makes the underlying networking.
The client calls the server's procedure with a long integer argument, and the return value is the square of that value.
We define two structures, one for the arguments (a single long), and one for the results (a single long)
We define an RPC program named SQUARE-PROG that consists of one version (SQUARE-VERS), and in that version is a single procedure named SQUAREPROC.
The argument to this procedure is a square- in structure, and its return value is a square-out structure.
We say more about these program numbers in Figure 16.9
We compile this specification file using a program supplied with the Sun RPC package, rpcgen.
The next program we write is the client main function that calls our remote procedure.
Declare client handle We declare a client handle named cl.
Client handles are intended to look like standard I/O FILE pointers (hence the uppercase name of CLIENT)
We call clnt-create, which returns a client handle upon success.
As with standard I/O FILE pointers, we don't care what the client handle points to.
It is probably some structure of information that is maintained by the RPC runtime system.
The first argument to clnt-create is either the hostname or IP address of the host running our server.
The second argument is the program name, and the third argument is the version number, both from our square.
The final argument is our choice of protocol, and we normally specify either TCP or UDP.
Similarly, the CLIENT handle is normally the final argument to the RPC functions.
The return value is a pointer to the result structure.
Notice that we allocate room for the input structure, but the RPC runtime allocates the result structure.
In our square .x specification file, we named our procedure SQUAREPROC, but from the client we call squareproc-1
On the server side, all we write is our server procedure, which we show in Figure 16.3
Figure 16.3 Server procedure that is called using Sun RPC.
This allows two ANSI C function prototypes in the square.h header, one for the function called by the client in Figure 16.2 (which had the client handle as an argument) and one for the actual server function (which has different arguments)
When our server procedure is called, the first argument is a pointer to the input structure, and the second argument is a pointer to a structure passed by the RPC runtime that contains information about this invocation (which we ignore in this simple procedure)
The result is stored in a structure whose address is the return value from this function.
Since we are returning the address of a variable from the function, that variable cannot be an automatic variable.
We declare it as s t a t i c.
Astute readers will note that this prevents our server function from being thread safe.
We discuss this in Section 16.2 and show a thread-safe version there.
We now compile our client under Solaris and our server under BSD/OS, start the server, and run the client.
The first time we specify the server's hostname, and the second time its IP address.
This demonstrates that the c ln t- create  function and the RPC runtime functions that it calls allow either a hostname or an IP address.
We now demonstrate some error returns from cln t- create  when either the host does not exist, or the host exists but is not running our server.
We have written a client and server and shown their use without any explicit network programming at all.
Our client just calls two functions (clnt- create and squareproc-I), and on the server side, we have just written the function squareproc-1-svc.
All the details involving XTI under Solaris, sockets under BSD/OS, and network I/O are handled by the RPC runtime.
This is the purpose of RPC: to allow the programming of distributed applications without requiring explicit knowledge of network programming.
Another important point in this example is that the two systems, a Sparc running Solaris and an Intel x86 running BSD/OS, have different byte orders.
That is, the Sparc is big endian and the Intel is little endian (which we show in Section 3.4 of UNPvl)
These byte ordering differences are also handled automatically by the runtime library, using a standard called XDR (external data representation), which we discuss in Section 16.8
More steps are involved in building this client and server than in the other p r e grams in this text.
Here are the steps involved in building the client executable:
The -C option to rpcgen tells it to generate ANSI C prototypes in the square.
Our library (with functions used in this book) is l ibunpipc .a, and -Ins1 specifies the system library with the networking functions under Solaris (which includes the RPC and XDR runtime)
We see similar commands when we build the server, although rpcgen does not need to be run again.
This generates a client and server that both run under Solaris.
Figure 16.4 summarizes the files and steps required to build our client-server example.
The three shaded boxes are the files that we must write.
Figure 16.5 summarizes the steps that normally take place in a remote procedure call.
The sever is started and it registers itself with the port mapper on the server host.
The client is then started, and it calls clnt-create, which contacts the port mapper on the server host to find the server's ephemeral port.
We do not show these steps in the figure and save our detailed description for Section 16.3
The client calls a local procedure, called the client stub.
To the client, the client stub appears to be the actual server procedure that it wants to call.
The purpose of the stub is to package up the arguments to the remote procedure, possibly put them into some standard format, and then build one or more network messages.
The packaging of the client's arguments into a network message is termed marshaling.
The client routines and the stub normally call functions in the RPC runtime library (e.g., clnt-create in our earlier example)
When link editing under Solaris, these runtime functions are loaded from the -1nsl library, whereas under BSD/OS, they are in the standard C library.
These network messages are sent to the remote system by the client stub.
This normally requires a system call into the local kernel (e.g., write or sendto)
The typical networking protocols used for this step are either TCP or UDP.
A server stub procedure is waiting on the remote system for the client's request.
When the server procedure is finished, it returns to the server stub, returning whatever its return values are.
The server stub converts the return values, if necessary, and marshals them into one or more network messages to send back to the client.
The messages are transferred back across the network to the client.
This step appears to be a normal procedure return to the client.
Xerox was implementing RPC on workstations before most people knew what workstations were! A Unix implementation of Courier was distributed for many years with the 4.x BSD releases, but today Courier is of historical interest only.
It was developed by Bob Lyon, who had left Xerox in 1983 to join Sun.
The original releases of Sun RPC were written using the sockets API and worked with either TCP or UDI? The publicly available source code release was called RPCSRC.
Publicly available source code implementations of both are available from f tp : / /playground.
Probably the most widespread application that uses Sun RPC is NFS, Sun's network filesystem.
Normally, NFS is not built using the standard RPC tools, rpcgen and the RPC runtime library that we describe in this chapter.
Instead, most of the library routines are hand-optimized and reside within the kernel for performance reasons.
Nevertheless, most systems that support NFS also support Sun RPC.
In the mid-1980s, Apollo competed against Sun in the workstation market, and they designed their own RPC package to compete against Sun's, called NCA (Network Computing Architecture), and their implementation was called NCS (Network Computing System)
Apollo was acquired by Hewlett Packard in 1989, and NCA was developed into the Open Software Foundation's Distributed Computing Environment (DCE), of which RPC is a fundamental element from which most pieces are built.
More information on DCE is available from ht tp : / /www.
An implementation of the DCE RPC package has been made publicly available at f tp : / /gatekeeper.
This directory also contains a 171-page document describing the internals of the DCE RPC package.
Sun RPC is more widespread than DCE RPC, probably because of its freely available implementation and its packaging as part of the basic system with most versions of Unix.
Widespread porting of the publicly available implementation has not o&curred, although a Linux port is underway.
All three RPC packages-Courier, Sun RPC, and DCE RPC-are amazingly similar, because the basic RPC concepts are the same.
Most Unix vendors provide additional, detailed documentation on Sun RPC.
For example, the Sun documentation is available at http: //docs.
In this chapter, we assume TI-RPC (the transport independent version of RPC mentioned earlier) for most examples, and we talk about TCP and UDP as the supported protocols, even though TI-RPC supports any protocols that are supported by the host.
Recall Figure 15.9, in which we showed the automatic thread management performed by a doors server, providing a concurrent server by default.
We now show that Sun RPC provides an iterative server by default.
We start with the example from the previous section and modify only the server procedure.
Although we cannot tell from this output, a 5-second wait occurs between the printing of each result by the client.
If we look at the server output, we see that the clients are handled iteratively: the first client's request is handled to completion, and then the second client's request is handled to completion, and finally the t h d  client's request is handled to completion.
One thread handles all client requests: the server is not multithreaded by default.
Our doors servers in Chapter 15 all ran in the foreground when started from the shell, as in solaris % server.
That allowed us to place debugging calls to p r i n t f  in our server procedures.
But Sun RPC servers, by default, run as daemons, performing the steps as outlined in Section 12.4 of UNPvl.
This requires calling syslog from the server procedure to print any diagnostic information.
What we have done, however, is specify the C compiler flag -DDEBUG when we compile our server, which is the equivalent of placing the line.
This stops the server main function from making itself a daemon, and leaves it connected to the terminal on which it was started.
That is why we can call p r i n t f  from our server procedure.
The provision for a multithreaded server appeared with Solaris 2.4 and is enabled by a -M command-line option to rpcgen.
This makes the server code generated by rpcgen thread safe.
Another option, -A, has the server automatically create threads as.
Both the client and server also require source code changes, which we should expect, given our use of static in Figure 16.3
Nothing changes in the declarations of the procedure's argument and result structures.
Declare variable to hold result 8 We declare a variable of type square-out, not a pointer to this type.
Instead of this function returning a pointer to the result (as in Figure 16.2), it now returns either RPC-SUCCESS or some other value if an error occurs.
The changes required for multithreading involve the function arguments and return value.
The pointer to the svc-req structure moves to the third position.
The return value is now TRUE upon success, or FALSE if an error is encountered.
This function is called from the server stub after the server procedure returns and after the result has been sent to the client.
In our example, we just call the generic xdr-f ree routine.
If our server procedure had allocated any storage necessary to hold the result (say a linked list), it would free that memory from this new function.
We build our client and server and again run three copies of the client at the same time:
This time we can tell that the three results are printed one right after the other.
Looking at the server output, we see that three threads are used, and all run simultaneously.
One unfortunate side effect of the source code changes required for multithreading is that not all systems support this feature.
That means if we want to compile and run a program on both types of systems, we need # i f  def s to handle the differences in the calling sequences at the client and server ends.
Of course, a nonthreaded client on BSD/OS, say, can still call a multithreaded server procedure running on Solaris, but if we have an RPC client (or server) that we want to compile on both types of systems, we need to modify the source code to handle the differences.
We first note that any host running an RPC server must be running the port mapper.
When a client starts, it must first contact the port mapper on the server's host, ask for the server's ephemeral port number, and then contact the server on that ephemeral port.
The port mapper is providing a name service whose scope is confined to that system.
Although many implementations use this port by default, and some older implementations still have this port number hardcoded into the client and server, most current implementations allow other ports to be used.
Most NFS clients also contact the port mapper on the server host to obtain the port number.
The reason for this change is that the term "port" implied Internet ports, whereas the TI-RPC package can work with any networking protocol, not just TCF and UDI? We will use the traditional name of port mapper.
Also, in our discussion that follows, we assume that TCP and UDP are the only protocols supported on the server host.
The steps performed by the server and client are as follows:
When the system goes into multiuser mode, the port mapper is started.
When our server starts, its main function (which is part of the server stub that is generated by rpcgen) calls the library function svc-create.
This function determines the networking protocols supported by the host and creates a transport endpoint (e.g., socket) for each protocol, binding an ephemeral port to the TCP and UDP endpoints.
It then contacts the local port mapper to register the TCP and UDP ephemeral port numbers with the RPC program number and version number.
The port mapper is itself an RPC program and the server registers itself with the port mapper using RPC calls (albeit to a known port, 111)
We can see all the RPC programs that are registered with the port mapper by executing the rpcinf o program.
We can execute this program to verify that port number 111 is used by the port mapper itself:
The mapping from the RPC program number to the service name is normally found in the file / etc /rpc.
Our server process then goes to sleep, waiting for a client request to arrive.
This could be a new TCP connection on its TCP port, or the arrival of a UDP datagram on its UDP port.
If we execute rpcinf o after starting our server from Figure 16.3, we see.
We also assigned a version number of 1 in that figure.
Notice that a server is ready to accept clients using either TCP or UDP, and the client chooses which of these two protocols to use when it creates the client handle (the final argument to clnt-create in Figure 16.2)
The arguments (Figure 16.2) are the server's hostname or IP address, the program number, version number, and a string specifying the protocol.
An RPC request is sent to the server host's port mapper (normally using UDP as the protocol for this RPC message), asking for the information on the specified program, version, and protocol.
Assuming success, the port number is saved in the client handle for all future RPC calls using this handle.
The rpcinfo program shows the programs currently registered on your system.
Another source of information on the RPC programs supported on a given system is normally the.
Description defined by Sun definedbyuser transient (for customer-written applications) reserved.
By default, servers created by rpcgen can be invoked by the inetd superserver.
Examining the server stub generated by rpcgen shows that when the server main starts, it checks whether standard input is a XTT endpoint and, if so, assumes it was started by inetd.
Backing up, after creating an RPC server that will be invoked by inetd, the / e t c / ine td.
As an example, here is one line (wrapped to fit on this page) from the Solaris configuration file:
The next field specifies a XTI endpoint (as opposed to a socket endpoint), and the third field specifies that all visible datagram protocols are supported.
Looking at the file /etc/netconf ig, there are two of these protocols: UDP and /dev/ cl t s.
Chapter 29 of UNPvl describes this file and XTI addresses.
The fourth field, wait, tells inetd to wait for this server to terminate before monitoring the XTI endpoint for another client request.
The next field, root, specifies the user ID under which the program will run, and the last two fields are the pathname of the executable and the program name with any.
When a UDP datagram arrives for port 32779, i n e t d  will detect that a datagram is ready to be read and it will fo rk  and then exec the program / u s r / l i b / n e t s v c / r s t a t  / r p c.
Assuming this program was generated by rpcgen, it will detect that standard input is a XTI endpoint and initialize it accordingly as an RPC server endpoint.
This is done by calling the RMZ functions svc-tli-create and svc-reg, two functions that we do not cover.
The second function does not register this server with the port mapper-that is done only once by i n e t d  when it starts.
The RPC server loop, a function named svc-run, will read the pending datagram and call the appropriate server procedure to handle the client's request.
Normally, servers invoked by i n e t d  handle one client's request and terminate, allowing ine td  to wait for the next client request.
As an optimization, RPC servers generated by rpcgen wait around for a small amount of time (2 minutes is the default) in case another client request arrives.
If so, this existing server that is already running will read the datagram and process the request.
This avoids the overhead of a fork and an exec for multiple client requests that arrive in quick succession.
This will generate SIGCHLD for inetd,  causing it to start looking for arriving datagrams on the XTI endpoint again.
By default, there is no information in an RPC request to identify the client.
The server replies to the client's request without worrying about who the client is.
The client must tell the RMZ runtime to include its identification (hostname, effective user ID, effective group ID, and supplementary group IDS) with each request.
We first call auth-destroy to destroy the previous authentication associated with this client handle, the null authentication that is created by default.
The remainder of the client has not changed from Figure 16.7
We do not show the squarejrog-2-f  r e e r e s u l t  function, wluch does not change.
We now use the pointer to the svc-req structure that is always passed as an argument to the server procedure.
The rq-cred member contains the raw authentication information, and its oa-f lavor member is an integer that identifies the type of authentication.
The term "raw" means that the RPC runtime has not processed the information pointed to by oa-base.
But if the authentication type is one supported by the runtime, then the cooked credentials pointed to by rq-clntcred have been processed by the runtime into some structure appropriate for that type of authentication.
We print the type of authentication and then check whether it equals AUTH-SYS.
For Unix authentication, the pointer to the cooked credentials (rq-clntcred) points to an authsys~arms structure containing the client's identity:
We obtain the pointer to this structure and print the client's hostname, effective user ID, and effective group ID.
If we start our server and run the client once, we get the following output from the server:
Unix authentication is rarely used, because it is simple to defeat.
We can easily build our own RPC packets containing Unix authentication information, setting the user ID and group IDS to any values we want, and send it to the server.
The server has no way to verify that we are who we claim to be.
Actually, NFS uses Unix authentication by default, but the requests are normally sent by the NFS client's kernel and usually with a reserved port (Section 2.7 of UNPvl)
Some NFS servers are configured to respond to a client's request only if it arrives from a reserved port.
If you.are trusting the client host to mount your filesystems, you are trusting that client's kernel to identify its users correctly.
If a reserved port is not required by the server, then hackers can write their own programs that send NFS requests to an NFS server, setting the Unix authentication IDS to any values desired.
Even if a reserved port is required by the server, if you have your own system on which you have superuser privileges, and you can plug your system into the network, you can still send your own NFS requests to the server.
A common analogy is a picture ID (passport, driver's license, or whatever)
There are also different forms of credentials: a picture is better than just listing the height, weight, and sex, for example.
If we had an ID card without any form of identifying information (library cards are often examples of this), then we would have credentials without any verifier, and anyone could use the card and claim to be the owner.
In the case of null authentication, both the credentials and the verifier are empty.
With Unix authentication, the credentials contain the hostname and the user and group IDS, but the verifier is empty.
Other forms of authentication are supported, and the credentials and verifiers contain other information:
AUTH-SHORT An alternate form of Unix authentication that is sent in the verifier field from the server back to the client in the RPC reply.
It is a smaller amount of information than full Unix authentication, and the client can send this back to the server as the credentials in subsequent requests.
The intent of this type of credential is to save network bandwidth and server CPU cycles.
AUTH-DES DES is an acronym for the Data Encryption Standard, and this form of authentication is based on secret key and public key cryptography.
This scheme is also called secure RPC, and when used as the basis for NFS, this is called secure NFS.
AUTH-KERB This scheme is based on MIT's Kerberos system for authentication.
We now look at the timeout and retransmission strategy used by Sun RPC.
The total timeout is the total amount of time that a client waits for the server's reply.
The retry timeout is used only by UDP and is the amount of time between retransmissions of the client's request, waiting for the server's reply.
First, no need exists for a retry timeout with TCP because TCP is a reliable protocol.
If the server host never receives the client's request, the client's TCP will time out and retransmit the request.
When the server host receives the client's request, the server's TCP will acknowledge its receipt to the client's TCP.
If the server's acknowledgment is lost, causing the client's TCP to retransmit the request, when the server TCP receives this duplicate data, it will be discarded and another acknowledgment sent by the server TCI? With a reliable protocol, the reliability (timeout, retransmission, handling of duplicate data or duplicate ACKs) is provided by the transport layer, and is not a concern of the RPC runtime.
One request sent by the client RPC layer will be received as one request by the server RPC layer (or the client RPC layer will get an error indication if the request never gets acknowledged), regardless of what happens at the network and transport layers.
After we have created a client handle, we can call c ln t -cont ro l  to both query and set options that affect the handle.
This is similar to calling f c n t  1 for a descriptor, or calling getsockopt  and se tsockopt  for a socket.
I Returns: TRUE if OK, FALSE on error cl is the client handle, and what is pointed to by ptr depends on the request.
We modify our client from Figure 16.2 to call this function and print the two timeouts.
We now specify the protocol as another command-line argument and use this as the final argument to clnt-create.
The first argument to c ln t -cont ro l  is the client handle, the second is the request, and the third is normally a pointer to a buffer.
Our first request is CLGET-TIMEOUT, which returns the total timeout in the t imeval  structure whose address is the third argument.
Therefore, if the return value is FALSE, we print nothing.
Figure 16.12 Client that queries and prints the two RPC timeout values.
To see what is happening here, look at the client stub, the function squareproc-1 in the file square-clnt.
This function calls a library function named c ln t -ca l l ,  and the final argument is a t imeval  structure named.
This argument is used until the client explicitly sets the total timeout by calling c ln t -cont ro l  with a request of CLSET-TIMEOUT.
If we want to change the total timeout, we should call c ln t -cont ro l  and should not modify the structure in the client stub.
The only way to verify the UDP retry timeout is to watch the packets using tcpdurnp.
This shows that the first datagram is sent as soon as the client starts, and the next datagram is about 15 seconds later.
If we watch the TCP client-server that we just described using t cpdump, we see TCP's three-way handshake, followed by the client sending its request, and the server acknowledging this request.
About 25 seconds later, the client sends a FIN, which is caused by the client process terminating, and the remaining three segments of the TCP connection termination sequence follow.
Section 2.5 of UNPvl describes these segments in more detail.
We want to show the following characteristics of Sun RPC's usage of TCP connections: a new TCP connection is established by the client's call to clnt-create ,  and this connection is used by all procedure calls associated with the specified program and version.
A client's TCP connection is terminated either explicitly by calling clnt-des t r o y  or implicitly by the termination of the client process.
We start with our client from Figure 16.2 and modify it to call the server procedure twice, then call clnt- destroy, and then pause.
This shows that one TCP connection is created (by the call to c ln t -c rea te )  and is used for both client requests.
The connection is then terminated by the call to clnt-destroy, even though our client process does not terminate.
Another part of the timeout and retransmission strategy is the use of a transaction ID or XID to identify the client requests and server replies.
When a client issues an RPC call, the RPC runtime assigns a 32-bit integer XID to the call, and this value is sent in the.
The XID does not change when the RPC runtime retransmits a request.
The client verifies that the XID of the reply equals the XID that was sent with the request; otherwise the client ignores this reply.
If TCP is being used, the client should rarely receive a reply with the incorrect XID, but with UDP, and the possibility of retransmitted requests and a lossy network, the receipt of a reply with the incorrect XID is a definite possibility.
The server is allowed to maintain a cache of the replies that it sends, and one of the items that it uses to determine whether a request is a duplicate is the XID.
The TI-RPC package uses the following algorithm for choosing an XID for a new request, where the A operator is C's bitwise exclusive OR:
To enable the RPC runtime to maintain a duplicate request cache, the server must call svc-dg-enablecache.
Once this cache is enabled, there is no way to turn it off (other than termination of the server process)
The address of this structure is an argument to the server procedure.
When this cache is enabled, the server maintains a FIFO (first-in, first-out) cache of all the replies that it sends.
Each time the RPC runtime in the server receives a client request, it first searches the cache to see whether it already has a reply for this request.
If so, the cached reply is returned to the client instead of calling the server procedure again.
The purpose of the duplicate request cache is to avoid calling a server procedure multiple times when duplicate requests are received, probably because the server procedure is not idempotent.
A duplicate request can be received because the reply was lost or because the client retransmission passes the reply in the network.
Notice that this duplicate request cache applies only to datagram protocols such as UDP, because if TCP is being used, a duplicate request never makes it to the application; it is handled completely by TCP (see Exercise 16.6)
In Figure 15.29, we showed a doors client that retransmitted its request to the server when the client's call to door-call was interrupted by a caught signal.
But we then showed that this caused the server procedure to be called twice, not once.
We then categorized server procedures into those that are idempotent (can be called any number of times without harm), and those that are not idempotent (such as subtracting money from a bank account)
Procedure calls can be placed into one of three categories:
This type of operation is hard to achieve, owing to the possibility of server crashes.
At most once means the procedure was not executed at all or it was executed once.
If a normal return is made to the caller, we know the procedure was executed once.
But if an error return is made, we're not certain whether the procedure was executed once or not at all.
At least once means the procedure was executed at least once, but perhaps more.
This is OK for idempotent procedures-the client keeps transmitting its request until it receives a valid response.
But if the client has to send its request more than once to receive a valid response, a possibility exists that the procedure was executed more than once.
With a local procedure call, if the procedure returns, we know that it was executed exactly once, but if the process crashes after the procedure has been called, we don't know whether it was executed once or not at all.
If TCP is being used and a reply is received, we know that the remote procedure was called exactly once.
But if a reply is not received (say the server crashes), we don't know whether the server procedure executed to completion before the host crashed, or whether the server procedure had not yet been called (at-mostonce semantics)
Providing exactly-once semantics in the face of server crashes and extended network outages requires a transaction processing system, something that is beyond the capability of an RPC package.
If UDP is being used without a server cache and a reply is received, we know that the server procedure was called at least once, but possibly more than once (at-least-once semantics)
If UDP is being used with a server cache and a reply is received, we know that the server procedure was called exactly once.
But if a reply is not received, we have at-most-once semantics, similar to the TCP scenario.
Always use TCP unless the overhead of the TCP connections is excessive for the application.
Use a transaction processing system for nonidempotent procedures that are important to do correctly (i.e., bank accounts, airline reservations, and the like)
For a nonidempotent procedure, using TCP is preferable to UDP with a server cache.
Using UDP without a server cache for an idempotent procedure is OK.
Using UDP without a server cache for a nonidempotent procedure is dangerous.
We cover additional advantages of TCP in the next section.
We now consider what happens when either the client or the server terminates prematurely and TCP is being used as the transport protocol.
Since UDP is connedionless, when a process with an open UDP endpoint terminates, nothing is sent to the peer.
All that will happen in the UDP scenario when one end crashes is that the peer will time out, possibly retransmit, and eventually give up, as discussed in the previous section.
We first terminate the server prematurely while it is processing a client's request.
In our server procedure, we add a call to the abor t  function.
This terminates the server process, causing the server's TCP to send a FIN to the client, which we can verify with tcpdump.
We first run our Solaris client to our BSD/OS server: solaris % client bsdi 22 tcp bsdi: RPC: Unable to receive; An event requires attention.
When the server's FIN is received by the client, the RPC runtime is waiting for the server's reply.
It detects the unexpected reply and returns an error from our call to squareproc-1
The error (RPC-CANTRECV) is saved by the runtime in the client handle, and the call to clnt-sperror  (from our Clnt-create wrapper function) prints this as "Unable to receive." The remainder of the error message, "An event requires attention," corresponds to the XTI error saved by the runtime, and is also printed by clnt-sperror.
If we swap the hosts for the client and server, we see the same scenario, with the same error returned by the RPC runtime (RPC-CANTRECV), but a different message at the end.
The Solaris server that we aborted above was not compiled as a multithreaded server, and when we called abort,  the entire process was terminated.
Things change if we are running a multithreaded server and only the thread servicing the client's call.
To force this scenario, we replace the call to abort with a call to pthread-exit ,  as we did with our doors example in Figure 15.25
We run our client under BSD/OS and our multithreaded server under Solaris.
When the server thread terminates, the TCP connection to the client is not closed; it remains open in the server process.
Therefore, no FIN is sent to the client, so the client just times out.
We would see the same error if the server host crashed after the client's request was sent to the server and acknowledged by the server's TCl?
When an RPC client terminates while it has an RPC procedure call in progress using TCP, the client's TCP will send a FIN to the server when the client process terminates.
Our question is whether the server's RPC runtime detects this condition and possibly notifies the server procedure.
Recall from Section 15.11 that a doors server thread is canceled when the client prematurely terminates.
Since the client does not catch SIGALRM, the process is terminated by the kernel about 3 seconds before the server's reply is sent.
We run our client under BSD/OS and our server under Solaris.
This is what we expect at the client, but nothing different happens at the server.
If we watch what happens with tcpdump we see the following:
When the client terminates (about 3 seconds after starting), the client TCP sends a FIN to the server, which the server TCP acknowledges.
The TCP term for this is a half-close (Section 18.5 of TCPvl)
About 6 seconds after the client and server started, the server sends its reply, which its TCP sends to the client.
Sending data across a TCP connection after receiving a FIN is OK, as we describe on pp.
The client TCP responds with an RST (reset), because the client process has terminated.
This will be recognized by the server on its next read or write on the connection, but nothing happens at this time.
They may time out when no response is received, but they cannot tell the type of error: premature process termination, crashing of the peer host, network unreachability, and so on.
An RPC client or server using TCP has a better chance of detecting problems at the peer, because premature termination of the peer process automatically causes the peer TCP to close its end of the connection.
But this does not help if the peer is a threaded RPC server, because termination of the peer thread does not close the connection.
Also this does not help detect a crashing of the peer host, because when that happens, the peer TCP does not close its open connections.
A timeout is still required to handle all these scenarios.
When we used doors in the previous chapter to call a procedure in one process from another process, both processes were on the same host, so we had no data conversion problems.
But with RPC between different hosts, the various hosts can use different data formats.
As a comparison, in the OSI world, ASN.1 (Abstract Syntax Notation one) is the normal way to describe the data, and BER (Basic Encoding Rules) is a common way to encode the data.
This scheme also uses explicit typing, which means each data value is preceded by some value (a "specifier") describing the datatype that follows.
In our example, the stream of bytes would contain the following fields, in order: a specifier that the next value is an integer, the integer value, a specifier that the next value is an integer, the integer value, a specifier that the next value is a floating point value, the floating point value, a specifier that the next value is a character string, the character string.
The XDR representation of all datatypes requires a multiple of 4 bytes, and these bytes are always transmitted in the big-endian byte order.
Signed integer values are stored using two's complement notation, and floating point values are stored using the IEEE format.
When describing XDR and the datatypes that it supports, we have three items to consider:
Which C datatype does rpcgen convert this to in the.
What is the actual format of the data that is transmitted?
To generate this table, an RPC specification file was created using all the supported XDR datatypes.
The file was run through rpcgen and the resulting C header examined.
We now describe the table entries in more detail, referencing each by the number in the first column (1-15)
A cons t declaration is turned into a C #define.
Indeed, these decade-old XDR names are unfortunate in today's world.
Your compiler may allow long double, but treat it as a double.
The percent sign at the beginning of the line tells rpcgen to place the remainder of the line in the.
Figure 16.14 Summary of datatypes supported by XDR and rpcgen.
An enumeration is equivalent to a signed integer and is the same as C's enum datatype.
Fixed-length opaque data is a specified number of bytes (n) that are transmitted as 8-bit values, uninterpreted by the runtime library.
Variable-length opaque data is also a sequence of uninterpreted bytes that are transmitted as 8-bit values, but the actual number of bytes is transmitted as an unsigned integer and precedes the data.
When sending this type of data (eg , when filling in the arguments prior to an RPC call), set the length before making the call.
When this type of data is received, the length must be examined to determine how much data follows.
The maximum length m can be omitted in the declaration.
But if the length is specified at compile time, the runtime library will check that the actual length (what we show as the var-len member of the structure) does not exceed the value of m.
In memory, a string is stored as a normal null-terminated C character string, but when a string is transmitted, it is preceded by an unsigned integer that specifies the actual number of characters that follows (not including the terminating null)
When sending this type of data, the runtime determines the number of characters by calling strlen.
When this type of data is received, it is stored as a null-terminated C character string.
The maximum length m can be omitted in the declaration.
But if the length is specified at compile time, the runtime library will check that the actual length does not exceed the value of m.
A fixed-length array of any datatype is transmitted as a sequence of n elements of that datatype.
A variable-length away of any datatype is transmitted as an unsigned integer that specifies the actual number of elements in the array, followed by the array elements.
The maximum number of elements m can be omitted in the declaration.
But if this maximum is specified at compile time, the runtime library will check that the actual length does not exceed the value of m.
A structure is transmitted by transmitting each member in turn.
A discriminated union is composed of an integer discriminant followed by a set of datatypes (called arms) based on the value of the discriminant.
When a discriminated union is transmitted, the 32-bit value of.
The default declaration is often void, which means that nothing is transmitted following the 32-bit value of the discriminant.
The XDR declaration looks like a C pointer declaration, and that is what the generated.
Figure 16.16 summarizes the encoding used by XDR for its various datatypes.
We now show an example of XDR but without RPC.
This technique can be used to write files in a machineindependent format or to send data to another computer across a network in a machineindependent format.
Figure 16.16 Encoding used by XDR for its various datatypes.
If the discriminant value is RESULT-INT, then an integer value is transmitted after the discriminant value.
If the discriminant value is RESULT-DOUBLE, then a double precision floating point value is transmitted after the discriminant value; otherwise, nothing is transmitted after the discriminant value.
Since we do not declare any RPC procedures, if we look at all the files generated by rpcgen in Figure 16.4, we see that the client stub and server stub are not generated by rpcgen.
The contents of this header are what we expect, given the conversions shown in Figure 16.14
The fundion name suffix of -data comes from the name of our structure in Figure 16.15
The first program that we write is called w r i t e.
We first set all the members of the d a t a  structure to some nonzero value.
In the case of variable-length fields, we must set the count and that number of values.
We call malloc to allocate room for the buffer that the XDR routines will store into, since it must be aligned on a 4-byte boundary, and just allocating a cha r  array does not guarantee this alignment.
The runtime function xdrmem-create initializes the buffer pointed to by buff for XDR to use as a memory stream.
We allocate a variable of type XDR named xhandle and pass the address of this variable as the first argument.
The XDR runtime maintains the information in this variable (buffer pointer, current position in the buffer, and so on)
The final argument is XDR-ENCODE, which tells XDR that we will be going from host format (our ou t  structure) into XDR format.
Encode the structure -36 We call the xdr-data function, which was generated by rpcgen in the file.
The function xdr-getpos returns the current position of the XDR runtime in the output buffer (i.e., the byte offset of the next byte to store into), and we use this as the size of our wri te.
Figure 16.19 shows our read program, which reads the file that was written by the previous program, printing the values of all the members of the da ta  structure.
We call malloc to allocate a buffer that is suitably aligned and read the file that was generated by the previous program into the buffer.
We initialize an XDR memory stream, this time specifying XDR-DECODE to indicate that we want to convert from XDR format into host format.
We initialize our i n  structure to 0 and call xdr-data to decode the buffer buff into our structure in.
We must initialize the XDR destination to 0 (the i n  structure), because some of the XDR routines (notably xdr-string) require this.
This value is saved in the XDR handle (xhandle) by xdrmem-create and then used by the XDR runtime to determine whether to encode or decode the data.
We print all the members of our d a t a  structure.
We call xdr-f r e e  to free the dynamic memory that the XDR runtime might have allocated (see also Exercise 16.10)
We now run our w r i t e  program on a Sparc, redirecting standard output to a file named data:
Figure 16.19 Read the data structure in XIlR format and print the values.
If we read this binary data file under BSD/OS or under Digital Unix, the results are what we expect:
Unfortunately, no simple way exists to calculate the total size required by the XDR encoding of a given.
Just calculating the sizeof the structure is wrong, because each member is encoded separately by XDR.
What we must do is go through the structure, member by member, adding the size that will be used by the XDR encoding of each member.
For example, Figure 16.21 shows a simple structure with three members.
For a fixed-length array, we calculate the size of each element and multiply this by the number of elements.
But we cannot calculate a size for a variable-length declaration without a maximum, such as float e o.
The easiest solution is to allocate a buffer that should be larger than needed, and check for failure of the XDR routines (Exercise 16.5)
There are three ways to specify optional data in an XDR specification file, all of which we show in Figure 16.23
When the discriminant flag is TRUE, a long value follows; otherwise, nothing follows.
When encoded by the XDR runtime, this will be encoded as either.
This is a handy way of encoding optional data when the data is referenced in our code by a pointer.
One implementation detail that makes the first two declarations generate identical encodings is that the value of TRUE is 1, which is also the length of the variable-length array when one element is present.
Figure 16.25 is a simple program that sets the values of the three arguments so that none of the long values are encoded.
Allocate suitably aligned buffer and encode 5-19 We allocate a buffer and encode our out structure into an XDR memory stream.
This shows exactly what has been encoded into the buffer by the XDR runtime:
Figure 16.26 is a modification of the previous program that assigns values to all three arguments, encodes them into an XDR memory stream, and prints the stream.
To assign a value to the variable-length array, we set the array length to 1, and its associated pointer points to the value.
To assign a value to the third argument, we set the pointer to the address of the value.
Given the capability to encode optional data from the previous example, we can extend XDR's pointer notation and use it to encode and decode linked lists containing a variable number of elements.
Our example is a linked list of name-value pairs, and Figure 16.27 shows the XDR specification file.
Our my1 is t structure contains one name-value pair and a pointer to the next structure.
The last structure in the list will have a null next pointer.
Figure 16.29 is our program that initializes a linked list containing three name-value pairs and then calls the XDR runtime to encode it.
Our reason for initializing the list in this order is just to show that the XDR runtime follows the pointers, and the order of the linked list entries that are encoded has nothing to do with which array entries are being used.
We have also initialized the values to hexadecimal values, because we will print the long integer values in hex, because this makes it easier to see the ASCII values in each byte.
If XDR decodes a linked list of this form, it will dynamically allocate memory for the list entries and pointers, and link the pointers together, allowing us to traverse the list easily in C.
Figure 16.30 shows the format of an RPC request when encapsulated in a TCP segment.
Since TCP is a byte stream and provides no message boundaries, some method of.
Sun RPC defines a record as either a request or reply, and each record is composed of one or more fragments.
If the final-fragment bit is 0, then additional fragments make up the record.
If UDP is being used instead of TCP, the first field following the UDP header is the XID, as we show in Figure 16.32
With TCP, virtually no limit exists to the size of the RPC request and reply, because any number of fragments can be used and each fragment has a 31-bit length field.
The names that we show in Figure 16.30 were taken from this specification.
The contents of the variable-length opaque data containing the credentials and verifier depend on the flavor of authentication.
For Unix authentication, the opaque data contains the following information:
When the credential flavor is AUTH-SYS, the verifier flavor should be AUTH-NONE.
The format of an RPC reply is more complicated than that of a request, because errors can occur in the request.
Figure 16.32 shows the format of a successful RPC reply, this time showing the UDP encapsulation.
Figure 16.32 Successful RPC reply encapsulated as a UDP datagram.
The call can be rejected by the server if the RPC version number is wrong or if an authentication error occurs.
Sun RPC allows us to code distributed applications with the client running on one host and the server on another host.
We first define the server procedures that the client can call and then write an RPC specification file that describes the arguments and return values for each of these procedures.
We then write the client m a i n  function that calls the server procedures, and the server procedures themselves.
The client code appears to just call the server procedures, but underneath the covers, network communication is taking place, hidden by the various RPC runtime routines.
The rpcgen program is a fundamental part of building applications using RPC.
It reads our specification file, and generates the client stub and the server stub, as well as generating functions that call the required XDR runtime routines that will handle all the data conversions.
The XDR runtime is also a fundamental part of this process.
As we showed, we can use XDR by itself, independent of the RPC package, just for exchanging data in a standard format using any form of communications to actually transfer the data (programs written using sockets or XTI, floppy disks, CD-ROMs, or whatever)
Each host that runs an RPC server must run a program named the port mapper (now called RPCBIND)
When an RPC client starts, it contacts the port mapper on the server's host to obtain the desired port number, and then contacts the server itself, normally using either TCP or UDP.
By default, no authentication is provided by RPC clients, and RPC servers handle any client request that they receive.
This is the same as if we were to write our own client-server using either sockets or XTI.
Sun RPC provides three additional forms of authentication: Unix authentication (providing the client's hostname, user ID, and group IDS), DES authentication (based on secret key and public key cryptography), and Kerberos authentication.
Understanding the timeout and retransmission strategy of the underlying RPC package is essential to using RPC (or any form of network programming)
When a reliable transport layer such as TCP is used, only a total timeout is needed by the RPC client, as any lost or duplicated packets are handled completely by the transport layer.
When an unreliable transport such as UDP is used, however, the RPC package has a retry timeout in addition to a total timeout.
A transaction ID is used by the RPC client to verify that a received reply is the one desired.
Any procedure call can be classified as having exactly-once semantics, at-most-once semantics, or at-least-once semantics.
With local procedure calls, we normally ignore this issue, but with RPC, we must be aware of the differences, as well as understanding the difference between an idempotent procedure (one that can be called any number of times without harm) and one that is not idempotent (and must be called only once)
Sun RPC is a large package, and we have just scratched the surface.
Nevertheless, given the basics that have been covered in this chapter, complete applications can be written.
Using rpcgen hides many of the details and simplifies the coding.
The Sun manuals refer to various levels of RPC coding-the simplified interface, top level, intermediate level, expert level, and bottom level-but these categorizations are meaningless.
The number of functions provided by the RPC runtime is 164, with the division as follows:
Exercises When we start one of our servers, it registers itself with the port mapper.
But if we terminate it, say with our terminal interrupt key, what happens to this registration? What happens if a client request arrives at some time later for this server? We have a client-server using RPC with UDP, and it has no server reply cache.
The client sends a request to the server but the server takes 20 seconds before sending its reply.
The client times out after 15 seconds, causing the server procedure to be called a second time.
What happens to the server's second reply? The XDR s t r i n g  datatype is always encoded as a length followed by the characters.
What happens? Now remove the maximum length specifier from the s t r i n g  declaration, that is, write s t r i n g  v s t r i n g - a r g o  and compare the data-xdr.
In Section 16.5, we described the duplicate request cache that can be enabled when UDP is being used.
We could say that TCP maintains its own duplicate request cache.
What will the sizes be if we use UDP instead of TCP? Can an RPC client on a system that does not support threads call a server procedure that has been compiled to support threads? What about the differences in the arguments that we described in Section 16.2?
But where is the string stored that is pointed to by vstr ing- arg? Modify the program to verify your assumption.
Furthermore, every server stub generated by rpcgen automatically defines this procedure (which you can easily verify by looking at any of the server stubs generated by the examples in this chapter)
The null procedure takes no arguments and returns nothing, and is often used for verifying that a given server is running, or to measure the round-trip time to the server.
But if we look at the client stub, no stub is generated for this procedure.
Look up the manual page for the c ln t - ca l l  function and use it to call the null procedure for any of the servers shown in this chapter.
Run the program and watch its memory size using ps.
Then move the ending brace to follow the call to xdr-f ree and run the program again, watching its memory size.
This text has described in detail four different techniques for interprocess communication (IPC):
Message passing and procedure calls are often used by themselves, that is, they normally provide their own synchronization.
The synchronization techniques are sometimes used by themselves; that is, without the other forms of IPC.
After covering 16 chapters of details, the obvious question is: which form of IPC should be used to solve some particular problem? Unfortunately, there is no silver bullet regarding IPC.
The vast number of different types of IPC provided by Unix indicates that no one solution solves all (or even most) problems.
All that you can do is become familiar with the facilities provided by each form of IPC and then compare the features with the needs of your specific application.
We first list four items that must be considered, in case they are important for your application.
We assume that this decision has already been made and that IPC is being used between processes or threads on a single host.
If the application might be distributed across multiple hosts, consider using sockets instead of IPC, to simplify the later move to a networked application.
As of 1998, most Unix systems support System V IPC (messages, semaphores, and shared memory), whereas only a few support Posix IPC (messages, semaphores, and shared memory)
Many Unix systems support Posix threads (which include mutexes and condition variables) or should support them in the near future.
Some systems that support Posix threads do not support the process-shared attributes of mutexes and condition variables.
The read-write locks required by Unix 98 should be adopted by Posix, and many versions of Unix already support some type of read-write lock.
Memory-mapped I/O is widespread, and most Unix systems also provide anonymous memory mapping (either /dev/zero or MAP-ANON)
Sun RPC should be available on almost all Unix systems, whereas doors are a Solaris-only feature (for now)
If this is a critical item in your design, run the programs developed in Appendix A on your own systems.
Better yet, modify these programs to simulate the environment of your particular application and measure their performance in this environment.
If you need this feature and your system supports the Posix realtime scheduling option, consider the Posix functions for message passing and synchronization (message queues, semaphores, mutexes, and condition variables)
For example, when someone posts to a Posix semaphore on which multiple threads are blocked, the thread that is unblocked is chosen in a manner appropriate to the scheduling policies and parameters of the blocked threads.
System V semaphores, on the other hand, make no such guarantee.
To help understand some of the features and limitations of the various types of IPC, we summarize some of the major differences:
Pipes and FIFOs are byte streams with no message boundaries.
Posix messages and System V messages have record boundaries that are maintained from the sender to the receiver.
With regard to the Internet protocols described in UNPv1, TCP is a byte stream, but UDP provides messages with record boundaries.
Posix message queues can send a signal to a process or initiate a new thread when a message is placed onto an empty queue.
No similar form of notification is provided for System V message queues.
The bytes of data in a pipe or FIFO are first-in, first-out.
Posix messages and System V messages have a priority that is assigned by the sender.
When reading a Posix message queue, the highest priority message is always returned first.
When reading a System V message queue, the reader can ask for any priority message that it wants.
When a message is placed onto a Posix or System V message queue, or written to a pipe or FIFO, one copy is delivered to exactly one thread.
Mutexes, condition variables, and read-write locks are all unnamed: they are memory-based.
They can be shared easily between the different threads within a single process.
They can be shared between different processes only if they are stored in memory that is shared between the different processes.
Posix semaphores, on the other hand, come in two flavors: named and memory-based.
Named semaphores can always be shared between different processes (since they are identified by Posix IPC names), and memory-based semaphores can be shared between different processes if the semaphore is stored in memory that is shared between the different processes.
System V semaphores are also named, using the key-t datatype, which is often obtained from the pathname of a file.
Mutexes, condition variables, read-write locks, and Posix semaphores do not have this feature.
Each fcntl lock is associated with some range of bytes (what we called a "record") in the file referenced by the descriptor.
Read-write locks are not associated with any type of record.
Posix shared memory and System V shared memory both have kernel persistence.
They remain in existence until explicitly deleted, even if they are not currently being used by some process.
The size of a Posix shared memory object can be extended while the object is being used.
The size of a System V shared memory segment is fixed when it is created.
The kernel limits for the three types of System V IPC often require tuning by the system administrator, because their default values are usually inadequate for real-world applications (Section 3.8)
The kernel limits for the three types of Posix IPC usually require no tuning at all.
No standard way exists to obtain this information about Posix IPC objects.
If the implementation uses files in the filesystem for these objects, then the information is available with the stat function or with the Is command, if we know the mapping from the Posix.
But if the implementation does not use files, this information may not be available.
Of the various message passing techniques-pipes, FIFOs, and Posix and System V message queues-the only functions that can be called from a signal handler are read and write (for pipes and FIFOs)
Of all the message passing techniques, only doors accurately provide the client's identity to the server (Section 15.5)
We now develop some simple programs to measure the performance of these types of IPC, so we can make intelligent decisions about when to use a particular form of PC.
When comparing the different forms of message passing, we are interested in two measurements.
The bandwidth is the speed at which we can move data through the IPC channel.
To measure this, we send lots of data (millions of bytes) from one process to another.
The latency is how long a small IPC message takes to go from one process to another and back.
We measure this as the time for a 1-byte message to go from one process to another, and back (the round-trip time)
In the real world, the bandwidth tells us how long bulk data takes to be sent across an IPC channel, but IPC is also used for small control messages, and the time required by the system to handle these small messages is provided by latency.
To measure the various forms of synchronization, we modify our program that increments a counter in shared memory, with either multiple threads or multiple processes incrementing the counter.
Since the increment is a simple operation, the time required is dominated by the time of the synchronization primitives.
The numbers shown in this Appendix are provided to let us compare the techniques described in this book.
An ulterior motive is to show how simple measuring these values is.
Before making choices among the various techniques, you should measure these performance numbers on your own systems.
Unfortunately, as easy as the numbers are to measure, when anomalies are detected, explaining these is often very hard, without access to the source code for the kernel or libraries in question.
We now summarize all the results from this Appendix, for easy reference when going through the various programs that we show.
The following lines were added to the Solaris / e t c / sys tern file:
The same changes were accomplished with Digital Unix by specifying the following lines as input to the Digital Unix sysconf i g  program:
As we might expect, the bandwidth normally increases as the size of the message increases.
The decrease in bandwidth above 4096 bytes for Solaris is probably caused by the configuration of the internal message queue limits.
For comparison with UNPv1, we also show the values for a TCP socket and a Unix domain socket.
These two values were measured using programs in the lmbench package using only 65536-byte messages.
For the TCP socket, the two processes were both on the same host.
Figure A.l Latency to exchange a I-byte message using various forms of IPC.
In Section A.4, we show the programs that measured the first six values, and the remaining three are from the lmbench suite.
For the TCP and UDP measurements, the two processes were on the same host.
Pipe - - - - _ - - - -
Each thread increments the counter 1,000,000 times, and the number of threads incrementing the counter varied from one to five.
The reason for increasing the number of threads is to verify that the code using the synchronization technique is correct and to see whether the time starts increasing nonlinearly as the number of threads increases.
We can measure f cnt 1 record locking only for a single thread, because this form of synchronization works between processes and not between multiple threads within a single process.
Under Digital Unix, the times become very large for the two types of Posix semaphores with more than one thread, indicating some type of anomaly.
That is, the threads do nothing but synchronization, and the lock is held essentially all the time.
Since the threads are created with process contention scope, by default, each time a thread loses its timeslice, it probably holds the lock, so the new thread that is switched to probably blocks immediately.
The results are similar to the threaded numbers, although the two forms of Posix semaphores are now similar for Solaris.
We plot only the first value for f cntl record locking, since the remaining values are so large.
We again see some type of anomaly for Posix semaphores under Digital Unix when multiple processes are involved.
Time required to increment a counter in shared memory (seconds)
Figure A.ll llrne required to increment a counter in shared memory (Solaris 2.6)
Time required to increment a counter in shared memory (seconds)
This section shows the three programs that measure the bandwidth of pipes, Posix message queues, and System V message queues.
Figure A.14 shows an overview of the program that we are about to describe.
Figure A.14 Overview of program to measure the bandwidth of a pipe.
Figure A.15 shows the first half of our b w j i p e  program, which measures the bandwidth of a pipe.
Start-time ( ) ; for (i = 0; i i nloop; i++)
Our Valloc wrapper function calls malloc if valloc is not supported.
Our start-time function is called immediately before the loop begins, and our stop-time function is called as soon as the loop terminates.
The bandwidth that is printed is the total number of bytes transferred each time around the loop, divided by the time needed to transfer the data (stop-time returns this as the number of microseconds since start-time was called), times the number of loops.
The child is then killed with the SIGTERM signal, and the program terminates.
The second half of the program is shown in Figure A.16, and contains the two functions writer and reader.
Figure A.16 writer and reader functions to measure bandwidth of a pipe.
When this notification is received, the child writes the data across the pipe to the parent, xfersize bytes per write.
This function is called by the parent in a loop.
Each time the function is called, it writes an integer to the control pipe telling the child how many bytes to write to the pipe.
The function then calls read in a loop, until all the data has been received.
Our start-t ime, stop-t ime, and touch functions are shown in Figure A.17
The tv-sub function is shown in Figure A.18; it subtracts two timeval structures, storing the result in the first structure.
Figure A.19 is our main program that measures the bandwidth of a Posix message queue.
This program is similar to our previous program that measures the bandwidth of a pipe.
Note that our program must specify the maximum number of messages that can exist on the queue, when we create the queue, and we specify this as four.
The capacity of the IPC channel can affect the performance, because the writing process can send this many messages before its call to mcsend blocks, forcing a context switch to the reading process.
Therefore, the performance of this program depends on this magic number.
We would have guessed the performance would increase with a larger number of messages, because this could halve the number of context switches.
But if a memory-mapped file is used, this doubles the size of that file and the amount of memory that is mmaped.
Start-time ( ) ; for (i = 0; i i nloop; i++)
Figure A.19 main function to measure bandwidth of a Posix message queue.
Figure A.20 writer and reader functions to measure bandwidth of a Posix message queue.
Start-time ( ) ; for (i = 0; i c nloop; i++)
Figure A21 main function to measure bandwidth of a System V message queue.
Figure A22 writer and reader functions to measure bandwidth of a System V message queue.
Our program to measure the bandwidth of the doors API is more complicated than the previous ones in this section, because we must fork before creating the door.
Our parent creates the door and then notifies the child that the door can be opened by writing to a pipe.
Another change is that unlike Figure A.14, the reader function is not receiving the data.
Instead, the data is being received by a function named server that is the server procedure for the door.
Figure A.23 Overview of program to measure the bandwidth of the doors API.
Since doors are supported only under Solaris, we simplify the program by assuming a full-duplex pipe (Section 4.4)
Another change from the previous programs is the fundamental difference between message passing, and procedure calling.
In our Posix message queue program, for example, the writer just writes messages to a queue in a loop, and this is asynchronous.
At some point, the queue will fill, or the writing process will lose its time slice of the processor, and the reader runs and reads the messages.
But the doors API is synchronous: the caller blocks each time it calls door- call and cannot resume until the server procedure returns.
We will encounter the same problem when we measure the bandwidth of RPC calls.
The w r i t e r ,  server ,  and reader functions are shown in Figure A.25
Since procedure calls in Sun RPC are synchronous, we have the same limitation that we mentioned with our doors program.
It is also easier with RPC to generate two programs, a client and a server, because that is what rpcgen generates.
We declare a single procedure that takes a variable-length of opaque data as input and returns nothing.
We specify the protocol (TCP or UDP) as a command-line argument for the client, allowing us to measure both protocols.
Figure A24 main function to measure the bandwidth of the doors API.
Figure A25 writer, server, and reader functions for doors API bandwidth measurement.
We now show the three programs that measure the latency of pipes, Posix message queues, and System V message queues.
The program to measure the latency of a pipe is shown in Figure A.29
This function runs in the parent and its clock time is measured.
This is what we described as the latency: how long it takes to send a small message and receive a small message in reply.
Two pipes are created and fork creates a child, leading to the arrangement shown in Figure 4.6 (but without the unused ends of each pipe closed, which is OK)
Two pipes are needed for this test, since pipes are half-duplex, and we want two-way communication between the parent and child.
The child is an infinite loop that reads a 1-byte message and sends it back.
The doit function is then called in a loop and the clock time is measured.
These times include two context switches (parent-to-child, then child-teparent), four system calls ( w r i t e  by parent, read by child, w r i t e  by child, and read by parent), and the pipe overhead for 1 byte of data in each direction.
Our program to measure the latency of a Posix message queue is shown in Figure A.30
Although Posix messages have a priority, allowing us to assign different priorities for the messages in the two different directions, m ~ r e c e i v e  always returns the next message on the queue.
Therefore, we cannot use just one queue for this test.
Figure A.31 shows our program that measures the latency of a System V message queue.
Only one message queue is created, and it contains messages in both directions: parent-techild and child-to-parent.
These standards do not guarantee the order of these members, and the structures might contain other, nonstandard, members too.
But in this program, we statically initialize the msgbuf structures, because System V message queues guarantee that this structure contains a long message type field followed by the actual data.
Figure A.30 Program to measure the latency of a Posix message queue.
Figure A.31 Program to measure the latency of a System V message queue.
Our program to measure the latency of the doors API is shown in Figure A.32
The child creates the door and associates the function server with the door.
The parent then opens the door and invokes door-call in a loop.
One byte of data is passed as an argument, and nothing is returned.
Figure A.32 Program to measure the latency of the doors API.
To measure the latency of the Sun RPC API, we write two programs, a client and a server (similar to what we did when we measured the bandwidth)
We use the same RPC specification file (Figure A.26), but our client calls the null procedure this time.
Recall from Exercise 16.11 that this procedure takes no arguments and returns nothing, which is what we want to measure the latency.
As in the solution to Exercise 16.11, we must call clnt-call  directly to call the null procedure; a stub function is not provided in the client stub.
We compile our server with the server function from Figure A.28, but that function is never called.
Since we used rpcgen to build the client and server, we need to define at least one server procedure, but we never call it.
The reason we used rpcgen is that it automatically generates the server main with the null procedure, which we need.
To measure the time required by the various synchronization techniques, we create some number of threads (one to five for the measurements shown in Figures A.6 and A.8) and each thread increments a counter in shared memory a large number of times, using the different forms of synchronization to coordinate access to the shared counter.
Figure A.34 shows the global variables and the m a i n  function for ow program to measure Posix mutexes.
Shared data The shared data between the threads consists of the mutex itself and the counter.
The main thread locks the mutex before the threads are created, so that no thread can obtain the mutex until all the threads have been created and the mutex is released by the main thread.
Our set-concurrency function is called and the threads are created.
Each thread executes the incr function, which we show next.
Once all the threads are created, the timer is started and the mutex is released.
The main thread then waits for all the threads to finish, at which time the timer is stopped and the total number of microseconds is printed.
Figure A.35 shows the incr function that is executed by each thread.
Our program that uses read-write locks is a slight modification to our program that uses Posix mutexes.
Each thread must obtain a write lock on the read-write lock before incrementing the shared counter.
This implementation provides the same functionality as the proposed read-write locks, and the wrapper functions required to use these functions from the functions we described in Chapter 8 are trivial.
Under Digital Unix 4.08, our measurements were made using the Digital thread-independent services read-write locks, described on the tis-rwlock manual pages.
Figure A37 Increment a shared counter using a read-write lock.
We measure both Posix memory-based semaphores and Posix named semaphores.
After all the threads are created, the timer is started and semjos t is called once by the main thread.
Figure A.38 Increment a shared counter using a Posix memory-based semaphore.
Figure A.39 main function to measure Posix memory-based semaphore synchronization.
Figure A.40 Increment a shared counter using a Posix named semaphore.
Figure A.41 main function to measure Posix named semaphore synchronization.
Figure A.42 main function to measure System V semaphore synchronization.
Our final program uses f cntl record locking to provide synchronization.
This program will run successfully when only one thread is specified, because f cntl locks are between different processes, not between the different threads of a single process.
When multiple threads are specified, each thread can always obtain the requested lock (that is, the calls to writew-lock never block, since the calling process already owns the lock), and the final value of the counter is wrong.
The pathname of the file to create and then use for locking is a command-line argument.
This allows us to measure this program when this file resides on different filesystems.
We expect this program to run slower when this file is on an NFS mounted filesystem, which requires that both systems (the NFS client and NFS server) support NFS record locking.
The incr function using record locking is shown in Figure A.44
Figure A.44 Increment a shared counter using f cntl record locking.
We now modify these programs to provide synchronization between different processes.
To share the counter between a parent and its children, we store the counter in shared memory that is allocated by our my-shm function, shown in Figure A.46
Figure A.46 Create some shared memory for a parent and its children.
Further modifications depend on the type of synchronization and what happens to the underlying datatype when fork is called.
Posix memory-based semaphores: the semaphore must be stored in shared memory (with the shared counter), and the second argument to sem-init must be 1, to specify that the semaphore is shared between processes.
Posix named semaphores: either we can have the parent and each child call sem-open or we can have the parent call sem-open, knowing that the semaphore will be shared by the child across the fork.
System V semaphores: nothing special need be coded, since these semaphores can always be shared between processes.
We show only the code for the Posix mutex program.
The main function for our first program uses a Posix mutex to provide synchronization and is shown in Figure A.48
Its i nc r  function is shown in Figure A.47
Since we are using multiple processes (the children of a parent), we must place our shared structure into shared memory.
All the children are created, the timer is started, and the mutex is unlocked.
The parent waits for all the children and then stops the timer.
Figure A.47 incr function to measure Posix mutex locking between processes.
Figure A.48 main function to measure Posix mutex locking between processes.
In the traditional Unix model, when a process needs something performed by another entity, it forks a child process and lets the child perform the processing.
Most network servers under Unix, for example, are written this way.
Although this paradigm has served well for many years, there are problems with fork:
Memory is copied from the parent to the child, all descriptors are duplicated in the child, and so on.
Current implementations use a technique called copy-on-write, which avoids a copy of the parent's data space to the child until the child needs its own copy; but regardless of this optimization, fork is expensive.
Interprocess communication (IPC) is required to pass information between the parent and child after the fork.
Information from the parent to the child before the fork  is easy, since the child starts with a copy of the parent's data space and with a copy of all the parent's descriptors.
But returning information from the child to the parent takes more work.
Threads are sometimes called lightweight processes, since a thread is "lighter weight" than a process.
That is, thread creation can be 10-100 times faster than process creation.
All threads within a process share the same global memory.
This makes the sharing of information easy between the threads, but along with this simplicity comes the prob lem of synchronization.
When a program is started by exec, a single thread is created, called the initial thread or main thread.
Each thread within a process is identified by a thread ID, whose datatype is pthread-t.
On successful creation of a new thread, its ID is returned through the pointer tid.
Each thread has numerous attributes: its priority, its initial stack size, whether it should be a daemon thread or not, and so on.
When a thread is created, we can specify these attributes by initializing a pthread-attr-t  variable that overrides the default.
We normally take the default, in which case, we specify the attr argument as a null pointer.
Finally, when we create a thread, we specify a function for it to execute, called its thread start function.
The thread starts by calling this function and then terminates either explicitly (by calling pthread-exit) or implicitly (by letting this function return)
If we need multiple arguments to the function, we must package them into a structure and then pass the address of this structure as the single argument to the start function.
This lets us pass one pointer (to anything we want) to the thread, and lets the thread return one pointer (again, to anything we want)
The return value from the Pthread functions is normally !I if OK or nonzero on an error.
But unlike most system functions, which return -1 on an error and set errno to a positive value, the Pthread functions return the positive error indication as the function's return value.
For example, if pthread-create cannot create a new thread because we have exceeded some system limit on the number of threads, the function return value is EAGAIN.
We can wait for a given thread to terminate by calling pthread-join.
Comparing threads to Unix processes, pthread-create is similar to fork, and pthread-j oin is similar to wai tpid.
We must specify the tid of the thread for which we wish to wait.
Unfortunately, we have no way to wait for any of our threads (similar to wai tpid with a process ID argument of -1)
If the status pointer is nonnull, the return value from the thread (a pointer to some object) is stored in the location pointed to by status.
Each thread has an ID that identifies it within a given process.
The thread ID is returned by pthread-create, and we saw that it was used by pthread-join.
Comparing threads to Unix processes, pthread-self is similar to getpid.
When a joinable thread terminates, its thread ID and exit status are retained until another thread in the process calls pthread- j oin.
But a detached thread is l i e  a daemon process: when it terminates, all its resources are released, and we cannot wait for it to terminate.
If one thread needs to know when another thread terminates, it is best to leave the thread as joinable.
The pthread-detach function changes the specified thread so that it is detached.
One way for a thread to terminate is to call pthread-exit.
Does not return to caller 1 If the thread is not detached, its thread ID and exit status are retained for a later pthread-j o i n  by some other thread in the calling process.
The pointer status must not point to an object that is local to the calling thread (e.g., an automatic variable in the thread start function), since that object disappears when the thread terminates.
The function that started the thread (the third argument to pthread-create) can return.
Since this function must be declared as returning a void pointer, that return value is the exit status of the thread.
If the main function of the process returns or if any thread calls e x i t  or -exit, the process terminates immediately, including any threads that are still running.
This header includes all the standard system headers that most network programs need, along with some general system headers.
It also defines constants such as MAXLINE and ANSI C function prototypes for the functions that we define in the text (e.g., px-ipc-name) and all the wrapper functions that we use.
Define bzeroo as a macro if it's not in standard C library.
This is the standard value, but there's no guarantee it is -1
Other systems in addition to BSD/OS might have this * problem too.
The lines that are commented out and contain #undef are features that the system does not provide.
We define our own set of error functions that are used throughout the text to hand11 error conditions.
The reason for our own error functions is to let us write our error han dling with a single line of C code, as in.
Our error functions use the variable-length argument list facility from ANSI C.
The kernel then ensures that each wr i t e  is appended to the file.
This is the easiest form of file synchronization to specify.
Pages 60-61 of APUE talk about this in more detail.
The synchronization issues become more complex when existing data in the file is updated, as in a database system.
If -REENTRANT is defined, references to errno call a function named -errno that returns the address of the calling thread's errno variable.
If -REENTRANT is not defined, then errno is a global i n t.
This second call should succeed, but a chance exists (albeit small) that it fails with an error of ENOENT, which indicates that some other thread or process has removed the object between the two calls.
When we run this program we see that our file mode creation mask is 2 (turn off the other-write bit) and this bit is turned off in the FIFO, but this bit is not turned off in the message queue.
With IPC-PRIVATE, the server knows that it is creating a new message queue, but the server must then write the resulting identifier into some file for the clients to read.
In the find program, we ignore files with more than one link (since each link will have the same i-node), and we ignore symbolic links (since the stat function follows the link)
This means lots of collisions can occur on any filesystem with more than 4096 files.
This call then returns immediately, and the next call to open (for write-only) also returns immediately, since the FIFO is already open for reading.
But to avoid an error from readline, the 0-NONBLOCK flag must be turned off for the descriptor readf if o before calling readline.
The only way to avoid the deadlock is to open the two FIFOs in the order shown in Figure 4.24 or to use the nonblocking flag.
Figure D.3 Determine whether f stat returns the number of bytes in a FIFO.
Figure D.4 Determine what select returns for writability when the read end of a pipe is closed.
First create the queue without specifying any attributes, followed by a call to m ~ g e t a t  t r  to obtain the default attributes.
Then remove the queue and create it again, using the default value of either attribute that is not specified.
The signal is not generated for the second message, because the registration is removed every time the notification occurs.
The signal is not generated for the second message, because the queue was not empty when the message was received.
The GNU C compiler under Solaris 2.6 (which defines both constants as calls to sysconf) generates the errors.
This makes us think that Posix message queues are implemented using memory-mapped files, and that mq_open maps the file into the address space of the calling process.
A size argument of 0 is OK for the ANSI C memXXX functions.
For two-way communication between two processes, two message queues are needed (see for example, Figure A.30)
Indeed, if we were to modify Figure 4.14 to use Posix message queues instead of pipes, we would see the parent read back what it wrote to the queue.
The mutex and condition variable are contained in the memory-mapped file, which is shared by all processes that have the queue open.
Other processes may have the queue open, so a process that is closing its handle to the queue cannot destroy the mutex and condition variable.
An array cannot be assigned across an equals sign in C, whereas a structure can.
The main function spends almost all of its time blocked in a call to se lec t ,  waiting for the pipe to be readable.
Every time the signal is delivered, the return from the signal handler interrupts this call to s e l e c t ,  causing it to return an error of.
Page 124 of UNPvl talks more about interrupted system calls.
This change could be made with a new command-line option in these other programs, or the assump tion could be made that a pathname argument that is entirely numeric is an identifier and not a pathname.
Since most pathnames that are passed to ftok are absolute pathnames, and not relative (i.e., they contain at least one slash character), this assumption is probably OK.
When we have one return queue per client (Figure 6-19), this client affects only its own queue.
We do not see this under Digital Unix 4.OB, which just implies an implementation difference.
The calls to the matching destroy functions are still required.
From an implementation perspective, Digital Unix appears to use the a t t r - t variable as the attributes object itself, whereas Solaris uses this variable as a pointer to a dynamically allocated object.
Depending on your system, you may need to increase the loop counter from 20, to see the errors.
This should have no effect, because there is only one call to p r i n t  f and the string is terminated with a newline.
Normally, standard output is line buffered, so in either case (line buffered or unbuffered), the single call to p r i n t  f ends up in a single wr i t e  call to the kernel.
We change the call to p r i n t  f to be.
If we leave in the call to setvbuf, making standard output unbuffered, this causes the standard 110 library to call wr i te  once per character that is output, instead of once per line.
This involves more CPU time, and provides more opportunities for the kernel to switch between the two processes.
Since multiple processes are allowed to have read locks for the same region of a file, this is the same as having no locks at all for our example.
Nothing changes, because the nonblocking flag for a descriptor has no effect on f cnt  1 advisory locking.
What determines whether a call to f cnt  1 blocks or not is whether the command is F-SETLKW (which always blocks) or F-SETLK (which never blocks)
The loopf cntlnonb program operates as expected, because, as we showed in the previous exercise, the nonblocking flag has no effect on a program that performs f c n t l  locking.
But the nonblocking flag does affect the loopnonenonb program, which performs no locking.
As we said in Section 9.5, a nonblocking call to read or wri te  for a file for which mandatory locking is enabled, returns an error of EAGAIN if the read or wr i t e  conflicts with an existing lock.
The user CPU time remains the same, as we expect, because the extra time is within the kernel checking every read and write,  not within our process.
Locks are granted on a per-process basis, not on a per-thread basis.
To see contention for lock requests, we must have different processes trying to obtain the locks.
If another copy of the daemon were running and we open with the 0-TRUNC flag, this would wipe out the process ID stored by the first copy of the daemon.
We cannot truncate the file until we know we are the only copy running.
The problem with SEEK-CUR is that it depends on the current offset in the file, which is specified by lseek.
But if we call l seek and then f cn t  1, we are using two function calls to perform what is a single operation, and a chance exists that another thread can change the current offset by calling l s e e k  between our two function calls.
Also recall that f c n t l  record locks are for locking between different processes and not for locking between the different threads within one process.
Similarly, if we specify SEEK-END, a chance exists that another thread can append data to the file before we obtain a lock based on what we think is the end of the file.
This is OK given the rules for semaphore initialization that we specified when we described sem-open: if the semaphore already exists it, is not initialized.
When the remaining three call sem-open with the 0-CREAT flag, the semaphore will already exist, so its value is not initialized again.
The semaphore is automatically closed when the process terminates, but the value of the semaphore is not changed.
This will prevent any of the other three programs from obtaining the lock, causing another type of deadlock.
If we did not initialize the descriptors to -1, their initial value is unknown, since malloc does not initialize the memory that it allocates.
So if one of the calls to open fails, the calls to c l o s e  at the label e r r o r  could close some descriptor that the process is using.
By initializing the descriptors to -1, we know that the calls to c lose  will have no effect (other than returning an error that we ignore) if that descriptor has not been opened yet.
A chance exists, albeit slight, that c l o s e  could be called for a valid descriptor and could return some error, thereby changing e r rno  from the value that we want to return.
Since we want to save the value of er rno to return to the caller, to do so explicitly is better than counting on some side effect (that c lose  will not return an error when a valid descriptor is closed)
No race condition exists in this function, because the mkf i f o function returns an error if the FIFO already exists.
If two processes call this function at about the same time, the FIFO is created only once.
The second process to call mkf i f o will receive an error of EEXIST, causing the 0-CREAT flag to be turned off, preventing another initialization of the FIFO.
If the process that creates the FIFO is suspended by the kernel after it calls mkf i f o but before it wr i tes  the data bytes to the FIFO, the second process will just open the FIFO and block the first time it calls s e m - w a i t ,  because the newly created FIFO will be empty until the first process (which created the FIFO) writes the data bytes to the FIFO.
Our implementation using FIFOs returns EINTR, because s e m - w a i t  blocks in a call to read on a FIFO, which must return the error.
Our implementation using System V semaphores returns EINTR, because s e m- w a i t  blocks in a call to semop, which returns the error.
The my-lock function could call f tok before calling semget, check for an error of ENOENT, and create the file if it does not exist.
The reason we say "might" and not "will" is that it depends on the page size.
Any data written to this device is simply discarded, just like writes to /dev/null.
Figure D.8 Sending messages using a Posix message queue implemented using map.
Figure D.9 Example of parent and child setup to use select with System V messages.
Figure D.10 Memory mapping when m a p  equals shared memory size.
Figure D.ll Memory-map example that lets the shared memory size grow.
If the pointer is needed at a later time, it must be either saved, or not modified.
But we said with Figure 15.23 that for all the server threads automatically created by the doors library, cancellation is disabled, and hence this thread is not terminated.
When starting the server 20 times in a row, the error occurred five times.
All that is required is to enable cancellation each time the server procedure is called, as we do in Figure 15.31
Although this technique calls the function pthread-setcancels t a  t e every time the server procedure is invoked, instead of just once when the thread starts, this overhead is probably trivial.
The first invocation returns successfully, verifying our statement that door-revoke does not affect a call that is in progress.
The second invocation tells us that the error from door-call is EBADF.
Figure D.12 Using the cookie pointer to avoid making f d a global.
In this example, the thread attributes never change, so we could initialize the attributes once (in the main function)
The port mapper does not monitor the servers that register with it, to try and detect if they crash.
After we terminate our client, the port mapper mappings remain in place, as we can verify with the rpcinf o program.
So a client who contacts the port mapper after our server terminates will get an OK return from the port mapper with the port numbers in use before the server terminated.
But when a client tries to contact the TCP server, the RPC runtime will receive an RST (reset) in response to its SYN (assuming that no other process has since been assigned that same port on the server host), causing an error return from clnt-create.
A UDP client's call to clnt-create will succeed (since there is no connection to establish), but when the client sends a UDP datagram to the old server port, nothing will be returned (assuming again that no other process has since been assigned that same port on the server host) and the client's procedure call will eventually time out.
The RPC runtime returns the server's first reply to the client when it is received, about 20 seconds after the client's call.
The next reply for the server will just be held in the client's network buffer for this endpoint until either the endpoint is closed, or until the next read of this buffer by the RPC runtime.
Assume that the client issues a second call to this server immediately after receiving the first reply.
Assuming no network loss, the next datagram that will arrive on this endpoint will be the server's reply to the client's retransmission.
But the RPC runtime will ignore this reply, since the XID will correspond to the client's first procedure call, which cannot equal the XID used for this second procedure call.
If you really want a fixed-length string, use the fixed-length opaque datatype.
When a maximum length is specified, it is coded as the final argument to xdr-string.
The XDR routines all check that adequate room is available in the buffer for the data that is being encoded into the buffer, and they return an error of FALSE when the buffer is full.
Unfortunately, there is no way to distinguish among the different possible errors from the XDR functions.
We could say that TCP's use of sequence numbers to detect duplicate data is, in effect, a duplicate request cache, because these sequence numbers identify any.
Since all five values for a given request must be equal to all five values in the cache entry, the first value compared should be the one most likely to be unequal, and the last value compared should be the one least likely to be unequal.
Given that the XID changes for every request, to compare it first makes sense.
With the default of null authentication, the credential data and verifier data will both be empty.
When UDP is used, the only change in the request and reply is the absence of the 28 bytes, which we can verify with tcpdump.
The difference in argument handling, both at the client end and at the server end, is local to that host and independent of the packets that traverse the network.
The client main calls a function in the client stub to generate a network record, and the server main calls a function in the server stub to process this network record.
The RPC record that is transmitted across the network is defined by the RPC protocol, and this does not change, regardless of whether either end supports threads or not.
We verify this fact by adding the following line to our read program:
The sbrk  function returns the current address at the top of the program's data segment, and the memory just below this is normally the region from which ma1 l o c  takes its memory.
Note that the final argument to c l n t - c a l l  is an actual t imeval  structure and not a pointer to one of these structures.
Also note that the third and fifth arguments to c l n t - c a l l  must be nonnull function pointers to XDR routines, so we specify xdr-void, the XDR function that does nothing.
You can verify that this is the way to call a function with no arguments or no return values, by writing a trivial RPC specification file that defines a function with no arguments and no return values, running rpcgen, and examining the client stub that is generated.
Figure D.13 Client program that calls the server's null procedure.
Whenever an electronic copy was found of a paper or report referenced in this bibliography, its URL is included.
Be aware that these URLs can change over time, and readers are encouraged to check the Errata for this text on the author's home page for any changes: h t t p :  //www.
Ordering information on IEEE standards and draft standards is available at http: / /www.
Unfortunately, the IEEE standards are not freely available on the Internet.
Also note that many of the Unix 98 specifications (e.g., all of the manual pages) are available onlineathttp://www.UNIX-systems.org/online.html.
This suite of benchmark tools, along with this paper, are available from http://www.bitmover.com/lmbench.
The implementation of the Internet protocols in the 4.4BSD-Lite operaling system.
Rather than provide a separate glossary (with most of the entries being acronyms), this index also serves as a glossary for all the acronyms used in this book.
The primary entry for the acronym appears under the acronym name.
The entry under the compound term "Remote Procedure Call" refers back to the main entry under RPC.
The notation "definition of" appearing with a C function refers to the boxed function prototype for that function, its primary description.
The "definition of" notation for a structure refers to its primary definition.
Some functions also contain the notation "source code" if a source code implementation for that function appears in the text.
