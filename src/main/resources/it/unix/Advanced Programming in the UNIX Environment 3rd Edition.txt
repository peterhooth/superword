The Addison-Wesley Professional Computing Series was created in 1990 to provide serious programmers and networking professionals with well-written and practical reference books.
There are few places to turn for accurate and authoritative books on current and cutting-edge technology.
We hope that our books will help you understand the state of the art in programming languages, operating systems, and networks.
Stephen Rago’s update is a long overdue benefit to the community of professionals using the versatile family of UNIX and UNIX-like operating environments.
It also thoroughly updates the context of all topics, examples, and applications to recent releases of popular implementations of UNIX and UNIX-like environments.
This is the definitive reference book for any serious or professional UNIX systems programmer.
Rago has updated and extended the classic Stevens text while keeping true to the original.
The APIs are illuminated by clear examples of their use.
The Addison-Wesley Professional Computing Series was created in 1990 to provide serious programmers and networking professionals with well-written and practical reference books.
There are few places to turn for accurate and authoritative books on current and cutting-edge technology.
We hope that our books will help you understand the state of the art in programming languages, operating systems, and networks.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals.
The authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions.
No  liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein.
The publisher offers excellent discounts on this book when ordered in quantity for bulk  purchases or special sales, which may include electronic versions and/or custom covers and content particular to your business, training goals, marketing focus, and branding interests.
This publication is protected by  copyright, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise.
At some point during nearly every interview I give, as well as in question periods after talks, I get asked some variant of the same question: ‘‘Did you expect Unix to last for so long?’’ And of course the answer is always the same: No, we didn’t quite anticipate what has happened.
Even the observation that the system, in some form, has been around for well more than half the lifetime of the commercial computing industry is now dated.
Computer technology has changed greatly since the early 1970s, most notably in universal networking, ubiquitous graphics, and readily available personal computing, but the system has somehow managed to accommodate all of these phenomena.
The commercial environment, although today dominated on the desktop by Microsoft and Intel, has in some ways moved from single-supplier to multiple sources and, in recent years, to increasing reliance on public standards and on freely available source.
Fortunately, Unix, considered as a phenomenon and not just a brand, has been able to move with and even lead this wave.
For example, the SVID—the System V Interface Definition — was published by AT&T, and it became the basis for the POSIX work and its follow-ons.
As it happened, Unix was able to adapt rather gracefully to a networked environment and, perhaps less elegantly, but still adequately, to a graphical one.
And as it also happened, the basic Unix kernel interface and many of its characteristic user-level tools were incorporated into the technological foundations of the open-source movement.
It is important that papers and writings about the Unix system were always encouraged, even while the software of the system itself was proprietary, for example Maurice Bach’s book, The Design of the Unix Operating System.
Brian Kernighan is one of these; Rich Stevens is certainly another.
The first edition of this book, along with his series of books about networking, are rightfully regarded as remarkably well-crafted works of exposition, and became hugely popular.
However, the first edition of this book was published before Linux and the several open-source renditions of the Unix interface that stemmed from the Berkeley CSRG became widespread, and also at a time when many people’s networking consisted of a serial modem.
Steve Rago has carefully updated this book to account for the technology changes, as well as developments in various ISO and IEEE standards since its first publication.
It’s been almost eight years since I first updated Advanced Programming in the UNIX Environment, and already so much has changed.
Before the second edition was published, The Open Group created a 2004 edition of the Single UNIX Specification, folding in the changes from two sets of corrigenda.
In 2008, The Open Group created a new version of the Single UNIX Specification, updating the base definitions, adding new interfaces, and removing obsolete ones.
The Solaris operating system was released in open source form to try to compete with the popularity of the open source model followed by FreeBSD, Linux, and Mac OS X.
Instead, the Solaris community formed the Illumos project to continue open source development based on OpenSolaris.
Most notably, the platforms used in the second edition have become out-of-date.
In this book, the third edition, I cover the following platforms:
I chose to switch to an Intel platform instead of continuing with one based on the PowerPC, because the latest versions of Mac OS X are no longer being ported to the PowerPC platform.
The drawback to this choice is that the processors covered are now slanted in favor of Intel.
When discussing issues of heterogeneity, it is helpful to have processors with different characteristics, such as byte ordering and integer size.
One of the biggest changes to the Single UNIX Specification in POSIX.1-2008 is the demotion of the STREAMS-related interfaces to obsolescent status.
This is the first step before these interfaces are removed entirely in a future version of the standard.
Because of this, I have reluctantly removed the STREAMS content from this edition of the book.
This is an unfortunate change, because the STREAMS interfaces provided a nice contrast to the socket interfaces, and in many ways were more flexible.
Admittedly, I am not entirely unbiased when it comes to the STREAMS mechanism, but there is no debating the reduced role it is playing in current systems:
Linux doesn’t include STREAMS in its base system, although packages (LiS and OpenSS7) are available to add this functionality.
So with the removal of the STREAMS-related material, an opportunity exists to replace it with new topics, such as POSIX asynchronous I/O.
In the second edition, the Linux version covered was based on the 2.4 version of the source.
In this edition, I have updated the version of Linux to 3.2
In total, this edition includes more than 70 new interfaces, including interfaces to handle asynchronous I/O, spin locks, barriers, and POSIX semaphores.
Most obsolete interfaces are removed, except for a few ubiquitous ones.
The technical reviewers improved the accuracy of the information presented.
Once again, the staff at Addison-Wesley was great to work with.
In addition, thanks to Jill Hobbs for providing her copyediting expertise this time around.
Finally, thanks to my family for their understanding while I spent so much time working on this updated edition.
I welcome e-mail from any readers with comments, suggestions, or bug fixes.
Rich Stevens and I first met through an e-mail exchange when I reported a typographical error in his first book, UNIX Network Programming.
He used to kid me about being the person to send him his first errata notice for the book.
Until his death in 1999, we exchanged e-mail irregularly, usually when one of us had a question we thought the other might be able to answer.
We met for dinner at USENIX conferences and when Rich was teaching in the area.
Rich Stevens was a friend who always conducted himself as a gentleman.
As was his nature, Rich gladly reviewed chapters for me, and treated me not as a competitor, but as a colleague.
We often talked about collaborating on a STREAMS version of his TCP/IP Illustrated book.
Had events been different, we might have actually done it, but since Rich is no longer with us, revising Advanced Programming in the UNIX Environment is the closest I’ll ever get to writing a book with him.
When the editors at Addison-Wesley told me that they wanted to update Rich’s book, I thought that there wouldn’t be too much to change.
Even after 13 years, Rich’s work still holds up well.
But the UNIX industry is vastly different today from what it was when the book was first published.
The System V variants are slowly being replaced by Linux.
The major system vendors that ship their hardware with their own versions of the UNIX System have either made Linux ports available or announced support for Linux.
Solaris is perhaps the last descendant of UNIX System V Release 4 with any appreciable market share.
After 4.4BSD was released, the Computing Science Research Group (CSRG) from the University of California at Berkeley decided to put an end to its development of the UNIX operating system, but several different groups of volunteers still maintain publicly available versions.
The introduction of Linux, supported by thousands of volunteers, has made it possible for anyone with a computer to run an operating system similar to the UNIX System, with freely available source code for the newest hardware devices.
The success of Linux is something of a curiosity, given that several free BSD alternatives are readily available.
Continuing its trend as an innovative company, Apple Computer abandoned its old Mac operating system and replaced it with one based on Mach and FreeBSD.
Thus, I’ve tried to update the information presented in this book to reflect these four platforms.
After Rich wrote Advanced Programming in the UNIX Environment in 1992, I got rid of most of my UNIX programmer ’s manuals.
To this day, the two books I keep closest to my desk are a dictionary and a copy of Advanced Programming in the UNIX Environment.
I’ve tried not to change his original vision for this book, but a lot has happened in 13 years.
This is especially true with the standards that affect the UNIX programming interface.
Throughout the book, I’ve updated interfaces that have changed from the ongoing efforts in standards organizations.
This is most noticeable in Chapter 2, since its primary topic is standards.
A lot more interfaces are now covered by the POSIX.1 specification.
Threads and multithreaded programming are important concepts because they present a cleaner way for programmers to deal with concurrency and asynchrony.
It provides a single interface to interprocess communication (IPC), regardless of the location of the process, and is a natural extension of the IPC chapters.
I’ve omitted most of the real-time interfaces that appear in POSIX.1
These are best treated in a text devoted to real-time programming.
I’ve updated the case studies in the last chapters to cover more relevant real-world examples.
For example, few systems these days are connected to a PostScript printer.
Most PostScript printers today are accessed via a network, so I’ve changed the case study that deals with PostScript printer communication to take this into account.
The chapter on modem communication is less relevant these days.
So that the original material is not lost, however, it is available on the book’s Web site in two formats: PostScript (http://www.apuebook.com/lostchapter/modem.ps) and PDF (http://www.apuebook.com/lostchapter/modem.pdf)
Most of the examples have been run on four platforms:
Rich Stevens wrote the first edition of this book on his own, and it became an instant classic.
I couldn’t have updated this book without the support of my family.
They put up with piles of papers scattered about the house (well, more so than usual), my monopolizing most of the computers in the house, and lots of hours with my face buried behind a computer terminal.
My wife, Jeanne, even helped out by installing Linux for me on one of the test machines.
The technical reviewers suggested many improvements and helped make sure that the content was accurate.
I’d also like to thank Andy Rudoff for answering questions about Solaris and Dennis Ritchie for digging up old papers and answering history questions.
Once again, the staff at Addison-Wesley was great to work with.
My thanks to Evelyn Pyle for the fine job of copyediting.
As Rich did, I also welcome electronic mail from any readers with comments, suggestions, or bug fixes.
This book describes the programming interface to the Unix system—the system call interface and many of the functions provided in the standard C library.
It is intended for anyone writing programs that run under Unix.
Like most operating systems, Unix provides numerous services to the programs that are running — open a file, read a file, start a new program, allocate a region of memory, get the current time-of-day, and so on.
Examples and rationale are missing from the Unix Programmer ’s Manual, and that’s what this book provides.
These include the ANSI standard for the C programming language, the IEEE POSIX family (still being developed), and the X/Open portability guide.
This provides a realworld description, which is often lacking from the standard itself and from books that describe only the standard.
A reading familiarity with C would be beneficial as would some experience using Unix.
This text is intended for programmers familiar with Unix and programmers familiar with some other operating system who wish to learn the details of the services provided by most Unix systems.
Almost every function and system call is demonstrated with a small, complete program.
This lets us see the arguments and return values and is often easier to comprehend than the use of the function in a much larger program.
These larger examples demonstrate the programming techniques in larger, real-world examples.
All the examples have been included in the text directly from their source files.
Obtaining the source code allows you to modify the programs from this text and experiment with them on your system.
The following diagram shows the recent evolution of the various versions of System V and 4.xBSD.
POSIX.1 is the IEEE and ISO standard for the interface to a Unix-like system.
Nevertheless a simple name was needed to refer to this system and 4.3+BSD is used throughout the text.
Most of the examples in this text have been run on four different versions of Unix:
This system is almost identical to what we call 4.3+BSD.
Numerous timing tests are provided in the text and the systems used for the test are identified.
Once again I am indebted to my family for their love, support, and many lost weekends over the past year and a half.
Writing a book is, in many ways, a family affair.
I am especially grateful to Brian Kernighan for his help in the book.
His numerous thorough reviews of the entire manuscript and his gentle prodding for better prose hopefully show in the final result.
Steve Rago was also a great resource, both in reviewing the entire manuscript and answering many questions about the details and history of System V.
Berkeley CSRG provided an account that was used to test the examples on the latest BSD system.
Sam Nataros and Joachim Sacksen at UHC provided the copy of SVR4 used to test the examples.
My editor at Addison-Wesley, John Wait, has been a great friend through it all.
He never complained when the due date slipped and the page count kept increasing.
Real Unix books are written using troff and this book follows that time-honored tradition.
Camera-ready copy of the book was produced by the author using the groff package written by James Clark.
Many thanks to James Clark for providing this excellent system and for his rapid response to bug fixes.
I welcome electronic mail from any readers with comments, suggestions, or bug fixes.
Typical services include executing a new program, opening a file, reading a file, allocating a region of memory, getting the current time of day, and so on.
The focus of this text is to describe the services provided by various versions of the UNIX operating system.
Describing the UNIX System in a strictly linear fashion, without any forward references to terms that haven’t been described yet, is nearly impossible (and would probably be boring)
This chapter provides a whirlwind tour of the UNIX System from a programmer ’s perspective.
We’ll give some brief descriptions and examples of terms and concepts that appear throughout the text.
We describe these features in much more detail in later chapters.
This chapter also provides an introduction to and overview of the services provided by the UNIX System for programmers new to this environment.
In a strict sense, an operating system can be defined as the software that controls the hardware resources of the computer and provides an environment under which programs can run.
Generally, we call this software the kernel, since it is relatively small and resides at the core of the environment.
Figure 1.1 shows a diagram of the UNIX System architecture.
The interface to the kernel is a layer of software called the system calls (the shaded portion in Figure 1.1)
Libraries of common functions are built on top of the system call.
We talk more about system calls and library functions in Section 1.11
The shell is a special application that provides an interface for running other applications.
In a broad sense, an operating system consists of the kernel and all the other software that makes a computer useful and gives the computer its personality.
This other software includes system utilities, applications, shells, libraries of common functions, and so on.
For example, Linux is the kernel used by the GNU operating system.
Some people refer to this combination as the GNU/Linux operating system, but it is more commonly referred to as simply Linux.
Although this usage may not be correct in a strict sense, it is understandable, given the dual meaning of the phrase operating system.
When we log in to a UNIX system, we enter our login name, followed by our password.
The system then looks up our login name in its password file, usually the file /etc/passwd.
All contemporary systems have moved the encrypted password to a different file.
In Chapter 6, we’ll look at these files and some functions to access them.
Once we log in, some system information messages are typically displayed, and then we can type commands to the shell program.
Some systems start a window management program when you log in, but you generally end up with a shell running in one of the windows.
A shell is a command-line interpreter that reads user input and executes commands.
The user input to a shell is normally from the terminal (an interactive shell) or sometimes from a file (called a shell script)
The common shells in use are summarized in Figure 1.2
The system knows which shell to execute for us based on the final field in our entry in the password file.
The Bourne shell, developed by Steve Bourne at Bell Labs, has been in use since Version 7 and is provided with almost every UNIX system in existence.
The C shell, developed by Bill Joy at Berkeley, is provided with all the BSD releases.
We’ll have more to say about these different versions of the UNIX System in the next chapter.
The C shell was built on the 6th Edition shell, not the Bourne shell.
Its control flow looks more like the C language, and it supports additional features that weren’t provided by the Bourne shell: job control, a history mechanism, and command-line editing.
The Korn shell is considered a successor to the Bourne shell and was first provided with SVR4
The Korn shell, developed by David Korn at Bell Labs, runs on most UNIX systems, but before SVR4 was usually an extra-cost add-on, so it is not as widespread as the other two shells.
It is upward compatible with the Bourne shell and includes those features that made the C shell popular: job control, command-line editing, and so on.
The Bourne-again shell is the GNU shell provided with all Linux systems.
It was designed to be POSIX conformant, while still remaining compatible with the Bourne shell.
It supports features from both the C shell and the Korn shell.
The TENEX C shell is an enhanced version of the C shell.
It borrows several features, such as command completion, from the TENEX operating system (developed in 1972 at Bolt Beranek and Newman)
The TENEX C shell adds many features to the C shell and is often used as a replacement for the C shell.
The specification was based on features from the Korn shell and Bourne shell.
Others use the BSD replacement for the Bourne shell, called dash (Debian Almquist shell, originally written by Kenneth Almquist and later ported to Linux)
The default user shell in FreeBSD is derived from the Almquist shell.
Solaris, having its heritage in both BSD and System V, provides all the shells shown in Figure 1.2
Free ports of the shells are available on the Internet.
Throughout the text, we will use parenthetical notes such as this to describe historical notes and to compare different implementations of the UNIX System.
Often the reason for a particular implementation technique becomes clear when the historical reasons are described.
Throughout this text, we’ll show interactive shell examples to execute a program that we’ve developed.
These examples use features common to the Bourne shell, the Korn shell, and the Bourne-again shell.
The UNIX file system is a hierarchical arrangement of directories and files.
Everything starts in the directory called root, whose name is the single character /
Logically, we can think of each directory entry as containing a filename along with a structure of information describing the attributes of the file.
The attributes of a file are such things as the type of file (regular file, directory), the size of the file, the owner of the file, permissions for the file (whether other users may access this file), and when the file was last modified.
The stat and fstat functions return a structure of information containing all the attributes of a file.
In Chapter 4, we’ll examine all the attributes of a file in great detail.
We make a distinction between the logical view of a directory entry and the way it is actually stored on disk.
Most implementations of UNIX file systems don’t store attributes in the directory entries themselves, because of the difficulty of keeping them in synch when a file has multiple hard links.
The only two characters that cannot appear in a filename are the slash character (/) and the null character.
The slash separates the filenames that form a pathname (described next) and the null character terminates a pathname.
Nevertheless, it’s good practice to restrict the characters in a filename to a subset of the normal printing characters.
If we use some of the shell’s special characters in the filename, we have to use the shell’s quoting mechanism to reference the filename, and this can get complicated.
Two filenames are automatically created whenever a new directory is created:
Dot refers to the current directory, and dot-dot refers to the parent directory.
In the root directory, dot-dot is the same as dot.
The Research UNIX System and some older UNIX System V file systems restricted a filename to 14 characters.
Today, almost all commercial UNIX file systems support at least 255-character filenames.
Pathname A sequence of one or more filenames, separated by slashes and optionally starting with a slash, forms a pathname.
A pathname that begins with a slash is called an absolute pathname; otherwise, it’s called a relative pathname.
Relative pathnames refer to files relative to the current directory.
The name for the root of the file system (/) is a special-case absolute pathname that has no filename component.
Example Listing the names of all the files in a directory is not difficult.
The notation ls(1) is the normal way to reference a particular entry in the UNIX system manuals.
Throughout this text, we assume that you have a copy of the manuals for your UNIX system.
Historically, UNIX systems lumped all eight sections together into what was called the UNIX Programmer ’s Manual.
As the page count increased, the trend changed to distributing the sections among separate manuals: one for users, one for programmers, and one for system administrators, for example.
Some UNIX systems further divide the manual pages within a given section, using an uppercase letter.
Other systems have replaced the numeric sections with alphabetic ones, such as C for commands.
If your manuals are online, the way to see the manual pages for the ls command would be something like.
Figure 1.3 is a program that just prints the name of every file in a directory, and nothing else.
If the source file is named myls.c, we compile it into the default a.out executable file by running.
On systems with the GNU C compilation system, the C compiler is gcc(1)
Throughout this text, we’ll show commands that we run and the resulting output in this fashion: Characters that we type are shown in this font, whereas output from programs is shown like this.
If we need to add comments to this output, we’ll show.
The dollar sign that precedes our input is the prompt that is printed by the shell.
We’ll always show the shell prompt as a dollar sign.
Note that the directory listing is not in alphabetical order.
There are many details to consider in this 20-line program.
We include this header in almost every program in this text.
This header includes some standard system headers and defines numerous constants and function prototypes that we use throughout the examples in the text.
Next, we include a system header, dirent.h, to pick up the function prototypes for opendir and readdir, in addition to the definition of the dirent structure.
On some systems, the definitions are split into multiple header files.
The declaration of the main function uses the style supported by the ISO C standard.
We’ll have more to say about the ISO C standard in the next chapter.
We take an argument from the command line, argv[1], as the name of the directory to list.
In Chapter 7, we’ll look at how the main function is called and how the command-line arguments and environment variables are accessible to the program.
Because the actual format of directory entries varies from one UNIX system to another, we use the functions opendir, readdir, and closedir to manipulate the directory.
The opendir function returns a pointer to a DIR structure, and we pass this pointer to the readdir function.
We then call readdir in a loop, to read each directory entry.
The readdir function returns a pointer to a dirent structure or, when it’s finished with the directory, a null pointer.
All we examine in the dirent structure is the name of each directory entry (d_name)
Using this name, we could then call the stat function (Section 4.2) to determine all the attributes of the file.
These two error functions are shown and described in Appendix B.
We also talk more about error handling in Section 1.7
In Section 8.5, we show how any program, such as a shell or a program that we write, can obtain the exit status of a program that it executes.
Working Directory Every process has a working directory, sometimes called the current working directory.
This is the directory from which all relative pathnames are interpreted.
A process can change its working directory with the chdir function.
For example, the relative pathname doc/memo/joe refers to the file or directory joe, in the directory memo, in the directory doc, which must be a directory within the working directory.
From looking just at this pathname, we know that both doc and memo have to be directories, but we can’t tell whether joe is a file or a directory.
The pathname /usr/lib/lint is an absolute pathname that refers to the file or directory lint in the directory lib, in the directory usr, which is in the root directory.
Home Directory When we log in, the working directory is set to our home directory.
Our home directory is obtained from our entry in the password file (Section 1.3)
File Descriptors File descriptors are normally small non-negative integers that the kernel uses to identify the files accessed by a process.
Whenever it opens an existing file or creates a new file, the kernel returns a file descriptor that we use when we want to read or write the file.
Standard Input, Standard Output, and Standard Error By convention, all shells open three descriptors whenever a new program is run: standard input, standard output, and standard error.
If nothing special is done, as in the simple command.
Most shells provide a way to redirect any or all of these three descriptors to any file.
Unbuffered I/O Unbuffered I/O is provided by the functions open, read, write, lseek, and close.
Example If we’re willing to read from the standard input and write to the standard output, then the program in Figure 1.4 copies any regular file on a UNIX system.
This header contains function prototypes for many of the UNIX system services, such as the read and write functions that we call.
In Section 3.9, we’ll examine the BUFFSIZE constant in detail, seeing how various values affect the efficiency of the program.
Regardless of the value of this constant, however, this program still copies any regular file.
If we compile the program into the standard name (a.out) and execute it as ./a.out > data.
If this output file doesn’t exist, the shell creates it by default.
The program copies lines that we type to the standard output until we type the end-of-file character (usually Control-D)
In Chapter 3, we describe the unbuffered I/O functions in more detail.
Standard I/O The standard I/O functions provide a buffered interface to the unbuffered I/O functions.
Using standard I/O relieves us from having to choose optimal buffer sizes, such as the BUFFSIZE constant in Figure 1.4
The standard I/O functions also simplify dealing with lines of input (a common occurrence in UNIX applications)
The read function, in contrast, reads a specified number of bytes.
As we shall see in Section 5.4, the standard I/O library provides functions that let us control the style of buffering used by the library.
This program copies standard input to standard output and can copy any regular file.
Figure 1.5 Copy standard input to standard output, using standard I/O.
The function getc reads one character at a time, and this character is written by putc.
After the last byte of input has been read, getc returns the constant EOF (defined in <stdio.h>)
The standard I/O constants stdin and stdout are also defined in the <stdio.h> header and refer to the standard input and standard output.
Program A program is an executable file residing on disk in a directory.
A program is read into memory and is executed by the kernel as a result of one of the seven exec functions.
Processes and Process ID An executing instance of a program is called a process, a term used on almost every page of this text.
Some operating systems use the term task to refer to a program that is being executed.
The UNIX System guarantees that every process has a unique numeric identifier called the process ID.
If we compile this program into the file a.out and execute it, we have.
When this program runs, it calls the function getpid to obtain its process ID.
As we shall see later, getpid returns a pid_t data type.
We don’t know its size; all we know is that the standards guarantee that it will fit in a long integer.
Because we have to tell printf the size of each argument to be printed, we have to cast the value to the largest data type that it might use (in this case, a long integer)
Although most process IDs will fit in an int, using a long promotes portability.
There are three primary functions for process control: fork, exec, and waitpid.
The exec function has seven variants, but we often refer to them collectively as simply the exec function.
Example The process control features of the UNIX System are demonstrated using a simple program (Figure 1.7) that reads commands from standard input and executes the commands.
Figure 1.7 Read commands from standard input and execute them.
There are several features to consider in this 30-line program.
We use the standard I/O function fgets to read one line at a time from the standard input.
When we type the end-of-file character (which is often Control-D) as the first character of a line, fgets returns a null pointer, the loop stops, and the process terminates.
In Chapter 18, we describe all the special terminal characters—end of file, backspace one character, erase entire line, and so on—and how to change them.
Because each line returned by fgets is terminated with a newline character, followed by a null byte, we use the standard C function strlen to calculate the length of the string, and then replace the newline with a null byte.
We do this because the execlp function wants a null-terminated argument, not a newline-terminated argument.
We call fork to create a new process, which is a copy of the caller.
We say that the caller is the parent and that the newly created process is the child.
Because fork creates a new process, we say that it is called once—by the parent — but returns twice—in the parent and in the child.
In the child, we call execlp to execute the command that was read from the standard input.
This replaces the child process with the new program file.
The combination of fork followed by exec is called spawning a new process on some operating systems.
In the UNIX System, the two parts are separated into individual functions.
Because the child calls execlp to execute the new program file, the parent wants to wait for the child to terminate.
This is done by calling waitpid, specifying which process to wait for: the pid argument, which is the process ID of the child.
The waitpid function also returns the termination status of the child — the status variable — but in this simple program, we don’t do anything with this value.
We could examine it to determine how the child terminated.
The most fundamental limitation of this program is that we can’t pass arguments to the command we execute.
We can’t, for example, specify the name of a directory to list.
To allow arguments would require that we parse the input line, separating the arguments by some convention, probably spaces or tabs, and then pass each argument as a separate parameter to the execlp function.
Nevertheless, this program is still a useful demonstration of the UNIX System’s process control functions.
If we run this program, we get the following results.
Note that our program has a different prompt — the percent sign—to distinguish it from the shell’s prompt.
Threads and Thread IDs Usually, a process has only one thread of control — one set of machine instructions executing at a time.
Some problems are easier to solve when more than one thread of control can operate on different parts of the problem.
Additionally, multiple threads of control can exploit the parallelism possible on multiprocessor systems.
All threads within a process share the same address space, file descriptors, stacks, and process-related attributes.
Each thread executes on its own stack, although any thread can access the stacks of other threads in the same process.
Because they can access the same memory, the threads need to synchronize access to shared data among themselves to avoid inconsistencies.
A thread ID from one process has no meaning in another process.
We use thread IDs to refer to specific threads as we manipulate the threads within a process.
Functions to control threads parallel those used to control processes.
The file <errno.h> defines the symbol errno and constants for each value that errno can assume.
For example, if errno is equal to the constant EACCES, this indicates a permission problem, such as insufficient permission to open the requested file.
On Linux, the error constants are listed in the errno(3) manual page.
This can be either an integer that contains the error number or a function that returns a pointer to the error number.
But in an environment that supports threads, the process address space is shared among multiple threads, and each thread needs its own local copy of errno to prevent one thread from interfering with another.
Linux, for example, supports multithreaded access to errno by defining it as.
There are two rules to be aware of with respect to errno.
First, its value is never cleared by a routine if an error does not occur.
Therefore, we should examine its value only when the return value from a function indicates that an error occurred.
Two functions are defined by the C standard to help with printing error messages.
This function maps errnum, which is typically the errno value, into an error message string and returns a pointer to the string.
The perror function produces an error message on the standard error, based on the current value of errno, and returns.
It outputs the string pointed to by msg, followed by a colon and a space, followed by the error message corresponding to the value of errno, followed by a newline.
Figure 1.8 shows the use of these two error functions.
If this program is compiled into the file a.out, we have $ ./a.out EACCES: Permission denied ./a.out: No such file or directory.
Note that we pass the name of the program—argv[0], whose value is ./a.out—as the argument to perror.
By doing this, if the program is executed as part of a pipeline, as in.
Instead of calling either strerror or perror directly, the examples in this text use the error functions shown in Appendix B.
These functions let us use the variable argument list facility of ISO C to handle error conditions with a single C statement.
Error Recover y The errors defined in <errno.h> can be divided into two categories: fatal and nonfatal.
The best we can do is print an error message on the user ’s screen or to a log file, and then exit.
Nonfatal errors, on the other hand, can sometimes be dealt with more robustly.
Most nonfatal errors are temporary, such as a resource shortage, and might not occur when there is less activity on the system.
Sometimes, EINTR can be treated as a nonfatal error when it interrupts a slow system call (more on this in Section 10.5)
The typical recovery action for a resource-related nonfatal error is to delay and retry later.
For example, if an error indicates that a network connection is no longer functioning, it might be possible for the application to delay a short time and then reestablish the connection.
Some applications use an exponential backoff algorithm, waiting a longer period of time in each subsequent iteration.
Ultimately, it is up to the application developer to determine the cases where an application can recover from an error.
If a reasonable recovery strategy can be used, we can improve the robustness of our application by avoiding an abnormal exit.
The user ID from our entry in the password file is a numeric value that identifies us to the system.
This user ID is assigned by the system administrator when our login name is assigned, and we cannot change it.
The user ID is normally assigned to be unique for every user.
We’ll see how the kernel uses the user ID to check whether we have the appropriate permissions to perform certain operations.
We call the user whose user ID is 0 either root or the superuser.
The entry in the password file normally has a login name of root, and we refer to the special privileges of this user as superuser privileges.
As we’ll see in Chapter 4, if a process has superuser privileges, most file permission checks are bypassed.
Client versions of Mac OS X ship with the superuser account disabled; server versions ship with the account already enabled.
Instructions are available on Apple’s Web site describing how to enable it.
Group ID Our entry in the password file also specifies our numeric group ID.
This, too, is assigned by the system administrator when our login name is assigned.
Typically, the password file contains multiple entries that specify the same group ID.
Groups are normally used to collect users together into projects or departments.
This allows the sharing of resources, such as files, among members of the same group.
We’ll see in Section 4.5 that we can set the permissions on a file so that all members of a group can access the file, whereas others outside the group cannot.
There is also a group file that maps group names into numeric group IDs.
The use of numeric user IDs and numeric group IDs for permissions is historical.
With every file on disk, the file system stores both the user ID and the group ID of a file’s owner.
Storing both of these values requires only four bytes, assuming that each is stored as a two-byte integer.
If the full ASCII login name and group name were used instead, additional disk space would be required.
In addition, comparing strings during permission checks is more expensive than comparing integers.
Users, however, work better with names than with numbers, so the password file maintains the mapping between login names and user IDs, and the group file provides the mapping between group names and group IDs.
The ls -l command, for example, prints the login name of the owner of a file, using the password file to map the numeric user ID into the corresponding login name.
Early UNIX systems used 16-bit integers to represent user and group IDs.
Example The program in Figure 1.9 prints the user ID and the group ID.
We call the functions getuid and getgid to return the user ID and the group ID.
In addition to the group ID specified in the password file for a login name, most versions of the UNIX System allow a user to belong to other groups.
These supplementary group IDs are obtained at login time by reading the file /etc/group and finding the first 16 entries that list the user as a member.
Signals are a technique used to notify a process that some condition has occurred.
For example, if a process divides by zero, the signal whose name is SIGFPE (floating-point exception) is sent to the process.
The process has three choices for dealing with the signal.
This option isn’t recommended for signals that denote a hardware exception, such as dividing by zero or referencing memory outside the address space of the process, as the results are undefined.
For a divide-by-zero condition, the default is to terminate the process.
Provide a function that is called when the signal occurs (this is called ‘‘catching’’ the signal)
By providing a function of our own, we’ll know when the signal occurs and we can handle it as we wish.
Two terminal keys, called the interrupt keyoften the DELETE key or Control-C — and the quit key—often Control-backslash — are used to interrupt the currently running process.
Another way to generate a signal is by calling the kill function.
We can call this function from a process to send a signal to another process.
Naturally, there are limitations: we have to be the owner of the other process (or the superuser) to be able to send it a signal.
If we invoke this program and press the interrupt key, the process terminates because the default action for this signal, named SIGINT, is to terminate the process.
The process hasn’t told the kernel to do anything other than the default with this signal, so the process terminates.
To catch this signal, the program needs to call the signal function, specifying the name of the function to call when the SIGINT signal is generated.
The function is named sig_int; when it’s called, it just prints a message and a new prompt.
The 11 new lines are indicated with a plus sign at the beginning of the line.
Figure 1.10 Read commands from standard input and execute them.
In Chapter 10, we’ll take a long look at signals, as most nontrivial applications deal with them.
These time values are used to record the time when a file was last modified, for example.
The primitive system data type time_t holds these time values.
The primitive system data type clock_t holds these time values.
We’ll show how to obtain the number of clock ticks per second with the sysconf function in Section 2.5.4
When we measure the execution time of a process, as in Section 3.9, we’ll see that the UNIX System maintains three values for a process:
The clock time, sometimes called wall clock time, is the amount of time the process takes to run, and its value depends on the number of other processes being run on the system.
Whenever we report the clock time, the measurements are made with no other activities on the system.
The user CPU time is the CPU time attributed to user instructions.
The system CPU time is the CPU time attributed to the kernel when it executes on behalf of the process.
For example, whenever a process executes a system service, such as read or write, the time spent within the kernel performing that system service is charged to the process.
The sum of user CPU time and system CPU time is often called the CPU time.
It is easy to measure the clock time, user time, and system time of any process: simply execute the time(1) command, with the argument to the time command being the command we want to measure.
The output format from the time command depends on the shell being used, because some shells don’t run /usr/bin/time, but instead have a separate built-in function to measure the time it takes commands to run.
In Section 8.17, we’ll see how to obtain these three times from a running process.
The general topic of times and dates is covered in Section 6.10
All operating systems provide service points through which programs request services from the kernel.
All implementations of the UNIX System provide a well-defined, limited number of entry points directly into the kernel called system calls (recall Figure 1.1)
The exact number of system calls varies depending on the operating system version.
More recent systems have seen incredible growth in the number of supported system calls.
The system call interface has always been documented in Section 2 of the UNIX Programmer ’s Manual.
Its definition is in the C language, no matter which implementation technique is actually used on any given system to invoke a system call.
This differs from many older operating systems, which traditionally defined the kernel entry points in the assembly language of the machine.
The technique used on UNIX systems is for each system call to have a function of the same name in the standard C library.
The user process calls this function, using the standard C calling sequence.
This function then invokes the appropriate kernel service, using whatever technique is required on the system.
For example, the function may put one or more of the C arguments into general registers and then execute some machine instruction that generates a software interrupt in the kernel.
For our purposes, we can consider the system calls to be C functions.
Section 3 of the UNIX Programmer ’s Manual defines the general-purpose library functions available to programmers.
These functions aren’t entry points into the kernel, although they may invoke one or more of the kernel’s system calls.
For example, the printf function may use the write system call to output a string, but the strcpy (copy a string) and atoi (convert ASCII to integer) functions don’t involve the kernel at all.
From an implementor’s point of view, the distinction between a system call and a library function is fundamental.
From a user’s perspective, however, the difference is not as critical.
From our perspective in this text, both system calls and library functions appear as normal C functions.
We should realize, however, that we can replace the library functions, if desired, whereas the system calls usually cannot be replaced.
There are many ways to do memory allocation and its associated garbage collection (best fit, first fit, and so on)
The UNIX system call that handles memory allocation, sbrk(2), is not a general-purpose memory manager.
It increases or decreases the address space of the process by a specified number of bytes.
How that space is managed is up to the process.
The memory allocation function, malloc(3), implements one particular type of allocation.
If we don’t like its operation, we can define our own malloc function, which will probably use the sbrk system call.
In fact, numerous software packages implement their own memory allocation algorithms with the sbrk system call.
Figure 1.11 shows the relationship between the application, the malloc function, and the sbrk system call.
Figure 1.11 Separation of malloc function and sbrk system call.
Here we have a clean separation of duties: the system call in the kernel allocates an additional chunk of space on behalf of the process.
The malloc library function manages this space from user level.
Another example to illustrate the difference between a system call and a library function is the interface the UNIX System provides to determine the current time and date.
Some operating systems provide one system call to return the time and another to return the date.
Any special handling, such as the switch to or from daylight saving time, is handled by the kernel or requires human intervention.
Any interpretation of this value, such as converting it to a human-readable time and date using the local time zone, is left to the user process.
The standard C library provides routines to handle most cases.
These library routines handle such details as the various algorithms for daylight saving time.
An application can either make a system call or call a library routine.
Also realize that many library routines invoke a system call.
Another difference between system calls and library functions is that system calls usually provide a minimal interface, whereas library functions often provide more elaborate functionality.
We’ve seen this already in the difference between the sbrk system call and the malloc library function.
Figure 1.12 Difference between C library functions and system calls.
The process control system calls (fork, exec, and waitpid) are usually invoked by the user’s application code directly.
But some library routines exist to simplify certain common cases: the system and popen library routines, for example.
In Section 8.13, we’ll show an implementation of the system function that invokes the basic process control system calls.
We’ll enhance this example in Section 10.18 to handle signals correctly.
To define the interface to the UNIX System that most programmers use, we have to describe both the system calls and some of the library functions.
In this text, we’ll use the term function to refer to both system calls and library functions, except when the distinction is necessary.
This chapter has provided a short tour of the UNIX System.
We’ve described some of the fundamental terms that we’ll encounter over and over again.
We’ve seen numerous small examples of UNIX programs to give us a feel for what the remainder of the text talks about.
The next chapter is about standardization of the UNIX System and the effect of work in this area on current systems.
Standards, particularly the ISO C standard and the POSIX.1 standard, will affect the rest of the text.
Much work has gone into standardizing the UNIX programming environment and the C programming language.
Although applications have always been quite portable across different versions of the UNIX operating system, the proliferation of versions and differences during the 1980s led many large users, such as the U.S.
In this chapter we first look at the various standardization efforts that have been under way over the past two and a half decades.
We then discuss the effects of these UNIX programming standards on the operating system implementations that are described in this book.
An important part of all the standardization efforts is the specification of various limits that each implementation must define, so we look at these limits and the various ways to determine their values.
The intent of the ISO C standard is to provide portability of conforming C programs to a wide variety of operating systems, not only the UNIX System.
This library is important because all contemporary UNIX systems, such as the ones described in this book, provide the library routines that are specified in the C standard.
The changes don’t affect the POSIX interfaces described in this book, except for the addition of the restrict keyword to some of the function prototypes.
This keyword is used to tell the compiler which pointer references can be optimized, by indicating that the object to which the pointer refers is accessed in the function only via that pointer.
As with most standards, there is a delay between the standard’s approval and the modification of software to conform to it.
As each vendor’s compilation systems evolve, they add more support for the latest version of the ISO C standard.
The POSIX.1 standard includes these headers, as well as others.
The ISO C headers depend on which version of the C compiler is used with the operating system.
Of specific interest to this book is the 1003.1 operating system interface standard, whose goal is to promote the portability of applications among various UNIX System environments.
This standard defines the services that an operating system must.
Although the 1003.1 standard is based on the UNIX operating system, the standard is not restricted to UNIX and UNIX-like systems.
Indeed, some vendors supplying proprietary operating systems claim that these systems have been made POSIX compliant, while still leaving all their proprietary features in place.
Because the 1003.1 standard specifies an interface and not an implementation, no distinction is made between system calls and library functions.
Standards are continually evolving, and the 1003.1 standard is no exception.
No new interfaces or features were added, but the text was revised.
This standard was commonly referred to as POSIX.1, a term which we’ll use in this text to refer to the different versions of the standard.
The IEEE 1003.1 working group continued to make changes to the standard.
The resulting standard, IEEE Standard 1003.1-2001, included the following other standards:
All four figures summarize which headers are included in the implementations discussed in this book.
Its interfaces are divided into required ones and optional ones.
The optional interfaces are further divided into 40 sections, based on functionality.
The sections containing nonobsolete programming interfaces are summarized in Figure 2.5 with their respective option codes.
Option codes are two- to three-character abbreviations that identify the interfaces that belong to.
In this text, however, we use the traditional terminology and refer to operations that require superuser privilege.
Figure 2.3 XSI option headers defined by the POSIX standard.
After more than twenty years of work, the standards are mature and stable.
The POSIX.1 standard is maintained by an open working group known as the Austin Group (http://www.opengroup.org/austin)
To ensure that they are still relevant, the standards need to be either updated or reaffirmed every so often.
The Open Group owns the UNIX trademark and uses the Single UNIX Specification to define the interfaces an implementation must support to call itself a UNIX system.
Vendors must file conformance statements, pass test suites to verify conformance, and license the right to use the UNIX trademark.
Several of the interfaces that are optional for XSI-conforming systems are divided into option groups based on common functionality, as follows:
The Single UNIX Specification is a publication of The Open Group, which was formed in 1996 as a merger of X/Open and the Open Software Foundation (OSF), both industry consortia.
X/Open used to publish the X/Open Portability Guide, which adopted specific standards and filled in the gaps where functionality was missing.
The goal of these guides was to improve application portability beyond what was possible by merely conforming to published standards.
It grew out of the Common Open Software Environment (COSE) initiative, whose goal was to improve application portability across all implementations of the UNIX operating system.
The COSE group — Sun, IBM, HP, Novell/USL, and OSF—went further than endorsing standards by including interfaces used by common commercial applications.
The new version added support for threads, real-time interfaces, 64-bit processing, large files, and enhanced multibyte character processing.
It merged more technical corrections into the main text of the standard.
In 2008, the Single UNIX Specification was updated, including corrections and new interfaces, removing obsolete interfaces, and marking other interfaces as being obsolescent in preparation for future removal.
Additionally, some previously optional interfaces were promoted to nonoptional status, including asynchronous I/O, barriers, clock selection, memory-mapped files, memory protection, reader–writer locks, realtime signals, POSIX semaphores, spin locks, thread-safe functions, threads, timeouts, and timers.
The POSIX.1 FIPS has since been withdrawn, so we won’t consider it further in this text.
The previous section described ISO C, IEEE POSIX, and the Single UNIX Specification — three standards originally created by independent organizations.
How do these standards relate to the real world? These standards are taken by vendors and turned into actual implementations.
In this book, we are interested in both these standards and their implementation.
These were the first releases widely distributed outside of Bell Laboratories.
One at the University of California at Berkeley that led to the 4.xBSD implementations.
Xenix was originally developed from Version 7, with many features later taken from System V.
As with POSIX.1, the SVID specified an interface, not an implementation.
No distinction was made in the SVID between system calls and library functions.
To obtain the source code to the BSD system you had to have a UNIX source license from AT&T.
Its introduction was delayed, however, because of legal battles with USL.
This version of BSD is described in the book by McKusick et al.
The UNIX system development done at Berkeley started with PDP-11s, then moved to the VAX minicomputer, and then to other so-called workstations.
This support was provided by Bill Jolitz and was documented in a series of monthly articles in Dr.
The FreeBSD project was formed to carry on the BSD line after the Computing Science Research Group at the University of California at Berkeley decided to end its work on the BSD versions of the UNIX operating system, and the 386BSD project seemed to be neglected for too long.
All software produced by the FreeBSD project is freely available in both binary and source forms.
The FreeBSD 8.0 operating system was one of the four operating systems used to test the examples in this book.
The NetBSD project (http://www.netbsd.org) is similar to the FreeBSD project, but emphasizes portability between hardware platforms.
The OpenBSD project (http://www.openbsd.org) is similar to FreeBSD but places a greater emphasis on security.
Linux is an operating system that provides a rich programming environment similar to that of a UNIX System; it is freely available under the GNU Public License.
The popularity of Linux is somewhat of a phenomenon in the computer industry.
Linux is distinguished by often being the first operating system to support new hardware.
Linux was created in 1991 by Linus Torvalds as a replacement for MINIX.
A grass-roots effort then sprang up, whereby many developers across the world volunteered their time to use and enhance it.
The Ubuntu 12.04 distribution of Linux was one of the operating systems used to test the examples in this book.
That distribution uses the 3.2.0 version of the Linux operating system kernel.
Mac OS X is based on entirely different technology than prior versions.
As of version 10.5, the Intel port of Mac OS X has been certified to be a UNIX system.
Solaris is based on System V Release 4, but includes more than fifteen years of enhancements from the engineers at Sun Microsystems.
It is arguably the only commercially successful SVR4 descendant, and is formally certified to be a UNIX system.
In 2005, Sun Microsystems released most of the Solaris operating system source code to the public as part of the OpenSolaris open source operating system in an attempt to build an external developer community around Solaris.
The Solaris 10 UNIX system was one of the operating systems used to test the examples in this book.
Other versions of the UNIX system that have been certified in the past include.
The standards that we’ve mentioned define a subset of any actual system.
Although only Mac OS X and Solaris can call themselves UNIX systems, all four provide a similar programming environment.
Because all four are POSIX compliant to varying degrees, we will also concentrate on the features required by the POSIX.1 standard, noting any differences between POSIX and the actual implementations of these four systems.
Those features and routines that are specific to only a particular implementation are clearly marked.
We’ll also note any features that are required on UNIX systems but are optional on other POSIX-conforming systems.
In this text, we’ll use only the POSIX.1 feature, although we’ll mention the nonstandard feature that it replaces.
Many of these have been hard coded into programs or were determined using ad hoc techniques.
Compile-time limits (e.g., what’s the largest value of a short integer?)
Compile-time limits can be defined in headers that any program can include at compile time.
But runtime limits require the process to call a function to obtain the limit’s value.
Additionally, some limits can be fixed on a given implementation—and could therefore be defined statically in a header—yet vary on another implementation and would require a runtime function call.
An example of this type of limit is the maximum number of bytes in a filename.
Most UNIX System implementations these days support multiple file system types, and each type has its own limit.
This is the case of a runtime limit that depends on where in the file system the file in question is located.
To solve these problems, three types of limits are provided:
Runtime limits not associated with a file or directory (the sysconf function)
If it is not defined in a header, however, the application must call one of the three conf functions (which we describe shortly) to determine its value at runtime.
The third column in Figure 2.6 shows the minimum acceptable values from the ISO C standard.
This allows for a system with 16-bit integers using one’s-complement arithmetic.
The fourth column shows the values from a Linux system with 32-bit integers using two’scomplement arithmetic.
Note that none of the unsigned data types has a minimum value, as this value must be 0 for an unsigned data type.
On a 64-bit system, the values for long integer maximums match the maximum values for long long integers.
One difference that we will encounter is whether a system provides signed or unsigned character values.
From the fourth column in Figure 2.6, we see that this.
The floating-point data types in the header <float.h> have a similar set of definitions.
Another ISO C constant that we’ll encounter is FOPEN_MAX, the minimum number of standard I/O streams that the implementation guarantees can be open at once.
It is the maximum number of unique filenames generated by the tmpnam function.
We’ll have more to say about this constant in Section 5.13
POSIX.1 defines numerous constants that deal with implementation limits of the operating system.
Unfortunately, this is one of the more confusing aspects of POSIX.1
These limits and constants are divided into the following seven categories:
Of these limits and constants, some may be defined in <limits.h>, and others may or may not be defined, depending on certain conditions.
We describe the limits and constants that may or may not be defined in Section 2.5.4, when we describe the sysconf, pathconf, and fpathconf functions.
These minimum values do not change from one system to another.
A conforming POSIX.1 implementation must provide values that are at least this large.
This is why they are called minimums, although their names all contain MAX.
Also, to ensure portability, a strictly conforming application must not require a larger value.
We describe what each of these constants refers to as we proceed through the text.
A strictly conforming POSIX application is different from an application that is merely POSIX conforming.
A POSIX-conforming application uses only interfaces defined in IEEE Standard 1003.1-2008
A strictly conforming POSIX application must meet further restrictions, such as not relying on any undefined behavior, not using any obsolescent interfaces, and not requiring values of constants larger than the minimums shown in Figure 2.8
Unfortunately, some of these invariant minimum values are too small to be of practical use.
For example, most UNIX systems today provide far more than 20 open files per process.
The names without the leading _POSIX_ were intended to be the actual values that a given implementation supports.
For example, a particular value may not be included in the header if its actual value for a given process depends on the amount of memory on the system.
If the values are not defined in the header, we can’t use them as array bounds at compile time.
To determine the actual implementation value at runtime, POSIX.1 decided to provide three functions for us to call—sysconf, pathconf, and fpathconf.
This means that the value has no practical upper bound.
On Solaris, for example, the number of functions you can register with atexit to be run when a process ends is limited only by the amount of memory on the system.
We’ll return to this problem of indeterminate runtime limits in Section 2.5.5
We’ve listed various minimum values that an implementation must support, but how do we find out the limits that a particular system actually supports? As we mentioned earlier, some of these limits might be available at compile time; others must be determined at runtime.
We’ve also mentioned that some limits don’t change in a given system, whereas others can change because they are associated with a file or directory.
The runtime limits are obtained by calling one of the following three functions.
The difference between the last two functions is that one takes a pathname as its argument and the other takes a file descriptor argument.
Figure 2.11 lists the name arguments that sysconf uses to identify system limits.
Constants beginning with _SC_ are used as arguments to sysconf to identify the runtime limit.
Figure 2.12 lists the name arguments that are used by pathconf and fpathconf to identify system limits.
Constants beginning with _PC_ are used as arguments to pathconf and fpathconf to identify the runtime limit.
We need to look in more detail at the different return values from these three functions.
Some restrictions apply to the pathconf pathname argument and the fpathconf fd argument.
If any of these restrictions isn’t met, the results are undefined.
If the referenced file is a directory, the return value applies to the directory itself, not to the filename entries within the directory.
The value returned is the maximum length of a relative pathname when the specified directory is the working directory.
Unfortunately, this isn’t the real maximum length of an absolute pathname, which is what we want to know.
Figure 2.12 Limits and name arguments to pathconf and fpathconf.
The referenced file for _PC_PIPE_BUF must be a pipe, FIFO, or directory.
In the first two cases (pipe or FIFO), the return value is the limit for the referenced pipe or FIFO.
For the other case (a directory), the return value is the limit for any FIFO created in that directory.
The value returned is the maximum length of the string that a symbolic link in that directory can contain.
Figure 2.13 Build C program to print all supported configuration limits.
All symbols are not defined on every platform, so the awk program surrounds each call to pathconf and sysconf with the necessary #ifdef statements.
The program in Figure 2.14, generated by the awk program, prints all these limits, handling the case in which a limit is not defined.
In contrast, the entry ‘‘unsupported’’ means that the symbol is defined by the system but unrecognized by the sysconf or pathconf functions.
The entry ‘‘no limit’’ means that the system defines no limit for the constant, but this doesn’t mean that the limit is infinite; it just means that the limit is indeterminite.
Another potential source of inaccuracy in Linux is that the pathconf and fpathconf functions are implemented in the C library.
The configuration limits returned by these functions depend on the underlying file system type, so if your file system is unknown to the C library, the functions return an educated guess.
We mentioned that some of the limits can be indeterminate.
The problem we encounter is that if these limits aren’t defined in the <limits.h> header, we can’t use them at compile time.
But they might not be defined at runtime if their value is indeterminate! Let’s look at two specific cases: allocating storage for a pathname and determining the number of file descriptors.
Pathnames Many programs need to allocate storage for a pathname.
Figure 2.16 shows a function that we’ll use throughout this text to allocate storage dynamically for a pathname.
The value returned by pathconf is the maximum size of a relative pathname when the first argument is the working directory, so we specify the root as the first argument and add 1 to the result.
If pathconf indicates that PATH_MAX is indeterminate, we have to punt and just guess a value.
If the operating system implementation conforms to one of these prior versions and doesn’t conform to any version of the Single UNIX Specification (which does require the terminating null byte to be included), we need to add 1 to the amount of memory we allocate for a pathname, just to be on the safe side.
The correct way to handle the case of an indeterminate result depends on how the allocated space is being used.
If we are allocating space for a call to getcwd, for example — to return the absolute pathname of the current working directory; see Section 4.23—and if the allocated space is too small, an error is returned and errno is set to ERANGE.
We could keep doing this until the call to getcwd succeeded.
A common sequence of code in a daemon process — a process that runs in the background, not connected to a terminal—is one that closes all open files.
Some programs have the following code sequence, assuming the constant NOFILE was defined in the <sys/param.h> header:
As with our pathname example, this strategy is not guaranteed to work for all cases, but it’s the best we can do without using a more exotic approach.
We might be tempted to call close until we get an error return, but the error return from close (EBADF) doesn’t distinguish between an invalid descriptor and a descriptor that wasn’t open.
Some implementations will return LONG_MAX for limit values that are effectively unlimited.
This isn’t a good idea, because it can cause programs to behave badly.
For example, we can use the ulimit command built into the Bourne-again shell to change the maximum number of files our processes can have open at one time.
This generally requires special (superuser) privileges if the limit is to be effectively unlimited.
It can be used to return the maximum number of descriptors that a process can have open.
With it, we can detect that there is no configured upper bound to the number of open files our processes can open, so we can avoid this problem.
The OPEN_MAX value is called runtime invariant by POSIX, meaning that its value should not change during the lifetime of a process.
This value can also be changed from the C shell with the limit command, and from the Bourne, Bourne-again, Debian Almquist, and Korn shells with the ulimit command.
If our system supports this functionality, we could change the function in Figure 2.17 to call sysconf every time it is called, not just the first time.
If we are to write portable applications that depend on any of these optionally supported features, we need a portable way to determine whether an implementation supports a given option.
If the symbolic constant is not defined, we must use sysconf, pathconf, or fpathconf to determine whether the option is supported.
For each option, we have three possibilities for a platform’s support status.
If the symbolic constant is defined to be greater than zero, then the corresponding option is supported.
If the symbolic constant is defined to be equal to zero, then we must call sysconf, pathconf, or fpathconf to determine whether the option is supported.
The symbolic constants used with pathconf and fpathconf are summarized in Figure 2.18
As with the system limits, there are several points to note regarding how options are treated by sysconf, pathconf, and fpathconf.
The value returned for _SC_VERSION indicates the four-digit year and two-digit month of the standard.
The value returned for _SC_XOPEN_VERSION indicates the version of the XSI that the system supports.
Platforms conforming to POSIX.1-2008 are also required to support the following options:
Their corresponding _SC symbols are also retained for backward compatibility.
If it is a directory, the return value indicates whether this option applies to files within that directory.
For _PC_NO_TRUNC, the return value applies to filenames within the directory.
The referenced file for _PC_VDISABLE must be a terminal file.
Figure 2.18 Options and name arguments to pathconf and fpathconf.
The headers define numerous POSIX.1 and XSI symbols, as we’ve described.
Even so, most implementations can add their own definitions to these headers, in addition to the POSIX.1 and XSI definitions.
When used, they are typically defined in the cc command, as in.
This causes the feature test macro to be defined before any header files are included by the C program.
If we want to use only the POSIX.1 definitions, we can also set the first line of a source file to.
The Single UNIX Specification defines the c99 utility as the interface to the C compilation environment.
Historically, certain C data types have been associated with certain UNIX system variables.
But many larger systems need more than 256 values for these device numbers, so a different technique is needed.
More of these data types are defined in other headers as well.
These data types are defined in the headers with the C typedef facility.
Figure 2.21 lists many of the primitive system data types that we’ll encounter in this text.
By defining these data types this way, we do not build into our programs implementation details that can change from one system to another.
We describe what each of these data types is used for when we encounter them later in the text.
Conflicts are unintended, but if they should arise, POSIX.1 defers to the ISO C standard.
The value returned is a clock_t value, but ISO C doesn’t specify its units.
POSIX.1 defines the function times that returns both the CPU time (for the caller and all its terminated children) and the clock time.
The sysconf function is used to obtain the number of clock ticks per second for use with the return values from the times function.
Thus we must take care when using variables of type clock_t so that we don’t mix variables with different units.
Another area of potential conflict is when the ISO C standard specifies a function, but doesn’t specify it as strongly as POSIX.1 does.
This is the case for functions that require a different implementation in a POSIX environment (with multiple processes) than in an ISO C environment (where very little can be assumed about the host operating system)
Nevertheless, POSIX-compliant systems implement the ISO C function for compatibility.
If we unknowingly use the signal function provided by Solaris (hoping to write portable code that can be run in ISO C environments and under older UNIX systems), it will provide semantics different from the POSIX.1 sigaction function.
Much has happened with the standardization of the UNIX programming environment over the past two and a half decades.
We’ve described the dominant standards — ISO C, POSIX, and the Single UNIX Specification—and their effect on the four platforms that we’ll examine in this text—FreeBSD, Linux, Mac OS X, and Solaris.
These standards try to define certain parameters that can change with each implementation, but we’ve seen that these limits are imperfect.
We’ll encounter many of these limits and magic constants as we proceed through the text.
The bibliography specifies how to obtain copies of the standards discussed in this chapter.
Because all 29 headers could be included in a program and because ISO C does not allow multiple typedefs for the same name, how must the headers be written?
We’ll start our discussion of the UNIX System by describing the functions available for file I/O—open a file, read a file, write a file, and so on.
Most file I/O on a UNIX system can be performed using only five functions: open, read, write, lseek, and close.
We then examine the effect of various buffer sizes on the read and write functions.
The term unbuffered means that each read or write invokes a system call in the kernel.
These unbuffered I/O functions are not part of ISO C, but are part of POSIX.1 and the Single UNIX Specification.
Whenever we describe the sharing of resources among multiple processes, the concept of an atomic operation becomes important.
We examine this concept with regard to file I/O and the arguments to the open function.
This leads to a discussion of how files are shared among multiple processes and which kernel data structures are involved.
After describing these features, we describe the dup, fcntl, sync, fsync, and ioctl functions.
To the kernel, all open files are referred to by file descriptors.
When we open an existing file or create a new file, the kernel returns a file descriptor to the process.
When we want to read or write a file, we identify the file with the file descriptor that was returned by open or creat as an argument to either read or write.
This convention is used by the shells and many applications; it is not a feature of the UNIX kernel.
Nevertheless, many applications would break if these associations weren’t followed.
A file is opened or created by calling either the open function or the openat function.
We show the last argument as ..., which is the ISO C way to specify that the number and types of the remaining arguments may vary.
For these functions, the last argument is used only when a new file is being created, as we describe later.
We show this argument as a comment in the prototype.
The path parameter is the name of the file to open or create.
This function has a multitude of options, which are specified by the oflag argument.
This argument is formed by ORing together one or more of the following constants from the <fcntl.h> header:
The purpose of the O_SEARCH constant is to evaluate search permissions at the time a directory is opened.
Further operations using the directory’s file descriptor will not reevaluate permission to search the directory.
None of the versions of the operating systems covered in this book support O_SEARCH yet.
One and only one of the previous five constants must be specified.
O_APPEND Append to the end of file on each write.
This option requires a third argument to the open function (a fourth argument to the openat function) — the mode, which specifies the access permission bits of the new file.
When we describe a file’s access permission bits in Section 4.5, we’ll see how to specify the mode and how it can be modified by the umask value of a process.
O_DIRECTORY Generate an error if path doesn’t refer to a directory.
This test for whether the file already exists and the creation of the file if it doesn’t exist is an atomic operation.
We describe atomic operations in more detail in Section 3.11
O_NOCTTY If path refers to a terminal device, do not allocate the device as the controlling terminal for this process.
O_NOFOLLOW Generate an error if path refers to a symbolic link.
O_NONBLOCK If path refers to a FIFO, a block special file, or a character special file, this option sets the nonblocking mode for both the opening of the file and subsequent I/O.
In earlier releases of System V, the O_NDELAY (no delay) flag was introduced.
This option is similar to the O_NONBLOCK (nonblocking) option, but an ambiguity was introduced in the return value from a read operation.
SVR4-based systems still support the no-delay option, with the old semantics, but new applications should use the nonblocking option instead.
O_SYNC Have each write wait for physical I/O to complete, including I/O necessary to update file attributes modified as a result of the write.
O_TTY_INIT When opening a terminal device that is not already open, set the nonstandard termios parameters to values that result in behavior that conforms to the Single UNIX Specification.
They are part of the synchronized input and output option of the Single UNIX Specification (and thus POSIX.1)
O_DSYNC Have each write wait for physical I/O to complete, but don’t wait for file attributes to be updated if they don’t affect the ability to read the data just written.
The O_DSYNC flag affects a file’s attributes only when they need to be updated to reflect a change in the file’s data (for example, update the file’s size to reflect more data)
With the O_SYNC flag, data and attributes are always updated synchronously.
When overwriting an existing part of a file opened with the O_DSYNC flag, the file times wouldn’t be updated synchronously.
In contrast, if we had opened the file with the O_SYNC flag, every write to the file would update the file’s times before the write returns, regardless of whether we were writing over existing bytes or appending to the file.
O_RSYNC Have each read operation on the file descriptor wait until any pending writes for the same portion of the file are complete.
Because the two flags are equivalent, they define the flags to have the same value.
The file descriptor returned by open and openat is guaranteed to be the lowestnumbered unused descriptor.
This fact is used by some applications to open a new file on standard input, standard output, or standard error.
The fd parameter distinguishes the openat function from the open function.
In this case, the fd parameter is ignored and the openat function behaves like the open function.
The path parameter specifies a relative pathname and the fd parameter is a file descriptor that specifies the starting location in the file system where the relative pathname is to be evaluated.
The fd parameter is obtained by opening the directory where the relative pathname is to be evaluated.
The path parameter specifies a relative pathname and the fd parameter has the special value AT_FDCWD.
In this case, the pathname is evaluated starting in the current working directory and the openat function behaves like the open function.
The openat function is one of a class of functions added to the latest version of.
First, it gives threads a way to use relative pathnames to open files in directories other than the current working directory.
As we’ll see in Chapter 11, all threads in the same process share the same current working directory, so this makes it difficult for multiple threads in the same process to work in different directories at the same time.
The basic idea behind TOCTTOU errors is that a program is vulnerable if it makes two file-based function calls where the second call depends on the results of the first call.
Because the two calls are not atomic, the file can change between the two calls, thereby invalidating the results of the first call, leading to a program error.
Wei and Pu [2005] discuss TOCTTOU weaknesses in the UNIX file system interface.
BSD-derived systems, in contrast, returned an error status, with errno set to ENAMETOOLONG.
Silently truncating the filename presents a problem that affects more than simply the creation of new files.
As we saw in Chapter 2, this value can vary based on the type of the file system, and we can use fpathconf or pathconf to query a directory to see which behavior is supported.
For the BSD-style file system (known as UFS), however, SVR4-based systems do generate an error.
Most modern file systems support a maximum of 255 characters for filenames.
Because filenames are usually shorter than this limit, this constraint tends to not present problems for most applications.
A new file can also be created by calling the creat function.
There was no way to open a file that didn’t already exist.
Therefore, a separate system call, creat, was needed to create new files.
We’ll show how to specify mode in Section 4.5 when we describe a file’s access permissions in detail.
One deficiency with creat is that the file is opened only for writing.
Before the new version of open was provided, if we were creating a temporary file that we wanted to write and then read back, we had to call creat, close, and then open.
A better way is to use the open function, as in.
An open file is closed by calling the close function.
Closing a file also releases any record locks that the process may have on the file.
When a process terminates, all of its open files are closed automatically by the kernel.
Many programs take advantage of this fact and don’t explicitly close open files.
Every open file has an associated ‘‘current file offset,’’ normally a non-negative integer that measures the number of bytes from the beginning of the file.
We describe some exceptions to the ‘‘non-negative’’ qualifier later in this section.
Read and write operations normally start at the current file offset and cause the offset to be incremented by the number of bytes read or written.
The interpretation of the offset depends on the value of the whence argument.
If whence is SEEK_SET, the file’s offset is set to offset bytes from the beginning of the file.
If whence is SEEK_CUR, the file’s offset is set to its current value plus the offset.
If whence is SEEK_END, the file’s offset is set to the size of the file plus the offset.
Because a successful call to lseek returns the new file offset, we can seek zero bytes from the current position to determine the current offset:
Similar functionality was provided in Version 6 by the functions seek and tell.
Example The program in Figure 3.1 tests its standard input to see whether it is capable of seeking.
Figure 3.1 Test whether standard input is capable of seeking.
This offset is then used by the next read or write operation.
The file’s offset can be greater than the file’s current size, in which case the next write to the file will extend the file.
This is referred to as creating a hole in a file and is allowed.
A hole in a file isn’t required to have storage backing it on disk.
Depending on the file system implementation, when you write after seeking past the end of a file, new disk blocks might be allocated to store the data, but there is no need to allocate disk blocks for the data between the old end of file and the location where you start writing.
Example The program shown in Figure 3.2 creates a file with a hole in it.
Figure 3.2 Create a file with a hole in it.
We use the od(1) command to look at the contents of the file.
The -c flag tells it to print the contents as characters.
We can see that the unwritten bytes in the middle are read back as zero.
The seven-digit number at the beginning of each line is the byte offset in octal.
To prove that there is really a hole in the file, let’s compare the file we just created with a file of the same size, but without holes:
In this example, we call the write function (Section 3.8)
We’ll have more to say about files with holes in Section 4.12
Because the offset address that lseek uses is represented by an off_t, implementations are allowed to support whatever size is appropriate on their particular platform.
The Single UNIX Specification provides a way for applications to determine which environments are supported through the sysconf function (Section 2.5.4)
Figure 3.3 Data size options and name arguments to sysconf.
Different flags and libraries might be needed, depending on the environments supported by each platform.
Unfortunately, this is one area in which implementations haven’t caught up to the standards.
Data is read from an open file with the read function.
If the read is successful, the number of bytes read is returned.
If the end of file is encountered, 0 is returned.
There are several cases in which the number of bytes actually read is less than the amount requested:
When reading from a regular file, if the end of file is reached before the requested number of bytes has been read.
The next time we call read, it will return 0 (end of file)
Normally, up to one line is read at a time.
Buffering within the network may cause less than the requested amount to be returned.
If the pipe contains fewer bytes than requested, read will return only what is available.
Some record-oriented devices, such as magnetic tape, can return up to a single record at a time.
When interrupted by a signal and a partial amount of data has already been read.
Before a successful return, the offset is incremented by the number of bytes actually read.
POSIX.1 changed the prototype for this function in several ways.
Data is written to an open file with the write function.
The return value is usually equal to the nbytes argument; otherwise, an error has occurred.
For a regular file, the write operation starts at the file’s current offset.
If the O_APPEND option was specified when the file was opened, the file’s offset is set to the current end of file before each write operation.
After a successful write, the file’s offset is incremented by the number of bytes actually written.
The program in Figure 3.5 copies a file, using only the read and write functions.
It reads from standard input and writes to standard output, assuming that these have been set up by the shell before this program is executed.
Indeed, all normal UNIX system shells provide a way to open a file for reading on standard input and to create (or rewrite) a file on standard output.
This prevents the program from having to open the input and output files, and allows the user to take advantage of the shell’s I/O redirection facilities.
The program doesn’t close the input file or output file.
Instead, the program uses the feature of the UNIX kernel that closes all open file descriptors in a process when that process terminates.
This example works for both text files and binary files, since there is no difference between the two to the UNIX kernel.
One question we haven’t answered, however, is how we chose the BUFFSIZE value.
Before answering that, let’s run the program using different values for BUFFSIZE.
Figure 3.6 Timing results for reading with different buffer sizes on Linux.
The file was read using the program shown in Figure 3.5, with standard output redirected to /dev/null.
This accounts for the minimum in the system time occurring at the few timing measurements starting around a BUFFSIZE of 4,096
Increasing the buffer size beyond this limit has little positive effect.
Most file systems support some kind of read-ahead to improve performance.
When sequential reads are detected, the system tries to read in more data than an application requests, assuming that the application will read it shortly.
We’ll return to this timing example later in the text.
Beware when trying to measure the performance of programs that read and write files.
The operating system will try to cache the file incore, so if you measure the performance of the program repeatedly, the successive timings will likely be better than the first.
This improvement occurs because the first run causes the file to be entered into the system’s cache, and successive runs access the file from the system’s cache instead of from the disk.
Back in the day, a computer ’s main memory was built out of ferrite core.
This is where the phrase ‘‘core dump’’ comes from: the main memory image of a program stored in a file on disk for diagnosis.
In the tests reported in Figure 3.6, each run with a different buffer size was made using a different copy of the file so that the current run didn’t find the data in the cache from the previous run.
The files are large enough that they all don’t remain in the cache (the test system was configured with 6 GB of RAM)
The UNIX System supports the sharing of open files among different processes.
Before describing the dup function, we need to describe this sharing.
To do this, we’ll examine the data structures used by the kernel for all I/O.
The following description is conceptual; it may or may not match a particular implementation.
Refer to Bach [1986] for a discussion of these structures in System V.
For a similar discussion of Solaris, see McDougall and Mauro [2007]
The kernel uses three data structures to represent an open file, and the relationships among them determine the effect one process has on another with regard to file sharing.
Within each process table entry is a table of open file descriptors, which we can think of as a vector, with one entry per descriptor.
Each open file (or device) has a v-node structure that contains information about the type of file and pointers to functions that operate on the file.
This information is read from disk when the file is opened, so that all the pertinent information about the file is readily available.
For example, the i-node contains the owner of the file, the size of the file, pointers to where the actual data blocks for the file are located on disk, and so on.
We talk more about i-nodes in Section 4.14 when we describe the typical UNIX file system in more detail.
Although the implementations differ, the v-node is conceptually the same as a generic i-node.
Both point to an i-node structure specific to the file system.
We’re ignoring some implementation details that don’t affect our discussion.
For example, the table of open file descriptors can be stored in the user area (a separate perprocess structure that can be paged out) instead of the process table.
Also, these tables can be implemented in numerous ways—they need not be arrays; one alternate implementation is a linked lists of structures.
Regardless of the implementation details, the general concepts remain the same.
The arrangement of these three tables has existed since the early versions of the UNIX System [Thompson 1978]
This arrangement is critical to the way files are shared among processes.
We’ll return to this figure in later chapters, when we describe additional ways that files are shared.
The v-node was invented to provide support for multiple file system types on a single computer system.
Sun called this the Virtual File System and called the file system–independent portion of the i-node the v-node [Kleiman 1986]
The v-node propagated through various vendor implementations as support for Sun’s Network File System (NFS) was added.
The first release from Berkeley to provide v-nodes was the 4.3BSD Reno release, when NFS was added.
Instead of splitting the data structures into a v-node and an i-node, Linux uses a file system–independent i-node and a file system–dependent i-node.
If two independent processes have the same file open, we could have the arrangement shown in Figure 3.8
Figure 3.8 Two independent processes with the same file open.
Each process that opens the file gets its own file table entry, but only a single v-node table entry is required for a given file.
One reason each process gets its own file table entry is so that each process has its own current offset for the file.
Given these data structures, we now need to be more specific about what happens with certain operations that we’ve already described.
After each write is complete, the current file offset in the file table entry is incremented by the number of bytes written.
If this causes the current file offset to exceed the current file size, the current file size in the i-node table entry is set to the current file offset (for example, the file is extended)
If a file is opened with the O_APPEND flag, a corresponding flag is set in the file status flags of the file table entry.
Each time a write is performed for a file with this append flag set, the current file offset in the file table entry is first set to the current file size from the i-node table entry.
This forces every write to be appended to the current end of file.
If a file is positioned to its current end of file using lseek, all that happens is the current file offset in the file table entry is set to the current file size from the i-node table entry.
The lseek function modifies only the current file offset in the file table entry.
It is possible for more than one file descriptor entry to point to the same file table entry, as we’ll see when we discuss the dup function in Section 3.12
This also happens after a fork when the parent and the child share the same file table entry for each open descriptor (Section 8.3)
Note the difference in scope between the file descriptor flags and the file status flags.
The former apply only to a single descriptor in a single process, whereas the latter apply to all descriptors in any process that point to the given file table entry.
When we describe the fcntl function in Section 3.14, we’ll see how to fetch and modify both the file descriptor flags and the file status flags.
Everything that we’ve described so far in this section works fine for multiple processes that are reading the same file.
Each process has its own file table entry with its own current file offset.
Unexpected results can arise, however, when multiple processes write to the same file.
To see how to avoid some surprises, we need to understand the concept of atomic operations.
Consider a single process that wants to append to the end of a file.
Older versions of the UNIX System didn’t support the O_APPEND option to open, so the program was coded as follows:
This works fine for a single process, but problems arise if multiple processes use this technique to append to the same file.
This scenario can arise if multiple instances of the same program are appending messages to a log file, for example.
Assume that two independent processes, A and B, are appending to the same file.
Each has opened the file but without the O_APPEND flag.
Each process has its own file table entry, but they share a single v-node table entry.
Assume that process A does the lseek and that this sets the current offset for the file for process A to byte offset 1,500 (the current end of file)
Process B then does the lseek, which sets the current offset for the file for process B to byte offset 1,500 also (the current end of file)
Then B calls write, which increments B’s current file offset for the file to 1,600
Because the file’s size has been extended, the kernel also updates the current file size in the v-node to 1,600
When A calls write, the data is written starting at the current file offset for A, which is byte offset 1,500
This overwrites the data that B wrote to the file.
The problem here is that our logical operation of ‘‘position to the end of file and write’’ requires two separate function calls (as we’ve shown it)
The solution is to have the positioning to the current end of file and the write be an atomic operation with regard to other processes.
Any operation that requires more than one function call cannot be atomic, as there is always the possibility that the kernel might temporarily suspend the process between the two function calls (as we assumed previously)
The UNIX System provides an atomic way to do this operation if we set the O_APPEND flag when a file is opened.
As we described in the previous section, this causes the kernel to position the file to its current end of file before each write.
We no longer have to call lseek before each write.
The Single UNIX Specification includes two functions that allow applications to seek and perform I/O atomically: pread and pwrite.
Calling pread is equivalent to calling lseek followed by a call to read, with the following exceptions.
There is no way to interrupt the two operations that occur when we call pread.
Calling pwrite is equivalent to calling lseek followed by a call to write, with similar exceptions.
When both of these options are specified, the open will fail if the file already exists.
We also said that the check for the existence of the file and the creation of the file was performed as an atomic operation.
If we didn’t have this atomic operation, we might try.
The problem occurs if the file is created by another process between the open and the creat.
If the file is created by another process between these two function calls, and if that other process writes something to the file, that data is erased when this creat is executed.
Combining the test for existence and the creation into a single atomic operation avoids this problem.
In general, the term atomic operation refers to an operation that might be composed of multiple steps.
If the operation is performed atomically, either all the steps are performed (on success) or none are performed (on failure)
It must not be possible for only a subset of the steps to be performed.
An existing file descriptor is duplicated by either of the following functions:
The new file descriptor returned by dup is guaranteed to be the lowest-numbered available file descriptor.
The new file descriptor that is returned as the value of the functions shares the same file table entry as the fd argument.
In this figure, we assume that when it’s started, the process executes.
Because both descriptors point to the same file table entry, they share the same file status flags—read, write, append, and so on—and the same current file offset.
Each descriptor has its own set of file descriptor flags.
As we describe in Section 3.14, the close-on-exec file descriptor flag for the new descriptor is always cleared by the dup functions.
Another way to duplicate a descriptor is with the fcntl function, which we describe in Section 3.14
In this last case, the dup2 is not exactly the same as a close followed by an fcntl.
It is possible in the latter case to have a signal catcher called between the close and the fcntl that could modify the file descriptors.
The same problem could occur if a different thread changes the file descriptors.
The fcntl method for duplicating file descriptors appeared with System III and continued with System V.
Traditional implementations of the UNIX System have a buffer cache or page cache in the kernel through which most disk I/O passes.
When we write data to a file, the data is normally copied by the kernel into one of its buffers and queued for writing to disk at some later time.
The kernel eventually writes all the delayed-write blocks to disk, normally when it needs to reuse the buffer for some other disk block.
To ensure consistency of the file system on disk with the contents of the buffer cache, the sync, fsync, and fdatasync functions are provided.
The sync function simply queues all the modified block buffers for writing and returns; it does not wait for the disk writes to take place.
The function sync is normally called periodically (usually every 30 seconds) from a system daemon, often called update.
The function fsync refers only to a single file, specified by the file descriptor fd, and waits for the disk writes to complete before returning.
This function is used when an application, such as a database, needs to be sure that the modified blocks have been written to the disk.
The fdatasync function is similar to fsync, but it affects only the data portions of a file.
All four of the platforms described in this book support sync and fsync.
The fcntl function can change the properties of a file that is already open.
In the examples in this section, the third argument is always an integer, corresponding to the comment in the function prototype just shown.
When we describe record locking in Section 14.3, however, the third argument becomes a pointer to a structure.
Refer to Figure 3.7, as we’ll discuss both the file descriptor flags associated with each file descriptor in the process table entry and the file status flags associated with each file table entry.
The new file descriptor is returned as the value of the function.
It is the lowest-numbered descriptor that is not already open, and that is greater than or equal to the third argument (taken as an integer)
The new descriptor shares the same file table entry as fd.
But the new descriptor has its own set of file descriptor flags, and its FD_CLOEXEC file descriptor flag is cleared.
F_GETFD Return the file descriptor flags for fd as the value of the function.
Currently, only one file descriptor flag is defined: the FD_CLOEXEC flag.
The new flag value is set from the third argument (taken as an integer)
Be aware that some existing programs that deal with the file descriptor flags don’t use the constant FD_CLOEXEC.
F_GETFL Return the file status flags for fd as the value of the function.
We described the file status flags when we described the open function.
Also, these five values are mutually exclusive; a file can have only one of them enabled.
Therefore, we must first use the O_ACCMODE mask to obtain the access-mode bits and then compare the result against any of the five values.
F_SETFL Set the file status flags to the value of the third argument (taken as an integer)
F_GETOWN Get the process ID or process group ID currently receiving the SIGIO and SIGURG signals.
F_SETOWN Set the process ID or process group ID to receive the SIGIO and SIGURG signals.
A negative arg implies a process group ID equal to the absolute value of arg.
The program in Figure 3.11 takes a single command-line argument that specifies a file descriptor and prints a description of selected file flags for that descriptor.
When we modify either the file descriptor flags or the file status flags, we must be careful to fetch the existing flag value, modify it as desired, and then set the new flag value.
Figure 3.12 shows a function that sets one or more of the file status flags for a descriptor.
Figure 3.12 Turn on one or more of the file status flags for a descriptor.
This statement logically ANDs the one’s complement of flags with the current val.
This causes each write to wait for the data to be written to disk before returning.
Normally in the UNIX System, a write only queues the data for writing; the actual disk write operation can take place sometime later.
A database system is a likely candidate for using O_SYNC, so that it knows on return from a write that the data is actually on the disk, in case of an abnormal system failure.
We expect the O_SYNC flag to increase the system and clock times when the program runs.
The results in Figure 3.6 were measured while reading a disk file and writing to /dev/null, so there was no disk output.
The second row in Figure 3.13 corresponds to reading a disk file and writing to another disk file.
This is why the first and second rows in Figure 3.13 are different.
The system time increases when we write to a disk file, because the kernel now copies the data from our process and queues the data for writing by the disk driver.
We expect the clock time to increase as well when we write to a disk file.
When we enable synchronous writes, the system and clock times should increase significantly.
As the third row shows, the system time for writing synchronously is not much more expensive than when we used delayed writes.
This implies that the Linux operating system is doing the same amount of work for delayed and synchronous writes (which is unlikely), or else the O_SYNC flag isn’t having the desired effect.
In this case, the Linux operating system isn’t allowing us to set the O_SYNC flag using fcntl, instead failing without returning an error (but it would have honored the flag if we were able to specify it when the file was opened)
The clock time in the last three rows reflects the extra time needed to wait for all of the writes to be committed to disk.
After writing a file synchronously, we expect that a call to fsync will have no effect.
This case is supposed to be represented by the last.
Note that the times match our expectations: synchronous writes are far more expensive than delayed writes, and using fsync with synchronous writes makes very little difference.
Note also that adding a call to fsync at the end of the delayed writes makes little measurable difference.
It is likely that the operating system flushed previously written data to disk as we were writing new data to the file, so by the time that we called fsync, very little work was left to be done.
Compare fsync and fdatasync, both of which update a file’s contents when we say so, with the O_SYNC flag, which updates a file’s contents every time we write to the file.
The performance of each alternative will depend on many factors, including the underlying operating system implementation, the speed of the disk drive, and the type of the file system.
Our program operates on a descriptor (standard output), never knowing the name of the file that was opened on that descriptor.
We can’t set the O_SYNC flag when the file is opened, since the shell opened the file.
With fcntl, we can modify the properties of a descriptor, knowing only the descriptor for the open file.
We’ll see another need for fcntl when we describe nonblocking pipes (Section 15.2), since all we have with a pipe is a descriptor.
The ioctl function has always been the catchall for I/O operations.
Anything that couldn’t be expressed using one of the other functions in this chapter usually ended up being specified with an ioctl.
Some implementations have even extended it for use with regular files.
This detail doesn’t matter, since the second argument is always a #defined name from a header.
For the ISO C prototype, an ellipsis is used for the remaining arguments.
Normally, however, there is only one more argument, and it’s usually a pointer to a variable or a structure.
In this prototype, we show only the headers required for the function itself.
Each device driver can define its own set of ioctl commands.
The system, however, provides generic ioctl commands for different classes of devices.
Examples of some of the categories for these generic ioctl commands supported in FreeBSD are summarized in Figure 3.15
The mag tape operations allow us to write end-of-file marks on a tape, rewind a tape, space forward over a specified number of files or records, and the like.
None of these operations is easily expressed in terms of the other functions in the chapter (read, write, lseek, and so on), so the easiest way to handle these devices has always been to access their operations using ioctl.
Opening the file /dev/fd/n is equivalent to duplicating descriptor n, assuming that descriptor n is open.
The /dev/fd feature was developed by Tom Duff and appeared in the 8th Edition of the Research UNIX System.
For example, if descriptor 0 was opened read-only, we can only read on fd.
Even if the system ignores the open mode and the call.
It maps file descriptors into symbolic links pointing to the underlying physical files.
When you open /dev/fd/0, for example, you are really opening the file associated with your standard input.
Thus the mode of the new file descriptor returned is unrelated to the mode of the /dev/fd file descriptor.
We can also call creat with a /dev/fd pathname argument as well as specify O_CREAT in a call to open.
This allows a program that calls creat to still work if the pathname argument is /dev/fd/1, for example.
Because the Linux implementation uses symbolic links to the real files, using creat on a /dev/fd file will result in the underlying file being truncated.
The main use of the /dev/fd files is from the shell.
It allows programs that use pathname arguments to handle standard input and standard output in the same manner as other pathnames.
For example, the cat(1) program specifically looks for an input filename of - and uses it to mean standard input.
If /dev/fd is supported, the special handling of - can be removed from cat, and we can enter.
The special meaning of - as a command-line argument to refer to the standard input or the standard output is a kludge that has crept into many programs.
There are also problems if we specify - as the first file, as it looks like the start of another command-line option.
This chapter has described the basic I/O functions provided by the UNIX System.
These are often called the unbuffered I/O functions because each read or write invokes a system call into the kernel.
Using only read and write, we looked at the effect of various I/O sizes on the amount of time required to read a file.
We also looked at several ways to flush written data to disk and their effect on application performance.
Atomic operations were introduced when multiple processes append to the same file and when multiple processes create the same file.
We also looked at the data structures used by the kernel to share information about open files.
We’ll return to these data structures later in the text.
We return to both of these functions later in the book.
In the previous chapter we covered the basic functions that perform I/O.
The discussion centered on I/O for regular files—opening a file, and reading or writing a file.
We’ll now look at additional features of the file system and the properties of a file.
We’ll start with the stat functions and go through each member of the stat structure, looking at all the attributes of a file.
In this process, we’ll also describe each of the functions that modify these attributes: change the owner, change the permissions, and so on.
We’ll also look in more detail at the structure of a UNIX file system and symbolic links.
We finish this chapter with the functions that operate on directories, and we develop a function that descends through a directory hierarchy.
The discussion in this chapter centers on the four stat functions and the information they return.
Given a pathname, the stat function returns a structure of information about the named file.
The fstat function obtains information about the file that is already open on the descriptor fd.
The lstat function is similar to stat, but when the named file is a symbolic link, lstat returns information about the symbolic link, not the file referenced by the symbolic link.
We’ll need lstat in Section 4.22 when we walk down a directory hierarchy.
We describe symbolic links in more detail in Section 4.17
The fstatat function provides a way to return the file statistics for a pathname relative to an open directory represented by the fd argument.
Otherwise, the default is to follow symbolic links, returning information about the file to which the symbolic link points.
If the fd argument has the value AT_FDCWD and the pathname argument is a relative pathname, then fstatat evaluates the pathname argument relative to the current directory.
If the pathname argument is an absolute pathname, then the fd argument is ignored.
In these two cases, fstatat behaves like either stat or lstat, depending on the value of flag.
The buf argument is a pointer to a structure that we must supply.
The definition of the structure can differ among implementations, but it could look like.
They are defined as part of the XSI option in the Single UNIX Specification.
The timespec structure type defines time in terms of seconds and nanoseconds.
The old names can be defined in terms of the tv_sec members for compatibility.
Note that most members of the stat structure are specified by a primitive system data type (see Section 2.8)
We’ll go through each member of this structure to examine the attributes of a file.
The biggest user of the stat functions is probably the ls -l command, to learn all the information about a file.
We’ve talked about two different types of files so far: regular files and directories.
Most files on a UNIX system are either regular files or directories, but there are additional types of files.
The most common type of file, which contains data of some form.
There is no distinction to the UNIX kernel whether this data is text or binary.
Any interpretation of the contents of a regular file is left to the application processing the file.
One notable exception to this is with binary executable files.
To execute a program, the kernel must understand its format.
All binary executable files conform to a format that allows the kernel to identify where to load a program’s text and data.
A file that contains the names of other files and pointers to information on these files.
Any process that has read permission for a directory file can read the contents of the directory, but only the kernel can write directly to a directory file.
Processes must use the functions described in this chapter to make changes to a directory.
A type of file providing buffered I/O access in fixed-size units to devices such as disk drives.
All access to devices is through the character special interface.
A type of file providing unbuffered I/O access in variable-sized units to devices.
All devices on a system are either block special files or character special files.
A type of file used for network communication between processes.
A socket can also be used for non-network communication between processes on a single host.
The type of a file is encoded in the st_mode member of the stat structure.
We can determine the file type with the macros shown in Figure 4.1
The argument to each of these macros is the st_mode member from the stat structure.
POSIX.1 allows implementations to represent interprocess communication (IPC) objects, such as message queues and semaphores, as files.
The macros shown in Figure 4.2 allow us to determine the type of IPC object from the stat structure.
However, none of the various implementations of the UNIX System discussed in this book represent these objects as files.
The program in Figure 4.3 prints the type of file for each command-line argument.
Figure 4.3 Print type of file for each command-line argument.
Here, we have explicitly entered a backslash at the end of the first command line, telling the shell that we want to continue entering the command on another line.
The shell then prompted us with its secondary prompt, >, on the next line.
We have specifically used the lstat function instead of the stat function to detect symbolic links.
If we used the stat function, we would never see symbolic links.
Historically, early versions of the UNIX System didn’t provide the S_ISxxx macros.
Most systems define this mask and the related constants in the file <sys/stat.h>
If we examine this file, we’ll find the S_ISDIR macro defined something like.
We’ve said that regular files are predominant, but it is interesting to see what percentage of the files on a given system are of each file type.
Figure 4.4 shows the counts and percentages for a Linux system that is used as a single-user workstation.
This data was obtained from the program shown in Section 4.22
Every process has six or more IDs associated with it.
Figure 4.5 User IDs and group IDs associated with each process.
The real user ID and real group ID identify who we really are.
These two fields are taken from our entry in the password file when we log in.
Normally, these values don’t change during a login session, although there are ways for a superuser process to change them, which we describe in Section 8.11
The effective user ID, effective group ID, and supplementary group IDs determine our file access permissions, as we describe in the next section.
The saved set-user-ID and saved set-group-ID contain copies of the effective user ID and the effective group ID, respectively, when a program is executed.
We describe the function of these two saved values when we describe the setuid function in Section 8.11
Normally, the effective user ID equals the real user ID, and the effective group ID equals the real group ID.
When we execute a program file, the effective user ID of the process is usually the real user ID, and the effective group ID is usually the real group ID.
These two bits in the file’s mode word are called the set-user-ID bit and the set-group-ID bit.
For example, if the owner of the file is the superuser and if the file’s set-user-ID bit is set, then while that program file is running as a process, it has superuser privileges.
This happens regardless of the real user ID of the process that executes the file.
As an example, the UNIX System program that allows anyone to change his or her password, passwd(1), is a set-user-ID program.
This is required so that the program can write the new password to the password file, typically either /etc/passwd or /etc/shadow, files that should be writable only by the superuser.
Because a process that is running set-user-ID to some other user usually assumes extra permissions, it must be written carefully.
Returning to the stat function, the set-user-ID bit and the set-group-ID bit are contained in the file’s st_mode value.
The st_mode value also encodes the access permission bits for the file.
When we say file, we mean any of the file types that we described earlier.
All the file types — directories, character special files, and so on—have permissions.
Many people think of only regular files as having access permissions.
There are nine permission bits for each file, divided into three categories.
The term user in the first three rows in Figure 4.6 refers to the owner of the file.
The chmod(1) command, which is typically used to modify these nine permission bits, allows us to specify u for user (owner), g for group, and o for other.
Some books refer to these three as owner, group, and world; this is confusing, as the chmod command.
We’ll use the terms user, group, and other, to be consistent with the chmod command.
The three categories in Figure 4.6 — read, write, and execute—are used in various ways by different functions.
We’ll summarize them here, and return to them when we describe the actual functions.
The first rule is that whenever we want to open any type of file by name, we must have execute permission in each directory mentioned in the name, including the current directory, if it is implied.
This is why the execute permission bit for a directory is often called the search bit.
We then need appropriate permission for the file itself, depending on how we’re trying to open it: read-only, read–write, and so on.
If the current directory is /usr/include, then we need execute permission in the current directory to open the file stdio.h.
This is an example of the current directory being implied, not specifically mentioned.
Note that read permission for a directory and execute permission for a directory mean different things.
Read permission lets us read the directory, obtaining a list of all the filenames in the directory.
Execute permission lets us pass through the directory when it is a component of a pathname that we are trying to access.
We need to search the directory to look for a specific filename.
Another example of an implicit directory reference is if the PATH environment variable, described in Section 8.10, specifies a directory that does not have execute permission enabled.
In this case, the shell will never find executable files in that directory.
We must have write permission for a file to specify the O_TRUNC flag in the open function.
We cannot create a new file in a directory unless we have write permission and execute permission in the directory.
To delete an existing file, we need write permission and execute permission in the directory containing the file.
We do not need read permission or write permission for the file itself.
Execute permission for a file must be on if we want to execute the file using any of the seven exec functions (Section 8.10)
The two owner IDs are properties of the file, whereas the two effective IDs and the supplementary group IDs are properties of the process.
If the effective user ID of the process is 0 (the superuser), access is allowed.
This gives the superuser free rein throughout the entire file system.
If the effective user ID of the process equals the owner ID of the file (i.e., the process owns the file), access is allowed if the appropriate user access permission bit is set.
By appropriate access permission bit, we mean that if the process is opening the file for reading, the user-read bit must be on.
If the process is opening the file for writing, the user-write bit must be on.
If the process is executing the file, the user-execute bit must be on.
If the effective group ID of the process or one of the supplementary group IDs of the process equals the group ID of the file, access is allowed if the appropriate group access permission bit is set.
If the appropriate other access permission bit is set, access is allowed.
Note that if the process owns the file (step 2), access is granted or denied based only on the user access permissions; the group permissions are never looked at.
Similarly, if the process does not own the file but belongs to an appropriate group, access is granted or denied based only on the group access permissions; the other permissions are not looked at.
When we described the creation of a new file in Chapter 3 using either open or creat, we never said which values were assigned to the user ID and group ID of the new file.
We’ll see how to create a new directory in Section 4.21 when we describe the mkdir function.
The rules for the ownership of a new directory are identical to the rules in this section for the ownership of a new file.
The user ID of a new file is set to the effective user ID of the process.
POSIX.1 allows an implementation to choose one of the following options to determine the group ID of a new file:
The group ID of a new file can be the effective group ID of the process.
The group ID of a new file can be the group ID of the directory in which the file is being created.
Several Linux file systems allow the choice between the two options to be selected using a mount(1) command option.
If this bit is set, the new file’s group ID is copied from the directory; otherwise, the new file’s group ID is set to the effective group ID of the process.
Using the second option—inheriting the directory’s group ID—assures us that all files and directories created in that directory will have the same group ID as the directory.
This group ownership of files and directories will then propagate down the hierarchy from that point.
This is used in the Linux directory /var/mail, for example.
As we described earlier, when we open a file, the kernel performs its access tests based on the effective user and group IDs.
Sometimes, however, a process wants to test accessibility based on the real user and group IDs.
This is useful when a process is running as someone else, using either the set-user-ID or the set-group-ID feature.
Even though a process might be set-user-ID to root, it might still want to verify that the real user can access a given file.
The access and faccessat functions base their tests on the real user and group IDs.
Replace effective with real in the four steps at the end of Section 4.5
The faccessat function behaves like access when the pathname argument is absolute or when the fd argument has the value AT_FDCWD and the pathname argument is relative.
Otherwise, faccessat evaluates the pathname relative to the open directory referenced by the fd argument.
The flag argument can be used to change the behavior of faccessat.
If the AT_EACCESS flag is set, the access checks are made using the effective user and group IDs of the calling process instead of the real user and group IDs.
In this example, the set-user-ID program can determine that the real user cannot normally read the file, even though the open function will succeed.
In the preceding example and in Chapter 8, we’ll sometimes switch to become the superuser to demonstrate how something works.
If you’re on a multiuser system and do not have superuser permission, you won’t be able to duplicate these examples completely.
Now that we’ve described the nine permission bits associated with every file, we can describe the file mode creation mask that is associated with every process.
The umask function sets the file mode creation mask for the process and returns the previous value.
This is one of the few functions that doesn’t have an error return.
The file mode creation mask is used whenever the process creates a new file or a new directory.
Both accept a mode argument that specifies the new file’s access permission bits.
We describe how to create a new directory in Section 4.21
Any bits that are on in the file mode creation mask are turned off in the file’s mode.
If we run this program, we can see how the permission bits have been set.
Most users of UNIX systems never deal with their umask value.
It is usually set once, on login, by the shell’s start-up file, and never changed.
Nevertheless, when writing programs that create new files, if we want to ensure that specific access permission bits are enabled, we must modify the umask value while the process is running.
Otherwise, the umask value that is in effect when our process is running can cause permission bits to be turned off.
In the preceding example, we use the shell’s umask command to print the file mode creation mask both before we run the program and after it completes.
This shows us that changing the file mode creation mask of a process doesn’t affect the mask of its parent (often a shell)
All of the shells have a built-in umask command that we can use to set or print the current file mode creation mask.
Users can set the umask value to control the default permissions on the files they create.
This value is expressed in octal, with one bit representing one permission to be masked off, as shown in Figure 4.10
The Single UNIX Specification requires that the umask command support a symbolic mode of operation.
Unlike the octal format, the symbolic format specifies which permissions are to be allowed (i.e., clear in the file creation mask) instead of which ones are to be denied (i.e., set in the file creation mask)
The chmod, fchmod, and fchmodat functions allow us to change the file access permissions for an existing file.
The chmod function operates on the specified file, whereas the fchmod function operates on a file that has already been opened.
The fchmodat function behaves like chmod when the pathname argument is absolute or when the fd argument has the value AT_FDCWD and the pathname argument is relative.
Otherwise, fchmodat evaluates the pathname relative to the open directory referenced by the fd argument.
To change the permission bits of a file, the effective user ID of the process must be equal to the owner ID of the file, or the process must have superuser permissions.
The mode is specified as the bitwise OR of the constants shown in Figure 4.11
Recall the final state of the files foo and bar when we ran the program in Figure 4.9 to demonstrate the umask function:
The program shown in Figure 4.12 modifies the mode of these two files.
After running the program in Figure 4.12, we see that the final state of the two files is.
In this example, we have set the permissions of the file bar to an absolute value, regardless of the current permission bits.
For the file foo, we set the permissions relative to their current state.
To do this, we first call stat to obtain the current permissions and then modify them.
We have explicitly turned on the set-group-ID bit and turned off the group-execute bit.
Note that the ls command lists the group-execute permission as S to signify that the set-group-ID bit is set without the group-execute bit being set.
On Solaris, the ls command displays an l instead of an S to indicate that mandatory file and record locking has been enabled for this file.
This behavior applies only to regular files, but we’ll discuss this more in Section 14.3
Finally, note that the time and date listed by the ls command did not change after we ran the program in Figure 4.12
We’ll see in Section 4.19 that the chmod function updates only the time that the i-node was last changed.
By default, the ls -l lists the time when the contents of the file were last modified.
The chmod functions automatically clear two of the permission bits under the following conditions:
On systems, such as Solaris, that place special meaning on the sticky bit when used with regular files, if we try to set the sticky bit (S_ISVTX) on a regular file and do not have superuser privileges, the sticky bit in the mode is automatically turned off.
To prevent malicious users from setting the sticky bit and adversely affecting system performance, only the superuser can set the sticky bit of a regular file.
Although the bit also has no meaning when applied to regular files on FreeBSD, everyone except the superuser is prevented from setting it on a regular file.
The group ID of a newly created file might potentially be a group that the calling process does not belong to.
Recall from Section 4.6 that it’s possible for the group ID of the new file to be the group ID of the parent directory.
Specifically, if the group ID of the new file does not equal either the effective group ID of the process or one of the process’s supplementary group IDs and if the process does not have superuser privileges, then the set-group-ID bit is automatically turned off.
This prevents a user from creating a set-group-ID file owned by a group that the user doesn’t belong to.
FreeBSD 8.0 fails an attempt to set the set-group-ID in this case.
The other systems silently turn the bit off, but don’t fail the attempt to change the file access permissions.
If a process that does not have superuser privileges writes to a file, the set-user-ID and set-group-ID bits are automatically turned off.
If malicious users find a set-group-ID or a set-user-ID file they can write to, even though they can modify the file, they lose the special privileges of the file.
On versions of the UNIX System that predated demand paging, this bit was known as the sticky bit.
If it was set for an executable program file, then the first time the program was executed, a copy of the program’s text was saved in the swap area when the process terminated.
The program would then load into memory more quickly the next time it was executed, because the swap area was handled as a contiguous file, as compared to the possibly random location of data blocks in a normal UNIX file system.
The sticky bit was often set for common application programs, such as the text editor and the passes of the C compiler.
Naturally, there was a limit to the number of sticky files that could be contained in the swap area before running out of swap space, but it was a useful technique.
The name sticky came about because the text portion of the file stuck around in the swap area until the system was rebooted.
Later versions of the UNIX System referred to this as the saved-text bit; hence the constant S_ISVTX.
With today’s newer UNIX systems, most of which have a virtual memory system and a faster file system, the need for this technique has disappeared.
On contemporary systems, the use of the sticky bit has been extended.
The Single UNIX Specification allows the sticky bit to be set for a directory.
If the bit is set for a directory, a file in the directory can be removed or renamed only if the user has write permission for the directory and meets one of the following criteria:
The directories /tmp and /var/tmp are typical candidates for the sticky bit—they are directories in which any user can typically create files.
The permissions for these two directories are often read, write, and execute for everyone (user, group, and other)
But users should not be able to delete or rename files owned by others.
Solaris 10 places special meaning on the sticky bit if it is set on a regular file.
In this case, if none of the execute bits is set, the operating system will not cache the contents of the file.
These four functions operate similarly unless the referenced file is a symbolic link.
The fchown function changes the ownership of the open file referenced by the fd argument.
Since it operates on a file that is already open, it can’t be used to change the ownership of a symbolic link.
The fchownat function behaves like either chown or lchown when the pathname argument is absolute or when the fd argument has the value AT_FDCWD and the pathname argument is relative.
When the fd argument is set to the file descriptor of an open directory and the pathname argument is a relative pathname, fchownat evaluates the pathname relative to the open directory.
Historically, BSD-based systems have enforced the restriction that only the superuser can change the ownership of a file.
This is to prevent users from giving away their files to others, thereby defeating any disk space quota restrictions.
System V, however, has allowed all users to change the ownership of any files they own.
With Solaris 10, this functionality is a configuration option, whose default value is to enforce the restriction.
Also recall that this option can depend on the referenced file; it can be enabled or disabled on a per file system basis.
Only a superuser process can change the user ID of the file.
You can change the group ID of files that you own, but only to groups that you belong to.
If these functions are called by a process other than a superuser process, on successful return, both the set-user-ID and the set-group-ID bits are cleared.
The st_size member of the stat structure contains the size of the file in bytes.
This field is meaningful only for regular files, directories, and symbolic links.
For a regular file, a file size of 0 is allowed.
We’ll get an end-of-file indication on the first read of the file.
For a symbolic link, the file size is the number of bytes in the filename.
For example, in the following case, the file size of 7 is the length of the pathname usr/lib:
The first is the preferred block size for I/O for the file, and the latter is the actual number of 512-byte blocks that are allocated.
Holes are created by seeking past the current end of file and writing some data.
The du command on many BSD-derived systems reports the number of 1,024-byte blocks.
On Linux, the units reported depend on the whether the POSIXLY_CORRECT environment is set.
If we execute the following command, we can see that the normal I/O operations read up through the size of the file:
The wc(1) command with the -c option counts the number of characters (bytes) in the file.
Sometimes we would like to truncate a file by chopping off data at the end of the file.
Emptying a file, which we can do with the O_TRUNC flag to open, is a special case of truncation.
These two functions truncate an existing file to length bytes.
If the previous size of the file was greater than length, the data beyond length is no longer accessible.
Otherwise, if the previous size was less than length, the file size will increase and the data between the old end of file and the new end of file will read as 0 (i.e., a hole is probably created in the file)
Solaris also includes an extension to fcntl (F_FREESP) that allows us to free any part of a file, not just a chunk at the end of the file.
We use ftruncate in the program shown in Figure 13.6 when we need to empty a file after obtaining a lock on the file.
To appreciate the concept of links to a file, we need a conceptual understanding of the structure of the UNIX file system.
Understanding the difference between an i-node and a directory entry that points to an i-node is also useful.
Various implementations of the UNIX file system are in use today.
Solaris, for example, supports several types of disk file systems: the traditional BSD-derived UNIX file system (called UFS), a file system (called PCFS) to read and write DOS-formatted diskettes, and a file system (called HSFS) to read CD file systems.
We saw one difference between file system types in Figure 2.20
Each file system type has its own characteristic features — and some of these features can be confusing.
Thus, if you create one file named file.txt and another named file.TXT, then two distinct files are created.
On Mac OS X, however, the HFS file system is case-preserving with case-insensitive comparisons.
Thus, if you create file.txt, when you try to create file.TXT, you will overwrite file.txt.
However, only the name used when the file was created is stored in the file system (the case-preserving aspect)
In fact, any permutation of uppercase and lowercase letters in the sequence f, i, l, e, ., t, x, t will match when searching for the file (the caseinsensitive comparison aspect)
As a consequence, besides file.txt and file.TXT, we can access the file with the names File.txt, fILE.tXt, and FiLe.TxT.
We can think of a disk drive being divided into one or more partitions.
Each partition can contain a file system, as shown in Figure 4.13
The i-nodes are fixed-length entries that contain most of the information about a file.
If we examine the i-node and data block portion of a cylinder group in more detail, we could have the arrangement shown in Figure 4.14
Figure 4.14 Cylinder group’s i-nodes and data blocks in more detail.
Every i-node has a link count that contains the number of directory entries that point to it.
Only when the link count goes to 0 can the file be deleted (thereby releasing the data blocks associated with the file)
In the stat structure, the link count is contained in the st_nlink member.
The other type of link is called a symbolic link.
With a symbolic link, the actual contents of the file—the data blocks—store the name of the file that the symbolic link points to.
In the following example, the filename in the directory entry is the three-character string lib and the 7 bytes of data in the file are usr/lib:
The file type in the i-node would be S_IFLNK so that the system knows that this is a symbolic link.
The i-node contains all the information about the file: the file type, the file’s access permission bits, the size of the file, pointers to the file’s data blocks, and so on.
Most of the information in the stat structure is obtained from the i-node.
Only two items of interest are stored in the directory entry: the filename and the i-node number.
The other items—the length of the filename and the length of the directory record—are not of interest to this discussion.
Because the i-node number in the directory entry points to an i-node in the same file system, a directory entry can’t refer to an i-node in a different file system.
This is why the ln(1) command (make a new directory entry that points to an existing file) can’t cross file systems.
When renaming a file without changing file systems, the actual contents of the file need not be moved—all that needs to be done is to add a new directory entry that points to the existing i-node and then unlink the old directory entry.
For example, to rename the file /usr/lib/foo to /usr/foo, the contents of the file foo need not be moved if the directories /usr/lib and /usr are on the same file system.
We’ve talked about the concept of a link count for a regular file, but what about the link count field for a directory? Assume that we make a new directory in the working directory, as in.
Note that in this figure, we explicitly show the entries for dot and dot-dot.
Figure 4.15 Sample cylinder group after creating the directory testdir.
The value of 2 comes from the directory entry that names the directory (testdir) and from the entry for dot in that directory.
As we saw in the previous section, a file can have multiple directory entries pointing to its i-node.
We can use either the link function or the linkat function to create a link to an existing file.
These functions create a new directory entry, newpath, that references the existing file existingpath.
With the linkat function, the existing file is specified by both the efd and existingpath arguments, and the new pathname is specified by both the nfd and newpath arguments.
By default, if either pathname is relative, it is evaluated relative to the corresponding file descriptor.
If either file descriptor is set to AT_FDCWD, then the corresponding pathname, if it is a relative pathname, is evaluated relative to the current directory.
If either pathname is absolute, then the corresponding file descriptor argument is ignored.
When the existing file is a symbolic link, the flag argument controls whether the linkat function creates a link to the symbolic link or to the file to which the symbolic link points.
If the AT_SYMLINK_FOLLOW flag is set in the flag argument, then a link is created to the target of the symbolic link.
If this flag is clear, then a link is created to the symbolic link itself.
The creation of the new directory entry and the increment of the link count must be an atomic operation.
Most implementations require that both pathnames be on the same file system, although POSIX.1 allows an implementation to support linking across file systems.
If an implementation supports the creation of hard links to directories, it is restricted to only the superuser.
This constraint exists because such hard links can cause loops in the file system, which most utilities that process the file system aren’t capable of handling.
We show an example of a loop introduced by a symbolic link in Section 4.17
Many file system implementations disallow hard links to directories for this reason.
To remove an existing directory entry, we call the unlink function.
These functions remove the directory entry and decrement the link count of the file referenced by pathname.
If there are other links to the file, the data in the file is still accessible through the other links.
As mentioned earlier, to unlink a file, we must have write permission and execute permission in the directory containing the directory entry, as it is the directory entry that we will be removing.
Also, as mentioned in Section 4.10, if the sticky bit is set in this directory we must have write permission for the directory and meet one of the following criteria:
Only when the link count reaches 0 can the contents of the file be deleted.
One other condition prevents the contents of a file from being deleted: as long as some process has the file open, its contents will not be deleted.
When a file is closed, the kernel first checks the count of the number of processes that have the file open.
If the pathname argument is a relative pathname, then the unlinkat function evaluates the pathname relative to the directory represented by the fd file descriptor argument.
If the fd argument is set to the value AT_FDCWD, then the pathname is evaluated relative to the current working directory of the calling process.
If the pathname argument is an absolute pathname, then the fd argument is ignored.
The flag argument gives callers a way to change the default behavior of the unlinkat function.
When the AT_REMOVEDIR flag is set, then the unlinkat function can be used to remove a directory, similar to using rmdir.
If this flag is clear, then unlinkat operates like unlink.
Example The program shown in Figure 4.16 opens a file and then unlinks it.
The program then goes to sleep for 15 seconds before terminating.
This property of unlink is often used by a program to ensure that a temporary file it creates won’t be left around in case the program crashes.
The process creates a file using either open or creat and then immediately calls unlink.
The file is not deleted, however, because it is still open.
Only when the process either closes the file or terminates, which causes the kernel to close all its open files, is the file deleted.
If pathname is a symbolic link, unlink removes the symbolic link, not the file referenced by the link.
There is no function to remove the file referenced by a symbolic link given the name of the link.
The superuser can call unlink with pathname specifying a directory if the file system supports it, but the function rmdir should be used instead to unlink a directory.
We can also unlink a file or a directory with the remove function.
The name was changed from the historical UNIX name of unlink because most non-UNIX systems that implement the C standard didn’t support the concept of links to a file at the time.
A file or a directory is renamed with either the rename or renameat function.
The rename function is defined by ISO C for files.
POSIX.1 expanded the definition to include directories and symbolic links.
There are several conditions to describe for these functions, depending on whether oldname refers to a file, a directory, or a symbolic link.
We must also describe what happens if newname already exists.
If oldname specifies a file that is not a directory, then we are renaming a file or a symbolic link.
In this case, if newname exists, it cannot refer to a directory.
If newname exists and is not a directory, it is removed, and oldname is renamed to newname.
We must have write permission for the directory containing oldname and the directory containing newname, since we are changing both directories.
If oldname specifies a directory, then we are renaming a directory.
If newname exists, it must refer to a directory, and that directory must be empty.
When we say that a directory is empty, we mean that the only entries in the directory are dot and dot-dot.
If newname exists and is an empty directory, it is removed, and oldname is renamed to newname.
Additionally, when we’re renaming a directory, newname cannot contain a path prefix that names oldname.
For example, we can’t rename /usr/foo to /usr/foo/testdir, because the old name (/usr/foo) is a path prefix of the new name and cannot be removed.
If either oldname or newname refers to a symbolic link, then the link itself is processed, not the file to which it resolves.
More precisely, neither dot nor dot-dot can appear as the last component of oldname or newname.
As a special case, if oldname and newname refer to the same file, the function returns successfully without changing anything.
If newname already exists, we need permissions as if we were deleting it.
Also, because we’re removing the directory entry for oldname and possibly creating a directory entry for newname, we need write permission and execute permission in the directory containing oldname and in the directory containing newname.
The renameat function provides the same functionality as the rename function, except when either oldname or newname refers to a relative pathname.
If oldname specifies a relative pathname, it is evaluated relative to the directory referenced by oldfd.
Similarly, newname is evaluated relative to the directory referenced by newfd if newname specifies a relative pathname.
Either the oldfd or newfd arguments (or both) can be set to AT_FDCWD to evaluate the corresponding pathname relative to the current directory.
A symbolic link is an indirect pointer to a file, unlike the hard links described in the previous section, which pointed directly to the i-node of the file.
Symbolic links were introduced to get around the limitations of hard links.
Hard links normally require that the link and the file reside in the same file system.
Only the superuser can create a hard link to a directory (when supported by the underlying file system)
There are no file system limitations on a symbolic link and what it points to, and anyone can create a symbolic link to a directory.
Symbolic links are typically used to ‘‘move’’ a file or an entire directory hierarchy to another location on a system.
When using functions that refer to a file by name, we always need to know whether the function follows a symbolic link.
If the function follows a symbolic link, a pathname argument to the function refers to the file pointed to by the symbolic link.
Otherwise, a pathname argument refers to the link itself, not the file pointed to by the link.
Figure 4.17 summarizes whether the functions described in this chapter follow a symbolic link.
The functions mkdir, mkfifo, mknod, and rmdir do not appear in this figure, as they return an error when the pathname is a symbolic link.
Also, the functions that take a file descriptor argument, such as fstat and fchmod, are not listed, as the function that returns the file descriptor (usually open) handles the symbolic link.
Historically, implementations have differed in whether chown follows symbolic links.
In all modern systems, however, chown does follow symbolic links.
Initially, chown didn’t follow symbolic links, but this behavior was changed in 4.4BSD.
System V included support for symbolic links in SVR4, but diverged from the original BSD behavior by implementing chown to follow symbolic links.
In older versions of Linux (those before version 2.1.81), chown didn’t follow symbolic links.
All of these platforms provide implementations of lchown to change the ownership of symbolic links themselves.
Figure 4.17 Tr eatment of symbolic links by various functions.
In this case, if the pathname refers to a symbolic link, open will fail with errno set to EEXIST.
This behavior is intended to close a security hole so that privileged processes can’t be fooled into writing to the wrong files.
It is possible to introduce loops into the file system by using symbolic links.
Most functions that look up a pathname return an errno of ELOOP when this occurs.
This creates a directory foo that contains the file a and a symbolic link that points to foo.
We show this arrangement in Figure 4.18, drawing a directory as a circle and a file as a square.
If we write a simple program that uses the standard function ftw(3) on Solaris to descend through a file hierarchy, printing each pathname encountered, the output is.
In Section 4.22, we provide our own version of the ftw function that uses lstat instead of stat, to prevent it from following symbolic links.
Note that on Linux, the ftw and nftw functions record all directories seen and avoid processing a directory more than once, so they don’t display this behavior.
We can unlink the file foo/testdir, as unlink does not follow a symbolic link.
But if we create a hard link that forms a loop of this type, its removal is much more difficult.
This is why the link function will not form a hard link to a directory unless the process has superuser privileges.
Indeed, Rich Stevens did this on his own system as an experiment while writing the original version of this section.
The file system got corrupted and the normal fsck(1) utility couldn’t fix things.
The need for hard links to directories has long since passed.
With symbolic links and the mkdir function, there is no longer any need for users to create hard links to directories.
When we open a file, if the pathname passed to open specifies a symbolic link, open follows the link to the specified file.
If the file pointed to by the symbolic link doesn’t exist, open returns an error saying that it can’t open the file.
This response can confuse users who aren’t familiar with symbolic links.
The file myfile does exist, yet cat says there is no such file, because myfile is a symbolic link and the file pointed to by the symbolic link doesn’t exist.
The -l option to ls gives us two hints: the first character is an l, which means a symbolic link, and the sequence -> also indicates a symbolic link.
The ls command has another option (-F) that appends an at-sign (@) to filenames that are symbolic links, which can help us spot symbolic links in a directory listing without the -l option.
A symbolic link is created with either the symlink or symlinkat function.
A new directory entry, sympath, is created that points to actualpath.
It is not required that actualpath exist when the symbolic link is created.
We saw this in the example at the end of the previous section.
Also, actualpath and sympath need not reside in the same file system.
The symlinkat function is similar to symlink, but the sympath argument is evaluated relative to the directory referenced by the open file descriptor for that directory (specified by the fd argument)
If the sympath argument specifies an absolute pathname or if the fd argument has the special value AT_FDCWD, then symlinkat behaves the same way as symlink.
Because the open function follows a symbolic link, we need a way to open the link itself and read the name in the link.
These functions combine the actions of open, read, and close.
If successful, they return the number of bytes placed into buf.
The contents of the symbolic link that are returned in buf are not null terminated.
The readlinkat function behaves the same way as the readlink function when the pathname argument specifies an absolute pathname or when the fd argument has the special value AT_FDCWD.
However, when the fd argument is a valid file descriptor of an open directory and the pathname argument is a relative pathname, then readlinkat evaluates the pathname relative to the open directory represented by fd.
The actual resolution stored with each file’s attributes depends on the file system implementation.
For file systems that store timestamps in second granularity, the nanoseconds fields will be filled with zeros.
For file systems that store timestamps in a resolution higher than seconds, the partial seconds value will be converted into nanoseconds and returned in the nanoseconds fields.
Figure 4.19 The three time values associated with each file.
The modification time indicates when the contents of the file were last modified.
The changed-status time indicates when the i-node of the file was last modified.
In this chapter, we’ve described many operations that affect the i-node without changing the actual contents of the file: changing the file access permissions, changing the user ID, changing the number of links, and so on.
Because all the information in the i-node is stored separately from the actual contents of the file, we need the changed-status time, in addition to the modification time.
Note that the system does not maintain the last-access time for an i-node.
This is why the functions access and stat, for example, don’t change any of the three times.
The access time is often used by system administrators to delete files that have not been accessed for a certain amount of time.
The classic example is the removal of files named a.out or core that haven’t been accessed in the past week.
The find(1) command is often used for this type of operation.
The modification time and the changed-status time can be used to archive only those files that have had their contents modified or their i-node modified.
The ls command displays or sorts only on one of the three time values.
By default, when invoked with either the -l or the -t option, it uses the modification time of a file.
The -u option causes the ls command to use the access time, and the -c option causes it to use the changed-status time.
Figure 4.20 summarizes the effects of the various functions that we’ve described on these three times.
Recall from Section 4.14 that a directory is simply a file containing directory entries: filenames and associated i-node numbers.
Adding, deleting, or modifying these directory entries can affect the three times associated with that directory.
This is why Figure 4.20 contains one column for the three times associated with the file or directory and another column for the three times associated with the parent directory of the referenced file or directory.
For example, creating a new file affects the directory that contains the new file, and it affects the i-node for the new file.
Reading or writing a file, however, affects only the i-node of the file and has no effect on the directory.
Figure 4.20 Effect of various functions on the access, modification, and changed-status times.
The mkdir and rmdir functions are covered in Section 4.21
The utimes, utimensat, and futimens functions are covered in the next section.
Several functions are available to change the access time and the modification time of a file.
The futimens and utimensat functions provide nanosecond granularity for specifying timestamps, using the timespec structure (the same structure used by the stat family of functions; see Section 4.2)
In both functions, the first element of the times array argument contains the access time, and the second element contains the modification time.
The two time values are calendar times, which count seconds since the Epoch, as described in Section 1.10
In this case, both timestamps are set to the current time.
The times argument points to an array of two timespec structures.
The times argument points to an array of two timespec structures.
The privileges required to execute these functions depend on the value of the times argument.
Merely having write permission for the file is not adequate.
With futimens, you need to open the file to change its times.
The utimensat function provides a way to change a file’s times using the file’s name.
The pathname argument is evaluated relative to the fd argument, which is either a file descriptor of an open directory or the special value AT_FDCWD to force evaluation relative to the current directory of the calling process.
If pathname specifies an absolute pathname, then the fd argument is ignored.
The flag argument to utimensat can be used to further modify the default behavior.
The default behavior is to follow a symbolic link and modify the times of the file to which the link refers.
A third function, utimes, is included in the Single UNIX Specification as part of the XSI option.
The times argument is a pointer to an array of two timestamps—access time and modification time—but they are expressed in seconds and microseconds:
Note that we are unable to specify a value for the changed-status time, st_ctim—the time the i-node was last changed—as this field is automatically updated when the utime function is called.
On some versions of the UNIX System, the touch(1) command uses one of these functions.
To do this, the program first obtains the times with the stat function, truncates the file, and then resets the times with the futimens function.
We can demonstrate the program in Figure 4.21 on Linux with the following commands:
As we would expect, the last-modification times and the last-access times have not changed.
The changed-status times, however, have changed to the time that the program was run.
Directories are created with the mkdir and mkdirat functions, and deleted with the rmdir function.
The specified file access permissions, mode, are modified by the file mode creation mask of the process.
A common mistake is to specify the same mode as for a file: read and write permissions only.
But for a directory, we normally want at least one of the execute bits enabled, to allow access to filenames within the directory.
The user ID and group ID of the new directory are established according to the rules we described in Section 4.6
Files created in the new directory will then inherit the group ID of that directory.
With Linux, the file system implementation determines whether this behavior is supported.
With the Linux implementation of the UFS file system, however, the behavior is not selectable; it inherits the set-group-ID bit to mimic the historical BSD implementation, where the group ID of a directory is inherited from the parent directory.
BSD-based implementations don’t propagate the set-group-ID bit; they simply inherit the group ID as a matter of policy.
On these platforms, newly created files and directories always inherit the group ID of the parent directory, regardless of whether the set-group-ID bit is set.
In the earlier versions, a process had to call the mknod function to create a new directory — but use of the mknod function was restricted to superuser processes.
To circumvent this constraint, the normal command that created a directory, mkdir(1), had to be owned by root with the set-user-ID bit on.
When the fd argument has the special value AT_FDCWD, or when the pathname argument specifies an absolute pathname, mkdirat behaves exactly like mkdir.
Otherwise, the fd argument is an open directory from which relative pathnames will be evaluated.
Recall that an empty directory is one that contains entries only for dot and dot-dot.
If the link count of the directory becomes 0 with this call, and if no other process has the directory open, then the space occupied by the directory is freed.
If one or more processes have the directory open when the link count reaches 0, the last link is removed and the dot and dot-dot entries are removed before this function returns.
Additionally, no new files can be created in the directory.
The directory is not freed, however, until the last process closes it.
Even though some other process has the directory open, it can’t be doing much in the directory, as the directory had to be empty for the rmdir function to succeed.
Directories can be read by anyone who has access permission to read the directory.
But only the kernel can write to a directory, to preserve file system sanity.
Recall from Section 4.5 that the write permission bits and execute permission bits for a directory determine if we can create new files in the directory and remove files from the directory — they don’t specify if we can write to the directory itself.
The actual format of a directory depends on the UNIX System implementation and the design of the file system.
When longer filenames were added to 4.2BSD, each entry became variable length, which means that any program that reads a directory is now system dependent.
To simplify the process of reading a directory, a set of directory routines were developed and are part of POSIX.1
The fdopendir function first appeared in version 4 of the Single UNIX Specification.
It provides a way to convert an open file descriptor into a DIR structure for use by the directory handling functions.
The telldir and seekdir functions are not part of the base POSIX.1 standard.
They are included in the XSI option in the Single UNIX Specification, so all conforming UNIX System implementations are expected to provide them.
Recall our use of several of these functions in the program shown in Figure 1.3, our bare-bones implementation of the ls command.
Implementations define the structure to contain at least the following two members:
Since the filename is null terminated, however, it doesn’t matter how d_name is defined in the header, because the array size doesn’t indicate the length of the filename.
The DIR structure is an internal structure used by these seven functions to maintain information about the directory being read.
The pointer to a DIR structure returned by opendir and fdopendir is then used with the other five functions.
The opendir function initializes things so that the first readdir returns the first entry in the directory.
When the DIR structure is created by fdopendir, the first entry returned by readdir depends on the file offset associated with the file descriptor passed to fdopendir.
Note that the ordering of entries within the directory is implementation dependent and is usually not alphabetical.
Example We’ll use these directory routines to write a program that traverses a file hierarchy.
The goal is to produce a count of the various types of files shown in Figure 4.4
The program shown in Figure 4.22 takes a single argument — the starting pathname—and recursively descends the hierarchy from that point.
Solaris provides a function, ftw(3), that performs the actual traversal of the hierarchy, calling a user-defined function for each file.
The problem with this function is that it calls the stat function for each file, which causes the program to follow symbolic links.
For example, if we start at the root and have a symbolic link named /lib that points to /usr/lib, all the files in the directory /usr/lib are counted twice.
To correct this problem, Solaris provides an additional function, nftw(3), with an option that stops it from following symbolic links.
Although we could use nftw, we’ll write our own simple file walker to show the use of the directory routines.
In SUSv4, nftw is included as part of the XSI option.
In SUSv4, the ftw function has been marked as obsolescent.
BSD-based systems have a different function, fts(3), that provides similar functionality.
If "fullpath" is anything other than a directory, we lstat() it, * call func(), and return.
For a directory, we call ourself * recursively for each name in the directory.
First call func() for the directory, * then process each filename in the directory.
Figure 4.22 Recursively descend a directory hierarchy, counting file types.
To illustrate the ftw and nftw functions, we have provided more generality in this program than needed.
For example, the function myfunc always returns 0, even though the function that calls it is prepared to handle a nonzero return.
For additional information on descending through a file system and using this technique in many standard UNIX System commands—find, ls, tar, and so on — refer to Fowler, Korn, and Vo [1989]
This directory is where the search for all relative pathnames starts (i.e., with all pathnames that do not begin with a slash)
When a user logs in to a UNIX system, the current working directory normally starts at the directory specified by the sixth field in the /etc/passwd file — the user ’s home directory.
The current working directory is an attribute of a process; the home directory is an attribute of a login name.
We can change the current working directory of the calling process by calling the chdir or fchdir function.
We can specify the new current working directory either as a pathname or through an open file descriptor.
Example Because it is an attribute of a process, the current working directory cannot affect processes that invoke the process that executes the chdir.
As a result, the program in Figure 4.23 doesn’t do what we might expect.
The current working directory for the shell that executed the mycd program didn’t change.
This is a side effect of the way that the shell executes programs.
Each program is run in a separate process, so the current working directory of the shell is unaffected by the call to chdir in the program.
For this reason, the chdir function has to be called directly from the shell, so the cd command is built into the shells.
Because the kernel must maintain knowledge of the current working directory, we should be able to fetch its current value.
Unfortunately, the kernel doesn’t maintain the full pathname of the directory.
Instead, the kernel keeps information about the directory, such as a pointer to the directory’s v-node.
Its components are distributed throughout the mount table and the dcache table, and are reassembled, for example, when you read the /proc/self/cwd symbolic link.
What we need is a function that starts at the current working directory (dot) and works its way up the directory hierarchy, using dot-dot to move up one level.
At each level, the function reads the directory entries until it finds the name that corresponds to the i-node of the directory that it just came from.
Repeating this procedure until the root is encountered yields the entire absolute pathname of the current working directory.
Fortunately, a function already exists that does this work for us.
Returns: buf if OK, NULL on error We must pass to this function the address of a buffer, buf, and its size (in bytes)
The buffer must be large enough to accommodate the absolute pathname plus a terminating null byte, or else an error will be returned.
Recall the discussion of allocating space for a maximum-sized pathname in Section 2.5.5
Some older implementations of getcwd allow the first argument buf to be NULL.
In this case, the function calls malloc to allocate size number of bytes dynamically.
This is not part of POSIX.1 or the Single UNIX Specification and should be avoided.
Example The program in Figure 4.24 changes to a specific directory and then calls getcwd to print the working directory.
Note that chdir follows the symbolic link—as we expect it to, from Figure 4.17 — but when it goes up the directory tree, getcwd has no idea when it hits the /var/spool directory that it is pointed to by the symbolic link /usr/spool.
The getcwd function is useful when we have an application that needs to return to the location in the file system where it started out.
We can save the starting location by calling getcwd before we change our working directory.
After we complete our processing, we can pass the pathname obtained from getcwd to chdir to return to our starting location in the file system.
The fchdir function provides us with an easy way to accomplish this task.
Instead of calling getcwd, we can open the current directory and save the file descriptor before we change to a different location in the file system.
When we want to return to where we started, we can simply pass the file descriptor to fchdir.
We’ll need to use these fields in Section 18.9 when we write the ttyname function.
Every file system is known by its major and minor device numbers, which are encoded in the primitive system data type dev_t.
The major number identifies the device driver and sometimes encodes which peripheral board to communicate with; the minor number identifies the specific subdevice.
Recall from Figure 4.13 that a disk drive often contains several file systems.
Each file system on the same disk drive would usually have the same major number, but a different minor number.
We can usually access the major and minor device numbers through two macros defined by most implementations: major and minor.
Consequently, we don’t care how the two numbers are stored in a dev_t object.
The macros major and minor are defined by most implementations.
Which header they are defined in depends on the system.
The st_dev value for every filename on a system is the device number of the file system containing that filename and its corresponding i-node.
Only character special files and block special files have an st_rdev value.
This value contains the device number for the actual device.
The program in Figure 4.25 prints the device number for each command-line argument.
Additionally, if the argument refers to a character special file or a block special file, the st_rdev value for the special file is printed.
Running this program on Linux gives us the following output:
The first two arguments to the program are directories (/ and /home/sar), and the next two are the device names /dev/tty[01]
We use the shell’s regular expression language to shorten the amount of typing we need to do.
The output from the program shows that the root directory has a different device number than does the /home/sar directory, which indicates that they are on different file systems.
We then use ls to look at the two disk devices reported by mount and the two terminal devices.
The two disk devices are block special files, and the two terminal devices are character special files.
Normally, the only types of devices that are block special files are those that can contain random-access file systems—disk drives, floppy disk drives, and CD-ROMs, for example.
Some older UNIX systems supported magnetic tapes for file systems, but this was never widely used.
We’ve covered all the file access permission bits, some of which serve multiple purposes.
Figure 4.26 summarizes these permission bits and their interpretation when applied to a regular file and a directory.
S_ISVTX sticky bit control caching of file contents (if supported)
S_IRUSR user-read user permission to read file user permission to read directory entries.
S_IWUSR user-write user permission to write file user permission to remove and create files in directory.
S_IXUSR user-execute user permission to execute file user permission to search for given pathname in directory.
S_IRGRP group-read group permission to read file group permission to read directory entries.
S_IWGRP group-write group permission to write file group permission to remove and create files in directory.
S_IXGRP group-execute group permission to execute file group permission to search for given pathname in directory.
S_IROTH other-read other permission to read file other permission to read directory entries.
S_IWOTH other-write other permission to write file other permission to remove and create files in directory.
S_IXOTH other-execute other permission to execute file other permission to search for given pathname in directory.
The final nine constants can also be grouped into threes, as follows:
We’ve gone through each member in the stat structure in detail.
This, in turn, led us to examine all the attributes of UNIX files and directories.
We’ve looked at how files and directories might be laid out in a file.
A thorough understanding of all the properties of files and directories and all the functions that operate on them is essential to UNIX programming.
Should we ever see a file size of 0 for a directory or a symbolic link?
If the umask value didn’t change between the creation of the two files, explain how the difference could have occurred.
Modify this routine so that each time it encounters a directory, it uses the chdir function to change to that directory, allowing it to use the filename and not the pathname for each call to lstat.
When all the entries in a directory have been processed, execute chdir("..")
Compare the time used by this version and the version in the text.
This root directory can be changed with the chroot function.
Look up the description for this function in your manuals.
How can the program determine these two times and dates?
These descriptions are usually found in Section 5 of the UNIX Programmer ’s Manual.
How many of the three possible time values are saved for each file? When a file is restored, what value do you think the access time is set to, and why?
Make certain that the length of the absolute pathname of the leaf of this directory is greater than your system’s PATH_MAX limit.
Can you call getcwd to fetch the directory’s pathname? How do the standard UNIX System tools deal with this long pathname? Can you archive the directory using either tar or cpio?
For any user to be able to access these files, their permissions must be rw-rw-rw-
Some programs that create an output file delete the file first, in case it already exists, ignoring the return code:
This library is specified by the ISO C standard because it has been implemented on many operating systems other than the UNIX System.
Additional interfaces are defined as extensions to the ISO C standard by the Single UNIX Specification.
The standard I/O library handles such details as buffer allocation and performing I/O in optimal-sized chunks, obviating our need to worry about using the correct block size (as in Section 3.9)
This makes the library easy to use, but at the same time introduces another set of problems if we’re not cognizant of what’s going on.
It was a major revision of the Portable I/O library written by Mike Lesk.
Surprisingly little has changed in the standard I/O library after more than 35 years.
In Chapter 3, all the I/O routines centered on file descriptors.
When a file is opened, a file descriptor is returned, and that descriptor is then used for all subsequent I/O operations.
With the standard I/O library, the discussion centers on streams.
Do not confuse the standard I/O term stream with the STREAMS I/O system that is part of System V and was standardized in the XSI STREAMS option in the Single UNIX Specification, but is now marked obsolescent in SUSv4
When we open or create a file with the standard I/O library, we say that we have associated a stream with the file.
With the ASCII character set, a single character is represented by a single byte.
With international character sets, a character can be represented by more than one byte.
Standard I/O file streams can be used with both single-byte and multibyte (‘‘wide’’) character sets.
A stream’s orientation determines whether the characters that are read and written are single byte or multibyte.
Initially, when a stream is created, it has no orientation.
If a multibyte I/O function (see <wchar.h>) is used on a stream without orientation, the stream’s orientation is set to wide oriented.
If a byte I/O function is used on a stream without orientation, the stream’s orientation is set to byte oriented.
The freopen function (discussed shortly) will clear a stream’s orientation; the fwide function can be used to set a stream’s orientation.
Returns: positive if stream is wide oriented, negative if stream is byte oriented, or 0 if stream has no orientation.
The fwide function performs different tasks, depending on the value of the mode argument.
If the mode argument is negative, fwide will try to make the specified stream byte oriented.
If the mode argument is positive, fwide will try to make the specified stream wide oriented.
If the mode argument is zero, fwide will not try to set the orientation, but will still return a value identifying the stream’s orientation.
Note that fwide will not change the orientation of a stream that is already oriented.
The only recourse we have is to clear errno before calling fwide and check the value of errno when we return.
Throughout the rest of this book, we will deal only with byte-oriented streams.
When we open a stream, the standard I/O function fopen (Section 5.5) returns a pointer to a FILE object.
This object is normally a structure that contains all the information required by the standard I/O library to manage the stream: the file descriptor used for actual I/O, a pointer to a buffer for the stream, the size of the buffer, a count of the number of characters currently in the buffer, an error flag, and the like.
Application software should never need to examine a FILE object.
To reference the stream, we pass its FILE pointer as an argument to each standard I/O function.
Throughout this text, we’ll refer to a pointer to a FILE object, the type FILE *, as a file pointer.
Throughout this chapter, we describe the standard I/O library in the context of a UNIX system.
As we mentioned, this library has been ported to a wide variety of other operating systems.
To provide some insight about how this library can be implemented, we will talk about its typical implementation on a UNIX system.
Three streams are predefined and automatically available to a process: standard input, standard output, and standard error.
These three standard I/O streams are referenced through the predefined file pointers stdin, stdout, and stderr.
The goal of the buffering provided by the standard I/O library is to use the minimum number of read and write calls.
Recall Figure 3.6, which showed the amount of CPU time required to perform I/O using various buffer sizes.
Also, this library tries to do its buffering automatically for each I/O stream, obviating the need for the application to worry about it.
Unfortunately, the single aspect of the standard I/O library that generates the most confusion is its buffering.
In this case, actual I/O takes place when the standard I/O buffer is filled.
Files residing on disk are normally fully buffered by the standard I/O library.
The buffer used is usually obtained by one of the standard I/O functions calling malloc (Section 7.8) the first time I/O is performed on a stream.
The term flush describes the writing of a standard I/O buffer.
A buffer can be flushed automatically by the standard I/O routines, such as when a buffer fills, or we can call the function fflush to flush a stream.
Unfortunately, in the UNIX environment, flush means two different things.
In terms of the standard I/O library, it means writing out the contents of a buffer, which may be partially filled.
In terms of the terminal driver, such as the tcflush function in Chapter 18, it means to discard the data that’s already stored in a buffer.
In this case, the standard I/O library performs I/O when a newline character is encountered on input or output.
This allows us to output a single character at a time (with the standard I/O fputc function), knowing that actual I/O will take place only when we finish writing each line.
Line buffering is typically used on a stream when it refers to a terminal—standard input and standard output, for example.
First, the size of the buffer that the standard I/O library uses to collect each line is fixed, so I/O might take place if we fill this buffer before writing a newline.
Second, whenever input is requested through the standard I/O library from either (a) an unbuffered stream.
The reason for the qualifier on (b) is that the requested data may already be in the buffer, which doesn’t require data to be read from the kernel.
Obviously, any input from an unbuffered stream, item (a), requires data to be obtained from the kernel.
The standard error stream, for example, is normally unbuffered so that any error messages are displayed as quickly as possible, regardless of whether they contain a newline.
Standard input and standard output are fully buffered, if and only if they do not refer to an interactive device.
This, however, doesn’t tell us whether standard input and standard output are unbuffered or line buffered if they refer to an interactive device and whether standard error should be unbuffered or line buffered.
All other streams are line buffered if they refer to a terminal device; otherwise, they are fully buffered.
The four platforms discussed in this book follow these conventions for standard I/O buffering: standard error is unbuffered, streams open to terminal devices are line buffered, and all other streams are fully buffered.
If we don’t like these defaults for any given stream, we can change the buffering by.
These functions must be called after the stream has been opened (obviously, since each requires a valid file pointer as its first argument) but before any other operation is performed on the stream.
To enable buffering, buf must point to a buffer of length BUFSIZ, a constant defined in <stdio.h>
Normally, the stream is then fully buffered, but some systems may set line buffering if the stream is associated with a terminal device.
With setvbuf, we specify exactly which type of buffering we want.
If we specify an unbuffered stream, the buf and size arguments are ignored.
If we specify fully buffered or line buffered, buf and size can optionally specify a buffer and its size.
If the stream is buffered and buf is NULL, the standard I/O library will automatically allocate its own buffer of the appropriate size for the stream.
By appropriate size, we mean the value specified by the constant BUFSIZ.
As we will see later in this chapter, the GNU C library uses this method.
Figure 5.1 summarizes the actions of these two functions and their various options.
Be aware that if we allocate a standard I/O buffer as an automatic variable within a function, we have to close the stream before returning from the function.
Also, some implementations use part of the buffer for internal bookkeeping, so the actual number of bytes of data that can be stored in the buffer can be less than size.
In general, we should let the system choose the buffer size and automatically allocate the buffer.
When we do this, the standard I/O library automatically releases the buffer when we close the stream.
At any time, we can force a stream to be flushed.
The fflush function causes any unwritten data for the stream to be passed to the kernel.
As a special case, if fp is NULL, fflush causes all output streams to be flushed.
The fopen, freopen, and fdopen functions open a standard I/O stream.
All three return: file pointer if OK, NULL on error.
If the stream previously had an orientation, freopen clears it.
This function is typically used to open a specified file as one of the predefined streams: standard input, standard output, or standard error.
The fdopen function takes an existing file descriptor, which we could obtain from the open, dup, dup2, fcntl, pipe, socket, socketpair, or accept functions, and associates a standard I/O stream with the descriptor.
This function is often used with descriptors that are returned by the functions that create pipes and network communication channels.
Because these special types of files cannot be opened with the standard I/O fopen function, we have to call the device-specific function to obtain a file descriptor, and then associate this descriptor with a standard I/O stream using fdopen.
Both fopen and freopen are part of ISO C; fdopen is part of POSIX.1, since ISO C doesn’t deal with file descriptors.
Figure 5.2 The type argument for opening a standard I/O stream.
Using the character b as part of the type allows the standard I/O system to differentiate between a.
Since the UNIX kernel doesn’t differentiate between these types of files, specifying the character b as part of the type has no effect.
With fdopen, the meanings of the type argument differ slightly.
The descriptor has already been opened, so opening for writing does not truncate the file.
If the descriptor was created by the open function, for example, and the file already existed, the O_TRUNC flag would control whether the file was truncated.
The fdopen function cannot simply truncate any file it opens for writing.
Also, the standard I/O append mode cannot create the file (since the file has to exist if a descriptor refers to it)
When a file is opened with a type of append, each write will take place at the then current end of file.
If multiple processes open the same file with the standard I/O append mode, the data from each process will be correctly written to the file.
These versions do an lseek to the end of file when the stream is opened.
Doing an lseek before each write won’t work either, as we discussed in Section 3.11
When a file is opened for reading and writing (the plus sign in the type), two restrictions apply.
Output cannot be directly followed by input without an intervening fflush, fseek, fsetpos, or rewind.
Input cannot be directly followed by output without an intervening fseek, fsetpos, or rewind, or an input operation that encounters an end of file.
Restriction r w a r+ w+ a+ file must already exist.
Figure 5.3 Six ways to open a standard I/O stream.
POSIX.1 requires implementations to create the file with the following permissions bit set:
Recall from Section 4.8, however, that we can restrict these permissions by adjusting our umask value.
By default, the stream that is opened is fully buffered, unless it refers to a terminal device, in which case it is line buffered.
Any buffered output data is flushed before the file is closed.
If the standard I/O library had automatically allocated a buffer for the stream, that buffer is released.
When a process terminates normally, either by calling the exit function directly or by returning from the main function, all standard I/O streams with unwritten buffered data are flushed and all open standard I/O streams are closed.
Once we open a stream, we can choose from among three types of unformatted I/O:
We can read or write one character at a time, with the standard I/O functions handling all the buffering, if the stream is buffered.
If we want to read or write a line at a time, we use fgets and fputs.
Each line is terminated with a newline character, and we have to specify the maximum line length that we can handle when we call fgets.
This type of I/O is supported by the fread and fwrite functions.
For each I/O operation, we read or write some number of objects, where each object is of a specified size.
These two functions are often used for binary files where we read or write a structure with each operation.
The term direct I/O, from the ISO C standard, is known by many names: binary I/O, object-at-a-time I/O, record-oriented I/O, or structure-oriented I/O.
Don’t confuse this feature with the O_DIRECT open flag supported by FreeBSD and Linux—they are unrelated.
We describe the formatted I/O functions, such as printf and scanf, in Section 5.11
Input Functions Three functions allow us to read one character at a time.
All three return: next character if OK, EOF on end of file or error.
The function getchar is defined to be equivalent to getc(stdin)
The difference between getc and fgetc is that getc can be implemented as a macro, whereas fgetc cannot be implemented as a macro.
The argument to getc should not be an expression with side effects, because it could be evaluated more than once.
Since fgetc is guaranteed to be a function, we can take its address.
This allows us to pass the address of fgetc as an argument to another function.
Calls to fgetc probably take longer than calls to getc, as it usually takes more time to call a function.
Note that these functions return the same value whether an error occurs or the end of file is reached.
To distinguish between the two, we must call either ferror or feof.
Both return: nonzero (true) if condition is true, 0 (false) otherwise.
In most implementations, two flags are maintained for each stream in the FILE object:
After reading from a stream, we can push back characters by calling ungetc.
The characters that are pushed back are returned by subsequent reads on the stream in reverse order of their pushing.
Be aware, however, that although ISO C allows an implementation to support any amount of pushback, an implementation is required to provide only a single character of pushback.
We should not count on more than a single character.
The character that we push back does not have to be the same character that was read.
When we reach the end of file, however, we can push back a character.
The next read will return that character, and the read after that will return EOF.
This works because a successful call to ungetc clears the end-offile indication for the stream.
Pushback is often used when we’re reading an input stream and breaking the input into words or tokens of some form.
Sometimes we need to peek at the next character to determine how to handle the current character.
It’s then easy to push back the character that we peeked at, for the next call to getc to return.
If the standard I/O library didn’t provide this pushback capability, we would have to store the character in a variable of our own, along with a flag telling us to use this character instead of calling getc the next time we need a character.
When we push characters back with ungetc, they are not written back to the underlying file or device.
Instead, they are kept incore in the standard I/O library’s buffer for the stream.
Output functions are available that correspond to each of the input functions we’ve already described.
As with the input functions, putchar(c) is equivalent to putc(c, stdout), and putc can be implemented as a macro, whereas fputc cannot be implemented as a macro.
Line-at-a-time input is provided by the two functions, fgets and gets.
Both return: buf if OK, NULL on end of file or error.
Both specify the address of the buffer to read the line into.
The gets function reads from standard input, whereas fgets reads from the specified stream.
The problem is that it doesn’t allow the caller to specify the buffer size.
This allows the buffer to overflow if the line is longer than the buffer, writing over whatever happens to follow the buffer in memory.
An additional difference with gets is that it doesn’t store the newline in the buffer, as fgets does.
This difference in newline handling between the two functions goes way back in the evolution of the UNIX System.
Even though ISO C requires an implementation to provide gets, you should use fgets instead.
The function fputs writes the null-terminated string to the specified stream.
Note that this need not be line-at-a-time output, since the string need not contain a newline as the last non-null character.
Usually, this is the case — the last non-null character is a newline—but it’s not required.
The puts function writes the null-terminated string to the standard output, without writing the null byte.
But puts then writes a newline character to the standard output.
The puts function is not unsafe, like its counterpart gets.
Nevertheless, we’ll avoid using it, to prevent having to remember whether it appends a newline.
If we always use fgets and fputs, we know that we always have to deal with the newline character at the end of each line.
Using the functions from the previous section, we can get an idea of the efficiency of the standard I/O system.
Figure 5.4 Copy standard input to standard output using getc and putc.
We can make another version of this program that uses fgetc and fputc, which should be functions, not macros.
We don’t show this trivial change to the source code.
Finally, we have a version that reads and writes lines, shown in Figure 5.5
Figure 5.5 Copy standard input to standard output using fgets and fputs.
Instead, we know that the exit function will flush any unwritten data and then close all open streams.
It is interesting to compare the timing of these three programs with the timing data from Figure 3.6
This difference in clock times stems from the difference in user times and the difference in the times spent waiting for I/O to complete, as the system times are comparable.
The system CPU time is about the same as before, because roughly the same number of kernel requests are being made.
One advantage of using the standard I/O routines is that we don’t have to worry about buffering or choosing the optimal I/O size.
We do have to determine the maximum line size for the version that uses fgets, but that’s easier than trying to choose the optimal I/O size.
The final column in Figure 5.6 is the number of bytes of text space—the machine instructions generated by the C compiler—for each of the main functions.
We can see that the version using getc and putc takes the same amount of space as the one using the fgetc and fputc functions.
Usually, getc and putc are implemented as macros, but in the GNU C library implementation the macro simply expands to a function call.
What is happening with this example is that the line-at-a-time functions are implemented using memccpy(3)
Often, the memccpy function is implemented in assembly language instead of C, for efficiency.
System calls are usually much more expensive than ordinary function calls.
As a disclaimer, you should be aware that these timing results are valid only on the single system they were run on.
The results depend on many implementation features that aren’t the same on every UNIX system.
Nevertheless, having a set of numbers such as these, and explaining why the various versions differ, helps us understand the system better.
From this section and Section 3.9, we’ve learned that the standard I/O.
For most nontrivial applications, the largest amount of user CPU time is taken by the application, not by the standard I/O routines.
If we’re doing binary I/O, we often would like to read or write an entire structure at a time.
To do this using getc or putc, we have to loop through the entire structure, one byte at a time, reading or writing each byte.
We can’t use the line-at-a-time functions, since fputs stops writing when it hits a null byte, and there might be null bytes within the structure.
Similarly, fgets won’t work correctly on input if any of the data bytes are nulls or newlines.
Therefore, the following two functions are provided for binary I/O.
Here, we specify size as the size of each element of the array and nobj as the number of elements.
Here, we specify size as the size of structure and nobj as 1 (the number of objects to write)
The obvious generalization of these two cases is to read or write an array of structures.
To do this, size would be the sizeof the structure, and nobj would be the number of elements in the array.
Both fread and fwrite return the number of objects read or written.
For the read case, this number can be less than nobj if an error occurs or if the end of file is encountered.
For the write case, if the return value is less than the requested nobj, an error has occurred.
A fundamental problem with binary I/O is that it can be used to read only data that has been written on the same system.
This was OK many years ago, when all the UNIX systems were PDP-11s, but the norm today is to have heterogeneous systems connected together with networks.
It is common to want to write data on one system and process it on another.
The offset of a member within a structure can differ between compilers and systems because of different alignment requirements.
Indeed, some compilers have an option allowing structures to be packed tightly, to save space with a possible runtime performance penalty, or aligned accurately, to optimize runtime access of each member.
This means that even on a single system, the binary layout of a structure can differ, depending on compiler options.
The binary formats used to store multibyte integers and floating-point values differ among machine architectures.
The real solution for exchanging binary data among different systems is to use an agreed-upon canonical format.
We’ll return to the fread function in Section 8.14 when we use it to read a binary structure, the UNIX process accounting records.
There are three ways to position a standard I/O stream:
They have been around since Version 7, but they assume that a file’s position can be stored in a long integer.
They replace the long integer with the off_t data type.
They use an abstract data type, fpos_t, that records a file’s position.
This data type can be made as big as necessary to record a file’s position.
When porting applications to non-UNIX systems, use fgetpos and fsetpos.
For a binary file, a file’s position indicator is measured in bytes from the beginning of the file.
The value returned by ftell for a binary file is this byte position.
To position a binary file using fseek, we must specify a byte offset and indicate how that offset is interpreted.
Under the UNIX System, however, SEEK_END is supported for binary files.
For text files, the file’s current position may not be measurable as a simple byte offset.
Again, this is mainly under non-UNIX systems that might store text files in a different format.
A stream can also be set to the beginning of the file with the rewind function.
The ftello function is the same as ftell, and the fseeko function is the same as fseek, except that the type of the offset is off_t instead of long.
As we mentioned earlier, the fgetpos and fsetpos functions were introduced by the ISO C standard.
The fgetpos function stores the current value of the file’s position indicator in the object pointed to by pos.
This value can be used in a later call to fsetpos to reposition the stream to that location.
All three return: number of characters output if OK, negative value if output error.
Returns: number of characters stored in array if OK, negative value if encoding error.
Returns: number of characters that would have been stored in array if buffer was large enough, negative value if encoding error.
The printf function writes to the standard output, fprintf writes to the specified stream, dprintf writes to the specified file descriptor, and sprintf places the formatted characters in the array buf.
The sprintf function automatically appends a null byte at the end of the array, but this null byte is not included in the return value.
Note that it’s possible for sprintf to overflow the buffer pointed to by buf.
The caller is responsible for ensuring that the buffer is large enough.
Because buffer overflows can lead to program instability and even security violations, snprintf was introduced.
With it, the size of the buffer is an explicit parameter; any characters that would have been written past the end of the buffer are discarded instead.
The snprintf function returns the number of characters that would have been written to the buffer had it been big enough.
As with sprintf, the return value doesn’t include the terminating null byte.
If snprintf returns a positive value less than the buffer size n, then the output was not truncated.
If an encoding error occurs, snprintf returns a negative value.
Although dprintf doesn’t deal with a file pointer, we include it with the rest of the related functions that handle formatted output.
Note that using dprintf removes the need to call fdopen to convert a file descriptor into a file pointer for use with fprintf.
The format specification controls how the remainder of the arguments will be encoded and ultimately displayed.
Each argument is encoded according to a conversion specification that starts with a percent sign (%)
A conversion specification has four optional components, shown in square brackets below:
The fldwidth component specifies a minimum field width for the conversion.
If the conversion results in fewer characters, it is padded with spaces.
The field width is a non-negative decimal integer or an asterisk.
The precision component specifies the minimum number of digits to appear for integer conversions, the minimum number of digits to appear to the right of the decimal point for floating-point conversions, or the maximum number of bytes for string conversions.
Either the field width or precision (or both) can be an asterisk.
In this case, an integer argument specifies the value to be used.
The argument appears directly before the argument to be converted.
Figure 5.8 The length modifier component of a conversion specification.
With the normal conversion specification, conversions are applied to the arguments in the order they appear after the format argument.
An alternative conversion specification syntax allows the arguments to be named explicitly with the sequence %n$
Figure 5.9 The conversion type component of a conversion specification.
Note, however, that the two syntaxes can’t be mixed in the same format specification.
With the alternative syntax, arguments are numbered starting at one.
If either the field width or precision is to be supplied by an argument, the asterisk syntax is modified to *m$, where m specifies the position of the argument supplying the value.
All three return: number of characters output if OK, negative value if output error.
Returns: number of characters stored in array if OK, negative value if encoding error.
Returns: number of characters that would have been stored in array if buffer was large enough, negative value if encoding error.
We use the vsnprintf function in the error routines in Appendix B.
All three return: number of input items assigned, EOF if input error or end of file before any conversion.
The scanf family is used to parse an input string and convert character sequences into variables of specified types.
The arguments following the format contain the addresses of the variables to initialize with the results of the conversions.
The format specification controls how the arguments are converted for assignment.
The percent sign (%) indicates the beginning of a conversion specification.
Except for the conversion specifications and white space, other characters in the format have to match the input.
If a character doesn’t match, processing stops, leaving the remainder of the input unread.
There are three optional components to a conversion specification, shown in square brackets below:
Input is converted as specified by the rest of the conversion specification, but the result is not stored in an argument.
The fldwidth component specifies the maximum field width in characters.
The lenmodifier component specifies the size of the argument to be initialized with the result of the conversion.
The same length modifiers supported by the printf family of functions are supported by the scanf family of functions (see Figure 5.8 for a list of the length modifiers)
Figure 5.10 The conversion type component of a conversion specification.
In this case, the corresponding argument should be the address of a pointer to which the address of the allocated buffer will be copied.
If the call succeeds, the caller is responsible for freeing the buffer by calling the free function when the buffer is no longer needed.
The scanf family of functions also supports the alternative conversion specification syntax allowing the arguments to be named explicitly: the sequence %n$ represents the nth argument.
With the printf family of functions, the same numbered argument can be referenced in the format string more than once.
In this case, however, the Single UNIX Specification states that the behavior is undefined with the scanf family of functions.
Like the printf family, the scanf family supports functions that use variable argument lists as specified by <stdarg.h>
All three return: number of input items assigned, EOF if input error or end of file before any conversion.
Refer to your UNIX system manual for additional details on the scanf family of functions.
Each standard I/O stream has an associated file descriptor, and we can obtain the descriptor for a stream by calling fileno.
Note that fileno is not part of the ISO C standard, but rather an extension supported by POSIX.1
We need this function if we want to call the dup or fcntl functions, for example.
To look at the implementation of the standard I/O library on your system, start.
This will show how the FILE object is defined, the definitions of the per-stream flags, and any standard I/O routines, such as getc, that are defined as macros.
The implementation of the GNU standard I/O library is also publicly available.
Example The program in Figure 5.11 prints the buffering for the three standard streams and for a stream that is associated with a regular file.
Note that we perform I/O on each stream before printing its buffering status, since the first I/O operation usually causes the buffers to be allocated for a stream.
The structure members and the constants used in this example are defined by the implementations of.
Be aware that implementations of the standard I/O library vary, and programs like this example are nonportable, since they embed knowledge specific to particular implementations.
If we run the program in Figure 5.11 twice, once with the three standard streams connected to the terminal and once with the three standard streams redirected to files, we get the following result:
We can see that the default for this system is to have standard input and standard output line buffered when they’re connected to a terminal.
Note that this doesn’t restrict us to 1,024-byte input and output lines; that’s just the size of the buffer.
Writing a 2,048-byte line to standard output will require two write system calls.
When we redirect these two streams to regular files, they become fully buffered, with buffer sizes equal to the preferred I/O size—the st_blksize value from the stat structure—for the file system.
We also see that the standard error is always unbuffered, as it should be, and that a regular file defaults to fully buffered.
The ISO C standard defines two functions that are provided by the standard I/O library to assist in creating temporary files.
The tmpnam function generates a string that is a valid pathname and that does not match the name of any existing file.
This function generates a different pathname each time it is called, up to TMP_MAX times.
The Single UNIX Specification, however, requires that XSI-conforming systems support a value of at least 10,000
This minimum value allows an implementation to use four digits (0000–9999), although most implementations on UNIX systems use alphanumeric characters.
The tmpnam function is marked obsolescent in SUSv4, but the ISO C standard continues to support it.
If ptr is NULL, the generated pathname is stored in a static area, and a pointer to this area is returned as the value of the function.
Thus, if we call this function more than once and we want to save the pathname, we have to save a copy of the pathname, not a copy of the pointer.
If ptr is not NULL, it is assumed that it points to an array of at least L_tmpnam characters.
The generated pathname is stored in this array, and ptr is returned as the value of the function.
The tmpfile function creates a temporary binary file (type wb+) that is automatically removed when it is closed or on program termination.
Under the UNIX System, it makes no difference that this file is a binary file.
Example The program in Figure 5.12 demonstrates these two functions.
If we execute the program in Figure 5.12, we get.
The standard technique often used by the tmpfile function is to create a unique pathname by calling tmpnam, then create the file, and immediately unlink it.
Recall from Section 4.15 that unlinking a file does not delete its contents until the file is closed.
This way, when the file is closed, either explicitly or on program termination, the contents of the file are deleted.
The Single UNIX Specification defines two additional functions as part of the XSI option for dealing with temporary files: mkdtemp and mkstemp.
Older versions of the Single UNIX Specification defined the tempnam function as a way to create a temporary file in a caller-specified location.
The mkdtemp function creates a directory with a unique name, and the mkstemp function creates a regular file with a unique name.
This string is a pathname whose last six characters are set to XXXXXX.
The function replaces these placeholders with different characters to create a unique pathname.
If successful, these functions modify the template string to reflect the name of the temporary file.
Note that the file mode creation mask of the calling process can restrict these permissions further.
If directory creation is successful, mkdtemp returns the name of the new directory.
The mkstemp function creates a regular file with a unique name and opens it.
The file descriptor returned by mkstemp is open for reading and writing.
Unlike tmpfile, the temporary file created by mkstemp is not removed automatically for us.
If we want to remove it from the file system namespace, we need to unlink it ourselves.
Use of tmpnam and tempnam does have at least one drawback: a window exists between the time that the unique pathname is returned and the time that an application creates a file with that name.
During this timing window, another process can create a file of the same name.
The tmpfile and mkstemp functions should be used instead, as they don’t suffer from this problem.
The program in Figure 5.13 shows how to use (and how not to use) the mkstemp function.
If we execute the program in Figure 5.13, we get.
The difference in behavior comes from the way the two template strings are declared.
For the first template, the name is allocated on the stack, because we use an array variable.
In this case, only the memory for the pointer itself resides on the stack; the compiler arranges for the string to be stored in the read-only segment of the executable.
When the mkstemp function tries to modify the string, a segmentation fault occurs.
We’ve also seen that we can provide our own buffer for the library to use by calling setbuf or setvbuf.
These are standard I/O streams for which there are no underlying files, although they are still accessed with FILE pointers.
All I/O is done by transferring bytes to and from buffers in main memory.
As we shall see, even though these streams look like file streams, several features make them more suited for manipulating character strings.
The fmemopen function allows the caller to provide a buffer to be used for the memory stream: the buf argument points to the beginning of the buffer and the size argument specifies the size of the buffer in bytes.
If the buf argument is null, then the fmemopen function allocates a buffer of size bytes.
In this case, the buffer will be freed when the stream is closed.
The type argument controls how the stream can be used.
The possible values for type are summarized in Figure 5.14
Figure 5.14 The type argument for opening a memory stream.
Note that these values correspond to the ones for file-based standard I/O streams, but there are some subtle differences.
If the buffer contains no null bytes, then the current position is set to one byte past the end of the buffer.
When a stream is not opened for append, the current position is set to the beginning of the buffer.
Because the append mode determines the end of the data by the first null byte, memory streams aren’t well suited for storing binary data (which might contain null bytes before the end of the data)
Second, if the buf argument is a null pointer, it makes no sense to open the stream for only reading or only writing.
Because the buffer is allocated by fmemopen in this case, there is no way to find the buffer ’s address, so to open the stream only for writing means we could never read what we’ve written.
Similarly, to open the stream only for reading means we can only read the contents of a buffer into which we can never write.
Third, a null byte is written at the current position in the stream whenever we increase the amount of data in the stream’s buffer and call fclose, fflush, fseek, fseeko, or fsetpos.
It’s instructive to look at how writes to a memory stream operate on a buffer we provide.
Figure 5.15 shows a sample program that seeds the buffer with a known pattern to see how writes to the stream behave.
When we run the program on Linux, we get the following:
This example shows the policy for flushing memory streams and appending null bytes.
A null byte is appended automatically whenever we write to a memory stream and advance the stream’s notion of the size of the stream’s contents (as opposed to the size of the buffer, which is fixed)
The size of the stream’s contents is determined by how much we write to it.
Of the four platforms covered in this book, only Linux 3.2.0 provides support for memory streams.
This is a case of the implementations not having caught up yet with the latest standards, and will change with time.
We can’t specify our own buffer, but we can get access to the buffer ’s address and size through the bufp and sizep arguments, respectively.
We need to free the buffer ourselves after closing the stream.
The buffer will grow as we add bytes to the stream.
We must follow some rules, however, regarding the use of the buffer address and its length.
First, the buffer address and length are only valid after a call to fclose or fflush.
Second, these values are only valid until the next write to the stream or a call to fclose.
Because the buffer can grow, it may need to be reallocated.
If this happens, then we will find that the value of the buffer ’s memory address will change the next time we call fclose or fflush.
Memory streams are well suited for creating strings, because they prevent buffer overflows.
They can also provide a performance boost for functions that take standard I/O stream arguments used for temporary files, because memory streams access only main memory instead of a file stored on disk.
Korn and Vo [1991] list numerous defects — some in the basic design, but most in the various implementations.
One inefficiency inherent in the standard I/O library is the amount of data copying that takes place.
When we use the line-at-a-time functions, fgets and fputs, the data is usually copied twice: once between the kernel and the standard I/O buffer (when the corresponding read or write is issued) and again between the standard I/O buffer and our line buffer.
Korn and Vo [1991] describe another replacement for the standard I/O library: sfio.
This package is similar in speed to the fio library and normally faster than the standard I/O library.
The sfio package also provides some new features that aren’t found in most other packages: I/O streams generalized to represent both files and regions of memory, processing modules that can be written and stacked on an I/O stream to change the operation of a stream, and better exception handling.
As with the sfio package, ASI attempts to minimize the amount of data copying by using pointers.
Several implementations of the standard I/O library are available in C libraries that were designed for systems with small memory footprints, such as embedded systems.
These implementations emphasize modest memory requirements over portability, speed, or functionality.
Two such implementations are the uClibc C library (see http://www.uclibc.org for more information) and the Newlib C library (http://sources.redhat.com/newlib)
The standard I/O library is used by most UNIX applications.
In this chapter, we looked at many of the functions provided by this library, as well as at some implementation details and efficiency considerations.
Be aware of the buffering that takes place with this library, as this is the area that generates the most problems and confusion.
What happens if you copy lines that exceed this length? Explain what is happening.
A UNIX system requires numerous data files for normal operation: the password file /etc/passwd and the group file /etc/group are two files that are frequently used by various programs.
For example, the password file is used every time a user logs in to a UNIX system and every time someone executes an ls -l command.
Historically, these data files have been ASCII text files and were read with the standard I/O library.
But for larger systems, a sequential scan through the password file becomes time consuming.
We want to be able to store these data files in a format other than ASCII text, but still provide an interface for an application program that works with any file format.
The portable interfaces to these data files are the subject of this chapter.
We also cover the system identification functions and the time and date functions.
These fields are contained in a passwd structure that is defined in <pwd.h>
Historically, the password file has been stored in /etc/passwd and has been an ASCII file.
Each line contains the fields described in Figure 6.1, separated by colons.
For example, four lines from the /etc/passwd file on Linux could be.
There is usually an entry with the user name root.
This entry has a user ID of 0 (the superuser)
The encrypted password field contains a single character as a placeholder where older versions of the UNIX System used to store the encrypted password.
Because it is a security hole to store the encrypted password in a file that is readable by everyone, encrypted passwords are now kept elsewhere.
We’ll cover this issue in more detail in the next section when we discuss passwords.
Some fields in a password file entry can be empty.
If the encrypted password field is empty, it usually means that the user does not have a password.
The entry for squid has one blank field: the comment field.
The shell field contains the name of the executable program to be used as the login shell for the user.
The default value for an empty shell field is usually /bin/sh.
Note, however, that the entry for squid has /dev/null as the login shell.
Obviously, this is a device and cannot be executed, so its use here is to prevent anyone from logging in to our system as user squid.
Many services have separate user IDs for the daemon processes (Chapter 13) that help implement the service.
The squid entry is for the processes implementing the squid proxy cache service.
There are several alternatives to using /dev/null to prevent a particular user from logging in to a system.
For example, /bin/false is often used as the login shell.
It simply exits with an unsuccessful (nonzero) status; the shell evaluates the exit status as false.
It is also common to see /bin/true used to disable an account; it simply exits with a successful (zero) status.
Some systems provide the nologin command, which prints a customizable error message and exits with a nonzero exit status.
The only files that this user ID and group ID can access are those that are readable or writable by the world.
Some systems that provide the finger(1) command support additional information in the comment field.
Each of these fields is separated by a comma: the user’s name, office location, office phone number, and home phone number.
Additionally, an ampersand in the comment field is replaced with the login name (capitalized) by some utilities.
Then we could use finger to print information about Steve Rago.
Even if your system doesn’t support the finger command, these fields can still go into the comment field, since that field is simply a comment and not interpreted by system utilities.
Some systems provide the vipw command to allow administrators to edit the password file.
The vipw command serializes changes to the password file and makes sure that any additional files are consistent with the changes made.
It is also common for systems to provide similar functionality through graphical user interfaces.
POSIX.1 defines two functions to fetch entries from the password file.
These functions allow us to look up an entry given a user’s login name or numerical user ID.
The getpwuid function is used by the ls(1) program to map the numerical user ID contained in an i-node into a user’s login name.
The getpwnam function is used by the login(1) program when we enter our login name.
Both functions return a pointer to a passwd structure that the functions fill in.
This structure is usually a static variable within the function, so its contents are overwritten each time we call either of these functions.
These two POSIX.1 functions are fine if we want to look up either a login name or a user ID, but some programs need to go through the entire password file.
Three functions can be used for this purpose: getpwent, setpwent, and endpwent.
These three functions are not part of the base POSIX.1 standard.
They are defined as part of the XSI option in the Single UNIX Specification.
As such, all UNIX systems are expected to provide them.
We call getpwent to return the next entry in the password file.
As with the two POSIX.1 functions, getpwent returns a pointer to a structure that it has filled in.
This structure is normally overwritten each time we call this function.
If this is the first call to this function, it opens whatever files it uses.
There is no order implied when we use this function; the entries can be in any order, because some systems use a hashed version of the file /etc/passwd.
The function setpwent rewinds whatever files it uses, and endpwent closes these files.
When using getpwent, we must always be sure to close these files by calling endpwent when we’re through.
Although getpwent is smart enough to know when it has to open its files (the first time we call it), it never knows when we’re through.
The call to setpwent at the beginning of this function is self-defense: we ensure that the files are rewound, in case the caller has already opened them by calling getpwent.
We call endpwent when we’re done, because neither getpwnam nor getpwuid should leave any of the files open.
The encrypted password is a copy of the user’s password that has been put through a one-way encryption algorithm.
Because this algorithm is one-way, we can’t guess the original password from the encrypted version.
The more characters used to store the encrypted password, the more combinations there are, and the harder it will be to guess the password by trying all possible variations.
When we place a single character in the encrypted password field, we ensure that an encrypted password will never match this value.
Given an encrypted password, we can’t apply an algorithm that inverts it and returns the plaintext password.
The plaintext password is what we enter at the Password: prompt.
But we could guess a password, run it through the one-way algorithm, and compare the result to the encrypted password.
If user passwords were randomly chosen, this brute-force approach wouldn’t be too successful.
Users, however, tend to choose nonrandom passwords, such as spouse’s name, street names, or pet names.
A common experiment is for someone to obtain a copy of the password file and try guessing the passwords.
To make it more difficult to obtain the raw materials (the encrypted passwords), systems now store the encrypted password in another file, often called the shadow password file.
Minimally, this file has to contain the user name and the encrypted password.
Other information relating to the password is also stored here (Figure 6.3)
The only two mandatory fields are the user’s login name and encrypted password.
The other fields control how often the password is to change — known as ‘‘password aging’’—and how long an account is allowed to remain active.
The shadow password file should not be readable by the world.
With shadow passwords, the regular password file, /etc/passwd, can be left readable by the world.
These fields are contained in a group structure that is defined in <grp.h>
The field gr_mem is an array of pointers to the user names that belong to this group.
We can look up either a group name or a numerical group ID with the following two functions, which are defined by POSIX.1
Like the password file functions, both of these functions normally return pointers to a static variable, which is overwritten on each call.
If we want to search the entire group file, we need some additional functions.
The following three functions are like their counterparts for the password file.
Returns: pointer if OK, NULL on error or end of file.
These three functions are not part of the base POSIX.1 standard.
They are defined as part of the XSI option in the Single UNIX Specification.
The setgrent function opens the group file, if it’s not already open, and rewinds it.
The getgrent function reads the next entry from the group file, opening the file first, if it’s not already open.
The use of groups in the UNIX System has changed over time.
With Version 7, each user belonged to a single group at any point in time.
When we logged in, we were assigned the real group ID corresponding to the numerical group ID in our password file entry.
We could change this at any point by executing newgrp(1)
If the newgrp command succeeded (refer to the manual page for the permission rules), our real group ID was changed to the new group’s ID, and this value was used for all subsequent file access permission checks.
We could always go back to our original group by executing newgrp without any arguments.
With 4.2BSD, the concept of supplementary group IDs was introduced.
Not only did we belong to the group corresponding to the group ID in our password file entry, but we could also belong to as many as 16 additional groups.
The file access permission checks were modified so that in addition to comparing the the file’s group ID to the process effective group ID, it was also compared to all the supplementary group IDs.
The advantage of using supplementary group IDs is that we no longer have to change groups explicitly.
It is not uncommon to belong to multiple groups (i.e., participate in multiple projects) at the same time.
Three functions are provided to fetch and set the supplementary group IDs.
Of these three functions, only getgroups is specified by POSIX.1
Because setgroups and initgroups are privileged operations, they are not part of POSIX.1
All four platforms covered in this book support all three functions, but on Mac OS X 10.6.8, basegid is declared to be of type int.
The getgroups function fills in the array grouplist with the supplementary group IDs.
The number of supplementary group IDs stored in the array is returned by the function.
As a special case, if gidsetsize is 0, the function returns only the number of supplementary group IDs.
This allows the caller to determine the size of the grouplist array to allocate.
The setgroups function can be called by the superuser to set the supplementary group ID list for the calling process: grouplist contains the array of group IDs, and ngroups specifies the number of elements in the array.
The setgroups function is usually called from the initgroups function, which reads the entire group file—with the functions getgrent, setgrent, and endgrent, which we described earlier—and determines the group membership for username.
It then calls setgroups to initialize the supplementary group ID list for the user.
One must be superuser to call initgroups, since it calls setgroups.
In addition to finding all the groups that username is a member of in the group file, initgroups includes basegid in the supplementary group ID list; basegid is the group ID from the password file for username.
The initgroups function is called by only a few programs.
The login(1) program, for example, calls it when we log in.
We’ve already discussed the shadow password file supported by Linux and Solaris.
Figure 6.5 summarizes how the four platforms covered in this book store user and group information.
Special commands are used to edit it, which in turn generate a copy of /etc/passwd from the shadow password file.
In multiuser mode—during normal operation—the Directory Services daemon provides access to account information for users and groups.
Although Linux and Solaris support similar shadow password interfaces, there are some subtle differences.
For example, the integer fields shown in Figure 6.3 are defined as type int on Solaris, but as long int on Linux.
Another difference is the account-inactive field: Solaris defines it to be the number of days since the user last logged in to the system after which the account will be automatically disabled, whereas Linux defines it to be the number of days after the maximum password age has been reached during which the password will still be accepted.
On many systems, the user and group databases are implemented using the Network Information Service (NIS)
This allows administrators to edit a master copy of the databases and distribute them automatically to all servers in an organization.
Client systems contact servers to look up information about users and groups.
We’ve discussed only two of the system’s data files so far: the password file and the group file.
Numerous other files are used by UNIX systems in normal day-to-day operation.
For example, the BSD networking software has one data file for the services provided by the various network servers (/etc/services), one for the protocols (/etc/protocols), and one for the networks (/etc/networks)
Fortunately, the interfaces to these various files are like the ones we’ve already described for the password and group files.
The general principle is that every data file has at least three functions:
A get function that reads the next record, opening the file if necessary.
A null pointer is returned when the end of file is reached.
Most of the get functions return a pointer to a static structure, so we always have to copy the structure if we want to save it.
A set function that opens the file, if not already open, and rewinds the file.
We use this function when we know we want to start again at the beginning of the file.
As we mentioned earlier, we always have to call this function when we’re done, to close all the files.
Additionally, if the data file supports some form of keyed lookup, routines are provided to search for a record with a specific key.
For example, two keyed lookup routines are provided for the password file: getpwnam looks for a record with a specific user name, and getpwuid looks for a record with a specific user ID.
Figure 6.6 shows some of these routines, which are common to UNIX systems.
In this figure, we show the functions for the password files and group file, which we discussed earlier in this chapter, and some of the networking functions.
There are get, set, and end functions for all the data files in this figure.
Under Solaris, the last four data files in Figure 6.6 are symbolic links to files of the same name in the directory /etc/inet.
Most UNIX System implementations have additional functions that are like these, but the additional functions tend to deal with system administration files and are specific to each implementation.
Two data files provided with most UNIX systems are the utmp file, which keeps track of all the users currently logged in, and the wtmp file, which keeps track of all logins and logouts.
With Version 7, one type of record was written to both files, a binary record consisting of the following structure:
On login, one of these structures was filled in and written to the utmp file by the login program, and the same structure was appended to the wtmp file.
On logout, the entry in the utmp file was erased—filled with null bytes—by the init process, and a new entry was appended to the wtmp file.
This logout entry in the wtmp file had the ut_name field zeroed out.
Special entries were appended to the wtmp file to indicate when the system was rebooted and right before and after the system’s time and date was changed.
The who(1) program read the utmp file and printed its contents in a readable form.
Later versions of the UNIX System provided the last(1) command, which read through the wtmp file and printed selected entries.
Most versions of the UNIX System still provide the utmp and wtmp files, but as expected, the amount of information in these files has grown.
The detailed format of these records in Solaris is given in the utmpx(4) manual page.
With Solaris 10, both files are in the /var/adm directory.
Solaris provides numerous functions described in getutxent(3) to read and write these two files.
The pathnames of these two files are /var/run/utmp and /var/log/wtmp.
On Mac OS X 10.6.8, the utmp and wtmp files do not exist.
As of Mac OS X 10.5, the information found in the wtmp file can be obtained from the system logging facility, and the utmpx file contains information about the active login sessions.
POSIX.1 defines the uname function to return information on the current host and operating system.
We pass the address of a utsname structure to this function, and the function then fills it in.
POSIX.1 defines only the minimum fields in the structure, which are all character arrays, and it’s up to each implementation to set the size of each array.
The maximum name lengths, including the terminating null byte, supported by the four platforms discussed in this book are listed in Figure 6.7
The information in the utsname structure can usually be printed with the uname(1) command.
POSIX.1 warns that the nodename element may not be adequate to reference the host on a communications network.
This function is from System V, and in older days, the nodename element was adequate for referencing the host on a UUCP network.
Realize also that the information in this structure does not give any information on the POSIX.1 level.
Finally, this function gives us a way only to fetch the information in the structure; there is nothing specified by POSIX.1 about initializing this information.
Historically, BSD-derived systems provided the gethostname function to return only the name of the host.
This name is usually the name of the host on a TCP/IP network.
The namelen argument specifies the size of the name buffer.
If enough space is provided, the string returned through name is null terminated.
If insufficient room is provided, however, it is unspecified whether the string is null terminated.
Figure 6.7 summarizes the maximum name lengths supported by the four implementations covered in this book.
If the host is connected to a TCP/IP network, the host name is normally the fully qualified domain name of the host.
There is also a hostname(1) command that can fetch or set the host name.
The host name is set by the superuser using a similar function, sethostname.
The host name is normally set at bootstrap time from one of the start-up files invoked by /etc/rc or init.
These calendar times represent both the time and the date.
The UNIX System has always differed from other operating systems in (a) keeping time in UTC instead of the local time, (b) automatically handling conversions, such as daylight saving time, and (c) keeping the time and date as a single quantity.
The time value is always returned as the value of the function.
If the argument is nonnull, the time value is also stored at the location pointed to by calptr.
The real-time extensions to POSIX.1 added support for multiple system clocks.
In Version 4 of the Single UNIX Specification, the interfaces used to control these clocks were moved from an option group to the base.
The clock_gettime function can be used to get the time of the specified clock.
The time is returned in a timespec structure, introduced in Section 4.2, which expresses time values in terms of seconds and nanoseconds.
We can use the clock_getres function to determine the resolution of a given system clock.
We need the appropriate privileges to change a clock’s time.
Version 4 of the Single UNIX Specification specifies that the gettimeofday function is now obsolescent.
However, a lot of programs still use it, because it provides greater resolution (up to a microsecond) than the time function.
The only legal value for tzp is NULL; other values result in unspecified behavior.
Some platforms support the specification of a time zone through the use of tzp, but this is implementation specific and not defined by the Single UNIX Specification.
The gettimeofday function stores the current time as measured from the Epoch in the memory pointed to by tp.
This time is represented as a timeval structure, which stores seconds and microseconds.
Once we have the integer value that counts the number of seconds since the Epoch, we normally call a function to convert it to a broken-down time structure, and then call another function to generate a human-readable time and date.
Figure 6.9 shows the relationships between the various time functions.
The three functions in this figure that are shown with dashed lines—localtime, mktime, and strftime—are all affected by the TZ environment variable, which we describe later in this section.
The dotted lines show how the calendar time is obtained from time-related structures.
The two functions localtime and gmtime convert a calendar time into what’s called a broken-down time, a tm structure.
The reason that the seconds can be greater than 59 is to allow for a leap second.
Note that all the fields except the day of the month are 0-based.
The daylight saving time flag is positive if daylight saving time is in effect, 0 if it’s not in effect, and negative if the information isn’t available.
In older versions of the Single UNIX Specification, double leap seconds were allowed.
The formal definition of UTC doesn’t allow for double leap seconds, so the valid range for seconds is now 0–60
The difference between localtime and gmtime is that the first converts the calendar time to the local time, taking into account the local time zone and daylight saving time flag, whereas the latter converts the calendar time into a broken-down time expressed as UTC.
The function mktime takes a broken-down time, expressed as a local time, and converts it into a time_t value.
The strftime function is a printf-like function for time values.
It is complicated by the multitude of arguments available to customize the string it produces.
Both return: number of characters stored in array if room, 0 otherwise.
However, these functions are now marked obsolescent, because they are susceptible to buffer overflow problems.
The strftime function uses the locale specified by the TZ environment variable.
The tmptr argument is the time value to format, specified by a pointer to a brokendown time value.
The formatted result is stored in the array buf whose size is maxsize characters.
If the size of the result, including the terminating null, fits in the buffer, these functions return the number of characters stored in buf, excluding the terminating null.
The format argument controls the formatting of the time value.
Like the printf functions, conversion specifiers are given as a percent sign followed by a special character.
All other characters in the format string are copied to the output.
Two percent signs in a row generate a single percent sign in the output.
The only specifiers that are not self-evident are %U, %V, and %W.
Otherwise, it is treated as the last week of the previous year.
In both cases, Monday is treated as the first day of the week.
As with printf, strftime supports modifiers for some of the conversion specifiers.
The E and O modifiers can be used to generate an alternative format if one is supported by the locale.
Some systems support additional, nonstandard extensions to the format string for strftime.
Figure 6.11 shows how to use several of the time functions discussed in this chapter.
In particular, it shows how strftime can be used to print a string containing the current date and time.
Recall the relationship of the various time functions shown in Figure 6.9
Before we can print the time in a human-readable format, we need to get the time and convert it into a broken-down time structure.
It takes a string and converts it into a broken-down time.
Returns: pointer to one character past last character parsed, NULL otherwise.
The format argument describes the format of the string in the buffer pointed to by the buf argument.
The format specification is similar, although it differs slightly from the specification for the strftime function.
The conversion specifiers for the strptime function are summarized in Figure 6.12
We mentioned that the three functions in Figure 6.9 with dashed lines were affected by the TZ environment variable: localtime, mktime, and strftime.
If defined, the value of this environment variable is used by these functions instead of the default time.
If the variable is defined to be a null string, such as TZ=, then UTC is normally used.
Refer to the Environment Variables chapter of the Single UNIX Specification [Open Group 2010] for all the details on the TZ variable.
More information on the TZ environment variable can be found in the tzset(3) manual page.
The password file and the group file are used on all UNIX systems.
We’ve looked at the various functions that read these files.
We’ve also talked about shadow passwords, which can enhance system security.
Supplementary group IDs provide a way to participate in multiple groups at the same time.
We also looked at how similar functions are provided by most systems to access other system-related data files.
We discussed the POSIX.1 functions that programs can use to identify the system on which they are running.
We finished the chapter by looking at the time and date functions provided by ISO C and the Single UNIX Specification.
Compare the output to the output from the uname(1) command.
Set the TZ environment variable to different values and see what happens.
Before looking at the process control primitives in the next chapter, we need to examine the environment of a single process.
In this chapter, we’ll see how the main function is called when the program is executed, how command-line arguments are passed to the new program, what the typical memory layout looks like, how to allocate additional memory, how the process can use environment variables, and various ways for the process to terminate.
Additionally, we’ll look at the longjmp and setjmp functions and their interaction with the stack.
We finish the chapter by examining the resource limits of a process.
A C program starts execution with a function called main.
When a C program is executed by the kernel—by one of the exec functions, which we describe in Section 8.10—a special start-up routine is called before the main function is called.
The executable program file specifies this routine as the starting address for the program; this is set up by the link editor when it is invoked by the C compiler.
This start-up routine takes values from the kernel—the command-line arguments and the environment — and sets things up so that the main function is called as shown earlier.
Return of the last thread from its start routine (Section 11.5)
The start-up routine that we mentioned in the previous section is also written so that if the main function returns, the exit function is called.
If the start-up routine were coded in C (it is often coded in assembly language) the call to main could look like.
We’ll discuss the effect of these three functions on other processes, such as the children and the parent of the terminating process, in Section 8.5
Historically, the exit function has always performed a clean shutdown of the standard I/O library: the fclose function is called for all open streams.
Recall from Section 5.5 that this causes all buffered output data to be flushed (written to the file)
All three exit functions expect a single integer argument, which we call the exit status.
Most UNIX System shells provide a way to examine the exit status of a process.
If (a) any of these functions is called without an exit status, (b) main does a return without a return value, or (c) the main function is not declared to return an integer, the exit status of the process is undefined.
This behavior is new with the 1999 version of the ISO C standard.
Historically, the exit status was undefined if the end of the main function was reached without an explicit return statement or a call to the exit function.
Returning an integer value from the main function is equivalent to calling exit with the same value.
When we compile and run the program in Figure 7.1, we see that the exit code is random.
If we compile the same program on different systems, we are likely to get different exit codes, depending on the contents of the stack and register contents at the time that the main function returns:
Now if we enable the 1999 ISO C compiler extensions, we see that the exit code changes:
Note the compiler warning when we enable the 1999 ISO C extensions.
This warning is printed because the type of the main function is not explicitly declared to be an integer.
If we were to add this declaration, the message would go away.
However, if we were to enable all recommended warnings from the compiler (with the -Wall flag), then we would see a warning message something like ‘‘control reaches end of nonvoid function.’’
The declaration of main as returning an integer and the use of exit instead of return produces needless warnings from some compilers and the lint(1) program.
The problem is that these compilers don’t know that an exit from main is the same as a return.
One way around these warnings, which become annoying after a while, is to use return instead of exit from main.
But doing this prevents us from using the UNIX System’s grep utility to locate all calls to exit from a program.
Another solution is to declare main as returning void, instead of int, and continue calling exit.
This gets rid of the compiler warning but doesn’t look right (especially in a programming text), and can generate other compiler warnings, since the return type of main is supposed to be a signed integer.
In this text, we show main as returning an integer, since that is the definition specified by both ISO C and POSIX.1
Note that the GNU C compiler usually doesn’t emit these extraneous compiler warnings unless additional warning options are used.
In the next chapter, we’ll see how any process can cause a program to be executed, wait for the process to complete, and then fetch its exit status.
With ISO C, a process can register at least 32 functions that are automatically called by exit.
These are called exit handlers and are registered by calling the atexit function.
This declaration says that we pass the address of a function as the argument to atexit.
When this function is called, it is not passed any arguments and is not expected to return a value.
The exit function calls these functions in reverse order of their registration.
Each function is called as many times as it was registered.
The sysconf function can be used to determine the maximum number of exit handlers supported by a given platform, as illustrated in Figure 2.14
With ISO C and POSIX.1, exit first calls the exit handlers and then closes (via fclose) all open streams.
POSIX.1 extends the ISO C standard by specifying that any exit handlers installed will be cleared if the program calls any of the exec family of functions.
Figure 7.2 summarizes how a C program is started and the various ways it can terminate.
Figure 7.2 How a C program is started and how it terminates.
The only way a program can be executed by the kernel is if one of the exec functions is called.
A process can also be involuntarily terminated by a signal (not shown in Figure 7.2)
The program in Figure 7.3 demonstrates the use of the atexit function.
An exit handler is called once for each time it is registered.
In Figure 7.3, the first exit handler is registered twice, so it is called two times.
Note that we don’t call exit; instead, we return from main.
When a program is executed, the process that does the exec can pass command-line arguments to the new program.
This is part of the normal operation of the UNIX system shells.
We have already seen this in many of the examples from earlier chapters.
The program in Figure 7.4 echoes all its command-line arguments to standard output.
Note that the normal echo(1) program doesn’t echo the zeroth argument.
We are guaranteed by both ISO C and POSIX.1 that argv[argc] is a null pointer.
Like the argument list, the environment list is an array of character pointers, with each pointer containing the address of a null-terminated C string.
The address of the array of pointers is contained in the global variable environ:
For example, if the environment consisted of five strings, it could look like Figure 7.5
Here we explicitly show the null bytes at the end of each string.
Most predefined names are entirely uppercase, but this is only a convention.
Historically, most UNIX systems have provided a third argument to the main function that is the address of the environment list:
Because ISO C specifies that the main function be written with two arguments, and because this third argument provides no benefit over the global variable environ, POSIX.1 specifies that environ should be used instead of the (possible) third argument.
Access to specific environment variables is normally through the getenv and putenv functions, described in Section 7.9, instead of through the environ variable.
But to go through the entire environment, the environ pointer must be used.
Historically, a C program has been composed of the following pieces:
Text segment, consisting of the machine instructions that the CPU executes.
Usually, the text segment is sharable so that only a single copy needs to be in memory for frequently executed programs, such as text editors, the C compiler, the shells, and so on.
Also, the text segment is often read-only, to prevent a program from accidentally modifying its instructions.
Initialized data segment, usually called simply the data segment, containing variables that are specifically initialized in the program.
Stack, where automatic variables are stored, along with information that is saved each time a function is called.
Each time a function is called, the address of where to return to and certain information about the caller’s environment, such as some of the machine registers, are saved on the stack.
The newly called function then allocates room on the stack for its automatic and temporary variables.
Each time a recursive function calls itself, a new stack frame is used, so one set of variables doesn’t interfere with the variables from another instance of the function.
Historically, the heap has been located between the uninitialized data and the stack.
This is a logical picture of how a program looks; there is no requirement that a given implementation arrange its memory in this fashion.
The stack grows from higher-numbered addresses to lower-numbered addresses on this particular architecture.
The unused virtual address space between the top of the heap and the top of the stack is large.
Several more segment types exist in an a.out, containing the symbol table, debugging information, linkage tables for dynamic shared libraries, and the like.
These additional sections don’t get loaded as part of the program’s image executed by a process.
The only portions of the program that need to be saved in the program file are the text segment and the initialized data.
The size(1) command reports the sizes (in bytes) of the text, data, and bss segments.
The fourth and fifth columns are the total of the three sizes, displayed in decimal and hexadecimal, respectively.
Arnold [1986] describes an early implementation under System V, and Gingell et al.
Shared libraries remove the common library routines from the executable file, instead maintaining a single copy of the library routine somewhere in memory that all processes reference.
This reduces the size of each executable file but may add some runtime overhead, either when the program is first executed or the first time each shared library function is called.
Another advantage of shared libraries is that library functions can be replaced with new versions without having to relink edit every program that uses the library (assuming that the number and type of arguments haven’t changed)
Different systems provide different ways for a program to say that it wants to use or not use the shared libraries.
If we compile this program to use shared libraries, the text and data sizes of the executable file are greatly decreased:
When the size increases, it may involve moving the previously allocated area somewhere else, to provide the additional room at the end.
Also, when the size increases, the initial value of the space between the old contents and the end of the new area is indeterminate.
All three return: non-null pointer if OK, NULL on error.
The pointer returned by the three allocation functions is guaranteed to be suitably aligned so that it can be used for any data object.
For example, if the most restrictive alignment requirement on a particular system requires that doubles must start at memory locations that are multiples of 8, then all pointers returned by these three functions would be so aligned.
The default return value for undeclared functions is int, so using a cast without the proper function declaration could hide an error on systems where the size of type int differs from the size of a function’s return value (a pointer in this case)
The function free causes the space pointed to by ptr to be deallocated.
This freed space is usually put into a pool of available memory and can be allocated in a later call to one of the three alloc functions.
The realloc function lets us change the size of a previously allocated area.
The most common usage is to increase an area’s size.
For example, if we allocate room for 512 elements in an array that we fill in at runtime but later find that we need more room, we can call realloc.
If there is room beyond the end of the existing region for the requested space, then realloc simply allocates this additional area at the end and returns the same pointer that we passed it.
But if there isn’t room, realloc allocates another area that is large enough, copies the existing 512-element array to the new area, frees the old area, and returns the pointer to the new area.
Because the area may move, we shouldn’t have any pointers into this area.
Figure 17.27 shows an example that uses realloc to avoid arrays with fixed, compile-time sizes.
Note that the final argument to realloc is the new size of the region, not the difference between the old and new sizes.
As a special case, if ptr is a null pointer, realloc behaves like malloc and allocates a region of the specified newsize.
Older versions of these routines allowed us to realloc a block that we had freed since the last call to malloc, realloc, or calloc.
This trick dates back to Version 7 and exploited the search strategy of malloc to perform storage compaction.
Solaris still supports this feature, but many other platforms do not.
The allocation routines are usually implemented with the sbrk(2) system call.
This system call expands (or contracts) the heap of the process.
Although sbrk can expand or contract the memory of a process, most versions of malloc and free never decrease their memory size.
The space that we free is available for a later allocation, but the freed space is not usually returned to the kernel; instead, that space is kept in the malloc pool.
Most implementations allocate more space than requested and use the additional space for record keeping — the size of the block, a pointer to the next allocated block, and the like.
As a consequence, writing past the end or before the start of an allocated area could overwrite this record-keeping information in another block.
These types of errors are often catastrophic, but difficult to find, because the error may not show up until much later.
Writing past the end or before the beginning of a dynamically allocated buffer can corrupt more than internal record-keeping information.
The memory before and after a dynamically allocated buffer can potentially be used for other dynamically allocated.
These objects can be unrelated to the code corrupting them, making it even more difficult to find the source of the corruption.
Other possible errors that can be fatal are freeing a block that was already freed and calling free with a pointer that was not obtained from one of the three alloc functions.
If a process calls malloc but forgets to call free, its memory usage will continually increase; this is called leakage.
If we do not call free to return unused space, the size of a process’s address space will slowly increase until no free space is left.
During this time, performance can degrade from excess paging overhead.
Because memory allocation errors are difficult to track down, some systems provide versions of these functions that do additional error checking every time one of the three alloc functions or free is called.
These versions of the functions are often specified by including a special library for the link editor.
There are also publicly available sources that you can compile with special flags to enable additional runtime checking.
FreeBSD, Mac OS X, and Linux support additional debugging through the setting of environment variables.
In addition, options can be passed to the FreeBSD library through the symbolic link /etc/malloc.conf.
Alternate Memory Allocators Many replacements for malloc and free are available.
Some systems already include libraries providing alternative memory allocator implementations.
Other systems provide only the standard allocator, leaving it up to software developers to download alternatives, if desired.
SVR4-based systems, such as Solaris, include the libmalloc library, which provides a set of interfaces matching the ISO C memory allocation functions.
The libmalloc library includes mallopt, a function that allows a process to set certain variables that control the operation of the storage allocator.
A function called mallinfo is also available to provide statistics on the memory allocator.
Vo [1996] describes a memory allocator that allows processes to allocate memory using different techniques for different regions of memory.
In addition to the functions specific to vmalloc, the library provides emulations of the ISO C memory allocation functions.
Historically, the standard malloc algorithm used either a best-fit or a first-fit memory allocation strategy.
Quick-fit is faster than either, but tends to use more memory.
Weinstock and Wulf [1988] describe the algorithm, which is based on splitting up memory into buffers of various sizes and maintaining unused buffers on different free lists, depending on the buffer sizes.
It was designed to scale well when used with multithreaded applications running on multiprocessor systems.
TCMalloc TCMalloc was designed as a replacement for the malloc family of functions to provide high performance, scalability, and memory efficiency.
It uses thread-local caches to avoid locking overhead when allocating buffers from and releasing buffers to the cache.
It also has a heap checker and a heap profiler built in to aid in debugging and analyzing dynamic memory usage.
The TCMalloc library is available as open source from Google.
The function alloca has the same calling sequence as malloc; however, instead of allocating memory from the heap, the memory is allocated from the stack frame of the current function.
The advantage is that we don’t have to free the space; it goes away automatically when the function returns.
The alloca function increases the size of the stack frame.
The disadvantage is that some systems can’t support alloca, if it’s impossible to increase the size of the stack frame after the function has been called.
Nevertheless, many software packages use it, and implementations exist for a wide variety of systems.
All four platforms discussed in this text provide the alloca function.
As we mentioned earlier, the environment strings are usually of the form.
The UNIX kernel never looks at these strings; their interpretation is up to the various applications.
Some, such as HOME and USER, are set automatically at login; others are left for us to set.
We normally set environment variables in a shell start-up file to control the shell’s actions.
If we set the environment variable MAILPATH, for example, it tells the Bourne shell, GNU Bourne-again shell, and Korn shell where to look for mail.
Returns: pointer to value associated with name, NULL if not found.
Note that this function returns a pointer to the value of a name=value string.
We should always use getenv to fetch a specific value from the environment, instead of accessing environ directly.
Some environment variables are defined by POSIX.1 in the Single UNIX Specification, whereas others are defined only if the XSI option is supported.
Figure 7.7 lists the environment variables defined by the Single UNIX Specification and notes which implementations support the variables.
In addition to fetching the value of an environment variable, sometimes we may want to set an environment variable.
We may want to change the value of an existing variable or add a new variable to the environment.
In the next chapter, we’ll see that we can affect the environment of only the current process and any child processes that we invoke.
We cannot affect the environment of the parent process, which is often a shell.
Nevertheless, it is still useful to be able to modify the environment list.
Figure 7.8 shows the functions that are supported by the various standards and implementations.
The clearenv function is not part of the Single UNIX Specification.
It is used to remove all entries from the environment list.
The prototypes for the middle three functions listed in Figure 7.8 are.
The putenv function takes a string of the form name=value and places it in the environment list.
If name already exists, its old definition is first removed.
If name already exists in the environment, then (a) if rewrite is nonzero, the existing definition for name is first removed; or (b) if rewrite is 0, an existing definition for name is not removed, name is not set to the new value, and no error occurs.
It is not an error if such a definition does not exist.
Whereas setenv must allocate memory to create the name=value string from its arguments, putenv is free to place the string passed to it directly into the environment.
Indeed, many implementations do exactly this, so it would be an error to pass putenv a string allocated on the stack, since the memory would be reused after we return from the current function.
It is interesting to examine how these functions must operate when modifying the environment list.
Recall Figure 7.6: the environment list—the array of pointers to the actual name=value strings — and the environment strings are typically stored at the top of a process’s memory space, above the stack.
Deleting a string is simple; we just find the pointer in the environment list and move all subsequent pointers down one.
But adding a string or modifying an existing string is more difficult.
The space at the top of the stack cannot be expanded, because it is often at the top of the address space of the.
If the size of the new value is less than or equal to the size of the existing value, we can just copy the new string over the old string.
If the size of the new value is larger than the old one, however, we must malloc to obtain room for the new string, copy the new string to this area, and then replace the old pointer in the environment list for name with the pointer to this allocated area.
First, we have to call malloc to allocate room for the name=value string and copy the string to this area.
Then, if it’s the first time we’ve added a new name, we have to call malloc to obtain room for a new list of pointers.
We copy the old environment list to this new area and store a pointer to the name=value string at the end of this list of pointers.
We also store a null pointer at the end of this list, of course.
Finally, we set environ to point to this new list of pointers.
Note from Figure 7.6 that if the original environment list was contained above the top of the stack, as is common, then we have moved this list of pointers to the heap.
But most of the pointers in this list still point to name=value strings above the top of the stack.
If this isn’t the first time we’ve added new strings to the environment list, then we know that we’ve already allocated room for the list on the heap, so we just call realloc to allocate room for one more pointer.
The pointer to the new name=value string is stored at the end of the list (on top of the previous null pointer), followed by a null pointer.
In C, we can’t goto a label that’s in another function.
Instead, we must use the setjmp and longjmp functions to perform this type of branching.
As we’ll see, these two functions are useful for handling error conditions that occur in a deeply nested function call.
It consists of a main loop that reads lines from standard input and calls the function do_line to process each line.
This function then calls get_token to fetch the next token from the input line.
The first token of a line is assumed to be a command of some form, and a switch statement selects each command.
For the single command shown, the function cmd_add is called.
The skeleton in Figure 7.9 is typical for programs that read commands, determine the command type, and then call functions to process each command.
Storage for the automatic variables is within the stack frame for each function.
As we’ve said, this type of arrangement of the stack is typical, but not required.
Stacks do not have to grow toward lower memory addresses.
On systems that don’t have built-in hardware support for stacks, a C implementation might use a linked list for its stack frames.
The coding problem that’s often encountered with programs like the one shown in Figure 7.9 is how to handle nonfatal errors.
For example, if the cmd_add function encounters an error — say, an invalid number—it might want to print an error message, ignore the rest of the input line, and return to the main function to read the next input line.
But when we’re deeply nested numerous levels down from the main function, this is difficult to do in C.
In this example, the cmd_add function is only two levels down from main, but it’s not uncommon to be five or more levels down from the point to which we want to return.
It becomes messy if we have to code each function with a special return value that tells it to return one level.
The solution to this problem is to use a nonlocal goto: the setjmp and longjmp functions.
The adjective ‘‘nonlocal’’ indicates that we’re not doing a normal C goto statement within a function; instead, we’re branching back through the call frames to a function that is in the call path of the current function.
Returns: 0 if called directly, nonzero if returning from a call to longjmp.
We call setjmp from the location that we want to return to, which in this example is in the main function.
In this case, setjmp returns 0 because we called it directly.
In the call to setjmp, the argument env is of the special type jmp_buf.
This data type is some form of array that is capable of holding all the information required to restore the status of the stack to the state when we call longjmp.
Normally, the env variable is a global variable, since we’ll need to reference it from another function.
When we encounter an error — say, in the cmd_add function — we call longjmp with two arguments.
The first is the same env that we used in a call to setjmp, and the second, val, is a nonzero value that becomes the return value from setjmp.
The second argument allows us to use more than one longjmp for each setjmp.
Calling longjmp causes the setjmp in main to return, but this time it returns with a value of 1 (the second argument for longjmp)
We’ve seen what the stack looks like after calling longjmp.
If you have an automatic variable that you don’t want rolled back, define it with the volatile attribute.
Variables that are declared as global or static are left alone when longjmp is executed.
The program in Figure 7.13 demonstrates the different behavior that can be seen with automatic, global, register, static, and volatile variables after calling longjmp.
Figure 7.13 Effect of longjmp on various types of variables.
If we compile and test the program in Figure 7.13, with and without compiler optimizations, the results are different:
Note that the optimizations don’t affect the global, static, and volatile variables; their values after the longjmp are the last values that they assumed.
The setjmp(3) manual page on one system states that variables stored in memory will have values as of the time of the longjmp, whereas variables in the CPU and floating-point registers are restored to their values when setjmp was called.
This is indeed what we see when we run the program in Figure 7.13
Without optimization, all five variables are stored in memory (the register hint is ignored for regival)
When we enable optimization, both autoval and regival go into registers, even though the former wasn’t declared register, and the volatile variable stays in memory.
The important thing to realize with this example is that you must use the volatile attribute if you’re writing portable code that uses nonlocal jumps.
Anything else can change from one system to the next.
Some printf format strings in Figure 7.13 are longer than will fit comfortably for display in a programming text.
Instead of making multiple calls to printf, we rely on ISO C’s string concatenation feature, where the sequence.
We’ll return to these two functions, setjmp and longjmp, in Chapter 10 when we discuss signal handlers and their signal versions: sigsetjmp and siglongjmp.
Having looked at the way stack frames are usually handled, it is worth looking at a potential error in dealing with automatic variables.
The basic rule is that an automatic variable can never be referenced after the function that declared it returns.
Numerous warnings about this can be found throughout the UNIX System manuals.
The problem is that when open_data returns, the space it used on the stack will be used by the stack frame for the next function that is called.
But the standard I/O library will still be using that portion of memory for its stream buffer.
To correct this problem, the array databuf needs to be allocated from global memory, either statically (static or extern) or dynamically (one of the alloc functions)
Every process has a set of resource limits, some of which can be queried and changed by the getrlimit and setrlimit functions.
These two functions are defined in the XSI option in the Single UNIX Specification.
The resource limits for a process are normally established by process 0 when the system is initialized and then inherited by each successive process.
Each implementation has its own way of tuning the various limits.
Each call to these two functions specifies a single resource and a pointer to the following structure:
A process can change its soft limit to a value less than or equal to its hard limit.
A process can lower its hard limit to a value greater than or equal to its soft limit.
This lowering of the hard limit is irreversible for normal users.
The resource argument takes on one of the following values.
RLIMIT_AS The maximum size in bytes of a process’s total available memory.
RLIMIT_CORE The maximum size in bytes of a core file.
A limit of 0 prevents the creation of a core file.
When the soft limit is exceeded, the SIGXCPU signal is sent to the process.
RLIMIT_FSIZE The maximum size in bytes of a file that may be created.
When the soft limit is exceeded, the process is sent the SIGXFSZ signal.
RLIMIT_MSGQUEUE The maximum amount of memory in bytes that a process can allocate for POSIX message queues.
RLIMIT_NPROC The maximum number of child processes per real user ID.
If available physical memory is low, the kernel takes memory from processes that exceed their RSS.
RLIMIT_SBSIZE The maximum size in bytes of socket buffers that a user can consume at any given time.
RLIMIT_SIGPENDING The maximum number of signals that can be queued for a process.
This limit is enforced by the sigqueue function (Section 10.20)
RLIMIT_SWAP The maximum amount of swap space in bytes that a user can.
The resource limits affect the calling process and are inherited by any of its children.
This means that the setting of resource limits needs to be built into the shells to affect all our future processes.
Indeed, the Bourne shell, the GNU Bourne-again shell, and the Korn shell have the built-in ulimit command, and the C shell has the built-in limit command.
The umask and chdir functions also have to be handled as shell built-ins.
The program in Figure 7.16 prints out the current soft limit and hard limit for all the resource limits supported on the system.
Note that some systems define rlim_t to be an unsigned long long instead of an unsigned long.
This definition can even change on the same system, depending on whether we compile the program to support 64-bit files.
Some limits apply to file size, so the rlim_t type has to be large enough to represent a file size limit.
To avoid compiler warnings that use the wrong format specification, we first copy the limit into a 64-bit integer so that we have to deal with only one format.
Note that we’ve used the ISO C string-creation operator (#) in the doit macro, to generate the string value for each resource name.
Running this program under FreeBSD gives us the following output:
Exercise 10.11 continues the discussion of resource limits, after we’ve covered signals.
Understanding the environment of a C program within a UNIX system’s environment is a prerequisite to understanding the process control features of the UNIX System.
In this chapter, we’ve looked at how a process is started, how it can terminate, and how it’s passed an argument list and an environment.
Although both the argument list and the environment are uninterpreted by the kernel, it is the kernel that passes both from the caller of exec to the new process.
We’ve also examined the typical memory layout of a C program and seen how a process can dynamically allocate and free memory.
It is worthwhile to look in detail at the functions available for manipulating the environment, since they involve memory allocation.
The functions setjmp and longjmp were presented, providing a way to perform nonlocal branching within a process.
We finished the chapter by describing the resource limits that various implementations provide.
We now turn to the process control provided by the UNIX System.
This includes the creation of new processes, program execution, and process termination.
We also look at the various IDs that are the property of the process — real, effective, and saved; user and group IDs—and how they’re affected by the process control primitives.
We conclude the chapter by looking at the process accounting provided by most UNIX systems.
This lets us look at the process control functions from a different perspective.
Every process has a unique process ID, a non-negative integer.
Because the process ID is the only well-known identifier of a process that is always unique, it is often used as a piece of other identifiers, to guarantee uniqueness.
For example, applications sometimes include the process ID as part of a filename in an attempt to generate unique filenames.
Most UNIX systems implement algorithms to delay reuse, however, so that newly created processes are assigned IDs different from those used by processes that terminated recently.
This prevents a new process from being mistaken for the previous process to have used the same ID.
There are some special processes, but the details differ from implementation to implementation.
Process ID 0 is usually the scheduler process and is often known as the swapper.
No program on disk corresponds to this process, which is part of the.
Process ID 1 is usually the init process and is invoked by the kernel at the end of the bootstrap procedure.
The program file for this process was /etc/init in older versions of the UNIX System and is /sbin/init in newer versions.
This process is responsible for bringing up a UNIX system after the kernel has been bootstrapped.
It is a normal user process, not a system process within the kernel, like the swapper, although it does run with superuser privileges.
Later in this chapter, we’ll see how init becomes the parent process of any orphaned child process.
In Mac OS X 10.4, the init process was replaced with the launchd process, which performs the same set of tasks as init, but has expanded functionality.
Each UNIX System implementation has its own set of kernel processes that provide operating system services.
For example, on some virtual memory implementations of the UNIX System, process ID 2 is the pagedaemon.
This process is responsible for supporting the paging of the virtual memory system.
In addition to the process ID, there are other identifiers for every process.
Note that none of these functions has an error return.
We’ll return to the parent process ID in the next section when we discuss the fork function.
The real and effective user and group IDs were discussed in Section 4.4
An existing process can create a new one by calling the fork function.
The new process created by fork is called the child process.
The only difference in the returns is that the return value in the child is 0, whereas the return value in the parent is the process ID of the new child.
The reason the child’s process ID is returned to the parent is that a process can have more than one child, and there is no function that allows a process to obtain the process IDs of its children.
The reason fork returns 0 to the child is that a process can have only a single parent, and the child can always call getppid to obtain the process ID of its parent.
Both the child and the parent continue executing with the instruction that follows the call to fork.
For example, the child gets a copy of the parent’s data space, heap, and stack.
Note that this is a copy for the child; the parent and the child do not share these portions of memory.
The parent and the child do share the text segment, however (Section 7.6)
Modern implementations don’t perform a complete copy of the parent’s data, stack, and heap, since a fork is often followed by an exec.
These regions are shared by the parent and the child and have their protection changed by the kernel to read-only.
If either process tries to modify these regions, the kernel then makes a copy of that piece of memory only, typically a ‘‘page’’ in a virtual memory system.
Variations of the fork function are provided by some platforms.
All four platforms discussed in this book support the vfork(2) variant discussed in the next section.
This is a generalized form of fork that allows the caller to control what is shared between parent and child.
Solaris 10 provides two threads libraries: one for POSIX threads (pthreads) and one for Solaris threads.
In previous releases, the behavior of fork differed between the two thread libraries.
For POSIX threads, fork created a process containing only the calling thread, but for Solaris threads, fork created a process containing copies of all threads from the process of the calling thread.
In Solaris 10, this behavior has changed; fork creates a child containing a copy of the calling thread only, regardless of which thread library is used.
Solaris also provides the fork1 function, which can be used to create a process that duplicates only the calling thread, and the forkall function, which can be used to create a process that duplicates all the threads in the process.
Example The program in Figure 8.1 demonstrates the fork function, showing how changes to variables in a child process do not affect the value of the variables in the parent process.
In general, we never know whether the child starts executing before the parent, or vice versa.
The order depends on the scheduling algorithm used by the kernel.
If it’s required that the child and parent synchronize their actions, some form of interprocess.
There is no guarantee that the length of this delay is adequate, and we talk about this and other types of synchronization in Section 8.9 when we discuss race conditions.
In Section 10.16, we show how to use signals to synchronize a parent and a child after a fork.
When we write to standard output, we subtract 1 from the size of buf to avoid writing the terminating null byte.
Although strlen will calculate the length of a string not including the terminating null byte, sizeof calculates the size of the buffer, which does include the terminating null byte.
Another difference is that using strlen requires a function call, whereas sizeof calculates the buffer length at compile time, as the buffer is initialized with a known string and its size is fixed.
Note the interaction of fork with the I/O functions in the program in Figure 8.1
Recall from Chapter 3 that the write function is not buffered.
Because write is called before the fork, its data is written once to standard output.
Recall from Section 5.12 that standard output is line buffered if it’s connected to a terminal device; otherwise, it’s fully buffered.
When we run the program interactively, we get only a single copy of the first printf line, because the standard output buffer is flushed by the newline.
When we redirect standard output to a file, however, we get two copies of the printf line.
In this second case, the printf before the fork is called once, but the line remains in the buffer when fork is called.
This buffer is then copied into the child when the parent’s data space is copied to the child.
Both the parent and the child now have a standard I/O buffer with this line in it.
The second printf, right before the exit, just appends its data to the existing buffer.
When each process terminates, its copy of the buffer is finally flushed.
When we redirect the standard output of the parent from the program in Figure 8.1, the child’s standard output is also redirected.
Indeed, one characteristic of fork is that all file descriptors that are open in the parent are duplicated in the child.
We say ‘‘duplicated’’ because it’s as if the dup function had been called for each descriptor.
The parent and the child share a file table entry for every open descriptor (recall Figure 3.9)
Consider a process that has three different files opened for standard input, standard output, and standard error.
On return from fork, we have the arrangement shown in Figure 8.2
It is important that the parent and the child share the same file offset.
Consider a process that forks a child, then waits for the child to complete.
Assume that both processes write to standard output as part of their normal processing.
If the parent has its standard output redirected (by a shell, perhaps), it is essential that the parent’s file offset be updated by the child when the child writes to standard output.
In this case, the child can write to standard output while the parent is waiting for it; on completion of the child, the parent can continue writing to standard output, knowing that its output will be appended to whatever the child wrote.
If the parent and the child did not share the same file offset, this type of interaction would be more difficult to accomplish and would require explicit actions by the parent.
Figure 8.2 Sharing of open files between parent and child after fork.
If both parent and child write to the same descriptor, without any form of synchronization, such as having the parent wait for the child, their output will be intermixed (assuming it’s a descriptor that was open before the fork)
Although this is possible — we saw it in Figure 8.2 — it’s not the normal mode of operation.
There are two normal cases for handling the descriptors after a fork.
When the child terminates, any of the shared descriptors that the child read from or wrote to will have their file offsets updated accordingly.
Both the parent and the child go their own ways.
Here, after the fork, the parent closes the descriptors that it doesn’t need, and the child does the same thing.
Besides the open files, numerous other properties of the parent are inherited by the child:
Real user ID, real group ID, effective user ID, and effective group ID.
The two processes have different parent process IDs: the parent process ID of the child is the parent; the parent process ID of the parent doesn’t change.
File locks set by the parent are not inherited by the child.
The set of pending signals for the child is set to the empty set.
Many of these features haven’t been discussed yet—we’ll cover them in later chapters.
The two main reasons for fork to fail are (a) if too many processes are already in.
This is common for network servers—the parent waits for a service request from a client.
When the request arrives, the parent calls fork and lets the child handle the request.
The parent goes back to waiting for the next service request to arrive.
In this case, the child does an exec (which we describe in Section 8.10) right after it returns from the fork.
Some operating systems combine the operations from step 2—a fork followed by an exec—into a single operation called a spawn.
The UNIX System separates the two, as there are numerous cases where it is useful to fork without doing an exec.
Also, separating the two operations allows the child to change the per-process attributes between the fork and the exec, such as I/O redirection, user ID, signal disposition, and so on.
The Single UNIX Specification does include spawn interfaces in the advanced real-time option group.
These interfaces are not intended to be replacements for fork and exec, however.
They are intended to support systems that have difficulty implementing fork efficiently, especially systems without hardware support for memory management.
The function vfork has the same calling sequence and same return values as fork, but the semantics of the two functions differ.
Some consider the function a blemish, but all the platforms covered in this book support it.
The vfork function creates the new process, just like fork, without copying the address space of the parent into the child, as the child won’t reference that address space; the child simply calls exec (or exit) right after the vfork.
Instead, the child runs in the address space of the parent until it calls either exec or exit.
This optimization is more efficient on some implementations of the UNIX System, but leads to undefined results if the child modifies any data (except the variable used to hold the return value from vfork), makes function calls, or returns without calling exec or exit.
As we mentioned in the previous section, implementations use copy-on-write to improve the efficiency of a fork followed by an exec, but no copying is still faster than some copying.
Another difference between the two functions is that vfork guarantees that the child runs first, until the child calls exec or exit.
When the child calls either of these functions, the parent resumes.
This can lead to deadlock if the child depends on further actions of the parent before calling either of these two functions.
We’ve replaced the call to fork with vfork and removed the write to standard output.
Also, we don’t need to have the parent call sleep, as we’re guaranteed that it is put to sleep by the kernel until the child calls either exec or exit.
Here, the incrementing of the variables done by the child changes the values in the parent.
Because the child runs in the address space of the parent, this doesn’t surprise us.
Depending on the implementation of the standard I/O library, we might see no difference in the output, or we might find that the output from the first printf in the parent has disappeared.
Most modern implementations of exit do not bother to close the streams.
Because the process is about to exit, the kernel will close all the file descriptors open in the process.
Closing them in the library simply adds overhead without any benefit.
As we described in Section 7.3, a process can terminate normally in five ways:
As we saw in Section 7.3, this is equivalent to calling exit.
This function is defined by ISO C and includes the calling of all exit handlers that have been registered by calling atexit and closing all standard I/O streams.
Because ISO C does not deal with file descriptors, multiple processes (parents and children), and job control, the definition of this function is incomplete for a UNIX system.
Whether standard I/O streams are flushed depends on the implementation.
Executing a return from the start routine of the last thread in the process.
The return value of the thread is not used as the return value of the process, however.
Calling the pthread_exit function from the last thread in the process.
The signal can be generated by the process itself (e.g., by calling the abort function), by some other process, or by the kernel.
By default, cancellation occurs in a deferred manner: one thread requests that another be canceled, and sometime later the target thread terminates.
Regardless of how a process terminates, the same code in the kernel is eventually executed.
This kernel code closes all the open descriptors for the process, releases the memory that it was using, and so on.
For any of the preceding cases, we want the terminating process to be able to notify its parent how it terminated.
In the case of an abnormal termination, however, the kernel—not the process — generates a termination status to indicate the reason for the abnormal termination.
In any case, the parent of the process can obtain the termination status from either the wait or the waitpid function (described in the next section)
Note that we differentiate between the exit status, which is the argument to one of the three exit functions or the return value from main, and the termination status.
Figure 8.4 describes the various ways the parent can examine the termination status of a child.
If the child terminated normally, the parent can obtain the exit status of the child.
When we described the fork function, it was obvious that the child has a parent process after the call to fork.
Now we’re talking about returning a termination status to the parent.
But what happens if the parent terminates before the child? The answer is that the init process becomes the parent process of any process whose parent terminates.
In such a case, we say that the process has been inherited by init.
What normally happens is that whenever a process terminates, the kernel goes through all active processes to see whether the terminating process is the parent of any process that still exists.
If so, the parent process ID of the surviving process is changed to be 1 (the process ID of init)
This way, we’re guaranteed that every process has a parent.
Another condition we have to worry about is when a child terminates before its parent.
If the child completely disappeared, the parent wouldn’t be able to fetch its termination status when and if the parent was finally ready to check if the child had terminated.
The kernel keeps a small amount of information for every terminating process, so that the information is available when the parent of the terminating process calls wait or waitpid.
Minimally, this information consists of the process ID, the termination status of the process, and the amount of CPU time taken by the process.
The kernel can discard all the memory used by the process and close its open files.
In UNIX System terminology, a process that has terminated, but whose parent has not yet waited for it, is called a zombie.
The ps(1) command prints the state of a zombie process as Z.
If we write a long-running program that forks many child processes, they become zombies unless we wait for them and fetch their termination status.
Some systems provide ways to prevent the creation of zombies, as we describe in Section 10.7
The final condition to consider is this: What happens when a process that has been inherited by init terminates? Does it become a zombie? The answer is ‘‘no,’’ because init is written so that whenever one of its children terminates, init calls one of the wait functions to fetch the termination status.
By doing this, init prevents the system from being clogged by zombies.
When a process terminates, either normally or abnormally, the kernel notifies the parent by sending the SIGCHLD signal to the parent.
Because the termination of a child is an asynchronous event—it can happen at any time while the parent is running — this signal is the asynchronous notification from the kernel to the parent.
The parent can choose to ignore this signal, or it can provide a function that is called when the signal occurs: a signal handler.
The default action for this signal is to be ignored.
For now, we need to be aware that a process that calls wait or waitpid can.
Return immediately with the termination status of a child, if a child has terminated and is waiting for its termination status to be fetched.
Return immediately with an error, if it doesn’t have any child processes.
If the process is calling wait because it received the SIGCHLD signal, we expect wait to return immediately.
But if we call it at any random point in time, it can block.
The wait function can block the caller until a child process terminates, whereas waitpid has an option that prevents it from blocking.
The waitpid function doesn’t wait for the child that terminates first; it has a number of options that control which process it waits for.
If a child has already terminated and is a zombie, wait returns immediately with that child’s status.
If the caller blocks and has multiple children, wait returns when one terminates.
We can always tell which child terminated, because the process ID is returned by the function.
For both functions, the argument statloc is a pointer to an integer.
If this argument is not a null pointer, the termination status of the terminated process is stored in the location pointed to by the argument.
If we don’t care about the termination status, we simply pass a null pointer as this argument.
Traditionally, the integer status that these two functions return has been defined by the implementation, with certain bits indicating the exit status (for a normal return), other bits indicating the signal number (for an abnormal return), one bit indicating whether a core file was generated, and so on.
Four mutually exclusive macros tell us how the process terminated, and they all begin with WIF.
Based on which of these four macros is true, other macros are used to obtain the exit status, signal number, and the like.
The four mutually exclusive macros are shown in Figure 8.4
WIFEXITED(status) Tr ue if status was returned for a child that terminated normally.
WTERMSIG(status) to fetch the signal number that caused the termination.
Additionally, some implementations (but not the Single UNIX Specification) define the macro.
WCOREDUMP(status) that returns true if a core file of the terminated process was generated.
WIFSTOPPED(status) Tr ue if status was returned for a child that is currently stopped.
WSTOPSIG(status) to fetch the signal number that caused the child to stop.
Figure 8.4 Macros to examine the termination status returned by wait and waitpid.
We’ll discuss how a process can be stopped in Section 9.8 when we discuss job control.
We’ll call this function from numerous programs in the text.
Note that this function handles the WCOREDUMP macro, if it is defined.
If we run the program in Figure 8.6, we get.
We’ll see a portable way to map a signal number to a descriptive name in Section 10.22
As we mentioned, if we have more than one child, wait returns on termination of any of the children.
But what if we want to wait for a specific process to terminate (assuming we know which process ID we want to wait for)? In older versions of the UNIX System, we would have to call wait and compare the returned process ID with the one we’re interested in.
If the terminated process wasn’t the one we wanted, we would have to save the process ID and termination status and call wait again.
We would need to continue doing this until the desired process terminated.
The next time we wanted to wait for a specific process, we would go through the list of already terminated processes to see whether we had already waited for it, and if not, call wait.
What we need is a function that waits for a specific process.
This functionality (and more) is provided by the POSIX.1 waitpid function.
The waitpid function returns the process ID of the child that terminated and stores the child’s termination status in the memory location pointed to by statloc.
With wait, the only real error is if the calling process has no children.
Another error return is possible, in case the function call is interrupted by a signal.
With waitpid, however, it’s also possible to get an error if the specified process or process group does not exist or is not a child of the calling process.
The options argument lets us further control the operation of waitpid.
The WIFSTOPPED macro determines whether the return value corresponds to a stopped child process.
The waitpid function provides three features that aren’t provided by the wait function.
The waitpid function lets us wait for one particular process, whereas the wait function returns the status of any terminated child.
We’ll return to this feature when we discuss the popen function.
There are times when we want to fetch a child’s status, but we don’t want to block.
The waitpid function provides support for job control with the WUNTRACED and WCONTINUED options.
If we want to write a process so that it forks a child but we don’t want to wait for the child to complete and we don’t want the child to become a zombie until we terminate, the trick is to call fork twice.
We’re the second child; our parent becomes init as soon * as our real parent calls exit() in the statement above.
Here’s where we’d continue executing, knowing that when * we’re done, init will reap our status.
We call sleep in the second child to ensure that the first child terminates before printing the parent process ID.
After a fork, either the parent or the child can continue executing; we never know which will resume execution first.
Note that the shell prints its prompt when the original process terminates, which is before the second child prints its parent process ID.
The Single UNIX Specification includes an additional function to retrieve the exit status of a process.
The waitid function is similar to waitpid, but provides extra flexibility.
Like waitpid, waitid allows a process to specify which children to wait for.
Instead of encoding this information in a single argument combined with the process ID or process group ID, two separate arguments are used.
The id parameter is interpreted based on the value of idtype.
P_PID Wait for a particular process: id contains the process ID of the child to wait for.
P_PGID Wait for any child process in a particular process group: id contains the process.
The options argument is a bitwise OR of the flags shown in Figure 8.10
These flags indicate which state changes the caller is interested in.
At least one of WCONTINUED, WEXITED, or WSTOPPED must be specified in the options argument.
The infop argument is a pointer to a siginfo structure.
This structure contains detailed information about the signal generated that caused the state change in the child process.
Note, however, that Mac OS X 10.6.8 doesn’t set all the information we expect in the siginfo structure.
Historically, these two variants descend from the BSD branch of the UNIX System.
The only feature provided by these two functions that isn’t provided by the wait, waitid, and waitpid functions is an additional argument that allows the kernel to return a summary of the resources used by the terminated process and all its child processes.
The resource information includes such statistics as the amount of user CPU time, amount of system CPU time, number of page faults, number of signals received, and the like.
This resource information differs from the resource limits we described in Section 7.11
Figure 8.11 details the various arguments supported by the wait functions.
Figure 8.11 Arguments supported by wait functions on various systems.
The wait3 function was included in earlier versions of the Single UNIX Specification.
For our purposes, a race condition occurs when multiple processes are trying to do something with shared data and the final outcome depends on the order in which the processes run.
The fork function is a lively breeding ground for race conditions, if any of the logic after the fork either explicitly or implicitly depends on whether the parent or child runs first after the fork.
Even if we knew which process would run first, what happens after that process starts running depends on the system load and the kernel’s scheduling algorithm.
We saw a potential race condition in the program in Figure 8.8 when the second child printed its parent process ID.
If the second child runs before the first child, then its parent process will be the first child.
But if the first child runs first and has enough time to exit, then the parent process of the second child is init.
If the system was heavily loaded, the second child could resume after sleep returns, before the first child has a chance to run.
Problems of this form can be difficult to debug because they tend to work ‘‘most of the time.’’
A process that wants to wait for a child to terminate must call one of the wait functions.
If a process wants to wait for its parent to terminate, as in the program from Figure 8.8, a loop of the following form could be used:
The problem with this type of loop, called polling, is that it wastes CPU time, as the caller is awakened every second to test the condition.
To avoid race conditions and to avoid polling, some form of signaling is required between multiple processes.
Signals can be used for this purpose, and we describe one way to do this in Section 10.16
Various forms of interprocess communication (IPC) can also be used.
For a parent and child relationship, we often have the following scenario.
After the fork, both the parent and the child have something to do.
For example, the parent could update a record in a log file with the child’s process ID, and the child might have to create a file for the parent.
In this example, we require that each process tell the other when it has finished its initial set of operations, and that each wait for the other to complete, before heading off on its own.
We assume that the header apue.h defines whatever variables are required.
Let’s look at an example that uses these five routines.
The program in Figure 8.12 outputs two strings: one from the child and one from the parent.
The program contains a race condition because the output depends on the order in which the processes are run by the kernel and the length of time for which each process runs.
We set the standard output unbuffered, so every character output generates a write.
The goal in this example is to allow the kernel to switch between the two processes as often as possible to demonstrate the race condition.
If we didn’t do this, we might never see the type of output that follows.
The following actual output shows how the results can vary:
We need to change the program in Figure 8.12 to use the TELL and WAIT functions.
The lines preceded by a plus sign are new lines.
When we run this program, the output is as we expect; there is no intermixing of output from the two processes.
In the program shown in Figure 8.13, the parent goes first.
The child goes first if we change the lines following the fork to be.
We mentioned in Section 8.3 that one use of the fork function is to create a new process (the child) that then causes another program to be executed by calling one of the exec functions.
When a process calls one of the exec functions, that process is completely replaced by the new program, and the new program starts executing at its main function.
The process ID does not change across an exec, because a new process is not created; exec merely replaces the current process — its text, data, heap, and stack segments — with a brand-new program from disk.
There are seven different exec functions, but we’ll often simply refer to ‘‘the exec function,’’ which means that we could use any of the seven functions.
These seven functions round out the UNIX System process control primitives.
With fork, we can create new processes; and with the exec functions, we can initiate new programs.
The exit function and the wait functions handle termination and waiting for termination.
We’ll use these primitives in later sections to build additional functions, such as popen and system.
The first difference in these functions is that the first four take a pathname argument, the next two take a filename argument, and the last one takes a file descriptor argument.
If filename contains a slash, it is taken as a pathname.
Otherwise, the executable file is searched for in the directories specified by the PATH environment variable.
The PATH variable contains a list of directories, called path prefixes, that are separated by colons.
It can be specified as a colon at the beginning of the value, two colons in a row, or a colon at the end of the value.
There are security reasons for never including the current directory in the search path.
If either execlp or execvp finds an executable file using one of the path prefixes, but the file isn’t a machine executable that was generated by the link editor, the function assumes that the file is a shell script and tries to invoke /bin/sh with the filename as input to the shell.
With fexecve, we avoid the issue of finding the correct executable file altogether and rely on the caller to do this.
By using a file descriptor, the caller can verify the file is in fact the intended file and execute it without a race.
Otherwise, a malicious user with appropriate privileges could replace the executable file (or a portion of the path to the executable file) after it has been located and verified, but before the caller can execute it (recall the discussion of TOCTTOU errors in Section 3.3)
The next difference concerns the passing of the argument list (l stands for list and v stands for vector)
The functions execl, execlp, and execle require each of the command-line arguments to the new program to be specified as separate arguments.
We mark the end of the arguments with a null pointer.
For the other four functions (execv, execvp, execve, and fexecve), we have to build an array of pointers to the arguments, and the address of this array is the argument to these three functions.
Before using ISO C prototypes, the normal way to show the command-line arguments for the three functions execl, execle, and execlp was.
This syntax explicitly shows that the final command-line argument is followed by a null pointer.
If this null pointer is specified by the constant 0, we must cast it to a pointer; if we don’t, it’s interpreted as an integer argument.
If the size of an integer is different from the size of a char *, the actual arguments to the exec function will be wrong.
The final difference is the passing of the environment list to the new program.
The three functions whose names end in an e (execle, execve, and fexecve) allow us to pass a pointer to an array of pointers to the environment strings.
We mentioned that if the system supported such functions as setenv and putenv, we could change the current environment and the environment of any subsequent child processes, but we couldn’t affect the environment of the parent process.
Normally, a process allows its environment to be propagated to its children, but in some cases, a process wants to specify a certain environment for a child.
One example of the latter is the login program when a new login shell is initiated.
Normally, login creates a specific environment with only a few variables defined and lets us, through the shell start-up file, add variables to the environment when we log in.
This syntax specifically shows that the final argument is the address of the array of character pointers to the environment strings.
The arguments for these seven exec functions are difficult to remember.
The letter p means that the function takes a filename argument and uses the PATH environment variable to find the executable file.
The letter l means that the function takes a list of arguments and is mutually exclusive with the letter v, which means that it takes an argv[] vector.
Finally, the letter e means that the function takes an envp[] array instead of using the current environment.
Every system has a limit on the total size of the argument list and the environment list.
We sometimes encounter this limit when using the shell’s filename expansion feature to generate a list of filenames.
Historically, the limit in older System V implementations was 5,120 bytes.
To get around the limitation in argument list size, we can use the xargs(1) command to break up long argument lists.
To look for all the occurrences of getrlimit in the man pages on our system, we could use.
If the man pages on our system are compressed, however, we could try.
We use the type -f option to the find command to restrict the list so that it contains only regular files, because the grep commands can’t search for patterns in directories, and we want to avoid unnecessary error messages.
We’ve mentioned that the process ID does not change after an exec, but the new program inherits additional properties from the calling process:
The handling of open files depends on the value of the close-on-exec flag for each descriptor.
If this flag is set, the descriptor is closed across an exec.
The default is to leave the descriptor open across the exec unless we specifically set the close-on-exec flag using fcntl.
This is normally done by the opendir function calling fcntl to set the close-on-exec flag for the descriptor corresponding to the open directory stream.
Note that the real user ID and the real group ID remain the same across the exec, but the effective IDs can change, depending on the status of the set-user-ID and the setgroup-ID bits for the program file that is executed.
If the set-user-ID bit is set for the new program, the effective user ID becomes the owner ID of the program file.
Otherwise, the effective user ID is not changed (it’s not set to the real user ID)
In many UNIX system implementations, only one of these seven functions, execve, is a system call within the kernel.
The other six are just library functions that eventually invoke this system call.
We can illustrate the relationship among these seven functions as shown in Figure 8.15
In this arrangement, the library functions execlp and execvp process the PATH environment variable, looking for the first path prefix that contains an executable file named filename.
The fexecve library function uses /proc to convert the file descriptor argument into a pathname that can be used by execve to execute the program.
For example, a system without /proc or /dev/fd could implement fexecve as a system call veneer that translates the file descriptor argument into an i-node pointer, implement execve as a system call veneer that translates the pathname argument into an i-node pointer, and place all the rest of the exec code common to both execve and fexecve in a separate function to be called with an i-node pointer for the file to be executed.
We first call execle, which requires a pathname and a specific environment.
The next call is to execlp, which uses a filename and passes the caller’s environment to the new program.
The only reason the call to execlp works is that the directory /home/sar/bin is one of the current path prefixes.
Note also that we set the first argument, argv[0] in the new program, to be the filename component of the pathname.
Some shells set this argument to be the complete pathname.
This is a convention only; we can set argv[0] to any string we like.
The login command does this when it executes the shell.
Before executing the shell, login adds a dash as a prefix to argv[0] to indicate to the shell that it is being invoked as a login shell.
A login shell will execute the start-up profile commands, whereas a nonlogin shell will not.
It is a trivial program that echoes all its command-line arguments and its entire environment list.
Figure 8.17 Echo all command-line arguments and all environment strings.
When we execute the program from Figure 8.16, we get.
Note that the shell prompt appeared before the printing of argv[0] from the second exec.
This occurred because the parent did not wait for this child process to finish.
In the UNIX System, privileges, such as being able to change the system’s notion of the current date, and access control, such as being able to read or write a particular file, are based on user and group IDs.
When our programs need additional privileges or need to gain access to resources that they currently aren’t allowed to access, they need to change their user or group ID to an ID that has the appropriate privilege or access.
Similarly, when our programs need to lower their privileges or prevent access to certain resources, they do so by changing either their user ID or group ID to an ID without the privilege or ability access to the resource.
In general, we try to use the least-privilege model when we design our applications.
According to this model, our programs should use the least privilege necessary to accomplish any given task.
This reduces the risk that security might be compromised by a malicious user trying to trick our programs into using their privileges in unintended ways.
We can set the real user ID and effective user ID with the setuid function.
Similarly, we can set the real group ID and the effective group ID with the setgid function.
Everything we describe for the user ID also applies to the group ID.
If the process does not have superuser privileges, but uid equals either the real user ID or the saved set-user-ID, setuid sets only the effective user ID to uid.
The real user ID and the saved set-user-ID are not changed.
If this feature isn’t provided, then delete all preceding references to the saved set-user-ID.
We can make a few statements about the three user IDs that the kernel maintains.
Because login is a superuser process, it sets all three user IDs when it calls setuid.
The effective user ID is set by the exec functions only if the set-user-ID bit is set for the program file.
If the set-user-ID bit is not set, the exec functions leave the effective user ID as its current value.
We can call setuid at any time to set the effective user ID to either the real user ID or the saved set-user-ID.
Naturally, we can’t set the effective user ID to any random value.
The saved set-user-ID is copied from the effective user ID by exec.
If the file’s set-user-ID bit is set, this copy is saved after exec stores the effective user ID from the file’s user ID.
Figure 8.18 summarizes the various ways these three user IDs can be changed.
Note that we can obtain only the current value of the real user ID and the effective user ID with the functions getuid and geteuid from Section 8.2
We have no portable way to obtain the current value of the saved set-user-ID.
Historically, BSD supported the swapping of the real user ID and the effective user ID with the setreuid function.
The rule is simple: an unprivileged user can always swap between the real user ID and the effective user ID.
This allows a set-user-ID program to swap to the user’s normal permissions and swap back again later for set-user-ID operations.
When the saved set-user-ID feature was introduced with POSIX.1, the rule was enhanced to also allow an unprivileged user to set its effective user ID to its saved set-user-ID.
Both setreuid and setregid are included in the XSI option in POSIX.1
As such, all UNIX System implementations are expected to provide support for them.
This allowed an unprivileged user to swap back and forth between the two values.
Be aware, however, that when programs that used this feature spawned a shell, they had to set the real user ID to the normal user ID before the exec.
If they didn’t do this, the real user ID could be privileged (from the swap done by setreuid) and the shell process could call setreuid to swap the two and assume the permissions of the more privileged user.
As a defensive programming measure to solve this problem, programs set both the real user ID and the effective user ID to the normal user ID before the call to exec in the child.
These functions are similar to setuid and setgid, but only the effective user ID or effective group ID is changed.
An unprivileged user can set its effective user ID to either its real user ID or its saved set-user-ID.
For a privileged user, only the effective user ID is set to uid.
This behavior differs from that of the setuid function, which changes all three user IDs.
Figure 8.19 summarizes all the functions that we’ve described here that modify the three user IDs.
Figure 8.19 Summary of all the functions that set the various user IDs.
Everything that we’ve said so far in this section also applies in a similar fashion to group IDs.
The supplementary group IDs are not affected by setgid, setregid, or setegid.
To see the utility of the saved set-user-ID feature, let’s examine the operation of a program that uses it.
We’ll look at the at(1) program, which we can use to schedule commands to be run at some time in the future.
On Linux 3.2.0, the at program is installed set-user-ID to user daemon.
This allows the at command to write privileged files owned by the daemon that will run the commands on behalf of the user running the at command.
To prevent being tricked into running commands that we aren’t allowed to run, or reading or writing files that we aren’t allowed to access, the at command and the daemon that ultimately runs the commands on our behalf have to switch between sets of privileges: ours and those of the daemon.
Assuming that the at program file is owned by root and has its set-user-ID bit set, when we run it, we have.
The first thing the at command does is reduce its privileges so that it runs with our privileges.
It calls the seteuid function to set the effective user ID to our real user ID.
The at program runs with our privileges until it needs to access the configuration files that control which commands are to be run and the time at which they need to run.
These files are owned by the daemon that will run the commands for us.
The at command calls seteuid to set the effective user ID to root.
This call is allowed because the argument to seteuid equals the saved set-user-ID.
After the files are modified to record the commands to be run and the time at which they are to be run, the at command lowers its privileges by calling.
Because the child is running with root privileges, this changes all of the IDs.
Now the daemon can safely execute commands on our behalf, because it can access only the files to which we normally have access.
By using the saved set-user-ID in this fashion, we can use the extra privileges granted to us by the set-user-ID of the program file only when we need elevated privileges.
Any other time, however, the process runs with our normal permissions.
If we weren’t able to switch back to the saved set-user-ID at the end, we might be tempted to retain the extra permissions the whole time we were running (which is asking for trouble)
These files are text files that begin with a line of the form.
The most common of these interpreter files begin with the line.
The recognition of these files is done within the kernel as part of processing the exec system call.
The actual file that gets executed by the kernel is not the interpreter file, but rather the file specified by the pathname on the first line of the interpreter file.
Be sure to differentiate between the interpreter file—a text file that begins with #!—and the interpreter, which is specified by the pathname on the first line of the interpreter file.
Be aware that systems place a size limit on the first line of an interpreter file.
This limit includes the #!, the pathname, the optional argument, the terminating newline, and any spaces.
Let’s look at an example to see what the kernel does with the arguments to the exec function when the file being executed is an interpreter file and the optional argument on the first line of the interpreter file.
The following shows the contents of the one-line interpreter file that is executed and the result from running the program in Figure 8.20:
The program echoarg (the interpreter) just echoes each of its command-line arguments.
Note that the kernel takes the pathname from the execl call instead of the first argument (testinterp), on the assumption that the pathname might contain more information than the first argument.
A common use for the optional argument following the interpreter pathname is to specify the -f option for programs that support this option.
Systems derived from UNIX System V often include two versions of the awk language.
In contrast, nawk (new awk) contains numerous enhancements and corresponds to the language described in Aho, Kernighan, and Weinberger [1988]
This newer version provides access to the command-line arguments, which we need for the example that follows.
This utility is also based on the language described in Aho, Kernighan, and Weinberger [1988]
The version of awk in Mac OS X 10.6.8 is based on the Bell Laboratories version, which has been placed in the public domain.
FreeBSD 8.0 and some Linux distributions ship with GNU awk, called gawk, which is linked to the name awk.
Because they are more up-to-date, gawk and the version of awk from Bell Laboratories are preferred to either nawk or old awk.
The Bell Labs version of awk is available at http://cm.bell-labs.com/cm/cs/awkbook/index.html.
Using the -f option with an interpreter file lets us write #!/bin/awk -f (awk program follows in the interpreter file)
If one of the path prefixes is /usr/local/bin, we can execute the program in Figure 8.21 (assuming that we’ve turned on the execute bit for the file) as.
The filename portion of this pathname (what we typed to the shell) isn’t adequate, because the interpreter (/bin/awk in this example) can’t be expected to use the PATH variable to locate files.
When it reads the interpreter file, awk ignores the first line, since the pound sign is awk’s comment character.
We can verify these command-line arguments with the following commands:
In this example, the -f option for the interpreter is required.
As we said, this tells awk where to look for the awk program.
If we remove the -f option from the interpreter file, an error message usually results when we try to run it.
The exact text of the message varies, depending on where the interpreter file is stored and whether the remaining arguments represent existing files.
This is because the command-line arguments in this case are.
If we couldn’t pass at least a single optional argument to the interpreter (-f in this case), these interpreter files would be usable only with the shells.
They provide an efficiency gain for the user at some expense in the kernel (since it’s the kernel that recognizes these files)
They hide that certain programs are scripts in some other language.
For example, to execute the program in Figure 8.21, we just say.
We could still hide that the program is an awk script, by wrapping it in a shell script:
The problem with this solution is that more work is required.
First, the shell reads the command and tries to execlp the filename.
Because the shell script is an executable file but isn’t a machine executable, an error is returned and execlp assumes that the file is a shell script (which it is)
Then /bin/sh is executed with the pathname of the shell script as its argument.
The shell correctly runs our script, but to run the awk program, the shell does a fork, exec, and wait.
Thus there is more overhead involved in replacing an interpreter script with a shell script.
Interpreter scripts let us write shell scripts using shells other than /bin/sh.
When it finds an executable file that isn’t a machine executable, execlp has to choose a shell to invoke, and it always uses /bin/sh.
Again, we could wrap all of this in a /bin/sh script (that invokes the C shell), as we described earlier, but more overhead is required.
None of this would work as we’ve shown here if the three shells and awk didn’t use the pound sign as their comment character.
It is convenient to execute a command string from within a program.
For example, assume that we want to put a time-and-date stamp into a certain file.
We could use the functions described in Section 6.10 to do this: call time to get the current calendar time, then call localtime to convert it to a broken-down time, then call strftime to format the result, and finally write the result to the file.
POSIX.1 includes the system interface, expanding on the ISO C definition to describe its behavior in a POSIX environment.
If cmdstring is a null pointer, system returns nonzero only if a command processor is available.
This feature determines whether the system function is supported on a given operating system.
Because system is implemented by calling fork, exec, and waitpid, there are three types of return values.
If the exec fails, implying that the shell can’t be executed, the return value is as if the shell had executed exit(127)
Otherwise, all three functions—fork, exec, and waitpid—succeed, and the return value from system is the termination status of the shell, in the format specified for waitpid.
Some older implementations of system returned an error (EINTR) if waitpid was interrupted by a caught signal.
Because there is no strategy that an application can use to recover from this type of error (the process ID of the child is hidden from the caller), POSIX later added the requirement that system not return an error in this case.
We’ll update this function with signal handling in Section 10.18
The shell parses this null-terminated C string and breaks it up into separate command-line arguments for the command.
The actual command string that is passed to the shell can contain any valid shell commands.
If we didn’t use the shell to execute the command, but tried to execute the command ourself, it would be more difficult.
First, we would want to call execlp, instead of execl, to use the PATH variable, like the shell.
We would also have to break up the null-terminated C string into separate command-line arguments for the call to execlp.
Finally, we wouldn’t be able to use any of the shell metacharacters.
We do this to prevent any standard I/O buffers, which would have been copied from the parent to the child across the fork, from being flushed in the child.
We can test this version of system with the program shown in Figure 8.23
The advantage in using system, instead of using fork and exec directly, is that system does all the required error handling and (in our next version of this function in Section 10.18) all the required signal handling.
Instead, the parent waited for the child, using a statement such as.
A problem occurs if the process that calls system has spawned its own children before calling system.
Because the while statement above keeps looping until the child that was generated by system terminates, if any children of the process terminate before the process identified by pid, then the process ID and termination status of these other children are discarded by the while statement.
Indeed, this inability to wait for a specific child is one of the reasons given in the POSIX.1 Rationale for including the waitpid function.
We’ll see in Section 15.3 that the same problem occurs with the popen and pclose functions if the system doesn’t provide a waitpid function.
What happens if we call system from a set-user-ID program? Doing so creates a security hole and should never be attempted.
Figure 8.24 shows a simple program that just calls system for its command-line argument.
Figure 8.25 shows another simple program that prints its real and effective user IDs.
The superuser permissions that we gave the tsys program are retained across the fork and exec that are done by system.
Some implementations have closed this security hole by changing /bin/sh to reset the effective user ID to the real user ID when they don’t match.
On these systems, the previous example doesn’t work as shown.
Instead, the same effective user ID will be printed regardless of the status of the set-user-ID bit on the program calling system.
If it is running with special permissions—either set-user-ID or set-group-ID — and wants to spawn another process, a process should use fork and exec directly, being certain to change back to normal permissions after the fork, before calling exec.
The system function should never be used from a set-user-ID or a set-group-ID program.
One reason for this admonition is that system invokes the shell to parse the command string, and the shell uses its IFS variable as the input field separator.
Older versions of the shell didn’t reset this variable to a normal set of characters when invoked.
As a result, a malicious user could set IFS before system was called, causing system to execute a different program.
Most UNIX systems provide an option to do process accounting.
When enabled, the kernel writes an accounting record each time a process terminates.
These accounting records typically contain a small amount of binary data with the name of the command, the amount of CPU time used, the user ID and group ID, the starting time, and so on.
We’ll take a closer look at these accounting records in this section, as it gives us a chance to look at processes again and to use the fread function from Section 5.9
Process accounting is not specified by any of the standards.
Linux 3.2.0, on the other hand, doesn’t try to maintain I/O statistics at all.
Each implementation also has its own set of administrative commands to process raw accounting data.
A function we haven’t described (acct) enables and disables process accounting.
The only use of this function is from the accton(8) command (which happens to be one of the few similarities among platforms)
A superuser executes accton with a pathname argument to enable accounting.
Accounting is turned off by executing accton without any arguments.
The structure of the accounting records is defined in the header <sys/acct.h>
Although the implementation of each system differs, the accounting records look something like.
Times are recorded in units of clock ticks on most platforms, but FreeBSD stores microseconds instead.
The ac_flag member records certain events during the execution of the process.
The data required for the accounting record, such as CPU times and number of characters transferred, is kept by the kernel in the process table and initialized whenever a new process is created, as in the child after a fork.
First, we don’t get accounting records for processes that never terminate.
Processes like init that run for the lifetime of the system don’t generate accounting records.
This also applies to kernel daemons, which normally don’t exit.
Second, the order of the records in the accounting file corresponds to the termination order of the processes, not the order in which they were started.
To know the starting order, we would have to go through the accounting file and sort by the starting calendar time.
But this isn’t perfect, since calendar times are in units of seconds (Section 1.10), and it’s possible for many processes to be started in any given second.
But we don’t know the ending time of a process; all we know is its starting time and ending order.
Thus, even though the elapsed time is more accurate than the starting time, we still can’t reconstruct the exact starting order of various processes, given the data in the accounting file.
A new record is initialized by the kernel for the child after a fork, not when a new program is executed.
Although exec doesn’t create a new accounting record, the command name changes, and the AFORK flag is cleared.
This means that if we have a chain of three programs — A.
The command name in the record corresponds to program C, but the CPU times, for example, are the sum for programs A, B, and C.
Example To have some accounting data to examine, we’ll create a test program to implement the diagram shown in Figure 8.27
The source for the test program is shown in Figure 8.28
We’ll run the test program on Solaris and then use the program in Figure 8.29 to print out selected fields from the accounting records.
Basing the defined symbol on the feature instead of on the platform makes the code read better and allows us to modify the program simply by adding the new definition to our compilation command.
We define similar constants to determine whether the platform supports the ACORE.
We can’t use the flag symbols themselves, because on Linux, they are defined as enum values, which we can’t use in a #ifdef expression.
Note that when this command terminates, accounting should be on; therefore, the first record in the accounting file should be from this command.
Exit the superuser shell and run the program in Figure 8.28
This should append six records to the accounting file: one for the superuser shell, one for the test parent, and one for each of the four test children.
A new process is not created by the execl in the second child.
There is only a single accounting record for the second child.
Since accounting is off when this accton command terminates, it should not appear in the accounting file.
Run the program in Figure 8.29 to print the selected fields from the accounting file.
We have appended the description of the process in italics to selected lines, for the discussion later.
For this system, the elapsed time values are measured in units of clock ticks.
Note that the amount of time a process sleeps is not exact.
Also, the calls to fork and exit take some amount of time.
Note that the ac_stat member is not the true termination status of the process, but rather corresponds to a portion of the termination status that we discussed in.
The only information in this byte is a core-flag bit (usually the high-order bit) and the signal number (usually the seven low-order bits), if the process terminated abnormally.
If the process terminated normally, we are not able to obtain the exit status from the accounting file.
The value 9 for the fourth child corresponds to the value of SIGKILL.
The size of the file /etc/passwd that the dd process copies in the second child is 777 bytes.
The number of characters of I/O is just over twice this value.
Even though the output goes to the null device, the bytes are still accounted for.
The 31 additional bytes come from the dd command reporting the summary of bytes read and written, which it prints to stdout.
The F flag is set for all the child processes except the second child, which does the execl.
The F flag is not set for the parent, because the interactive shell that executed the parent did a fork and then an exec of the a.out file.
The first child process calls abort, which generates a SIGABRT signal to generate the core dump.
Note that neither the X flag nor the D flag is on, as they are not supported on Solaris; the information they represent can be derived from the ac_stat field.
The fourth child also terminates because of a signal, but the SIGKILL signal does not generate a core dump; it just terminates the process.
As a final note, the first child has a 0 count for the number of characters of I/O, yet this process generated a core file.
It appears that the I/O required to write the core file is not charged to the process.
Any process can find out its real and effective user ID and group ID.
Sometimes, however, we want to find out the login name of the user who’s running the program.
The system normally keeps track of the name we log in under (Section 6.8), and the getlogin function provides a way to fetch that login name.
Returns: pointer to string giving login name if OK, NULL on error.
This function can fail if the process is not attached to a terminal that a user logged in to.
Given the login name, we can then use it to look up the user in the password file — to determine the login shell, for example—using getpwnam.
FreeBSD and Mac OS X store the login name in the session structure associated with the process table entry and provide system calls to fetch and store this name.
System V provided the cuserid function to return the login name.
This function called getlogin and, if that failed, did a getpwuid(getuid())
The IEEE Standard 1003.1-1988 specified cuserid, but it called for the effective user ID to be used, instead of the real user ID.
The environment variable LOGNAME is usually initialized with the user’s login name by login(1) and inherited by the login shell.
Realize, however, that a user can modify an environment variable, so we shouldn’t use LOGNAME to validate the user in any way.
Historically, the UNIX System provided processes with only coarse control over their scheduling priority.
The scheduling policy and priority were determined by the kernel.
A process could choose to run with lower priority by adjusting its nice value (thus a process could be ‘‘nice’’ and reduce its share of the CPU by adjusting its nice value)
Only a privileged process was allowed to increase its scheduling priority.
The real-time extensions in POSIX added interfaces to select among multiple scheduling classes and fine-tune their behavior.
We discuss only the interfaces used to adjust the nice value here; they are part of the XSI option in POSIX.1
Refer to Gallmeister [1995] for more information on the real-time scheduling extensions.
Be aware that the header file defining NZERO differs among systems.
A process can retrieve and change its nice value with the nice function.
With this function, a process can affect only its own nice value; it can’t affect the nice value of any other process.
The getpriority function can be used to get the nice value for a process, just like the nice function.
However, getpriority can also get the nice value for a group of related processes.
The which argument controls how the who argument is interpreted and the who argument selects the process or processes of interest.
If the who argument is 0, then it indicates the calling process, process group, or user (depending on the value of the which argument)
When the which argument applies to more than one process, the highest priority (lowest value) of all the applicable processes is returned.
The setpriority function can be used to set the priority of a process, a process group, or all the processes belonging to a particular user ID.
The which and who arguments are the same as in the getpriority function.
The value is added to NZERO and this becomes the new nice value.
The nice system call originated with an early PDP-11 version of the Research UNIX System.
The Single UNIX Specification leaves it up to the implementation whether the nice value is inherited by a child process after a fork.
However, XSI-compliant systems are required to preserve the nice value across a call to exec.
The program in Figure 8.30 measures the effect of adjusting the nice value of a process.
Two processes run in parallel, each incrementing its own counter.
The parent runs with the default nice value, and the child runs with an adjusted nice value as specified by the.
After running for 10 seconds, both processes print the value of their counter and exit.
By comparing the counter values for different nice values, we can get an idea how the nice value affects process scheduling.
Figure 8.30 Evaluate the effect of changing the nice value.
We run the program twice: once with the default nice value, and once with the highest valid nice value (the lowest scheduling priority)
We run this on a uniprocessor Linux system to show how the scheduler shares the CPU among processes with different nice values.
With an otherwise idle system, a multiprocessor system (or a multicore CPU) would allow both processes to run without the need to share a CPU, and we wouldn’t see much difference between two processes with different nice values.
The percentages aren’t exactly equal, because process scheduling isn’t exact, and because the child and parent perform different amounts of processing between the time that the end time is calculated and the time that the processing loop begins.
These values will vary based on how the process scheduler uses the nice value, so a different UNIX system will produce different ratios.
In Section 1.10, we described three times that we can measure: wall clock time, user CPU time, and system CPU time.
Any process can call the times function to obtain these values for itself and any terminated children.
Note that the structure does not contain any measurement for the wall clock time.
Instead, the function returns the wall clock time as the value of the function, each time it’s called.
This value is measured from some arbitrary point in the past, so we can’t use its absolute value; instead, we use its relative value.
For example, we call times and save the return value.
At some later time, we call times again and subtract the earlier return value from the new return value.
It is possible, though unlikely, for a long-running process to overflow the wall clock time; see Exercise 1.5
The two structure fields for child processes contain values only for children that we have waited for with one of the wait functions discussed earlier in this chapter.
This function returns the CPU times and 14 other values indicating resource usage.
Historically, this function originated with the BSD operating system, so BSD-derived implementations generally support more of the fields than do other implementations.
The program in Figure 8.31 executes each command-line argument as a shell command string, timing the command and printing the values from the tms structure.
In the first two commands, execution is fast enough to avoid registering any CPU time at the reported resolution.
In the third command, however, we run a command that takes enough processing time to note that all the CPU time appears in the child process, which is where the shell and the command execute.
A thorough understanding of the UNIX System’s process control is essential for advanced programming.
There are only a few functions to master: fork, the exec family, _exit, wait, and waitpid.
The fork function also gave us an opportunity to look at race conditions.
Our examination of the system function and process accounting gave us another look at all these process control functions.
An understanding of the various user IDs and group IDs that are provided — real, effective, and saved—is critical to writing safe set-user-ID programs.
Given an understanding of a single process and its children, in the next chapter we examine the relationship of a process to other processes — sessions and job control.
We then complete our discussion of processes in Chapter 10 when we describe signals.
Because the stack frames corresponding to each function call are usually stored in the stack, and because after a vfork the child runs in the address space of the parent, what happens if the call to vfork is from a function other than main and the child does a return from this function after the vfork? Write a test program to verify this, and draw a picture of what’s happening.
Instead of calling pr_exit, determine the equivalent information from the siginfo structure.
But if we execute the program multiple times, one right after the other, as in.
What’s happening? How can we correct this? Can this problem happen if we let the child write its output first?
If we called execlp instead, specifying a filename of testinterp, and if the directory /home/sar/bin was a path prefix, what would be printed as argv[2] when the program is run?
Verify this as follows: call opendir for the root directory, peek at your system’s implementation of the DIR structure, and print the close-on-exec flag.
Then open the same directory for reading, and print the close-on-exec flag.
We learned in the previous chapter that there are relationships between processes.
First, every process has a parent process (the initial kernel-level process is usually its own parent)
The parent is notified when the child terminates, and the parent can obtain the child’s exit status.
We also mentioned process groups when we described the waitpid function (Section 8.6) and explained how we can wait for any process in a process group to terminate.
In this chapter, we’ll look at process groups in more detail and the concept of sessions that was introduced by POSIX.1
We’ll also look at the relationship between the login shell that is invoked for us when we log in and all the processes that we start from our login shell.
It is impossible to describe these relationships without talking about signals, and to talk about signals, we need many of the concepts in this chapter.
If you are unfamiliar with the UNIX System signal mechanism, you may want to skim through Chapter 10 at this point.
Let’s start by looking at the programs that are executed when we log in to a UNIX system.
In early UNIX systems, such as Version 7, users logged in using dumb terminals that were connected to the host with hard-wired connections.
The terminals were either local (directly connected) or remote (connected through a modem)
In either case, these logins came through a terminal device driver in the kernel.
A host had a fixed number of these terminal devices, so there was a known upper limit on the number of simultaneous logins.
As bitmapped graphical terminals became available, windowing systems were developed to provide users with new ways to interact with host computers.
Applications were developed to create ‘‘terminal windows’’ to emulate character-based terminals, allowing users to interact with hosts in familiar ways (i.e., via the shell command line)
Today, some platforms allow you to start a windowing system after logging in, whereas other platforms automatically start the windowing system for you.
In the latter case, you might still have to log in, depending on how the windowing system is configured (some windowing systems can be configured to log you in automatically)
The procedure that we now describe is used to log in to a UNIX system using a terminal.
The procedure is similar regardless of the type of terminal we use—it could be a character-based terminal, a graphical terminal emulating a simple character-based terminal, or a graphical terminal running a windowing system.
The BSD terminal login procedure has not changed much over the past 35 years.
The system administrator creates a file, usually /etc/ttys, that has one line per terminal device.
Each line specifies the name of the device and other parameters that are passed to the getty program.
One parameter is the baud rate of the terminal, for example.
When the system is bootstrapped, the kernel creates process ID 1, the init process, and it is init that brings the system up in multiuser mode.
The init process reads the file /etc/ttys and, for every terminal device that allows a login, does a fork followed by an exec of the program getty.
Figure 9.1 Processes invoked by init to allow terminal logins.
The init process also execs the getty program with an empty environment.
It is getty that calls open for the terminal device.
If the device is a modem, the open may delay inside the device driver until the modem is dialed and the call is answered.
Then getty outputs something like login: and waits for us to enter our user name.
If the terminal supports multiple speeds, getty can detect special characters that tell it to change the terminal’s speed (baud rate)
Consult your UNIX system manuals for additional details on the getty program and the data files (gettytab) that can drive its actions.
When we enter our user name, getty’s job is complete, and it then invokes the login program, similar to.
There can be options in the gettytab file to have it invoke other programs, but the default is the login program.
The -p flag to login tells it to preserve the environment that it is passed and to add to that environment, not replace it.
Figure 9.2 shows the state of these processes right after login has been invoked.
Figure 9.2 State of processes after login has been invoked.
All the processes shown in Figure 9.2 have superuser privileges, since the original init process has superuser privileges.
The process ID of the bottom three processes in Figure 9.2 is the same, since the process ID does not change across an exec.
Since it has our user name, it can call getpwnam to fetch our password file entry.
Then login calls getpass(3) to display the prompt Password: and read our password (with echoing disabled, of course)
It calls crypt(3) to encrypt the password that we entered and compares the encrypted.
This termination will be noticed by the parent (init), and it will do another fork followed by an exec of getty, starting the procedure over again for this terminal.
This is the traditional authentication procedure used on UNIX systems.
Modern UNIX systems, however, have evolved to support multiple authentication procedures.
If our application needs to verify that a user has the appropriate permission to perform a task, we can either hard code the authentication mechanism in the application or use the PAM library to give us the equivalent functionality.
The advantage to using PAM is that administrators can configure different ways to authenticate users for different tasks, based on the local site policies.
Change the ownership of our terminal device (chown) so we own it.
Change the access permissions for our terminal device so we have permission to.
Initialize the environment with all the information that login has: our home.
The minus sign as the first character of argv[0] is a flag to all the shells that indicates they are being invoked as a login shell.
The shells can look at this character and modify their start-up accordingly.
The login program really does more than we’ve described here.
It optionally prints the message-of-the-day file, checks for new mail, and performs other tasks.
In this chapter, we’re interested only in the features that we’ve described.
Recall from our discussion of the setuid function in Section 8.11 that since it is called by a superuser process, setuid changes all three user IDs: the real user ID, effective user ID, and saved set-user-ID.
The call to setgid that was done earlier by login has the same effect on all three group IDs.
Its parent process ID is the original init process (process ID 1), so when our login shell terminates, init is notified (it is sent a SIGCHLD signal) and it starts the whole procedure over again for this terminal.
Figure 9.3 Arrangement of processes after everything is set for a terminal login.
These start-up files usually change some of the environment variables and add many other variables to the environment.
For example, most users set their own PATH and often prompt for the actual terminal type (TERM)
When the start-up files are done, we finally get the shell’s prompt and can enter commands.
On Mac OS X, the terminal login process follows essentially the same steps as in the BSD login process, since Mac OS X is based in part on FreeBSD.
We are presented with a graphical-based login screen from the start.
The Linux login procedure is very similar to the BSD procedure.
Indeed, the Linux login command is derived from the 4.3BSD login command.
The main difference between the BSD login procedure and the Linux login procedure is in the way the terminal configuration is specified.
Some Linux distributions ship with a version of the init program that uses administrative files patterned after System V’s init file formats.
Depending on the version of getty in use, the terminal characteristics are specified either on the command line (as with agetty) or in the file /etc/gettydefs (as with mgetty)
Solaris supports two forms of terminal logins: (a) getty style, as described previously for BSD, and (b) ttymon logins, a feature introduced with SVR4
Normally, getty is used for the console, and ttymon is used for other terminal logins.
The ttymon command is part of a larger facility termed SAF, the Service Access Facility.
The goal of the SAF was to provide a consistent way to administer services that provide access to a system.
For our purposes, we end up with the same picture as in Figure 9.3, with a different set of steps between init and the login shell.
The ttymon program monitors all the terminal ports listed in its configuration file and does a fork when we enter our login name.
This child of ttymon does an exec of login, and login prompts us for our password.
Once this is done, login execs our login shell, and we’re at the position shown in Figure 9.3
One difference is that the parent of our login shell is now ttymon, whereas the parent of the login shell from a getty login is init.
The main (physical) difference between logging in to a system through a serial terminal and logging in to a system through a network is that the connection between the terminal and the computer isn’t point-to-point.
In this case, login is simply a service available, just like any other network service, such as FTP or SMTP.
With the terminal logins that we described in the previous section, init knows which terminal devices are enabled for logins and spawns a getty process for each device.
In the case of network logins, however, all the logins come through the kernel’s network interface drivers (e.g., the Ethernet driver), and we don’t know ahead of time how many of these will occur.
Instead of having a process waiting for each possible login, we now have to wait for a network connection request to arrive.
To allow the same software to process logins over both terminal logins and network logins, a software driver called a pseudo terminal is used to emulate the behavior of a serial terminal and map terminal operations to network operations, and vice versa.
In Chapter 19, we’ll talk about pseudo terminals in detail.
In BSD, a single process waits for most network connections: the inetd process, sometimes called the Internet superserver.
In this section, we’ll look at the sequence of processes involved in network logins for a BSD system.
We are not interested in the detailed network programming aspects of these processes; refer to Stevens, Fenner, and Rudoff [2004] for all the details.
As part of the system start-up, init invokes a shell that executes the shell script /etc/rc.
One of the daemons that is started by this shell script is inetd.
Once the shell script terminates, the parent process of inetd becomes init; inetd waits for TCP/IP connection requests to arrive at the host.
When a connection request arrives for it to handle, inetd does a fork and exec of the appropriate program.
Let’s assume that a TCP connection request arrives for the TELNET server.
A user on another host (that is connected to the server’s host through a network of some form) or on the same host initiates the login by starting the TELNET client:
The client opens a TCP connection to hostname, and the program that’s started on hostname is called the TELNET server.
The client and the server then exchange data across the TCP connection using the TELNET application protocol.
What has happened is that the user who started the client program is now logged in to the server’s host.
This assumes, of course, that the user has a valid account on the server’s host.
Figure 9.4 shows the sequence of processes involved in executing the TELNET server, called telnetd.
Figure 9.4 Sequence of processes involved in executing TELNET server.
The telnetd process then opens a pseudo terminal device and splits into two processes using fork.
The parent handles the communication across the network connection, and the child does an exec of the login program.
The parent and the child are connected through the pseudo terminal.
If we log in correctly, login performs the same steps we described in Section 9.2: it changes to our home directory and sets our group IDs, user ID, and our initial environment.
Then login replaces itself with our login shell by calling exec.
Figure 9.5 shows the arrangement of the processes at this point.
Figure 9.5 Arrangement of processes after everything is set for a network login.
Obviously, a lot is going on between the pseudo terminal device driver and the actual user at the terminal.
We’ll show all the processes involved in this type of arrangement in Chapter 19 when we talk about pseudo terminals in more detail.
We’ll see in the coming sections that this login shell is the start of a POSIX.1 session, and that the terminal or pseudo terminal is the controlling terminal for the session.
Logging in to a Mac OS X system over a network is identical to logging in to a BSD system, because Mac OS X is based partially on FreeBSD.
However, on Mac OS X, the telnet daemon is run from launchd.
By default, the telnet daemon is disabled on Mac OS X (although it can be enabled with the launchctl(1) command)
The preferred way to perform a network login on Mac OS X is with ssh, the secure shell command.
Network logins under Linux are the same as under BSD, except that some distributions use an alternative inetd process called the extended Internet services daemon, xinetd.
The xinetd process provides a finer level of control over services it starts compared to inetd.
The scenario for network logins under Solaris is almost identical to the steps under BSD and Linux.
An inetd server is used that is similar in concept to the BSD version, except that the Solaris version runs as a restarter in the Service Management Facility (SMF)
A restarter is a daemon that has the responsibility to start and monitor other daemon processes, and restart them if they fail.
Although the inetd server is started by the master restarter in the SMF, the master restarter is started by init and we end up with the same overall picture as in Figure 9.5
The Solaris Service Management Facility is a framework that manages and monitors system services and provides a way to recover from failures affecting system services.
In addition to having a process ID, each process belongs to a process group.
A process group is a collection of one or more processes, usually associated with the same job (job control is discussed in Section 9.8), that can receive signals from the same terminal.
Process group IDs are similar to process IDs: they are positive integers and can be stored in a pid_t data type.
The function getpgrp returns the process group ID of the calling process.
In older BSD-derived systems, the getpgrp function took a pid argument and returned the process group for that process.
The Single UNIX Specification defines the getpgid function that mimics this behavior.
If pid is 0, the process group ID of the calling process is returned.
The leader is identified by its process group ID being equal to its process ID.
It is possible for a process group leader to create a process group, create processes in the group, and then terminate.
The process group still exists, as long as at least one process is in the group, regardless of whether the group leader terminates.
This is called the process group lifetime—the period of time that begins when the group is created and ends when the last remaining process leaves the group.
The last remaining process in the process group can either terminate or enter some other process group.
A process joins an existing process group or creates a new process group by calling setpgid.
In the next section, we’ll see that setsid also creates a new process group.
This function sets the process group ID to pgid in the process whose process ID equals pid.
If the two arguments are equal, the process specified by pid becomes a process group leader.
If pid is 0, the process ID of the caller is used.
Also, if pgid is 0, the process ID specified by pid is used as the process group ID.
A process can set the process group ID of only itself or any of its children.
Furthermore, it can’t change the process group ID of one of its children after that child has called one of the exec functions.
In most job-control shells, this function is called after a fork to have the parent set the process group ID of the child, and to have the child set its own process group ID.
One of these calls is redundant, but by doing both, we are guaranteed that the child is placed into its own process group before either process assumes that this has happened.
If we didn’t do this, we would have a race condition, since the child’s process group membership would depend on which process executes first.
When we discuss signals, we’ll see how we can send a signal to either a single process (identified by its process ID) or a process group (identified by its process group ID)
Similarly, the waitpid function from Section 8.6 lets us wait for either a single process or one process from a specified process group.
A session is a collection of one or more process groups.
For example, we could have the arrangement shown in Figure 9.6
Here we have three process groups in a single session.
Figure 9.6 Arrangement of processes into process groups and sessions.
The processes in a process group are usually placed there by a shell pipeline.
For example, the arrangement shown in Figure 9.6 could have been generated by shell commands of the form.
A process establishes a new session by calling the setsid function.
If the calling process is not a process group leader, this function creates a new session.
The process becomes the session leader of this new session.
A session leader is the process that creates a session.
The process is the only process in this new session.
The process becomes the process group leader of a new process group.
The new process group ID is the process ID of the calling process.
If the process had a controlling terminal before calling setsid, that association is broken.
This function returns an error if the caller is already a process group leader.
To ensure this is not the case, the usual practice is to call fork and have the parent terminate and the child continue.
We are guaranteed that the child is not a process group leader, because the process group ID of the parent is inherited by the child, but the child gets a new process ID.
Hence, it is impossible for the child’s process ID to equal its inherited process group ID.
Obviously, a session leader is a single process that has a unique process ID, so we could talk about a session ID that is the process ID of the session leader.
This concept of a session ID was introduced in SVR4
Historically, BSD-based systems didn’t support this notion, but have since been updated to include it.
The getsid function returns the process group ID of a process’s session leader.
If pid is 0, getsid returns the process group ID of the calling process’s session leader.
For security reasons, some implementations may restrict the calling process from obtaining the process group ID of the session leader if pid doesn’t belong to the same session as the caller.
This is usually the terminal device (in the case of a terminal login) or pseudo terminal device (in the case of a network login) on which we log in.
The session leader that establishes the connection to the controlling terminal is called the controlling process.
The process groups within a session can be divided into a single foreground process group and one or more background process groups.
If a session has a controlling terminal, it has a single foreground process group and all other process groups in the session are background process groups.
Whenever we press the terminal’s interrupt key (often DELETE or Control-C), the interrupt signal is sent to all processes in the foreground process group.
If a modem (or network) disconnect is detected by the terminal interface, the hang-up signal is sent to the controlling process (the session leader)
Usually, we don’t have to worry about the controlling terminal; it is established automatically when we log in.
POSIX.1 leaves the choice of the mechanism used to allocate a controlling terminal up to each individual implementation.
BSD-based systems allocate the controlling terminal for a session when the session leader calls ioctl with a request argument of TIOCSCTTY (the third argument is a null pointer)
The session cannot already have a controlling terminal for this call to succeed.
Normally, this call to ioctl follows a call to setsid, which guarantees that the process is a session leader without a controlling terminal.
Figure 9.8 summarizes the way each platform discussed in this book allocates a controlling terminal.
Note that although Mac OS X 10.6.8 is derived from BSD, it behaves like System V when allocating a controlling terminal.
There are times when a program wants to talk to the controlling terminal, regardless of whether the standard input or standard output is redirected.
The way a program guarantees that it is talking to the controlling terminal is to open the file /dev/tty.
This special file is a synonym within the kernel for the controlling terminal.
Naturally, if the program doesn’t have a controlling terminal, the open of this device will fail.
The classic example is the getpass(3) function, which reads a password (with terminal echoing turned off, of course)
This function is called by the crypt(1) program and can be used in a pipeline.
Because crypt reads its input file on its standard input, the standard input can’t be used to enter the password.
Also, crypt is designed so that we have to enter the encryption password each time we run the program, to prevent us from saving the password in a file (which could be a security hole)
There are known ways to break the encoding used by the crypt program.
We need a way to tell the kernel which process group is the foreground process group, so that the terminal device driver knows where to send the terminal input and the terminal-generated signals (Figure 9.7)
The function tcgetpgrp returns the process group ID of the foreground process group associated with the terminal open on fd.
If the process has a controlling terminal, the process can call tcsetpgrp to set the foreground process group ID to pgrpid.
The value of pgrpid must be the process group ID of a process group in the same session, and fd must refer to the controlling terminal of the session.
The tcgetsid function allows an application to obtain the process group ID for the session leader given a file descriptor for the controlling TTY.
Applications that need to manage controlling terminals can use tcgetsid to identify the session ID of the controlling terminal’s session leader (which is equivalent to the session leader’s process group ID)
This feature allows us to start multiple jobs (groups of processes) from a single terminal and to control which jobs can access the terminal and which jobs are run in the background.
SVR3 provided a different form of job control called shell layers.
The BSD form of job control, however, was selected by POSIX.1 and is what we describe here.
In earlier versions of the standard, job control support was optional, but POSIX.1 now requires platforms to support it.
From our perspective, when using job control from a shell, we can start a job in either the foreground or the background.
A job is simply a collection of processes, often a pipeline of processes.
All the processes invoked by these background jobs are in the background.
As we said, to use the features provided by job control, we need to use a shell that supports job control.
With older systems, it was simple to say which shells supported job control and which didn’t.
The C shell supported job control, the Bourne shell didn’t, and it was an option with the Korn shell, depending on whether the host supported job control.
But the C shell has been ported to systems (e.g., earlier versions of System V) that don’t support job control, and the SVR4 Bourne shell, when invoked by the name jsh instead of sh, supports job control.
We’ll just talk generically about a shell that supports job control, versus one that doesn’t, when the difference between the various shells doesn’t matter.
When we start a background job, the shell assigns it a job identifier and prints one or more of the process IDs.
The following script shows how the Korn shell handles this:
When the jobs are done and we press RETURN, the shell tells us that the jobs are complete.
The reason we have to press RETURN is to have the shell print its prompt.
The shell doesn’t print the changed status of background jobs at any random time—only right before it prints its prompt, to let us enter a new command line.
If the shell didn’t do this, it could produce output while we were entering an input line.
The interaction with the terminal driver arises because a special terminal character affects the foreground job: the suspend key (typically Control-Z)
Entering this character causes the terminal driver to send the SIGTSTP signal to all processes in the foreground process group.
The terminal driver looks for three special characters, which generate signals to the foreground process group.
In Chapter 18, we’ll see how we can change these three characters to be any characters we choose and how we can disable the terminal driver’s processing of these special characters.
Another job control condition can arise that must be handled by the terminal driver.
Since we can have a foreground job and one or more background jobs, which of these receives the characters that we enter at the terminal? Only the foreground job receives terminal input.
It is not an error for a background job to try to read from the terminal, but the terminal driver detects this and sends a special signal to the background job: SIGTTIN.
This signal normally stops the background job; by using the shell, we are notified of this event and can bring the job into the foreground so that it can read from the terminal.
Note that this example doesn’t work on Mac OS X 10.6.8
When we try to bring the cat command into the foreground, the read fails with errno set to EINTR.
The shell starts the cat process in the background, but when cat tries to read its standard input (the controlling terminal), the terminal driver, knowing that it is a background job, sends the SIGTTIN signal to the background job.
The shell detects this change in status of its child (recall our discussion of the wait and waitpid function in Section 8.6) and tells us that the job has been stopped.
We then move the stopped job into the foreground with the shell’s fg command.
Refer to the manual page for the shell that you are using for all the details on its job control commands, such as fg and bg, and the various ways to identify the different jobs.
Doing this causes the shell to place the job into the foreground process group (tcsetpgrp) and send the continue signal (SIGCONT) to the process group.
Since it is now in the foreground process group, the job can read from the controlling terminal.
What happens if a background job sends its output to the controlling terminal? This is an option that we can allow or disallow.
Normally, we use the stty(1) command to change this option.
We’ll see in Chapter 18 how we can change this option from a program.
When we disallow background jobs from writing to the controlling terminal, cat will block when it tries to write to its standard output, because the terminal driver identifies the write as coming from a background process and sends the job the SIGTTOU signal.
As with the previous example, when we use the shell’s fg command to bring the job into the foreground, the job completes.
Figure 9.9 summarizes some of the features of job control that we’ve been describing.
The solid lines through the terminal driver box mean that the terminal I/O and the terminal-generated signals are always connected from the foreground process.
Figure 9.9 Summary of job control features with foreground and background jobs, and terminal driver.
The dashed line corresponding to the SIGTTOU signal means that whether the output from a process in the background process group appears on the terminal is an option.
Is job control necessary or desirable? Job control was originally designed and implemented before windowing terminals were widespread.
Some people claim that a well-designed windowing system removes any need for job control.
Some complain that the implementation of job control — requiring support from the kernel, the terminal.
Some use job control with a windowing system, claiming a need for both.
Regardless of your opinion, job control is a required feature of POSIX.1
Let’s examine how the shells execute programs and how this relates to the concepts of process groups, controlling terminals, and sessions.
First, we’ll use a shell that doesn’t support job control — the classic Bourne shell running on Solaris.
The parent of the ps command is the shell, which we would expect.
Both the shell and the ps command are in the same session and foreground process group (949)
We say that 949 is the foreground process group because that is what you get when you execute a command with a shell that doesn’t support job control.
Some platforms support an option to have the ps(1) command print the process group ID associated with the session’s controlling terminal.
Unfortunately, the output of the ps command often differs among versions of the UNIX System.
This shell doesn’t know about job control, so the background job is not put into its own process group and the controlling terminal isn’t taken away from the background job.
Now let’s look at how the Bourne shell handles a pipeline.
The program cat1 is just a copy of the standard cat program, with a different name.
We have another copy of cat with the name cat2, which we’ll use later in this section.
When we have two copies of cat in a pipeline, the different names let us differentiate between the two programs.
Note that the last process in the pipeline is the child of the shell and that the first process in the pipeline is a child of the last process.
It appears that the shell forks a copy of itself and that this copy then forks to make each of the previous processes in the pipeline.
Since the shell doesn’t handle job control, the process group ID of the background processes remains 949, as does the process group ID of the session.
What happens in this case if a background process tries to read from its controlling terminal? For example, suppose that we execute.
With job control, this is handled by placing the background job into a background process group, which causes the signal SIGTTIN to be generated if the background job tries to read from the controlling terminal.
The way this is handled without job control is that the shell automatically redirects the standard input of a background process to /dev/null, if the process doesn’t redirect standard input itself.
This means that our background cat process immediately reads an end of file and terminates normally.
The previous paragraph adequately handles the case of a background process accessing the controlling terminal through its standard input, but what happens if a background process specifically opens /dev/tty and reads from the controlling terminal? The answer is ‘‘It depends,’’ but the result is probably not what we want.
We run it in the background, but the crypt program opens /dev/tty, changes the terminal characteristics (to disable echoing), reads from the device, and resets the terminal characteristics.
When we execute this background pipeline, the prompt Password: from crypt is printed on the terminal, but what we enter (the encryption password) is read by the shell, which tries to execute a command.
The next line we enter to the shell is taken as the password, and the file is not encrypted correctly, sending junk to the printer.
Here we have two processes trying to read from the same device at the same time, and the result depends on the system.
Job control, as we described earlier, handles this multiplexing of a single terminal between multiple processes in a better fashion.
Returning to our Bourne shell example, if we execute three processes in the pipeline, we can examine the process control used by this shell:
Don’t be alarmed if the output on your system doesn’t show the proper command names.
What’s happening here is that the ps process is racing with the shell, which is forking and executing the cat commands.
In this case, the shell hasn’t yet completed the call to exec when ps has obtained the list of processes to print.
Again, the last process in the pipeline is the child of the shell, and all previous processes in the pipeline are children of the last process.
Since the last process in the pipeline is the child of the login shell, the shell is notified when that process (cat2) terminates.
Now let’s examine the same examples using a job-control shell running on Linux.
We’ll use the Bourne-again shell in this example; the results with other job-control shells are almost identical.
Starting with this example, we show the foreground process group in a bolder font.
We immediately see a difference from our Bourne shell example.
The Bourne-again shell places the foreground job (ps) into its own process group (5796)
The ps command is the process group leader and the only process in this process group.
Furthermore, this process group is the foreground process group, since it has the controlling terminal.
Our login shell is a background process group while the ps command executes.
Indeed, we’ll see that the session never changes through our examples in this section.
Again, the ps command is placed into its own process group, but this time the process group (5797) is no longer the foreground process group — it is a background process group.
The TPGID of 2837 indicates that the foreground process group is our login shell.
We can also see another difference between this example and the similar Bourne shell example.
The Bourne shell created the last process in the pipeline first, and this final process was the parent of the first process.
Here, the Bourne-again shell is the parent of both processes.
Note that the order in which a shell creates processes can differ depending on the particular shell in use.
We’ve mentioned that a process whose parent terminates is called an orphan and is inherited by the init process.
We now look at entire process groups that can be orphaned and see how POSIX.1 handles this situation.
Example Consider a process that forks a child and then terminates.
Although this is nothing abnormal (it happens all the time), what happens if the child is stopped (using job control) when the parent terminates? How will the child ever be continued, and does the child know that it has been orphaned? Figure 9.11 shows this situation: the parent process has forked a child that stops, and the parent is about to exit.
Figure 9.11 Example of a process group about to be orphaned.
The program that creates this situation is shown in Figure 9.13
The child inherits the process group of its parent (6099)
This is our (imperfect) way of letting the child execute before the parent terminates.
The child establishes a signal handler for the hang-up signal (SIGHUP) so we can see whether it is sent to the child.
The child sends itself the stop signal (SIGTSTP) with the kill function.
This stops the child, similar to our stopping a foreground job with our terminal’s suspend character (Control-Z)
When the parent terminates, the child is orphaned, so the child’s parent process ID becomes 1, which is the init process ID.
At this point, the child is now a member of an orphaned process group.
The POSIX.1 definition of an orphaned process group is one in which the parent of every member is either itself a member of the group or is not a member of the group’s session.
Another way of saying this is that the process group is not orphaned as long as a process in the group has a parent in a different process group but in the same session.
If the process group is not orphaned, there is a chance that one of those parents in a different process group but in the same session will restart a stopped process in the process group that is not orphaned.
Since the process group is orphaned when the parent terminates, and the process group contains a stopped process, POSIX.1 requires that every process in the newly orphaned process group be sent the hang-up signal (SIGHUP) followed by the continue signal (SIGCONT)
This causes the child to be continued, after processing the hang-up signal.
The default action for the hang-up signal is to terminate the process, so we have to provide a signal handler to catch the signal.
Here is the output from the program shown in Figure 9.13:
Note that our shell prompt appears with the output from the child, since two processes — our login shell and the child—are writing to the terminal.
After calling pr_ids in the child, the program tries to read from standard input.
As we saw earlier in this chapter, when a process in a background process group tries to read from its controlling terminal, SIGTTIN is generated for the background process group.
But here we have an orphaned process group; if the kernel were to stop it with this signal, the processes in the process group would probably never be continued.
Finally, note that our child was placed in a background process group when the parent terminated, since the parent was executed as a foreground job by the shell.
We’ll see another example of orphaned process groups in Section 19.5 with the pty program.
Having talked about the various attributes of a process, process group, session, and controlling terminal, it’s worth looking at how all this can be implemented.
Figure 9.13 shows the various data structures used by FreeBSD.
Let’s look at all the fields that we’ve labeled, starting with the session structure.
One of these structures is allocated for each session (e.g., each time setsid is called)
When this counter is decremented to 0, the structure can be freed.
Recall that the concept of a session ID is not part of the.
When setsid is called, a new session structure is allocated within the kernel.
The kernel contains one of these structures for each terminal device and each pseudo terminal device.
Note that the tty structure points to the session structure, and vice versa.
This pointer is used by the terminal to send a hangup signal to the session leader if the terminal loses carrier (Figure 9.7)
This field is used by the terminal driver to send signals to the foreground process group.
The three signals generated by entering special characters (interrupt, quit, and suspend) are sent to the foreground process group.
When the size of the terminal window changes, the SIGWINCH signal is sent to the foreground process group.
We show how to set and fetch the terminal’s current window size in Section 18.12
The pgrp structure contains the information for a particular process group.
The p_pglist structure in that proc structure is a doubly linked list entry that points to both the next process and the previous process in the group, and so on, until a null pointer is encountered in the proc structure of the last process in the group.
The proc structure contains all the information for a single process.
This structure is allocated when the controlling terminal device is opened.
All references to /dev/tty in a process go through this vnode structure.
This chapter has described the relationships between groups of processes — sessions, which are made up of process groups.
Job control is a feature supported by most UNIX systems today, and we’ve described how it’s implemented by a shell that supports job control.
The controlling terminal for a process, /dev/tty, is also involved in these process relationships.
We’ve made numerous references to the signals that are used in all these process relationships.
The next chapter continues the discussion of signals, looking at all the UNIX System signals in detail.
Signals provide a way of handling asynchronous events—for example, a user at a terminal typing the interrupt key to stop a program or the next program in a pipeline terminating prematurely.
Signals have been provided since the early versions of the UNIX System, but the signal model provided with systems such as Version 7 was not reliable.
Signals could get lost, and it was difficult for a process to turn off selected signals when executing critical regions of code.
But the changes made by Berkeley and AT&T were incompatible.
Fortunately, POSIX.1 standardized the reliable-signal routines, and that is what we describe here.
In this chapter, we start with an overview of signals and a description of what each signal is normally used for.
It is often important to understand what is wrong with an implementation before seeing how to do things correctly.
This chapter contains numerous examples that are not entirely correct and a discussion of the defects.
For example, SIGABRT is the abort signal that is generated when a process calls the abort function.
Signal names are all defined by positive integer constants (the signal number) in the header <signal.h>
Implementations actually define the individual signals in a different header file, but this header file is included by <signal.h>
It is considered bad form for the kernel to include header files meant for user-level applications, so if the applications and the kernel both need the same definitions, the information is placed in a kernel header file that is then included by the user-level header file.
The terminal-generated signals occur when users press certain terminal keys.
Pressing the DELETE key on the terminal (or Control-C on many systems) normally causes the interrupt signal (SIGINT) to be generated.
We’ll see in Chapter 18 how this signal can be mapped to any character on the terminal.
Hardware exceptions generate signals: divide by 0, invalid memory reference, and the like.
These conditions are usually detected by the hardware, and the kernel is notified.
The kernel then generates the appropriate signal for the process that was running at the time the condition occurred.
For example, SIGSEGV is generated for a process that executes an invalid memory reference.
The kill(2) function allows a process to send any signal to another process or process group.
Naturally, there are limitations: we have to be the owner of the process that we’re sending the signal to, or we have to be the superuser.
The kill(1) command allows us to send signals to other processes.
This program is just an interface to the kill function.
This command is often used to terminate a runaway background process.
Software conditions can generate signals when a process should be notified of various events.
These aren’t hardware-generated conditions (as is the divideby-0 condition), but software conditions.
Examples are SIGURG (generated when out-of-band data arrives over a network connection), SIGPIPE (generated when a process writes to a pipe that has no reader), and SIGALRM (generated when an alarm clock set by the process expires)
They occur at what appear to be random times to the process.
The process can’t simply test a variable (such as errno) to see whether a signal has occurred; instead, the process has to tell the kernel ‘‘if and when this signal occurs, do the following.’’
We can tell the kernel to do one of three things when a signal occurs.
We call this the disposition of the signal, or the action associated with a signal.
This works for most signals, but two signals can never be ignored: SIGKILL and SIGSTOP.
The reason these two signals can’t be ignored is to provide the kernel and the superuser with a surefire way of either killing or stopping any process.
Also, if we ignore some of the signals that are generated by a hardware exception (such as illegal memory reference or divide by 0), the behavior of the process is undefined.
To do this, we tell the kernel to call a function of ours whenever the signal occurs.
In our function, we can do whatever we want to handle the condition.
If we’re writing a command interpreter, for example, when the user generates the interrupt signal at the keyboard, we probably want to return to the main loop of the program, terminating whatever command we were executing for the user.
If the SIGCHLD signal is caught, it means that a child process has terminated, so the signal-catching function can call waitpid to fetch the child’s process ID and termination status.
As another example, if the process has created temporary files, we may want to write a signal-catching function for the SIGTERM signal (the termination signal that is the default signal sent by the kill command) to clean up the temporary files.
Note that the two signals SIGKILL and SIGSTOP can’t be caught.
Note that the default action for most signals is to terminate the process.
Figure 10.1 lists the names of all the signals, an indication of which systems support the signal, and the default action for the signal.
Because the file is named core, it shows how long this feature has been part of the UNIX System.
This file can be used with most UNIX System debuggers to examine the state of the process at the time it terminated.
The generation of the core file is an implementation feature of most versions of the UNIX System.
On FreeBSD 8.0, for example, the core file is named cmdname.core, where cmdname is the name of the command corresponding to the process that received the signal.
On Mac OS X 10.6.8, the core file is named core.pid, where pid is the ID of the process that received the signal.
These systems allow the core filename to be configured via a sysctl parameter.
Most implementations leave the core file in the current working directory of the corresponding process; Mac OS X places all core files in /cores instead.
The core file will not be generated if (a) the process was set-user-ID and the current user is not the owner of the program file, (b) the process was set-group-ID and the current user is not the group owner of the file, (c) the user does not have permission to write in the current working directory, (d) the file already exists and the user does not have permission to write to it, or (e) the file is too big (recall the RLIMIT_CORE limit in Section 7.11)
The permissions of the core file (assuming that the file doesn’t already exist) are usually user-read and user-write, although Mac OS X sets only user-read.
Many of these names are taken from the original PDP-11 implementation of the UNIX System.
Check your system’s manuals to determine exactly which type of error these signals correspond to.
We now describe each of these signals in more detail.
This signal is also generated when an interval timer set by the setitimer(2) function expires.
Implementations usually generate this signal on certain types of memory faults, as we describe in Section 14.8
By default, this signal is ignored, so the parent must catch this signal if it wants to be notified whenever a child’s status changes.
The normal action in the signal-catching function is to call one of the wait functions to fetch the child’s process ID and termination status.
Earlier releases of System V had a similar signal named SIGCLD (without the H)
The semantics of this signal were different from those of other signals, and as far back as SVR2, the manual page strongly discouraged its use in new programs.
Applications should use the standard SIGCHLD signal, but be aware that many systems define SIGCLD to be the same as SIGCHLD for backward compatibility.
If you maintain software that uses SIGCLD, you need to check your system’s manual page to see which semantics it follows.
The default action is to continue a stopped process, but to ignore the signal if the process wasn’t stopped.
A full-screen editor, for example, might catch this signal and use the signal handler to make a note to redraw the terminal screen.
On Linux, for example, SIGEMT is supported only for selected architectures, such as SPARC, MIPS, and PA-RISC.
It is used to notify processes that need to take special action before freezing the system state, such as might happen when a system goes into hibernation or suspended mode.
This signal is generated for this condition only if the terminal’s CLOCAL flag is not set.
The CLOCAL flag for a terminal is set if the attached terminal is local.
The flag tells the terminal driver to ignore all modem status lines.
Note that the session leader that receives this signal may be in the background; see Figure 9.7 for an example.
This differs from the normal terminal-generated signals (interrupt, quit, and suspend), which are always delivered to the foreground process group.
This signal is also generated if the session leader terminates.
In this case, the signal is sent to each process in the foreground process group.
This signal is commonly used to notify daemon processes (Chapter 13) to reread their configuration files.
The reason SIGHUP is chosen for this task is that a daemon should not have a controlling terminal and would normally never receive this signal.
This signal is sent to all processes in the foreground process group (refer to Figure 9.9)
This signal normally causes status information on processes in the foreground process group to be displayed on the terminal.
Linux doesn’t provide support for SIGINFO, although the symbol is defined to be the same value as SIGPWR on the Alpha platform.
This is most likely to provide some level of compatibility with software developed for OSF/1
This signal is sent to all processes in the foreground process group (refer to Figure 9.9)
This signal is often used to terminate a runaway program, especially when it’s generating a lot of unwanted output on the screen.
Under System V, SIGIO is identical to SIGPOLL, so its default action is to terminate the process.
Earlier versions of System V generated this signal from the abort function.
SIGJVM1 A signal reserved for use by the Java virtual machine on Solaris.
SIGJVM2 Another signal reserved for use by the Java virtual machine on Solaris.
On FreeBSD, SIGLWP is defined to be an alias for SIGTHR.
This signal is also generated when a process writes to a socket of type SOCK_STREAM that is no longer connected.
It can be generated when a specific event occurs on a pollable device.
We describe this signal with the poll function in Section 14.4.2
On Linux and Solaris, SIGPOLL is defined to have the same value as SIGIO.
This signal is generated when a profiling interval timer set by the setitimer(2) function expires.
Its main use is on a system that has an uninterruptible power supply (UPS)
If power fails, the UPS takes over and the software can usually be notified.
Nothing needs to be done at this point, as the system continues running on battery power.
But if the battery gets low (for example, if the power is off for an extended period), the software is usually notified again; at this point, it behooves the system to shut everything down.
On most systems, the process that is notified of the low-battery condition sends the SIGPWR signal to the init process, and init handles the system shutdown.
Solaris 10 and some Linux distributions have entries in the inittab file for this purpose: powerfail and powerwait (or powerokwait)
This signal is sent to all processes in the foreground process group (refer to Figure 9.9)
This signal not only terminates the foreground process group (as does SIGINT), but also generates a core file.
It showed up in the earliest versions of Linux, where it was intended to be used for stack faults taken by the math coprocessor.
This signal is not generated by the kernel, but remains for backward compatibility.
It is similar to the interactive stop signal (SIGTSTP), but SIGSTOP cannot be caught or ignored.
Somehow, the process executed a machine instruction that the kernel thought was a system call, but the parameter with the instruction that indicates the type of system call was invalid.
This might happen if you build a program that uses a new system call and you then try to run the same binary on an older version of the operating system where the system call doesn’t exist.
Because it can be caught by applications, using SIGTERM gives programs a chance to terminate gracefully by cleaning up before exiting (in contrast to SIGKILL, which can’t be caught or ignored)
It is defined to have the same value as SIGLWP.
Implementations often use this signal to transfer control to a debugger when a breakpoint instruction is executed.
This signal is sent to all processes in the foreground process group (refer to Figure 9.9)
When discussing job control and signals, we talk about stopping and continuing jobs.
The terminal driver, however, has historically used the term stop to refer to stopping and starting the terminal output using the Control-S and Control-Q characters.
Therefore, the terminal driver calls the character that generates the interactive stop signal the suspend character, not the stop character.
Refer to the discussion of this topic in Section 9.8
As special cases, if either (a) the reading process is ignoring or blocking this signal or (b) the process group of the reading process is orphaned, then the signal is not generated; instead, the read operation fails with errno set to EIO.
Unlike the case with background reads, a process can choose to allow background writes to the controlling terminal.
If background writes are not allowed, then like the SIGTTIN signal, there are two special cases: if either (a) the writing process is ignoring or blocking this signal or (b) the process group of the writing process is orphaned, then the signal is not generated; instead, the write operation returns an error with errno set to EIO.
Regardless of whether background writes are allowed, certain terminal operations (other than writing) can also generate the SIGTTOU signal.
It is optionally generated when out-of-band data is received on a network connection.
SIGUSR1 This is a user-defined signal, for use in application programs.
A process can get and set the window size with the ioctl function, which we describe in Section 18.12
If a process changes the window size from its previous value using the ioctl set-window-size command, the kernel generates the SIGWINCH signal for the foreground process group.
If the process exceeds its soft CPU time limit, the SIGXCPU signal is generated.
The Single UNIX Specification requires that the default action be to terminate the process abnormally.
Whether a core file is generated is left up to the implementation.
Just as with SIGXCPU, the default action taken with SIGXFSZ depends on the operating system.
The Single UNIX Specification requires that the default action be to terminate the process abnormally.
Whether a core file is generated is left up to the implementation.
It is optionally used to notify processes that have exceeded a preconfigured resource value.
The Solaris resource control mechanism is a general facility for controlling the use of shared resources among independent application sets.
Returns: previous disposition of signal (see following) if OK, SIG_ERR on error.
The signal function is defined by ISO C, which doesn’t involve multiple processes, process groups, terminal I/O, and the like.
Therefore, its definition of signals is vague enough to be almost useless for UNIX systems.
Implementations derived from UNIX System V support the signal function, but it provides the old unreliable-signal semantics.
The signal function provides backward compatibility for applications that require the older semantics.
Most current systems follow this strategy, but Solaris 10 follows the System V semantics for the signal function.
Because the semantics of signal differ among implementations, we must use the sigaction function instead.
We provide an implementation of signal that uses sigaction in Section 10.14
All the examples in this text use the signal function from Figure 10.18 to give us consistent semantics regardless of which particular platform we use.
The signo argument is just the name of the signal from Figure 10.1
If we specify SIG_IGN, we are telling the system to ignore the signal.
Remember that we cannot ignore the two signals SIGKILL and SIGSTOP.
When we specify the address of a function to be called when the signal occurs, we are arranging to ‘‘catch’’ the signal.
We call the function either the signal handler or the signal-catching function.
The prototype for the signal function states that the function requires two arguments and returns a pointer to a function that returns nothing (void)
The second argument is a pointer to a function that takes a single integer argument and returns nothing.
The function whose address is returned as the value of signal takes a single integer argument (the final (int))
In plain English, this declaration says that the signal handler is passed a single integer argument (the signal number) and that it returns nothing.
When we call signal to establish the signal handler, the second argument is a pointer to the function.
The return value from signal is the pointer to the previous signal handler.
The perplexing signal function prototype shown at the beginning of this section can be made much simpler through the use of the following typedef [Plauger 1992]:
We’ve included this typedef in apue.h (Appendix B) and use it with the functions in this chapter.
If we examine the system’s header <signal.h>, we will probably find declarations of the form.
Figure 10.2 shows a simple signal handler that catches either of the two user-defined signals and prints the signal number.
In Section 10.10, we describe the pause function, which simply suspends the calling process until a signal is received.
We invoke the program in the background and use the kill(1) command to send it signals.
Note that the term kill in the UNIX System is a misnomer.
Whether that signal terminates the process depends on which signal is sent and whether the process has arranged to catch the signal.
When we send the SIGTERM signal, the process is terminated, since it doesn’t catch the signal, and the default action for the signal is termination.
When a program is executed, the status of all signals is either default or ignore.
Normally, all signals are set to their default action, unless the process that calls exec is ignoring the signal.
Specifically, the exec functions change the disposition of any signals being caught to their default action and leave the status of all other signals alone.
Naturally, a signal that is being caught by a process that calls exec cannot be caught by the same function in the new program, since the address of the signalcatching function in the caller probably has no meaning in the new program file that is executed.
One specific example of this signal status behavior is how an interactive shell treats the interrupt and quit signals for a background process.
With a shell that doesn’t support job control, when we execute a process in the background, as in.
This is done so that if we type the interrupt character, it doesn’t affect the background process.
If this weren’t done and we typed the interrupt character, it would terminate not only the foreground process, but also all the background processes.
Many interactive programs that catch these two signals have code that looks like.
Following this approach, the process catches the signal only if the signal is not currently being ignored.
These two calls to signal also show a limitation of the signal function: we are not able to determine the current disposition of a signal without changing the disposition.
We’ll see later in this chapter how the sigaction function allows us to determine a signal’s disposition without changing it.
When a process calls fork, the child inherits the parent’s signal dispositions.
Here, since the child starts off with a copy of the parent’s memory image, the address of a signal-catching function has meaning in the child.
In earlier versions of the UNIX System (such as Version 7), signals were unreliable.
By this we mean that signals could get lost: a signal could occur and the process would never know about it.
Also, a process had little control over a signal: a process could catch the signal or ignore it.
Sometimes, we would like to tell the kernel to block a signal: don’t ignore it, just remember if it occurs, and tell us later when we’re ready.
Changes were made with 4.2BSD to provide what are called reliable signals.
A different set of changes was then made in SVR3 to provide reliable signals under System V.
One problem with these early versions was that the action for a signal was reset to its default each time the signal occurred.
In the previous example, when we ran the program in Figure 10.2, we avoided this detail by catching each signal only once.
The classic example from programming books that described these earlier systems concerns how to handle the interrupt signal.
The reason the signal handler is declared as returning an integer is that these early systems didn’t support the ISO C void data type.
The problem with this code fragment is that there is a  window of time—after the signal has occurred, but before the call to signal in the signal handler—when the interrupt signal could occur another time.
This is one of those conditions that works correctly most of the time, causing us to think that it is correct, when it isn’t.
Another problem with these earlier systems was that the process was unable to turn a signal off when it didn’t want the signal to occur.
There are times when we would like to tell the system ‘‘prevent the following signals from interrupting me, but remember if they do occur.’’ The classic example that demonstrates this flaw is shown by a piece of code that catches a signal and sets a flag for the process that indicates that the signal occurred:
Here, the process is calling the pause function to put it to sleep until a signal is caught.
When the signal is caught, the signal handler just sets the flag sig_int_flag to a nonzero value.
The process is automatically awakened by the kernel after the signal handler returns, notices that the flag is nonzero, and does whatever it needs to do.
But there is a window of time when things can go wrong.
If the signal occurs after the test of sig_int_flag but before the call to pause, the process could go to sleep forever (assuming that the signal is never generated again)
This is another example of some code that isn’t right, yet it works most of the time.
A characteristic of earlier UNIX systems was that if a process caught a signal while the process was blocked in a ‘‘slow’’ system call, the system call was interrupted.
The system call returned an error and errno was set to EINTR.
This was done under the assumption that since a signal occurred and the process caught it, there is a good chance that something has happened that should wake up the blocked system call.
Here, we have to differentiate between a system call and a function.
It is a system call within the kernel that is interrupted when a signal is caught.
To support this feature, the system calls are divided into two categories: the ‘‘slow’’ system calls and all the others.
The slow system calls are those that can block forever.
Reads that can block the caller forever if data isn’t present with certain file types (pipes, terminal devices, and network devices)
Writes that can block the caller forever if the data can’t be accepted immediately by these same file types.
Opens on certain file types that block the caller until some condition occurs (such as a terminal device open waiting until an attached modem answers the phone)
The pause function (which by definition puts the calling process to sleep until a signal is caught) and the wait function.
The notable exception to these slow system calls is anything related to disk I/O.
Although a read or a write of a disk file can block the caller temporarily (while the disk driver queues the request and then the request is executed), unless a hardware error occurs, the I/O operation always returns and unblocks the caller quickly.
One condition that is handled by interrupted system calls, for example, is when a process initiates a read from a terminal device and the user at the terminal walks away from the terminal for an extended period.
In this example, the process could be blocked for hours or days and would remain so unless the system was taken down.
Earlier versions gave implementations a choice of how to deal with reads and writes that have processed partial amounts of data.
If read has received and transferred data to an application’s buffer, but has not yet received all that the application requested and is then interrupted, the operating system could either fail the system call, with errno set to EINTR, or allow the system call to succeed, returning the partial amount of data received.
Similarly, if write is interrupted after transferring some of the data in an application’s buffer, the operation system could either fail the system call, with errno set to EINTR, or allow the system call to succeed, returning the partial amount of data written.
Historically, implementations derived from System V fail the system call, whereas BSD-derived implementations return partial success.
The problem with interrupted system calls is that we now have to handle the error return explicitly.
The typical code sequence (assuming a read operation and assuming that we want to restart the read even if it’s interrupted) would be.
To prevent applications from having to handle interrupted system calls, 4.2BSD introduced the automatic restarting of certain interrupted system calls.
The system calls that were automatically restarted are ioctl, read, readv, write, writev, wait, and waitpid.
As we’ve mentioned, the first five of these functions are interrupted by a signal only if they are operating on a slow device; wait and waitpid are always interrupted when a signal is caught.
Since this caused a problem for some applications that didn’t want the operation restarted if it was interrupted, 4.3BSD allowed the process to disable this feature on a per-signal basis.
As we will see in Section 10.14, this flag is used with the sigaction function to allow applications to request that interrupted system calls be restarted.
Historically, when using the signal function to establish a signal handler, implementations varied with respect to how interrupted system calls were handled.
BSD, in contrast, restarted them if the calls were interrupted by signals.
The default on Solaris 10, however, is to return an error (EINTR) instead when system calls are interrupted by signal handlers installed with the signal function.
By using our own implementation of the signal function (shown in Figure 10.18), we avoid having to deal with these differences.
One of the reasons 4.2BSD introduced the automatic restart feature is that sometimes we don’t know that the input or output device is a slow device.
If the program we write can be used interactively, then it might be reading or writing a slow device, since terminals fall into this category.
If we catch signals in this program, and if the system doesn’t provide the restart capability, then we have to test every read or write for the interrupted error return and reissue the read or write.
Figure 10.3 summarizes the signal functions and their semantics provided by the various implementations.
Be aware that UNIX systems from other vendors can have values different from those shown in this figure.
In Figure 10.18, we provide our own version of the signal function that automatically tries to restart interrupted system calls (other than for the SIGALRM signal)
We talk more about interrupted system calls in Section 14.4 with regard to the select and poll functions.
When a signal that is being caught is handled by a process, the normal sequence of instructions being executed by the process is temporarily interrupted by the signal handler.
The process then continues executing, but the instructions in the signal handler are now executed.
If the signal handler returns (instead of calling exit or longjmp, for example), then the normal sequence of instructions that the process was executing when the signal was caught continues executing.
This is similar to what happens when a hardware interrupt occurs.
But in the signal handler, we can’t tell where the process was executing when the signal was caught.
What if the process was in the middle of allocating additional memory on its heap using malloc, and we call malloc from the signal handler? Or, what if the process was in the middle of a call to a function, such as getpwnam (Section 6.2), that stores its result in a static location, and we call the same function from the signal handler? In the malloc example, havoc can result for the process, since malloc usually maintains a linked list of all its allocated areas, and it may have been in the middle of changing this list.
In the case of getpwnam, the information returned to the normal caller can get overwritten with the information returned to the signal handler.
The Single UNIX Specification specifies the functions that are guaranteed to be safe to call from within a signal handler.
These functions are reentrant and are called async-signal safe by the Single UNIX Specification.
Besides being reentrant, they block any signals during operation if delivery of a signal might cause inconsistencies.
Most of the functions that are not included in Figure 10.4 are missing because (a) they are known to use static data structures, (b) they call malloc or free, or (c) they are part of the standard I/O library.
Most implementations of the standard I/O library use global data structures in a nonreentrant way.
Note that even though we call printf from signal handlers in some of our examples, it is not guaranteed to produce the expected results, since the signal handler can interrupt a call to printf from our main program.
Consider a signal handler that is invoked right after main has set errno.
If the signal handler calls read, for example, this call can change the value of errno, wiping out the value that was just.
Figure 10.4 Reentrant functions that may be called from a signal handler.
Therefore, as a general rule, when calling the functions listed in Figure 10.4 from a signal handler, we should save and restore errno.
Be aware that a commonly caught signal is SIGCHLD, and its signal handler usually calls one of the wait functions.
This data structure could be left half updated if we call siglongjmp instead of returning from the signal handler.
If it is going to do such things as update global data structures, as we describe here, while catching signals that cause sigsetjmp to be executed, an application needs to block the signals while updating the data structures.
Figure 10.5 shows a program that calls the nonreentrant function getpwnam from a signal handler that is called every second.
We use it here to generate a SIGALRM signal every second.
Figure 10.5 Call a nonreentrant function from a signal handler.
Usually, the program would be terminated by a SIGSEGV signal when the signal handler returned after several iterations.
An examination of the core file showed that the main function had called getpwnam, but that when getpwnam called free, the signal handler interrupted it and called getpwnam, which in turn called free.
The data structures maintained by malloc and free had been corrupted when the signal handler (indirectly) called free while the main function was also calling free.
Occasionally, the program would run for several seconds before crashing with a SIGSEGV error.
When the main function did run correctly after the signal had been caught, the return value was sometimes corrupted and sometimes fine.
As shown by this example, if we call a nonreentrant function from a signal handler, the results are unpredictable.
Two signals that continually generate confusion are SIGCLD and SIGCHLD.
The name SIGCLD (without the H) is from System V, and this signal has different semantics from the BSD signal, named SIGCHLD.
The semantics of the BSD SIGCHLD signal are normal, in the sense that its semantics are similar to those of all other signals.
When the signal occurs, the status of a child has changed, and we need to call one of the wait functions to determine what has happened.
System V, however, has traditionally handled the SIGCLD signal differently from other signals.
This older handling of SIGCLD consists of the following behavior:
POSIX.1 does not specify what happens when SIGCHLD is ignored, so this behavior is allowed.
The XSI option requires this behavior to be supported for SIGCHLD.
If we want to avoid zombies, we have to wait for our children.
With SVR4, if either signal or sigset is called to set the disposition of SIGCHLD to be ignored, zombies are never generated.
All four platforms described in this book follow SVR4 in this behavior.
If we set the disposition of SIGCLD to be caught, the kernel immediately checks whether any child processes are ready to be waited for and, if so, calls the SIGCLD handler.
Item 2 changes the way we have to write a signal handler for this signal, as illustrated in the following example.
Recall from Section 10.4 that the first thing to do on entry to a signal handler is to call signal again, to reestablish the handler.
This action is intended to minimize the window of time when the signal is reset back to its default and could get lost.
The output is a continual string of SIGCLD received lines.
Eventually, the process runs out of stack space and terminates abnormally.
Linux 3.2.0 also doesn’t exhibit this problem, because it doesn’t call the SIGCHLD signal handler when a process arranges to catch SIGCHLD and child processes are ready to be waited for, even though SIGCLD and SIGCHLD are defined to be the same value.
Solaris 10, on the other hand, does call the signal handler in this situation, but includes extra code in the kernel to avoid this problem.
Although the four platforms described in this book solve this problem, realize that platforms (such as AIX) still exist that haven’t addressed it.
The problem with this program is that the call to signal at the beginning of the signal handler invokes item 2 from the preceding discussion—the kernel checks whether a child needs to be waited for (which is the case, since we’re processing a SIGCLD signal), so it generates another call to the signal handler.
The signal handler calls signal, and the whole process starts over again.
To fix this program, we have to move the call to signal after the call to wait.
By doing this, we call signal after fetching the child’s termination status; the signal is generated again by the kernel only if some other child has since terminated.
POSIX.1 states that when we establish a signal handler for SIGCHLD and there exists a terminated child we have not yet waited for, it is unspecified whether the signal is generated.
Be especially aware of some systems that #define SIGCHLD to be SIGCLD, or vice versa.
Changing the name may allow you to compile a program that was written for another system, but if that program depends on the other semantics, it may not work.
We need to define some of the terms used throughout our discussion of signals.
First, a signal is generated for a process (or sent to a process) when the event that causes the signal occurs.
The event could be a hardware exception (e.g., divide by 0), a software condition (e.g., an alarm timer expiring), a terminal-generated signal, or a call to the kill function.
When the signal is generated, the kernel usually sets a flag of some form in the process table.
We say that a signal is delivered to a process when the action for a signal is taken.
During the time between the generation of a signal and its delivery, the signal is said to be pending.
A process has the option of blocking the delivery of a signal.
If a signal that is blocked is generated for a process, and if the action for that signal is either the default action or to catch the signal, then the signal remains pending for the process until the process either (a) unblocks the signal or (b) changes the action to ignore the signal.
The system determines what to do with a blocked signal when the signal is delivered, not when it’s generated.
This allows the process to change the action for the signal before it’s delivered.
The sigpending function (Section 10.13) can be called by a process to determine which signals are blocked and pending.
What happens if a blocked signal is generated more than once before the process unblocks the signal? POSIX.1 allows the system to deliver the signal either once or more than once.
If the system delivers the signal more than once, we say that the signals are queued.
Most UNIX systems, however, do not queue signals unless they support the real-time extensions to POSIX.1
With SUSv4, the real-time signal functionality moved from the real-time extensions to the base specification.
As time goes on, more systems will support queueing signals even if they don’t support the real-time extensions.
The manual pages for SVR2 claimed that the SIGCLD signal was queued while the process was executing its SIGCLD signal handler.
Although this might have been true on a conceptual level, the actual implementation was different.
Instead, the signal was regenerated by the kernel as we described in Section 10.7
In SVR3, the manual was changed to indicate that the SIGCLD signal was ignored while the process was executing its signal handler for SIGCLD.
The SVR4 manual removed any mention of what happens to SIGCLD signals that are generated while a process is executing its SIGCLD signal handler.
Apparently, this feature was partially implemented within the kernel, but it is not enabled in SVR4
Curiously, the SVID didn’t make the same claims of reliable queuing.
What happens if more than one signal is ready to be delivered to a process? POSIX.1 does not specify the order in which the signals are delivered to the process.
The Rationale for POSIX.1 does suggest, however, that signals related to the current state of the process be delivered before other signals.
Each process has a signal mask that defines the set of signals currently blocked from delivery to that process.
We can think of this mask as having one bit for each possible signal.
If the bit is on for a given signal, that signal is currently blocked.
A process can examine and change its current signal mask by calling sigprocmask, which we describe in Section 10.12
The signal mask, for example, is stored in one of these signal sets.
We describe five functions that operate on signal sets in Section 10.11
The kill function sends a signal to a process or a group of processes.
The raise function allows a process to send a signal to itself.
Since ISO C does not deal with multiple processes, it could not define a function, such as kill, that requires a process ID argument.
There are four different conditions for the pid argument to kill.
For most UNIX systems, this set of system processes includes the kernel processes and init (pid 1)
Again, the set of all processes excludes certain system processes, as described earlier.
As we’ve mentioned, a process needs permission to send a signal to another process.
For other users, the basic rule is that the real or effective user ID of the sender has to equal the real or effective user ID of the receiver.
One special case for the permission testing also exists: if the signal being sent is SIGCONT, a process can send it to any other process in the same session.
Also understand that the test for process existence is not atomic.
By the time that kill returns the answer to the caller, the process in question might have exited, so the answer is of limited value.
If the call to kill causes the signal to be generated for the calling process and if the signal is not blocked, either signo or some other pending, unblocked signal is delivered to the process before kill returns.
Additional conditions occur with threads; see Section 12.8 for more information.
The alarm function allows us to set a timer that will expire at a specified time in the future.
If we ignore or don’t catch this signal, its default action is to terminate the process.
Returns: 0 or number of seconds until previously set alarm.
The seconds value is the number of clock seconds in the future when the signal should be generated.
When that time occurs, the signal is generated by the kernel, although additional time could elapse before the process gets control to handle the signal, because of processor scheduling delays.
Earlier UNIX System implementations warned that the signal could also be sent up to 1 second early.
There is only one of these alarm clocks per process.
If, when we call alarm, a previously registered alarm clock for the process has not yet expired, the number of seconds left for that alarm clock is returned as the value of this function.
That previously registered alarm clock is replaced by the new value.
If a previously registered alarm clock for the process has not yet expired and if the seconds value is 0, the previous alarm clock is canceled.
The number of seconds left for that previous alarm clock is still returned as the value of the function.
Although the default action for SIGALRM is to terminate the process, most processes that use an alarm clock catch this signal.
If the process then wants to terminate, it can perform whatever cleanup is required before terminating.
If we intend to catch SIGALRM, we need to be careful to install its signal handler before calling alarm.
If we call alarm first and are sent SIGALRM before we can install the signal handler, our process will terminate.
The pause function suspends the calling process until a signal is caught.
Example Using alarm and pause, we can put a process to sleep for a specified amount of time.
This function looks like the sleep function, which we describe in Section 10.19, but this simple implementation has three problems.
If the caller already has an alarm set, that alarm is erased by the first call to alarm.
We can correct this by looking at alarm’s return value.
If the number of seconds until some previously set alarm is less than the argument, then we should wait only until the existing alarm expires.
If the previously set alarm will go off after ours, then before returning we should reset this alarm to occur at its designated time in the future.
If we’re writing a function for others to call, we should save the disposition when our function is called and restore it when we’re done.
We can correct this by saving the return value from signal and resetting the disposition before our function returns.
There is a race condition between the first call to alarm and the call to pause.
On a busy system, it’s possible for the alarm to go off and the signal handler to be called before we call pause.
If that happens, the caller is suspended forever in the call to pause (assuming that some other signal isn’t caught)
The first uses setjmp, which we show in the next example.
The other uses sigprocmask and sigsuspend, and we describe it in Section 10.19
Even if the pause is never executed, the sleep2 function returns when the SIGALRM occurs.
There is, however, another subtle problem with the sleep2 function that involves its interaction with other signals.
If the SIGALRM interrupts some other signal handler, then when we call longjmp, we abort the other signal handler.
The loop in the SIGINT handler was written so that it executes for longer than 5 seconds on one of the systems used by the author.
We simply want it to execute longer than the argument to sleep2
The integer k is declared as volatile to prevent an optimizing compiler from discarding the loop.
When we execute the program shown in Figure 10.9 and interrupt the sleep by typing the interrupt character, we get the following output:
This is what you’ll encounter if you mix the SVR2 sleep function with other signal handling.
The following sections will show ways around all these problems, so we can handle signals reliably, without interfering with other pieces of code.
A common use for alarm, in addition to implementing the sleep function, is to put an upper time limit on operations that can block.
The program in Figure 10.10 does this, reading one line from standard input and writing it to standard output.
This sequence of code is common in UNIX applications, but this program has two problems.
Figure 10.7: a race condition between the first call to alarm and the call to read.
If the kernel blocks the process between these two function calls for longer than the alarm period, the read could block forever.
Most operations of this type use a long alarm period, such as a minute or more, making this unlikely; nevertheless, it is a race condition.
Here, we specifically want a slow system call to be interrupted.
We’ll see a portable way to do this in Section 10.14
This way, we don’t need to worry about whether a slow system call is interrupted.
This version works as expected, regardless of whether the system restarts interrupted system calls.
Realize, however, that we still have the problem of interactions with other signal handlers, as in Figure 10.8
If we want to set a time limit on an I/O operation, we need to use longjmp, as shown previously, while recognizing its possible interaction with other signal handlers.
We need a data type to represent multiple signals—a signal set.
We’ll use this data type with such functions as sigprocmask (in the next section) to tell the kernel not to allow any of the signals in the set to occur.
As we mentioned earlier, the number of different signals can exceed the number of bits in an integer, so in general we can’t use an integer to represent the set with one bit per signal.
The function sigemptyset initializes the signal set pointed to by set so that all signals are excluded.
The function sigfillset initializes the signal set so that all signals are included.
All applications have to call either sigemptyset or sigfillset once for each signal set, before using the signal set, because we cannot assume that the C initialization for external and static variables (0) corresponds to the implementation of signal sets on a given system.
Once we have initialized a signal set, we can add and delete specific signals in the set.
The function sigaddset adds a single signal to an existing set, and sigdelset removes a single signal from a set.
In all the functions that take a signal set as an argument, we always pass the address of the signal set as the argument.
If the implementation has fewer signals than bits in an integer, a signal set can be implemented using one bit per signal.
The sigemptyset function zeros the integer, and the sigfillset function turns on all the bits in the integer.
These two functions can be implemented as macros in the <signal.h> header:
Note that sigfillset must return 0, in addition to setting all the bits on in the signal set, so we use C’s comma operator, which returns the value after the comma as the value of the expression.
Using this implementation, sigaddset turns on a single bit and sigdelset turns off a single bit; sigismember tests a certain bit.
This is more difficult to do in a macro than in a function.
Recall from Section 10.8 that the signal mask of a process is the set of signals currently blocked from delivery to that process.
A process can examine its signal mask, change its signal mask, or perform both operations in one step by calling the following function.
First, if oset is a non-null pointer, the current signal mask for the process is returned through oset.
Second, if set is a non-null pointer, the how argument indicates how the current signal mask is modified.
SIG_BLOCK The new signal mask for the process is the union of its current signal mask and the signal set pointed to by set.
That is, set contains the additional signals that we want to block.
SIG_UNBLOCK The new signal mask for the process is the intersection of its current signal mask and the complement of the signal set pointed to by set.
That is, set contains the signals that we want to unblock.
SIG_SETMASK The new signal mask for the process is replaced by the value of the signal set pointed to by set.
Figure 10.13 Ways to change the current signal mask using sigprocmask.
If set is a null pointer, the signal mask of the process is not changed, and how is ignored.
After calling sigprocmask, if any unblocked signals are pending, at least one of these signals is delivered to the process before sigprocmask returns.
A separate function is provided to manipulate a thread’s signal mask in a multithreaded process.
Figure 10.14 shows a function that prints the names of the signals in the signal mask of the calling process.
To save space, we don’t test the signal mask for every signal that we listed in Figure 10.1
The sigpending function returns the set of signals that are blocked from delivery and currently pending for the calling process.
The set of signals is returned through the set argument.
Figure 10.15 shows many of the signal features that we’ve been describing.
The process blocks SIGQUIT, saving its current signal mask (to restore later), and then goes to sleep for 5 seconds.
Any occurrence of the quit signal during this period is blocked and won’t be delivered until the signal is unblocked.
At the end of the 5-second sleep, we check whether the signal is pending and unblock the signal.
Note that we saved the old mask when we blocked the signal.
To unblock the signal, we did a SIG_SETMASK of the old mask.
Alternatively, we could SIG_UNBLOCK only the signal that we had blocked.
Be aware, however, if we write a function that can be called by others and if we need to block a signal in our function, we can’t use SIG_UNBLOCK to unblock the signal.
In this case, we have to use SIG_SETMASK and restore the signal mask to its prior value, because it’s possible that the caller had specifically blocked this signal before calling our function.
We’ll see an example of this in the system function in Section 10.18
If we generate the quit signal during this sleep period, the signal is now pending and unblocked, so it is delivered before sigprocmask returns.
We’ll see this occur because the printf in the signal handler is output before the printf that follows the call to sigprocmask.
The message Quit(coredump) is printed by the shell when it sees that its child terminated abnormally.
Note that when we run the program the second time, we generate the quit signal ten times while the process is asleep, yet the signal is delivered only once to the process when it’s unblocked.
This demonstrates that signals are not queued on this system.
The sigaction function allows us to examine or modify (or both) the action associated with a particular signal.
This function supersedes the signal function from earlier releases of the UNIX System.
Indeed, at the end of this section, we show an implementation of signal using sigaction.
The argument signo is the signal number whose action we are examining or modifying.
If the act pointer is non-null, we are modifying the action.
If the oact pointer is non-null, the system returns the previous action for the signal through the oact pointer.
If and when the signal-catching function returns, the signal mask of the process is reset to its previous value.
This way, we are able to block certain signals whenever a signal handler is invoked.
The operating system includes the signal being delivered in the signal mask when the handler is invoked.
Hence, we are guaranteed that whenever we are processing a given signal, another occurrence of that same signal is blocked until we’re finished processing the first occurrence.
Recall from Section 10.8 that additional occurrences of the same signal are usually not queued.
If the signal occurs five times while it is blocked, when we unblock the signal, the signal-handling function for that signal will usually be invoked only one time.
Once we install an action for a given signal, that action remains installed until we explicitly change it by calling sigaction.
Unlike earlier systems with their unreliable signals, POSIX.1 requires that a signal handler remain installed until explicitly changed.
The sa_flags field of the act structure specifies various options for the handling of this signal.
Figure 10.16 details the meaning of these options when set.
System calls interrupted by this signal are not automatically restarted (the XSI default for sigaction)
If signo is SIGCHLD, do not generate this signal when a child process stops (job control)
This signal is still generated, of course, when a child terminates (but see the SA_NOCLDWAIT option below)
When the XSI option is supported, SIGCHLD won’t be sent when a stopped child continues if this flag is set.
When this signal is caught, the signal is not automatically blocked by the system while the signal-catching function executes (unless the signal is also included in sa_mask)
Note that this type of operation corresponds to the earlier unreliable signals.
If an alternative stack has been declared with sigaltstack(2), this signal is delivered to the process on the alternative stack.
Note that this type of operation corresponds to the earlier unreliable signals.
The disposition for the two signals SIGILL and SIGTRAP can’t be reset automatically, however.
Setting this flag can optionally cause sigaction to behave as if SA_NODEFER is also set.
This option provides additional information to a signal handler: a pointer to a siginfo structure and a pointer to an identifier for the process context.
The siginfo structure contains information about why the signal was generated.
An example of what it might look like is shown below.
Additionally, implementations that are XSI compliant contain at least the following fields:
If the signal is SIGBUS, SIGILL, SIGFPE, or SIGSEGV, then the si_addr contains the address responsible for the fault, although the address might not be accurate.
The si_errno field contains the error number corresponding to the condition that caused the signal to be generated, although its use is implementation defined.
The context argument to the signal handler is a typeless pointer that can be cast to a ucontext_t structure identifying the process context at the time of signal delivery.
The uc_stack field describes the stack used by the current context.
When an implementation supports the real-time signal extensions, signal handlers established with the SA_SIGINFO flag will result in signals being queued reliably.
A separate range of reserved signal numbers is available for real-time application use.
Applications can pass information along with the signal by using the sigqueue function (Section 10.20)
Example —signal Function Let’s now implement the signal function using sigaction.
This is what many platforms do (and what a note in the POSIX.1 Rationale states was the intent of POSIX)
Systems with binary compatibility constraints, on the other hand, might provide a signal function that supports the older, unreliable-signal semantics.
Unless you specifically require these older, unreliable semantics (for backward compatibility), you should use the following implementation of signal or call sigaction directly.
All the examples in this text that call signal call the function shown in Figure 10.18
Note that we must use sigemptyset to initialize the sa_mask member of the structure.
We intentionally set the SA_RESTART flag for all signals other than SIGALRM, so that any system call interrupted by these other signals will be automatically restarted.
The reason we don’t want SIGALRM restarted is to allow us to set a timeout for I/O operations.
Some older systems, such as SunOS, define the SA_INTERRUPT flag.
These systems restart interrupted system calls by default, so specifying this flag causes system calls to be interrupted.
Linux defines the SA_INTERRUPT flag for compatibility with applications that use it, but by default does not restart system calls when the signal handler is installed with sigaction.
The Single UNIX Specification specifies that the sigaction function not restart interrupted system calls unless the SA_RESTART flag is specified.
Figure 10.19 shows a version of the signal function that tries to prevent any interrupted system calls from being restarted.
For improved portability, we specify the SA_INTERRUPT flag, if defined by the system, to prevent interrupted system calls from being restarted.
In Section 7.10, we described the setjmp and longjmp functions, which can be used for nonlocal branching.
The longjmp function is often called from a signal handler to return to the main loop of a program, instead of returning from the handler.
When a signal is caught, the signal-catching function is entered, with the current signal automatically being added to the signal mask of the process.
This prevents subsequent occurrences of that signal from interrupting the signal handler.
If we longjmp out of the signal handler, what happens to the signal mask for the process?
To allow either form of behavior, POSIX.1 does not specify the effect of setjmp and longjmp on signal masks.
Instead, two new functions, sigsetjmp and siglongjmp, are defined by POSIX.1
These two functions should always be used when branching from a signal handler.
Returns: 0 if called directly, nonzero if returning from a call to siglongjmp.
The only difference between these functions and the setjmp and longjmp functions is that sigsetjmp has an additional argument.
If savemask is nonzero, then sigsetjmp also saves the current signal mask of the process in env.
When siglongjmp is called, if the env argument was saved by a call to sigsetjmp with a nonzero savemask, then siglongjmp restores the saved signal mask.
The program in Figure 10.20 demonstrates how the signal mask that is installed by the system when a signal handler is invoked automatically includes the signal being caught.
This program also illustrates the use of the sigsetjmp and siglongjmp functions.
This program demonstrates another technique that should be used whenever siglongjmp is called from a signal handler.
We set the variable canjump to a nonzero value only after we’ve called sigsetjmp.
This variable is examined in the signal handler, and siglongjmp is called only if the flag canjump is nonzero.
This technique provides protection against the signal handler being called at some earlier or later time, when the jump buffer hasn’t been initialized by sigsetjmp.
In this trivial program, we terminate quickly after the siglongjmp, but in larger programs, the signal handler may remain installed long after the siglongjmp.
Providing this type of protection usually isn’t required with longjmp in normal C code (as opposed to a signal handler)
Since a signal can occur at any time, however, we need the added protection in a signal handler.
Here, we use the data type sig_atomic_t, which is defined by the ISO C standard to be the type of variable that can be written without being interrupted.
By this we mean that a variable of this type should not extend across page boundaries on a system with virtual memory and can be accessed with a single machine instruction, for example.
We always include the ISO type qualifier volatile for these data types as well, since the variable is being accessed by two different threads of control: the main function and the asynchronously executing signal handler.
While the process is executing in the left part, its signal mask is 0 (no signals are blocked)
While executing in the center part, its signal mask is SIGUSR1
While executing in the right part, its signal mask is SIGUSR1|SIGALRM.
Let’s examine the output when the program in Figure 10.20 is executed:
The output is what we expect: when a signal handler is invoked, the signal being caught is added to the current signal mask of the process.
The original mask is restored when the signal handler returns.
Also, siglongjmp restores the signal mask that was saved by sigsetjmp.
This means that the main function is executing with the SIGUSR1 signal blocked, after the call to setjmp.
We have seen how we can change the signal mask for a process to block and unblock selected signals.
We can use this technique to protect critical regions of code that we don’t want interrupted by a signal.
But what if we want to unblock a signal and then pause, waiting for the previously blocked signal to occur? Assuming that the signal is SIGINT, the incorrect way to do this is.
If the signal is sent to the process while it is blocked, the signal delivery will be deferred until the signal is unblocked.
To the application, this can look as if the signal occurs between the unblocking and the pause (depending on how the kernel implements signals)
If this happens, or if the signal does occur between the unblocking and the pause, we have a problem.
Any occurrence of the signal in this window of time is lost, in the sense that we might not see the signal again, in which case the pause will block indefinitely.
To correct this problem, we need a way to both restore the signal mask and put the process to sleep in a single atomic operation.
The signal mask of the process is set to the value pointed to by sigmask.
Then the process is suspended until a signal is caught or until a signal occurs that terminates the process.
If a signal is caught and if the signal handler returns, then sigsuspend returns, and the signal mask of the process is set to its value before the call to sigsuspend.
Figure 10.22 shows the correct way to protect a critical region of code from a specific signal.
When sigsuspend returns, it sets the signal mask to its value before the call.
In this example, the SIGINT signal will be blocked, so we restore the signal mask to the value that we saved earlier (oldmask)
Running the program from Figure 10.22 produces the following output:
We added SIGUSR1 to the mask installed when we called sigsuspend so that when the signal handler ran, we could tell that the mask had actually changed.
We can see that when sigsuspend returns, it restores the signal mask to its value before the call.
Example Another use of sigsuspend is to wait for a signal handler to set a global variable.
In the program shown in Figure 10.23, we catch both the interrupt signal and the quit signal, but want to wake up the main routine only when the quit signal is caught.
Figure 10.23 Using sigsuspend to wait for a global variable to be set.
Example As another example of signals, we show how signals can be used to synchronize a parent and child.
Figure 10.24 Routines to allow a parent and child to synchronize.
In Figure 15.7, we show another implementation of these five functions using pipes.
The sigsuspend function is fine if we want to go to sleep while we’re waiting for a signal to occur (as we’ve shown in the previous two examples), but what if we want to call other system functions while we’re waiting? Unfortunately, this problem has no bulletproof solution unless we use multiple threads and dedicate a separate thread to handling signals, as we discuss in Section 12.8
Without using threads, the best we can do is to set a global variable in the signal handler when the signal occurs.
For example, if we catch both SIGINT and SIGALRM and install the signal handlers using the signal_intr function, the signals will interrupt any slow system call that is blocked.
The signals are most likely to occur when we’re blocked in a call to the read function waiting for input from a slow device.
This is especially true for SIGALRM, since we set the alarm clock to prevent us from waiting forever for input.
The code to handle this looks similar to the following:
We test each of the global flags before calling read and again if read returns an interrupted system call error.
The problem occurs if either signal is caught between the first two if statements and the subsequent call to read.
Signals occurring in here are lost, as indicated by the code comment.
The signal handlers are called, and they set the appropriate global variable, but the read never returns (unless some data is ready to be read)
What we would like to be able to do is the following sequence of steps, in order.
Call read (or any other system function) and unblock the two signals, as an atomic operation.
The sigsuspend function helps us only if step 3 is a pause operation.
We mentioned earlier that the abort function causes abnormal program termination.
POSIX.1 also specifies that abort overrides the blocking or ignoring of the signal by the process.
The intent of letting the process catch the SIGABRT is to allow it to perform any cleanup that it wants to do before the process terminates.
If the process doesn’t terminate itself from this signal handler, POSIX.1 states that, when the signal handler returns, abort terminates the process.
The ISO C specification of this function leaves it up to the implementation as to whether output streams are flushed and whether temporary files (Section 5.13) are deleted.
POSIX.1 goes further and allows an implementation to call fclose on open standard I/O streams before terminating if the call to abort terminates the process.
Earlier versions of System V generated the SIGIOT signal from the abort function.
Furthermore, it was possible for a process to ignore this signal or to catch it and return from the signal handler, in which case abort returned to its caller.
This prevented a process from either ignoring the signal or catching it.
Historically, implementations of abort have differed in how they deal with standard I/O streams.
For defensive programming and improved portability, if we want standard I/O streams to be flushed, we specifically do it before calling abort.
Since most UNIX System implementations of tmpfile call unlink immediately after creating the file, the ISO C warning about temporary files does not usually concern us.
We first see whether the default action will occur; if so, we flush all the standard I/O streams.
This is not equivalent to calling fclose on all the open streams (since it just flushes them and doesn’t close them), but when the process terminates, the system closes all open files.
If the process catches the signal and returns, we flush all the streams again, since the process could have generated more output.
In this case, any unflushed standard I/O buffers in memory are discarded.
We assume that a caller that does this doesn’t want the buffers flushed.
We block all signals except SIGABRT, so we know that if the call to kill returns, the process caught the signal and the signal handler returned.
In Section 8.13, we showed an implementation of the system function.
POSIX.1 requires that system ignore SIGINT and SIGQUIT and block SIGCHLD.
Before showing a version that handles these signals correctly, let’s see why we need to worry about signal handling.
This editor has been part of UNIX systems for a long time.
We use it here because it is an interactive program that catches the interrupt and quit signals.
If we invoke ed from a shell and type the interrupt character, it catches the interrupt signal and prints a question mark.
The ed program also sets the disposition of the quit signal so that it is ignored.
The program in Figure 10.26 catches both SIGINT and SIGCHLD.
When the editor terminates, the system sends the SIGCHLD signal to the parent (the a.out process)
But if it is catching the SIGCHLD signal, the parent should be doing so because it has created its own children, so that it knows when its children have terminated.
Otherwise, when the child created by system terminates, it would fool the caller of system into thinking that one of its own children terminated.
The caller would then use one of the wait functions to get the termination status of the child, thereby preventing the system function from being able to obtain the child’s termination status for its return value.
If we run the program again, this time sending the editor an interrupt signal, we get.
Recall from Section 9.6 that typing the interrupt character causes the interrupt signal to be sent to all the processes in the foreground process group.
Figure 10.27 shows the arrangement of the processes when the editor is running.
In this example, SIGINT is sent to all three foreground processes.
As we can see from the output, both the a.out process and the editor catch the signal.
But when we’re running another program with the system function, we shouldn’t have both the parent and the child catching the two terminal-generated signals: interrupt and quit.
Instead, these two signals should be sent to the program that is running: the child.
Since the command that is executed by system can be an interactive command (as is the ed program in this example) and since the caller of system gives up control while the program executes, waiting for it to finish, the caller of system should not be receiving these two terminal-generated signals.
For this reason, POSIX.1 specifies that the system function should ignore these two signals while waiting for the command to complete.
Figure 10.28 shows an implementation of the system function with the required signal handling.
If we link the program in Figure 10.26 with this implementation of the system function, the resulting binary differs from the last (flawed) one in the following ways.
No signal is sent to the calling process when we type the interrupt or quit character.
When the ed command exits, SIGCHLD is not sent to the calling process.
Instead, it is blocked until we unblock it in the last call to sigprocmask, after the system function retrieves the child’s termination status by calling waitpid.
Many older texts show the ignoring of the interrupt and quit signals as follows:
The problem with this sequence of code is that we have no guarantee after the fork regarding whether the parent or child runs first.
If the child runs first and the parent doesn’t run for some time after, an interrupt signal might be generated before the parent is able to change its disposition to be ignored.
For this reason, in Figure 10.28, we change the disposition of the signals before the fork.
Note that we have to reset the dispositions of these two signals in the child before the call to execl.
This allows execl to change their dispositions to the default, based on the caller’s dispositions, as we described in Section 8.10
The return value from system is the termination status of the shell, which isn’t always the termination status of the command string.
Let’s run the program in Figure 8.24 and send some signals to the command that’s executing:
The same thing happens when we kill the sleep call with the quit key.
As this example demonstrates, the Bourne shell has a poorly documented feature in which its termination status is 128 plus the signal number, when the command it was executing is terminated by a signal.
Let’s try a similar example, but this time we’ll send a signal directly to the shell and see what is returned by system:
Here, we can see that the return value from system reports an abnormal termination only when the shell itself terminates abnormally.
Other shells behave differently when handling terminal-generated signals, such as SIGINT and SIGQUIT.
With bash and dash, for example, pressing the interrupt or quit key will result in an exit status indicating abnormal termination with the corresponding signal number.
However, if we find our process executing sleep and send it a signal directly, so that the signal goes only to the individual process instead of the entire foreground process group, we will find that these shells behave like the Bourne shell and exit with a normal termination status of 128 plus the signal number.
When writing programs that use the system function, be sure to interpret the return value correctly.
If you call fork, exec, and wait yourself, the termination status is not the same as if you call system.
This function causes the calling process to be suspended until either.
The amount of wall clock time specified by seconds has elapsed.
A signal is caught by the process and the signal handler returns.
As with an alarm signal, the actual return may occur at a time later than requested because of other system activity.
When sleep returns early because of some signal being caught (case 2), the return value is the number of unslept seconds (the requested time minus the actual time slept)
Although sleep can be implemented with the alarm function (Section 10.10), this isn’t required.
If alarm is used, however, there can be interactions between the two functions.
For portability, you shouldn’t make any assumptions about the implementation of sleep, but if you have any intentions of mixing calls to sleep with any other timing functions, you need to be aware of possible interactions.
This function is a modification of Figure 10.7, which handles signals reliably, avoiding the race condition in the earlier implementation.
We still do not handle any interactions with previously set alarms.
As we mentioned, these interactions are explicitly undefined by POSIX.1
It takes more code to write this reliable implementation than what is shown in Figure 10.7
We don’t use any form of nonlocal branching (as we did in Figure 10.8 to avoid the race condition between alarm and pause), so there is no effect on other signal handlers that may be executing when the SIGALRM is handled.
The nanosleep function is similar to the sleep function, but provides nanosecond-level granularity.
This function suspends the calling process until either the requested time has elapsed or the function is interrupted by a signal.
If the sleep interval is interrupted by a signal and the process doesn’t terminate, the timespec structure pointed to by the remtp parameter will be set to the amount of time left in the sleep interval.
We can set this parameter to NULL if we are uninterested in the time unslept.
If the system doesn’t support nanosecond granularity, the requested time is rounded up.
Because the nanosleep function doesn’t involve the generation of any signals, we can use it without worrying about interactions with other functions.
The nanosleep function used to belong to the Timers option in the Single UNIX Specification, but was moved to the base in SUSv4
With the introduction of multiple system clocks (recall Section 6.10), we need a way to suspend the calling thread using a delay time relative to a particular clock.
Returns: 0 if slept for requested time or error number on failure.
The clock_id argument specifies the clock against which the time delay is evaluated.
The flags argument is used to control whether the delay is absolute or relative.
When flags is set to 0, the sleep time is relative (i.e., how long we want to sleep)
When it is set to TIMER_ABSTIME, the sleep time is absolute (i.e., we want to sleep until the clock reaches the specified time)
The other arguments, reqtp and remtp, are the same as in the nanosleep function.
However, when we use an absolute time, the remtp argument is unused, because it isn’t needed; we can reuse the same value for the reqtp argument for additional calls to clock_nanosleep until the clock reaches the specified absolute time value.
The problem with using a relative sleep is that some applications require precision with how long they sleep, and a relative sleep time can lead to sleeping longer than desired.
For example, if an application wants to perform a task at regular intervals, it would have to get the current time, calculate the amount of time until the next time to execute the task, and then call nanosleep.
Between the time that the current time is obtained and the call to nanosleep is made, processor scheduling and preemption can result in the relative sleep time extending past the desired interval.
Using an absolute time improves the precision, even though a time-sharing process scheduler makes no guarantee that our task will execute immediately after our sleep time has ended.
In older versions of the Single UNIX Specification, the clock_nanosleep function belonged to the Clock Selection option.
In Section 10.8 we said that most UNIX systems don’t queue signals.
With the real-time extensions to POSIX.1, some systems began adding support for queueing signals.
With SUSv4, the queued signal functionality has moved from the real-time extensions to the base specification.
Generally a signal carries one bit of information: the signal itself.
In addition to queueing signals, these extensions allow applications to pass more information along with the delivery (recall Section 10.14)
Along with system-provided information, applications can pass an integer or a pointer to a buffer containing more information to the signal handler.
To use queued signals we have to do the following:
Specify the SA_SIGINFO flag when we install a signal handler using the sigaction function.
If we don’t specify this flag, the signal will be posted, but it is left up to the implementation whether the signal is queued.
Implementations might allow us to use the sa_handler field, but we won’t be able to obtain the extra information sent with the sigqueue function.
The sigqueue function is similar to the kill function, except that we can only direct signals to a single process with sigqueue, and we can use the value argument to transmit either an integer or a pointer value to the signal handler.
When this limit is reached, sigqueue can fail with errno set to EAGAIN.
With the real-time signal enhancements, a separate set of signals was introduced for application use.
These are the signal numbers between SIGRTMIN and SIGRTMAX, inclusive.
Be aware that the default action for these signals is to terminate the process.
Figure 10.30 summarizes the way queued signals differ in behavior among the implementations covered in this text.
Mac OS X 10.6.8 doesn’t support sigqueue or real-time signals.
On Solaris 10, sigqueue is in the real-time library, librt.
Except for SIGCHLD, most application programs don’t handle these signals: interactive shells usually do all the work required to handle them.
When we type the suspend character (usually Control-Z), SIGTSTP is sent to all processes in the foreground process group.
When we tell the shell to resume a job in the foreground or background, the shell sends all the processes in the job the SIGCONT signal.
Similarly, if SIGTTIN or SIGTTOU is delivered to a process, the process is stopped by default, and the job-control shell recognizes this and notifies us.
An exception is a process that is managing the terminal—the vi(1) editor, for example.
It needs to know when the user wants to suspend it so that it can restore the terminal’s state to the way it was when vi was started.
Also, when it resumes in the foreground, the vi editor needs to set the terminal state back to the way it wants it, and it needs to redraw the terminal screen.
We see how a program such as vi handles this in the example that follows.
When any of the four stop signals (SIGTSTP, SIGSTOP, SIGTTIN, or SIGTTOU) is generated for a process, any pending SIGCONT signal for that process is discarded.
Similarly, when the SIGCONT signal is generated for a process, any pending stop signals for that same process are discarded.
Note that the default action for SIGCONT is to continue the process, if it is stopped; otherwise, the signal is ignored.
Normally, we don’t have to do anything with this signal.
When SIGCONT is generated for a process that is stopped, the process is continued, even if the signal is blocked or ignored.
Example The program in Figure 10.31 demonstrates the normal sequence of code used when a program handles job control.
This program simply copies its standard input to its standard output, but comments are given in the signal handler for typical actions performed by a program that manages a screen.
The reason is that when the program is started by a shell that doesn’t support job control (/bin/sh, for example), the signal’s disposition.
In fact, the shell doesn’t explicitly ignore this signal; init sets the disposition of the three job-control signals (SIGTSTP, SIGTTIN, and SIGTTOU) to SIG_IGN.
Only a job-control shell should reset the disposition of these three signals to SIG_DFL.
When we type the suspend character, the process receives the SIGTSTP signal and the signal handler is invoked.
At this point, we would do any terminal-related processing: move the cursor to the lower-left corner, restore the terminal mode, and so on.
We then send ourself the same signal, SIGTSTP, after resetting its disposition to its default (stop the process) and unblocking the signal.
We have to unblock it since we’re currently handling that same signal, and the system blocks it automatically while it’s being caught.
It is continued only when it receives (usually from the job-control shell, in response to an interactive fg command) a SIGCONT signal.
Its default disposition is to continue the stopped process; when this happens, the program continues as though it returned from the kill function.
When the program is continued, we reset the disposition for the SIGTSTP signal and do whatever terminal processing we want (we could redraw the screen, for example)
In this section, we describe how to map between signal numbers and names.
The array index is the signal number, giving a pointer to the character string name of the signal.
To print the character string corresponding to a signal number in a portable manner, we can use the psignal function.
The string msg (which normally includes the name of the program) is output to the standard error, followed by a colon and a space, followed by a description of the signal, followed by a newline.
If msg is NULL, then only the description is written to the standard error.
If you have a siginfo structure from an alternative sigaction signal handler, you can print the signal information with the psiginfo function.
It operates in a similar manner to the psignal function.
Although this function has access to more information than just the signal number, platforms vary in exactly what additional information is printed.
If you only need the string description of the signal and don’t necessarily want to write it to standard error (you might want to write it to a log file, for example), you can use the strsignal function.
This function is similar to strerror (also described in Section 1.7)
Given a signal number, strsignal will return a string that describes the signal.
This string can be used by applications to print error messages about signals received.
All the platforms discussed in this book provide the psignal and strsignal functions, but differences do occur.
Solaris provides a couple of functions to map a signal number to a signal name, and vice versa.
These functions are useful when writing interactive programs that need to accept and print signal names and numbers.
The sig2str function translates the given signal number into a string and stores the result in the memory pointed to by str.
The caller must ensure that the memory is large enough to hold the longest string, including the terminating null byte.
The string consists of the signal name without the ‘‘SIG’’ prefix.
For example, translating SIGKILL would result in the string ‘‘KILL’’ being stored in the str memory buffer.
The str2sig function translates the given name into a signal number.
The signal number is stored in the integer pointed to by signop.
An understanding of the hows and whys of signal handling is essential to advanced UNIX System programming.
This chapter has taken a long and thorough look at UNIX System signals.
We started by looking at the warts in previous implementations of signals and how they manifest themselves.
We then proceeded to the POSIX.1 reliable-signal concept and all the related functions.
Once we covered all these details, we were able to provide implementations of the POSIX.1 abort, system, and sleep functions.
We finished with a look at the job-control signals and the ways that we can convert between signal names and signal numbers.
The process creates a file and writes the integer 0 to the file.
The process then calls fork, and the parent and child alternate incrementing the counter in the file.
Each time the counter is incremented, print which process (parent or child) is doing the increment.
The function should consist of a single loop that iterates once for every signal in the current signal mask (not once for every possible signal)
How would a program such as the cron daemon, which runs every minute on the minute, handle this situation?
Try to set the soft resource limit from your shell.
If you can’t do this from your shell, call setrlimit directly from the program.
Run this program on the different systems that you have access to.
Before calling fwrite, call alarm to schedule a signal in 1 second.
In your signal handler, print that the signal was caught and return.
We learned about the environment of a UNIX process, the relationships between processes, and ways to control processes.
We saw that a limited amount of sharing can occur between related processes.
In this chapter, we’ll look inside a process further to see how we can use multiple threads of control (or simply threads) to perform multiple tasks within the environment of a single process.
All threads within a single process have access to the same process components, such as file descriptors and memory.
Anytime you try to share a single resource among multiple users, you have to deal with consistency.
We’ll conclude this chapter with a look at the synchronization mechanisms available to prevent multiple threads from viewing inconsistencies in their shared resources.
A typical UNIX process can be thought of as having a single thread of control: each process is doing only one thing at a time.
With multiple threads of control, we can design our programs to do more than one thing at a time within a single process, with each thread handling a separate task.
We can simplify code that deals with asynchronous events by assigning a separate thread to handle each event type.
Each thread can then handle its event using a synchronous programming model.
A synchronous programming model is much simpler than an asynchronous one.
Threads, in contrast, automatically have access to the same memory address space and file descriptors.
Some problems can be partitioned so that overall program throughput can be improved.
A single-threaded process with multiple tasks to perform implicitly serializes those tasks, because there is only one thread of control.
With multiple threads of control, the processing of independent tasks can be interleaved by assigning a separate thread per task.
Two tasks can be interleaved only if they don’t depend on the processing performed by each other.
Similarly, interactive programs can realize improved response time by using multiple threads to separate the portions of the program that deal with user input and output from the other parts of the program.
Some people associate multithreaded programming with multiprocessor or multicore systems.
The benefits of a multithreaded programming model can be realized even if your program is running on a uniprocessor.
A program can be simplified using threads regardless of the number of processors, because the number of processors doesn’t affect the program structure.
Furthermore, as long as your program has to block when serializing tasks, you can still see improvements in response time and throughput when running on a uniprocessor, because some threads might be able to run while others are blocked.
A thread consists of the information necessary to represent an execution context within a process.
Everything within a process is sharable among the threads in a process, including the text of the executable program, the program’s global and heap memory, the stacks, and the file descriptors.
The threads interfaces we’re about to see are from POSIX.1-2001
Just as every process has a process ID, every thread has a thread ID.
Unlike the process ID, which is unique in the system, the thread ID has significance only within the context of the process to which it belongs.
Recall that a process ID, represented by the pid_t data type, is a non-negative integer.
A thread ID is represented by the pthread_t data type.
Implementations are allowed to use a structure to represent the pthread_t data type, so portable implementations can’t treat them as integers.
Therefore, a function must be used to compare two thread IDs.
A consequence of allowing the pthread_t data type to be a structure is that there is no portable way to print its value.
Sometimes, it is useful to print thread IDs during program debugging, but there is usually no need to do so otherwise.
At worst, this results in nonportable debug code, so it is not much of a limitation.
A thread can obtain its own thread ID by calling the pthread_self function.
This function can be used with pthread_equal when a thread needs to identify data structures that are tagged with its thread ID.
For example, a master thread might place work assignments on a queue and use the thread ID to control which jobs go to each worker thread.
A single master thread places new jobs on a work queue.
A pool of three worker threads removes jobs from the queue.
Instead of allowing each thread to process whichever job is at the head of the queue, the master thread controls job assignment by placing the ID of the thread that should process the job in each job structure.
Each worker thread then removes only jobs that are tagged with its own thread ID.
The traditional UNIX process model supports only one thread of control per process.
Conceptually, this is the same as a threads-based model whereby each process is made up of only one thread.
With pthreads, when a program runs, it also starts out as a single process with a single thread of control.
As the program runs, its behavior should be indistinguishable from the traditional process, until it creates more threads of control.
Additional threads can be created by calling the pthread_create function.
The memory location pointed to by tidp is set to the thread ID of the newly created thread when pthread_create returns successfully.
The attr argument is used to customize various thread attributes.
We’ll cover thread attributes in Section 12.3, but for now, we’ll set this to NULL to create a thread with the default attributes.
The newly created thread starts running at the address of the start_rtn function.
This function takes a single argument, arg, which is a typeless pointer.
If you need to pass more than one argument to the start_rtn function, then you need to store them in a structure and pass the address of the structure in arg.
When a thread is created, there is no guarantee which will run first: the newly created thread or the calling thread.
The newly created thread has access to the process address space and inherits the calling thread’s floating-point environment and signal mask; however, the set of pending signals for the thread is cleared.
Note that the pthread functions usually return an error code when they fail.
The per-thread copy of errno is provided only for compatibility with existing functions that use it.
With threads, it is cleaner to return the error code from the function, thereby restricting the scope of the error to the function that caused it, instead of relying on some global state that is changed as a side effect of the function.
Although there is no portable way to print the thread ID, we can write a small test program that does, to gain some insight into how threads work.
Figure 11.2 creates one thread and prints the process and thread IDs of the new thread and the initial thread.
This example has two oddities, which are necessary to handle races between the main thread and the new thread.
We’ll learn better ways to deal with these conditions later in this chapter.
The first is the need to sleep in the main thread.
If it doesn’t sleep, the main thread might exit, thereby terminating the entire process before the new thread gets a chance to run.
This behavior is dependent on the operating system’s threads implementation and scheduling algorithms.
The second oddity is that the new thread obtains its thread ID by calling pthread_self instead of reading it out of shared memory or receiving it as an argument to its thread-start routine.
In our example, the main thread stores this ID in ntid, but the new thread can’t safely use it.
If the new thread runs before the main thread returns from calling pthread_create, then the new thread will see the uninitialized contents of ntid instead of the thread ID.
Running the program in Figure 11.2 on Solaris gives us.
As we expect, both threads have the same process ID, but different thread IDs.
Running the program in Figure 11.2 on FreeBSD gives us.
As we expect, both threads have the same process ID.
If we look at the thread IDs as decimal integers, the values look strange, but if we look at them in hexadecimal format, they make more sense.
As we noted earlier, FreeBSD uses a pointer to the thread data structure for its thread ID.
We would expect Mac OS X to be similar to FreeBSD; however, the thread ID for the main thread is from a different address range than the thread IDs for threads created with pthread_create:
The Linux thread IDs look like pointers, even though they are represented as unsigned long integers.
In Linux 2.4, LinuxThreads implemented each thread with a separate process.
This made it difficult to match the behavior of POSIX threads.
In Linux 2.6, the Linux kernel and threads library were overhauled to use a new threads implementation called the Native POSIX Thread Library (NPTL)
This supported a model of multiple threads within a single process and made it easier to support POSIX threads semantics.
Similarly, when the default action is to terminate the process, a signal sent to a thread will terminate the entire process (we’ll talk more about the interactions between signals and threads in Section 12.8)
A single thread can exit in three ways, thereby stopping its flow of control, without terminating the entire process.
The thread can be canceled by another thread in the same process.
The rval_ptr argument is a typeless pointer, similar to the single argument passed to the start routine.
This pointer is available to other threads in the process by calling the pthread_join function.
The calling thread will block until the specified thread calls pthread_exit, returns from its start routine, or is canceled.
If the thread simply returned from its start routine, rval_ptr will contain the return code.
By calling pthread_join, we automatically place the thread with which we’re joining in the detached state (discussed shortly) so that its resources can be recovered.
If we’re not interested in a thread’s return value, we can set rval_ptr to NULL.
In this case, calling pthread_join allows us to wait for the specified thread, but does not retrieve the thread’s termination status.
Figure 11.3 shows how to fetch the exit code from a thread that has terminated.
The pointer can be used to pass the address of a structure containing more complex information.
Be careful that the memory used for the structure is still valid when the caller has completed.
If the structure was allocated on the caller’s stack, for example, the memory contents might have changed by the time the structure is used.
Of course, the results vary, depending on the memory architecture, the compiler, and the implementation of the threads library.
As we can see, the contents of the structure (allocated on the stack of thread tid1) have changed by the time the main thread can access the structure.
Note how the stack of the second thread (tid2) has overwritten the first thread’s stack.
To solve this problem, we can either use a global structure or allocate the structure using malloc.
In this case, the memory is no longer valid when the parent tries to access the structure passed to it by the first thread that exited, and the parent is sent the SIGSEGV signal.
On FreeBSD, the memory hasn’t been overwritten by the time the parent accesses it, and we get.
Even though the memory is still intact after the thread exits, we can’t depend on this always being the case.
It certainly isn’t what we observe on the other platforms.
One thread can request that another in the same process be canceled by calling the pthread_cancel function.
However, a thread can elect to ignore or otherwise control how it is canceled.
Note that pthread_cancel doesn’t wait for the thread to terminate; it merely makes the request.
A thread can arrange for functions to be called when it exits, similar to the way that the atexit function (Section 7.3) can be used by a process to arrange that functions are to be called when the process exits.
More than one cleanup handler can be established for a thread.
The handlers are recorded in a stack, which means that they are executed in the reverse order from that with which they were registered.
If the execute argument is set to zero, the cleanup function is not called.
Although the example is somewhat contrived, it illustrates the mechanics involved.
Running the program in Figure 11.5 on Linux or Solaris gives us.
From the output, we can see that both threads start properly and exit, but that only the second thread’s cleanup handlers are called.
Thus, if the thread terminates by returning from its start routine, its cleanup handlers are not called, although this behavior varies among implementations.
Also note that the cleanup handlers are called in the reverse order from which they were installed.
If we run the same program on FreeBSD or Mac OS X, we see that the program incurs a segmentation violation and drops core.
The only portable way to return in between these two functions is to call pthread_exit.
By now, you should begin to see similarities between the thread functions and the process functions.
By default, a thread’s termination status is retained until we call pthread_join for that thread.
A thread’s underlying storage can be reclaimed immediately on termination if the thread has been detached.
As we will see in the next chapter, we can create a thread that is already in the detached state by modifying the thread attributes we pass to pthread_create.
When multiple threads of control share the same memory, we need to make sure that each thread sees a consistent view of its data.
If each thread uses variables that other threads don’t read or modify, no consistency problems will exist.
Similarly, if a variable is read-only, there is no consistency problem with more than one thread reading its value at the same time.
However, when one thread can modify a variable that other threads can read or modify, we need to synchronize the threads to ensure that they don’t use an invalid value when accessing the variable’s memory contents.
When one thread modifies a variable, other threads can potentially see inconsistencies when reading the value of that variable.
On processor architectures in which the modification takes more than one memory cycle, this can happen when the memory read is interleaved between the memory write cycles.
Of course, this behavior is architecture dependent, but portable programs can’t make any assumptions about what type of processor architecture is being used.
Figure 11.7 shows a hypothetical example of two threads reading and writing the same variable.
In this example, thread A reads the variable and then writes a new value to it, but the write operation takes two memory cycles.
If thread B reads the same variable between the two write cycles, it will see an inconsistent value.
To solve this problem, the threads have to use a lock that will allow only one thread to access the variable at a time.
Similarly, when thread A updates the variable, it acquires the same lock.
Thus thread B will be unable to read the variable until thread A releases the lock.
We also need to synchronize two or more threads that might try to modify the same variable at the same time.
Consider the case in which we increment a variable (Figure 11.9)
The increment operation is usually broken down into three steps.
If two threads try to increment the same variable at almost the same time without synchronizing with each other, the results can be inconsistent.
You end up with a value that is either one or two greater than before, depending on the value observed when the second thread starts its operation.
If the modification is atomic, then there isn’t a race.
In the previous example, if the increment takes only one memory cycle, then no race exists.
If our data always appears to be sequentially consistent, then we need no additional synchronization.
Our operations are sequentially consistent when multiple threads can’t observe inconsistencies in our data.
In modern computer systems, memory accesses take multiple bus cycles, and multiprocessors generally interleave bus cycles among multiple processors, so we aren’t guaranteed that our data is sequentially consistent.
In a sequentially consistent environment, we can explain modifications to our data as a sequential step of operations taken by the running threads.
We can say such things as ‘‘Thread A incremented the variable, then thread B incremented the variable, so its value is two greater than before’’ or ‘‘Thread B incremented the variable, then thread A incremented the variable, so its value is two greater than before.’’ No possible ordering of the two threads can result in any other value of the variable.
Besides the computer architecture, races can arise from the ways in which our programs use variables, creating places where it is possible to view inconsistencies.
For example, we might increment a variable and then make a decision based on its value.
The combination of the increment step and the decision-making step isn’t atomic, which opens a window where inconsistencies can arise.
We can protect our data and ensure access by only one thread at a time by using the pthreads mutual-exclusion interfaces.
A mutex is basically a lock that we set (lock) before accessing a shared resource and release (unlock) when we’re done.
While it is set, any other thread that tries to set it will block until we release it.
If more than one thread is blocked when we unlock the mutex, then all threads blocked on the lock will be made runnable, and the first one to run will be able to set the lock.
In this way, only one thread will proceed at a time.
This mutual-exclusion mechanism works only if we design our threads to follow the same data-access rules.
The operating system doesn’t serialize access to data for us.
If we allow one thread to access a shared resource without first acquiring a lock, then inconsistencies can occur even though the rest of our threads do acquire the lock before attempting to access the shared resource.
A mutex variable is represented by the pthread_mutex_t data type.
To initialize a mutex with the default attributes, we set attr to NULL.
If the mutex is already locked, the calling thread will block until the mutex is unlocked.
Figure 11.10 illustrates a mutex used to protect a data structure.
When more than one thread needs to access a dynamically allocated object, we can embed a reference count in the object to ensure that we don’t free its memory before all threads are done using it.
Figure 11.10 Using a mutex to protect a data structure.
We lock the mutex before incrementing the reference count, decrementing the reference count, and checking whether the reference count reaches zero.
If we were to place the structure on a list at this point, it could be found by other threads, so we would need to lock it first.
Before using the object, threads are expected to add a reference to it by calling foo_hold.
When they are done, they must call foo_rele to release the reference.
When the last reference is released, the object’s memory is freed.
In this example, we have ignored how threads find an object before calling foo_hold.
We can avoid this problem by ensuring that the object can’t be found before freeing its memory.
We’ll see how to do this in the examples that follow.
A thread will deadlock itself if it tries to lock the same mutex twice, but there are less obvious ways to create deadlocks with mutexes.
For example, when we use more than one mutex in our programs, a deadlock can occur if we allow one thread to hold a mutex and block while trying to lock a second mutex at the same time that another thread holding the second mutex tries to lock the first mutex.
Neither thread can proceed, because each needs a resource that is held by the other, so we have a deadlock.
Deadlocks can be avoided by carefully controlling the order in which mutexes are locked.
For example, assume that you have two mutexes, A and B, that you need to lock at the same time.
If all threads always lock mutex A before mutex B, no deadlock can occur from the use of the two mutexes (but you can still deadlock on other resources)
Similarly, if all threads always lock mutex B before mutex A, no deadlock will occur.
You’ll have the potential for a deadlock only when one thread attempts to lock the mutexes in the opposite order from another thread.
Sometimes, an application’s architecture makes it difficult to apply a lock ordering.
If enough locks and data structures are involved that the functions you have available can’t be molded to fit a simple hierarchy, then you’ll have to try some other approach.
In this case, you might be able to release your locks and try again at a later time.
If it can’t acquire the lock, however, you can release the locks you already hold, clean up, and try again later.
Example In this example, we update Figure 11.10 to show the use of two mutexes.
We avoid deadlocks by ensuring that when we need to acquire two mutexes at the same time, we always lock them in the same order.
The second mutex protects a hash list that we use to keep track of the foo data structures.
Thus the hashlock mutex protects both the fh hash table and the f_next hash link field in the foo structure.
The f_lock mutex in the foo structure protects access to the remainder of the foo structure’s fields.
Since the new structure is placed on a global list, other threads can find it, so we need to block them if they try to access the new structure, until we are done initializing it.
The foo_find function locks the hash list lock and searches for the requested structure.
If it is found, we increase the reference count and return a pointer to the structure.
Now with two locks, the foo_rele function is more complicated.
If this is the last reference, we need to unlock the structure mutex so that we can acquire the hash list lock, since we’ll need to remove the structure from the hash list.
Because we could have blocked since the last time we held the structure mutex, we need to recheck the condition to see whether we still need to free the structure.
If another thread found the structure and added a reference to it while we blocked to honor the lock ordering, we simply need to decrement the reference count, unlock everything, and return.
This locking approach is complex, so we need to revisit our design.
We can simplify things considerably by using the hash list lock to protect the structure reference count, too.
The structure mutex can be used to protect everything else in the foo structure.
The lock-ordering issues surrounding the hash list and the reference count go away when we use the same lock for both purposes.
If your locking granularity is too coarse, you end up with too many threads blocking behind the same locks, with little improvement possible from concurrency.
If your locking granularity is too fine, then you suffer bad performance from excess locking overhead, and you end up with complex code.
As a programmer, you need to find the correct balance between code complexity and performance, while still satisfying your locking requirements.
One additional mutex primitive allows us to bound the time that a thread blocks when a mutex it is trying to acquire is already locked.
The timeout specifies how long we are willing to wait in terms of absolute time (as opposed to relative time; we specify that we are willing to block until time X instead of saying that we are willing to block for Y seconds)
The timeout is represented by the timespec structure, which describes time in terms of seconds and nanoseconds.
Here is the output from the program in Figure 11.13
This strategy is not recommended in practice, because it can lead to deadlock.
Note that the time blocked can vary for several reasons: the start time could have been in the middle of a second, the resolution of the system’s clock might not be fine enough to support the resolution of our timeout, or scheduling delays could prolong the amount of time until the program continues execution.
Solaris 10 also provides an alternative function that uses a relative timeout.
Reader–writer locks are similar to mutexes, except that they allow for higher degrees of parallelism.
With a mutex, the state is either locked or unlocked, and only one thread can lock it at a time.
Three states are possible with a reader–writer lock: locked in read mode, locked in write mode, and unlocked.
Only one thread at a time can hold a reader–writer lock in write mode, but multiple threads can hold a reader–writer lock in read mode at the same time.
When a reader–writer lock is write locked, all threads attempting to lock it block until it is unlocked.
When a reader–writer lock is read locked, all threads attempting to lock it in read mode are given access, but any threads attempting to lock it in write mode block until all the threads have released their read locks.
Although implementations vary, reader–writer locks usually block additional readers if a lock is already held in read mode and a thread is blocked trying to acquire the lock in write mode.
This prevents a constant stream of readers from starving waiting writers.
Reader–writer locks are well suited for situations in which data structures are read more often than they are modified.
When a reader–writer lock is held in write mode, the data structure it protects can be modified safely, since only one thread at a time can hold the lock in write mode.
When the reader–writer lock is held in read mode, the data structure it protects can be read by multiple threads, as long as the threads first acquire the lock in read mode.
When a reader–writer lock is read locked, it is said to be locked in shared mode.
When it is write locked, it is said to be locked in exclusive mode.
As with mutexes, reader–writer locks must be initialized before use and destroyed before freeing their underlying memory.
We can pass a null pointer for attr if we want the reader–writer lock to have the default attributes.
It can be used to initialize a statically allocated reader–writer lock when the default attributes are sufficient.
The only error returns defined are when we use them improperly, such as with an uninitialized lock, or when we might deadlock by attempting to acquire a lock we already own.
However, be aware that specific implementations might define additional error returns.
The Single UNIX Specification also defines conditional versions of the reader–writer locking primitives.
These functions can be used to avoid deadlocks in situations where conforming to a lock hierarchy is difficult, as we discussed previously.
Example The program in Figure 11.14 illustrates the use of reader–writer locks.
A queue of job requests is protected by a single reader–writer lock.
This example shows a possible implementation of Figure 11.1, whereby multiple worker threads obtain jobs assigned to them by a single master thread.
In this example, we lock the queue’s reader–writer lock in write mode whenever we need to add a job to the queue or remove a job from the queue.
Whenever we search the queue, we grab the lock in read mode, allowing all the worker threads to search the queue concurrently.
Using a reader–writer lock will improve performance in this case only if threads search the queue much more frequently than they add or remove jobs.
The worker threads take only those jobs that match their thread ID off the queue.
Since the job structures are used only by one thread at a time, they don’t need any extra locking.
Just as with mutexes, the Single UNIX Specification provides functions to lock reader–writer locks with a timeout to give applications a way to avoid blocking indefinitely while trying to acquire a reader–writer lock.
The tsptr argument points to a timespec structure specifying the time at which the thread should stop blocking.
If they can’t acquire the lock, these functions return the ETIMEDOUT error when the timeout expires.
These synchronization objects provide a place for threads to rendezvous.
When used with mutexes, condition variables allow threads to wait in a race-free way for arbitrary conditions to occur.
A thread must first lock the mutex to change the condition state.
Other threads will not notice the change until they acquire the mutex, because the mutex must be locked to be able to evaluate the condition.
Before a condition variable is used, it must first be initialized.
A condition variable, represented by the pthread_cond_t data type, can be initialized in two ways.
Unless you need to create a conditional variable with nondefault attributes, the attr argument to pthread_cond_init can be set to NULL.
We use pthread_cond_wait to wait for a condition to be true.
A variant is provided to return an error code if the condition hasn’t been satisfied in the specified amount of time.
The caller passes it locked to the function, which then atomically places the calling thread on the list of threads waiting for the condition and unlocks the mutex.
This closes the window between the time that the condition is checked and the time that the thread goes to sleep waiting for the condition to change, so that the thread doesn’t miss a change in the condition.
The timeout value specifies how long we are willing to wait expressed as a timespec structure.
Just as we saw in Figure 11.13, we need to specify how long we are willing to wait as an absolute time instead of a relative time.
For example, suppose we are willing to wait 3 minutes.
However, this function is not yet supported on all platforms.
Alternatively, we can use the gettimeofday function to get the current time expressed as a timeval structure and translate it into a timespec structure.
There are two functions to notify threads that a condition has been satisfied.
We have to be careful to signal the threads only after changing the state of the condition.
Figure 11.15 shows an example of how to use a condition variable and a mutex together to synchronize threads.
We protect the condition with a mutex and evaluate the condition in a while loop.
When we put a message on the work queue, we need to hold the mutex, but we don’t need to hold the mutex when we signal the waiting threads.
As long as it is okay for a thread to pull the message off the queue before we call cond_signal, we can do this after releasing the mutex.
Since we check the condition in a while loop, this doesn’t present a problem; a thread will wake up, find that the queue is still empty, and go back to waiting again.
If the code couldn’t tolerate this race, we would need to hold the mutex when we signal the threads.
A spin lock is like a mutex, except that instead of blocking a process by sleeping, the process is blocked by busy-waiting (spinning) until the lock can be acquired.
A spin lock could be used in situations where locks are held for short periods of times and threads don’t want to incur the cost of being descheduled.
Spin locks are often used as low-level primitives to implement other types of locks.
Depending on the system architecture, they can be implemented efficiently using testand-set instructions.
Although efficient, they can lead to wasting CPU resources: while a thread is spinning and waiting for a lock to become available, the CPU can’t do anything else.
This is why spin locks should be held only for short periods of time.
Spin locks are useful when used in a nonpreemptive kernel: besides providing a mutual exclusion mechanism, they block interrupts so an interrupt handler can’t deadlock the system by trying to acquire a spin lock that is already locked (think of interrupts as another type of preemption)
In these types of kernels, interrupt handlers can’t sleep, so the only synchronization primitives they can use are spin locks.
However, at user level, spin locks are not as useful unless you are running in a realtime scheduling class that doesn’t allow preemption.
User-level threads running in a time-sharing scheduling class can be descheduled when their time quantum expires or when a thread with a higher scheduling priority becomes runnable.
In these cases, if a thread is holding a spin lock, it will be put to sleep and other threads blocked on the lock will continue spinning longer than intended.
Many mutex implementations are so efficient that the performance of applications using mutex locks is equivalent to their performance if they had used spin locks.
In fact, some mutex implementations will spin for a limited amount of time trying to acquire the mutex, and only sleep when the spin count threshold is reached.
These factors, combined with advances in modern processors that allow them to context switch at faster and faster rates, make spin locks useful only in limited circumstances.
The interfaces for spin locks are similar to those for mutexes, making it relatively easy to replace one with the other.
We can initialize a spin lock with the pthread_spin_init function.
Only one attribute is specified for spin locks, which matters only if the platform supports the Thread Process-Shared Synchronization option (now mandatory in the Single UNIX Specification; recall Figure 2.5)
The pshared argument represents the process-shared attribute, which indicates how the spin lock will be acquired.
Note that if a spin lock is currently unlocked, then the pthread_spin_lock function can lock it without spinning.
If the thread already has it locked, the results are undefined.
The call to pthread_spin_lock could fail with the EDEADLK error (or some other error), or the call could spin indefinitely.
If we try to unlock a spin lock that is not locked, the results are also undefined.
We need to be careful not to call any functions that might sleep while holding the spin lock.
If we do, then we’ll waste CPU resources by extending the time other threads will spin if they try to acquire it.
Barriers are a synchronization mechanism that can be used to coordinate multiple threads working in parallel.
A barrier allows each thread to wait until all cooperating threads have reached the same point, and then continue executing from there.
We’ve already seen one form of barrier—the pthread_join function acts as a barrier to allow one thread to wait until another thread exits.
They allow an arbitrary number of threads to wait until all of the threads have completed processing, but the threads don’t have to exit.
They can continue working after all threads have reached the barrier.
When we initialize a barrier, we use the count argument to specify the number of threads that must reach the barrier before all of the threads will be allowed to continue.
We use the attr argument to specify the attributes of the barrier object, which we’ll look at more closely in the next chapter.
For now, we can set attr to NULL to initialize a barrier with the default attributes.
This allows one thread to continue as the master to act on the results of the work done by all of the other threads.
Once the barrier count is reached and the threads are unblocked, the barrier can be used again.
Example Figure 11.16 shows how a barrier can be used to synchronize threads cooperating on a single task.
Worker thread to sort a portion of the set of numbers.
This example shows the use of a barrier in a simplified situation where the threads perform only one task.
In the example, we use eight threads to divide the job of sorting 8 million numbers.
Then the main thread calls a function to merge the results.
That is why we specify the barrier count as one more than the number of worker threads; the main thread counts as one waiter.
In this chapter, we introduced the concept of threads and discussed the POSIX.1 primitives available to create and destroy them.
We discussed five fundamental synchronization mechanisms — mutexes, reader–writer locks, condition variables, spin locks, and barriers — and we saw how to use them to protect shared resources.
In Chapter 11, we learned the basics about threads and thread synchronization.
In this chapter, we will learn the details of controlling thread behavior.
We will look at thread attributes and synchronization primitive attributes, which we ignored in the previous chapter in favor of the default behavior.
We will follow this with a look at how threads can keep data private from other threads in the same process.
Then we will wrap up the chapter with a look at how some process-based system calls interact with threads.
The Single UNIX Specification defines several limits associated with the operation of threads, which we didn’t show in Figure 2.11
As with other system limits, the thread limits can be queried using sysconf.
As with the other limits reported by sysconf, use of these limits is intended to promote application portability among different operating system implementations.
For example, if your application requires that you create four threads for every file you manage, you might have to limit the number of files you can manage concurrently if the system won’t let you create enough threads.
Figure 12.2 shows the values of the thread limits for the four implementations described in this book.
If the implementation’s limit is indeterminate, ‘‘no limit’’ is listed.
Note that although an implementation may not provide access to these limits, that doesn’t mean that the limits don’t exist.
It just means that the implementation doesn’t provide us with a way to get at them using sysconf.
The pthread interface allows us to fine-tune the behavior of threads and synchronization objects by setting various attributes associated with each object.
Generally, the functions for managing these attributes follow the same pattern:
Each object is associated with its own type of attribute object (threads with thread attributes, mutexes with mutex attributes, and so on)
This means that applications aren’t supposed to know anything about its internal structure, which promotes application portability.
An initialization function exists to set the attributes to their default values.
If the initialization function allocated any resources associated with the attributes object, the destroy function frees those resources.
Each attribute has a function to get the value of the attribute from the attribute object.
Because the function returns 0 on success or an error number on failure, the value is returned to the caller by storing it in the memory location specified by one of the arguments.
Each attribute has a function to set the value of the attribute.
In this case, the value is passed as an argument, by value.
We can use the pthread_attr_t structure to modify the default attributes, and associate these attributes with threads that we create.
POSIX.1 defines additional attributes in the Thread Execution Scheduling option, intended to support real-time applications, but we don’t discuss them here.
In Figure 12.3, we also show which platforms support each thread attribute.
In Section 11.5, we introduced the concept of detached threads.
If we are no longer interested in an existing thread’s termination status, we can use pthread_detach to allow the operating system to reclaim the thread’s resources when the thread exits.
If we know that we don’t need the thread’s termination status at the time we create the thread, we can arrange for the thread to start out in the detached state by modifying the detachstate thread attribute in the pthread_attr_t structure.
Example Figure 12.4 shows a function that can be used to create a thread in the detached state.
Nonetheless, if it does fail, cleaning up would be difficult: we would have to destroy the thread we just created, which might already be running, asynchronous to the execution of this function.
Support for thread stack attributes is optional for a POSIX-conforming operating system, but is required if the system supports the XSI option in the Single UNIX Specification.
If one of these symbols is defined, then the system supports the corresponding thread stack attribute.
With a process, the amount of virtual address space is fixed.
Since there is only one stack, its size usually isn’t a problem.
With threads, however, the same amount of virtual address space must be shared by all the thread stacks.
You might have to reduce your default thread stack size if your application uses so many threads that the cumulative size of their stacks exceeds the available virtual address space.
On the other hand, if your threads call functions that allocate large automatic variables or call functions many stack frames deep, you might need more than the default stack size.
The address specified by the stackaddr parameter is the lowest addressable address in the range of memory to be used as the thread’s stack, aligned at the proper boundary for the processor architecture.
Of course, this assumes that the virtual address range used by malloc or mmap is different from the range currently in use for a thread’s stack.
The stackaddr thread attribute is defined as the lowest memory address for the stack.
This is not necessarily the start of the stack, however.
If stacks grow from higher addresses to lower addresses for a given processor architecture, the stackaddr thread attribute will be the end of the stack instead of the beginning.
When setting the stacksize attribute, the size we choose can’t be smaller than PTHREAD_STACK_MIN.
The guardsize thread attribute controls the size of the memory extent after the end of the thread’s stack to protect against stack overflow.
Its default value is implementation defined, but a commonly used value is the system page size.
We can set the guardsize thread attribute to 0 to disable this feature: no guard buffer will be provided in this case.
If the guardsize thread attribute is modified, the operating system might round it up to an integral multiple of the page size.
If the thread’s stack pointer overflows into the guard area, the application will receive an error, possibly with a signal.
The Single UNIX Specification defines several other optional thread attributes intended for use by real-time applications.
Threads have other attributes not represented by the pthread_attr_t structure: the cancelability state and the cancelability type.
Just as threads have attributes, so too do their synchronization objects.
In Section 11.6.7, we saw how spin locks have one attribute called the process-shared attribute.
In this section, we discuss the attributes of mutexes, reader–writer locks, condition variables, and barriers.
Whenever we initialized a mutex in Chapter 11, we accepted the default attributes by using the.
There are three attributes of interest: the process-shared attribute, the robust attribute, and the type attribute.
Although this option is not required to be provided by POSIXconforming operating systems, the Single UNIX Specification requires that XSIconforming operating systems do support it.
Within a process, multiple threads can access the same synchronization object.
Access to shared data by multiple processes usually requires synchronization, just as does access to shared data by multiple threads.
The robust mutex attribute is related to mutexes that are shared among multiple processes.
It is meant to address the problem of mutex state recovery when a process terminates while holding a mutex.
When this happens, the mutex is left in a locked state and recovery is difficult.
Threads blocked on the lock in other processes will block indefinitely.
Applications can use this special return value as an indication that they need to recover whatever state the mutex was protecting, if possible (the details of what state is being protected and how it can be recovered will vary among applications)
Note that the EOWNERDEAD error return isn’t really an error in this case, because the caller will own the lock.
However, if we don’t use robust mutexes, then we can continue to check only for success and failure.
Of the four platforms covered in this text, only Linux 3.2.0 currently supports robust pthread mutexes.
If the application state can’t be recovered, the mutex will be in a permanently unusable state after the thread unlocks the mutex.
The type mutex attribute controls the locking characteristics of the mutex.
A recursive mutex maintains a lock count and isn’t released until it is unlocked the same number of times it is locked.
Thus, if you lock a recursive mutex twice and then unlock it, the mutex remains locked until it is unlocked a second time.
Implementations are free to map it to one of the other mutex types.
The behavior of the four types is summarized in Figure 12.5
The ‘‘Unlock when not owned’’ column refers to one thread unlocking a mutex that was locked by a different thread.
The ‘‘Unlock when unlocked’’ column refers to what happens when a thread unlocks a mutex that is already unlocked, which usually is a coding mistake.
Mutex type Relock without unlock? Unlock when not owned? Unlock when unlocked?
Recall from Section 11.6.6 that a mutex is used to protect the condition that is associated with a condition variable.
This allows other threads to acquire the mutex, change the condition, release the mutex, and signal the condition variable.
Since the mutex must be held to change the condition, it is not a good idea to use a recursive mutex.
Recursive mutexes are useful when you need to adapt existing single-threaded interfaces to a multithreaded environment, but can’t change the interfaces to your functions because of compatibility constraints.
However, using recursive locks can be tricky, and they should be used only when no other solution is possible.
Figure 12.6 illustrates a situation in which a recursive mutex might seem to solve a concurrency problem.
To keep the interfaces the same, we embed a mutex in the data structure whose address (x) is passed in as an argument.
This is possible only if we have provided an allocator function for the structure, so the application doesn’t know about its size (assuming we must increase its size when we add a mutex to it)
This is also possible if we originally defined the structure with enough padding to allow us now to replace some pad fields with a mutex.
Unfortunately, most programmers are unskilled at predicting the future, so this is not a common practice.
We could avoid using a recursive mutex if we could release.
This may not be acceptable, depending on what protection the mutex is intended to provide.
Figure 12.7 shows an alternative to using a recursive mutex in this case.
To call func2_locked, we must hold the mutex embedded in the data structure whose address we pass as the argument.
If we didn’t have to leave the interfaces to the library functions unchanged, we could have added a second parameter to each function to indicate whether the structure is locked by the caller.
It is usually better to leave the interfaces unchanged if we can, however, instead of polluting them with implementation artifacts.
The strategy of providing locked and unlocked versions of functions is usually applicable in simple situations.
In more complex situations, such as when the library needs to call a function outside the library, which then might call back into the library, we need to rely on recursive locks.
The program in Figure 12.8 illustrates another situation in which a recursive mutex is necessary.
Here, we have a ‘‘timeout’’ function that allows us to schedule another function to be run at some time in the future.
Assuming that threads are an inexpensive resource, we can create a thread for each pending timeout.
The thread waits until the time has been reached, and then it calls the function we’ve requested.
The problem arises when we can’t create a thread or when the scheduled time to run the function has already passed.
In these cases, we simply call the requested function now, from the current context.
Since the function acquires the same lock that we currently hold, a deadlock will occur unless the lock is recursive.
We use the makethread function from Figure 12.4 to create a thread in the detached state.
Because the func function argument passed to the timeout function will run in the future, we don’t want to wait around for the thread to complete.
We could call sleep to wait for the timeout to expire, but that gives us only second granularity.
If we want to wait for some time other than an integral number of seconds, we need to use nanosleep or clock_nanosleep, both of which allow us to sleep at higher resolution.
The caller of timeout needs to hold a mutex to check the condition and to schedule the retry function as an atomic operation.
The retry function will try to lock the same mutex.
Unless the mutex is recursive, a deadlock will occur if the timeout function calls retry directly.
The only attribute supported for reader–writer locks is the process-shared attribute.
Just as with the mutex process-shared attributes, a pair of functions is provided to get and set the process-shared attributes of reader–writer locks.
Although POSIX defines only one reader–writer lock attribute, implementations are free to define additional, nonstandard ones.
The Single UNIX Specification currently defines two attributes for condition variables: the process-shared attribute and the clock attribute.
As with the other attribute objects, a pair of functions initialize and deinitialize condition variable attribute objects.
The process-shared attribute is the same as with the other synchronization attributes.
It controls whether condition variables can be used by threads within a single process only or from within multiple processes.
Curiously, the Single UNIX Specification doesn’t define the clock attribute for any of the other attribute objects that have a wait function with a timeout.
The only barrier attribute currently defined is the process-shared attribute, which controls whether a barrier can be used by threads from multiple processes or only from within the process that initialized the barrier.
We discussed reentrant functions and signal handlers in Section 10.6
Threads are similar to signal handlers when it comes to reentrancy.
In both cases, multiple threads of control can potentially call the same function at the same time.
If a function can be safely called by multiple threads at the same time, we say that the function is thread-safe.
All functions defined in the Single UNIX Specification are guaranteed to be thread-safe, except those listed in Figure 12.9
In addition, the ctermid and tmpnam functions are not guaranteed to be thread-safe if they are passed a null pointer.
Similarly, there is no guarantee that wcrtomb and wcsrtombs are thread-safe when they are passed a null pointer for their mbstate_t argument.
Prior to Version 4 of the Single UNIX Specification, all XSI-conforming implementations were required to support thread-safe functions.
With SUSv4, however, thread-safe function support is now required for an implementation to be considered POSIX conforming.
With thread-safe functions, implementations provide alternative, thread-safe versions of some of the POSIX.1 functions that aren’t thread-safe.
The functions have the same names as their non-thread-safe relatives, but with an _r appended at the end of the name, signifying that these versions are reentrant.
Many functions are not thread-safe, because they return data stored in a static memory buffer.
They are made thread-safe by changing their interfaces to require that the caller provide its own buffer.
If a function is reentrant with respect to multiple threads, we say that it is thread-safe.
This doesn’t tell us, however, whether the function is reentrant with respect to signal handlers.
We say that a function that is safe to be reentered from an asynchronous signal handler is async-signal safe.
You can use flockfile and ftrylockfile to obtain a lock associated with a given FILE object.
This lock is recursive: you can acquire it again, while you already hold it, without deadlocking.
Although the exact implementation of the lock is unspecified, all standard I/O routines that manipulate FILE objects are required to behave as if they call flockfile and funlockfile internally.
Returns: 0 if OK, nonzero if lock can’t be acquired.
Although the standard I/O routines might be implemented to be thread-safe from the perspective of their own internal data structures, it is still useful to expose the locking to applications.
This allows applications to compose multiple calls to standard I/O functions into atomic sequences.
Of course, when dealing with multiple FILE objects, you need to beware of potential deadlocks and to order your locks carefully.
In this situation, we end up acquiring and releasing a lock for every character read or written.
To avoid this overhead, unlocked versions of the character-based standard I/O routines are available.
These four functions should not be called unless they are surrounded by calls to flockfile (or ftrylockfile) and funlockfile.
Otherwise, unpredictable results can occur (i.e., the types of problems that result from unsynchronized access to data by multiple threads of control)
Once you lock the FILE object, you can make multiple calls to these functions before releasing the lock.
This amortizes the locking overhead across the amount of data read or written.
If two threads call it at the same time, they will see inconsistent results, because the string returned is stored in a single static buffer that is shared by all threads calling getenv.
We show a reentrant version of getenv in Figure 12.12
To make getenv_r reentrant, we changed the interface so that the caller must provide its own buffer.
Thus each thread can use a different buffer to avoid interfering with the others.
Note, however, that this is not enough to make getenv_r thread-safe.
To make getenv_r thread-safe, we need to protect against changes to the environment while we are searching for the requested string.
We can use a mutex to serialize access to the environment list by getenv_r and putenv.
We could have used a reader–writer lock to allow multiple concurrent calls to getenv_r, but the added concurrency probably wouldn’t improve the performance of our program by very much, for two reasons.
First, the environment list usually isn’t very long, so we won’t hold the mutex for too long while we scan the list.
Second, calls to getenv and putenv are infrequent, so if we improve their performance, we won’t affect the overall performance of the program very much.
Even though we can make getenv_r thread-safe, that doesn’t mean that it is reentrant with respect to signal handlers.
If we were to use a nonrecursive mutex, we would run the risk that a thread would deadlock itself if it called getenv_r from a signal handler.
Thus we must use a recursive mutex to prevent other threads from changing the data structures while we look at them and to prevent deadlocks from signal handlers.
The problem is that the pthread functions are not guaranteed to be async-signal safe, so we can’t use them to make another function async-signal safe.
Thread-specific data, also known as thread-private data, is a mechanism for storing and finding data associated with a particular thread.
The reason we call the data thread-specific, or thread-private, is that we’d like each thread to access its own separate copy of the data, without worrying about synchronizing access with other threads.
Many people went to a lot of trouble designing a threads model that promotes sharing process data and attributes.
So why would anyone want to promote interfaces that prevent sharing in this model? There are two reasons.
First, sometimes we need to maintain data on a per-thread basis.
Since there is no guarantee that thread IDs are small, sequential integers, we can’t simply allocate an array of per-thread data and use the thread ID as the index.
Even if we could depend on small, sequential thread IDs, we’d like a little extra protection so that one thread can’t mess with another’s data.
The second reason for thread-private data is to provide a mechanism for adapting process-based interfaces to a multithreaded environment.
Older interfaces (before the advent of threads) defined errno as an integer that is accessible globally within the context of a process.
System calls and library routines set errno as a side effect of failing.
To make it possible for threads to use these same system calls and library.
Thus one thread making a call that sets errno doesn’t affect the value of errno for the other threads in the process.
Recall that all threads in a process have access to the entire address space of the process.
Other than using registers, there is no way for one thread to prevent another from accessing its data.
Even though the underlying implementation doesn’t prevent access, the functions provided to manage thread-specific data promote data separation among threads by making it more difficult for threads to gain access to thread-specific data from other threads.
Before allocating thread-specific data, we need to create a key to associate with the data.
The key will be used to gain access to the thread-specific data.
The key created is stored in the memory location pointed to by keyp.
The same key can be used by all threads in the process, but each thread will associate a different thread-specific data address with the key.
When the key is created, the data address for each thread is set to a null value.
In addition to creating a key, pthread_key_create associates an optional destructor function with the key.
When the thread exits, if the data address has been set to a non-null value, the destructor function is called with the data address as the only argument.
If destructor is null, then no destructor function is associated with the key.
When the thread exits normally, either by calling pthread_exit or by returning, the destructor is called.
Also, if the thread is canceled, the destructor is called, but only after the last cleanup handler returns.
Threads typically use malloc to allocate memory for their thread-specific data.
The destructor function usually frees the memory that was allocated.
If the thread exited without freeing the memory, then the memory would be lost—leaked by the process.
There can be a different destructor function for each key, or all of the keys can use the same function.
It is possible for the destructor to call another function that creates new thread-specific data and associate it with the key.
After all destructors are called, the system will check whether any non-null thread-specific values were associated with the keys and, if so, call the destructors again.
We can break the association of a key with the thread-specific data values for all threads by calling pthread_key_delete.
Note that calling pthread_key_delete will not invoke the destructor function associated with the key.
To free any memory associated with the key’s thread-specific data values, we need to take additional steps in the application.
We need to ensure that a key we allocate doesn’t change because of a race during initialization.
Depending on how the system schedules threads, some threads might see one key value, whereas other threads might see a different value.
The way to solve this race is to use pthread_once.
The initflag must be a nonlocal variable (i.e., global or static) and initialized to PTHREAD_ONCE_INIT.
The proper way to create a key without a race is as follows:
Returns: thread-specific data value or NULL if no value has been associated with the key.
In Figure 12.11, we showed a hypothetical implementation of getenv.
We came up with a new interface to provide the same functionality, but in a thread-safe way (Figure 12.12)
But what would happen if we couldn’t modify our application programs to use the new interface? In that case, we could use thread-specific data to maintain a per-thread copy of the data buffer used to hold the return string.
We use pthread_once to ensure that only one key is created for the thread-specific data we will use.
For the destructor function, we use free to free the memory previously allocated by malloc.
The destructor function will be called with the value of the thread-specific data only if the value is non-null.
Note that although this version of getenv is thread-safe, it is not async-signal safe.
Even if we made the mutex recursive, we could not make it reentrant with respect to signal handlers because it calls malloc, which itself is not async-signal safe.
Two thread attributes that are not included in the pthread_attr_t structure are the cancelability state and the cancelability type.
In the default case, a thread will continue to execute after a cancellation request is made until the thread reaches a cancellation point.
A cancellation point is a place where the thread checks whether it has been canceled, and if so, acts on the request.
When the state is enabled again, the thread will act on any pending cancellation requests at the next cancellation point.
Several of the functions listed in Figure 12.15, such as the ones dealing with message catalogs and wide character sets, are not discussed further in this text.
When cancellation is disabled, however, calls to pthread_testcancel have no effect.
The default cancellation type we have been describing is known as deferred cancellation.
After a call to pthread_cancel, the actual cancellation doesn’t occur until the thread hits a cancellation point.
Asynchronous cancellation differs from deferred cancellation in that the thread can be canceled at any time.
The thread doesn’t necessarily need to hit a cancellation point for it to be canceled.
Dealing with signals can be complicated even with a process-based paradigm.
Introducing threads into the picture makes things even more complicated.
Each thread has its own signal mask, but the signal disposition is shared by all threads in the process.
As a consequence, individual threads can block signals, but when a thread modifies the action associated with a given signal, all threads share the action.
Thus, if one thread chooses to ignore a given signal, another thread can undo that choice by restoring the default disposition or installing a signal handler for that signal.
Signals are delivered to a single thread in the process.
If the signal is related to a hardware fault, the signal is usually sent to the thread whose action caused the event.
Other signals, on the other hand, are delivered to an arbitrary thread.
In Section 10.12, we discussed how processes can use the sigprocmask function to block signals from delivery.
However, the behavior of sigprocmask is undefined in a multithreaded process.
A thread can wait for one or more signals to occur by calling sigwait.
The set argument specifies the set of signals for which the thread is waiting.
On return, the integer to which signop points will contain the number of the signal that was delivered.
If one of the signals specified in the set is pending at the time sigwait is called, then sigwait will return without blocking.
Before returning, sigwait removes the signal from the set of signals pending for the process.
If the implementation supports queued signals, and multiple instances of a signal are pending, sigwait will remove only one instance of the signal; the other instances will remain queued.
To avoid erroneous behavior, a  thread must block the signals it is waiting for before calling sigwait.
The sigwait function will atomically unblock the signals and wait until one is delivered.
If the signals are not blocked at the time that sigwait is called, then a timing window is opened up where one of the signals can be delivered to the thread before it completes its call to sigwait.
The advantage to using sigwait is that it can simplify signal handling by allowing us to treat asynchronously generated signals in a synchronous manner.
We can prevent the signals from interrupting the threads by adding them to each thread’s signal mask.
Then we can dedicate specific threads to handling the signals.
These dedicated threads can make function calls without having to worry about which functions are safe to call from a signal handler, because they are being called from normal thread context, not from a traditional signal handler interrupting a normal thread’s execution.
If multiple threads are blocked in calls to sigwait for the same signal, only one of the threads will return from sigwait when the signal is delivered.
The implementation could either allow sigwait to return or invoke the signal handler, but not both.
To send a signal to a process, we call kill (Section 10.9)
To send a signal to a thread, we call pthread_kill.
We can pass a signo value of 0 to check for existence of the thread.
If the default action for a signal is to terminate the process, then sending the signal to a thread will still kill the entire process.
Note that alarm timers are a  process resource, and all threads share the same set of alarms.
Thus, it is not possible for multiple threads in a process to use alarm timers without interfering (or cooperating) with one another (this is the subject of Exercise 12.6)
Recall that in Figure 10.23, we waited for the signal handler to set a flag indicating that the main program should exit.
The only threads of control that could run were the main thread and the signal handler, so blocking the signals was sufficient to avoid missing a change to the flag.
With threads, we need to use a mutex to protect the flag, as we show in Figure 12.16
Instead of relying on a signal handler that interrupts the main thread of control, we dedicate a separate thread of control to handle the signals.
Note that we block SIGINT and SIGQUIT in the beginning of the main thread.
When we create the thread to handle signals, the thread inherits the current signal mask.
Since sigwait will unblock the signals, only one thread is available to receive signals.
This enables us to code the main thread without having to worry about interrupts from these signals.
If we run this program, we get output similar to that from Figure 10.23:
When a thread calls fork, a copy of the entire process address space is made for the child.
The child is an entirely different process from the parent, and as long as neither one makes changes to its memory contents, copies of the memory pages can be shared between parent and child.
By inheriting a copy of the address space, the child also inherits the state of every mutex, reader–writer lock, and condition variable from the parent process.
If the parent consists of more than one thread, the child will need to clean up the lock state if it isn’t going to call exec immediately after fork returns.
It is made from a copy of the thread that called fork in the parent.
If the threads in the parent process hold any locks, the same locks will also be held in the child process.
The problem is that the child process doesn’t contain copies of the threads holding the locks, so there is no way for the child to know which locks are held and need to be unlocked.
This problem can be avoided if the child calls one of the exec functions directly after returning from fork.
In this case, the old address space is discarded, so the lock state doesn’t matter.
This is not always possible, however, so if the child needs to continue processing, we need to use a different strategy.
To avoid problems with inconsistent state in a multithreaded process, POSIX.1 states that only async-signal safe functions should be called by a child process between the time that fork returns and the time that the child calls one of the exec functions.
This limits what the child can do before calling exec, but doesn’t address the problem of lock state in the child process.
To clean up the lock state, we can establish fork handlers by calling the function pthread_atfork.
With pthread_atfork, we can install up to three functions to help clean up the locks.
The prepare fork handler is called in the parent before fork creates the child process.
This fork handler’s job is to acquire all locks defined by the parent.
The parent fork handler is called in the context of the parent after fork has created the child process, but before fork has returned.
This fork handler’s job is to unlock all the locks acquired by the prepare fork handler.
The child fork handler is called in the context of the child process before returning from fork.
Like the parent fork handler, the child fork handler must release all the locks acquired by the prepare fork handler.
Note that the locks are not locked once and unlocked twice, as it might appear.
When the child address space is created, it gets a copy of all locks that the parent defined.
Because the prepare fork handler acquired all the locks, the memory in the parent and the memory in the child start out with identical contents.
When the parent and the child unlock their ‘‘copy’’ of the locks, new memory is allocated for the child, and the memory contents from the parent are copied to the child’s memory (copy-onwrite), so we are left with a situation that looks as if the parent locked all its copies of the locks and the child locked all its copies of the locks.
The parent and the child end up unlocking duplicate locks stored in different memory locations, as if the following sequence of events occurred:
We can call pthread_atfork multiple times to install more than one set of fork handlers.
If we don’t have a need to use one of the handlers, we can pass a null pointer for the particular handler argument, and it will have no effect.
When multiple fork handlers are used, the order in which the handlers are called differs.
The parent and child fork handlers are called in the order in which they were registered, whereas the prepare fork handlers are called in the opposite order from which they were registered.
This ordering allows multiple modules to register their own fork handlers and still honor the locking hierarchy.
For example, assume that module A calls functions from module B and that each module has its own set of locks.
If the locking hierarchy is A before B, module B must install its fork handlers before module A.
When the parent calls fork, the following steps are taken, assuming that the child process runs before the parent:
The prepare fork handler from module A is called to acquire all of module A’s locks.
The prepare fork handler from module B is called to acquire all of module B’s locks.
The child fork handler from module B is called to release all of module B’s locks in the child process.
The child fork handler from module A is called to release all of module A’s locks in the child process.
The parent fork handler from module A is called to release all of module A’s locks in the parent process.
If the fork handlers serve to clean up the lock state, what cleans up the state of.
However, an implementation that uses a lock as part of the implementation of condition variables will require cleaning up.
The problem is that no interface exists to allow us to do this.
If the lock is embedded in the condition variable data structure, then we can’t use condition variables after calling fork, because there is no portable way to clean up its state.
On the other hand, if an implementation uses a global lock to protect all condition variable data structures in a process, then the implementation itself can clean up the lock in the fork library routine.
Application programs shouldn’t rely on implementation details like this, however.
The prepare fork handler acquires them both, the child fork handler releases them in the context of the child process, and the parent fork handler releases them in the context of the parent process.
When we run this program, we get the following output:
As we can see, the prepare fork handler runs after fork is called, the child fork handler runs before fork returns in the child, and the parent fork handler runs before fork returns in the parent.
Although the pthread_atfork mechanism is intended to make locking state consistent after a fork, it has several drawbacks that make it usable in only limited circumstances:
There is no good way to reinitialize the state for more complex synchronization objects such as condition variables and barriers.
Some implementations of error-checking mutexes will generate errors when the child fork handler tries to unlock a mutex that was locked by the parent.
Recursive mutexes can’t be cleaned up in the child fork handler, because there is no way to determine the number of times one has been locked.
If child processes are allowed to call only async-signal safe functions, then the child fork handler shouldn’t even be able to clean up synchronization objects, because none of the functions that are used to manipulate them are async-signal safe.
The practical problem is that a synchronization object might be in an intermediate state when one thread calls fork, but the synchronization object can’t be cleaned up unless it is in a consistent state.
If an application calls fork in a signal handler (which is legal, because fork is async-signal safe), then the fork handlers registered by pthread_atfork can call only async-signal safe functions, or else the results are undefined.
We introduced the pread and pwrite functions in Section 3.11
These functions are helpful in a multithreaded environment, because all threads in a process share the same file descriptors.
Consider two threads reading from or writing to the same file descriptor at the same time.
If thread A executes the call to lseek and then thread B calls lseek before thread A calls read, then both threads will end up reading the same record.
To solve this problem, we can use pread to make the setting of the offset and the reading of the data one atomic operation.
We can use pwrite to solve the problem of concurrent threads writing to the same file.
Threads provide an alternative model for partitioning concurrent tasks in UNIX systems.
They promote sharing among separate threads of control, but present unique synchronization problems.
In this chapter, we looked at how we can fine-tune our threads and their synchronization primitives.
We also looked at how threads interact with some of the process-oriented system calls.
Make sure that your implementation is async-signal safe as well as thread-safe.
They are often started when the system is bootstrapped and terminate only when the system is shut down.
Because they don’t have a controlling terminal, we say that they run in the background.
In this chapter, we look at the process structure of daemons and explore how to write a daemon.
Since a daemon does not have a controlling terminal, we need to see how a daemon can report error conditions when something goes wrong.
For a discussion of the historical background of the term daemon as it applies to computer systems, see Raymond [1996]
The ps(1) command prints the status of various processes in the system.
There are a multitude of options—consult your system’s manual for all the details.
The -a option shows the status of processes owned by others, and -x shows processes that don’t have a controlling terminal.
The -j option displays the job-related information: the session ID, process group ID, controlling terminal, and terminal process group ID.
Under System V–based systems, a similar command is ps -efj.
In an attempt to improve security, some UNIX systems don’t allow us to use ps to look at any processes other than our own.
We have removed a few columns that don’t interest us, such as the accumulated CPU time.
The column headings, in order, are the user ID, process ID, parent process ID, process group ID, session ID, terminal name, and command string.
The session ID is simply the process ID of the session leader.
The system processes you see will depend on the operating system implementation.
Anything with a parent process ID of 0 is usually a kernel process started as part of the system bootstrap procedure.
An exception is init, which is a user-level command started by the kernel at boot time.
Kernel processes are special and generally exist for the entire lifetime of the system.
They run with superuser privileges and have no controlling terminal and no command line.
In the sample ps output, kernel daemons appear with their names in square brackets.
This version of Linux uses a special kernel process, kthreadd, to create other kernel processes, so kthreadd appears as the parent of the other kernel daemons.
Each kernel component that needs to perform work in a process context, but that isn’t invoked from the context of a user-level process, will usually have its own kernel daemon.
The kswapd daemon is also known as the pageout daemon.
It supports the virtual memory subsystem by writing dirty pages to disk slowly over time, so the pages can be reclaimed.
The flush daemon flushes dirty pages to disk when available memory reaches a configured minimum threshold.
It also flushes dirty pages back to disk at regular intervals to decrease data loss in the event of a system failure.
The sync_supers daemon periodically flushes file system metadata to disk.
The jbd daemon helps implement the journal in the ext4 file system.
It is a system daemon responsible for, among other things, starting system services specific to various run levels.
These services are usually implemented with the help of their own daemons.
The rpcbind daemon provides the service of mapping RPC (Remote Procedure Call) program numbers to network port numbers.
The rsyslogd daemon is available to any program to log system messages for an administrator.
The messages may be printed on a console device and also written to a file.
It listens on the system’s network interfaces for incoming requests for various network servers.
Note that the first four are kernel daemons, while the last three are user-level daemons.
The cron daemon executes commands at regularly scheduled dates and times.
Numerous system administration tasks are handled by cron running programs at regularly intervals.
The atd daemon is similar to cron; it allows users to execute jobs at specified times, but it executes each job once only, instead of repeatedly at regularly scheduled times.
The cupsd daemon is a print spooler; it handles print requests on the system.
The sshd daemon provides secure remote login and execution facilities.
Note that most of the daemons run with superuser (root) privileges.
None of the daemons has a controlling terminal: the terminal name is set to a question mark.
The lack of a controlling terminal in the user-level daemons is probably the result of the daemons having called setsid.
Most of the user-level daemons are process group leaders and session leaders, and are the only processes in their process group and session.
Finally, note that the parent of the user-level daemons is the init process.
Some basic rules to coding a daemon prevent unwanted interactions from happening.
We state these rules here and then show a function, daemonize, that implements them.
The inherited file mode creation mask could be set to deny certain permissions.
If the daemon process creates files, it may want to set specific permissions.
For example, if it creates files with group-read and group-write enabled, a file mode creation mask that turns off either of these permissions would undo its efforts.
On the other hand, if the daemon calls library functions that result in files being created, then it might make sense to set the file mode create mask to a more restrictive value (such as 007), since the library functions might not allow the caller to specify the permissions through an explicit argument.
First, if the daemon was started as a simple shell command, having the parent terminate makes the shell think that the command is done.
Second, the child inherits the process group ID of the parent but gets a new process ID, so we’re guaranteed that the child is not a process group leader.
This is a prerequisite for the call to setsid that is done next.
The process (a) becomes the leader of a new session, (b) becomes the leader of a new process group, and (c) is disassociated from its controlling terminal.
Under System V–based systems, some people recommend calling fork again at this point, terminating the parent, and continuing the daemon in the child.
This guarantees that the daemon is not a session leader, which prevents it from acquiring a controlling terminal under the System V rules (Section 9.6)
Alternatively, to avoid acquiring a controlling terminal, be sure to specify O_NOCTTY whenever opening a terminal device.
The current working directory inherited from the parent could be on a mounted file system.
Since daemons normally exist until the system is rebooted, if the daemon stays on a mounted file system, that file system cannot be unmounted.
Alternatively, some daemons might change the current working directory to a specific location where they will do all their work.
For example, a line printer spooling daemon might change its working directory to its spool directory.
This prevents the daemon from holding open any descriptors that it may have inherited from its parent (which could be a shell or some other process)
Even if the daemon was started from an interactive session, the daemon runs in the background, and the login session can terminate without affecting the daemon.
If other users log in on the same terminal device, we wouldn’t want output from the daemon showing up on the terminal, and the users wouldn’t expect their input to be read by the daemon.
Figure 13.1 shows a function that can be called from a program that wants to initialize itself as a daemon.
Change the current working directory to the root so * we won’t prevent file systems from being unmounted.
If the daemonize function is called from a main program that then goes to sleep, we can check the status of the daemon with the ps command:
This means that our daemon is in an orphaned process group (Section 9.10) and is not a session leader and, therefore, has no chance of allocating a controlling terminal.
This is a result of performing the second fork in the daemonize function.
We can see that our daemon has been initialized correctly.
One problem a daemon has is how to handle error messages.
It can’t simply write to standard error, since it shouldn’t have a controlling terminal.
We don’t want all the daemons writing to the console device, because on many workstations the console device runs a windowing system.
We also don’t want each daemon writing its own error messages into a separate file.
It would be a headache for anyone administering the system to keep up with which daemon writes to which log file and to check these files on a regular basis.
The BSD syslog facility was developed at Berkeley and used widely in 4.2BSD.
Until SVR4, System V never had a central daemon logging facility.
The syslog function is included in the XSI option in the Single UNIX Specification.
The BSD syslog facility has been widely used since 4.2BSD.
These messages can be read by any user process that opens and reads the /dev/klog device.
We won’t describe this function any further, since we’re not interested in writing kernel routines.
Most user processes (daemons) call the syslog(3) function to generate log messages.
This causes the message to be sent to the UNIX domain datagram socket /dev/log.
Note that the syslog function never generates these UDP datagrams: they require explicit network programming by the process generating the log message.
Refer to Stevens, Fenner, and Rudoff [2004] for details on UNIX domain sockets and UDP sockets.
Normally, the syslogd daemon reads all three forms of log messages.
For example, urgent messages can be sent to the system administrator (if logged in) and printed on the console, whereas warnings may be logged to a file.
If it’s not called, the first time syslog is called, openlog is called automatically.
Calling closelog is also optional—it just closes the descriptor that was being used to communicate with the syslogd daemon.
Calling openlog lets us specify an ident that is added to each log message.
Figure 13.3 describes the available options, including a bullet in the XSI column if the option is included in the openlog definition in the Single UNIX Specification.
The facility argument for openlog is taken from Figure 13.4
Note that the Single UNIX Specification defines only a subset of the facility codes typically available on a given platform.
The reason for the facility argument is to let the configuration file specify that messages from different facilities are to be handled differently.
If we don’t call openlog, or if we call it with a facility of 0, we can still specify the facility as part of the priority argument to syslog.
These levels are ordered by priority, from highest to lowest.
If the log message can’t be sent to syslogd via the UNIX domain datagram, the message is written to the console instead.
Open the UNIX domain datagram socket to the syslogd daemon immediately; don’t wait until the first message is logged.
Normally, the socket is not opened until the first message is logged.
Do not wait for child processes that might have been created in the process of logging the message.
This prevents conflicts with applications that catch SIGCHLD, since the application might have retrieved the child’s status by the time that syslog calls wait.
Delay the opening of the connection to the syslogd daemon until the first message is logged.
LOG_PERROR Write the log message to standard error in addition to sending it to syslogd.
This is intended for daemons that fork a child process to handle different requests (as compared to daemons, such as syslogd, that never call fork)
The format argument and any remaining arguments are passed to the vsprintf function for formatting.
Any occurrences of the characters %m in format are first replaced with the error message string (strerror) corresponding to the value of errno.
The setlogmask function can be used to set the log priority mask for the process.
When the log priority mask is set, messages are not logged unless their priority is set in the log priority mask.
Note that attempts to set the log priority mask to 0 will have no effect.
The logger(1) program is also provided by many systems as a way to send log messages to the syslog facility.
Some implementations allow optional arguments to this program, specifying the facility, level, and ident, although the Single UNIX Specification doesn’t define any options.
The logger command is intended for a shell script running noninteractively that needs to generate log messages.
In a (hypothetical) line printer spooler daemon, you might encounter the sequence.
The first call sets the ident string to the program name, specifies that the process ID should always be printed, and sets the default facility to the line printer system.
The call to syslog specifies an error condition and a message string.
If we had not called openlog, the second call could have been.
In addition to syslog, many platforms provide a variant that handles variable argument lists.
All four platforms described in this book provide vsyslog, but this function is not included in the Single UNIX Specification.
Most syslogd implementations will queue messages for a short time.
If a duplicate message arrives during this period, the syslog daemon will not write it to the log.
Instead, the daemon prints a message similar to ‘‘last message repeated N times.’’
Some daemons are implemented so that only a single copy of the daemon should be running at a time for proper operation.
Such a daemon might need exclusive access to a device, for example.
In the case of the cron daemon, if multiple instances were running, each copy might try to start a single scheduled operation, resulting in duplicate operations and probably an error.
If the daemon needs to access a device, the device driver will sometimes prevent multiple attempts to open the corresponding device node in /dev.
This restricts us to one copy of the daemon running at a time.
If no such device is available, however, we need to do the work ourselves.
The file- and record-locking mechanism provides the basis for one way to ensure that only one copy of a daemon is running.
If each daemon creates a file with a fixed name and places a write lock on the entire file, only one such write lock will be allowed to be created.
Successive attempts to create write locks will fail, serving as an indication to successive copies of the daemon that another instance is already running.
If the daemon obtains a write-lock on an entire file, the lock will be removed automatically if the daemon exits.
This simplifies recovery, eliminating the need for us to clean up from the previous instance of the daemon.
The function shown in Figure 13.6 illustrates the use of file and record locking to ensure that only one copy of a daemon is running.
Figure 13.6 Ensure that only one copy of a daemon is running.
Each copy of the daemon will try to create a file and write its process ID in the file.
If the file is already locked, the lockfile function will fail with errno set to EACCES or EAGAIN, so we return 1, indicating that the daemon is already running.
We need to truncate the file, because the previous instance of the daemon might have had a process ID larger than ours, with a larger string length.
Tr uncating the file prevents data from the previous daemon appearing as if it applies to the current daemon.
Several common conventions are followed by daemons in the UNIX System.
If the daemon uses a lock file, the file is usually stored in /var/run.
Note, however, that the daemon might need superuser permissions to create a file here.
The name of the file is usually name.pid, where name is the name of the daemon or the service.
If the daemon supports configuration options, they are usually stored in /etc.
The configuration file is named name.conf, where name is the name of the daemon or the name of the service.
For example, the configuration for the syslogd daemon is usually /etc/syslog.conf.
If the daemon should be restarted automatically when it exits, we can arrange for init to restart it if we include a respawn entry for it in /etc/inittab (assuming the system uses a System V style init command)
If a daemon has a configuration file, the daemon reads the file when it starts, but usually won’t look at it again.
If an administrator changes the configuration, the daemon would need to be stopped and restarted to account for the configuration changes.
To avoid this, some daemons will catch SIGHUP and reread their configuration files when they receive the signal.
Since they aren’t associated with terminals and are either session leaders without controlling terminals or members of orphaned process groups, daemons have no reason to expect to receive SIGHUP.
The program shown in Figure 13.7 shows one way a daemon can reread its configuration file.
The program uses sigwait and multiple threads, as discussed in Section 12.8
Make sure only one copy of the daemon is running.
We call daemonize from Figure 13.1 to initialize the daemon.
At this point, SIGHUP is still ignored, so we need to reset the disposition to the default behavior; otherwise, the thread calling sigwait may never see the signal.
We block all signals, as is recommended for multithreaded programs, and create a thread to handle signals.
The thread’s only job is to wait for SIGHUP and SIGTERM.
When it receives SIGHUP, the thread calls reread to reread its configuration file.
When it receives SIGTERM, the thread logs a message and exits.
Recall from Figure 10.1 that the default action for SIGHUP and SIGTERM is to terminate the process.
Because we block these signals, the daemon will not die when one of them is sent to the process.
Instead, the thread calling sigwait will return with an indication that the signal has been received.
The program in Figure 13.8 shows how a singlethreaded daemon can catch SIGHUP and reread its configuration file.
Make sure only one copy of the daemon is running.
After initializing the daemon, we install signal handlers for SIGHUP and SIGTERM.
We can either place the reread logic in the signal handler or just set a flag in the handler and have the main thread of the daemon do all the work instead.
A common use for a daemon process is as a server process.
Indeed, in Figure 13.2, we can call the syslogd process a server that has messages sent to it by user processes (clients) using a UNIX domain datagram socket.
In general, a server is a process that waits for a client to contact it, requesting some type of service.
In Figure 13.2, the service being provided by the syslogd server is the logging of an error message.
In Figure 13.2, the communication between the client and the server is one way.
The client sends its service request to the server; the server sends nothing back to the client.
In the upcoming chapters, we’ll see numerous examples of two-way communication between a client and a server—the client sends a request to the server, and the server sends a reply back to the client.
It is common to find servers that fork and exec another program to provide service to a client.
These servers often manage multiple file descriptors: communication endpoints, configuration files, log files, and the like.
At best, it would be careless to leave these file descriptors open in the child process, because they probably won’t be used in the program executed by the child, especially if the program is unrelated to the server.
At worst, leaving them open could pose a security problem — the program executed could do something malicious, such as change the server’s configuration file or trick the client into thinking it is communicating with the server, thereby gaining access to unauthorized information.
An easy solution to this problem is to set the close-on-exec flag for all file descriptors that the executed program won’t need.
Figure 13.9 shows a function that we can use in a server process to do just this.
Daemon processes are running all the time on most UNIX systems.
In this chapter, we developed a function that can be called by a daemon process to initialize itself correctly.
We also discussed the ways a daemon can log error messages, since a daemon normally doesn’t have a controlling terminal.
We discussed several conventions that daemons follow on most UNIX systems and showed examples of how to implement some of these conventions.
What happens if the user process (the daemon) calls chroot before calling openlog?
The only user-level daemon that isn’t a session leader is the rsyslogd process.
After calling this function, call getlogin (Section 8.15) to see whether the process has a login name now that it has become a daemon.
This chapter covers numerous topics and functions that we lump under the term advanced I/O : nonblocking I/O, record locking, I/O multiplexing (the select and poll functions), asynchronous I/O, the readv and writev functions, and memory-mapped I/O (mmap)
The slow system calls are those that can block forever.
Reads that can block the caller forever if data isn’t present with certain file types (pipes, terminal devices, and network devices)
Writes that can block the caller forever if the data can’t be accepted immediately by these same file types (e.g., no room in the pipe, network flow control)
Opens that block until some condition occurs on certain file types (such as an open of a terminal device that waits until an attached modem answers the phone, or an open of a FIFO for writing only, when no other process has the FIFO open for reading)
Reads and writes of files that have mandatory record locking enabled.
We also said that system calls related to disk I/O are not considered slow, even though the read or write of a disk file can block the caller temporarily.
Nonblocking I/O lets us issue an I/O operation, such as an open, read, or write, and not have it block forever.
If the operation cannot be completed, the call returns immediately with an error noting that the operation would have blocked.
There are two ways to specify nonblocking I/O for a given descriptor.
Figure 3.12 shows a function that we can call to turn on any of the file status flags for a descriptor.
Instead of affecting only the file status flags for the descriptor, the flags for either the terminal device or the socket were also changed to be nonblocking, thereby affecting all users of the terminal or socket, not just the users sharing the same file table entry (4.3BSD nonblocking I/O worked only on terminals and sockets)
Also, 4.3BSD returned EWOULDBLOCK if an operation on a nonblocking descriptor could not complete without blocking.
These systems provide nonblocking semantics consistent with other POSIX-compatible systems: changes in file status flags affect all users of the same file table entry, but are independent of accesses to the same device through other file table entries.
The output is in a loop, with the results of each write being printed on the standard error.
This new function simply clears one or more of the flag bits.
If the standard output is a regular file, we expect the write to be executed once:
But if the standard output is a terminal, we expect the write to return a partial count sometimes and an error at other times.
The amount of data accepted by the terminal driver varies from system to system.
The results will also vary depending on how you are logged in to the system: on the system console, on a hard-wired terminal, on a network connection using a pseudo terminal.
If you are running a windowing system on your terminal, you are also going through a pseudo terminal device.
This type of loop, called polling, is a waste of CPU time on a multiuser system.
In Section 14.4, we’ll see that I/O multiplexing with a nonblocking descriptor is a more efficient way to do this.
Sometimes, we can avoid using nonblocking I/O by designing our applications to use multiple threads (see Chapter 11)
We can allow individual threads to block in I/O calls if we can continue to make progress in other threads.
This can sometimes simplify the design, as we shall see in Chapter 21; at other times, however, the overhead of synchronization can add more complexity than is saved from using threads.
What happens when two people edit the same file at the same time? In most UNIX systems, the final state of the file corresponds to the last process that wrote the file.
In some applications, however, such as a database system, a process needs to be certain that it alone is writing to a file.
To provide this capability for processes that need it, commercial UNIX systems provide record locking.
In Chapter 20, we develop a database library that uses record locking.
Record locking is the term normally used to describe the ability of a process to prevent other processes from modifying a region of a file while the first process is reading or modifying that portion of the file.
Under the UNIX System, ‘‘record’’ is a misnomer; the UNIX kernel does not have a notion of records in a file.
A better term is byte-range locking, given that it is a range of a file (possibly the entire file) that is locked.
Histor y One of the criticisms of early UNIX systems was that they couldn’t be used to run database systems, because they did not support locking portions of files.
As UNIX systems found their way into business computing environments, various groups added support for record locking (differently, of course)
This function locks only entire files, not regions of a file.
Record locking was added to System V Release 3 through the fcntl function.
The lockf function was built on top of this, providing a simplified interface.
These functions allowed callers to lock arbitrary byte ranges in a file, ranging from the entire file down to a single byte within the file.
Figure 14.2 shows the forms of record locking provided by various systems.
Figure 14.2 Forms of record locking supported by various UNIX systems.
We describe the difference between advisory locking and mandatory locking later in this section.
In this text, we describe only the POSIX.1 fcntl locking.
The system call entry into the kernel was a function named locking.
This function provided mandatory record locking and propagated through many versions of System III.
Let’s repeat the prototype for the fcntl function from Section 3.14
The third argument (which we’ll call flockptr) is a pointer to an flock structure.
The ID (l_pid) of the process holding the lock that can block the current.
Numerous rules apply to the specification of the region to be locked or unlocked.
The two elements that specify the starting offset of the region are similar to the last two arguments of the lseek function (Section 3.6)
Locks can start and extend beyond the current end of file, but cannot start or extend before the beginning of the file.
This allows us to lock a region starting anywhere in the file, up through and including any data that is appended to the file.
We don’t have to try to guess how many bytes might be appended to the file.
The basic rule is that any number of processes can have a shared read lock on a given byte, but only one process can have an exclusive write lock on a given byte.
Furthermore, if there are one or more read locks on a byte, there can’t be any write locks on that byte; if there is an exclusive write lock on a byte, there can’t be any read locks on that byte.
The compatibility rule applies to lock requests made from different processes, not to multiple lock requests made by a single process.
If a process has an existing lock on a range of a file, a subsequent attempt to place a lock on the same range by the same process will replace the existing lock with the new one.
To obtain a read lock, the descriptor must be open for reading; to obtain a write lock, the descriptor must be open for writing.
We can now describe the three commands for the fcntl function.
F_GETLK Determine whether the lock described by flockptr is blocked by some other lock.
If a lock exists that would prevent ours from being created, the information on that existing lock overwrites the information pointed to by flockptr.
Although POSIX allows an implementation to return either error code, all four implementations described in this text return EAGAIN if the locking request cannot be satisfied.
If the requested read lock or write lock cannot be granted because another process currently has some part of the requested region locked, the calling process is put to sleep.
The process wakes up either when the lock becomes available or when interrupted by a signal.
We have no guarantee that, between the two fcntl calls, some other process won’t come in and obtain the same lock.
If we don’t want to block while waiting for a lock to become available to us, we must handle the possible error returns from F_SETLK.
Note that POSIX.1 doesn’t specify what happens when one process read locks a range of a file, a second process blocks while trying to get a write lock on the same range, and a third processes then attempts to get another read lock on the range.
If the third process is allowed to place a read lock on the range just because the range is already read locked, then the implementation might starve processes with pending write locks.
Thus, as additional requests to read lock the same range arrive, the time that the process with the pending write-lock request has to wait is extended.
If the read-lock requests arrive quickly enough without a lull in the arrival rate, then the writer could wait for a long time.
When setting or releasing a lock on a file, the system combines or splits adjacent areas as required.
The resulting picture would be the first diagram in Figure 14.4, the same as when we started.
Figure 14.5 Function to lock or unlock a region of a file.
Since most locking calls are to lock or unlock a region (the command F_GETLK is rarely used), we normally use one of the following five macros, which are defined in apue.h (Appendix B)
We have purposely defined the first three arguments to these macros in the same order as the lseek function.
If a lock exists that would block the request specified by the arguments, this function returns the process ID of the process holding the lock.
We normally call this function from the following two macros (defined in apue.h):
Note that the lock_test function can’t be used by a process to see whether it is currently holding a portion of a file locked.
The definition of the F_GETLK command states that the information returned applies to an existing lock that would prevent us from creating our own lock.
Deadlock occurs when two processes are each waiting for a resource that the other has locked.
The potential for deadlock exists if a process that controls a locked region is put to sleep when it tries to lock another region that is controlled by a different process.
Each then tries to lock the other’s already locked byte.
When a deadlock is detected, the kernel has to choose one process to receive the error return.
In this example, the parent was chosen, but this is an implementation detail.
On some systems, you might even see the errors split between the child and the parent as multiple lock attempts are made.
Three rules govern the automatic inheritance and release of record locks.
The first is obvious: when a process terminates, all its locks are released.
The second is far from obvious: whenever a descriptor is closed, any locks on the file referenced by that descriptor for that process are released.
The same thing would happen if we replaced the dup with open, as in.
Locks are never inherited by the child across a fork.
This means that if a process obtains a lock and then calls fork, the child is considered another process with regard to the lock that was obtained by the parent.
The child has to call fcntl to obtain its own locks on any descriptors that were inherited across the fork.
This constraint makes sense because locks are meant to prevent multiple processes from writing to the same file at the same time.
If the child inherited locks across a fork, both the parent and the child could write to the same file at the same time.
Locks are inherited by a new program across an exec.
Note, however, that if the close-on-exec flag is set for a file descriptor, all locks for the underlying file are released when the descriptor is closed as part of an exec.
Let’s take a brief look at the data structures used in the FreeBSD implementation.
This should help clarify rule 1, which states that locks are associated with a process and a file.
Consider a process that executes the following statements (ignoring error returns):
Figure 14.8 shows the resulting data structures after both the parent and the child have paused.
What is new here are the lockf structures that are linked together from the i-node structure.
Each lockf structure describes one locked region (defined by an offset and length) for a given process.
When any one of these three file descriptors is closed, the kernel goes through.
The kernel can’t tell (and doesn’t care) which descriptor of the three was used by the parent to obtain the lock.
Figure 14.9 Place a write lock on an entire file.
Alternatively, we could define the lockfile function in terms of the write_lock function:
We need to use caution when locking or unlocking byte ranges relative to the end of file.
Often, however, we need to specify a lock relative to the file’s current length, but we can’t call fstat to obtain the current file size, since we don’t have a lock on the file.
There’s a chance that another process could change the file’s length between the call to fstat and the lock call.
This sequence of code might not do what you expect.
It obtains a write lock from the current end of the file onward, covering any future data we might append to the file.
Assuming that we are at end of file when we perform the first write, this operation will extend the file by one byte, and that byte will be locked.
The unlock operation that follows has the effect of removing the locks for future writes that append data to the file, but it leaves a lock on the last byte in the file.
When the second write occurs, the end of file is extended by one byte, but this byte is not locked.
The state of the file locks for this sequence of steps is shown in Figure 14.10
When a portion of a file is locked, the kernel converts the offset specified into an absolute file offset.
The kernel needs to remember the locks independent of the current file offset or end of file, because the current offset and end of file can change, and changes to these attributes shouldn’t affect the state of existing locks.
If all the functions in the library handle record locking in a consistent way, then we say that any set of processes using these functions to access a database are cooperating processes.
It is feasible for these database access functions to use advisory locking if they are the only ones being used to access the database.
But advisory locking doesn’t prevent some other process that has write permission for the database file from writing whatever it wants to the database file.
This rogue process would be an uncooperating process, since it’s not using the accepted method (the library of database functions) to access the database.
Mandatory locking causes the kernel to check every open, read, and write to verify that the calling process isn’t violating a lock on the file being accessed.
Mandatory record locking is not part of the Single UNIX Specification.
On Linux, if you want mandatory locking, you need to enable it on a per file system basis by using the -o mand option to the mount command.
Mandatory locking is enabled for a particular file by turning on the set-group-ID bit and turning off the group-execute bit.
Since the set-group-ID bit makes no sense when the group-execute bit is off, the designers of SVR3 chose this way to specify that the locking for a file is to be mandatory locking and not advisory locking.
What happens to a process that tries to read or write a file that has mandatory locking enabled and that part of the file is currently locked by another process? The answer depends on the type of operation (read or write), the type of lock held by the other process (read lock or write lock), and whether the descriptor for the read or write is nonblocking.
Type of existing Blocking descriptor, Nonblocking descriptor, tries to tries tolock on region held.
Figure 14.11 Effect of mandatory locking on reads and writes by other processes.
In addition to the read and write functions in Figure 14.11, the open function can be affected by mandatory record locks held by another process.
Normally, open succeeds, even if the file being opened has outstanding mandatory record locks.
The next read or write follows the rules listed in Figure 14.11
Only Solaris treats the O_CREAT flag as an error case.
Linux allows the O_CREAT flag to be specified when opening a file with an outstanding mandatory lock.
Generating the open error for O_TRUNC makes sense, because the file cannot be truncated if it is read locked or write locked by another process.
Generating the error for O_CREAT, however, makes little sense; this flag says to create the file only if it doesn’t already exist, but it has to exist to be record locked by another process.
This handling of locking conflicts with open can lead to surprising results.
While developing the exercises in this section, a test program was run that opened a file (whose mode specified mandatory locking), established a read lock on an entire file, and then went to sleep for a while.
Recall from Figure 14.11 that a read lock should prevent writing to the file by other processes.
During this sleep period, the following behavior was seen in other typical UNIX System programs.
The same file could be edited with the ed editor, and the results written back to disk! The mandatory record locking had no effect at all.
Using the system call trace feature provided by some versions of the UNIX System, it was seen that ed.
The mandatory record locking has no effect on the unlink function, which allowed this to happen.
The vi editor was never able to edit the file.
It could read the file’s contents, but whenever we tried to write new data to the file, EAGAIN was returned.
If we tried to append new data to the file, the write blocked.
The Bourne shell, however, doesn’t specify O_CREAT if the file already exists, so the open succeeds but the next write blocks.
Results will vary, depending on the version of the operating system you are using.
The bottom line, as demonstrated by this exercise, is to be wary of mandatory record locking.
As seen with the ed example, it can be circumvented.
Mandatory record locking can also be used by a malicious user to hold a read lock on a file that is publicly readable.
Of course, the file has to have mandatory record locking enabled for this to occur, which may require the user to be able to change the permission bits of the file.
Consider a database file that is world readable and has mandatory record locking enabled.
If a malicious user were to hold a read lock on the entire file, the file could not be written to by other processes.
We can run the program in Figure 14.12 to determine whether our system supports mandatory locking.
This program creates a file and enables mandatory locking for the file.
The program then splits into parent and child, with the parent obtaining a write lock on the entire file.
The child first sets its descriptor to be nonblocking and then attempts to obtain a read lock on the file, expecting to get an error.
This lets us see whether the system returns EACCES or EAGAIN.
Next, the child rewinds the file and tries to read from the file.
If mandatory locking is provided, the read should return EACCES or EAGAIN (since the descriptor is nonblocking)
Running this program under Solaris 10 (which supports mandatory locking) gives us.
Let’s return to the first question posed in this section: what happens when two people edit the same file at the same time? The normal UNIX System text editors do not use record locking, so the answer is still that the final result of the file corresponds to the last process that wrote the file.
Some versions of the vi editor use advisory record locking.
Even if we were using one of these versions of vi, it still doesn’t prevent users from running another editor that doesn’t use advisory record locking.
If the system provides mandatory record locking, we could modify our favorite editor to use it (if we have the editor’s source code)
Not having the source code for the editor, we might try the following.
We write our own program that is a front end to vi.
This program immediately calls fork, and the parent just waits for the child to complete.
The child opens the file specified on the command line, enables mandatory locking, obtains a write lock on the entire file, and then executes vi.
While vi is running, the file is write locked, so other users can’t modify it.
When vi terminates, the parent’s wait returns and our front end terminates.
A small front-end program of this type can be written, but it doesn’t work.
The problem is that it is common practice for editors to read their input file and then close it.
A lock is released on a file whenever a descriptor that references that file is closed.
As a result, when the editor closes the file after reading its contents, the lock is gone.
There is no way to prevent this from happening in the front-end program.
We’ll use record locking in Chapter 20 in our database library to provide concurrent access to multiple processes.
We’ll also provide some timing measurements to see how record locking affects a process.
When we read from one descriptor and write to another, we can use blocking I/O in a loop, such as.
We see this form of blocking I/O over and over again.
What if we have to read from two descriptors? In this case, we can’t do a blocking read on either descriptor, as data may appear on one descriptor while we’re blocked in a read on the other.
In this program, we read from the terminal (standard input) and write to a network connection, and we read from the network connection and write to the terminal (standard output)
At the other end of the network connection, the telnetd daemon reads what we typed and presents it to a shell as if we were logged in to the remote machine.
The telnetd daemon sends any output generated by the commands we type back to us through the telnet command, to be displayed on our terminal.
We can’t do a blocking read on either of the inputs, as we never know which input will have data for us.
One way to handle this particular problem is to divide the process in two pieces (using fork), with each half handling one direction of data.
The cu(1) command provided with System V’s uucp communication package was structured like this.
If we use two processes, we can let each process do a blocking read.
But this leads to a problem when the operation terminates.
But if the parent terminates (the user enters an end-of-file character at the terminal), then the parent has to tell the child to stop.
We can use a signal for this (SIGUSR1, for example), but it does complicate the program somewhat.
Instead of two processes, we could use two threads in a single process.
This avoids the termination complexity, but requires that we deal with synchronization between the threads, which could add more complexity than it saves.
We could use nonblocking I/O in a single process by setting both descriptors to be nonblocking and issuing a read on the first descriptor.
If data is present, we read it and process it.
If there is no data to read, the call returns immediately.
We then do the same thing with the second descriptor.
After this, we wait for some amount of time (a few seconds, perhaps) and then try to read from the first descriptor again.
Most of the time, there won’t be data to read, so we waste time performing the read system calls.
We also have to guess how long to wait each time around the loop.
Although it works on any system that supports nonblocking I/O, polling should be avoided on a multitasking system.
With this technique, we tell the kernel to notify us with a signal when a descriptor is ready for I/O.
First, although systems provide their own limited forms of asynchronous I/O, POSIX chose to standardize a different set of interfaces, so portability can be an issue.
In the past, POSIX asynchronous I/O was an optional facility in the Single UNIX Specification, but these interfaces are required as of SUSv4
System V provides the SIGPOLL signal to support a limited form of asynchronous I/O, but this signal works only if the descriptor refers to a STREAMS device.
The second problem with this technique is that the limited forms use only one signal per process (SIGPOLL or SIGIO)
If we enable this signal for two descriptors (in the example we’ve been talking about, reading from two descriptors), the occurrence of the signal doesn’t tell us which descriptor is ready.
Although the POSIX.1 asynchronous I/O interfaces allow us to select which signal to use for notification, the number of signals we can use is still far less than the number of possible open file descriptors.
To determine which descriptor is ready, we would need to set each file descriptor to nonblocking mode and try the descriptors in sequence.
To do this, we build a list of the descriptors that we are interested in (usually more than one descriptor) and call a function that doesn’t return until one of the descriptors is ready for I/O.
Three functions —poll, pselect, and select—allow us to perform I/O multiplexing.
On return from these functions, we are told which descriptors are ready for I/O.
Check the select manual page to see what your system supports.
I/O multiplexing was provided with the select function in 4.2BSD.
This function has always worked with any descriptor, although its main use has been for terminal I/O and network I/O.
SVR3 added the poll function when the STREAMS mechanism was added.
In SVR4, support was added to allow poll to work on any descriptor.
The select function lets us do I/O multiplexing under all POSIX-compatible platforms.
We can wait forever, wait a fixed amount of time, or not wait at all.
The total count of the number of descriptors that are ready.
Which descriptors are ready for each of the three conditions (read, write, or exception condition)
With this return information, we can call the appropriate I/O function (usually read or write) and know that the function won’t block.
It specifies how long we want to wait in terms of seconds and microseconds (recall Section 4.20)
POSIX.1 allows an implementation to modify the timeval structure, so after select returns, you can’t rely on the structure containing the same values it did before calling select.
The middle three arguments—readfds, writefds, and exceptfds—are pointers to descriptor sets.
These three sets specify which descriptors we’re interested in and for which conditions (readable, writable, or an exception condition)
A descriptor set is stored in an fd_set data type.
This data type is chosen by the implementation so that it can hold one bit for each possible descriptor.
We can consider it to be just a big array of bits, as shown in Figure 14.15
Figure 14.15 Specifying the read, write, and exception descriptors for select.
The only thing we can do with the fd_set data type is allocate a variable of this type, assign a variable of this type to another variable of the same type, or use one of the following four functions on a variable of this type.
These interfaces can be implemented as either macros or functions.
To turn on a single bit in a set, we use FD_SET.
Finally, we can test whether a given bit is turned on in the set with FD_ISSET.
After declaring a descriptor set, we must zero the set using FD_ZERO.
We then set bits in the set for each descriptor that we’re interested in, as in.
On return from select, we can test whether a given bit in the set is still on using FD_ISSET:
Any (or all) of the middle three arguments to select (the pointers to the descriptor sets) can be null pointers if we’re not interested in that condition.
If all three pointers are NULL, then we have a higher-precision timer than is provided by sleep.
Recall from Section 10.19 that sleep waits for an integral number of seconds.
With select, we can wait for intervals less than one second; the actual resolution depends on the system’s clock.
Some applications need many more descriptors, but these UNIX programs are atypical.
By specifying the highest descriptor that we’re interested in, we can prevent the kernel from going through hundreds of unused bits in the three descriptor sets, looking for bits that are turned on.
As an example, Figure 14.16 shows what two descriptor sets look like if we write.
A return value of 0 means that no descriptors are ready.
This happens if the time limit expires before any of the descriptors are ready.
When this happens, all the descriptor sets will be zeroed out.
A positive return value specifies the number of descriptors that are ready.
This value is the sum of the descriptors ready in all three sets, so if the same descriptor is ready to be read and written, it will be counted twice in the return value.
The only bits left on in the three descriptor sets are the bits corresponding to the descriptors that are ready.
We now need to be more specific about what ‘‘ready’’ means.
A descriptor in the read set (readfds) is considered ready if a read from that descriptor won’t block.
A descriptor in the write set (writefds) is considered ready if a write to that descriptor won’t block.
A descriptor in the exception set (exceptfds) is considered ready if an exception condition is pending on that descriptor.
Currently, an exception condition corresponds to either the arrival of out-of-band data on a network connection or certain conditions occurring on a pseudo terminal that has been placed into packet mode.
File descriptors for regular files always return ready for reading, writing, and exception conditions.
It is important to realize that whether a descriptor is blocking or not doesn’t affect whether select blocks.
Similarly, if we specify an infinite timeout, select blocks until data is ready for the descriptor or until a signal is caught.
If we encounter the end of file on a descriptor, that descriptor is considered readable by select.
We then call read and it returns 0—the way to signify end of file on UNIX systems.
Many people incorrectly assume that select indicates an exception condition on a descriptor when the end of file is reached.
The pselect function is identical to select, with the following exceptions.
The timeout value for select is specified by a timeval structure, but for pselect, a timespec structure is used.
Recall the definition of the timespec structure in Section 4.2
Instead of seconds and microseconds, the timespec structure represents the timeout value in seconds and nanoseconds.
This provides a higher-resolution timeout if the platform supports that fine a level of granularity.
The timeout value for pselect is declared const, and we are guaranteed that its value will not change as a result of calling pselect.
If sigmask is NULL, pselect behaves as select does with respect to signals.
Otherwise, sigmask points to a signal mask that is atomically installed when pselect is called.
The poll function is similar to select, but the programmer interface is different.
This function was originally introduced in System V to support the STREAMS subsystem, but we are able to use it with any type of file descriptor.
With poll, instead of building a set of descriptors for each condition (readability, writability, and exception condition) as we did with select, we build an array of pollfd structures, with each array element specifying a descriptor number and the conditions that we’re interested in for that descriptor:
The number of elements in the fdarray array is specified by nfds.
Historically, there have been differences in how the nfds parameter was declared.
SVR3 specified the number of elements in the array as an unsigned long, which seems excessive.
But the actual prototype in the <poll.h> header still showed the second argument as an unsigned long.
The Single UNIX Specification defines the new type nfds_t to allow the implementation to select the appropriate type and hide the details from applications.
Note that this type has to be large enough to hold an integer, since the return value represents the number of entries in the array with satisfied events.
We use the first declaration to reiterate that fdarray points to an array of structures and not a pointer to a single structure.
To tell the kernel which events we’re interested in for each descriptor, we have to set the events member of each array element to one or more of the values in Figure 14.17
On return, the revents member is set by the kernel, thereby specifying which events have occurred for each descriptor.
This behavior differs from that of select, which modifies its arguments to indicate what is ready.
The first four rows of Figure 14.17 test for readability, the next three test for writability, and the final three are for exception conditions.
The last three rows in Figure 14.17 are set by the kernel on return.
These three values are returned in revents when the condition occurs, even if they weren’t specified in the events field.
The poll event names containing the term BAND refer to priority bands in STREAMS.
Refer to Rago [1993] for more information about STREAMS and priority bands.
When a descriptor is hung up (POLLHUP), we can no longer write to the descriptor.
There may, however, still be data to be read from the descriptor.
The final argument to poll specifies how long we want to wait.
All the specified descriptors are tested, and we return immediately.
This is a way to poll the system to find out the status of multiple descriptors, without blocking in the call to poll.
We return when one of the specified descriptors is ready or when the timeout expires.
If your system doesn’t provide millisecond resolution, timeout is rounded up to the nearest supported value.
It is important to realize the difference between an end of file and a hangup.
If we’re entering data from the terminal and type the end-of-file character, POLLIN is turned on so we can read the end-of-file indication (read returns 0)
If we’re reading from a modem and the telephone line is hung up, we’ll receive the POLLHUP notification.
As with select, whether a descriptor is blocking doesn’t affect whether poll blocks.
This characteristic continues with most systems even if the SA_RESTART option is specified.
None of the implementations described in this book restart poll or select when a signal is received, even if the SA_RESTART flag is used.
Using select and poll, as described in the previous section, is a synchronous form of notification.
The system doesn’t tell us anything until we ask (by calling either select or poll)
As we saw in Chapter 10, signals provide an asynchronous form of notification that something has happened.
All systems derived from BSD and System V provide some form of asynchronous I/O, using a signal (SIGPOLL in System V; SIGIO in BSD) to notify the process that something of interest has happened on a descriptor.
As mentioned in the previous section, these forms of asynchronous I/O are limited: they don’t work with all file types and they allow the use of only one signal.
If we enable more than one descriptor for asynchronous I/O, we cannot tell which descriptor the signal corresponds to when the signal is delivered.
Version 4 of the Single UNIX Specification moved the general asynchronous I/O mechanism from the real-time extensions to the base specification.
This mechanism addresses the limitations that exist with these older asynchronous I/O facilities.
Before we look at the different ways to use asynchronous I/O, we need to discuss the costs.
When we decide to use asynchronous I/O, we complicate the design of our application by choosing to juggle multiple concurrent operations.
A simpler approach may be to use multiple threads, which would allow us to write the program using a synchronous model, and let the threads run asynchronous to each other.
We incur additional complexity when we use the POSIX asynchronous I/O interfaces:
We have to worry about three sources of errors for every asynchronous operation: one associated with the submission of the operation, one associated with the result of the operation itself, and one associated with the functions used to determine the status of the asynchronous operations.
The interfaces themselves involve a lot of extra setup and processing rules compared to their conventional counterparts, as we shall see.
We can’t really call the non-asynchronous I/O function calls ‘‘synchronous,’’ because although they are synchronous with respect to the program flow, they aren’t synchronous with respect to the I/O.
We call a write ‘‘synchronous’’ if the data we write is persistent when we return from the call to the write function.
We also can’t differentiate the conventional I/O function calls from the asynchronous ones by referring to the conventional calls as the ‘‘standard’’ I/O calls, because this confuses them with the function calls in the standard I/O library.
To avoid confusion, we’ll refer to the read and write functions as the ‘‘conventional’’ I/O function calls in this section.
For example, if we submit multiple asynchronous writes and one fails, how should we proceed? If the writes are related, we might have to undo the ones that succeeded.
System V provides a limited form of asynchronous I/O that works only with STREAMS devices and STREAMS pipes.
To enable asynchronous I/O for a STREAMS device, we have to call ioctl with a second argument (request) of I_SETSIG.
The third argument is an integer value formed from one or more of the constants in Figure 14.18
Interfaces related to the STREAMS mechanism were marked obsolescent in SUSv4, so we don’t cover them in any detail.
S_INPUT We can read data (other than high-priority data) without blocking.
S_MSG The SIGPOLL signal message has reached the stream head.
In addition to calling ioctl to specify the conditions that should generate the SIGPOLL signal, we have to establish a signal handler for this signal.
Recall from Figure 10.1 that the default action for SIGPOLL is to terminate the process, so we should establish the signal handler before calling ioctl.
Asynchronous I/O in BSD-derived systems is a combination of two signals: SIGIO and SIGURG.
The former is the general asynchronous I/O signal, and the latter is used only to notify the process that out-of-band data has arrived on a network connection.
To receive the SIGIO signal, we need to perform three steps.
Establish a signal handler for SIGIO, by calling either signal or sigaction.
Step 3 can be performed only on descriptors that refer to terminals or networks, which is a fundamental limitation of the BSD asynchronous I/O facility.
The POSIX asynchronous I/O interfaces give us a consistent way to perform asynchronous I/O, regardless of the type of file.
These interfaces were adopted from the real-time draft standard, which themselves were an option in the Single UNIX Specification.
In Version 4, the Single UNIX Specification moved these interfaces to the base, so they are now required to be supported by all platforms.
The asynchronous I/O interfaces use AIO control blocks to describe I/O operations.
It contains at least the fields shown in the following structure (implementations might include additional fields):
The aio_fildes field is the file descriptor open for the file to be read or written.
The read or write starts at the offset specified by aio_offset.
For a read, data is copied to the buffer that begins at the address specified by aio_buf.
The aio_nbytes field contains the number of bytes to read or write.
Note that we have to provide an explicit offset when we perform asynchronous I/O.
The asynchronous I/O interfaces don’t affect the file offset maintained by the operating system.
This won’t be a problem as long as we never mix asynchronous I/O functions with conventional I/O functions on the same file in a process.
The other fields don’t correspond to the conventional I/O functions.
The aio_reqprio field is a hint that gives applications a way to suggest an ordering for the asynchronous I/O requests.
The system has only limited control over the exact ordering, however, so there is no guarantee that the hint will be honored.
The aio_lio_opcode field is used only with list-based asynchronous I/O, which we’ll.
The aio_sigevent field controls how the application is notified about the completion of the I/O event.
SIGEV_NONE The process is not notified when the asynchronous I/O request completes.
If the application has elected to catch the signal and has specified the SA_SIGINFO flag when establishing the signal handler, the signal is queued (if the implementation supports queued signals)
It is passed the sigev_value field as its only argument.
When these functions return success, the asynchronous I/O request has been queued for processing by the operating system.
The return value bears no relation to the result of the actual I/O operation.
While the I/O operation is pending, we have to be careful to ensure that the AIO control block and data buffer remain stable; their underlying memory must remain valid and we can’t reuse them until the I/O operation completes.
To force all pending asynchronous writes to persistent storage without waiting, we can set up an AIO control block and call the aio_fsync function.
The aio_fildes field in the AIO control block indicates the file whose asynchronous writes are synched.
If the op argument is set to O_DSYNC, then the operation behaves like a call to fdatasync.
Otherwise, if op is set to O_SYNC, the operation behaves like a call to fsync.
The data won’t be persistent until the asynchronous synch completes.
To determine the completion status of an asynchronous read, write, or synch operation, we need to call the aio_error function.
We need to call the aio_return function to obtain the return value from the operation.
If the asynchronous operation succeeded, we can call the aio_return function to get the asynchronous operation’s return value.
Until the asynchronous operation completes, we need to be careful to avoid calling the aio_return function.
We also need to be careful to call aio_return only one time per asynchronous I/O operation.
Once we call this function, the operating system is free to deallocate the record containing the I/O operation’s return value.
We use asynchronous I/O when we have other processing to do and we don’t want to block while performing the I/O operation.
However, when we have completed the processing and find that we still have asynchronous operations outstanding, we can call the aio_suspend function to block until an operation completes.
The list argument is a pointer to an array of AIO control blocks and the nent argument indicates the number of entries in the array.
Null pointers in the array are skipped; the other entries must point to AIO control blocks that have been used to initiate asynchronous I/O operations.
When we have pending asynchronous I/O operations that we no longer want to complete, we can attempt to cancel them with the aio_cancel function.
The fd argument specifies the file descriptor with the outstanding asynchronous I/O operations.
If the aiocb argument is NULL, then the system attempts to cancel all outstanding asynchronous I/O operations on the file.
Otherwise, the system attempts to cancel the single asynchronous I/O operation described by the AIO control block.
We say that the system ‘‘attempts’’ to cancel the operations, because there is no guarantee that the system will be able to cancel any operations that are in progress.
AIO_ALLDONE All of the operations completed before the attempt to cancel them.
AIO_NOTCANCELED At least one of the requested operations could not be.
If an asynchronous I/O operation is successfully canceled, calling the aio_error function on the corresponding AIO control block will return the error ECANCELED.
If the operation can’t be canceled, then the corresponding AIO control block is unchanged by the call to aio_cancel.
One additional function is included with the asynchronous I/O interfaces, although it can be used in either a synchronous or an asynchronous manner.
The lio_listio function submits a set of I/O requests described by a list of AIO control blocks.
The mode argument determines whether the I/O is truly asynchronous.
The process is notified asynchronously when all of the I/O operations complete, as specified by the sigev argument.
If we don’t want to be notified, we can set sigev to NULL.
Note that the individual AIO control blocks themselves may also enable asynchronous notification when an individual operation completes.
The asynchronous notification specified by the sigev argument is in addition to these, and is sent only when all of the I/O operations complete.
The list argument points to a list of AIO control blocks specifying the I/O operations to perform.
The nent argument specifies the number of elements in the array.
The list of AIO control blocks can contain NULL pointers; these entries are ignored.
A read is treated as if the corresponding AIO control block had been passed to the aio_read function.
Similarly, a write is treated as if the AIO control block had been passed to aio_write.
Implementations can limit the number of asynchronous I/O operations we are allowed to have outstanding.
The limits are runtime invariants, and are summarized in Figure 14.19
The POSIX asynchronous I/O interfaces were originally introduced to provide realtime applications with a way to avoid being blocked while performing I/O operations.
Now we’ll look at an example of how to use the interfaces.
We don’t discuss real-time programming in this text, but because the POSIX asynchronous I/O interfaces are now part of the base specification in the Single UNIX Specification, we’ll look at how to use them.
To compare the asynchronous I/O interfaces with their conventional counterparts, we’ll look at the task of translating a file from one format to another.
The algorithm rotates the characters ’a’ to ’z’ and ’A’ to ’Z’ by 13 positions, but leaves all other characters unchanged.
The I/O portion of the program is straightforward: we read a block from the input file, translate it, and then write the block to the output file.
We repeat this until we hit the end of file and read returns zero.
The program in Figure 14.21 shows how to perform the same task using the equivalent asynchronous I/O functions.
Read from the input file if more data * remains unread.
A read is complete; translate the buffer * and write it.
Note that we use eight buffers, so we can have up to eight asynchronous I/O requests pending.
Surprisingly, this might actually reduce performance—if the reads are presented to the file system out of order, it can defeat the operating system’s readahead algorithm.
As long as we have work to do, we can submit asynchronous I/O operations.
When we have an unused AIO control block, we can submit an asynchronous read request.
When a read completes, we translate the buffer contents and then submit an asynchronous write request.
When all AIO control blocks are in use, we wait for an operation to complete by calling aio_suspend.
When we write a block to the output file, we retain the same offset at which we read the data from the input file.
This strategy works only because each character in the input file has a corresponding character in the output file at the same offset; we neither add nor delete characters in the output file.
We don’t use asynchronous notification in this example, because it is easier to use a synchronous programming model.
If we had something else to do while the I/O operations were in progress, then the additional work could be folded into the for loop.
If we needed to prevent this additional work from delaying the task of translating the file, however, then we might have to structure the code to use some form of asynchronous notification.
With multiple tasks, we need to prioritize the tasks before deciding how the program should be structured.
The readv and writev functions let us read into and write from multiple noncontiguous buffers in a single function call.
The second argument to both functions is a pointer to an array of iovec structures:
The number of elements in the iov array is specified by iovcnt.
Figure 14.22 shows a diagram relating the arguments to these two functions and the iovec structure.
The readv function scatters the data into the buffers in order, always filling one buffer before proceeding to the next.
A count of 0 is returned if there is no more data and the end of file is encountered.
These two functions are included in the XSI option of the Single UNIX Specification.
The second buffer to output is an argument passed by the caller, and the first buffer is one we create, containing the length of the second buffer and a file offset of other information in the file.
The solution we use in Section 20.8 is to use writev, but it’s instructive to compare it to the other two solutions.
Figure 14.23 shows the results from the three methods just described.
The test program has three separate cases—one for each of the techniques measured in Figure 14.23
We used times (Section 8.17) to obtain the user CPU time, system CPU time, and wall clock time before and after the writes.
As we expect, the system time increases when we call write twice, compared to calling either write or writev once.
Next, note that the sum of the CPU times (user plus system) is slightly less when we do a buffer copy followed by a single write compared to a single call to writev.
With the single write, we copy the buffers to a staging buffer at user level, and then the kernel will copy the data to its internal buffers when we call write.
With writev, we should do less copying, because the kernel only needs to copy the data directly into its staging buffers.
The fixed cost of using writev for such small amounts of data, however, is greater than the benefit.
As the amount of data we need to copy increases, the more expensive it will be to copy the buffers in our program, and the writev alternative will be more attractive.
Don’t infer too much about the relative performance of Linux and Mac OS X from the numbers shown in Figure 14.23
The two computers were very different: they had different processor generations, different amounts of RAM, and disks with different speeds.
To do an apples-toapples comparison of one operating system to another, we need to use the same hardware for each operating system.
In summary, we should always try to use the fewest number of system calls necessary to get the job done.
If we are writing small amounts of data, we will find it less expensive to copy the data ourselves and use a single write instead of using writev.
We might find, however, that the performance benefits aren’t worth the extra complexity cost needed to manage our own staging buffers.
Pipes, FIFOs, and some devices—notably terminals and networks—have the following two properties.
A read operation may return less than asked for, even though we have not encountered the end of file.
This is not an error, and we should simply continue reading from the device.
This may be caused by kernel output buffers becoming full, for example.
Again, it’s not an error, and we should continue writing the remainder of the data.
Normally, this short return from a write occurs only with a nonblocking descriptor or if a signal is caught.
We’ll never see this happen when reading or writing a disk file, except when the file system runs out of space or we hit our quota limit and we can’t write all that we requested.
Generally, when we read from or write to a pipe, network device, or terminal, we need to take these characteristics into consideration.
We can use the readn and writen functions to read and write N bytes of data, respectively, letting these functions handle a return value that’s possibly less than requested.
These two functions simply call read or write as many times as required to read or write the entire N bytes of data.
We define these functions as a convenience for later examples, similar to the error-handling routines used in many of the examples in this text.
The readn and writen functions are not part of any standard.
We call writen whenever we’re writing to one of the file types that we mentioned, but we call readn only when we know ahead of time that we will be receiving a certain number of bytes.
Figure 14.24 shows implementations of readn and writen that we will use in later examples.
Note that if we encounter an error and have previously read or written any data, we return the amount of data transferred instead of the error.
Memory-mapped I/O lets us map a file on disk into a buffer in memory so that, when we fetch bytes from the buffer, the corresponding bytes of the file are read.
Similarly, when we store data in the buffer, the corresponding bytes are automatically written to the file.
This lets us perform I/O without using read or write.
Memory-mapped I/O has been in use with virtual memory systems for many years.
These two functions were then removed in 4.2BSD and were intended to be replaced with the mmap function.
Version 4 of the Single UNIX Specification moved the mmap function from an option to the base specification.
To use this feature, we have to tell the kernel to map a given file to a region in memory.
Returns: starting address of mapped region if OK, MAP_FAILED on error.
The addr argument lets us specify the address where we want the mapped region to start.
We normally set this value to 0 to allow the system to choose the starting address.
The return value of this function is the starting address of the mapped area.
The fd argument is the file descriptor specifying the file that is to be mapped.
We have to open this file before we can map it into the address space.
The len argument is the number of bytes to map, and off is the starting offset in the file of the bytes to map.
Some restrictions on the value of off are described later.
The prot argument specifies the protection of the mapped region.
The protection specified for a region can’t allow more access than the open mode of the file.
For example, we can’t specify PROT_WRITE if the file was opened read-only.
Before looking at the flag argument, let’s see what’s going on here.
Recall the memory layout of a typical process, shown in Figure 7.6
In this figure, ‘‘start addr’’ is the return value from mmap.
The flag argument affects various attributes of the mapped region.
Use of this flag is discouraged, as it hinders portability.
If this flag is not specified and if addr is nonzero, then the kernel uses addr as a hint of where to place the mapped region, but there is no guarantee that the requested address will be used.
Support for the MAP_FIXED flag is optional on POSIX-conforming systems, but required on XSI-conforming systems.
MAP_SHARED This flag describes the disposition of store operations into the mapped region by this process.
This flag specifies that store operations modify the mapped file—that is, a store operation is equivalent to a write to the file.
Either this flag or the next (MAP_PRIVATE), but not both, must be specified.
MAP_PRIVATE This flag says that store operations into the mapped region cause a private copy of the mapped file to be created.
One use of this flag is for a debugger that maps the text portion of a program file but allows the user to modify the instructions.
Any modifications affect the copy, not the original program file.
Each implementation has additional MAP_xxx flag values, which are specific to that implementation.
Check the mmap(2) manual page on your system for details.
The value of off and the value of addr (if MAP_FIXED is specified) are usually required to be multiples of the system’s virtual memory page size.
Since off and addr are often specified as 0, this requirement is not a big deal.
Although the Single UNIX Specification no longer requires that this condition be satisfied, all the platforms covered in this book, except FreeBSD 8.0, have this requirement.
FreeBSD 8.0 allows us to use any address alignment and offset alignment as long as the alignments match.
We can modify the final 500 bytes, but any changes we make to them are not reflected in the file.
We must first grow the file, as we will see in Figure 14.27
This signal can also be generated if we try to store into a mapped region that we specified to mmap as read-only.
The SIGBUS signal can be generated if we access a portion of the mapped region that does not make sense at the time of the access.
For example, assume that we map a file using the file’s size, but before we reference the mapped region, the file’s size is truncated by some other process.
If we then try to access the memory-mapped region corresponding to the end portion of the file that was truncated, we’ll receive SIGBUS.
A memory-mapped region is inherited by a child across a fork (since it’s part of the parent’s address space), but for the same reason, is not inherited by the new program across an exec.
We can change the permissions on an existing mapping by calling mprotect.
The legal values for prot are the same as those for mmap (Figure 14.25)
Be aware that implementations may require the address argument to be an integral multiple of the system’s page size.
When we modify pages that we’ve mapped into our address space using the MAP_SHARED flag, the changes aren’t written back to the file immediately.
When the changes are written back, they are written in units of pages.
Thus, if we modify only one byte in a page, when the change is written back to the file, the entire page will be written.
If the pages in a shared mapping have been modified, we can call msync to flush the changes to the file that backs the mapping.
The msync function is similar to fsync (Section 3.13), but works on memory-mapped regions.
If the mapping is private, the file mapped is not modified.
As with the other memory-mapped functions, the address must be aligned on a page boundary.
The flags argument allows us some control over how the memory is flushed.
We can specify the MS_ASYNC flag to simply schedule the pages to be written.
If we want to wait for the writes to complete before returning, we can use the MS_SYNC flag.
An optional flag, MS_INVALIDATE, lets us tell the operating system to discard any pages that are out of sync with the underlying storage.
Some implementations will discard all pages in the specified range when we use this flag, but this behavior is not required.
The msync function is included in the XSI option in the Single UNIX Specification.
A memory-mapped region is automatically unmapped when the process terminates or we can unmap a region directly by calling the munmap function.
Closing the file descriptor used when we mapped the region does not unmap the region.
The munmap function does not affect the object that was mapped—that is, the call to munmap does not cause the contents of the mapped region to be written to the disk file.
The updating of the disk file for a MAP_SHARED region happens automatically by the kernel’s virtual memory algorithm sometime after we store into the memory-mapped region.
Modifications to memory in a MAP_PRIVATE region are discarded when the region is unmapped.
We first open both files and then call fstat to obtain the size of the input file.
We need this size for the call to mmap for the input file, and we also need to set the size of the output file.
We call ftruncate to set the size of the output file.
If we don’t set the output file’s size, the call to mmap for the output file is successful, but the first reference to the associated memory region generates a SIGBUS signal.
We then call mmap for each file, to map the file into memory, and finally call memcpy to copy data from the input buffer to the output buffer.
We copy at most 1 GB of data at a time to limit the amount of memory we use (it might not be possible to map the entire contents of a very large file if the system doesn’t have enough memory)
Before mapping the next sections of the files, we unmap the previous sections.
As the bytes of data are fetched from the input buffer (src), the input file is automatically read by the kernel; as the data is stored in the output buffer (dst), the data is automatically written to the output file.
Exactly when the data is written to the file depends on the system’s page management algorithms.
Some systems have daemons that write dirty pages to disk slowly over time.
If we want to ensure that the data is safely written to the file, we need to call msync with the MS_SYNC flag before exiting.
Let’s compare this memory-mapped file copy to a copy that is done by calling read and write (with a buffer size of 8,192)
The times are given in seconds and the size of the file copied was 300 MB.
Note that we don’t synch the data to disk before exiting.
On Solaris, copying using mmap and memcpy takes more user time but less system time than copying using read and write.
On Linux, the results are similar for the user time, but the system time for using read and write is slightly better than using mmap and memcpy.
The two versions do the same work, but they go about it differently.
The major difference is that with read and write, we execute a lot more system calls and do more copying than with mmap and memcpy.
With read and write, we copy the data from the kernel’s buffer to the application’s buffer (read), and then copy the data from the application’s buffer to the kernel’s buffer (write)
With mmap and memcpy, we copy the data directly from one kernel buffer mapped into our address space into another kernel buffer mapped into our address space.
This copying occurs as a result of page fault handling when we reference memory pages that don’t yet exist (there is one fault per page read and one fault per page written)
On Linux 3.2.0, as far as elapsed time is concerned, the two versions of the program show a large difference in clock time: the version using read and write completes four times faster than the version using mmap and memcpy.
However, on Solaris 10, the version with mmap and memcpy is faster than the version with read and write.
If the CPU times are almost the same, then why would the clock times differ? One possibility is that we might have to wait longer for I/O to complete in one version.
This wait time is not counted as CPU processing time.
Another possibility is that some system processing might not be counted against our program — the processing done by system daemons to write pages to disk, for example.
As we need to allocate pages for reading and writing, these system daemons will help make pages available.
If the page writes are random instead of sequential, then it will take longer to write them out to disk, so we will need to wait longer before the pages become available for us to reuse.
Depending on the system, memory-mapped I/O can be more efficient when copying one regular file to another.
We can’t use this technique to copy between certain devices (such as a network device or a terminal device), and we have to be careful if the size of the underlying file could change after we map it.
Nevertheless, some applications can benefit from memory-mapped I/O, as it can often simplify the algorithms, since we manipulate memory instead of reading and writing a file.
One example is the manipulation of a frame buffer device that references a bitmapped display.
We return to memory-mapped I/O in Section 15.9, showing an example of how it can be used to provide shared memory between related processes.
In this chapter, we’ve described numerous advanced I/O functions, many of which are used in the examples in later chapters:
Record locking (which we’ll look at in more detail through an example, the database library in Chapter 20)
I/O multiplexing—the select and poll functions (we’ll use these in many of the later examples)
The readv and writev functions (also used in many of the later examples)
Is the process requesting a write lock starved by the processes read locking the file?
Assume that we need to increase this limit to handle up to 2,048 descriptors.
Also compare the implementation of the two on your system.
What must you change to make it work properly? Keep in mind that you should get the same results whether the standard output is attached to a terminal, a pipe, or a regular file.
Determine the break-even point on your system where using writev is faster than copying the data yourself and using a single write.
In Chapter 8, we described the process control primitives and saw how to work with multiple processes.
But the only way for these processes to exchange information is by passing open files across a fork or an exec or through the file system.
We’ll now describe other techniques for processes to communicate with one another: interprocess communication (IPC)
In the past, UNIX System IPC was a hodgepodge of various approaches, few of which were portable across all UNIX system implementations.
Through the POSIX and The Open Group (formerly X/Open) standardization efforts, the situation has since improved, but differences still exist.
Figure 15.1 summarizes the various forms of IPC that are supported by the four implementations discussed in this text.
Note that the Single UNIX Specification (the ‘‘SUS’’ column) allows an implementation to support full-duplex pipes, but requires only half-duplex pipes.
An implementation that supports full-duplex pipes will still work with correctly written applications that assume that the underlying operating system supports only half-duplex pipes.
We use ‘‘(full)’’ instead of a bullet to show implementations that support half-duplex pipes by using full-duplex pipes.
In Figure 15.1, we show a bullet where basic functionality is supported.
Some implementations support the feature with pipes and UNIX domain sockets, so these entries have both ‘‘UDS’’ and a bullet.
The IPC interfaces introduced as part of the real-time extensions to POSIX.1 were included as options in the Single UNIX Specification.
In SUSv4, the semaphore interfaces were moved from an option to the base specification.
Named full-duplex pipes are provided as mounted STREAMS-based pipes, but are marked obsolescent in the Single UNIX Specification.
The first ten forms of IPC in Figure 15.1 are usually restricted to IPC between processes on the same host.
The final two rows — sockets and STREAMS—are the only two forms that are generally supported for IPC between processes on different hosts.
We have divided the discussion of IPC into three chapters.
In this chapter, we examine classical IPC: pipes, FIFOs, message queues, semaphores, and shared memory.
In the next chapter, we take a look at network IPC using the sockets mechanism.
In Chapter 17, we take a look at some advanced features of IPC.
Pipes are the oldest form of UNIX System IPC and are provided by all UNIX systems.
Historically, they have been half duplex (i.e., data flows in only one direction)
Some systems now provide full-duplex pipes, but for maximum portability, we should never assume that this is the case.
Pipes can be used only between processes that have a common ancestor.
Normally, a pipe is created by a process, that process calls fork, and the pipe is used between the parent and the child.
Despite these limitations, half-duplex pipes are still the most commonly used form of IPC.
Every time you type a sequence of commands in a pipeline for the shell to execute, the shell creates a separate process for each command and links the standard output of one process to the standard input of the next using a pipe.
Even though UNIX domain sockets are full duplex by default, these operating systems hobbled the sockets used with pipes so that they operated in half-duplex mode only.
Two ways to picture a half-duplex pipe are shown in Figure 15.2
The left half of the figure shows the two ends of the pipe connected in a single process.
The right half of the figure emphasizes that the data in the pipe flows through the kernel.
The fstat function (Section 4.2) returns a file type of FIFO for the file descriptor of either end of a pipe.
We can test for a pipe with the S_ISFIFO macro.
But when the fstat function is applied to the file descriptor for the read end of the pipe, many systems store in st_size the number of bytes available for reading in the pipe.
A pipe in a single process is next to useless.
Normally, the process that calls pipe then calls fork, creating an IPC channel from the parent to the child, or vice versa.
What happens after the fork depends on which direction of data flow we want.
When one end of a pipe is closed, two rules apply.
If we read from a pipe whose write end has been closed, read returns 0 to indicate an end of file after all the data has been read.
Technically, we should say that this end of file is not generated until there are no more writers for the pipe.
It’s possible to duplicate a pipe descriptor so that multiple processes have the pipe open for writing.
Normally, however, there is a single reader and a single writer for a pipe.
When we get to FIFOs in the next section, we’ll see that often there are multiple writers for a single FIFO.
When we’re writing to a pipe (or FIFO), the constant PIPE_BUF specifies the kernel’s pipe buffer size.
A write of PIPE_BUF bytes or less will not be interleaved with the writes from other processes to the same pipe (or FIFO)
But if multiple processes are writing to a pipe (or FIFO), and if we write more than PIPE_BUF bytes, the data might be interleaved with the data from the other writers.
Figure 15.5 shows the code to create a pipe between a parent and its child and to send data down the pipe.
Figure 15.5 Send data from parent to child over a pipe.
Note that the pipe direction here matches the orientation shown in Figure 15.4
In the previous example, we called read and write directly on the pipe descriptors.
What is more interesting is to duplicate the pipe descriptors onto standard input or standard output.
Often, the child then runs some other program, and that program can either read from its standard input (the pipe that we created) or write to its standard output (the pipe)
Consider a program that displays some output that it has created, one page at a time.
Rather than reinvent the pagination done by several UNIX system utilities, we want to invoke the user’s favorite pager.
To avoid writing all the data to a temporary file and calling system to display that file, we want to pipe the output directly to the pager.
To do this, we create a pipe, fork a child process, set up the child’s standard input to be the read end of the pipe, and exec the user’s pager program.
This example takes a command-line argument to specify the name of a file to display.
Often, a program of this type would already have the data to display to the terminal in memory.
After the fork, the parent closes its read end, and the child closes its write end.
The child then calls dup2 to have its standard input be the read end of the pipe.
When the pager program is executed, its standard input will be the read end of the pipe.
When we duplicate one descriptor onto another (fd[0] onto standard input in the child), we have to be careful that the descriptor doesn’t already have the desired value.
If the descriptor already had the desired value and we called dup2 and close, the single copy of the descriptor would be closed.
Nevertheless, whenever we call dup2 and close to duplicate one descriptor onto another, we’ll always compare the descriptors first, as a defensive programming measure.
Note how we try to use the environment variable PAGER to obtain the name of the user ’s pager program.
Figure 15.7 Routines to let a parent and child synchronize.
We create two pipes before the fork, as shown in Figure 15.8
The corresponding WAIT_xxx functions do a blocking read for the single character.
Note that each pipe has an extra reader, which doesn’t matter.
That is, in addition to the child reading from pfd1[0], the parent has this end of the top pipe open for reading.
This doesn’t affect us, since the parent doesn’t try to read from this pipe.
Since a common operation is to create a pipe to another process to either read its output or send it input, the standard I/O library has historically provided the popen and pclose functions.
These two functions handle all the dirty work that we’ve been doing ourselves: creating a pipe, forking a child, closing the unused ends of the pipe, executing a shell to run the command, and waiting for the command to terminate.
The function popen does a fork and exec to execute the cmdstring and returns a standard I/O file pointer.
If type is "r", the file pointer is connected to the standard output of cmdstring (Figure 15.9)
If type is "w", the file pointer is connected to the standard input of cmdstring, as shown in Figure 15.10
One way to remember the final argument to popen is to remember that, like fopen, the returned file pointer is readable if type is "r" or writable if type is "w"
The pclose function closes the standard I/O stream, waits for the command to terminate, and returns the termination status of the shell.
The system function, described in Section 8.13, also returns the termination status.
If the shell cannot be executed, the termination status returned by pclose is as if the shell had executed exit(127)
The cmdstring is executed by the Bourne shell, as in.
This means that the shell expands any of its special characters in cmdstring.
Although the core of popen is similar to the code we’ve used earlier in this chapter, there are many details that we need to take care of.
First, each time popen is called, we have to remember the process ID of the child that we create and either its file descriptor or FILE pointer.
We choose to save the child’s process ID in the array childpid, which we index by the file descriptor.
This way, when pclose is called with the FILE pointer as its argument, we call the standard I/O function fileno to get the file descriptor and then have the child process ID for the call to waitpid.
Since it’s possible for a given process to call popen more than once, we dynamically allocate the childpid array (the first time popen is called), with room for as many children as there are file descriptors.
Calling pipe and fork and then duplicating the appropriate descriptors for each process in the popen function is similar to what we did earlier in this chapter.
POSIX.1 requires that popen close any streams that are still open in the child from previous calls to popen.
To do this, we go through the childpid array in the child, closing any descriptors that are still open.
What happens if the caller of pclose has established a signal handler for SIGCHLD? The call to waitpid from pclose would return an error of EINTR.
Since the caller is allowed to catch this signal (or any other signal that might interrupt the call to waitpid), we simply call waitpid again if it is interrupted by a caught signal.
Some early versions of pclose returned an error of EINTR if a signal interrupted the wait.
Also, some early versions of pclose blocked or ignored the signals SIGINT, SIGQUIT, and SIGHUP during the wait.
Note that popen should never be called by a set-user-ID or set-group-ID program.
When it executes the command, popen does the equivalent of.
A malicious user can manipulate the environment so that the shell executes commands other than those intended, with the elevated permissions granted by the set-ID file mode.
One thing that popen is especially well suited for is executing simple filters to transform the input or output of the running command.
Such is the case when a command wants to build its own pipeline.
Consider an application that writes a prompt to standard output and reads a line from standard input.
With the popen function, we can interpose a program between the application and its input to transform the input.
Figure 15.13 shows the arrangement of processes in this situation.
The transformation could be pathname expansion, for example, or providing a history mechanism (remembering previously entered commands)
Figure 15.14 shows a simple filter to demonstrate this operation.
The filter copies standard input to standard output, converting any uppercase character to lowercase.
The reason we’re careful to fflush standard output after writing a newline is discussed in the next section when we talk about coprocesses.
We compile this filter into the executable file myuclc, which we then invoke from the program in Figure 15.15 using popen.
We need to call fflush after writing the prompt, because the standard output is normally line buffered, and the prompt does not contain a newline.
A UNIX system filter is a program that reads from standard input and writes to standard output.
A filter becomes a coprocess when the same program generates the filter’s input and reads the filter ’s output.
The Bourne shell, the Bourne-again shell, and the C shell don’t provide a way to connect processes together as coprocesses.
A coprocess normally runs in the background from a shell, and its standard input and standard output are connected to another program using a pipe.
Whereas popen gives us a one-way pipe to the standard input or from the standard output of another process, with a coprocess we have two one-way pipes to the other process: one to its standard input and one from its standard output.
We want to write to its standard input, let it operate on the data, and then read from its standard output.
The process creates two pipes: one is the standard input of the coprocess and the other is the standard output of the coprocess.
Figure 15.16 Driving a coprocess by writing its standard input and reading its standard output.
The program in Figure 15.17 is a simple coprocess that reads two numbers from its standard input, computes their sum, and writes the sum to its standard output.
Coprocesses usually do more interesting work than we illustrate here.
This example is admittedly contrived so that we can study the plumbing needed to connect the processes.
We compile this program and leave the executable in the file add2
The value from the coprocess is written to its standard output.
Here, we create two pipes, with the parent and the child closing the ends they don’t need.
We have to use two pipes: one for the standard input of the coprocess and one for its standard output.
The child then calls dup2 to move the pipe descriptors onto its standard input and standard output, before calling execl.
If we compile and run the program in Figure 15.18, it works as expected.
What happens if we rewrite this coprocess to use standard I/O? Figure 15.19 shows the new version.
Figure 15.19 Filter to add two numbers, using standard I/O.
If we invoke this new coprocess from the program in Figure 15.18, it no longer works.
When the program in Figure 15.19 is invoked, the first fgets on the standard input causes the standard I/O library to allocate a buffer and choose the type of buffering.
Since the standard input is a pipe, the standard I/O library defaults to fully buffered.
Here, we have control over the coprocess that’s being run.
We can change the program in Figure 15.19 by adding the following four lines before the while loop:
These lines cause fgets to return when a line is available and cause printf to do an fflush when a newline is output (refer to Section 5.4 for the details on standard I/O buffering)
Making these explicit calls to setvbuf fixes the program in Figure 15.19
If we aren’t able to modify the program that we’re piping the output into, other techniques are required.
The reason this won’t work is again the standard I/O buffering.
But in this case, we cannot change the way awk works (unless we have the source code for it)
We are unable to modify the executable of awk in any way to change the way the standard I/O buffering is handled.
The solution for this general problem is to make the coprocess being invoked (awk in this case) think that its standard input and standard output are connected to a terminal.
That causes the standard I/O routines in the coprocess to line buffer these two I/O streams, similar to what we did with the explicit calls to setvbuf previously.
Unnamed pipes can be used only between related processes when a common ancestor has created the pipe.
We saw in Chapter 4 that a FIFO is a type of file.
Indeed, the pathname for a FIFO exists in the file system.
The specification of the mode argument is the same as for the open function (Section 3.3)
The rules for the user and group ownership of the new FIFO are the same as we described in Section 4.6
The mkfifoat function is similar to the mkfifo function, except that it can be used to create a FIFO in a location relative to the directory represented by the fd file descriptor argument.
If the path parameter specifies an absolute pathname, then the fd parameter is ignored and the mkfifoat function behaves like the mkfifo function.
If the path parameter specifies a relative pathname and the fd parameter is a valid file descriptor for an open directory, the pathname is evaluated relative to this directory.
If the path parameter specifies a relative pathname and the fd parameter has the special value AT_FDCWD, the pathname is evaluated starting in the current working directory, and mkfifoat behaves like mkfifo.
Once we have used mkfifo or mkfifoat to create a FIFO, we open it using open.
Indeed, the normal file I/O functions (e.g., close, read, write, unlink) all work with FIFOs.
Applications can create FIFOs with the mknod and mknodat functions.
The mknod and mknodat functions are included in the XSI option in POSIX.1
All four platforms discussed in this text provide this command.
As a result, we can create a FIFO using a shell command and then access it with the normal shell I/O redirection.
When we open a FIFO, the nonblocking flag (O_NONBLOCK) affects what happens.
In the normal case (without O_NONBLOCK), an open for read-only blocks until.
Similarly, an open for writeonly blocks until some other process opens the FIFO for reading.
As with a pipe, if we write to a FIFO that no process has open for reading, the signal SIGPIPE is generated.
When the last writer for a FIFO closes the FIFO, an end of file is generated for the reader of the FIFO.
It is common to have multiple writers for a given FIFO.
This means that we have to worry about atomic writes if we don’t want the writes from multiple processes to be.
As with pipes, the constant PIPE_BUF specifies the maximum amount of data that can be written atomically to a FIFO.
FIFOs are used by shell commands to pass data from one shell pipeline to another without creating intermediate temporary files.
FIFOs are used as rendezvous points in client–server applications to pass data between the clients and the servers.
FIFOs can be used to duplicate an output stream in a series of shell commands.
This prevents writing the data to an intermediate disk file (similar to using pipes to avoid intermediate disk files)
But whereas pipes can be used only for linear connections between processes, a FIFO has a name, so it can be used for nonlinear connections.
Consider a procedure that needs to process a filtered input stream twice.
Figure 15.20 Procedure that processes a filtered input stream twice.
With a FIFO and the UNIX program tee(1), we can accomplish this procedure without using a temporary file.
The tee program copies its standard input to both its standard output and the file named on its command line.
We create the FIFO and then start prog3 in the background, reading from the FIFO.
Another use for FIFOs is to send data between a client and a server.
If we have a server that is contacted by numerous clients, each client can write its request to a well-known.
Figure 15.21 Using a FIFO and tee to send a stream to two different processes.
By ‘‘well-known,’’ we mean that the pathname of the FIFO is known to all the clients that need to contact the server.
Figure 15.22 Clients sending requests to a server using a FIFO.
Since there are multiple writers for the FIFO, the requests sent by the clients to the server need to be less than PIPE_BUF bytes in size.
The problem in using FIFOs for this type of client–server communication is how to send replies back from the server to each client.
A single FIFO can’t be used, as the clients would never know when to read their response versus responses for other clients.
One solution is for each client to send its process ID with the request.
The server then creates a unique FIFO for each client, using a pathname based on the client’s process ID.
This arrangement works, although it is impossible for the server to tell whether a client crashes.
A client crash leaves the client-specific FIFO in the file system.
To prevent the server from having to handle this case, a common trick is just to have the server open its well-known FIFO for read–write.
The three types of IPC that we call XSI IPC—message queues, semaphores, and shared memory — have many similarities.
In this section, we cover these similar features; in the following sections, we look at the specific functions for each of the three IPC types.
They are often criticized for inventing their own namespace instead of using the file system.
Each IPC structure (message queue, semaphore, or shared memory segment) in the kernel is referred to by a non-negative integer identifier.
To send a message to or fetch a message from a message queue, for example, all we need know is the identifier for the queue.
The identifier is an internal name for an IPC object.
Cooperating processes need an external naming scheme to be able to rendezvous using the same IPC object.
For this purpose, an IPC object is associated with a key that acts as an external name.
Whenever an IPC structure is being created (by calling msgget, semget, or shmget), a key must be specified.
This key is converted into an identifier by the kernel.
There are various ways for a client and a server to rendezvous at the same IPC structure.
The server can create a new IPC structure by specifying a key of IPC_PRIVATE and store the returned identifier somewhere (such as a file) for the client to obtain.
The key IPC_PRIVATE guarantees that the server creates a new IPC structure.
The disadvantage of this technique is that file system operations are required for the server to write the integer identifier to a file, and then for the clients to retrieve this identifier later.
The IPC_PRIVATE key is also used in a parent–child relationship.
The parent creates a new IPC structure specifying IPC_PRIVATE, and the resulting identifier is then available to the child after the fork.
The child can pass the identifier to a new program as an argument to one of the exec functions.
The client and the server can agree on a key by defining the key in a common header, for example.
The server then creates a new IPC structure specifying this key.
The problem with this approach is that it’s possible for the key to already be associated with an IPC structure, in which case the get function (msgget, semget, or shmget) returns an error.
The server must handle this error, deleting the existing IPC structure, and try to create it again.
The only service provided by ftok is a way of generating a key from a pathname and project ID.
Only the lower 8 bits of id are used when generating the key.
However, because both i-node numbers and keys are often stored in long integers, information loss can occur when creating a key.
This means that two different pathnames to different files can generate the same key if the same project ID is used.
The three get functions (msgget, semget, and shmget) all have two similar arguments: a key and an integer flag.
To reference an existing queue (normally done by a client), key must equal the key that was specified when the queue was created, and IPC_CREAT must not be specified.
Note that it’s never possible to specify IPC_PRIVATE to reference an existing queue, since this special key value always creates a new queue.
To reference an existing queue that was created with a key of IPC_PRIVATE, we must know the associated identifier and then use that identifier in the other IPC calls (such as msgsnd and msgrcv), bypassing the get function.
Doing this causes an error return of EEXIST if the IPC structure already exists.
This structure defines the permissions and owner and includes at least the following members:
All the fields are initialized when the IPC structure is created.
At a later time, we can modify the uid, gid, and mode fields by calling msgctl, semctl, or shmctl.
To change these values, the calling process must be either the creator of the IPC structure or the superuser.
Changing these fields is similar to calling chown or chmod for a file.
The values in the mode field are similar to the values we saw in Figure 4.6, but there is nothing corresponding to execute permission for any of the IPC structures.
Also, message queues and shared memory use the terms read and write, but semaphores use the terms read and alter.
Figure 15.24 shows the six permissions for each form of IPC.
Some implementations define symbolic constants to represent each permission, but these constants are not standardized by the Single UNIX Specification.
All three forms of XSI IPC have built-in limits that we may encounter.
Most of these limits can be changed by reconfiguring the kernel.
We describe the limits when we describe each of the three forms of IPC.
Each platform provides its own way to report and modify a particular limit.
On Solaris 10, changes to kernel IPC limits are made with the prctl command.
On Linux, you can display the IPC-related limits by running ipcs -l.
On Solaris, you can discover the tunable parameters by running sysdef -i.
A fundamental problem with XSI IPC is that the IPC structures are systemwide and do not have a reference count.
For example, if we create a message queue, place some messages on the queue, and then terminate, the message queue and its contents are not deleted.
They remain in the system until specifically read or deleted by some process calling msgrcv or msgctl, by someone executing the ipcrm(1) command, or by the system being rebooted.
Compare this with a pipe, which is completely removed when the last process to reference it terminates.
With a FIFO, although the name stays in the file system until explicitly removed, any data left in a FIFO is removed when the last process to reference the FIFO terminates.
Another problem with XSI IPC is that these IPC structures are not known by names in the file system.
Almost a dozen new system calls (msgget, semop, shmat, and so on) were added to the kernel to support these IPC objects.
We can’t see the IPC objects with an ls command, we can’t remove them with the rm command, and we can’t change their permissions with the chmod command.
Since these forms of IPC don’t use file descriptors, we can’t use the multiplexed I/O functions (select and poll) with them.
This makes it harder to use more than one of these IPC structures at a time or to use any of these IPC structures with file or device I/O.
For example, we can’t have a server wait for a message to be placed on one of two message queues without some form of busy–wait loop.
An overview of a transaction processing system built using System V IPC is given in Andrade, Carges, and Kovach [1989]
They claim that the namespace used by System V IPC (the identifiers) is an advantage, not a problem as we said earlier, because using identifiers allows a process to send a message to a message queue with a single function call (msgsnd), whereas other forms of IPC normally require an open, write, and close.
Clients still have to obtain the identifier for the server ’s queue somehow, to avoid using a key and calling msgget.
The identifier assigned to a particular queue depends on how many other message queues exist when the queue is created and how many times the table in the kernel assigned to the new queue has been used since the kernel was bootstrapped.
This is a dynamic value that can’t be guessed or stored in a header.
As we mentioned in Section 15.6.1, minimally a server has to write the assigned queue identifier to a file for its clients to read.
Other advantages listed by these authors for message queues are that they’re reliable, flow controlled, and record oriented, and that they can be processed in other than first-in, first-out order.
Figure 15.25 compares some of the features of these various forms of IPC.
Figure 15.25 Comparison of features of various forms of IPC.
As described previously, we don’t consider message queues connectionless, since some technique is required to obtain the identifier for a queue.
Since all these forms of IPC are restricted to a single host, all are reliable.
When the messages are sent across a network, the possibility of messages being lost becomes a concern.
Flow control’’ means that the sender is put to sleep if there is a shortage of system resources (buffers) or if the receiver can’t accept any more messages.
When the flow control condition subsides (i.e., when there is room in the queue), the sender should automatically be awakened.
One feature that we don’t show in Figure 15.25 is whether the IPC facility can automatically create a unique connection to a server for each client.
We’ll see in Chapter 17 that UNIX stream sockets provide this capability.
The next three sections describe each of the three forms of XSI IPC in detail.
A message queue is a linked list of messages stored within the kernel and identified by a message queue identifier.
We’ll call the message queue just a queue and its identifier a queue ID.
The Single UNIX Specification message-passing option includes an alternative IPC message queue interface derived from the POSIX real-time extensions.
A new queue is created or an existing queue opened by msgget.
New messages are added to the end of a queue by msgsnd.
Every message has a positive long integer type field, a non-negative length, and the actual data bytes (corresponding to the length), all of which are specified to msgsnd when the message is added to a queue.
We don’t have to fetch the messages in a first-in, first-out order.
Instead, we can fetch messages based on their type field.
The members shown are the ones defined by the Single UNIX Specification.
Figure 15.26 lists the system limits that affect message queues.
We show ‘‘derived’’ where a limit is derived from other limits.
On Linux, for example, the maximum number of messages is based on the maximum number of queues and the maximum amount of data allowed on the queues.
The maximum number of queues, in turn, is based on the amount of RAM installed in the system.
Note that the queue maximum byte size limit further limits the maximum size of a message to be placed on a queue.
The first function normally called is msgget to either open an existing queue or create a new queue.
In Section 15.6.1, we described the rules for converting the key into an identifier and discussed whether a new queue is created or an existing queue is referenced.
When a new queue is created, the following members of the msqid_ds structure are initialized.
The mode member of this structure is set to the corresponding permission bits of flag.
These permissions are specified with the values from Figure 15.24
This value is then used with the other three message queue functions.
This function and the related functions for semaphores and shared memory (semctl and shmctl) are the ioctl-like functions for XSI IPC (i.e., the garbage-can functions)
The cmd argument specifies the command to be performed on the queue specified by msqid.
IPC_RMID Remove the message queue from the system and any data still on the queue.
Any other process still using the message queue will get an error of EIDRM on its next attempted operation on the queue.
As we mentioned earlier, each message is composed of a positive long integer type field, a non-negative length (nbytes), and the actual data bytes (corresponding to the length)
Messages are always placed at the end of the queue.
The ptr argument points to a long integer that contains the positive integer message type, and it is immediately followed by the message data.
If the largest message we send is 512 bytes, we can define the following structure:
The ptr argument is then a pointer to a mymesg structure.
The message type can be used by the receiver to fetch messages in an order other than first in, first out.
In this case, the 32-bit application will see a truncated type value.
Note how ungracefully the removal of a message queue is handled.
Since a reference count is not maintained with each message queue (as there is for open files), the removal of a queue simply generates errors on the next queue operation by processes still using the queue.
In contrast, when a file is removed, the file’s contents are not deleted until the last open descriptor for the file is closed.
As with msgsnd, the ptr argument points to a long integer (where the message type of the returned message is stored) followed by a data buffer for the actual message data.
If the returned message is larger than nbytes and the MSG_NOERROR bit in flag is set, the message is truncated.
In this case, no notification is given to us that the message was truncated, and the remainder of the message is discarded.
If the message is too big and this flag value is not specified, an error of E2BIG is returned instead (and the message stays on the queue)
The type argument lets us specify which message we want.
A nonzero type is used to read the messages in an order other than first in, first out.
For example, the type could be a priority value if the application assigns priorities to the messages.
Another use of this field is to contain the process ID of the client if a single message queue is being used by multiple clients and a single server (as long as a process ID fits in a long integer)
If we need a bidirectional flow of data between a client and a server, we can use either message queues or full-duplex pipes.
Figure 15.27 shows a timing comparison of three of these techniques on Solaris: message queues, full-duplex (STREAMS) pipes, and UNIX domain sockets.
The tests consisted of a program that created the IPC channel, called fork, and then sent about 200 megabytes of data from the parent to the child.
When message queues were implemented, the only other form of IPC available was half-duplex pipes.
When we consider the problems in using message queues (Section 15.6.4), we come to the conclusion that we shouldn’t use them for new applications.
A semaphore isn’t a form of IPC similar to the others that we’ve described (pipes, FIFOs, and message queues)
A semaphore is a counter used to provide access to a shared data object for multiple processes.
The Single UNIX Specification includes an alternative set of semaphore interfaces that were originally part of its real-time extensions.
To obtain a shared resource, a process needs to do the following:
If the value of the semaphore is positive, the process can use the resource.
In this case, the process decrements the semaphore value by 1, indicating that it has used one unit of the resource.
If any other processes are asleep, waiting for the semaphore, they are awakened.
To implement semaphores correctly, the test of a semaphore’s value and the decrementing of this value must be an atomic operation.
For this reason, semaphores are normally implemented inside the kernel.
A common form of semaphore is called a binary semaphore.
In general, however, a semaphore can be initialized to any positive value, with the value indicating how many units of the shared resource are available for sharing.
Instead, we have to define a semaphore as a set of one or more semaphore values.
When we create a semaphore, we specify the number of values in the set.
The creation of a semaphore (semget) is independent of its initialization (semctl)
This is a fatal flaw, since we cannot atomically create a new semaphore set and initialize all the values in the set.
Since all forms of XSI IPC remain in existence even when no process is using them, we have to worry about a program that terminates without releasing the semaphores it has been allocated.
The undo feature that we describe later is supposed to handle this.
The Single UNIX Specification defines the fields shown, but implementations can define additional members in the semid_ds structure.
Each semaphore is represented by an anonymous structure containing at least the following members:
Figure 15.28 lists the system limits that affect semaphore sets.
When we want to use XSI semaphores, we first need to obtain a semaphore ID by calling the semget function.
In Section 15.6.1, we described the rules for converting the key into an identifier and discussed whether a new set is created or an existing set is referenced.
When a new set is created, the following members of the semid_ds structure are initialized.
The mode member of this structure is set to the corresponding permission bits of flag.
These permissions are specified with the values from Figure 15.24
The fourth argument is optional, depending on the command requested, and if present, is of type semun, a union of various command-specific arguments:
Note that the optional argument is the actual union, not a pointer to the union.
Any other process still using the semaphore will get an error of EIDRM on its next attempted operation on the semaphore.
The semoparray argument is a pointer to an array of semaphore operations, represented by sembuf structures:
The nops argument specifies the number of operations (elements) in the array.
The operation on each member of the set is specified by the corresponding sem_op.
In the following discussion, we refer to the ‘‘undo’’ flag for a semaphore.
This case corresponds to the returning of resources by the process.
The value of sem_op is added to the semaphore’s value.
If the undo flag is specified, sem_op is also subtracted from the semaphore’s adjustment value for this process.
If sem_op is negative, we want to obtain resources that the semaphore controls.
If the undo flag is specified, the absolute value of sem_op is also added to the semaphore’s adjustment value for this process.
If the semaphore’s value is less than the absolute value of sem_op (the resources are not available), the following conditions apply.
If IPC_NOWAIT is specified, semop returns with an error of EAGAIN.
If IPC_NOWAIT is not specified, the semncnt value for this semaphore is.
The semaphore’s value becomes greater than or equal to the absolute value of sem_op (i.e., some other process has released some resources)
The value of semncnt for this semaphore is decremented (since the calling process is done waiting), and the absolute value of sem_op is subtracted from the semaphore’s value.
If the undo flag is specified, the absolute value of sem_op is also added to the semaphore’s adjustment value for this process.
In this case, the function returns an error of EIDRM.
A signal is caught by the process, and the signal handler returns.
In this case, the value of semncnt for this semaphore is decremented (since the calling process is no longer waiting), and the function returns an error of EINTR.
If the semaphore’s value is currently 0, the function returns immediately.
If the semaphore’s value is nonzero, the following conditions apply.
If IPC_NOWAIT is specified, return is made with an error of EAGAIN.
If IPC_NOWAIT is not specified, the semzcnt value for this semaphore is.
The value of semzcnt for this semaphore is decremented (since the calling process is done waiting)
In this case, the function returns an error of EIDRM.
A signal is caught by the process, and the signal handler returns.
In this case, the value of semzcnt for this semaphore is decremented (since the calling process is no longer waiting), and the function returns an error of EINTR.
The semop function operates atomically; it does either all the operations in the array or none of them.
As we mentioned earlier, it is a problem if a process terminates while it has resources allocated through a semaphore.
When the process terminates, either voluntarily or involuntarily, the kernel checks whether the process has any outstanding semaphore adjustments and, if so, applies the adjustment to the corresponding semaphore.
If we are sharing a single resource among multiple processes, we can use one of three techniques to coordinate access.
We can use a a semaphore, record locking, or a mutex that is mapped into the address spaces of both processes.
It’s interesting to compare the timing differences between the three techniques.
With record locking, we create an empty file and use the first byte of the file (which need not exist) as the lock byte.
To allocate the resource, we obtain a write lock on the.
The record locking properties guarantee that if a process terminates while holding a lock, the kernel automatically releases the lock.
To allocate the resource, we lock the mutex; to release the resource, we unlock the mutex.
Figure 15.29 shows the time required to perform these three locking techniques on Linux.
In each case, the resource was allocated and then released 1,000,000 times.
The times in Figure 15.29 are the totals in seconds for all three processes.
On Linux, record locking is faster than semaphores, but mutexes in shared memory outperform both semaphores and record locking.
If we’re locking a single resource and don’t need all the fancy features of XSI semaphores, record locking is preferred over semaphores.
The reasons are that it is much simpler to use, it is faster (on this platform), and the system takes care of any lingering locks when a process terminates.
Even though using a mutex in shared memory is the fastest option on this platform, we still prefer to use record locking, unless performance is the primary concern.
First, recovery from process termination is more difficult using a mutex in memory shared among multiple processes.
In older versions of the Single UNIX Specification, it was optional.
Although it is still optional in SUSv4, it is now required by all XSI-conforming implementations.
Shared memory allows two or more processes to share a given region of memory.
This is the fastest form of IPC, because the data does not need to be copied between the client and the server.
The only trick in using shared memory is synchronizing access to a given region among multiple processes.
If the server is placing data into a shared memory region, the client shouldn’t try to access the data until the server is done.
But as we saw at the end of the previous section, record locking or mutexes can also be used.
The Single UNIX Specification shared memory objects option includes alternative interfaces, originally real-time extensions, to access shared memory.
We’ve already seen one form of shared memory when multiple processes map the same file into their address spaces.
The XSI shared memory differs from memorymapped files in that there is no associated file.
The XSI shared memory segments are anonymous segments of memory.
The kernel maintains a structure with at least the following members for each shared memory segment:
Implementations add other structure members to support shared memory segments.
The type shmatt_t is defined to be an unsigned integer at least as large as an.
Figure 15.30 lists the system limits that affect shared memory.
The first function called is usually shmget, to obtain a shared memory identifier.
In Section 15.6.1, we described the rules for converting the key into an identifier and whether a new segment is created or an existing segment is referenced.
When a new segment is created, the following members of the shmid_ds structure are initialized.
The mode member of this structure is set to the corresponding permission bits of flag.
These permissions are specified with the values from Figure 15.24
The size parameter is the size of the shared memory segment in bytes.
Implementations will usually round up this size to a multiple of the system’s page size, but if an application specifies size as a value other than an integral multiple of the system’s page size, the remainder of the last page will be unavailable for use.
If a new segment is being created (typically by the server), we must specify its size.
When a new segment is created, the contents of the segment are initialized with zeros.
The shmctl function is the catchall for various shared memory operations.
The cmd argument specifies one of the following five commands to be performed, on the segment specified by shmid.
IPC_RMID Remove the shared memory segment set from the system.
Regardless of whether the segment is still in use, the segment’s identifier is immediately removed so that shmat can no longer attach the segment.
Two additional commands are provided by Linux and Solaris, but are not part of the Single UNIX Specification.
Once a shared memory segment has been created, a process attaches it to its address space by calling shmat.
The address in the calling process at which the segment is attached depends on the addr argument and whether the SHM_RND bit is specified in flag.
If addr is 0, the segment is attached at the first available address selected by the kernel.
If addr is nonzero and SHM_RND is not specified, the segment is attached at the address given by addr.
Unless we plan to run the application on only a single type of hardware (which is highly unlikely today), we should not specify the address where the segment is to be attached.
Instead, we should specify an addr of 0 and let the system choose the address.
If the SHM_RDONLY bit is specified in flag, the segment is attached as read-only.
When we’re done with a shared memory segment, we call shmdt to detach it.
Note that this does not remove the identifier and its associated data structure from the system.
The identifier remains in existence until some process (often a server) specifically removes it by calling shmctl with a command of IPC_RMID.
The addr argument is the value that was returned by a previous call to shmat.
Where a kernel places shared memory segments that are attached with an address of 0 is highly system dependent.
Figure 15.31 shows a program that prints some information on where one particular system places various types of data.
Figure 15.31 Print where various types of data are stored.
Running this program on a 64-bit Intel-based Linux system gives us the following output:
Note that the shared memory segment is placed well below the stack.
Recall that the mmap function (Section 14.8) can be used to map portions of a file into the address space of a process.
This is conceptually similar to attaching a shared memory segment using the shmat XSI IPC function.
The main difference is that the memory segment mapped with mmap is backed by a file, whereas no file is associated with an XSI shared memory segment.
But if the processes are related, some implementations provide a different technique.
Mac OS X 10.6.8 currently doesn’t support the mapping of character devices into the address space of a process.
The device /dev/zero is an infinite source of 0 bytes when read.
This device also accepts any data that is written to it, ignoring the data.
Our interest in this device for IPC arises from its special properties when it is memory mapped.
An unnamed memory region is created whose size is the second argument to mmap, rounded up to the nearest page size on the system.
Multiple processes can share this region if a common ancestor specifies the MAP_SHARED flag to mmap.
The program in Figure 15.33 is an example that uses this special device.
Figure 15.33 IPC between parent and child using memory mapped I/O of /dev/zero.
The program opens the /dev/zero device and calls mmap, specifying a size of a long integer.
Note that once the region is mapped, we can close the device.
Since MAP_SHARED was specified in the call to mmap, writes to the memory-mapped region by one process are seen by the other process.
If we had specified MAP_PRIVATE instead, this example wouldn’t work.
The parent and the child then alternate running, incrementing a long integer in the shared memory-mapped region, using the synchronization functions from Section 8.9
Note that we have to use parentheses when we increment the value of the long integer in the update function, since we are incrementing the value and not the pointer.
The advantage of using /dev/zero in the manner that we’ve shown is that an actual file need not exist before we call mmap to create the mapped region.
Mapping /dev/zero automatically creates a mapped region of the specified size.
The disadvantage of this technique is that it works only between related processes.
Note that no matter which technique is used, we still need to synchronize access to the shared data.
The anonymous memory-mapping facility is supported by all four platforms discussed in this text.
To modify the program in Figure 15.33 to use this facility, we make three changes: (a) remove the open of /dev/zero, (b) remove the close of fd, and (c) change the call to mmap to the following:
The last two examples illustrate sharing memory among multiple related processes.
If shared memory is required between unrelated processes, there are two alternatives.
Applications can use the XSI shared memory functions, or they can use mmap to map the same file into their address spaces using the MAP_SHARED flag.
The POSIX semaphore mechanism is one of three IPC mechanisms that originated with the real-time extensions to POSIX.1
The Single UNIX Specification placed the three mechanisms (message queues, semaphores, and shared memory) in option classes.
Prior to SUSv4, the POSIX semaphore interfaces were included in the semaphores option.
In SUSv4, these interfaces were moved to the base specification, but the message queue and shared memory interfaces remained optional.
The POSIX semaphore interfaces were meant to address several deficiencies with the XSI semaphore interfaces:
The POSIX semaphore interfaces allow for higher-performance implementations compared to XSI semaphores.
The POSIX semaphore interfaces are simpler to use: there are no semaphore sets, and several of the interfaces are patterned after familiar file system operations.
Although there is no requirement that they be implemented in the file system, some systems do take this approach.
Recall that when an XSI semaphore is removed, operations using the same semaphore identifier fail with errno set to EIDRM.
With POSIX semaphores, operations continue to work normally until the last reference to the semaphore is released.
They differ in how they are created and destroyed, but otherwise work the same.
Unnamed semaphores exist in memory only and require that processes have access to the memory to be able to use the semaphores.
This means they can be used only by threads in the same process or threads in different processes that have mapped the same memory extent into their address spaces.
Named semaphores, in contrast, are accessed by name and can be used by threads in any processes that know their names.
To create a new named semaphore or use an existing one, we call the sem_open function.
When using an existing named semaphore, we specify only two arguments: the name of the semaphore and a zero value for the oflag argument.
When the oflag argument has the O_CREAT flag set, we create a new named semaphore if it does not yet exist.
If it already exists, it is opened for use, but no additional initialization takes place.
When we specify the O_CREAT flag, we need to provide two additional arguments.
It can take on the same values as the permission bits for opening a file: user-read, user-write, user-execute, group-read, group-write, group-execute, other-read, other-write, and other-execute.
The resulting permissions assigned to the semaphore are modified by the caller’s file.
Note, however, that only read and write access matter, but the interfaces don’t allow us to specify the mode when we open an existing semaphore.
The value argument is used to specify the initial value for the semaphore when we create it.
If we want to ensure that we are creating the semaphore, we can set the oflag argument to O_CREAT|O_EXCL.
This will cause sem_open to fail if the semaphore already exists.
To promote portability, we must follow certain conventions when selecting a semaphore name.
The first character in the name should be a slash (/)
Although there is no requirement that an implementation of POSIX semaphores uses the file system, if the file system is used, we want to remove any ambiguity as to the starting point from which the name is interpreted.
For example, if the file system is used, the names /mysem and //mysem would evaluate to the same filename, but if the implementation doesn’t use the file system, the two names could be treated as different (consider what would happen if the implementation hashed the name to an integer value used to identify the semaphore)
The maximum length of the semaphore name is implementation defined.
The sem_open function returns a semaphore pointer that we can pass to other semaphore functions when we want to operate on the semaphore.
When we are done with the semaphore, we can call the sem_close function to release any resources associated with the semaphore.
If our process exits without having first called sem_close, the kernel will close any open semaphores automatically.
Note that this doesn’t affect the state of the semaphore value — if we have incremented its value, this doesn’t change just because we exit.
Similarly, if we call sem_close, the semaphore value is unaffected.
There is no mechanism equivalent to the SEM_UNDO flag found with XSI semaphores.
To destroy a named semaphore, we can use the sem_unlink function.
If there are no open references to the semaphore, then it is destroyed.
Otherwise, destruction is deferred until the last open reference is closed.
Unlike with XSI semaphores, we can only adjust the value of a POSIX semaphore by one with a single function call.
Decrementing the count is analogous to locking a binary semaphore or acquiring a resource associated with a counting semaphore.
Note that there is no distinction of semaphore type with POSIX semaphores.
Whether we use a binary semaphore or a counting semaphore depends on how we initialize and use the semaphore.
A third alternative is to block for a bounded amount of time.
To increment the value of a semaphore, we call the sem_post function.
This is analogous to unlocking a binary semaphore or releasing a resource associated with a counting semaphore.
When we want to use POSIX semaphores within a single process, it is easier to use unnamed semaphores.
This only changes the way we create and destroy the semaphore.
To create an unnamed semaphore, we call the sem_init function.
The pshared argument indicates if we plan to use the semaphore with multiple processes.
The value argument specifies the initial value of the semaphore.
If we plan to use the semaphore between two processes, we need to ensure that the sem argument points into the memory extent that is shared between the processes.
When we are done using the unnamed semaphore, we can discard it by calling the sem_destroy function.
One other function is available to allow us to retrieve the value of a semaphore.
On success, the integer pointed to by the valp argument will contain the value of the semaphore.
Be aware, however, that the value of the semaphore can change by the time that we try to use the value we just read.
Unless we use additional synchronization mechanisms to avoid this race, the sem_getvalue function is useful only for debugging.
One of the motivations for introducing the POSIX semaphore interfaces was that they can be made to perform significantly better than the existing XSI semaphore interfaces.
It is instructive to see if this goal was reached in existing systems, even though these systems were not designed to support real-time applications.
Recall from Figure 12.5 that the Single UNIX Specification doesn’t define what happens when one thread locks a normal mutex and a different thread tries to unlock it, but that error-checking mutexes and recursive mutexes generate errors in this case.
Because a binary semaphore can be used like a mutex, we can use a semaphore to create our own locking primitive to provide mutual exclusion.
Assuming we were to create our own lock that could be locked by one thread and unlocked by another, our lock structure might look like.
The program in Figure 15.35 shows an implementation of a semaphore-based mutual exclusion primitive.
We create a name based on the process ID and a counter.
This destroys the name so that no other process can access it and simplifies cleanup when the process ends.
Let’s detail some of the properties of clients and servers that are affected by the various types of IPC used between them.
The simplest type of relationship is to have the client fork and exec the desired server.
Two half-duplex pipes can be created before the fork to allow data to be transferred in both directions.
The server that is executed can be a set-user-ID program, giving it special privileges.
Also, the server can determine the real identity of the client by looking at its real user ID.
Recall from Section 8.10 that the real user ID and real group ID don’t change across an exec.
We show an implementation of this client–server mechanism in Section 17.5
It opens files for the client instead of the client calling the open function.
This way, additional permission checking can be added, above and beyond the normal UNIX system user/group/other permissions.
We assume that the server is a set-user-ID program, giving it additional permissions (root permission, perhaps)
The server uses the real user ID of the client to determine whether to give it access to the requested file.
This way, we can build a server that allows certain users permissions that they don’t normally have.
In this example, since the server is a child of the parent, all the server can do is pass back the contents of the file to the parent.
Although this works fine for regular files, it can’t be used for special device files, for example.
We would like to be able to have the server open the requested file and pass back the file descriptor.
Whereas a parent can pass a child an open descriptor, a child cannot pass a descriptor back to the parent (unless special programming techniques are used, which we cover in Chapter 17)
We showed the next type of server in Figure 15.23
The server is a daemon process that is contacted using some form of IPC by all clients.
We can’t use pipes for this type of client–server arrangement.
A form of named IPC is required, such as FIFOs or message queues.
With FIFOs, we saw that an individual per-client FIFO is also required if the server is to send data back to the client.
If the client–server application sends data only from the client to the server, a single well-known FIFO suffices.
The System V line printer spooler used this form of client–server arrangement.
The client was the lp(1) command, and the server was the lpsched daemon process.
A single FIFO was used, since the flow of data was only from the client to the server.
A single queue can be used between the server and all the clients, using the type field of each message to indicate the message recipient.
Included in the request must be the client’s process ID.
The server then sends the response with the type field set to the client’s process ID.
The server receives only the messages with a type field of 1 (the fourth argument for msgrcv), and the clients receive only the messages with a type field equal to their process IDs.
Alternatively, an individual message queue can be used for each client.
Before sending the first request to a server, each client creates its own message queue.
The server also has its own queue, with a key or identifier known to all clients.
The client sends its first request to the server’s well-known queue, and this request must contain the message queue ID of the client’s queue.
The server sends its first response to the client’s queue, and all future requests and responses are exchanged on this queue.
One problem with this technique is that each client-specific queue usually has only a single message on it: a request for the server or a response for a client.
This seems wasteful of a limited systemwide resource (a message queue), and a FIFO can be used instead.
Another problem is that the server has to read messages from multiple queues.
Either of these two techniques using message queues can be implemented using shared memory segments and a synchronization method (a semaphore or record locking)
The problem with this type of client–server relationship (the client and the server being unrelated processes) is for the server to identify the client accurately.
Unless the server is performing a nonprivileged operation, it is essential that the server know who the client is.
This is required, for example, if the server is a set-user-ID program.
Although all these forms of IPC go through the kernel, there is no facility provided by them to have the kernel identify the sender.
With message queues, if a single queue is used between the client and the server (so that only a single message is on the queue at a time, for example), the msg_lspid of the queue contains the process ID of the other process.
But when writing the server, we want the effective user ID of the client, not its process ID.
There is no portable way to obtain the effective user ID, given the process ID.
Naturally, the kernel maintains both values in the process table entry, but other than rummaging around through the kernel’s memory, we can’t obtain one, given the other.
We’ll use the following technique in Section 17.2 to allow the server to identify the client.
The same technique can be used with FIFOs, message queues, semaphores, and shared memory.
For the following description, assume that FIFOs are being used, as in Figure 15.23
The client must create its own FIFO and set the file access permissions of the FIFO so that only user-read and user-write are on.
We assume that the server has superuser privileges (or else it probably wouldn’t care about the client’s true identity), so the server can still read and write to this FIFO.
When the server receives the client’s first request on the server’s well-known FIFO (which must contain the identity of the client-specific FIFO), the server calls either stat or fstat on the client-specific FIFO.
The server assumes that the effective user ID of the client is the owner of the FIFO (the st_uid field of the stat structure)
The server verifies that only the user-read and user-write permissions are enabled.
If a malicious client can create a FIFO with someone else as the owner and set the file’s permission bits to user-read and user-write only, then the system has other fundamental security problems.
To use this technique with XSI IPC, recall that the ipc_perm structure associated with each message queue, semaphore, and shared memory segment identifies the creator of the IPC structure (the cuid and cgid fields)
As with the example using FIFOs, the server should require the client to create the IPC structure and have the client set the access permissions to user-read and user-write only.
The times associated with the IPC structure should also be verified by the server to be recent (since these IPC structures hang around until explicitly deleted)
We’ll see in Section 17.3 that a far better way of doing this authentication is for the kernel to provide the effective user ID and effective group ID of the client.
This is done by the socket subsystem when file descriptors are passed between processes.
We’ve detailed numerous forms of interprocess communication: pipes, named pipes (FIFOs), the three forms of IPC commonly called XSI IPC (message queues, semaphores, and shared memory), and an alternative semaphore mechanism provided by POSIX.
Semaphores are really a synchronization primitive, not true IPC, and are often used to synchronize access to a shared resource, such as a shared memory segment.
With pipes, we looked at the implementation of the popen function, at coprocesses, and at the pitfalls that can be encountered with the standard I/O library’s buffering.
After comparing the timing of message queues versus full-duplex pipes, and semaphores versus record locking, we can make the following recommendations: learn pipes and FIFOs, since these two basic techniques can still be used effectively in numerous applications.
Avoid using message queues and semaphores in any new applications.
Full-duplex pipes and record locking should be considered instead, as they are far simpler.
Shared memory still has its use, although the same functionality can be provided through the use of the mmap function (Section 14.8)
In the next chapter, we will look at network IPC, which can allow processes to communicate across machine boundaries.
After entering a line of input, how can you tell that the parent was terminated by SIGPIPE?
What happens in this code if waitpid isn’t available and wait is used instead?
To determine the answer, write two small test programs: one using select and one using poll.
Redo this exercise, looking at an output descriptor that is a pipe, when the read end is closed.
Although most UNIX systems allow this, show another method for opening a FIFO for both reading and writing, without blocking.
It is usually considered antisocial, however, to go snooping around in other people’s files.
But what happens if a malicious process reads a message from a message queue that is being used by a server and several clients? What information does the malicious process need to know to read the message queue?
Execute a loop five times: create a message queue, print the queue identifier, delete the message queue.
Then execute the next loop five times: create a message queue with a key of IPC_PRIVATE, and place a message on the queue.
After the program terminates, look at the message queues using ipcs(1)
In the previous chapter, we looked at pipes, FIFOs, message queues, semaphores, and shared memory—the classical methods of IPC provided by various UNIX systems.
These mechanisms allow processes running on the same computer to communicate with one another.
In this chapter, we look at the mechanisms that allow processes running on different computers (connected to a common network) to communicate with one another—network IPC.
In this chapter, we describe the socket network IPC interface, which can be used by processes to communicate with other processes, regardless of where they are running — on the same machine or on different machines.
Indeed, this was one of the design goals of the socket interface.
The same interfaces can be used for both intermachine communication and intramachine communication.
Although the socket interface can be used to communicate using many different network protocols, we will restrict our discussion to the TCP/IP protocol suite in this chapter, since it is the de facto standard for communicating over the Internet.
This chapter is only an overview of the socket API.
Stevens, Fenner, and Rudoff [2004] discuss the socket interface in detail in the definitive text on network programming in the UNIX System.
Just as they would use file descriptors to access files, applications use socket descriptors to access sockets.
Socket descriptors are implemented as file descriptors in the UNIX System.
Indeed, many of the functions that deal with file descriptors, such as read and write, will work with a socket descriptor.
The domain argument determines the nature of the communication, including the address format (described in more detail in the next section)
The constants start with AF_ (for address family) because each domain has its own format for representing an address.
The type argument determines the type of the socket, which further determines the communication characteristics.
The protocol argument is usually zero, to select the default protocol for the given domain and socket type.
When multiple protocols are supported for the same domain and socket type, we can use the protocol argument to select a particular protocol.
Figure 16.3 lists the protocols defined for the Internet domain sockets.
With a datagram (SOCK_DGRAM) interface, no logical connection needs to exist between peers for them to communicate.
All you need to do is send a message addressed to the socket being used by the peer process.
A byte stream (SOCK_STREAM), in contrast, requires that, before you can exchange data, you set up a logical connection between your socket and the socket belonging to the peer with which you wish to communicate.
Sending a datagram is analogous to mailing someone a letter.
You can mail many letters, but you can’t guarantee the order of delivery, and some might get lost along the way.
Each letter contains the address of the recipient, making the letter independent from all the others.
First, you need to establish a connection by placing a phone call, but after the connection is in place, you can communicate bidirectionally with each other.
The connection is a peer-to-peer communication channel over which you talk.
Your words contain no addressing information, as a point-to-point virtual connection exists between both ends of the call, and the connection itself implies a particular source and destination.
A SOCK_STREAM socket provides a byte-stream service; applications are unaware of message boundaries.
This means that when we read data from a SOCK_STREAM socket, it might not return the same number of bytes written by the sender.
We will eventually get everything sent to us, but it might take several function calls.
This means that the amount of data received from a SOCK_SEQPACKET socket is the same amount as was written.
The Stream Control Transmission Protocol (SCTP) provides a sequential packet service in the Internet domain.
A SOCK_RAW socket provides a datagram interface directly to the underlying network layer (which means IP in the Internet domain)
Applications are responsible for building their own protocol headers when using this interface, because the transport protocols (TCP and UDP, for example) are bypassed.
In both cases, you get a file descriptor that can be used for I/O.
When you are done using the file descriptor, you call close to relinquish access to the file or socket and free up the file descriptor for reuse.
Although a socket descriptor is actually a file descriptor, you can’t use a socket descriptor with every function that accepts a file descriptor argument.
Figure 16.4 summarizes most of the functions we’ve described so far that are used with file descriptors and describes how they behave when used with socket descriptors.
For example, lseek doesn’t work with sockets, since sockets don’t support the concept of a file offset.
We can disable I/O on a socket with the shutdown function.
If how is SHUT_RD, then reading from the socket is disabled.
If how is SHUT_WR, then we can’t use the socket for transmitting data.
We can use SHUT_RDWR to disable both data transmission and reception.
Given that we can close a socket, why is shutdown needed? There are several reasons.
First, close will deallocate the network endpoint only when the last active reference is closed.
If we duplicate the socket (with dup, for example), the socket won’t be deallocated until we close the last file descriptor referring to it.
The shutdown function allows us to deactivate a socket independently of the number of active file descriptors referencing it.
Second, it is sometimes convenient to shut a socket down in one direction only.
For example, we can shut a socket down for writing if we want the process we are communicating with to be able to tell when we are done transmitting data, while still allowing us to use the socket to receive data sent to us by the process.
In the previous section, we learned how to create and destroy a socket.
Before we learn to do something useful with a socket, we need to learn how to identify the process with which we wish to communicate.
The machine’s network address helps us identify the computer on the network we wish to contact, and the service, represented by a port number, helps us identify the particular process on the computer.
When communicating with processes running on the same computer, we generally don’t have to worry about byte ordering.
The byte order is a characteristic of the processor architecture, dictating how bytes are ordered within larger data types, such as integers.
If the processor architecture supports big-endian byte order, then the highest byte address occurs in the least significant byte (LSB)
Little-endian byte order is the opposite: the least significant byte contains the lowest byte address.
Note that regardless of the byte ordering, the most significant byte (MSB) is always on the left, and the least significant byte is always on the right.
If we were then to cast a character pointer (cp) to the address of the integer, we would see a difference from the byte ordering.
Figure 16.6 summarizes the byte ordering for the four platforms discussed in this text.
To confuse matters further, some processors can be configured for either little-endian or big-endian operation.
Network protocols specify a byte ordering so that heterogeneous computer systems can exchange protocol information without confusing the byte ordering.
The byte ordering becomes visible to applications when they exchange formatted data.
With TCP/IP, addresses are presented in network byte order, so applications sometimes need to translate them between the processor ’s byte order and the network byte order.
This is common when printing an address in a human-readable form, for example.
Four functions are provided to convert between the processor byte order and the network byte order for TCP/IP applications.
It is also common for systems to implement these functions as macros.
An address identifies a socket endpoint in a particular communication domain.
So that addresses with different formats can be passed to the socket functions, the addresses are cast to a generic sockaddr address structure:
Implementations are free to add more members and define a size for the sa_data member.
These integer data types specify the number of bits in the data type and are defined in <stdint.h>
These are the definitions required by the Single UNIX Specification.
For example, on Linux, the sockaddr_in structure is defined as.
In Section 17.2, we will see that the structure of a UNIX domain socket address is different from both of the Internet domain socket address formats.
It is sometimes necessary to print an address in a format that is understandable by a person instead of a computer.
Returns: pointer to address string on success, NULL on error.
For inet_ntop, the size parameter specifies the size of the buffer (str) to hold the text string.
Ideally, an application won’t have to be aware of the internal structure of a socket address.
If an application simply passes socket addresses around as sockaddr structures and doesn’t rely on any protocol-specific features, then the application will work with many different protocols that provide the same type of service.
Historically, the BSD networking software has provided interfaces to access the various network configuration information.
In Section 6.7, we briefly discussed the networking data files and the functions used to access them.
In this section, we discuss them in a little more detail and introduce the newer functions used to look up addressing information.
The network configuration information returned by these functions can be kept in a number of places.
This information can be kept in static files (e.g., /etc/hosts, /etc/services), or it can be managed by a name service, such as DNS (Domain Name System) or NIS (Network Information Service)
Regardless of where the information is kept, the same functions can be used to access it.
If the host database file isn’t already open, gethostent will open it.
The gethostent function returns the next entry in the file.
The sethostent function will open the file or rewind it if it is already open.
When the stayopen argument is set to a nonzero value, the file remains open after calling gethostent.
The endhostent function can be used to close the file.
When gethostent returns, we get a pointer to a hostent structure, which might point to a static data buffer that is overwritten each time we call gethostent.
The hostent structure is defined to have at least the following members:
We can get network names and numbers with a similar set of interfaces.
The address type is one of the address family constants (AF_INET, for example)
We can map between protocol names and numbers with the following functions.
Services are represented by the port number portion of the address.
Each service is offered on a unique, well-known port number.
POSIX.1 defines several new functions to allow an application to map from a host name and a service name to an address, and vice versa.
The getaddrinfo function allows us to map a host name and a service name to an address.
We need to provide the host name, the service name, or both.
If we provide only one name, the other should be a null pointer.
The host name can be either a node name or the host address in dotted-decimal notation.
The getaddrinfo function returns a linked list of addrinfo structures.
We can use freeaddrinfo to free one or more of these structures, depending on how many structures are linked together using the ai_next field in the structures.
The remaining integer fields must be set to 0, and the pointer fields must be null.
AI_CANONNAME Request a canonical name (as opposed to an alias)
AI_NUMERICHOST The host address is specified in numeric format; don’t try to translate it.
AI_NUMERICSERV The service is specified as a numeric port number; don’t try to translate it.
AI_PASSIVE Socket address is intended to be bound for listening.
If getaddrinfo fails, we can’t use perror or strerror to generate an error message.
Instead, we need to call gai_strerror to convert the error code returned into an error message.
The getnameinfo function converts an address into host and service names.
The socket address (addr) is translated into a host name and a service name.
If host is non-null, it points to a buffer hostlen bytes long that will be used to return the host name.
Similarly, if service is non-null, it points to a buffer servlen bytes long that will be used to return the service name.
The flags argument gives us some control over how the translation is done.
NI_DGRAM The service is datagram based instead of stream based.
NI_NAMEREQD If the host name can’t be found, treat this as an error.
NI_NOFQDN Return only the node name portion of the fully qualified domain name.
NI_NUMERICHOST Return the numeric form of the host address instead of the name.
NI_NUMERICSERV Return the numeric form of the service address (i.e., the port number)
If multiple protocols provide the given service for the given host, the program will print more than one entry.
When we run the program on one of the test systems, we get.
The address associated with a client’s socket is of little interest, and we can let the system choose a default address for us.
For a server, however, we need to associate a well-known address with the server’s socket on which client requests will arrive.
Clients need a way to discover the address to use to contact a server, and the simplest scheme is for a server to reserve an address and register it in /etc/services or with a name service.
We use the bind function to associate an address with a socket.
There are several restrictions on the address we can use:
The address we specify must be valid for the machine on which the process is running; we can’t specify an address belonging to some other machine.
The address must match the format supported by the address family we used to create the socket.
The port number in the address cannot be less than 1,024 unless the process has the appropriate privilege (i.e., is the superuser)
Usually, only one socket endpoint can be bound to a given address, although some protocols allow duplicate bindings.
This means that we can receive packets from any of the network interface cards installed in the system.
We’ll see in the next section that the system will choose an address and bind it to our socket for us if we call connect or listen without first binding an address to the socket.
We can use the getsockname function to discover the address bound to a socket.
Before calling getsockname, we set alenp to point to an integer containing the size of the sockaddr buffer.
On return, the integer is set to the size of the address returned.
If the address won’t fit in the buffer provided, the address is silently truncated.
If no address is currently bound to the socket, the results are undefined.
If the socket is connected to a peer, we can find out the peer’s address by calling the getpeername function.
Other than returning the peer’s address, the getpeername function is identical to the getsockname function.
The address we specify with connect is the address of the server with which we wish to communicate.
If sockfd is not bound to an address, connect will bind a default address for the caller.
When we try to connect to a server, the connect request might fail for several reasons.
For a connect request to succeed, the machine to which we are trying to connect must be up and running, the server must be bound to the address we are trying to contact, and there must be room in the server’s pending connect queue (we’ll learn more about this shortly)
Thus, applications must be able to handle connect error returns that might be caused by transient conditions.
Figure 16.10 shows one way to handle transient connect errors.
These errors are likely with a server that is running on a heavily loaded system.
This function shows what is known as an exponential backoff algorithm.
If the call to connect fails, the process goes to sleep for a short time and then tries again, increasing the delay each time through the loop, up to a maximum delay of about 2 minutes.
There is a problem with the code shown in Figure 16.10: it isn’t portable.
This technique works on Linux and Solaris, but doesn’t work as expected on FreeBSD and Mac OS X.
If the first connection attempt fails, BSD-based socket implementations continue to fail successive connection attempts when the same socket descriptor is used with TCP.
The reason for this is historical, and thus the Single UNIX Specification warns that the state of a socket is undefined if connect fails.
Because of this, portable applications need to close the socket if connect fails.
If we want to retry, we have to open a new socket.
Note that because we might have to establish a new socket, it makes no sense to pass a socket descriptor to the connect_retry function.
Instead of returning an indication of success, we now return a connected socket descriptor to the caller.
The connect function can also be used with a connectionless network service (SOCK_DGRAM)
This might seem like a contradiction, but it is an optimization instead.
If we call connect with a SOCK_DGRAM socket, the destination address of all messages we send is set to the address we specified in the connect call, relieving us from having to provide the address every time we transmit a message.
In addition, we will receive datagrams only from the address we’ve specified.
A server announces that it is willing to accept connect requests by calling the listen function.
The backlog argument provides a hint to the system regarding the number of outstanding connect requests that it should enqueue on behalf of the process.
The actual value is determined by the system, but the upper limit is specified as SOMAXCONN in <sys/socket.h>
The particular maximum depends on the implementation of each protocol.
Once the queue is full, the system will reject additional connect requests, so the backlog value must be chosen based on the expected load of the server and the amount of processing it must do to accept a connect request and start the service.
Once a server has called listen, the socket used can receive connect requests.
We use the accept function to retrieve a connect request and convert it into a connection.
The file descriptor returned by accept is a socket descriptor that is connected to the client that called connect.
This new socket descriptor has the same socket type and address family as the original socket (sockfd)
The original socket passed to accept is not associated with the connection, but instead remains available to receive additional connect requests.
If we don’t care about the client’s identity, we can set the addr and len parameters to NULL.
Otherwise, before calling accept, we need to set the addr parameter to a buffer large enough to hold the address and set the integer pointed to by len to the size of the buffer in bytes.
On return, accept will fill in the client’s address in the buffer and update the integer pointed to by len to reflect the size of the address.
All four platforms discussed in this text define EAGAIN to be the same as EWOULDBLOCK.
If a server calls accept and no connect request is present, the server will block until one arrives.
Alternatively, a server can use either poll or select to wait for a connect request to arrive.
In this case, a socket with pending connect requests will appear to be readable.
Figure 16.12 shows a function we can use to allocate and initialize a socket for use by a server process.
Figure 16.12 Initialize a socket endpoint for use by a server.
We’ll see that TCP has some strange rules regarding address reuse that make this example inadequate.
Figure 16.22 shows a version of this function that bypasses these rules, solving the major drawback with this version.
Since a socket endpoint is represented as a file descriptor, we can use read and write to communicate with a socket, as long as it is connected.
Recall that a datagram socket can be ‘‘connected’’ if we set the default peer address using the connect function.
Using read and write with socket descriptors is significant, because it means that we can pass socket descriptors to functions that were originally designed to work with local files.
We can also arrange to pass the socket descriptors to child processes that execute programs that know nothing about sockets.
Although we can exchange data using read and write, that is about all we can do with these two functions.
If we want to specify options, receive packets from multiple clients, or send out-of-band data, we need to use one of the six socket functions designed for data transfer.
Three functions are available for sending data, and three are available for receiving data.
First, we’ll look at the ones used to send data.
It is similar to write, but allows us to specify flags to change how the data we want to transmit is treated.
Like write, the socket has to be connected to use send.
The buf and nbytes arguments have the same meaning as they do with write.
Three flags are defined by the Single UNIX Specification, but it is common for implementations to support additional ones.
If send returns success, it doesn’t necessarily mean that the process at the other end of the connection receives the data.
All we are guaranteed is that when send succeeds, the data has been delivered to the network drivers without error.
With a protocol that supports message boundaries, if we try to send a single message larger than the maximum supported by the protocol, send will fail with errno set to EMSGSIZE.
With a byte-stream protocol, send will block until the entire amount of data has been transmitted.
The difference is that sendto allows us to specify a destination address to be used with connectionless sockets.
Provide feedback to the link layer to keep address mapping valid.
Shut the sender side of the socket down after sending data.
Mark the end of the record if supported by protocol.
Delay sending the packet to allow more data to be written.
Send out-of-band data if supported by protocol (see Section 16.7)
We have one more choice when transmitting data over a socket.
We can call sendmsg with a msghdr structure to specify multiple buffers from which to transmit data, similar to the writev function (Section 14.6)
We’ll see the use of ancillary data in Section 17.4
The recv function is similar to read, but allows us to specify some options to control how we receive the data.
The flags that can be passed to recv are summarized in Figure 16.14
Set the close-on-exec flag for file descriptors received over a UNIX domain socket (see Section 17.4)
Retrieve out-of-band data if supported by protocol (see Section 16.7)
Request that the real length of the packet be returned, even if it was truncated.
When we specify the MSG_PEEK flag, we can peek at the next data to be read without actually consuming it.
The next call to read or one of the recv functions will return the same data we peeked at.
With SOCK_STREAM sockets, we can receive less data than we requested.
The MSG_WAITALL flag inhibits this behavior, preventing recv from returning until all the data we requested has been received.
If we are interested in the identity of the sender, we can use recvfrom to obtain the source address from which the data was sent.
If addr is non-null, it will contain the address of the socket endpoint from which the data was sent.
When calling recvfrom, we need to set the addrlen parameter to point to an integer containing the size in bytes of the socket buffer to which addr points.
On return, the integer is set to the actual size of the address in bytes.
Because it allows us to retrieve the address of the sender, recvfrom is typically used with connectionless sockets.
The msghdr structure (which we saw used with sendmsg) is used by recvmsg to specify the input buffers to be used to receive the data.
We can set the flags argument to change the default behavior of recvmsg.
On return, the msg_flags field of the msghdr structure is set to indicate various characteristics of the data received.
The possible values on return from recvmsg are summarized in Figure 16.15
Figure 16.16 shows a client command that communicates with a server to obtain the output from a system’s uptime command.
This program connects to a server, reads the string sent by the server, and prints the string on the standard output.
The getaddrinfo function might return more than one candidate address for us to use if the server supports multiple network interfaces or multiple network protocols.
We try each one in turn, giving up when we find one that allows us to connect to the service.
To find its address, the server needs to get the name of the host on which it is running.
If the maximum host name length is indeterminate, we use HOST_NAME_MAX instead.
If the system doesn’t define HOST_NAME_MAX, we define it ourselves.
The server gets the host name by calling gethostname and looks up the address for the remote uptime service.
Multiple addresses can be returned, but we simply choose the first one for which we can establish a passive socket endpoint (i.e., one used only to listen for connect requests)
We use the initserver function from Figure 16.12 to initialize the socket endpoint on which we will wait for connect requests to arrive.
Previously, we stated that using file descriptors to access sockets was significant, because it allowed programs that knew nothing about networking to be used in a networked environment.
The version of the server shown in Figure 16.18 illustrates this point.
Instead of reading the output of the uptime command and sending it to the client, the server arranges to have the standard output and standard error of the uptime command be the socket endpoint connected to the client.
Figure 16.18 Server program illustrating command writing directly to socket.
When we execute uptime, the command writes the results to its standard output, which is connected to the socket, and the data is sent back to the ruptime client command.
The parent can safely close the file descriptor connected to the client, because the child still has it open.
The parent waits for the child to complete before proceeding, so that the child doesn’t become a zombie.
Since it shouldn’t take too long to run the uptime command, the parent can afford to wait for the child to exit before accepting the next connect request.
This strategy might not be appropriate if the child takes a long time, however.
With a connectionless socket, packets can arrive out of order, so if we can’t fit all our data in one packet, we will have to worry about ordering in our application.
The maximum packet size is a characteristic of the communication protocol.
Also, with a connectionless socket, the packets can be lost.
If we intend to have reliable communication with our peer, we have to number our packets and request retransmission from the peer application when we detect a missing packet.
We also have to identify duplicate packets and discard them, since a packet might be delayed and appear to be lost, but show up after we have requested retransmission.
The other choice we have is to deal with the error by letting the user retry the command.
For simple applications this might be adequate, but for complex applications it usually isn’t a viable alternative.
The program in Figure 16.19 is a version of the uptime client command that uses the datagram socket interface.
We use the alarm function to avoid blocking indefinitely in the call to recvfrom.
The arrival of the connect request was enough for the server to determine that it needed to provide service to a client.
But with the datagram-based protocol, we need a way to notify the server that we want it to perform its service on our behalf.
In this example, we simply send the server a 1-byte message.
The server will receive it, get our address from the packet, and use this address to transmit its response.
If the server offered multiple services, we could use this request message to indicate the service we want, but since the server does only one thing, the content of the 1-byte message doesn’t matter.
If the server isn’t running, the client will block indefinitely in the call to recvfrom.
To avoid blocking indefinitely, we set an alarm clock before calling recvfrom.
The server blocks in recvfrom for a request for service.
When a request arrives, we save the requester ’s address and use popen to run the uptime command.
We send the output back to the client using the sendto function, with the destination address set to the requester ’s address.
The socket mechanism provides two socket-option interfaces for us to control the behavior of sockets.
One interface is used to set an option, and another interface allows us to query the state of an option.
Options that are managed at the socket level, but depend on the underlying protocols for support.
The Single UNIX Specification defines only the socket-layer options (the first two option types in the preceding list)
We can set a socket option with the setsockopt function.
The level argument identifies the protocol to which the option applies.
If the option is a generic socket-level option, then level is set to SOL_SOCKET.
Otherwise, level is set to the number of the protocol that controls the option.
Figure 16.21 summarizes the generic socket-level options defined by the Single UNIX Specification.
SO_ACCEPTCONN int Return whether a socket is enabled for listening (getsockopt only)
SO_ERROR int Return and clear pending socket error (getsockopt only)
SO_LINGER struct linger Delay time when unsent messages exist and socket is closed.
SO_RCVBUF int The size in bytes of the receive buffer.
SO_RCVLOWAT int The minimum amount of data in bytes to return on a receive.
SO_RCVTIMEO struct timeval The timeout value for a socket receive call.
SO_SNDBUF int The size in bytes of the send buffer.
SO_SNDLOWAT int The minimum amount of data in bytes to transmit in a send.
SO_SNDTIMEO struct timeval The timeout value for a socket send call.
The val argument points to a data structure or an integer, depending on the option.
If the integer is nonzero, then the option is enabled.
If the integer is zero, then the option is disabled.
The len argument specifies the size of the object to which val points.
We can find out the current value of an option with the getsockopt function.
Before calling getsockopt, we set the integer to the size of the buffer where the option is to be copied.
If the actual size of the option is greater than this size, the option is silently truncated.
If the actual size of the option is less than this size, then the integer is updated with the actual size on return.
The function in Figure 16.12 fails to operate properly when the server terminates and we try to restart it immediately.
Normally, the implementation of TCP will prevent us from binding the same address until a timeout expires, which is usually on the order of several minutes.
Figure 16.22 Initialize a socket endpoint for use by a server with address reuse.
To enable the SO_REUSEADDR option, we set an integer to a nonzero value and pass the address of the integer as the val argument to setsockopt.
We set the len argument to the size of an integer to indicate the size of the object to which val points.
Out-of-band data is an optional feature supported by some communication protocols, allowing higher-priority delivery of data than normal.
Out-of-band data is sent ahead of any data that is already queued for transmission.
The socket interface to out-of-band data is heavily influenced by TCP’s implementation of out-of-band data.
To generate urgent data, we specify the MSG_OOB flag to any of the three send functions.
If we send more than one byte with the MSG_OOB flag, the last byte will be treated as the urgent-data byte.
The F_GETOWN command can be used to retrieve the current socket ownership.
As with the F_SETOWN command, a negative value represents a process group ID and a positive value represents a process ID.
We can choose to receive the urgent data inline with the normal data if we use the SO_OOBINLINE socket option.
To help us identify when we have reached the urgent mark, we can use the sockatmark function.
When out-of-band data is present in a socket’s read queue, the select function.
We can choose to receive the urgent data inline with the normal data, or we can use the MSG_OOB flag with one of the recv functions to receive the urgent data ahead of any other queue data.
If another urgent byte arrives before we receive the current one, the existing one is discarded.
Normally, the recv functions will block when no data is immediately available.
Similarly, the send functions will block when there is not enough room in the socket’s output queue to send the message.
This behavior changes when the socket is in nonblocking mode.
In this case, these functions will fail instead of blocking, setting errno to either EWOULDBLOCK or EAGAIN.
When this happens, we can use either poll or select to determine when we can receive or transmit data.
The Single UNIX Specification includes support for a general asynchronous I/O mechanism (recall Section 14.5)
The socket mechanism has its own way of handling asynchronous I/O, but this isn’t standardized in the Single UNIX Specification.
Some texts refer to the classic socket-based asynchronous I/O mechanism as ‘‘signal-based I/O’’ to distinguish it from the general asynchronous I/O mechanism found in the Single UNIX Specification.
With socket-based asynchronous I/O, we can arrange to be sent the SIGIO signal when we can read data from a socket or when space becomes available in a socket’s write queue.
Establish socket ownership so signals can be delivered to the proper processes.
Inform the socket that we want it to signal us when I/O operations won’t block.
We have several options, but they are not universally supported.
In this chapter, we looked at the IPC mechanisms that allow processes to communicate with other processes on different machines as well as within the same machine.
We discussed how socket endpoints are named and how we can discover the addresses to use when contacting servers.
We briefly discussed asynchronous and nonblocking socket I/O and the interfaces used to manage socket options.
In the next chapter, we will look at some advanced IPC topics, including how we can use sockets to pass file descriptors between processes running on the same machine.
Exercises 16.1 Write a program to determine your system’s byte ordering.
Modify the program to support service on multiple endpoints (each with a different address) at the same time.
Redesign the server so that the time to service one request doesn’t delay the processing of incoming connect requests.
Use Figure 16.23 to make sure that the functions work on all platforms with as many socket types as possible.
In the previous two chapters, we discussed various forms of IPC, including pipes and sockets.
In this chapter, we look at an advanced form of IPC—the UNIX domain socket mechanism — and see what we can do with it.
With this form of IPC, we can pass open file descriptors between processes running on the same computer system, server processes can associate names with their file descriptors, and client processes running on the same system can use these names to rendezvous with the servers.
We’ll also see how the operating system provides a unique IPC channel per client.
Although Internet domain sockets can be used for this same purpose, UNIX domain sockets are more efficient.
You can use the network-oriented socket interfaces with them, or you can use the socketpair function to create a pair of unnamed, connected, UNIX domain sockets.
Although the interface is sufficiently general to allow socketpair to be used with other domains, operating systems typically provide support only for the UNIX domain.
A pair of connected UNIX domain sockets acts like a full-duplex pipe: both ends are open for reading and writing (see Figure 17.1)
We’ll refer to these as ‘‘fd-pipes’’ to distinguish them from normal, half-duplex pipes.
Some BSD-based systems use UNIX domain sockets to implement pipes.
But when pipe is called, the write end of the first descriptor and the read end of the second descriptor are both closed.
To get a full-duplex pipe, we must call socketpair directly.
In Section 15.6.4, we said one of the problems with using XSI message queues is that we can’t use poll or select with them, because they aren’t associated with file descriptors.
However, sockets are associated with file descriptors, and we can use them to notify us when messages arrive.
When a message arrives, the thread will write it down one end of a UNIX domain socket.
Our application will use the other end of the socket to receive the message when poll indicates data can be read from the socket.
The main function creates the message queues and UNIX domain sockets and starts one thread to service each queue.
Then it uses an infinite loop to poll one end of the sockets.
When a socket is readable, it reads from the socket and writes the message on the standard output.
Figure 17.3 Poll for XSI messages using UNIX domain sockets.
Note that we use datagram (SOCK_DGRAM) sockets instead of stream sockets.
This allows us to retain message boundaries so when we read from the socket, we read only one message at a time.
This technique allows us to use either poll or select (indirectly) with message queues.
As long as the costs of one thread per queue and copying each message two extra times (once to write it to the socket and once to read it from the socket) are acceptable, this technique will make it easier to use XSI message queues.
Figure 17.4 Post a message to an XSI message queue.
This program takes two arguments: the key associated with the queue and a string to be sent as the body of the message.
When we send messages to the server, it prints them as shown below.
Although the socketpair function creates sockets that are connected to each other, the individual sockets don’t have names.
This means that they can’t be addressed by unrelated processes.
In Section 16.3.4, we learned how to bind an address to an Internet domain socket.
Just as with Internet domain sockets, UNIX domain sockets can be named and used to advertise services.
The address format used with UNIX domain sockets differs from that used with Internet domain sockets, however.
Recall from Section 16.3 that socket address formats differ from one implementation to the next.
An address for a UNIX domain socket is represented by a sockaddr_un structure.
When we bind an address to a UNIX domain socket, the system creates a file of type S_IFSOCK with the same name.
This file exists only as a means of advertising the socket name to clients.
The file can’t be opened or otherwise used for communication by applications.
If the file already exists when we try to bind the same address, the bind request will fail.
When we close the socket, this file is not automatically removed, so we need to make sure that we unlink it before our application exits.
The program in Figure 17.5 shows an example of binding an address to a UNIX domain socket.
Figure 17.5 Binding an address to a UNIX domain socket.
If we run the program a second time, however, we get an error, because the file already exists.
The program won’t succeed again until we remove the file.
If you look in <stddef.h>, you’ll see a definition similar to the following:
A server can arrange for unique UNIX domain connections to clients using the standard bind, listen, and accept functions.
Clients use connect to contact the server; after the connect request is accepted by the server, a unique connection exists between the client and the server.
Figure 17.6 shows a client process and a server process before a connection exists between the two.
The server has bound its socket to a sockaddr_un address and is listening for connection requests.
Figure 17.7 shows the unique connection between the client and server after the server has accepted the client’s connection request.
We will now develop three functions that can be used to create unique connections between unrelated processes running on the same machine.
We use UNIX domain sockets for the underlying communication mechanism here.
Returns: file descriptor to listen on if OK, negative value on error.
Returns: new file descriptor if OK, negative value on error.
Clients will use this name when they want to connect to the server.
The return value is the server’s UNIX domain socket used to receive client connection requests.
When one arrives, the system automatically creates a new UNIX domain socket, connects it to the client’s socket, and returns the new socket to the server.
Additionally, the effective user ID of the client is stored in the memory to which uidptr points.
The name argument specified by the client must be the same name that was advertised by the server’s call to serv_listen.
On return, the client gets a file descriptor connected to the server.
First, we create a single UNIX domain socket by calling socket.
We then fill in a sockaddr_un structure with the well-known pathname to be assigned to the socket.
Note that we don’t need to set the sun_len field present on some platforms, because the operating system sets this for us, deriving it from the address length we pass to the bind function.
Finally, we call listen (Section 16.4) to tell the kernel that the process will be acting as a server awaiting connections from clients.
Wait for a client connection to arrive, and accept it.
We also obtain the client’s user ID from the pathname * that it must bind before calling us.
The server blocks in the call to accept, waiting for a client to call cli_conn.
When accept returns, its return value is a brand-new descriptor that is connected to the client.
Additionally, the pathname that the client assigned to its socket (the name that contained the client’s process ID) is returned by accept, through the second argument (the pointer to the sockaddr_un structure)
Then we call stat to verify that the pathname is indeed a socket and that the permissions allow only user-read, user-write, and user-execute.
We also verify that the three times associated with the socket are no older than 30 seconds.
Recall from Section 6.10 that the time function returns the current time and date in seconds past the Epoch.
If all these checks are OK, we assume that the identity of the client (its effective user ID) is the owner of the socket.
Although this check isn’t perfect, it’s the best we can do with current systems.
It would be better if the kernel returned the effective user ID to us through a parameter to accept.
We call socket to create the client’s end of a UNIX domain socket.
We then fill in a sockaddr_un structure with a client-specific name.
We don’t let the system choose a default address for us, because the server would be unable to distinguish one client from another (if we don’t explicitly bind a name to a UNIX domain socket, the kernel implicitly binds an address to it on our behalf and no file is created in the file system to represent the socket)
Instead, we bind our own address — a step we usually don’t take when developing a client program that uses sockets.
The last five characters of the pathname we bind are made from the process ID of the client.
We call unlink, just in case the pathname already exists.
We then call bind to assign a name to the client’s socket.
This creates a socket file in the file system with the same name as the bound pathname.
We call chmod to turn off all permissions other than user-read, user-write, and user-execute.
In serv_accept, the server checks these permissions and the user ID of the socket to verify the client’s identity.
We then have to fill in another sockaddr_un structure, this time with the well-known pathname of the server.
Finally, we call the connect function to initiate the connection with the server.
Passing an open file descriptor between processes is a powerful technique.
It can lead to different ways of designing client–server applications.
It allows one process (typically a server) to do everything that is required to open a file (involving such details as translating a network name to a network address, dialing a modem, and negotiating locks for the file) and simply pass back to the calling process a descriptor that can be used with all the I/O functions.
All the details involved in opening the file or device are hidden from the client.
We must be more specific about what we mean by ‘‘passing an open file descriptor’’ from one process to another.
Recall Figure 3.8, which showed two processes that have opened the same file.
Although they share the same v-node, each process has its own file table entry.
When we pass an open file descriptor from one process to another, we want the passing process and the receiving process to share the same file table entry.
Technically, we are passing a pointer to an open file table entry from one process to another.
This pointer is assigned the first available descriptor in the receiving process.
Saying that we are passing an open descriptor mistakenly gives the impression that the descriptor number in the receiving process is the same as in the sending process, which usually isn’t true.
Having two processes share an open file table is exactly what happens after a fork (recall Figure 8.2)
What normally happens when a descriptor is passed from one process to another is that the sending process, after passing the descriptor, then closes the descriptor.
Closing the descriptor by the sender doesn’t really close the file or device, since the descriptor is still considered open by the receiving process (even if the receiver hasn’t specifically received the descriptor yet)
We define the following three functions that we use in this chapter to send and receive file descriptors.
Later in this section, we’ll show the code for these three functions.
Figure 17.11 Passing an open file from the top process to the bottom process.
The process waiting to receive the descriptor (the client) calls recv_fd.
We implement our own protocol that is used by these three functions.
Any characters read up to this point are passed to the caller’s userfunc.
The next byte read by recv_fd is the status byte.
If the status byte is 0, a descriptor was passed; otherwise, there is no descriptor to receive.
Both functions take a pointer to a msghdr structure that contains all the information on what to send or receive.
The structure on your system might look similar to the following:
The first two elements are normally used for sending datagrams on a network connection, where the destination address can be specified with each datagram.
The next two elements allow us to specify an array of buffers (scatter read or gather write), as we described for the readv and writev functions (Section 14.6)
Two elements deal with the passing or receiving of control information.
To send a file descriptor, we set cmsg_len to the size of the cmsghdr structure, plus the size of an integer (the descriptor)
Access rights can be passed only across a UNIX domain socket.
Three macros are used to access the control data, and one macro is used to help calculate the value to be used for cmsg_len.
Returns: pointer to first cmsghdr structure associated with the msghdr structure, or NULL if none exists.
Returns: pointer to next cmsghdr structure associated with the msghdr structure given the current cmsghdr.
The CMSG_LEN macro returns the number of bytes needed to store a data object of size nbytes, after adding the size of the cmsghdr structure, adjusting for any alignment constraints required by the processor architecture, and rounding up.
In the sendmsg call, we send both the protocol data (the null and the status byte) and the descriptor.
If fd<0, then -fd is sent back instead as the error status.
Figure 17.13 Sending a file descriptor over a UNIX domain socket.
We use the CMSG_LEN macro to calculate the amount of space needed.
We read from the socket until we read the null byte that precedes the final status byte.
Everything up to this null byte is an error message from the sender.
See if this is the final data with null & status.
Null * is next to last byte of buffer; status byte is last byte.
Zero status means there is a file descriptor to receive.
Figure 17.14 Receiving a file descriptor over a UNIX domain socket.
It would have been better for the kernel to pass us the credentials of the caller on return from the call to accept.
Some UNIX domain socket implementations provide similar functionality when exchanging messages, but their interfaces differ.
Mac OS X 10.6.8 is derived in part from FreeBSD, but has credential passing disabled.
Solaris 10 doesn’t support sending credentials over UNIX domain sockets.
However, it supports the ability to obtain the credentials of a process passing a file descriptor over a STREAMS pipe, although we do not discuss the details here.
When we transmit credentials, we need to reserve space only for the cmsgcred structure.
The kernel will fill in this structure for us to prevent an application from pretending to have a different identity.
Unlike FreeBSD, Linux requires that we initialize this structure before transmission.
The kernel will ensure that applications either use values that correspond to the caller or have the appropriate privilege to use other values.
Note that we need to initialize the credentials structure only on Linux.
Instead of sending the contents of the file back to the calling process, however, this server sends back an open file descriptor.
As a result, the open server can work with any type of file (such as a device or a socket) and not simply regular files.
The client and server exchange a minimum amount of information using IPC: the filename and open mode sent by the client, and the descriptor returned by the server.
The contents of the file are not exchanged using IPC.
There are several advantages in designing the server to be a separate executable program (either one that is executed by the client, as we develop in this section, or a daemon server, which we develop in the next section)
The server can easily be contacted by any client, similar to the client calling a library function.
We are not hard-coding a particular service into the application, but designing a general facility that others can reuse.
If we need to change the server, only a single program is affected.
Conversely, updating a library function can require that all programs that call the function be updated (i.e., relinked with the link editor)
The server can be a set-user-ID program, providing it with additional permissions that the client does not have.
Note that a library function (or shared library function) can’t provide this capability.
The client process creates an fd-pipe and then calls fork and exec to invoke the server.
The client sends requests across the fd-pipe using one end, and the server sends back responses over the fd-pipe using the other end.
We define the following application protocol between the client and the server.
The <openmode> is the numeric value, in ASCII decimal, of the second argument to the open function.
This is an example of a process sending an open descriptor to its parent.
In Section 17.6, we’ll modify this example to use a single daemon server, where the server sends a descriptor to a completely unrelated process.
We first have the header, open.h (Figure 17.17), which includes the standard headers and defines the function prototypes.
The main function (Figure 17.18) is a loop that reads a pathname from standard input and copies the file to standard output.
The function calls csopen to contact the open server and return an open descriptor.
The function csopen (Figure 17.19) does the fork and exec of the server, after creating the fd-pipe.
Open the file by sending the "name" and "oflag" to the * connection server and reading a file descriptor back.
The child closes one end of the fd-pipe, and the parent closes the other.
For the server that it executes, the child also duplicates its end of the fd-pipe onto its standard input and standard output.
Another option would have been to pass the ASCII representation of the descriptor fd[1] as an argument to the server.
The parent sends to the server the request containing the pathname and open mode.
Finally, the parent calls recv_fd to return either the descriptor or an error.
If an error is returned by the server, write is called to output the message to standard error.
It is the program opend that is executed by the client in Figure 17.19
First, we have the opend.h header (Figure 17.20), which includes the standard headers and declares the global variables and function prototypes.
If all is OK, open is called to open the file, and then send_fd sends the descriptor back to the client across the fd-pipe (its standard output)
If an error is encountered, send_err is called to send back an error message, using the client–server protocol that we described earlier.
We use the ISO C function strtok to tokenize the string into separate arguments.
Note that user’s buf[] * array is modified (nulls placed after each token)
It verifies that the client sent the right number of arguments and stores the pathname and open mode in global variables.
This completes the open server that is invoked by a fork and exec from the client.
A single fd-pipe is created before the fork and is used to communicate between the client and the server.
In this section, we develop an open server as a daemon process.
We expect this design to be more efficient, since a fork and an exec are avoided.
We use a UNIX domain socket connection between the client and the server and demonstrate passing file descriptors between unrelated processes.
This server also demonstrates how a single server can handle multiple clients, using both the select and poll functions from Section 14.4
This version of the client is similar to the client from Section 17.5
We add the following line to the open.h header (Figure 17.17):
The protocol from the client to the server remains the same.
Since this server handles all clients, it must maintain the state of each client connection.
This is done with the client array declared in the opend.h header.
Called by loop() when connection request from a new client arrives.
After these ten entries are all in use, a later call to client_add causes realloc to allocate additional space.
By dynamically allocating space this way, we have not limited the size of the client array at compile time to some value that we guessed and put into a header.
These functions call the log_ functions (Appendix B) if an error occurs, since we assume that the server is a daemon.
Normally the server will run as a daemon, but we want to provide an option that allows it to be run in the foreground, with diagnostic messages sent to the standard error.
This should make the server easier to test and debug, especially if we don’t have permission to read the log file where the diagnostic messages are normally written.
We’ll use a command-line option to control whether the server runs in the foreground or as a daemon in the background.
It is important that all commands on a system follow the same conventions, because this makes them easier to use.
If someone is familiar with the way command-line options are formed with one command, it would create more chances for mistakes if another command followed different conventions.
This problem is sometimes visible when dealing with white space on the command line.
Some commands require that an option be separated from its argument by white space, but other commands require the argument to follow immediately after its option, without any intervening spaces.
Without a consistent set of rules to follow, users either have to memorize the syntax of all commands or resort to a trial-and-error process when invoking them.
Luckily, the getopt function exists to help command developers process command-line options in a consistent manner.
The argc and argv arguments are the same ones passed to the main function of the program.
The options argument is a string containing the option characters supported by the command.
If an option character is followed by a colon, then the option takes an argument.
For example, if the usage statement for a command was.
The getopt function is normally used in a loop that terminates when getopt.
The way to remove the file is to type rm -- -bar.
It starts at 1 and is incremented for each argument processed by getopt.
The open server’s main function (Figure 17.28) defines the global variables, processes the command-line options, and calls the function loop.
If we invoke the server with the -d option, the server runs interactively instead of as a daemon.
The remainder of the function is a loop that starts with a call to select.
The descriptor listenfd can be ready for reading, which means that a new client has called cli_conn.
We keep track of the highest descriptor number for the first.
We also keep track of the highest index in use in the client array.
This means that the client has either terminated or sent a new request.
We find out about a client termination by read returning 0 (end of file)
We keep track of which descriptors are currently in use in the allset descriptor set.
As new clients connect to the server, the appropriate bit is turned on in this descriptor set.
The appropriate bit is turned off when the client terminates.
We always know when a client terminates, whether the termination is voluntary or not, since all the client’s descriptors (including the connection to the server) are automatically closed by the kernel.
The loop function that uses poll is shown in Figure 17.30
We use the first entry (index 0) of the pollfd array for the listenfd descriptor.
The arrival of a new client connection is indicated by a POLLIN on the listenfd descriptor.
For an existing client, we have to handle two different events from poll: a client termination is indicated by POLLHUP, and a new request from an existing client is indicated by POLLIN.
The client can close its end of the connection while there is still data to be read from the server’s end of the connection.
Even though the endpoint is marked as hung up, the server can read all the data queued on its end.
But with this server, when we receive the hangup from the client, we can close the connection to the client, effectively throwing away any queued data.
There is no reason to process any requests still remaining, since we can’t send any responses back.
This function is similar to the earlier version (Figure 17.22)
This completes the second version of the open server, which uses a single daemon to handle all the client requests.
The key points in this chapter are the ability to pass file descriptors between processes and the ability of a server to accept unique connections from clients.
Although all platforms provide support for UNIX domain sockets (refer back to Figure 15.1), we’ve seen that there are differences in each implementation, which makes it more difficult for us to develop portable applications.
We saw how to use them to implement a full-duplex pipe and how they can be used to adapt the I/O multiplexing functions from Section 14.4 to work indirectly with XSI message queues.
One version was invoked directly by the client, using fork and exec.
The second was a daemon server that handled all client requests.
Both versions used the file descriptor passing and receiving functions.
We also saw how to use the getopt function to enforce consistent command-line processing for our programs.
Describe the changes that would be necessary to use regular pipes instead.
How can we avoid copying the messages two extra times?
The program calls fork, and the child opens an existing file and passes the open descriptor to the parent.
The child then positions the file using lseek and notifies the parent.
The parent reads the file’s current offset and prints it for verification.
If the file was passed from the child to the parent as we described, they should be sharing the same file table entry, so each time the child changes the file’s current offset, that change should also affect the parent’s descriptor.
Have the child position the file to a different offset and notify the parent again.
To avoid unintentionally removing a file that isn’t a socket, we could call stat first to verify the file type.
Try them out to see if they are supported by your operating system.
The handling of terminal I/O is a messy area, regardless of the operating system.
The manual page for terminal I/O is usually one of the longest in most editions of the programmer ’s manuals.
The System III style of terminal I/O continued through System V, and the Version 7 style became the standard for the BSD-derived systems.
As with signals, this difference between the two worlds has been conquered by POSIX.1
In this chapter, we look at all the POSIX.1 terminal functions and some of the platform-specific additions.
Part of the complexity of the terminal I/O system occurs because people use terminal I/O for so many different things: terminals, hard-wired lines between computers, modems, printers, and so on.
The terminal driver returns at most one line per read request.
If we don’t do anything special, canonical mode is the default.
For example, if the shell redirects standard input to the terminal and we use read and write to copy standard input to standard output, the terminal is in canonical mode, and each read returns at most one line.
Programs that manipulate the entire screen, such as the vi editor, use noncanonical mode, since the commands may be single characters and are not terminated by newlines.
Also, this editor doesn’t want processing by the system of the special characters, since they may overlap with the editor commands.
For example, the Control-D character is often the end-of-file character for the terminal, but it’s also a vi command to scroll down one-half screen.
The Version 7 and older BSD-style terminal drivers supported three modes for terminal input: (a) cooked mode (the input is collected into lines, and the special characters are processed), (b) raw mode (the input is not assembled into lines, and there is no processing of special characters), and (c) cbreak mode (the input is not assembled into lines, but some of the special characters are processed)
We’ve been using some of these throughout the text—the end-of-file character (usually Control-D) and the suspend character (usually Control-Z), for example.
We can think of a terminal device as being controlled by a terminal driver, usually within the kernel.
Each terminal device has an input queue and an output queue, shown in Figure 18.1
Figure 18.1 Logical picture of input and output queues for a terminal device.
If echoing is enabled, there is an implied link between the input queue and the output queue.
When the input queue for a particular device fills, the system behavior is implementation dependent.
Most UNIX systems echo the bell character when this happens.
There is another input limit, MAX_CANON, that we don’t show here.
This limit is the maximum number of bytes in a canonical input line.
Although the size of the output queue is finite, no constants defining that size are accessible to the program, because when the output queue starts to fill up, the kernel simply puts the writing process to sleep until room is available.
We’ll see how the tcflush flush function allows us to flush either the input queue or the output queue.
Similarly, when we describe the tcsetattr function, we’ll see how we can tell the system to change the attributes of a terminal device only after the output queue is empty.
We want to do this, for example, if we’re changing the output attributes.
We can also tell the system to discard everything in the input queue when changing the terminal attributes.
We want to do this if we’re changing the input attributes or changing between canonical and noncanonical modes, so that previously entered characters aren’t interpreted in the wrong mode.
Most UNIX systems implement all the canonical processing in a module called the terminal line discipline.
We can think of this module as a box that sits between the kernel’s generic read and write functions and the actual device driver (see Figure 18.2)
By isolating the canonical processing in a separate module, all terminal drivers can support canonical processing consistently.
All the terminal device characteristics that we can examine and change are contained in a termios structure.
This structure is defined in the header <termios.h>, which we use throughout this chapter:
Roughly speaking, the input flags control the input of characters by the terminal device driver (e.g., strip eighth bit on input, enable input parity checking), the output flags control the driver output (e.g., perform output processing, map newline to CR/LF), the control flags affect the RS-232 serial lines (e.g., ignore modem status lines, one or two stop bits per character), and the local flags affect the interface between the driver and the user (e.g., echo on or off, visually erase characters, enable terminal-generated signals, job control stop signal for background output)
The type tcflag_t is big enough to hold each of the flag values and is often defined as an unsigned int or an unsigned long.
The c_cc array contains all the special characters that we can change.
The cc_t type is large enough to hold each special character and is typically an unsigned char.
Versions of System V that predated the POSIX standard had a header named <termio.h> and a structure named termio.
POSIX.1 added an s to the names, to differentiate them from their predecessors.
Note that even though the Single UNIX Specification defines a common subset that all platforms start from, all the implementations have their own additions.
Most of these additions come from the historical differences between the systems.
We’ll discuss each of these flag values in detail in Section 18.5
Given all the options available, how do we examine and change these characteristics of a terminal device? Figure 18.7 summarizes the various functions defined by the Single UNIX Specification that operate on terminal devices.
All the functions listed are part of the base POSIX specification.
Note that the Single UNIX Specification doesn’t use the classic ioctl on terminal devices.
The reason is that the ioctl function for terminal devices uses a different data type for its final argument, which depends on the action being performed.
The handling of terminal devices is complicated by the large number of options available for terminal devices and the challenge of trying to determine which options are required for a particular device (be it a terminal, modem, printer, or whatever)
POSIX.1 doesn’t specify where in the termios structure the baud rate information is stored; that is an implementation detail.
Some systems, such as Solaris, store this information in the c_cflag field.
Linux and BSD-derived systems, such as FreeBSD and Mac OS X, have two separate fields in the structure: one for the input speed and one for the output speed.
To do this, we modify the appropriate entry in the c_cc array of the termios structure.
The elements in this array are referred to by name, with each name beginning with a V (the third column in Figure 18.9)
In early versions of the Single UNIX Specification, support for _POSIX_VDISABLE was optional.
All four platforms discussed in this text support this feature.
Before describing all the special characters in detail, let’s look at a small program that changes them.
The program in Figure 18.10 disables the interrupt character and sets the end-of-file character to Control-B.
We modify the terminal characters only if standard input is a terminal device.
The function tcgetattr (Section 18.4) fetches a termios structure from the.
After we’ve modified this structure, we call tcsetattr to set the attributes.
The only attributes that change are the ones we specifically modified.
Disabling the interrupt key is different from ignoring the interrupt signal.
The program in Figure 18.10 simply disables the special character that causes the terminal driver to generate SIGINT.
We can still use the kill function to send the signal to the process.
We now describe each of the special characters in more detail.
We call these the special input characters, but two of the characters, STOP and START (Control-S and Control-Q), are also handled specially when output.
Note that when recognized by the terminal driver and processed specially, most of these special characters are then discarded: they are not returned to the process in a read operation.
The exceptions are the newline characters (NL, EOL, EOL2) and the carriage return (CR)
When both ICANON (canonical mode) and ICRNL (map CR to NL) are set and IGNCR (ignore CR) is not set, the CR character is translated to NL and has the same effect as a NL character.
This character is returned to the reading process (perhaps after being translated to a NL)
This character, recognized on input in extended mode (IEXTEN), causes subsequent output to be discarded until another DISCARD character is entered or the discard condition is cleared (see the FLUSHO option)
This character is discarded when processed (i.e., it is not passed to the process)
This character is recognized on input in extended mode (IEXTEN) if job control is supported and if the ISIG flag is set.
Like the SUSP character, this delayed-suspend character generates the SIGTSTP signal that is sent to all processes in the foreground process group (refer to Figure 9.7)
However, the delayed-suspend character generates a signal only when a process reads from the controlling terminal, not when the character is typed.
This character is discarded when processed (i.e., it is not passed to the process)
This character is recognized on input in canonical mode (ICANON)
When we type this character, all bytes waiting to be read are immediately passed to the reading process.
If no bytes are waiting to be read, a count of 0 is returned.
Entering an EOF character at the beginning of the line is the normal way to indicate the end of file to a program.
This character is discarded when processed in canonical mode (i.e., it is not passed to the process)
This character is recognized on input in canonical mode (ICANON) and is returned to the reading process; however, this character is not normally used.
This character is recognized on input in canonical mode (ICANON) and erases the previous character in the line, not erasing beyond the beginning of the line.
The erase character is discarded when processed in canonical mode (i.e., it is not passed to the process)
This character is treated exactly like the erase character (ERASE)
This character is recognized on input if the ISIG flag is set and generates the SIGINT signal that is sent to all processes in the foreground process group (refer to Figure 9.7)
This character is discarded when processed (i.e., it is not passed to the process)
The name ‘‘kill’’ is overused; recall the kill function used to send a signal to a process.
This character should be called the line-erase character; it has nothing to do with signals.
It erases the entire line and is discarded when processed (i.e., it is not passed to the process)
This character is recognized on input in extended mode (IEXTEN) and causes any special meaning of the next character to be ignored.
This works for all special characters listed in this section.
We can use this character to type any character to a program.
The LNEXT character is discarded when processed, but the next character entered is passed to the process.
This character is recognized on input if the ISIG flag is set.
The quit character generates the SIGQUIT signal, which is sent to all processes in the foreground process group (refer to Figure 9.7)
This character is discarded when processed (i.e., it is not passed to the process)
Recall from Figure 10.1 that the difference between INTR and QUIT is that the QUIT character not only terminates the process by default, but also generates a core file.
This character is recognized on input in extended, canonical mode (both IEXTEN and ICANON flags set) and causes all unread input to be output (reechoed)
This character is discarded when processed (i.e., it is not passed to the process)
This character is recognized on input if the IXON flag is set and is automatically generated as output if the IXOFF flag is set.
A received START character with IXON set causes stopped output (from a previously entered STOP character) to restart.
In this case, the START character is discarded when processed (i.e., it is not passed to the process)
When IXOFF is set, the terminal driver automatically generates a START character to resume input that it had previously stopped, when the new input will not overflow the input buffer.
This character is recognized on input in extended, canonical mode (both IEXTEN and ICANON flags set) and generates the SIGINFO signal, which is sent to all processes in the foreground process group (refer to Figure 9.7)
Additionally, if the NOKERNINFO flag is not set, status information on the foreground process group is displayed on the terminal.
This character is discarded when processed (i.e., it is not passed to the process)
This character is recognized on input if the IXON flag is set and is automatically generated as output if the IXOFF flag is set.
A received STOP character with IXON set stops the output.
In this case, the STOP character is discarded when processed (i.e., it is not passed to the process)
The stopped output is restarted when a START character is entered.
When IXOFF is set, the terminal driver automatically generates a STOP character to prevent the input buffer from overflowing.
This character is recognized on input if job control is supported and if the ISIG flag is set.
The suspend character generates the SIGTSTP signal, which is sent to all processes in the foreground process group (refer to Figure 9.7)
This character is discarded when processed (i.e., it is not passed to the process)
This character is recognized on input in extended, canonical mode (both IEXTEN and ICANON flags set) and causes the previous word to be erased.
First, it skips backward over any white space (spaces or tabs), then skips backward over the previous token, leaving the cursor positioned where the first character of the previous token was located.
Normally, the previous token ends when a white space character is encountered.
We can change this behavior, however, by setting the ALTWERASE flag.
This flag causes the previous token to end when the first nonalphanumeric character is encountered.
The word-erase character is discarded when processed (i.e., it is not passed to the process)
Another ‘‘character ’’ that we need to define for terminal devices is the BREAK character.
A BREAK condition is signaled to the device driver in various ways, depending on the serial interface.
Most old serial terminals have a key labeled BREAK that generates the BREAK condition, which is why most people think of BREAK as a character.
On PCs, the break key might be mapped for another purpose.
For example, the Windows command interpreter can be interrupted by pressing Control-BREAK.
For asynchronous serial data transmission, a BREAK is a sequence of zero-valued bits that continues for longer than the time required to send one byte.
The entire sequence of zero-valued bits is considered a single BREAK.
In Section 18.8, we’ll see how to send a BREAK with the tcsendbreak function.
To get and set a termios structure, we call two functions: tcgetattr and tcsetattr.
This is how we examine and modify the various option flags and special characters to make the terminal operate the way we want it to.
The argument opt for tcsetattr lets us specify when we want the new terminal attributes to take effect.
This argument is specified as one of the following constants.
Furthermore, when the change takes place, all input data that has not been read is discarded (flushed)
The return status of tcsetattr can be confusing to use correctly.
This function returns OK if it was able to perform any of the requested actions, even if it couldn’t perform all the requested actions.
If the function returns OK, it is our responsibility to see whether all the requested actions were performed.
This means that after we call tcsetattr to set the desired attributes, we need to call tcgetattr and compare the actual terminal’s attributes to the desired attributes to detect any differences.
Other systems might leave the attributes with the values they had the last time that the terminal was used.
This will ensure that when we call tcgetattr, any nonstandard portions of the termios structure will be initialized so the terminal will behave as expected when we change the attributes and call tcsetattr.
This list is alphabetical and indicates in which of the four terminal flag fields the option appears.
The field that controls a given option is usually not apparent from the option name alone.
We also note whether each option is defined by the Single UNIX Specification and list the platforms that support it.
All the flags listed specify one or more bits that we turn on or clear, unless we call the flag a mask.
A mask defines multiple bits grouped together from which a set of values is defined.
We have a defined name for the mask and a name for each value.
Refer to the termio(7I) manual page on Solaris for the length of each delay value.
In all cases, a delay mask of 0 means no delay.
If a delay is specified, the OFILL and OFDEL flags determine whether the driver does an actual delay or whether fill characters are transmitted instead.
Example Figure 18.11 demonstrates the use of these masks to extract a value and to set a value.
Instead of moving backward until the previous white space character, this flag causes the WERASE character to move backward until the first nonalphanumeric character is encountered.
This signal is generated for the foreground process group if the terminal device is a controlling terminal.
When this flag is not set, an open of a terminal device usually blocks until the modem answers a call and establishes a connection, for example.
If PARODD is set, the parity bit is always 1 (mark parity)
The state of the Request-To-Send RS-232 signal controls the flow control.
This size does not include the parity bit, if any.
Input characters can be echoed in either canonical or noncanonical mode.
Be aware that some systems echo the EOF character differently, since its typical value is Control-D.
Control-D is the ASCII EOT character, which can cause some terminals to hang up.
This is usually done in the terminal driver by writing the three-character sequence backspace, space, backspace.
If the WERASE character is supported, ECHOE causes the previous word to be erased using one or more of the same three-character sequence.
If the ECHOPRT flag is supported, the actions described here for ECHOE assume that the ECHOPRT flag is not set.
If the ECHOKE flag is supported, this description of ECHOK assumes that ECHOKE is not set.
The way in which each character is erased is selected by the ECHOE and ECHOPRT flags.
This is often useful on a hard-copy terminal to see exactly which characters are being deleted.
This can be the case if the serial communication peripheral card can offload the host processor by doing some of the line discipline processing.
This can also be the case when using pseudo terminals (Chapter 19)
This flag is set when we type the DISCARD character; the flag is cleared when we type another DISCARD character.
We can also set or clear this condition by setting or clearing this terminal flag.
If canonical mode is not enabled, read requests are satisfied directly from the input queue.
A read does not return until at least MIN bytes have been received or the timeout value TIME has expired between bytes.
See BRKINT for a way to have a BREAK condition either generate a SIGINT signal or be read as data.
If this flag is not set, it is possible to translate the received CR into a NL character if the ICRNL flag is set.
If INPCK is not set, input parity checking is disabled.
When this flag is not set, all 8 bits are processed.
When it notices that the input queue is getting full, the terminal driver outputs a STOP character.
This character should be recognized by the device that is sending the data and cause the device to stop.
Later, when the characters on the input queue have been processed, the terminal driver will output a START character.
When the terminal driver receives a STOP character, output stops.
While the output is stopped, the next START character resumes the output.
If this flag is not set, the START and STOP characters are read by the process as normal characters.
Also, when it generates the SIGSUSP signal, the input queue is flushed.
If the NOFLSH flag is set, this normal flushing of the queues does not occur when these signals are generated.
Regardless of whether this flag is set, however, the STATUS character still causes the SIGINFO signal to be sent to the foreground process group.
This produces the same effect as setting the horizontal tab delay (TABDLY) to XTABS or TAB3
The parity is odd if PARODD is set; otherwise, it is even parity.
See also the discussion of the INPCK, IGNPAR, and PARMRK flags.
If PARODD is set, the parity bit is always 1 (mark parity)
If neither IGNPAR nor PARMRK is set, a byte with a framing error (other than a BREAK) or with a parity error is read as a single character \0
Note that the PARENB flag controls the generation and detection of parity.
The PARODD flag also controls whether mark or space parity is used when either the CMSPAR or PAREXT flag is set.
This action is similar to what happens when we type the REPRINT character.
This value causes the system to expand tabs into spaces.
The system assumes a tab stop every eight spaces, and we can’t change this assumption.
By default, this signal stops all the processes in the process group.
This signal is not generated by the terminal driver if the background process that is writing to the controlling terminal is either ignoring or blocking the signal.
To input an uppercase character, precede it with a backslash.
Similarly, the system outputs an uppercase character by preceding it with a backslash.
This option flag is obsolete today, since most, if not all, uppercase-only terminals have disappeared.
This command is simply an interface to the first six functions that we listed in Figure 18.7
The last four lines display the current settings for each of the terminal special characters (Section 18.3)
The first line displays the number of rows and columns for the current terminal window; we discuss the terminal window size in Section 18.12
The stty command uses its standard input to get and set the terminal option flags.
Although some older implementations used standard output, POSIX.1 requires that the standard input be used.
All four implementations discussed in this text provide versions of stty that operate on the standard input.
The term baud rate is a historical term that should be referred to today as ‘‘bits per second.’’ Although most terminal devices use the same baud rate for both input and output, the capability exists to set the two rates to different values, if the hardware allows this.
To use these functions, we must realize that the input and output baud rates are stored in the device’s termios structure, as shown in Figure 18.8
Before calling either of the cfget functions, we first have to obtain the device’s termios structure using tcgetattr.
Similarly, after calling either of the two cfset functions, all we’ve done is set the baud rate in a termios structure.
For this change to affect the device, we have to call tcsetattr.
If there is an error in either of the baud rates that we set, we may not find out about the error until we call tcsetattr.
The four baud rate functions exist to insulate applications from differences in the way that implementations represent baud rates in the termios structure.
The speed values we get from the cfget functions and pass to the cfset functions are untranslated from their representation as they are stored in the termios structure.
The tcdrain function waits for all output to be transmitted.
The tcflow function gives us control over both input and output flow control.
The action argument must be one of the following four values:
The tcflush function lets us flush (throw away) either the input buffer (data that has been received by the terminal driver, which we have not read) or the output buffer (data that we have written, which has not yet been transmitted)
The queue argument must be one of the following three constants:
The tcsendbreak function transmits a continuous stream of zero bits for a specified duration.
POSIX.1 specifies that if duration is nonzero, the transmission time is implementation dependent.
Historically, the name of the controlling terminal in most versions of the UNIX System has been /dev/tty.
POSIX.1 provides a runtime function that we can call to determine the name of the controlling terminal.
Returns: pointer to name of controlling terminal on success, pointer to empty string on error.
If ptr is non-null, it is assumed to point to an array of at least L_ctermid bytes, and the name of the controlling terminal of the process is stored in the array.
If ptr is a null pointer, the function allocates room for the array (usually as a static variable)
Again, the name of the controlling terminal of the process is stored in the array.
In both cases, the starting address of the array is returned as the value of the function.
Since most UNIX systems use /dev/tty as the name of the controlling terminal, this function is intended to aid portability to other operating systems.
All four platforms described in this text return the string /dev/tty when we call ctermid.
Note that we can’t protect against overrunning the caller’s buffer, because we have no way to determine its size.
Two functions that are more interesting for a UNIX system are isatty, which returns true if a file descriptor refers to a terminal device, and ttyname, which returns the pathname of the terminal device that is open on a file descriptor.
The isatty function is trivial to implement, as we show in Figure 18.13
We simply try one of the terminal-specific functions (that doesn’t change anything if it succeeds) and look at the return value.
We test our isatty function with the program in Figure 18.14
When we run the program from Figure 18.14, we get the following output:
The ttyname function (Figure 18.15) is longer, as we have to search all the device entries, looking for a match.
The technique is to read the /dev directory, looking for an entry with the same device number and i-node number.
We assume in this function that when we hit a matching device.
We could also verify that the two entries have matching st_rdev fields (the major and minor device numbers for the terminal device) and that the directory entry is a character special file.
However, since we’ve already verified that the file descriptor argument is both a terminal device and a character special file, and since a matching device number and i-node number pair is unique on a UNIX system, there is no need for the additional comparisons.
The name of our terminal might reside in a subdirectory in /dev.
Thus, we might need to search the entire file system tree under /dev.
We skip several directories that might produce incorrect or odd-looking results: /dev/., /dev/.., and /dev/fd.
We also skip the aliases /dev/stdin, /dev/stdout, and /dev/stderr, since they are symbolic links to files in /dev/fd.
We can test this implementation with the program shown in Figure 18.16
Canonical mode is simple: we issue a read, and the terminal driver returns when a line has been entered.
The read returns when the requested number of bytes have been read.
If we read a partial line, no information is lost; the next read starts where the previous read stopped.
Also, recall from Section 18.5 that if ICRNL is set and if IGNCR is not set, then the CR character also terminates a line, since it acts just like the NL character.
Of these five line delimiters, one (EOF) is discarded by the terminal driver when it’s processed.
The other four are returned to the caller as the last character of the line.
The read also returns if a signal is caught and if the function is not automatically restarted (Section 10.5)
We now examine the function getpass, which reads a password of some type from the user at a terminal.
To read the password, the function must turn off echoing, but it can leave the terminal in canonical mode, as whatever we type as the password forms a complete line.
Figure 18.17 shows a typical implementation on a UNIX system.
Instead of hard-wiring /dev/tty into the program, we call the function ctermid to open the controlling terminal.
We read and write only to the controlling terminal and return an error if we can’t open this device for reading and writing.
The version of getpass in the GNU C library reads from standard input and writes to standard error if the controlling terminal can’t be opened for reading and writing.
The Solaris version fails if it can’t open the controlling terminal.
If we didn’t do this, entering the INTR character would abort the program and leave the terminal with echoing disabled.
Similarly, entering the SUSP character would stop the program and return to the shell with echoing disabled.
We choose to block the signals while we have echoing disabled.
If they are generated while we’re reading the password, they are held until we return.
Some versions just ignore SIGINT (saving its previous action) while in getpass, resetting the action for this signal to its previous value before returning.
This means that any occurrence of the signal while it’s ignored is lost.
Other versions catch SIGINT (saving its previous action) and if the signal is caught, send themselves the signal with the kill function after resetting the terminal state and signal action.
None of the versions of getpass catch, ignore, or block SIGQUIT, so entering the QUIT character aborts the program and probably leaves the terminal with echoing disabled.
Be aware that some shells, notably the Korn shell, turn echoing back on whenever they read interactive input.
These shells are the ones that provide command-line editing and therefore manipulate the state of the terminal every time we enter an interactive command.
So, if we invoke this program under one of these shells and abort it with the QUIT character, it may reenable echoing for us.
Other shells that don’t provide this form of command-line editing, such as the Bourne shell, will abort the program and leave the terminal in no-echo mode.
If we do this to our terminal, the stty command can reenable echoing.
We use standard I/O to read and write the controlling terminal.
We specifically set the stream to be unbuffered; otherwise, there might be some interactions between the writing and reading of the stream (we would need some calls to fflush)
We could have also used unbuffered I/O (Chapter 3), but we would have to simulate the getc function using read.
We store only up to eight characters as the password.
The program in Figure 18.18 calls getpass and prints what we enter to let us verify that the ERASE and KILL characters work (as they should in canonical mode)
Whenever a program that calls getpass is done with the cleartext password, the program should zero it out in memory, just to be safe.
If the program were to generate a core file that others might be able to read or if some other process were somehow able to read our memory, they might be able to read the cleartext password.
By ‘‘cleartext,’’ we mean the password that we type at the prompt that is printed by getpass.
Most UNIX system programs then modify this cleartext password, turning it into an ‘‘encrypted’’ password.
Noncanonical mode is specified by turning off the ICANON flag in the c_lflag field of the termios structure.
In noncanonical mode, the input data is not assembled into lines.
As we said, understanding canonical mode is easy: the system returns up to one line at a time.
But with noncanonical mode, how does the system know when to return data to us? If it returned one byte at a time, overhead would be excessive.
Recall Figure 3.6, which showed the overhead in reading one byte at a time.
Each time we doubled the amount of data returned, we halved the system call overhead.
The system can’t always return multiple bytes at a time, since sometimes we don’t know how much data to read until we start reading it.
The solution is to tell the system to return when either a specified amount of data has been read or after a given amount of time has passed.
This technique uses two variables in the c_cc array in the termios structure: MIN and TIME.
These two elements of the array are indexed by the names VMIN and VTIME.
If MIN bytes are received before the timer expires, read returns MIN bytes.
If the timer expires before MIN bytes are received, read returns the bytes received.
At least one byte is returned if the timer expires, because the timer is not started until the first byte is received.
In this case, the caller blocks until the first byte is received.
If data is already available when read is called, it is as if the data had been received immediately after the read.
The read does not return until MIN bytes have been received.
Compare this to case A, in which a nonzero TIME represented an interbyte timer that was not started until the first byte was received.
The read returns when a single byte is received or when the timer expires.
If some data is available, read returns up to the number of bytes requested.
Realize in all these cases that MIN is only a minimum.
If the program requests more than MIN bytes of data, it’s possible to receive up to the requested amount.
In this figure, nbytes is the third argument to read (the maximum number of bytes to return)
Be aware that POSIX.1 allows the subscripts VMIN and VTIME to have the same values as VEOF and VEOL, respectively.
Indeed, Solaris does this for backward compatibility with older versions of System V.
In going from noncanonical to canonical mode, we must now restore VEOF and VEOL as well.
If VMIN equals VEOF and we don’t restore their values, when we set VMIN to its typical value of 1, the end-of-file character becomes Control-A.
The easiest way around this problem is to save the entire termios structure when going into noncanonical mode and restore it when going back to canonical mode.
The terms cbreak and raw come from the Version 7 terminal driver.
We can reset the terminal to its original state (the state before either of these functions was called) by calling the function tty_reset.
This improves the chances that the terminal will be left in a usable state if we encounter any errors.
Echo off, canonical mode off, extended input * processing off, signal chars off.
As we mentioned at the beginning of this section, this mode turns off some input character processing.
It does not turn off signal handling, so the user can always type one of the characters that triggers a terminal-generated signal.
Be aware that the caller should catch these signals; otherwise, there’s a chance that the signal will terminate the program, and the terminal will be left in cbreak mode.
As a general rule, whenever we write a program that changes the terminal mode, we should catch most signals.
This allows us to reset the terminal mode before terminating.
A read won’t return until at least one byte is available.
We also turn off processing of the signal-generating characters (ISIG) and the extended input character processing (IEXTEN)
Additionally, we disable a BREAK character from generating a signal, by turning off BRKINT.
We disable the CR-to-NL mapping on input (ICRNL), input parity detection (INPCK), the stripping of the eighth bit on input (ISTRIP), and output flow control (IXON)
The program in Figure 18.21 tests raw and cbreak modes.
Running the program in Figure 18.21, we can see what happens with these two terminal modes:
Most UNIX systems provide a way to keep track of the current terminal window size and to have the kernel notify the foreground process group when the size changes.
The kernel maintains a winsize structure for every terminal and pseudo terminal:
We can fetch the current value of this structure using an ioctl (Section 3.15) of TIOCGWINSZ.
We can store a new value of this structure in the kernel using an ioctl of TIOCSWINSZ.
If this new value differs from the current value stored in the kernel, a SIGWINCH signal is sent to the foreground process group.
Note from Figure 10.1 that the default action for this signal is to be ignored.
Other than storing the current value of the structure and generating a signal when the value changes, the kernel does nothing else with this structure.
This feature is provided to notify applications (such as the vi editor) when the window size changes.
When it receives the signal, the application can fetch the new size and redraw the screen.
Figure 18.22 shows a program that prints the current window size and goes to sleep.
Each time the window size changes, SIGWINCH is caught and the new size is printed.
Running the program in Figure 18.22 on a windowed terminal gives us.
The termcap scheme was developed at Berkeley to support the vi editor.
The termcap file contains descriptions of various terminals: which features the terminal supports (e.g., how many lines and rows, whether the terminal support backspace) and how to make the terminal perform certain operations (e.g., clear the screen, move the cursor to a given location)
Taking this information out of the compiled program and placing it into a text file that can easily be edited allows the vi editor to run on many different terminals.
The routines that support the termcap file were eventually extracted from the vi editor and placed into a separate curses library.
Many features were added to make this library usable for any program that wanted to manipulate the screen.
As more and more terminals were added to the data file, it took longer to scan the file, looking for a specific terminal.
The data file also used two-character names to identify the various terminal attributes.
These deficiencies led to development of the terminfo scheme and its associated curses library.
The terminal descriptions in terminfo are basically compiled versions of a textual description and can be located faster at runtime.
Historically, System V–based systems used terminfo, and BSD-derived systems used termcap, but it is now common for systems to provide both.
A description of terminfo and the curses library is provided by Goodheart [1991], but this book is currently out of print.
Strang [1986] describes the Berkeley version of the curses library.
The ncurses library, a free version that is compatible with the SVR4 curses interface, can be found at http://invisible-island.net/ncurses/ncurses.html.
Neither termcap nor terminfo, by itself, addresses the problems we’ve been looking at in this chapter: changing the terminal’s mode, changing one of the terminal special characters, handling the window size, and so on.
On the other hand, curses does help with some of the details that we’ve addressed in this chapter.
Functions are provided by curses to set raw mode, set cbreak mode, turn echo on and off, and the like.
Note that the curses library is designed for character-based dumb terminals, which have mostly been replaced by pixel-based graphics terminals today.
Terminals have many features and options, most of which we’re able to change to suit our needs.
In this chapter, we described numerous functions that change a terminal’s operation — namely, special input characters and the option flags.
We also looked at all the terminal special characters and the many options that can be set or reset for a terminal device.
There are two modes of terminal input—canonical (line at a time) and noncanonical.
We showed examples of both modes and provided functions that map between the POSIX.1 terminal options and the older BSD cbreak and raw modes.
We also described how to fetch and change the window size of a terminal.
If your system provides the reset(1) command (all four systems described in this text do), use it to restore the terminal mode.
Log in to the system twice and start the vi editor from one login.
Use the stty command from your other login to determine which values vi sets MIN and TIME to (since vi sets the terminal to noncanonical mode)
If you are running a windowing system on your terminal, you can do this same test by logging in once and using two separate windows instead.
In Chapter 9, we saw that terminal logins come in through a terminal device, automatically providing terminal semantics.
A terminal line discipline (Figure 18.2) exists between the terminal and the programs that we run, so we can set the terminal’s special characters (e.g., backspace, line erase, interrupt) and the like.
When a login arrives on a network connection, however, a terminal line discipline is not automatically provided between the incoming network connection and the login shell.
Figure 9.5 showed that a pseudo terminal device driver is used to provide terminal semantics.
In addition to network logins, pseudo terminals have other uses that we explore in this chapter.
We start with an overview on how to use pseudo terminals, followed by a discussion of specific use cases.
Next we provide functions to create pseudo terminals on various platforms, and then we use these functions to write a program that we call pty.
The term pseudo terminal implies that it looks like a terminal to an application program, but it’s not a real terminal.
Figure 19.1 shows the typical arrangement of the processes involved when a pseudo terminal is being used.
Figure 19.1 Typical arrangement of processes using a pseudo terminal.
Normally, a process opens the pseudo terminal master and then calls fork.
The child establishes a new session, opens the corresponding pseudo terminal slave, duplicates the file descriptor to the standard input, standard output, and standard error, and then calls exec.
The pseudo terminal slave becomes the controlling terminal for the child process.
It appears to the user process above the slave that its standard input, standard output, and standard error are a terminal device.
The process can issue all the terminal I/O functions from Chapter 18 on these descriptors.
But since the slave isn’t a real terminal device, functions that don’t make sense (e.g., change the baud rate, send a break character, set odd parity) are just ignored.
Anything written to the master appears as input to the slave, and vice versa.
Indeed, all the input to the slave comes from the user process above the pseudo terminal master.
This behaves like a bidirectional pipe, but with the terminal line discipline module above the slave, we have additional capabilities over a plain pipe.
Figure 19.1 shows what a pseudo terminal looks like on a FreeBSD, Mac OS X, or Linux system.
In Section 19.3, we show how to open these devices.
Under Solaris, a pseudo terminal is built using the STREAMS subsystem.
Figure 19.2 details the arrangement of the pseudo terminal STREAMS modules under Solaris.
The two STREAMS modules that are shown as dashed boxes are optional.
The pckt and ptem modules help provide semantics specific to pseudo terminals.
The other two modules (ldterm and ttcompat) provide line discipline processing.
In Section 19.3, we show how to build this arrangement of STREAMS modules.
We’ll now examine some of the typical uses of pseudo terminals.
Network Login Servers Pseudo terminals are built into servers that provide network logins.
Once the login shell is running on the remote host, we have the arrangement shown in Figure 19.3
We show two calls to exec between the rlogind server and the login shell, because the login program is usually between the two to validate the user.
A key point in Figure 19.3 is that the process driving the PTY master is normally reading and writing another I/O stream at the same time.
In this example, the other I/O stream is the TCP/IP box.
This implies that the process must be using some form of I/O multiplexing (Section 14.4), such as select or poll, or must be divided into two processes or threads.
Windowing systems typically provide a terminal emulator so that we can use a shell to run our programs from a familiar command-line environment.
The terminal emulator acts as an intermediary between a shell and the window manager.
This arrangement (with two shells running in different windows) is shown in Figure 19.4
The shell runs with its standard input, standard output, and standard error attached to the slave side of the PTY.
The terminal emulator program opens the master side of the PTY.
Besides acting as an interface to the windowing subsystem, the terminal emulator is responsible for emulating a particular type of terminal, which means it needs to respond to the escape codes associated with the type of device it is emulating.
These codes are listed in the termcap and terminfo databases.
When a user resizes a terminal emulator window, the window manager informs the terminal emulator.
The terminal emulator issues a TIOCSWINSZ ioctl command on the master side of the PTY to set the window size for the slave side.
If the new size differs from the current size, the kernel sends a SIGWINCH signal to the foreground.
If the application needs to redraw the screen when the window is resized, it can catch the SIGWINCH signal, issue the TIOCGWINSZ ioctl command to get the new screen dimensions, and redraw the screen.
The script(1) program that is supplied with most UNIX systems makes a copy in a file of everything that is input and output during a terminal session.
The program does this by placing itself between the terminal and a new invocation of our login shell.
Figure 19.5 details the interactions involved in the script program.
Here, we specifically show that the script program is normally run from a login shell, which then waits for script to terminate.
While script is running, everything output by the terminal line discipline above the PTY slave is copied to the script file (usually called typescript)
Since our keystrokes are normally echoed by that line discipline module, the script file also contains our input.
The script file won’t contain any passwords that we enter, however, since passwords aren’t echoed.
While writing the first edition of this book, Rich Stevens used the script program to capture the output of the example programs.
This avoided typographical errors that could have occurred if he had copied the program output by hand.
The drawback to using script, however, is having to deal with control characters that are present in the script file.
After developing the general pty program in Section 19.5, we’ll see that a trivial shell script turns it into a version of the script program.
Pseudo terminals can be used to drive interactive programs in noninteractive modes.
Numerous programs are hard-wired to require a terminal to run.
One example is the passwd(1) command, which requires that the user enter a password in response to a prompt.
Rather than modify all the interactive programs to support a batch mode of operation, a better solution is to provide a way to drive any interactive program from a script.
It uses pseudo terminals to run other programs, similar to the pty program in Section 19.5
But expect also provides a programming language to examine the output of the program being run to make decisions about what to send the program as input.
When an interactive program is being run from a script, we can’t just copy everything from the script to the program, and vice versa.
Instead, we have to send the program some input, look at its output, and decide what to send it next.
In the coprocess example in Figure 15.19, we couldn’t invoke a coprocess that used the standard I/O library for its input and output, because when we talked to the coprocess across a pipe, the standard I/O library fully buffered the standard input and standard output, leading to a deadlock.
If the coprocess is a compiled program for which we don’t have the source code, we can’t add fflush statements to solve this problem.
What we need to do is place a pseudo terminal between the two processes, as shown in Figure 19.6, to trick the coprocess into thinking that it is being driven from a terminal instead of from another process.
Now the standard input and standard output of the coprocess look like a terminal device, so the standard I/O library will set these two streams to be line buffered.
The parent can obtain a pseudo terminal between itself and the coprocess in two ways.
The parent in this case could be the program in Figure 15.18, which used two pipes to communicate with the coprocess.
Another is to exec the pty program (Section 19.5) with the coprocess as its argument.
We’ll look at these two solutions after showing the pty program.
If we have a program that runs for a long time, we can easily run it in the background using any of the standard shells.
Unfortunately, if we redirect its standard output to a file, and if it doesn’t generate much output, we can’t easily monitor its progress, because the standard I/O library will fully buffer its standard output.
All that we’ll see are blocks of output written by the standard I/O library to the output file, possibly in chunks as large as 8,192 bytes.
If we have the source code, we can insert calls to fflush to force the standard I/O buffers to be flushed at select points or change the buffering mode to line buffered using setvbuf.
If we don’t have the source code, however, we can run the program under the pty program, making its standard I/O library think that its standard output is a terminal.
Figure 19.7 shows this arrangement, where we have called the slow output program slowout.
The fork/exec arrow from the login shell to the pty process is shown as a dashed arrow to emphasize that the pty process is running as a background job.
Figure 19.7 Running a slow output program using a pseudo terminal.
PTYs act like physical terminal devices so that applications are unaware of which type of device they are using.
However, applications don’t need to set the O_TTY_INIT flag when opening PTY device files.
The Single UNIX Specification already requires that implementations initialize the slave side of a PTY device when it is first opened so that any nonstandard termios flags needed for the device to operate as expected are set.
This requirement is intended to allow the PTY device to operate properly with POSIXconforming applications that call tcgetattr and tcsetattr.
The way we open a PTY device differs among platforms.
The Single UNIX Specification includes several functions as part of the XSI option in an attempt to unify the methods.
The posix_openpt function is provided as a portable way to open an available PTY master device.
The oflag argument is a bitmask that specifies how the master device is to be opened, similar to the same argument used with open(2)
Before a slave pseudo terminal device can be used, its permissions need to be set so that it is accessible to applications.
It sets the user ID of the slave’s device node to the caller’s real user ID and sets the node’s group ID to an unspecified value, usually some group that has access to terminal devices.
The permissions are set to allow read and write access to individual owners and write access to group owners (0620)
Implementations commonly set the group ownership of the slave PTY device to group tty.
Programs that need permission to write to all active terminals on the system are set-group-ID to the group tty.
Because the group write permission is enabled on slave PTY devices, these programs can write to them.
To change permission on the slave device node, grantpt might need to fork and exec a set-user-ID program (/usr/lib/pt_chmod on Solaris, for example)
Thus, the behavior is unspecified if the caller is catching SIGCHLD.
The unlockpt function is used to grant access to the slave pseudo terminal device, thereby allowing applications to open the device.
By preventing others from opening the slave device, applications setting up the devices have an opportunity to initialize the slave and master devices properly before they can be used.
Note that in both grantpt and unlockpt, the file descriptor argument is the file descriptor associated with the master pseudo terminal device.
The ptsname function is used to find the pathname of the slave pseudo terminal device, given the file descriptor of the master.
This allows applications to identify the slave independent of any particular conventions that might be followed by a given platform.
Note that the name returned might be stored in static memory, so it can be overwritten on successive calls.
Returns: pointer to name of PTY slave if OK, NULL on error.
Figure 19.8 summarizes the pseudo terminal functions in the Single UNIX Specification and indicates which functions are supported by the platforms discussed in this text.
On FreeBSD, grantpt and unlockpt do nothing other than argument validation; the PTYs are created dynamically with the correct permissions.
FreeBSD does not allocate a controlling terminal as a side effect of opening a terminal device, so the O_NOCTTY flag has no effect.
The Single UNIX Specification has improved portability in this area, but differences remain.
The caller must allocate an array to hold the name of the slave; if the call succeeds, the name of the corresponding slave is returned through pts_name.
This name is then passed to ptys_open, which opens the slave device.
The reason for providing two functions to open the two devices will become obvious when we show the pty_fork function.
Normally, a process calls ptym_open to open the master and obtain the name of the slave.
The process then forks, and the child calls ptys_open to open the slave after calling setsid to establish a new session.
This is how the slave becomes the controlling terminal for the child.
The ptym_open function uses the XSI PTY functions to find and open an unused PTY master device and initialize the corresponding PTY slave device.
On a Solaris system, however, we might need to take additional steps before the slave PTY device will behave like a terminal.
On Solaris, after opening the slave device, we might need to push three STREAMS modules onto the slave’s stream.
Together, the pseudo terminal emulation module (ptem) and the terminal line discipline module (ldterm) act like a real terminal.
It’s an optional module, but since it’s automatically pushed for network logins, we push it onto the slave’s stream.
The reason why we might not need to push these three modules is that they might be there already.
The STREAMS system supports a facility known as autopush, which allows an administrator to configure a list of modules to be pushed onto a stream whenever a particular device is opened (see Rago [1993] for more details)
We use the I_FIND ioctl command to see whether ldterm is already present on the stream.
If so, we assume that the stream has been configured by the autopush mechanism and avoid pushing the modules a second time.
Linux, Mac OS X, and Solaris follow the historical System V behavior: if the caller is a session leader that does not already have a controlling terminal, the call to open allocates the PTY slave as the controlling terminal.
If we didn’t want this to happen, we could specify the O_NOCTTY flag for open.
However, on FreeBSD, the open of the slave PTY does not have the side effect of allocating the device as the controlling terminal.
In the next section, we’ll see how to allocate the controlling terminal when running on FreeBSD.
This new function combines the opening of the master and the slave with a call to fork, establishing the child as a session leader with a controlling terminal.
The file descriptor of the PTY master is returned through the ptrfdm pointer.
If slave_name is non-null, the name of the slave device is stored at that location.
If the pointer slave_termios is non-null, the system uses the referenced structure to.
Similarly, if the slave_winsize pointer is non-null, the referenced structure initializes the slave’s window size.
As we mentioned before, we want to wait to call ptys_open until in the child and after calling setsid to establish a new session.
When it calls setsid, the child is not a process group leader, so the three steps listed in Section 9.5 occur: (a) a new session is created with the child as the session leader, (b) a new process group is created for the child, and (c) the child loses any association it might have had with its previous controlling terminal.
Under Linux, Mac OS X, and Solaris, the slave becomes the controlling terminal of this new session when ptys_open is called.
Under FreeBSD, we have to use the TIOCSCTTY ioctl command to allocate the controlling terminal.
Recall Figure 9.8 — the other three platforms also support TIOCSCTTY, but we need to call it only on FreeBSD.
The two structures termios and winsize are then initialized in the child.
Finally, the slave file descriptor is duplicated onto standard input, standard output, and standard error in the child.
This means that whatever process the caller execs from the child will have these three descriptors connected to the slave PTY (its controlling terminal)
After the call to fork, the parent just returns the PTY master descriptor and the process ID of the child.
In the next section, we use the pty_fork function in the pty program.
The goal in writing the pty program is to be able to type.
When we use pty to execute another program, that program is executed in a session of its own, connected to a pseudo terminal.
Let’s look at the source code for the pty program.
In the next section, we’ll look at the various command-line options when we examine different uses of the pty program.
The getopt function helps us parse command-line arguments in a consistent manner.
To enforce POSIX behavior on Linux systems, we set the first character of the option string to a plus sign.
This way, the PTY slave assumes the same initial state as the current terminal.
After returning from pty_fork, the child optionally turns off echoing for the slave PTY and then calls execvp to execute the program specified on the command line.
All remaining command-line arguments are passed as arguments to this program.
The parent optionally sets the user’s terminal to raw mode.
In this case, the parent also sets an exit handler to reset the terminal state when exit is called.
The parent then calls the function loop (Figure 19.12), which copies everything received from the standard input to the PTY master and everything from the PTY master to standard output.
For variety, we have coded it in two processes this time, although a single process using select, poll, or multiple threads would also work.
The child sends us SIGTERM when it gets EOF on the pty slave or * when read() fails.
Note that because we use two processes, one has to notify the other when it terminates.
We’ll now look at various examples with the pty program, seeing the need for the command-line options.
If our shell is the Korn shell, we can execute the command.
If the file ttyname is the program we showed in Figure 18.16, we can run the pty.
In Section 6.8, we described the utmp file that records all users currently logged in to a UNIX system.
The question is whether a user running a program on a pseudo terminal is considered logged in.
In the case of remote logins, with telnetd and rlogind, obviously an entry should be made in the utmp file for the user logged in on the pseudo terminal.
There is little agreement, however, whether users running a shell on a pseudo terminal from a window system or from a program, such as script, should have entries made in the utmp file.
If a system doesn’t record these entries in the utmp file, the who(1) program normally won’t show the corresponding pseudo terminals as being used.
Unless the utmp file has other-write permission enabled (which is considered to be a security hole), random programs that use pseudo terminals won’t be able to write to this file.
If we run a job-control shell under pty, it works normally.
We can run programs under this new shell and use job control just as we do with our login shell.
But if we run an interactive program other than a job-control shell under pty, as in.
When we type the suspend character (Control-Z), it is recognized by the line discipline module beneath the cat process, since pty puts the terminal (beneath the pty parent) into raw mode.
But the kernel won’t stop the cat process, because it.
The parent of cat is the pty parent, and it belongs to another session.
POSIX.1 says only that the SIGTSTP signal can’t be delivered to the process.
Systems derived from 4.3BSD delivered SIGKILL instead, which the process can’t even catch.
Instead of sending SIGKILL, the 4.4BSD kernel silently discards the SIGTSTP signal if it has the default disposition and is to be delivered to a process in an orphaned process group.
When we use pty to run a job-control shell, the jobs invoked by this new shell are never members of an orphaned process group, because the job-control shell always belongs to the same session.
In that case, the Control-Z that we type is sent to the process invoked by the shell, not to the shell itself.
The only way to avoid this inability of the process invoked by pty to handle job-control signals is to add yet another command-line flag to pty, telling it to recognize the job control suspend character itself (in the pty child) instead of letting the character get all the way through to the other line discipline.
Watching the Output of Long-Running Programs Another example of job control interaction with the pty program is found in the configuration illustrated in Figure 19.7
If we run the program that generates output slowly as.
The reason is that the job is a background job and gets job-control stopped when it tries to access the terminal.
If we redirect standard input so that pty doesn’t try to read from the terminal, as in.
The solution for this problem is the -i option, which says to ignore an end of file on the standard input:
This flag causes the pty child in Figure 19.12 to exit when the end of file is encountered, but the child doesn’t tell the parent to terminate.
Instead, the parent continues copying the PTY slave output to standard output (the file file.out in the example)
Once we run this shell script, we can execute the ps command to see all the process relationships.
In this example, we assume that the SHELL variable is the Korn shell (probably /bin/ksh)
As we mentioned earlier, script copies only what is output by the new shell (and any processes that it invokes), but since the line discipline module above the PTY slave normally has echo enabled, most of what we type is also written to the typescript file.
In Figure 15.18, the coprocess couldn’t use the standard I/O functions, because standard input and standard output do not refer to a terminal, so the standard I/O functions treat them as fully buffered.
If we run the coprocess under pty by replacing the line.
Figure 19.15 shows the arrangement of processes when we run the coprocess with a.
It is an expansion of Figure 19.6, showing all the process connections and data flow.
Figure 19.15 Running a coprocess with a pseudo terminal as its input and output.
This example shows the need for the -e (no echo) option for the pty program.
The pty program is not running interactively, because its standard input is not connected to a terminal.
In Figure 19.11, the interactive flag defaults to false, since the call to.
This means that the line discipline above the actual terminal remains in canonical mode with echo enabled.
By specifying the -e option, we turn off echo in the line discipline module above the PTY slave.
If we don’t do this, everything we type is echoed twice—by both line discipline modules.
We also have the -e option turn off the ONLCR flag in the termios structure to prevent all the output from the coprocess from being terminated with a carriage return and a newline.
Testing this example on different systems revealed another problem that we alluded to in Section 14.7 when we described the readn and writen functions.
The amount of data returned by a read, when the descriptor refers to something other than an ordinary disk file, can differ between implementations.
This coprocess example using pty gave unexpected results that were tracked down to the read function on the pipe in the program from Figure 15.18, which returned less than a line.
With this approach, the fgets function does as many reads as required to obtain a complete line.
The while loop in Figure 15.18 assumes that each line sent to the coprocess causes one line to be returned.
Driving Interactive Programs Noninteractively Although it’s tempting to think that pty can run any coprocess, even a coprocess that is interactive, it doesn’t work.
The problem is that pty just copies everything on its standard input to the PTY and everything from the PTY to its standard output, never looking at what it sends or what it gets back.
As an example, we can run the telnet command under pty, talking directly to the remote host:
Doing this provides no benefit over just typing telnet 192.168.1.3, but we would like to run the telnet program from a script, perhaps to check some condition on the remote host.
Instead, the contents of the file telnet.cmd are sent to the remote host before it has a chance to prompt us for an account name and password.
When it turns off echoing to read the password, login uses the tcsetattr option, which discards any data already queued.
When we run the telnet program interactively, we wait for the remote host to prompt for a password before we type it, but the pty program doesn’t know to do this.
This is why it takes a more sophisticated program than pty, such as expect, to drive an interactive program from a script file.
With an interactive program, one line of input may generate many lines of output.
Furthermore, the program in Figure 15.18 always sent a line to the coprocess before reading from it.
This strategy won’t work when we want to read from the coprocess before sending it anything.
There are a few ways to proceed from here to be able to drive an interactive program from a script.
We could add a command language and interpreter to pty, but a reasonable command language would probably be ten times larger than the pty program.
Another option is to take a command language and use the pty_fork function to invoke interactive programs.
We’ll take a different path here and just provide an option (-d) to allow pty to be connected to a driver process for its input and output.
The standard output of the driver is pty’s standard input, and vice versa.
This is similar to a coprocess, but on ‘‘the other side’’ of pty.
The resulting arrangement of processes is almost identical to Figure 19.15, but in the current scenario, pty does the fork and exec of the driver process.
Also, instead of two half-duplex pipes, we’ll use a single bidirectional pipe between pty and the driver process.
Parent returns, but with stdin and stdout connected * to the driver.
By writing our own driver program that is invoked by pty, we can drive interactive programs in any way desired.
Even though it has its standard input and standard output connected to pty, the driver process can still interact with the user by reading and writing /dev/tty.
This solution still isn’t as general as the expect program, but it provides a useful option to pty for fewer than 50 lines of code.
Pseudo terminals have some additional capabilities that we briefly mention here.
Packet Mode Packet mode lets the PTY master learn of state changes in the PTY slave.
On Solaris, this mode is enabled by pushing the STREAMS module pckt onto the PTY master side.
The details of packet mode differ between Solaris and the other platforms.
Under Solaris, the process reading the PTY master has to call getmsg to fetch the messages from the stream head, because the pckt module converts certain events into nondata STREAMS messages.
With the other platforms, each read from the PTY master returns a status byte followed by optional data.
Regardless of the implementation details, the purpose of packet mode is to inform the process reading the PTY master when the following events occur at the line discipline module above the PTY slave: when the read queue is flushed, when the write queue is flushed, when output is stopped (e.g., Control-S), when output is restarted, when XON/XOFF flow control is enabled after being disabled, and when XON/XOFF flow control is disabled after being enabled.
These events are used, for example, by the rlogin client and the rlogind server.
Remote Mode A PTY master can set the PTY slave to remote mode by issuing the TIOCREMOTE ioctl command.
Remote mode is intended for an application, such as a window manager, that does its own line editing.
Window Siz e Chang es The process above the PTY master can issue the TIOCSWINSZ ioctl command to set the window size of the slave.
If the new size differs from the current size, a SIGWINCH signal is sent to the foreground process group of the PTY slave.
Signal Generation The process reading and writing the PTY master can send signals to the process group of the PTY slave.
Under Solaris 10, this is done using the TIOCSIGNAL ioctl command.
In both cases, the third argument is set to the signal number.
We started this chapter with an overview of how to use pseudo terminals and a look at some use cases.
We continued by examining the code required to set up a pseudo terminal under the four platforms discussed in this text.
We then used this code to provide the generic pty_fork function that can be used by many different applications.
We used this function as the basis for a small program (pty), which we then used to explore many of the properties of pseudo terminals.
Pseudo terminals are used daily on most UNIX systems to provide network logins.
We’ve examined other uses for pseudo terminals as well, ranging from the script program to driving interactive programs from a batch script.
Can you change standard input to be read-only and the other two to be write-only?
Add these features to the simple shell script that we showed.
The new program that the child execs must catch SIGTERM and SIGWINCH.
When it catches a signal, the program should print that it did; for the latter signal, it should also print the terminal’s window size.
Then have the parent process send the SIGTERM signal to the process group of the PTY slave with the ioctl command we described in Section 19.7
Read back from the slave to verify that the signal was caught.
Follow this with the parent setting the window size of the PTY slave, and then read back the slave’s output again.
Have the parent exit and determine whether the slave process also terminates; if so, how does it terminate?
During the early 1980s, the UNIX System was considered a hostile environment for running multiuser database systems.
Earlier systems, such as Version 7, did indeed present large obstacles, since they did not provide any form of IPC (other than half-duplex pipes) and did not provide any form of byte-range locking.
By the late 1980s, the UNIX System had evolved to provide a suitable environment for running reliable, multiuser database systems.
Since then, numerous commercial firms have offered these types of database systems.
In this chapter, we develop a simple, multiuser database library of C functions that any program can call to fetch and store records in a database.
This library of C functions is usually only one part of a complete database system.
We do not develop the other pieces, such as a query language, leaving these items to the many textbooks on database systems.
Our interest is the parts of the UNIX System interface required by a database library, and how they relate to the topics we’ve already covered (such as record—byte-range — locking, in Section 14.3)
One popular library of database functions in the UNIX System is the dbm(3) library.
This library was developed by Ken Thompson and uses a dynamic hashing scheme.
It was originally provided with Version 7, appears in all BSD releases, and was also.
The BSD developers extended the dbm library and called it ndbm.
The ndbm library was included in BSD as well as in SVR4
The ndbm functions are standardized in the XSI option of the Single UNIX Specification.
Seltzer and Yigit [1991] provide a detailed history of the dynamic hashing algorithm used by the dbm library and other implementations of this library, including gdbm, the GNU version of the dbm library.
Unfortunately, a basic limitation of all these implementations is that none allows concurrent updating of the database by multiple processes.
These implementations provide no type of concurrency controls (such as record locking)
Again, no form of concurrency was provided (as was plainly stated in the BUGS section of the db(3) manual page)
Oracle (http://www.oracle.com) provides versions of the db library that do support concurrent access, locking, and transactions.
Most commercial database libraries do provide the concurrency controls required for multiple processes to update a database simultaneously.
These systems typically use advisory locking, as we described in Section 14.3, but they often implement their own locking primitives to avoid the overhead of a system call to acquire an uncontested lock.
Figure 20.1 summarizes the database libraries commonly found in the four operating systems described in this book.
Note that on Linux, the gdbm library provides support for both dbm and ndbm functions.
The library we develop in this chapter will be similar to the ndbm library, but we’ll add the concurrency control mechanisms to allow multiple processes to update the same database at the same time.
We first describe the C interface to the database library, then in the next section describe the actual implementation.
When we open a database, we are returned a handle (an opaque pointer) representing the database.
If db_open is successful, two files are created: pathname.idx is the index file, and pathname.dat is the data file.
The oflag argument is used as the second argument to open (Section 3.3) to specify how the files are to be opened (e.g., read-only, read–write, create file if it doesn’t exist)
The mode argument is used as the third argument to open (the file access permissions) if the database files are created.
It closes the index file and the data file and releases any memory that it allocated for internal buffers.
When we store a new record in the database, we have to specify the key for the record and the data associated with the key.
If the database contained personnel records, the key could be the employee ID, and the data could be the employee’s name, address, telephone number, date of hire, and the like.
Our implementation requires that the key for each record be unique.
We can’t have two employee records with the same employee ID, for example.
The only restriction on these two strings is that neither can contain null bytes.
We can fetch any record from the database by specifying its key.
Returns: pointer to data if OK, NULL if record not found.
The return value is a pointer to the data that was stored with the key, if the record is found.
We can also delete a record from the database by specifying its key.
In addition to fetching a record by specifying its key, we can go through the entire database, reading each record in turn.
Returns: pointer to data if OK, NULL on end of file.
If key is a non-null pointer, db_nextrec returns the key by copying it to the memory starting at that location.
There is no order to the records returned by db_nextrec.
All we’re guaranteed is that we’ll read each record in the database once.
If we store three records with keys of A, B, and C, in that order, we have no idea in which order db_nextrec will return the three records.
It might return B, then A, then C, or some other (apparently random) order.
The actual order depends on the implementation of the database.
These seven functions provide the interface to the database library.
We now describe the actual implementation that we have chosen.
Database access libraries often use two files to store the information: an index file and a data file.
The index file contains the actual index value (the key) and a pointer to the corresponding data record in the data file.
Numerous techniques can be used to organize the index file so that it can be searched quickly and efficiently for any key: hashing and B+ trees are popular.
We have chosen to use a fixed-size hash table with chaining for the index file.
We mentioned in the description of db_open that we create two files: one with a suffix of .idx and one with a suffix of .dat.
We store the key and the index as null-terminated character strings; they cannot contain arbitrary binary data.
This complicates the functions and requires more work to make the database files portable between different computer systems.
For example, if a network has two systems that use different formats for storing binary integers, we need to account for this difference if we want both systems to access the database.
It is not at all uncommon today to have systems with different architectures sharing files on a network.
Storing all the records, both keys and data, as character strings simplifies everything.
It does require additional disk space, but that is a small cost for portability.
With db_store, only one record for each key is allowed.
Some database systems allow a key to have multiple records and then provide a way to access all the records associated with a given key.
Additionally, we have only a single index file, meaning that each data record can have only a single key (we don’t support secondary keys)
Some database systems allow each record to have multiple keys and often use one index file per key.
Each time a new record is inserted or deleted, all index files must be updated accordingly.
An example of a file with multiple indexes is an employee file.
We could have one index whose key is the employee ID and another whose key is the employee’s Social Security number.
Having an index whose key is the employee name could be a problem, as names are not always unique.
Figure 20.2 shows a general picture of the database implementation.
The index file consists of three portions: the free-list pointer, the hash table, and the index records.
In Figure 20.2, all the fields called ptr are simply file offsets stored as an ASCII number.
To find a record in the database given its key, db_fetch calculates the hash value of the key, which leads to one hash chain in the hash table.
The chain ptr field could be 0, indicating an empty chain.
We then follow this hash chain, which is a linked list of all the index records with this hash value.
When we encounter a chain ptr value of 0, we’ve hit the end of the hash chain.
The program in Figure 20.3 creates a new database and writes three records to it.
Since we store all the fields in the database as ASCII characters, we can look at the actual index file and data file using any of the standard UNIX System tools:
Since each ptr is a file offset, a four-character field limits the total size of the index file and data file to 10,000 bytes.
The next field (10) is the four-character idx len, the length of the remainder of this index record.
We read each index record using two reads: one to read the two fixed-size fields (the chain ptr and idx len) and another to read the remaining (variable-length) portion.
The remaining three fields—key, dat off, and dat len—are delimited by a separator character (a colon in this case)
We need the separator character, since each of these three fields is variable length.
The newline isn’t required, since idx len contains the length of the record.
We store the newline to separate each index record so we can use the normal UNIX System tools, such as cat and more, with the index file.
Figure 20.3 Create a database and write three records to it.
As with the index file, we automatically append a newline to each data record, so we can use the normal UNIX System tools with the file.
This newline at the end is not returned to the caller by db_fetch.
If we follow the three hash chains in this example, we see that the first record on the first hash chain is at offset 53 (gamma)
The next record on this chain is at offset 17 (alpha), and this is the last record on the chain.
The first record on the second hash chain is at offset 35 (beta), and it’s the last record on the chain.
In this case, db_store just appends the new index records and data records to the end of the corresponding file.
We’ll see later that db_store can also reuse portions of these two files that correspond to deleted records.
The choice of a fixed-size hash table for the index is a compromise.
It allows fast access as long as each hash chain isn’t too long.
We want to be able to search for any key quickly, but we don’t want to complicate the data structures by using either a B-tree or dynamic hashing.
Dynamic hashing has the advantage that any data record can be.
Have a single process that is the database manager, and have it be the only process that accesses the database.
The functions contact this central process using some form of IPC.
Have each function apply the required concurrency controls (locking) and then issue its own I/O function calls.
Database systems have been built using each of these techniques.
Given adequate locking routines, the decentralized implementation is usually faster, because IPC is avoided.
We purposely show the IPC going through the kernel, as most forms of message passing under the UNIX System operate this way.
Shared memory, as described in Section 15.9, avoids this copying of the data.
With the centralized approach, a record is read by the central process and then passed to the requesting process using IPC.
Note that the centralized database manager is the only process that does I/O with the database files.
The centralized approach has the advantage that customer tuning of its operation may be possible.
For example, we might be able to assign different priorities to different processes through the centralized process.
This could affect the scheduling of I/O operations by the centralized process.
With the decentralized approach, this is more difficult to do.
We are usually at the mercy of the kernel’s disk I/O scheduling policy and locking policy; that is, if three processes are waiting for a lock to become available, we cannot tell which process gets the lock next.
Another advantage of the centralized approach is that recovery is easier than with the decentralized approach.
All the state information is in one place in the centralized approach, so if the database processes are killed, we have only one place to look to identify the outstanding transactions we need to resolve to restore the database to a consistent state.
This is the design that we’ll implement in this chapter.
The user processes that call the functions in the database library to perform I/O are considered cooperating processes, since they use byte-range locking to provide concurrent access.
We purposely chose a two-file implementation (an index file and a data file) because that is a common implementation technique (it simplifies space management in the files)
It requires us to handle the locking interactions of both files.
But there are numerous ways to handle the locking of these two files.
The simplest form of locking is to use one of the two files as a lock for the entire database and to require the caller to obtain this lock before operating on the database.
For example, we can say that the process with a read lock on byte 0 of the index file has read access to the entire database.
A process with a write lock on byte 0 of the index file has write access to the entire database.
We can use the normal UNIX System byte-range locking semantics to allow any number of readers at one time, but only one writer at a time.
The reason db_open requires a write lock is that if the file is being created, it has to write the empty free list and hash chains at the front of the index file.
The problem with coarse-grained locking is that it limits concurrency.
If a process is adding a record to one hash chain, another process should be able to read a record on a different hash chain.
We enhance coarse-grained locking to allow more concurrency and call this fine-grained locking.
We first require a reader or a writer to obtain a read lock or a write lock on the hash chain for a given record.
We allow any number of readers at one time on any hash chain but only a single writer on a hash chain.
Finally, whenever it appends a new record to the end of either the index file or the data file, db_store has to obtain a write lock on that portion of the file.
We expect fine-grained locking to provide more concurrency than coarse-grained locking.
In Section 20.8, we show the source code for our implementation of fine-grained locking and discuss the details of implementing locking.
Coarse-grained locking is merely a simplification of the locking that we show.
In the source code, we call read, readv, write, and writev directly.
Although it is possible to use byte-range locking with the standard I/O library, careful handling of buffering is required.
Our discussion of concurrency is predicated on the simple needs of the database library.
The database library consists of two files: a public C header file and a C source file.
Applications that want to link with libapue_db.a will also need to link with libapue.a, since we use some of our common functions in the database library.
If, on the other hand, we want to build a dynamic shared library version of the database library, we can use the following commands:
Alternatively, we can place it in a private directory and modify our LD_LIBRARY_PATH environment variable to include the private directory in the search path of the dynamic linker/loader.
The steps used to build shared libraries vary among platforms.
Here, we have shown how to do it on a Linux system with the GNU C compiler.
This header is included by the library source code and all applications that call the library.
For the remainder of this text, we depart from the style of the previous examples in several ways.
First, because the source code example is longer than usual, we number the lines.
This makes it easier to link the discussion with the corresponding source code.
Second, we place the description of the source code immediately below the source code on the same page.
It simplifies the task of studying large amounts of source code.
Note that we do not bother to number blank lines.
Although this departs from the normal behavior of such tools as pr(1), we have nothing interesting to say about blank lines.
We use the _APUE_DB_H symbol to ensure that the contents of the header file are included only once.
The DBHANDLE type represents an active reference to the database and is used to isolate applications from the implementation details of the database.
Compare this technique with the way the standard I/O library exposes the FILE structure to applications.
Next, we declare the prototypes for the database library’s public functions.
Since this header is included by applications that want to use the library, we don’t declare the prototypes for the library’s private functions here.
The legal flags that can be passed to the db_store function are defined next, followed by fundamental limits of the implementation.
These limits can be changed, if desired, to support bigger databases.
Recall the format of an index record from Figure 20.2
An index record will usually be larger than IDXLEN_MIN bytes, but this is the bare minimum size.
The next file is db.c, the C source file for the library.
For simplicity, we include all functions in a single file.
This has the advantage that we can hide private functions by declaring them as static.
We include apue.h because we use some of the functions from our private library.
The size of an index record is specified by IDXLEN_SZ.
We use some characters, such as colon and newline, as delimiters in the database.
We use the space character as ‘‘white out’’ when we delete a record.
Some of the values that we have defined as constants could also be made variable, with some added complexity in the implementation.
For example, we set the size of the hash table to 137 entries.
A better technique would be to let the caller specify this as an argument to db_open, based on the expected size of the database.
We would then have to store this size at the beginning of the index file.
The DB structure is where we keep all the information for each open database.
The DBHANDLE value that is returned by db_open and used by all the other functions is really just a pointer to one of these structures, but we hide that from the callers.
Since we store pointers and lengths as ASCII in the database, we convert these to numeric values and save them in the DB structure.
We also save the hash table size even though it is fixed, just in case we decide to enhance the library to allow callers to specify the size when the database is created (see Exercise 20.7)
The last ten fields in the DB structure count both successful and unsuccessful operations.
If we want to analyze the performance of our database, we can write a function to return these statistics, but for now we only maintain the counters.
The public functions were declared in the library’s header file, apue_db.h.
We declare the internal functions as static so they are visible only to functions residing in the same file (the file containing the library implementation)
If the caller wants to create the database files, the optional third argument specifies the file permissions.
The db_open function opens the index file and the data file, initializing the index file, if necessary.
The function starts by calling _db_alloc to allocate and initialize a DB structure.
The pathname passed in by the caller specifies the prefix of the database filenames.
We append the suffix .idx to create the name for the database index file.
If the caller wants to create the database files, we use the variable argument functions from <stdarg.h> to find the optional third argument.
Then we use open to create and open the index file and data file.
Note that the filename of the data file starts with the same prefix as the index file but has .dat as a suffix instead.
If the caller doesn’t specify the O_CREAT flag, then we’re opening existing database files.
In this case, we simply call open with two arguments.
If an error occurs while we are opening or creating either database file, we call _db_free to clean up the DB structure and then return NULL to the caller.
If one open succeeded and one failed, _db_free will take care of closing the open file descriptor, as we shall see shortly.
Consider two processes trying to create the same database at about the same time.
Assume that the first process calls fstat and is blocked by the kernel after fstat returns.
The second process then writes one record to the database.
At this point, the second process is blocked, and the first process continues executing right after the call to fstat.
The first process finds the size of the index file to be 0 (since fstat was called before the second process initialized the index file), so the first process initializes the free list and hash chain, wiping out the record that the second process stored in the database.
If the size of the index file is 0, we have just created it, so we need to initialize the free list and hash chain pointers it contains.
Note that we use the format string %*d to convert a database pointer from an integer to an ASCII string.
This has the effect of forcing the string created to be at least PTR_SZ characters (padded on the left with spaces)
We build the hash table and write it to the index file.
Then we unlock the index file, reset the database file pointers, and return a pointer to the DB structure as the opaque handle for the caller to use with the other database functions.
We allocate space to hold the name of the database file.
We use this buffer to create both filenames by changing the suffix to refer to either the index file or the data file, as we saw in db_open.
We allocate space for buffers for the index and data files.
An enhancement to the database library would be to allow these buffers to expand as required.
We could keep track of the size of these two buffers and call realloc whenever we find we need a bigger buffer.
Finally, we return a pointer to the DB structure that we allocated.
We can safely pass a null pointer to free, so we don’t need to check the value of each buffer pointer beforehand, but we do so anyway because we consider it better style to free only those objects that we allocated.
The db_fetch function is used to read a record given its key.
We first try to find the record by calling _db_find_and_lock.
If the record can’t be found, we set the return value (ptr) to NULL and increment the count of unsuccessful record searches.
Because _db_find_and_lock returns with the database index file locked, we can’t return until we unlock it.
If the record is found, we call _db_readdat to read the corresponding data record and increment the count of the successful record searches.
Before returning, we unlock the index file by calling un_lock.
Then we return a pointer to the record found (or NULL if the record wasn’t found)
The _db_find_and_lock function is used internally by the library to find a record given its key.
We set the writelock parameter to a nonzero value if we want to acquire a write lock on the index file while we search for the record.
If we set writelock to zero, we read lock the index file while we search it.
We convert the key into a hash value, which we use to calculate the starting address of the hash chain in the file (chainoff)
We wait for the lock to be granted before going through the hash chain.
Note that we lock only the first byte in the start of the hash chain.
This increases concurrency by allowing multiple processes to search different hash chains at the same time.
We call _db_readptr to read the first pointer in the hash chain.
In the while loop, we go through each index record on the hash chain, comparing keys.
It populates the idxbuf field with the key of the current record.
If _db_readidx returns zero, we’ve reached the last entry in the chain.
It multiplies each ASCII character times its 1-based index and divides the result by the number of hash table entries.
The remainder from the division is the hash value for this key.
Recall that the number of hash table entries is 137, which is a prime number.
According to Knuth [1998], prime hashes generally provide good distribution characteristics.
We convert the pointer from ASCII to a long integer before returning it.
No locking is done by this function; that is up to the caller.
The _db_readidx function is used to read the record at the specified offset from the index file.
On success, the function will return the offset of the next record in the list.
In this case, the function will populate several fields in the DB structure: idxoff contains the offset of the current record in the index file, ptrval contains the offset of the next index entry in the list, idxlen contains the length of the current index record, idxbuf contains the actual index record, datoff contains the offset of the record in the data file, and datlen contains the length of the data record.
We start by seeking to the index file offset provided by the caller.
We record the offset in the DB structure, so even if the caller wants to read the record at the current file offset (by setting offset to 0), we still need to call lseek to determine the current offset.
We call readv to read the two fixed-length fields at the beginning of the index record: the chain pointer to the next index record and the size of the variable-length index record that follows.
We convert the offset of the next record to an integer and store it in the ptrval field (this will be used as the return value for this function)
Then we convert the length of the index record into an integer and save it in the idxlen field.
We read the variable-length index record into the idxbuf field in the DB structure.
The record should be terminated with a newline, which we replace with a null byte.
If the index file is corrupt, we terminate and drop core by calling err_dump.
We separate the index record into three fields: the key, the offset of the corresponding data record, and the length of the data record.
The strchr function finds the first occurrence of the specified character in the given string.
Here we look for the character that separates fields in the record (SEP, which we define to be a colon)
We convert the data record offset and length into integers and store them in the DB structure.
Then we return the offset of the next record in the hash chain.
Note that we do not read the data record; that task is left to the caller.
The _db_readdat function populates the datbuf field in the DB structure with the contents of the data record, expecting that the datoff and datlen fields will have been properly initialized already.
The db_delete function is used to delete a record given its key.
We use _db_find_and_lock to determine whether the record exists in the database.
If it does, we call _db_dodelete to do the work needed to delete the record.
The third argument to _db_find_and_lock controls whether the chain is read locked or write locked.
Here we are requesting a write lock, since we will potentially change the list.
Since _db_find_and_lock returns with the lock still held, we need to unlock it, regardless of whether the record was found.
The _db_dodelete function does all the work necessary to delete a record from the database.
Most of the function just updates two linked lists: the free list and the hash chain for this key.
When a record is deleted, we set its key and data record to blanks.
This fact is used by db_nextrec, which we’ll examine later in this section.
This step prevents two processes that are deleting records at the same time, on two different hash chains, from interfering with each other.
Since we’ll add the deleted record to the free list, which changes the free-list pointer, only one process at a time can be doing this.
Note that there is no need for _db_writedat to lock the data file in this case.
Since db_delete has write locked the hash chain for this record, we know that no other process is reading or writing this particular data record.
We read the free-list pointer and then update the index record so that its next record pointer is set to the first record on the free list.
Then we update the free-list pointer with the offset of the index record we are deleting.
This means that the free list is handled on a last-in, first-out basis; that is, deleted records are added to the front of the free list (although we remove entries from the free list on a first-fit basis)
We don’t have a separate free list for each file.
When we add a deleted index record to the free list, the index record still points to the deleted data record.
There are better ways to do this, in exchange for added complexity.
We update the previous record in the hash chain to point to the record after the one we are deleting, thus removing the deleted record from the hash chain.
Thus, no other process could be reading or writing this particular data record.
We seek to the location where we want to write the data record.
The amount to write is the record size plus 1 byte for the terminating newline we add.
We set up the iovec array and call writev to write the data record and newline.
We can’t assume that the caller’s buffer has room at the end for us to append the newline, so we write the newline from a separate buffer.
If we are appending a record to the file, we release the lock we acquired earlier.
The _db_writeidx function is called to write an index record.
After validating the next pointer in the chain, we create the index record and store the second half of it in idxbuf.
We need the size of this portion of the index record to create the first half of the index record, which we store in the local variable asciiptrlen.
Note that we use casts to force the size of the arguments in the sprintf statements to match the format specifications.
As with _db_writedat, this function deals with locking only when a new index record is being appended to the index file.
When _db_dodelete calls this function, we’re rewriting an existing index record.
In this case, the caller has write locked the hash chain, so no additional locking is required.
We seek to the location where we want to write the index record and save this offset in the idxoff field of the DB structure.
Since we built the index record in two separate buffers, we use writev to store it in the index file.
If we were appending to the file, we release the lock we acquired before seeking.
This makes the seek and the write an atomic operation from the perspective of concurrently running processes appending new records to the same database.
We validate that the chain pointer is within bounds, then convert it to an ASCII string.
We seek to the specified offset in the index file and write the pointer.
We use db_store to add a record to the database.
Then we make sure that the length of the data record is valid.
This is OK for an example, but if we were building a production-quality library, we’d return an error status instead, which would give the application a chance to recover.
We call _db_find_and_lock to see if the record already exists.
Replacing an existing record implies that the keys are identical but that the data records probably differ.
Note that the final argument to _db_find_and_lock specifies that the hash chain must be write locked, as we will probably be modifying this hash chain.
After we call _db_find_and_lock, the code divides into four cases.
In the first two, no record was found, so we are adding a new record.
We read the offset of the first entry on the hash list.
If no such record is found, we have to append the new record to the ends of the index and data files.
We increment a count (cnt_stor1) of the number of times we executed this case to allow us to characterize the behavior of the database.
The cnt_stor2 field counts how many times we’ve executed this case.
Now we reach the two cases in which a record with the same key already exists in the database.
If the caller isn’t replacing the record, we set the return code to indicate that a record exists, increment the count of the number of store errors, and jump to the end of the function, where we handle the common return logic.
Case 3: an existing record is being replaced, and the length of the new data record differs from the length of the existing one.
Recall that this places the deleted record at the head of the free list.
We could try to find a deleted record that has the correct data size.
The new record is added to the front of the hash chain by calling _db_writeptr.
The cnt_stor3 counter in the DB structure records the number of times we’ve executed this case.
Case 4: An existing record is being replaced, and the length of the new data record equals the length of the existing data record.
This is the easiest case; we simply rewrite the data record and increment the counter (cnt_stor4) for this case.
In the normal case, we set the return code to indicate success and fall through to the common return logic.
We unlock the hash chain that was locked as a result of calling _db_find_and_lock and return to the caller.
The _db_findfree function tries to find a free index record and associated data record of the specified sizes.
We need to write lock the free list to avoid interfering with any other processes using the free list.
After locking the free list, we get the pointer address at the head of the list.
The while loop in _db_findfree goes through the free list, looking for a record with matching key and data sizes.
In this simple implementation, we reuse a deleted record only if the key length and data length equal the lengths for the new record being inserted.
There are a variety of better ways to reuse this deleted space, in exchange for added complexity.
If we can’t find an available record of the requested key and data sizes, we set the return code to indicate failure.
Otherwise, we write the previous record’s chain pointer to point to the next chain pointer value of the record we have found.
Once we are done with the free list, we release the write lock.
Recall the structure of the index file from Figure 20.2
The db_nextrec function returns the next record in the database.
The return value is a pointer to the data buffer.
If the caller provides a non-null value for the key parameter, the corresponding key is copied to this address.
The caller is responsible for allocating a buffer big enough to store the key.
A buffer whose size is IDXLEN_MAX bytes is large enough to hold any key.
Records are returned sequentially, in the order that they happen to be stored in the database file.
Also, because we do not follow the hash chains, we can come across records that have been deleted, but we will not return these to the caller.
We first need to read lock the free list so that no other processes can remove a record while we are reading it.
Since we are reading the index file sequentially, we can come across records that have been deleted.
We want to return only valid records, so we skip any record whose key is all spaces (recall that _db_dodelete clears a key by setting it to all spaces)
When we find a valid key, we copy it to the caller’s buffer if one was supplied.
Then we read the data record and set the return value to point to the internal buffer containing the data record.
We increment a statistics counter, unlock the free list, and return the pointer to the data record.
As we warned earlier, there is no order to the returned records; they are not in key order.
Similarly, if a deleted record is reused right after db_nextrec skips over the deleted record, we won’t see that new record unless we rewind the database and go through it again.
We’re not going through any hash chain, and we can’t determine the hash chain that a record belongs on.
Therefore, it is possible for an index record to be in the process of being deleted when db_nextrec is reading the record.
Before we conclude our study of the db.c source file, we need to describe the locking when new index records or data records are appended to the end of the file.
This fourth argument is the flag to these two functions, indicating that the new record is being appended to the file.
The technique used by _db_writeidx is to write lock the index file from the end of the hash chain to the end of file.
This won’t interfere with any other readers or writers of the database (since they will lock a hash chain), but it does prevent other callers of db_store from trying to append at the same time.
The technique used by _db_writedat is to write lock the entire data file.
Again, this won’t interfere with other readers or writers of the database (since they don’t even try to lock the data file), but it does prevent other callers of db_store from trying to append to the data file at the same time.
We wrote a test program to test the database library and to obtain some timing measurements of the database access patterns of typical applications.
This program takes two command-line arguments: the number of children to create and the number of database records (nrec) for each child to write to the database.
The program then creates an empty database (by calling db_open), forks the number of child processes, and waits for all the children to terminate.
Every 37 times through the loop, delete a random record.
Every 11 times through the loop, insert a new record and read the record back.
Every 17 times through the loop, replace a random record with a new record.
Every other one of these replacements is a record with the same size data; the alternate is a record with a longer data portion.
Every time a record is deleted, ten random records are looked up.
The number of operations performed on the database is counted by the cnt_xxx variables in the DB structure, which were incremented in the functions.
The number of operations differs from one child to the next, since the random-number generator used to select records is initialized in each child to the child’s process ID.
A typical count of the operations performed in each child is shown in Figure 20.6
Figure 20.6 Typical count of operations performed by each child.
We performed about ten times more fetches than stores or deletions, which is probably typical of many database applications.
The first version used the source code shown in Section 20.8, which we’ve called fine-grained locking.
The second version changed the locking calls to implement coarse-grained locking, as described in Section 20.6
The third version had all locking calls removed, so we could measure the overhead involved in locking.
We can run the first and second versions (fine-grained locking and coarse-grained locking) using either advisory or mandatory locking, by changing the permission bits on the database files.
In all the tests reported in this section, we measured the times for mandatory locking using only the implementation of fine-grained locking.
This system has four cores, which allows up to four processes to run concurrently.
The last 12 columns give the corresponding times in seconds.
In all cases, the user CPU time plus the system CPU time approximately equals the clock time.
This set of tests was CPU limited and not disk limited.
The six columns under ‘‘Advisory locking’’ are almost equal for each row.
This makes sense because for a single process; there is no difference between coarse-grained locking and fine-grained locking, except for the extra calls to fcntl.
Even though the locks are never used (since only a single process is running), the system call overhead in the calls to fcntl adds time.
Also note that the user CPU time is about the same for all four versions of locking.
Since the user code is almost equivalent (except for the number of calls to fcntl), this makes sense.
Since the number of locking calls is the same for advisory fine-grained locking and mandatory fine-grained locking, the additional system call overhead must be from the reads and writes.
The next test was to try the no-locking program with multiple children.
Normally, records that were added to the database couldn’t be found, and the test program aborted.
Different errors occurred every time the test program was run.
This illustrates a classic race condition: multiple processes updating the same file without using any form of locking.
Multiple-Process Results The final set of measurements looks mainly at the differences between coarse-grained locking and fine-grained locking.
As we said earlier, intuitively, we expect fine-grained locking to provide additional concurrency, since there is less time that portions of the database are locked from other processes.
All times are in seconds and are the total for the parent and all its children.
The first thing to notice is that the sum of the user and system times exceeds the clock time when multiple processes are used.
This seems odd at first, but is normal when multiple cores are present.
What happens is that all concurrently executing processes accumulate time as they execute; the CPU processing times shown are the sum of the times of all the cores used by the program.
Because we can run multiple processes at the same time (one per core), the CPU processing times can exceed the clock time.
On the system used for these tests, coarsegrained locking is the same as fine-grained locking for one process, but becomes more expensive (by about 30%) with multiple processes.
We would like the clock time to decrease from coarse-grained to fine-grained locking, which it does as soon as we start using multiple processes.
However, we expect the system time to remain higher for fine-grained locking for any number of processes, because we are issuing more fcntl calls with fine-grained locking than with coarse-grained locking.
We expect this increase of 31% more calls to fcntl to result in increased system time for fine-grained locking.
Therefore, the decrease in system time for fine-grained locking with two processes, and the relatively small increase with more than two processes, is puzzling.
First, recall from Figure 20.7 that there is no significant difference between coarse-grained locking times and fine-grained locking times when there is no contention for the locks.
This shows that the CPU overhead of the extra fcntl calls doesn’t affect the performance of the test program.
The second reason is that with coarse-grained locking, we hold locks for longer periods of time, thus increasing the likelihood that other processes will block on a lock.
With finegrained locking, the locking is done over shorter intervals, so there is less chance that processes will block.
If we count the number of times fcntl blocks, we will see that processes block more frequently with coarse-grained locking.
For example, with four processes, coarse-grained locking blocks almost five times more frequently than with fine-grained locking.
The extra work that processes need to do to put themselves to sleep and wake up more often with coarse-grained locking increases the system time, reducing the difference in system times between the two locking approaches.
Since the user code for all these tests is almost identical (there are some additional fcntl calls for both advisory fine-grained and mandatory fine-grained locking), we expect the user CPU times to be the same across any row.
The first time we ran these tests, we measured the user times for coarse-grained locking to be almost twice as long as the times for fine-grained locking when multiple processes competed for the locks.
Because the two versions of the database are the same, except for the number of calls to fcntl, this made no sense.
After investigating, we discovered that because there was more contention with coarse-grained locking, processes were waiting longer, and the operating system decided to reduce the CPU clock frequency to save power.
With fine-grained locking, there was more activity, so the system increased the CPU clock frequency.
This (artificially) made the coarse-grained locking tests run more slowly than the fine-grained tests.
After disabling the frequency scaling feature, we measured the performance of the test without this bias, and the difference in user times was much smaller.
We also plot the user CPU time divided by the number of processes and the system CPU time divided by the number of processes.
Note that both CPU times, divided by the number of processes, are linear but that the plot of the clock time is nonlinear.
The probable reason is the added amount of CPU time used by the operating system to switch between processes as the number of processes increases.
This operating system overhead would show up as an increased clock time, but shouldn’t affect the CPU times of the individual processes.
The reason the user CPU time increases with the number of processes is that there are more records in the database.
Each hash chain is getting longer, so it takes the _db_find_and_lock function longer, on average, to find a record.
This chapter has taken a long look at the design and implementation of a database library.
Although we’ve kept the library small and simple for presentation purposes, it contains the record locking required to allow concurrent access by multiple processes.
We’ve also looked at the performance of this library with various numbers of processes using no locking, advisory locking (fine-grained and coarse-grained), and mandatory locking.
With a single process, we saw that advisory locking adds between.
We said that this locking didn’t interfere with other readers and writers except those making calls to db_store.
Print a histogram of the number of records on each hash chain.
Does the record locking provided by the database library still work?
Modify the database to allow larger buffer sizes on the free list to satisfy the request.
How do you have to change the persistent format of the database to support this feature?
We now develop a program that can communicate with a network printer.
These printers are connected to multiple computers via Ethernet and often support PostScript files as well as plain text files.
Applications generally use the Internet Printing Protocol (IPP) to communicate with these printers, although some support alternative communication protocols.
We are about to describe two programs: a print spooler daemon that sends jobs to a printer and a command to submit print jobs to the spooler daemon.
Since the print spooler has to do multiple things (e.g., communicate with clients submitting jobs, communicate with the printer, read files, scan directories), this gives us a chance to use many of the functions from earlier chapters.
By embedding an IPP server inside a printer with an Ethernet card, the printer can service requests from many computer systems.
These computer systems need not be located on the same physical network, however.
Proposed draft standards are developed by the Printer Working Group, which is associated with the IEEE.
The main documents are listed in Figure 21.1, although many other documents are available to further specify administrative procedures, job attributes, and the like.
Candidate Standard 5100.12-2011 specifies all features that implementations must support to conform to different versions of the IPP standard.
There are many proposed extensions to the IPP protocol (specific features are defined in other IPP-related documents)
These features are divided into groups to create different conformance levels; each level is a different protocol version.
For compatibility, each higher level of conformance requires that implementations meet most of the requirements defined by lower versions of the standard.
In this chapter, we will use IPP version 1.1 in our simple example.
A client sends a request message to a server, and the server answers with a response message.
The IPP header contains a field that indicates the requested operation.
Operations are defined to submit print jobs, cancel print jobs, get job attributes, get printer attributes, pause and restart the printer, place a job on hold, and release a held job.
Figure 21.3 shows the structure of an IPP message header.
For a protocol request, the next 2 bytes contain a value identifying the type of operation requested.
For a protocol response, these 2 bytes contain a status code instead.
The next 4 bytes contain an integer identifying the request, which allows requests to be matched up with responses.
Any data that might be associated with the request follows immediately after the end-of-attributes tag.
In the header, integers are stored as signed, two’s-complement, binary values in big-endian byte order (i.e., network byte order)
Each group starts with a single byte identifying the group.
The value can be encoded as a string, a binary integer, or a more complex structure, such as a date/timestamp.
Depending on the operation requested, some attributes are required to be provided in the request message, whereas others are optional.
For example, Figure 21.5 shows the attributes defined for a print-job request.
The IPP header contains a mixture of text and binary data.
Attribute names are stored as text, but sizes are stored as binary integers.
This complicates the process of building and parsing the header, since we need to worry about such things as network byte order and our host processor ’s ability to address an integer on an arbitrary byte boundary.
A better alternative would have been to design the header to contain text only.
This simplifies processing at the cost of slightly larger protocol messages.
A request message contains a start line, followed by header lines, a blank line, and an optional entity body.
The entity body contains the IPP header and data in this case.
The start line consists of a method that indicates which operation the client is requesting, a Uniform Resource Locator (URL) that describes the server and protocol, and a string indicating the HTTP version.
The only method used by IPP is POST, which is used to send data to a server.
The header lines specify attributes, such as the format and length of the entity body.
A header line consists of an attribute name followed by a colon, optional white space, and the attribute value, and is terminated by a carriage return and a line feed.
For example, to specify that the entity body contains an IPP message, we include the header line.
The following is a sample HTTP header for a print request submitted to the author ’s Xerox Phaser 8560 printer:
The Content-Length line specifies the size in bytes of the amount of data in the HTTP message.
This excludes the size of the HTTP header, but includes the size of the IPP header.
The Host line specifies the host name and port number of the server to which the message is being sent.
The start line in an HTTP response message contains a version string followed by a numeric status code and a status message, terminated by a carriage return and a line feed.
The remainder of the HTTP response message has the same format as the request message: headers followed by a blank line and an optional entity body.
In response to a print request, the printer might send us the following message:
As far as our print spooler is concerned, all we care about in this message is the first line: it tells us whether the request succeeded or failed using a numeric error code and a short string.
The remainder of the message contains additional information to control caching by nodes that might sit in between the client and the server and to indicate the software version running on the server.
The programs that we develop in this chapter form the basis of a simple printer spooler.
A simple user command sends a file to the printer spooler; the spooler saves it to disk, queues the request, and ultimately sends the file to the printer.
All UNIX Systems provide at least one print spooling system.
We need to develop a spooling system to solve the problem of multiuser access to a single resource (the printer)
We use a simple command that reads a file and sends it to the printer spooler daemon.
The command has one option to force the file to be treated as plaintext (the default assumes that the file is PostScript)
In our printer spooler daemon, printd, we use multiple threads to divide up the work that the daemon needs to accomplish.
One thread listens on a socket for new print requests arriving from clients running the print command.
A separate thread is spawned for each client to copy the file to be printed to a spooling area.
One thread communicates with the printer, sending it queued jobs one at a time.
It identifies the host name of the server running the printer spooling daemon and the host name of the network printer.
The spooling daemon is identified by a line starting with the printserver keyword, followed by white space and the host name of the server.
The printer is identified by a line starting with the printer keyword, followed by white space and the host name of the printer.
A sample printer configuration file might contain the following lines:
We can run the print command on the same machine where the printer spooling daemon is running, or we can run it from any machine on the same network.
Programs that run with superuser privileges have the potential to open a computer system up to attack.
Such programs usually aren’t more vulnerable than any other program, but when compromised can lead to attackers obtaining full access to your system.
The printer spooling daemon in this chapter starts out with superuser privileges in this example to be able to bind a socket to a privileged TCP port number.
To make the daemon less vulnerable to attack, we can.
Design the daemon to conform to the principles of least privilege (Section 8.11)
After we obtain a socket bound to a privileged port address, we can change the user and group IDs of the daemon to something other than root (lp, for example)
All the files and directories used to store queued print jobs should be owned by this nonprivileged user.
This way, the daemon, if compromised, will provide the attacker with access only to the printing subsystem.
This is still a concern, but it is far less serious than an attacker getting full access to your system.
Audit the daemon’s source code for all known potential vulnerabilities, such as buffer overruns.
Log unexpected or suspicious behavior so that an administrator can take note and investigate further.
The source code for this chapter comprises five files, not including some of the common library routines we’ve used in earlier chapters:
We start the ipp.h header with the standard #ifdef to prevent errors when it is included twice in the same file.
We don’t use these codes in the program shown here; their use is left as an exercise (See Exercise 21.1)
In our example, we will use only the print-job operation.
The attribute tags delimit the attribute groups in the IPP request and response messages.
The value tags indicate the format of individual attributes and parameters.
Request messages start with the same header as response messages, except that the operation ID in the request is replaced by a status code in the response.
We include all header files that an application might need if it included this header.
This makes it easy for applications to include print.h without having to track down all the header dependencies.
The directories must be created by an administrator and be owned by the same user account under which the printer spooling daemon runs.
The daemon won’t try to create these directories if they don’t exist, because the daemon would need root privileges to create directories in /var/spool.
We design the daemon to do as little as possible while running as root to minimize the chance of creating a security hole.
Next, we define the account name under which the printer spooling daemon will run.
FreeBSD, however, doesn’t define a separate account for the printer spooling daemon, so we use the account reserved for system daemons.
The permissions are restrictive because we don’t want ordinary users to be able to read each other’s files while they are waiting to be printed.
We define HOST_NAME_MAX as the largest host name we will support if we are unable to determine the system’s limit with sysconf.
The QLEN is the backlog parameter we pass to listen (see Section 16.4 for details)
Some platforms don’t define the error ETIME, so we define it to an alternate error code that makes sense for these systems.
This is the error code we return when a read times out (we don’t want the server to block indefinitely reading from a socket)
Next, we declare all the public routines contained in util.c (we’ll look at these shortly)
The printreq and printresp structures define the protocol between the print command and the printer spooling daemon.
The print command sends a printreq structure specifying the size of the job in bytes, job characteristics, the user name, and the job name to the printer spooling daemon.
The daemon responds with a printresp structure containing a return code, the job ID, and an error message if the request failed.
The PR_TEXT job characteristic indicates that the file being printed should be treated as plaintext (instead of PostScript)
We define a bitmask of flags instead of defining a separate field for each flag.
Although only one flag value is currently defined, we could extend the protocol in the future to add more characteristics.
For example, we could add a flag to request double-sided printing.
We have room for 31 additional flags without requiring that we change the size of the structure.
Changing the size of the structure means that we might introduce a compatibility problem between the client and the server unless we upgrade both at the same time.
An alternative approach is to add a version number to the messages to allow the structures to change with each version.
Note that we define all integers in the protocol structures with an explicit size.
This helps avoid misaligned structure elements when a client has a different long integer size than the server.
The next file we will look at is util.c, the file containing utility routines.
We first define the limits needed by the functions in this file.
It is a wrapper for getaddrinfo (Section 16.3.3), since we always call getaddrinfo with the same hint structure.
Note that we do not need mutex locking in this function.
The LOCKING comment at the beginning of each function is intended only for documenting multithreaded locking.
This comment lists the assumptions, if any, that are made regarding the locking, tells which locks the function might acquire or release, and tells which locks must be held to call the function.
The scan_configfile function searches through the printer configuration file for the specified keyword.
We open the configuration file for reading and build the format string corresponding to the search pattern.
The notation %%%ds builds a format specifier that limits the string size so we don’t overrun the buffers used to store the strings on the stack.
We read the file one line at a time and scan for two strings separated by white space; if we find them, we compare the first string with the keyword.
If we find a match or we reach the end of the file, the loop ends and we close the file.
If the keyword matches, we return a pointer to the buffer containing the string after the keyword; otherwise, we return NULL.
The string returned is stored in a static buffer (valbuf), which can be overwritten on successive calls.
Thus, scan_configfile can’t be called by a multithreaded application unless we take care to avoid calling it from multiple threads at the same time.
We use the get_printaddr function to get the address of the network printer.
It is similar to the previous function except that when we find the name of the printer in the configuration file, we use the name to find the corresponding network address.
We use tread to prevent denial-of-service attacks on the printer spooling daemon.
A malicious user might repeatedly try to connect to the daemon without sending it data, just to prevent other users from being able to submit print jobs.
By giving up after a reasonable amount of time, we prevent this from happening.
The tricky part is selecting a suitable timeout value that is large enough to prevent premature failures when the system is under load and tasks are taking longer to complete.
If we choose a value that is too large, however, we might enable denial-of-service attacks by allowing the daemon to consume too many resources to process the pending requests.
We also provide a variation of tread, called treadn, that reads exactly the number of bytes requested.
This is similar to the readn function described in Section 14.7, but with the addition of the timeout parameter.
The command used to submit a print job is shown next.
We need to define an integer called log_to_stderr to be able to use the log functions in our library.
If this integer is set to a nonzero value, error messages will be sent to the standard error stream instead of to a log file.
Although we don’t use any logging functions in print.c, we do link util.o with print.o to build the executable print command, and util.c contains functions for both user commands and daemons.
We support one option, -t, to force the file to be printed as text (instead of as a PostScript program, for example)
We use the getopt function (introduced in Section 17.6) to process the command options.
When getopt completes processing the command options, it leaves the variable optind set to the index of the first nonoptional argument.
If this is any value other than the index of the last argument, then the wrong number of arguments was specified (we support only one nonoptional argument)
Our error processing includes checks to ensure that we can open the file to be printed and that it is a regular file (as opposed to a directory or other type of file)
We get the name of the host where the printer spooling daemon is running by calling the get_printserver function from util.c.
Then we translate the host name into a network address by calling getaddrlist (also from util.c)
Note that we specify the service as ‘‘print.’’ As part of installing the printer spooling daemon on a system, we need to make sure that /etc/services (or the equivalent database) has an entry for the printer service.
When we select a port number for the daemon, it would be a good idea to select one that is privileged, to prevent malicious users from writing applications that pretend to be a printer spooling daemon but instead steal copies of the files we try to print.
We try to connect to the daemon using one address at a time from the list returned by getaddrinfo.
We will try to send the file to the daemon using the first address to which we can connect.
If we are able to connect to the printer spooling daemon, we call submit_file to transmit the file we want to print to the daemon.
Then we exit with a value of 0 to indicate success.
Appendix B contains the source code for err_exit and the other error routines.
The submit_file function sends a print request to the daemon and reads the response.
We use geteuid to get the caller’s effective user ID and pass this to getpwuid to look for the user in the system’s password file.
We copy the user’s name to the request header or use the string unknown if we can’t identify the user.
We use strncpy when copying the name from the password file to avoid writing past the end of the user name buffer in the request header.
If the name is longer than the size of the buffer, strncpy won’t store a terminating null byte in the buffer, so we need to do it ourselves.
We store the size of the file to be printed in the header after converting it to network byte order.
Then we do the same with the PR_TEXT flag if the file is to be printed as plaintext.
By translating these integers to network byte order, we can run the print command on a client system while the printer spooling daemon is running on another computer system.
If these systems use processors with different byte ordering, then the commands will still work.
We set the job name to the name of the file being printed.
If the name is longer than will fit in the job name field in the message, we copy only the last portion of the name that will fit.
In this case, we prepend an ellipsis to indicate that there were more characters than would fit in the field.
We send the request header to the daemon using writen.
Recall that we introduced the writen function in Figure 14.24
The writen function uses multiple calls to write, if necessary, to transmit the specified amount.
If the writen function returns an error or transmits less than we requested, we print an error message and exit.
After sending the header to the daemon, we send the file to be printed.
We read the file IOBUFSZ bytes at a time and use writen to send the data to the daemon.
As with the header, if the write fails or we write less than we expect, we print an error message and exit.
Once we have sent the file to be printed to the print spooling daemon, we read the daemon’s response.
If the print request failed, the return code (retcode) will be nonzero, so we print the textual error message included in the response.
If the request succeeded, we print the job ID so that the user knows how to refer to the request in the future.
Writing a command to cancel a pending print request is left as an exercise; the job ID can be used in the cancellation request to identify the job to be removed from the print queue.
When submit_file returns to the main function, we exit, indicating success.
Note that a successful response from the daemon does not mean that the printer was able to print the file; it merely means that the daemon successfully added the print job to the queue.
The last file we will look at is the C source file for the printer spooling daemon.
The printer spooling daemon includes the IPP header file that we saw earlier, because the daemon needs to communicate with the printer using this protocol.
The job and worker_thread structures are used by the spooling daemon to keep track of print jobs and threads accepting print requests, respectively.
We could have avoided this by splitting the utility functions into two separate files: one for the server and one for the client commands.
We use the global variable printer to hold the network address of the printer.
We store the host name of the printer in printer_name.
The configlock mutex protects access to the reread variable, which is used to indicate that the daemon needs to reread the configuration file, presumably because an administrator changed the printer or its network address.
We use workers as the head of a doubly linked list of threads that are receiving files from clients.
The signal mask used by the threads is held in the variable mask.
For the list of pending jobs, we define jobhead to be the start of the list and jobtail to be the tail of the list.
This list is also doubly linked, but we need to add jobs to the end of the list, so we must remember a pointer to the list tail.
With the list of worker threads, the order doesn’t matter, so we can add them to the head of the list and don’t need to remember the tail pointer.
The joblock mutex protects the linked list of jobs, as well as the condition represented by the jobwait condition variable.
We declare the function prototypes for the remaining functions in this file.
Doing this up front allows us to place the functions in the file without worrying about the order in which each is called.
The main function for the printer spooling daemon has two tasks to perform: initialize the daemon and then process connect requests from clients.
We call the daemonize function from Figure 13.1 to become a daemon.
After this point, we can’t print error messages to standard error; we need to log them instead.
We will be writing to socket file descriptors, and we don’t want a write error to trigger SIGPIPE, because the default action is to kill the process.
Next, we set the signal mask of the thread to include SIGHUP and SIGTERM.
We’ll send the SIGHUP signal to the daemon to tell it to reread its configuration file.
We’ll send the SIGTERM signal to the daemon to tell it to clean up and exit gracefully.
We call sysconf to get the maximum size of a host name.
If sysconf fails or the limit is undefined, we use HOST_NAME_MAX as a best guess.
Sometimes, this constant is defined for us by the platform, but if it isn’t, we chose our own value in print.h.
We allocate memory to hold the host name and call gethostname to retrieve it.
Next, we try to find the network address that the daemon is supposed to use to provide the printer spooling service.
Our daemon needs superuser privileges to bind a socket to a reserved port number.
Now that this is done, we can lower its privileges by changing its user and group IDs to the ones associated with the LPNAME account.
We follow the principles of least privilege to avoid exposing the system to any potential vulnerabilities in the daemon.
We call getpwnam to find the password entry for the daemon.
If no such user account exists, or if it exists with the same user ID as the superuser, we log an error message and exit.
Otherwise, we change both the real and effective IDs by calling setgid and setuid.
To avoid exposing our system, we choose to provide no service at all if we can’t reduce our privileges.
We create one thread to handle signals and one thread to communicate with the printer.
By restricting printer communication to one thread, we can simplify the locking of the printer-related data structures.
Then we call build_qonstart to search the directories in /var/spool/printer for any pending jobs.
For each job that we find on disk, we will create a structure to let the printer thread know that it should send the file to the printer.
At this point, we are done setting up the daemon, so we log a message to indicate that the daemon has initialized successfully.
We copy the rendezvous fd_set structure to rset and call select to wait for one of the file descriptors to become readable.
We have to copy rendezvous, because select will modify the fd_set structure that we pass to it to include only those file descriptors that satisfy the event.
Since the sockets have been initialized for use by a server, a readable file descriptor means that a connect request is pending.
After select returns, we check rset for a readable file descriptor.
If we find one, we call accept to accept the connection.
If this fails, we log an error message and continue checking for more readable file descriptors.
Otherwise, we create a thread to handle the client connection.
The main thread loops, farming requests out to other threads for processing, and should never reach the exit statement.
We place a write lock on the entire file to indicate that the daemon is running.
If someone tries to start additional copies of the printer spooling daemon while one is already running, these additional daemons will fail to obtain the write lock and will exit.
Thus, only one copy of the daemon can be running at a time.
The job file contains an ASCII integer string representing the next job number.
Otherwise, we use atol to convert the string to an integer and use this value as the next job number.
We leave jobfd open to the job file so that we can update the job number as jobs are created.
We can’t close the file, because this would release the write lock that we’ve placed on it.
We can safely reuse the filename buffer, because FILENMSZ is defined to be 64 in print.h.
The init_printer function is used to set the printer name and address.
We get the printer address by calling get_printaddr (from util.c)
The get_printaddr function logs its own message when it is unable to find the printer’s address.
If a printer address is found, however, we set the printer name to the ai_canonname field in the addrinfo structure.
If this field is null, we set the printer name to a default value of printer.
Note that we log the name of the printer we are using to aid administrators in diagnosing problems with the spooling system.
We seek to the beginning of the file, convert the integer job number into a string, and write it to the file.
The job number increases monotonically; handling wrap-around is left as an exercise (see Exercise 21.9)
The get_newjobno function is used to get the next job number.
We increment the nextjob variable and handle the case where it wraps around.
Then we unlock the mutex and return the value nextjob had before we incremented it.
Multiple threads can call get_newjobno at the same time; we need to serialize access to the next job number so that each thread gets a unique job number.
Refer to Figure 11.9 to see what could happen if we don’t serialize the threads in this case.
The add_job function is used to add a new print request to the end of the list of pending print jobs.
If this fails, we log an error message and exit.
At this point, the print request is stored safely on disk; when the printer spooling daemon is restarted, it will pick the request up.
After we allocate memory for the new job, we copy the request structure from the client into the job structure.
Recall from print.h that a job structure consists of a pair of list pointers, a job ID, and a copy of the printreq structure sent to us by the client print command.
We save the job ID and lock the joblock mutex to gain exclusive access to the linked list of print jobs.
We are about to add the new job structure to the end of the list.
We set the new structure’s previous pointer to the last job on the list.
If the list is empty, we set jobhead to point to the new structure.
Otherwise, we set the next pointer in the last entry on the list to point to the new structure.
Then we set jobtail to point to the new structure.
We unlock the mutex and signal the printer thread that another job is available.
The replace_job function is used to insert a job at the head of the pending job list.
We acquire the joblock mutex, set the previous pointer in the job structure to NULL, and set the next pointer in the job structure to point to the head of the list.
If the list is empty, we set jobtail to point to the job structure we are replacing.
Otherwise, we set the previous pointer in the first job structure on the list to point to the job structure we are replacing.
Then we set the jobhead pointer to the job structure we are replacing.
If the next pointer is non-null, we set the next entry’s previous pointer to the target’s previous pointer.
Otherwise, the entry is the last one on the list, so we set jobtail to the target’s previous pointer.
If the target’s previous pointer is non-null, we set the previous entry’s next pointer equal to the target’s next pointer.
Otherwise, this is the first entry in the list, so we set jobhead to point to the next entry in the list after the target.
If we can’t open the directory, no print jobs are pending, so we return.
We read each entry in the directory, one at a time.
For each entry, we create the full pathname of the file and open it for reading.
If the open call fails, we just skip the file.
If we don’t read the entire structure, we close the file, log an error message, and unlink the file.
Then we create the full pathname of the corresponding data file and unlink it, too.
If we were able to read a complete printreq structure, we convert the filename into a job ID (the name of the file is its job ID), log a message, and then add the request to the list of pending print jobs.
When we are done reading the directory, readdir will return NULL, and we close the directory and return.
The client_thread is spawned from the main thread when a connect request is accepted.
Its job is to receive the file to be printed from the client print command.
We create a separate thread for each client print request.
The first thing we do is install a thread cleanup handler (see Section 11.5 for a discussion of thread cleanup handlers)
The cleanup handler is client_cleanup, which we will see later.
At this point, we are done with the thread’s initialization tasks, so we read the request header from the client.
If the client sends less data than we expect or we encounter an error, we respond with a message indicating the reason for the error and call pthread_exit to terminate the thread.
We convert the integer fields in the request header to host byte order and call get_newjobno to reserve the next job ID for this print request.
We use permissions that prevent others from being able read the files (FILEPERM is defined as S_IRUSR|S_IWUSR in print.h)
If we can’t create the file, we log an error message, send a failure response back to the client, and terminate the thread by calling pthread_exit.
We read the file contents from the client, with the intention of writing the contents out to our private copy of the data file.
But before we write anything, we need to check if this is a PostScript file the first time through the loop.
If the file doesn’t begin with the pattern %!PS, we can assume that the file is plaintext, so we set the PR_TEXT flag in the request header in this case.
Recall that the client can also set this flag if the -t flag is included when the print command is executed.
Although PostScript programs are not required to start with the pattern %!PS, the document formatting guidelines (Adobe Systems [1999]) strongly recommends that they do.
We write the data that we read from the client to the data file.
If write fails, we log an error message, close the file descriptor for the data file, send an error message back to the client, delete the data file, and terminate the thread by calling pthread_exit.
Note that we do not explicitly close the socket file descriptor.
This is done for us by our thread cleanup handler as part of the processing that occurs when we call pthread_exit.
When we receive all the data to be printed, we close the file descriptor for the data file.
If this fails, we log an error message, send an error response to the client, remove the data file, and terminate the thread.
On error, we log a message, close the descriptor for the control file, send a failure response back to the client, remove the data and control files, and terminate the thread.
We close the file descriptor for the control file and send a message containing the job ID and a successful status (retcode set to 0) back to the client.
Note that before the thread exits, we must close any file descriptors we no longer need.
Unlike with process termination, file descriptors are not closed automatically when a thread ends if other threads exist in the process.
If we didn’t close unneeded file descriptors, we’d eventually run out of resources.
We allocate memory for the structure, initialize it, lock the workerlock mutex, add the structure to the head of the list, and unlock the mutex.
The kill_workers function walks the list of worker threads and cancels each one.
We hold the workerlock mutex while we walk the list.
Recall that pthread_cancel merely schedules a thread for cancellation; actual cancellation happens when each thread reaches the next cancellation point.
The client_cleanup function is the thread cleanup handler for the worker threads that communicate with client commands.
The argument is the thread ID of the thread terminating.
We lock the workerlock mutex and search the list of worker threads until we find a matching thread ID.
When we find a match, we remove the worker thread structure from the list and stop the search.
We unlock the workerlock mutex, close the socket file descriptor used by the thread to communicate with the client, and free the memory backing the worker_thread structure.
The signal_thread function is run by the thread that is responsible for handling signals.
In the main function, we initialized the signal mask to include SIGHUP and SIGTERM.
Here, we call sigwait to wait for one of these signals to occur.
If sigwait fails, we log an error message and exit.
If we receive SIGHUP, we acquire the configlock mutex, set the reread variable to 1, and release the mutex.
This tells the printer daemon to reread the configuration file on the next iteration in its processing loop.
If we receive SIGTERM, we call kill_workers to kill all the worker threads, log a message, and call exit to terminate the process.
If we receive a signal we are not expecting, we kill the worker threads and call log_quit to log an error message and exit.
The add_option function is used to add an option to the IPP header that we build to send to the printer.
Some processor architectures, such as the SPARC, can’t load an integer from an arbitrary address.
This means that we can’t store the integers in the header by casting a pointer to int16_t to the address in the header where the integer is to be stored.
Instead, we need to copy the integer 1 byte at a time.
We store the tag in the header and convert the length of the attribute name to network byte order.
We copy the length 1 byte at a time to the header.
We repeat this process for the attribute value and return the address in the header where the next part of the header should begin.
The printer_thread function is run by the thread that communicates with the network printer.
We’ll use icp and ibuf to build the IPP header.
We’ll use hcp and hbuf to build the HTTP header.
The HTTP header includes a length field in ASCII, and we won’t know how much space to reserve for it until we assemble the IPP header.
We’ll use writev to write these two headers in one call.
The printer thread runs in an infinite loop that waits for jobs to transmit to the printer.
We use the joblock mutex to protect the list of jobs.
If a job is not pending, we use pthread_cond_wait to wait for one to arrive.
When a job is ready, we remove it from the list by calling remove_job.
Now that we have a job to print, we check for a change in the configuration file.
We lock the configlock mutex and check the reread variable.
If it is nonzero, then we free the old printer addrinfo list, clear the pointers, unlock the mutex, and call init_printer to reinitialize the printer information.
Since only this context looks at and potentially changes the printer information after the main thread initialized it, we don’t need any synchronization other than using the configlock mutex to protect the state of the reread flag.
Note that although we acquire and release two different mutex locks in this function, we never hold both at the same time, so we don’t need to establish a lock hierarchy (Section 11.6.2)
If we can’t open the data file, we log an error message, free the job structure, and continue.
After opening the file, we call fstat to find the size of the file.
If this fails, we log an error message, clean up, and continue.
If the connect_retry call fails, we jump down to defer, where we will clean up, delay, and try again later.
If we omit it, the printer will interpret the file using some default format.
For a PostScript printer, this is probably PostScript, but some printers can autosense the format and choose between PostScript, plaintext, or PCL (Hewlett-Packard’s Printer Command Language)
If the PR_TEXT flag is set, we set the format to text/plain.
Then we delimit the end of the attributes portion of the header with an end-of-attributes tag and calculate the size of the IPP header.
The extra integer counts any extra characters we might need to transmit to the printer.
As we shall see shortly, we need to send an extra character to be able to print plain text reliably.
We need to account for this extra character when we calculate the content length.
Now that we know the IPP header size, we can set up the HTTP header.
We set the Content-Length to the size in bytes of the IPP header plus the size of the file to be printed plus any extra characters we might need to send.
We mark the end of the HTTP header with a carriage return and a line feed.
We set the first element of the iovec array to refer to the HTTP header and the second element to refer to the IPP header.
Then we use writev to send both headers to the printer.
If the write fails or we write less than we requested, we log a message and jump to defer, where we will clean up and delay before trying again.
Even if we specify plaintext, the Phaser 8560 will try to autosense the document format.
To prevent it from recognizing the beginning of a file we want to print as plaintext, the first character we send is a backspace.
This character doesn’t show up in the printout and defeats the printer’s ability to autosense the file format.
This allows us to print the source to a PostScript file instead of printing the image resulting from the PostScript file.
We send the data file to the printer in IOBUFSZ chunks.
We don’t worry about this condition when we write the headers, because they are small.
However, if read fails, we log an error message and jump to defer.
After sending the file to the printer, we call printer_status to read the printer ’s response to our request.
On success, printer_status returns a nonzero value and we delete the data and control files.
Then we free the job structure, set its pointer to NULL, and fall through to the defer label.
At the defer label, we close the file descriptor for the open data file.
On error, jp will point to the job structure for the job we are trying to print, so we place the job back on the head of the pending job list and delay for 1 minute.
On success, jp is NULL, so we simply go back to the top of the loop to get the next job to print.
The readmore function is used to read part of the response message from the printer.
The printer_status function reads the printer’s response to a print-job request.
We don’t know how the printer will respond; it might send a response in multiple messages, send the complete response in one message, or include intermediate acknowledgements, such as HTTP 100 Continue messages.
We allocate a buffer and read from the printer, expecting a response to be available within about 5 seconds.
We skip the HTTP/1.1 string and any white space that starts the message.
If it doesn’t, we log the contents of the message.
If we have found a numeric status code in the response, we convert the first nondigit character following the status code to a null byte (this character should be some form of white space)
We search for the terminating carriage return or line feed, also terminating the text string with a null byte.
We call the atoi function to convert the status code string into an integer.
If this is an informational message only, we ignore it and continue the loop to read more.
We expect to see either a success message or an error message.
If we get an error message, we log the error and break out of the loop.
If the HTTP request succeeds, we need to check the IPP status.
We search through the message until we find the Content-Length attribute.
If we run out of buffer space, we call readmore, which uses realloc to increase the buffer size.
Because the buffer address might change, we need to adjust cp to point to the correct place in the buffer.
We use the strncasecmp function to do a case-insensitive comparison.
If we find the Content-Length attribute string, we search for its value.
We convert this numeric string into an integer and break out of the for loop.
If the comparison fails, we continue searching the buffer byte by byte.
If we reach the end of the buffer without finding the Content-Length attribute, we read more from the printer and continue the search.
We now know the length of the message (specified by the Content-Length attribute)
If we’ve exhausted the contents of the buffer, we read more from the printer.
Next we search for the end of the HTTP header (a blank line)
If we find it, we set the found flag and skip the blank line.
Whenever we call readmore, we set cp to point to the same offset in the buffer that it had previously just in case the buffer address changed when it was reallocated.
When we find the end of the HTTP header, we calculate the number of bytes that the HTTP header consumed.
If the amount we’ve read minus the size of the HTTP header is not equal to the amount of data in the IPP message (the value we calculated from the content length), then we read some more.
We get the status and job ID from the IPP header in the message.
Both are stored as integers in network byte order, so we need to convert them to the host byte order by calling ntohs and ntohl, respectively.
If the job IDs don’t match, then this is not our response, so we log a message and break out of the outer while loop.
If the IPP status indicates success, then we save the return value and break out of the loop.
Before we return, we free the buffer we used to hold the response message.
This concludes our look at the extended example in this chapter.
The programs in this chapter were tested with a Xerox Phaser 8560 network-attached PostScript printer.
Unfortunately, this printer doesn’t disable its autosense feature when we set the document format to text/plain.
This led us to use a hack to trick the printer so that it wouldn’t autosense the document format when we wanted to treat a document as plaintext.
An alternative is to print the source to a PostScript program using a utility such as a2ps(1), which encapsulates the PostScript program before printing.
This chapter has examined in detail two complete programs: a print spooler daemon that sends a print job to a network printer and a command that can be used to submit a job to be printed to the spooling daemon.
This has given us a chance to see many features that we described in earlier chapters used in real programs: threads, I/O multiplexing, file I/O, reading directories, socket I/O, and signals.
Use the job ID as the argument to the command to specify which job to cancel.
How can you prevent one user from canceling another user’s print jobs?
Include a way to move print jobs from one printer to another.
This technique won’t work with a printer that responds using a chunked transfer encoding.
This can result in the daemon reading an incorrect number when it restarts.
This appendix contains the function prototypes for the standard ISO C, POSIX, and UNIX System functions described in the text.
These prototypes also show which headers need to be included to obtain the definitions of any special constants and to obtain the ISO C function prototype to help detect any compile-time errors.
The page number reference for each function prototype appears to the right of the first header file listed for the function.
The page number reference gives the page containing the prototype for the function.
That page should be consulted for additional information on the function.
Some functions are supported by only a few of the platforms described in this text.
In addition, some platforms support function flags that other platforms don’t support.
In these cases, we usually list the platforms for which support is provided.
In a few cases, however, we list platforms that lack support.
It defines constants (such as MAXLINE) and prototypes for our own functions.
So our header automatically includes these system headers, along with <string.h>
This also reduces the size of all the program listings in the text.
Our own header, to be included before all standard system headers.
The reasons we include our header before all the normal system headers are to allow us to define anything that might be required by headers before they are included, to control the order in which header files are included, and to allow us to redefine anything that needs to be fixed up to hide the differences between systems.
B.2 Standard Error Routines Two sets of error functions are used in most of the examples throughout the text to handle error conditions.
One set begins with err_ and outputs an error message to standard error.
The reason for defining our own error functions is to let us write our error handling with a single line of C code, as in.
Our error functions use the variable-length argument list facility from ISO C.
The names of the macros are the same, but the arguments to some of the macros have changed.
Figure B.2 summarizes the differences between the various error functions.
Figure B.3 shows the error functions that output to standard error.
These require the caller to define the variable log_to_stderr and set it nonzero if the process is not running as a daemon.
In this case, the error messages are sent to standard error.
Error routines for programs that can run as a daemon.
Print a message with the system’s errno value and return.
Other processes were running at the time this program was run.
The qualifier const, however, says that perror does not modify what the pointer points to.
If it is currently a 32-bit integer, applications will have to be recompiled to work properly.
Some file systems and backup media store times in 32-bit integers.
These would need to be updated as well, but we still need to be able to read the old format.
In each of the headers that can define the size_t primitive system data type, we have the sequence.
This way, the typedef for size_t is executed only once.
Since the per-process limit can be modified, we can’t cache the value obtained from the previous call (it might have changed)
Figure C.1 Alternative method for identifying the largest possible file descriptor.
The exception to this is I/O on a raw disk device, which we aren’t considering.
Some systems also provide a direct I/O option to allow applications to bypass the kernel buffers, but we aren’t considering this option either.
Since the data that we read or write is buffered by the kernel, the term unbuffered I/O refers to the lack of automatic buffering in the user process with these two functions.
However, since both opens reference the same file, both file table entries point to the same v-node table entry.
The call to dup references the existing file table entry.
After the three calls to dup2, all three descriptors point to the same file table entry.
The result is that standard output and standard error are set to the same file.
This makes it impossible to write anywhere other than at the end of file.
If the symbolic link points to a nonexistent file, stat returns an error.
We can verify this by running the program from Figure 4.9:
Note that the permissions didn’t change but that the files were truncated.
The size of a symbolic link is the number of characters in the pathname contained in the symbolic link, and this pathname must always contain at least one character.
This default value may or may not be modified by the umask value.
The shell also has a default setting for the file access permission bits when it creates a new file for redirection.
In this example, it was rw-rw-rw-, and this value is always modified by our current umask.
But when the unlink function returns, the directory entry for tempfile is gone.
We have to use the df command in this example to see the actual amount of free space on the file system.
In this case, the changed-status time of the file is updated.
But if the link being removed is the last link to the file, it makes no sense to update this time, because all the information about the file (the i-node) is removed with the file.
Assuming that opendir uses a single file descriptor, this means that each time we descend one level, we use another descriptor.
We assume that the descriptor isn’t closed until we’re finished with a directory and call closedir.
This limits the depth of the file system tree that we can traverse to the maximum number of open descriptors for the process.
Note that the nftw function as specified in the XSI option of the Single UNIX Specification allows the caller to specify the number of descriptors to use, implying that it can close and reuse descriptors.
Users without accounts on a system (termed anonymous FTP) are placed in a separate directory, and a chroot is done to that directory.
This prevents the user from accessing any file outside this new root directory.
In addition, chroot can be used to build a copy of a file system hierarchy at a new location and then modify this new copy without changing the original file system.
This could be used, for example, to test the installation of new software packages.
Only the superuser can execute chroot, and once you change the root of a process, it (and all its descendants) can never get back to the original root.
The value that we don’t want to change in the call to utime should be the corresponding value from stat.
The last-modification time is the time that mail was last received, and the last-access time is when the mail was last read.
The access time isn’t stored, because its value corresponds to the time the archive was created, since the file has to be read to be archived.
The -a option to cpio has it reset the access time of each input file after the file has been read.
This way, the creation of the archive doesn’t change the access time.
Resetting the access time, however, does modify the changed-status time.
The changed-status time isn’t stored in the archive, because we can’t set this value on extraction even if it was archived.
The utimes function and its related functions, futimens and utimensat, can change only the access time and the modification time.
When the archive is read back (extracted), tar, by default, restores the modification time to the value in the archive.
In all cases with tar, the access time after extraction will be the time of extraction.
In contrast, cpio sets the access time and the modification time to the time of extraction.
By default, it doesn’t try to set the modification time to the value on the archive.
The -m option to cpio has it set both the access time and the modification time to the value that was archived.
Nevertheless, many commands will fail on pathnames that exceed PATH_MAX.
We are able to create this structure on all platforms; however, we cannot obtain the absolute pathname of the directory at the 1,000th level using getcwd on all platforms.
On Mac OS X 10.6.8, we can never get getcwd to succeed while in the directory at the end of this long path.
We are not able to archive this directory, however, using cpio.
It complains that many of the filenames are too long.
In fact, cpio is unable to archive this directory on all four platforms.
However, we are unable to extract the directory hierarchy from the archive on Linux 3.2.0
Also, fputs writes everything in the buffer until it hits a null byte; it doesn’t care whether a newline is in the buffer.
So, if MAXLINE is too small, both functions still work; they’re just called more often than they would be if the buffer were larger.
If either of these functions removed or added the newline (as gets and puts do), we would have to ensure that our buffer was big enough for the largest line.
The deep directory is created, with a file at the leaf.
The argument to fsync is obtained with the fileno function.
Calling fsync without calling fflush might do nothing if all the data were still in memory buffers.
Instead, we need to find the user’s entry in the shadow file and use its encrypted password field.
On Mac OS X 10.6.8, the encrypted password is not accessible using these interfaces.
Unless this program is run with superuser permissions, the call to getspnam fails with an error of EACCES.
On Mac OS X 10.6.8, asterisks are printed regardless of the permissions with which it is run.
To verify this theory, change the length of the string printed and see if the new length matches the return value.
Also note that if you enable the ISO C extensions in gcc, then the return value is always 0, as required by the standard.
If standard output were directed to a file, however, it would probably be fully buffered, and the actual output wouldn’t occur until the standard I/O cleanup is performed.
Copies of argc and argv are not kept in global variables like environ is.
Automatic variables declared after the left brace that starts a compound statement disappear after the matching right brace.
You need to define the variables i and buf also.
This assumes that the standard I/O stream stdout is closed when the child calls exit, not the file descriptor STDOUT_FILENO.
Some versions of the standard I/O library close the file descriptor associated with standard output, which would cause the write to standard output to also fail.
In this case, dup standard output to another descriptor, and use this new descriptor for the write.
When vfork is called, the parent’s stack pointer points to the stack frame for the f1 function that calls vfork.
The return information is often stored in the stack frame, and that information has probably been modified by the child.
After the parent resumes, what happens with this example depends on many implementation features of your UNIX system (where in the stack frame the return information is stored, what information in the stack frame is wiped out when the automatic variables are modified, and so on)
The normal result is a core file, but your results may differ.
When the parent is done, the child writes its output, but we let the parent terminate.
Whether the parent terminates or whether the child finishes its output first depends on the kernel’s scheduling of the two processes (another race condition)
When the parent terminates, the shell starts up the next program, and this next program can interfere with the output from the previous child.
We can prevent this from happening by not letting the parent terminate until the child has also finished its output.
We won’t see this happen if we let the child go first, since the shell doesn’t start the next program until the parent terminates.
The reason is that execlp ends up calling execve with the same pathname as when we call execl directly.
Figure C.10 Create a zombie and look at its status with ps.
Instead, the login entries in the utmp and wtmp files, and their corresponding logout entries, are usually written by the process that handles the login and detects the logout (telnetd in our example)
The reason is that the pause function returns whenever a signal is caught.
If the process is blocked by the kernel between these two function calls, the alarm goes off, the signal handler is called, and longjmp is called.
But since setjmp was never called, the buffer env_alrm is not set.
The operation of longjmp is undefined if its jump buffer has not been initialized by setjmp.
A copy of this paper is available online at http://www.kohala.com/start/libes.timers.txt.
Therefore, the real user ID provides more information to the receiver of the signal.
This skew occurs because each call to sleep schedules an event for a time in the future, but is not awakened exactly when that event occurs (because of CPU scheduling)
In addition, a finite amount of time is required for our process to start running and call sleep again.
A program such as the cron daemon has to fetch the current time every minute, as well as to set its first sleep period so that it wakes up at the beginning of the next minute.
Convert the current time to the local time and look at the tm_sec value.
Every minute, it sets the next sleep period so that it’ll wake up at the next minute.
On Linux 3.2.0, for example, when we use the fwrite function to write a large buffer, the fwrite function calls write directly for the same number of bytes.
While in the middle of the write system call, the alarm fires, but we don’t see the signal until the write completes.
It appears as if the kernel is blocking the signal while we are in the middle of the write system call.
When the alarm fires, it is caught, interrupting the call to fwrite.
After we return from the signal handler, we return to the loop inside the fwrite function and continue writing in 8 KB increments.
This problem can be solved by embedding a reference count and a mutex inside the job structure and having job_find increment the reference count.
The code that changes the ID can then avoid any job in the list that has a nonzero reference count.
Second, the condition each thread should wait to be satisfied is that there is a job for it to process, so we need to create a per-thread data structure to represent this condition.
Alternatively, we can embed the mutex and condition variable in the queue structure, but this means that all worker threads will wait on the same condition.
If there are many worker threads, we can run into a thundering herd problem, whereby many threads are awakened without work to do, resulting in a waste of CPU resources and increased lock contention.
In general, both can be correct, but each alternative has drawbacks.
If the program is running on a multiprocessor, some threads will run and immediately block because we are still holding the mutex (recall that pthread_cond_wait returns with the mutex held)
This is why the awakened threads must recheck the condition and not assume that it is true merely because pthread_cond_wait returned.
When we call fork, each process gets a copy of the standard I/O data structures.
When we run the program with standard output attached to a terminal, the output is line buffered, so every time we print a line, the standard I/O library writes it to our terminal.
However, if we redirect the standard output to a file, then the standard output is fully buffered.
The output is written when the buffer fills or the process closes the stream.
When we fork in this example, the buffer contains several printed lines not yet written, so when the parent and the child finally flush their copies of the buffer, the initial duplicate contents are written to the file.
However, our thread-safe version of getenv calls back into the pthread library while it is in an intermediate, inconsistent state.
In addition, the thread initialization functions call malloc, which, in turn, call getenv to find the value of the MALLOC_OPTIONS environment variable.
To get around this problem, we could make the reasonable assumption that program start-up is single threaded, and use a flag to indicate whether the thread initialization had been completed by our version of getenv.
While this flag is false, our version of getenv can operate as the non-reentrant version does (and avoid all calls to pthread functions and malloc)
Then we could provide a separate initialization function to call pthread_once, instead of calling it from inside getenv.
This requires that the program call our initialization function before calling getenv.
This solves our problem, because this can’t be done until the program start-up initialization completes.
After the program calls our initialization function, our version of getenv operates in a thread-safe manner.
It is thread-safe because it doesn’t use any unprotected global or static data and calls only other thread-safe functions.
Because this is an implementation detail and therefore hidden, there is no portable way for us to acquire and release the lock in the fork handlers.
Since we can’t determine the state of the internal lock in a condition variable after calling fork, it is unsafe for us to use the condition variable in the child process.
The solution is for the daemon to call openlog with an option of LOG_NDELAY, before calling chroot.
This opens the special device file (the UNIX domain datagram socket), yielding a descriptor that is still valid, even after a call to chroot.
This scenario is encountered in daemons, such as ftpd (the File Transfer Protocol daemon), that specifically call chroot for security reasons but still need to call syslog to log error conditions.
Recall that daemonize closes all open file descriptors and then reopens the first three to /dev/null.
This means that the process won’t have a controlling terminal, so getlogin won’t be able to look in the utmp file for the process’s login entry.
This means that the process can always get the login name, unless the parent didn’t have one to start out (such as init when the system is bootstrapped)
See if a pending write lock will block the next * read-lock attempt.
In this case, the parent is unable to obtain a read lock because there is a process waiting for a write lock.
The four FD_ macros then manipulate this array of longs, turning specific bits on and off and testing specific bits.
One reason that the data type is defined to be a structure containing an array and not simply an array is to allow variables of type fd_set to be assigned to one another with the C assignment statement.
To use this technique with contemporary systems, we need to do several things:
Before we include any header files, we need to define whatever symbol prevents us from including <sys/select.h>
Before we run the program, we need to configure the system to allow us to open as many file descriptors as we might need so that we can actually make use of FD_SETSIZE file descriptors.
There is not an FD_xxx function that corresponds to sigfillset.
With signal sets, the pointer to the set is always the first argument, and the signal number is the second argument.
With descriptor sets, the descriptor number is the first argument, and the pointer to the set is the next argument.
As the BSD usleep(3) manual page states, usleep uses the nanosleep function, which doesn’t interfere with timers set by the calling process.
The problem, however, is that calling fork releases all the locks in the child, so the child can’t start off with any locks of its own.
The following table shows the values calculated for our four platforms.
Here, we calculate the amount of data that a pipe can hold independent of any atomicity constraints.
On all four platforms, the last-access time is updated when the file resides in the default file system type for the given operating system.
The pager program blocks forever reading from its standard input.
The read end of the pipe is automatically closed when the parent terminates.
But the parent is probably running ahead of the child by one pipe buffer, since the child (the pager program) is waiting for us to look at a page of output.
If we’re running a shell, such as the Korn shell, with interactive command-line editing enabled, the shell probably changes the terminal mode when our parent terminates and the shell prints a prompt.
This undoubtedly interferes with the pager program, which has also modified the terminal mode.
Most pager programs set the terminal to noncanonical mode when awaiting input to proceed to the next page.
But the shell can’t execute the nonexistent command, so it prints.
Bourne shell, Bourne-again shell, and Korn shell, the command is echo $?
Then use fdopen to associate the pipe descriptors with a standard I/O stream, and set the streams to be line buffered.
Do this before the while loop that reads from standard input:
The write and read in the while loop are replaced with if (fputs(line, fpout) == EOF)
Since that’s not the child that system created, it calls wait again and blocks until the sleep is done.
When pclose calls wait, an error is returned, since there are no more children to wait for.
After all the data has been read, read returns 0 to indicate the end of file.
But with poll, the POLLHUP event is returned, and this can happen while there is still data to be read.
Once we have read all the data, however, read returns 0 to indicate the end of file.
After all the data has been read, the POLLIN event is not returned, even though we need to issue a read to receive the end-of-file notification (the return value of 0)
R/W/E R R/W R/W/Eselect on read end of pipe with write end closed R/HUP HUP INV HUPpoll on read end of pipe with write end closed R/W/E R/W R/W R/Wselect on write end of pipe with read end closed R/HUP W/ERR INV HUPpoll on write end of pipe with read end closed.
With an output descriptor that refers to a pipe that has been closed by the reader, select indicates that the descriptor is writable.
But when we call write, the SIGPIPE signal is generated.
If we either ignore this signal or return from its.
To send standard error back to the parent, include the shell redirection 2>&1 in the cmdstring.
The shell in turn calls fork, and the child of the shell executes the command string.
When cmdstring terminates, the shell is waiting for this to happen.
The shell then exits, which is what the waitpid in pclose is waiting for.
Opening the FIFO twice requires some care, as a nonblocking open is required.
We have to do a nonblocking, read-only open first, followed by a blocking open for write-only.
If we tried a nonblocking open for writeonly first, it would return an error.
Figure C.20 Opening a FIFO for reading and writing, without blocking.
To read the queue, all that is needed is for the process to know the identifier for the queue and for the queue to allow world-read access.
Instead, when a linked list is built in a shared memory segment, the list pointers should be stored as offsets to other objects in the shared memory segment.
These offsets are formed by subtracting the start of the shared memory segment from the actual address of the object.
Parent i Child i Shared value update set to set to set to returns.
We will use select to wait for connect requests to arrive on multiple endpoints.
Recall from Section 16.4 that a passive endpoint will appear to be readable when a connect request arrives on it.
When a connect request does arrive, we will accept the request and process it as before.
Next, we need to remove the call to waitpid from our serve function.
After forking the child to service the request, the parent closes the new file descriptor and resumes listening for additional connect requests.
Finally, we need a signal handler for SIGCHLD, as follows:
To disable asynchronous socket I/O, we simply need to disable asynchronous signaling.
The reason we mix fcntl and ioctl commands is to find the methods that are most portable.
To detect message boundaries, we’d have to add a header to each message to indicate the length.
But this still involves two extra copy operations: one to write to the pipe and one to read from the pipe.
A more efficient approach is to use the pipe only to signal the main thread that a new message is available.
With this approach, we need to move the mymesg structure to the threadinfo structure and use a mutex and a condition variable to prevent the helper thread from reusing the mymesg structure until the main thread is done with it.
If the declaration also causes storage to be allocated, it is called a definition.
In the opend.h header, we declare the three global variables with the extern storage class.
These declarations do not cause storage to be allocated for the variables.
In the main.c file, we define the three global variables.
Sometimes, we’ll also initialize a global variable when we define it, but we typically let the C default apply.
The loop that goes through the client array can terminate when the number of ready descriptors has been processed.
The second problem is that if the name is a symbolic link pointing to the UNIX domain socket file, then stat will report that the name is a socket (recall that the stat function follows symbolic links), but when we call unlink, we will actually remove the symbolic link instead of the socket file.
To solve this problem, we should use lstat instead of stat, but this doesn’t solve the first problem.
This approach works on all four platforms covered in this book.
The second option is to pack two separate cmsghdr structures into a single message:
Unlike the first approach, this method works only on FreeBSD 8.0
It then uses 8-bit I/O, handling the parity generation itself.
Under Solaris, execute stty -a with standard input redirected from the terminal window running vi.
A call to read will wait for at least one character to be typed, but after that character is entered, read waits only one-tenth of a second for additional characters before returning.
The first two process groups constitute a session with the login shell as the session leader.
The first process group (the login shell) is a background process group, and the other two are foreground process groups.
This causes the PTY slave to terminate, which causes the PTY master to terminate.
This in turn generates an end of file for the pty parent that’s reading from the PTY master.
The parent sends SIGTERM to the child, so the child terminates next.
Finally, the parent calls exit(0) at the end of the main function.
The relevant output from the program shown in Figure 8.29 is.
This echoing is done by the line discipline module above the slave even though the program (ttyname) never reads the data.
This process is then stopped by the kernel, and another process runs.
This other process calls db_delete, and the record being read by the other process is deleted.
Both its key and its data are rewritten in the two files as all blanks.
The read lock by db_nextrec allows it to do the read of the index record, followed by the read of the data record, as an atomic operation (with regard to other cooperating processes using the same database)
If the process were to write the index record first, but be killed before writing the data record, then we’d have a valid index record that pointed to invalid data.
There are two places to check for queued jobs: the printer spooling daemon’s queue and the network printer’s internal queue.
Take care to prevent one user from being able to cancel someone else’s print job.
Of course, the superuser should be able to cancel any job.
The printer_thread function checks whether it needs to reread the configuration file before each attempt to send a job to the printer.
There are two simple approaches: either we can add 1 to the number of bytes we write, or we can use the dprintf function instead of calling sprintf and write.
A paper describing the Service Management Facility (SMF) in Solaris, which provides a framework for starting and monitoring administrative processes, and recovering from failures affecting the services they provide.
The version of awk described in this book is sometimes called ‘‘nawk’’ (for new awk)
This four-volume set specifies the source code interface and runtime behavior of System V.
This volume contains the traditional UNIX System manual pages (Sections 1–9)
A book on the details of the design and implementation of the UNIX operating system.
Although actual UNIX System source code is not provided in this text (since it was proprietary to AT&T at the time), many of the algorithms and data structures used by the UNIX kernel are presented and discussed.
A book describing how to use the Korn shell, both as a command interpreter and as a programming language.
Describes changes made to the virtual memory implementation of SVR4 to improve its performance, especially for fork and exec.
A paper describing the jemalloc implementation of the dynamic memory allocation library used in FreeBSD.
Describes an alternative library function to traverse a file system hierarchy.
Describes the initial implementation of the mmap function and related issues in the virtual memory design.
This was the first of the POSIX standards, and it defined the C language systems interface standard, based on the UNIX operating system.
The official standard for the C language and the standard libraries.
The latest version of the official standard for the C language and the standard libraries, which replaces the 1999 version.
This book covers numerous UNIX commands and utilities, such as grep, sed, awk, and the Bourne shell.
A book on the ANSI standard version of the C programming language.
Appendix B contains a description of the libraries defined by the ANSI standard.
If you thought this book was long, here is one that is half again as big, but focuses only on the Linux programming interface.
A description of an alternative to the standard I/O library.
An alternative to the standard I/O library based on mapped files.
Describes how to write a daemon in the UNIX System.
Describes the source code of the 6th Edition UNIX System.
An entire book on the FreeBSD operating system, version 5.2
A book on the internals of the Solaris 10 operating system.
A description of the history of the design of the password scheme used in UNIX Systems.
A book with many details on administering a UNIX system.
The POSIX and X/Open standards combined into a single reference.
The HTML version can be viewed for free online at http://www.opengroup.org.
A description of the Plan 9 operating system, developed in the same department where the UNIX System was invented.
The features are built on the stream input–output system and include full-duplex pipes, the ability to pass file descriptors between processes, and unique client connections to servers.
A book that describes the networking programming environment of UNIX System V Release 4, which is based on STREAMS.
A description of the dbm(3) library and its implementations, and a newer hashing package.
Roughly 1,600 pages on the design of the Mac OS X operating system.
A detailed book on network programming under the UNIX System.
The contents of the first edition of this book differ greatly from later editions.
Redesigned and split into two volumes in the second edition and updated in the third edition.
Describes operating system services and how they affect database operation.
Describes some problems in implementing databases in early UNIX systems.
Describes a memory allocation algorithm suitable for a wide variety of applications.
Although out of print, this has been replaced by the Single UNIX Specification [Open Group 2008]
The function subentries labeled ‘‘definition of’’ point to where the function prototype appears and, when applicable, to the source code for the function.
Also, significant functions and constants that occur in any of the examples in the text, such as select and poll, are also included in this index.
Trivial functions that occur frequently, such as printf, are sometimes not referenced when they occur in examples.
Here you will gain access to quality and trusted content and.
Access to supplemental content, including bonus chapters, source code, or project fi les.
Benefi ts will be listed on your Account page under Registered Products.
InformIT is a brand of Pearson and the online presence for the world’s leading technology publishers.
It’s your source for reliable and qualified content and knowledge, providing access to the top brands, authors, and contributors from the tech community.
LearnIT at InformIT Looking for a book, eBook, or training video on a new technology? Seeking timely and relevant information and tutorials? Looking for expert opinions, advice, and tips?  InformIT has the solution.
Learn about new releases and special promotions by subscribing to a wide variety of newsletters.
Discount applies to the Safari Library and is valid for fi rst 12 consecutive monthly billing cycles.
See it, believe it Watch hundreds of expert-led instructional videos on today’s hottest topics.
WAIT, THERE’S MORE! Gain a competitive edge Be first to learn about the newest technologies and subjects with Rough Cuts pre-published manuscripts and new technology overviews in Short Cuts.
Accelerate your project Copy and paste code, create smart searches that let you know when new books about your favorite topics are available, and customize your library with favorites, highlights, tags, notes, mash-ups and more.
Safari Books Online is a digital library providing searchable, on-demand access to thousands of technology, digital media, and professional development books and videos from leading publishers.
With one monthly or yearly subscription price, you get unlimited access to learning tools and information on topics including mobile app and software development, tips and tricks on using your favorite gadgets, networking, project management, graphic design, and much more.
If you have a website, blog, or even a Facebook page, you can start earning money by putting InformIT links on your page.
Whenever a visitor clicks on these links and makes a purchase on informit.com, you earn commissions* on all sales!
Every sale you bring to our site will earn you a commission.
All you have to do is post the links to the titles you want, as many as you want, and we’ll take care of the rest.
ApplY And get stArted! It’s quick and easy to apply.
