The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
Once the initial versions of your project are used by thousands of developers and a community starts to assemble around it, the challenge becomes communicating with a much larger audience of users who will use the project and pass judgment on its merits, and growing the size of the community ten-fold or a thousand-fold.
Gradle has already amassed a large audience, and we’ve seen tremendous growth over the last two years, but we’re getting ready for a still larger influx of end-users.
Therefore, the importance of having a good book cannot be overstated.
Developers with a range of skills and abilities need to be able to pick up a book that’s easy to understand and which can impart both the syntax and the philosophy behind the tool.
Only then will they be able to confidently grow the community xiii.
Additionally, this book gives new Gradle users a very good glimpse into how Gradle fits into a larger view of continuous delivery.
Benjamin is the sort of expert that you hope emerges from an open source community.
He has been a long term Gradle contributor and is the author of several popular Gradle plugins.
Benjamin has the rare ability to dive into the core details of a particularly challenging development problem and then explain the tool to end-users.
We’re happy that he has recently joined Gradleware and is now part of the Gradle development team.
FOREWORDxiv I hope you enjoy the book as well as working with Gradle.
May your software delivery process become both fun and efficient.
My tool of choice was the IDE, which allowed me to run all tasks required to fully automate my software development cycle.
I thought it was the most amazing thing to be able to describe my automation logic with the help of mostly pre-existing functionality and to execute it in a defined order.
Despite the fact that the definition language was XML (these were the days when XML was still fashionable), I soon began to become more ambitious by creating artifacts for different target platforms, writing deployment logic for web containers, and setting up a continuous integration server.
And while I explored other build tool options to meet these needs over the years, I found that there was always a Catch-22
Many developers accepted the status quo in this space, which left them with painful experiences.
Rarely is there a topic that’s discussed more religiously2 than the pros and cons of build tools and why people hate them so much.
The purpose of this book isn’t necessarily to convince you to switch your current build to Gradle.
Thanks again for the object-oriented mindset I picked up by working with you.
This topic is on par with Windows versus Linux or the comparison of web application frameworks.
I will, however, talk about the massive innovation that Gradle brings to the table and compare it to existing solutions.
I started to write this book with a specific goal in mind: teach the core concepts of Gradle, but don’t stop there.
In a world that embraces software development practices like continuous integration and delivery, you have to take into consideration the tooling ecosystem into which a build system must integrate.
If you have questions, comments, or ideas, I’d love to hear them.
Your feedback might spark the urge to write a second edition or add-on content.
Feel free to send me an email or contact me on the book’s forum at Manning.
It’s safe to say that it literally controls your life for an extended period of time.
This wouldn’t have been possible without the support, encouragement, and help of others.
In 2010, I started to evaluate Gradle for the first time as a replacement for a Maven project for a previous employer.
I probably wouldn’t have done that without the spike initiated by Jonathan Bodner, a long-term acquaintance, whom I deeply admire for his technical insight.
He started me on my way to getting excited about Gradle, becoming deeply involved with its community, and writing plugins of my own.
I’ve been a technical reviewer for books published by Manning for many years before writing my own.
My engagement got me a first glimpse of what it means to write a book.
I had always wanted to write a book, but never found the appropriate time or topic to jump on it.
Thanks, Dan, for your enthusiasm that inspired me to carry on the torch and make it my own.
One of the first things you do before writing a book is put together the outline and table of contents.
The first person I showed the draft to was David James, the organizer of the Washington DC–area Groovy user group.
Thanks for your outside perspective on the organization of the book, your meticulous attention to detail, and your strong encouragement to make the book a reality.
This goes out to everyone involved in the process at Manning Publications.
Michael Stephens, who I talked to first, bought into the idea of this book and ultimately trusted me to do a good job.
My gratitude also goes to Cynthia Kane, who helped me to find my writing style.
I’d also like to thank Jennifer Stout, my development editor, who always tried to get the best out of me, made me think about whole text passages in a different way, and tolerated my impatience.
Thanks also to the whole Manning production and marketing team for guidance along the way and for making the book what it is now.
Special thanks to Spencer Allain, Jonathan Keam, and Robert Wenner for thoroughly reading every chapter of the book and providing me with line-by-line edits and comments at different stages of development; Michael McGarr and Samuel Brown for bouncing around ideas that involved content on continuous delivery and DevOps; and Baruch Sadogursky from JFrog for the technical review of chapter 14 and for promoting the book even before it was released.
I also wish to thank the relentless Author Online forum participants for pushing the content to the next level.
Writing a book requires making sacrifices and puts tremendous strain on personal relationships.
I would like to thank my family and friends for being supportive, encouraging, and understanding while I’ve worked toward completing this ambitious goal.
And, yes, there will be time for hanging out without me thinking about the content of the current chapter.
I’m deeply grateful to my wife Sarah for her unending support and optimism.
You pushed me to believe in myself, made me take breaks from writing, and tolerated me falling asleep before 9:00 p.m.
Without you, the writing process would have been far more grueling than it was.
The first part gives an introduction to Gradle’s concepts and philosophy, explaining how it compares to other build tools and how to write scripts to automate simple tasks.
Part two explores the tool’s building blocks and core techniques in greater depth.
You should be able to use this knowledge to implement complex, extendable, enterprise builds.
The third part describes how Gradle can be used in the context of continuous deliver, focusing on topics like polyglot builds, code quality, artifact assembly, and deployment.
The chapters in part 1, Introducing Gradle, are as follows:
Introduction to project automation—This chapter gives a gentle introduction into why it’s a good idea to automate your projects and how build tools can help get xix.
Next generation builds with Gradle—How does Gradle compare to existing JVMlanguage build tools? This chapter covers Gradle’s extensive feature set and how it helps automate your software delivery process in the context of a Continuous Delivery deployment pipeline.
As a first taste, you’ll write a simple build script and run it on the command line.
Building a Gradle project by example—This chapter introduces a Java-based web application as a vehicle to demonstrate some of Gradle’s core features.
We’ll explore the use of the Java plugin for standardized and nonconventional use cases and examine productivity tools for fast development turnaround.
Build script essentials—What are the main building blocks of a Gradle project? This chapter discusses the use of important domain objects, namely projects and tasks.
We’ll touch on how these objects map to the corresponding classes in the Gradle API, Gradle’s build lifecycle, the incremental build feature, and the mechanics of registering lifecycle hooks.
Dependency management—No enterprise project can do without reusing functionality from external libraries.
This chapter explores Gradle’s declarative support for dependency management, version conflict resolution strategies, and the inner workings of its cache.
Multiproject builds—Does your project consist of multiple, modularized software components? This chapter covers the options for organizing build logic in a multiproject setting, how to declare project dependencies, and the use of partial builds to improve execution time.
Testing with Gradle—Testing your code is an important activity of the software development lifecycle.
By the end of this chapter, you’ll write tests with JUnit, TestNG, and Spock and execute them as part of the build lifecycle.
You’ll also learn how to configure test execution, register listeners to react to test lifecycle events, and organize different types of tests with the help of source sets.
If you want to add completely new functionality to a project or extend the existing domain model, this chapter is for you.
You’ll learn how to write your own plugin to deploy your sample application to the cloud.
Integration and migration—In this chapter, we’ll look at how Gradle integrates with Ant and Maven.
We’ll also explore migration strategies in case you decide to go with Gradle long term.
Part 3, From build to deployment, examines how Gradle can be used to bring the example application from the developer’s machine into the production environment with the help of a build pipeline:
This chapter explains Gradle’s capabilities for generating project files for popular IDEs like Eclipse, IntelliJ, and NetBeans.
We also discuss how to navigate and manage Gradle-backed projects within these IDEs.
Building polyglot projects—In this chapter, we’ll discuss how Gradle faces the challenge of organizing and building polyglot projects by using your case study application as an example.
Code quality management and monitoring—In this chapter we’ll focus on tools that measure code quality and visualize the results to help you pinpoint problem.
By the time you finish this chapter, you’ll know how to integrate code quality tools with your build.
This chapter discusses the installation and configuration procedures needed to run Gradle on Jenkins, an open-source CI server.
Artifact assembly and publishing—A build either consumes or produces binary artifacts.
This chapter explores the artifact assembly process and the configuration needed to publish artifacts, including their metadata, to a binary repository.
Infrastructure provisioning and deployment—A configured target environment is a prerequisite for any software deployment.
In this chapter, we’ll discuss the importance of “infrastructure as code” for setting up and configuring an environment and its services in an automated fashion.
A Driving the command line—This appendix explains how to operate Gradle from the command line.
We’ll explore tasks available to all Gradle builds, plus command line options and their use cases.
Who should read the book? This book is primarily for developers and build automation engineers who want to implement a repeatable build that’s easy to read and extend.
I assume that you have a basic understanding of an object-oriented programming language.
You’ll get the most out of the content if you have a working knowledge of Java.
In this book, you’ll use a lot of Groovy; however, I don’t assume you already have experience with the language.
For a jump-start on Groovy, look at appendix B, Groovy for Gradle users.
The appendix also provides additional references to books that dig deeper into more advanced aspects of the language.
Throughout the chapters, we’ll touch on topics you can’t circumnavigate when dealing with automated builds.
It will be helpful to have some knowledge of tools like Ant, Ivy, and Maven; practices like continuous integration and delivery; and concepts like dependency management.
Code conventions and downloads Source code in listings and text is in a fixed-width font like this to separate it from ordinary text.
Code annotations accompany many of the code listings and highlight important concepts.
You’ll find additional references to source code repositories that either take some examples from the book to the next level or demonstrate the use of Gradle in contexts not covered in the book.
Author Online The purchase of Gradle in Action includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the author and other users.
To access the forum and subscribe to it, visit http://www.manning.com/GradleInAction.
This page provides information on how to get on the forum once you’re registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to readers is to provide a venue for meaningful dialogue between individual readers and between readers and the author.
It is not a commitment to any specific amount of participation on the part of the author, whose contribution to the forum remains voluntary (and unpaid)
Let your voice be heard, and keep the author on his toes!
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
About the author Benjamin Muschko is a software engineer with more than 10 years of experience in developing and delivering business applications.
He is a member of the Gradleware engineering team and developer of several popular Gradle plugins.
Hacquet (1739–1815) was an Austrian physician and scientist who spent many years studying the botany, geology, and ethnography of many parts of the Austrian Empire, as well as the Veneto, the Julian Alps, and the western Balkans, inhabited in the past by peoples of the Illyrian tribes.
Hand-drawn illustrations accompany the many scientific papers and books that Hacquet published.
The rich diversity of the drawings in Hacquet's publications speaks vividly of the uniqueness and individuality of the eastern Alpine and northwestern Balkan regions just 200 years ago.
This was a time when the dress codes of two villages separated by a xxiii.
Dress codes have changed since then and the diversity by region, so rich at the time, has faded away.
It is now often hard to tell the inhabitant of one continent from another and today the inhabitants of the picturesque towns and villages in the Slovenian Alps or Balkan coastal towns are not readily distinguishable from the residents of other parts of Europe.
We at Manning celebrate the inventiveness, the initiative, and the fun of the computer business with book covers based on costumes from two centuries ago brought back to life by illustrations such as this one.
Efficient project automation is one of the key enablers for delivering software to the end user.
The build tool of choice shouldn’t stand in the way of this effort; rather, it should provide you with a flexible and maintainable way to model your automation needs.
Gradle’s core strength is that it provides you with easy-to-understand but powerful tooling to automate your project end-to-end.
In chapter 1, we’ll discuss the benefits of project automation and its impact on the ability to develop and deliver software in a repeatable, reliable, and portable fashion.
You’ll learn the basic concepts and components of a build tool and how they’re implemented with Ant and Maven.
By comparing their pros and cons, you’ll see the need for a next-generation build tool.
Gradle draws on lessons learned from established build tools and takes their best ideas to the next level.
You’ll install the Gradle runtime and explore how to write and execute a simple build script from the command line.
You’ll learn the configuration needed to compile, unit-test, package, and run the sample.
By the end of part 1, you’ll have a feel for Gradle’s expressiveness and flexibility.
Tom and Joe work as software developers for Acme Enterprises, a startup company that offers a free online service for finding the best deals in your area.
The company recently received investor funding and is now frantically working toward its first official launch.
By the end of next month, they’ll need to present a first version of the product to their investors.
Both developers are driven individuals, and they pump out features daily.
So far, development of the software has stayed within the time and budget constraints, which makes them happy campers.
The chief technology officer (CTO) pats them on the back; life is good.
However, the manual and error-prone build and delivery process slows Introduction to project automation.
As a result, the team has to live with sporadic compilation issues, inconsistently built software artifacts, and failed deployments.
This chapter will give you a gentle introduction into why it’s a good idea to automate your project and how build tools can help get the job done.
We’ll talk about the benefits that come with sufficient project automation, the types and characteristics of project automation, and the tooling that enables you to implement an automated process.
Two traditional build tools dominate Java-based projects: Ant and Maven.
We’ll go over their main features, look at some build code, and talk about their shortcomings.
Lastly, we’ll discuss the requirements for a build tool that will fulfill the needs of modernday project automation.
Going back to Tom and Joe’s predicament, let’s go over why project automation is such a no-brainer.
Believe it or not, lots of developers face the following situations.
At Acme, developers do all their coding within the IDE, from navigating through the source code, implementing new features, and compiling and refactoring code, to running unit and integration tests.
Whenever new code is developed, they press the Compile button.
If the IDE tells them that there’s no compilation error and the tests are passing, they check the code into version control so it can be shared with the rest of the team.
The IDE is a powerful tool, but every developer will need to install it first with a standardized version to be able to perform all of these tasks, a lesson Joe learns when he uses a new feature only supported by the latest version of the compiler.
Staring down a ticking clock, Joe checks out the code from version control and realizes that it doesn’t compile anymore.
It seems like one of the classes is missing from the source code.
He calls Tom, who’s puzzled that the code doesn’t compile on Joe’s machine.
After discussing the issue, Tom realizes that he probably forgot to check in one of his classes, which causes the compilation process to fail.
The rest of the team is now blocked and can’t continue their work until Tom checks in the missing source file.
Acme has two different development groups, one specializing in building the web-based user interface and the other working on the server-side backend code.
Both teams sit together at Tom’s computer to run the compilation for the whole application, build a deliverable, and deploy it to a web server in a test environment.
The first cheers quickly fade when the team sees that some of the functionality isn’t working as expected.
Some of the URLs simply don’t resolve or result in an error.
Even though the team wrote some functional tests, they didn’t get exercised regularly in the IDE.
The quality assurance (QA) team is eager to get their hands on a first version of the application.
Benefits of project automation too happy about testing low-quality software.
With every fix the development team puts into place, they have to run through the same manual process.
The team stops to check new changes into version control, a new version is built from an IDE, and the deliverable is copied to the test server.
Each and every time, a developer is fully occupied and can’t add any other value to the company.
After weeks of testing and a successful demo to the investor, the QA team says the application is ready for prime time.
From experience, the team knows that the outcome of deploying an application is unpredictable due to unforeseen problems.
The infrastructure and runtime environment has to be set up, the database has to be prepared with seed data, the actual deployment of the application has to happen, and initial health monitoring needs to be performed.
Of course, the team has an action plan in place, but each of the steps has to be executed manually.
The following week, the CTO swings by the developers’ desks; he already has new ideas to improve the user experience.
A friend has told him about agile development, a time-boxed iterative approach for implementing and releasing software.
Tom and Joe look at each other, both horrified at the manual and repetitive work that lies ahead.
Together, they plan to automate each step of the implementation and delivery process to reduce the risk of failed builds, late integration, and painful deployments.
This story makes clear how vital project automation is for team success.
These days, time to market has become more important than ever.
Being able to build and deliver software in a repeatable and consistent way is key.
Having to manually perform steps to produce and deliver software is time-consuming and error-prone.
Frankly, as a developer and system administrator, you have better things to do than to handhold a compilation process or to copy a file from directory A to directory B.
Not only can you make mistakes along the way, manual intervention also takes away from the time you desperately need to get your actual work done.
Any step in your software development process that can be automated should be automated.
The actual building of your software usually follows predefined and ordered steps.
For example, you compile your source code first, then run your tests, and lastly assemble a deliverable.
You’ll need to run the same steps over and over again—every day.
The outcome of this process needs to be repeatable for everyone who runs the build.
You’ve seen that being able to run a build from an IDE is very limiting.
First of all, you’ll need to have the particular product installed on your machine.
Second, the IDE may only be available for a specific operating system.
An automated build shouldn’t require a specific runtime environment to work, whether this is an operating system or an IDE.
Optimally, the automated tasks should be executable from the command line, which allows you to run the build from any machine you want, whenever you want.
You saw at the beginning of this chapter that a user can request a build to be run.
A user can be any stakeholder who wants to trigger the build, like a developer, a QA team member, or a product owner.
Our friend Tom, for example, pressed the Compile button in his IDE whenever he wanted the code to be compiled.
You can also schedule your build to be executed at predefined times or when a specific event occurs.
The typical use case for on-demand automation is when a user triggers a build on his or her machine, as shown in figure 1.1
It’s common practice that a version control system (VCS) manages the versioning of the build definition and source code files.
In most cases, the user executes a script on the command line that performs tasks in a predefined order—for example, compiling source code, copying a file from directory A to directory B, or assembling a deliverable.
Usually, this type of automation is executed multiple times per day.
Figure 1.1 On-demand builds execute build definitions backed by a VCS.
If you’re practicing agile software development, you’re interested in receiving fast feedback about the health of your project.
You’ll want to know if your source code can be compiled without any errors or if there’s a potential software defect indicated by a failed unit or integration test.
This type of automation is usually triggered if code was checked into version control, as shown in figure 1.2
Think of scheduled automation as a time-based job scheduler (in the context of a Unix-based operation system, also known as a cron job)
It runs in particular intervals or at concrete times—for example, every morning at 1:00 a.m.
As with all cron jobs, scheduled automation generally runs on a dedicated server.
This kind of automation is particularly useful for generating reports or documentation for your project.
The practice that implements scheduled and triggered builds is commonly referred to as continuous integration (CI)
Figure 1.2 Build triggered by a check-in of files into VCS.
After identifying the benefits and types of project automation, it’s time to discuss the tools that allow you to implement this functionality.
Naturally, you may ask yourself why you’d need another tool to implement automation for your project.
You could just write the logic as an executable script, such as a shell script.
Think back to the goals of project automation we discussed earlier.
You want a tool that allows you to create a repeatable, reliable, and portable build without manual intervention.
A shell script wouldn’t be easily portable from a UNIX-based system to a Windows-based system, so it doesn’t meet your criteria.
What you need is a programming utility that lets you express your automation needs as executable, ordered tasks.
Let’s say you want to compile your source code, copy the generated class files into a directory, and assemble a deliverable that contains the class files.
A deliverable could be a ZIP file, for example, that can be distributed to a runtime environment.
Figure 1.4 shows the tasks and their execution order for the described scenario.
Each of these tasks represents a unit of work—for example, compilation of source code.
You can’t create the ZIP archive if the required class files haven’t been compiled.
A DAG is a data structure from computer science and contains the following two elements:
Node : A unit of work; in the case of a build tool, this is a task (for example, compiling source code)
Directed edge : A directed edge, also called an arrow, representing the relationship between nodes.
If a task defines dependent tasks, they’ll need to execute before the task itself can be executed.
Often this is the case because the task relies on the output produced by another task.
Here’s an example: to execute the task “assemble deliverable,” you’ll need to run its dependent tasks “copy class files to directory” and “compile source code.”
For example, if two different tasks depend on the task “source code compilation,” you only want to execute it once.
You may have noticed that the nodes are shown in an inverted order from the tasks in figure 1.4
This is because the order is determined by node dependencies.
As a developer, you won’t have to deal directly with the DAG representation of your build.
Later in this chapter, you’ll see how some Java-based build tools use these concepts in practice.
Figure 1.4 A common scenario of tasks executed in a predefined order.
It’s important to understand the interactions among the components of a build tool, the actual definition of the build logic, and the data that goes in and out.
Let’s discuss each of the elements and their particular responsibilities.
Figure 1.6 illustrates a build file that describes four tasks and how they depend on each other.
The tasks we discussed in the scenario earlier—compiling source code, copying files to a directory, and assembling a ZIP file—would be defined in the build file.
Oftentimes, a scripting language is used to express the build logic.
That’s why a build file is also referred to as a build script.
Some tasks may not need any input to function correctly, nor is creating an output considered mandatory.
Complex task dependency graphs may use the output of a dependent task as input.
Figure 1.7 demonstrates the consumption of inputs and the creation of outputs in a task graph.
We took a bunch of source code files as input, compiled them to classes, and assembled a deliverable as output.
Figure 1.6 The build file expresses the rules of your build expressed by tasks and their interdependencies.
Build tools of the deliverable only makes sense if you compiled the source code first.
The build engine processes the build file at runtime, resolves dependencies between tasks, and sets up the entire configuration needed to command the execution, as shown in figure 1.8
Once the internal model is built, the engine will execute the series of tasks in the correct order.
Some build tools allow you to access this model via an API to query for this information at runtime.
A dependency is generally an external, reusable library in the form of a JAR file (for example, Log4J for logging support)
The repository acts as storage for dependencies, and organizes and describes them by identifiers, such as name and version.
A typical repository can be an HTTP server or the local file system.
Figure 1.9 illustrates how the dependency manager fits into the architecture of a build tool.
The dependency manager can use metadata stored in the repository to automatically resolve transitive dependencies as well.
A build tool is not required to provide a dependency management component.
In this section, we look at two popular, Java-based build tools: Ant and Maven.
We’ll discuss their characteristics, see a sample script in action, and outline the shortcomings of each tool.
Let’s start with the tool that’s been around the longest—Ant.
Apache Ant (Another Neat Tool) is an open source build tool written in Java.
Its main purpose is to provide automation for typical tasks needed in Java projects, such as compiling source files to classes, running unit tests, packaging JAR files, and creating Javadoc documentation.
Additionally, it provides a wide range of predefined tasks for file system and archiving operations.
If any of these tasks don’t fulfill your requirements, you can extend the build with new tasks written in Java.
While Ant’s core is written in Java, your build file is expressed through XML, which makes it portable across different runtime environments.
Ant does not provide a dependency manager, so you’ll need to manage external dependencies yourself.
However, Ant integrates well with another Apache project called Ivy, a full-fledged, standalone dependency manager.
Integrating Ant with Ivy requires additional effort and has to be done manually for each individual project.
A build script consists of three basic elements: the project, multiple targets, and the used tasks.
Figure 1.10 illustrates the relationship between each of the elements.
In Ant, a task is a piece of executable code— for example, for creating a new directory or moving a file.
Within your build script, use a task by its predefined XML tag name.
The task’s behavior can be configured by its exposed attributes.
The following code snippet shows the usage of the javac Ant task for compiling Java source code within your build script:
While Ant ships with a wide range of predefined tasks, you can extend your build script’s capabilities by writing your own task in Java.
A target is a set of tasks you want to be executed.
When running Ant on the command line, provide the name of the target(s) you want to execute.
By declaring dependencies between targets, a whole chain of commands can be created.
Mandatory to all Ant projects is the overarching container, the project.
It’s the top-level element in an Ant script and contains one or more targets.
The following code snippet shows the project in relation to the targets:
Figure 1.10 Ant’s hierarchical build script structure with the elements project, target, and task.
Source and destination directories are configured by attributes srcdir and destdir; compile Java source files located in directory src and put class files into directory dest.
Target named init that used task mkdir to create directory build.
Target named compile for compiling Java source code via javac Ant task.
This target depends on target init, so if you run it on the command line, init will be executed first.
With a basic understanding of Ant’s hierarchical structure, let’s look at a full-fledged scenario of a sample build script.
Your Java source code has a dependency on a class from the external library Apache Commons Lang.
You tell the compiler about it by referencing the library’s JAR file in the classpath.
After compiling the code, you want to assemble a JAR file.
Each unit of work, source code compilation, and JAR assembly will be grouped in an individual target.
You’ll also add two more targets for initializing and cleaning up the required output directories.
The structure of the Ant build script you’ll create is shown in figure 1.11
Project encloses one or more targets and defines optional attributes, such as the name, to describe the project.
Figure 1.11 Hierarchical project structure of sample Ant build script.
It’s time to implement this example as an Ant build script.
The following listing shows the whole project and the targets required to achieve your goal.
Ant doesn’t impose any restrictions on how to define your build’s structure.
This makes it easy to adapt to existing project layouts.
For example, the source and output directories in the sample script have been chosen arbitrarily.
It would be very easy to change them by setting a different value to their corresponding properties.
The same is true for target definition; you have full flexibility to choose which logic needs to be executed per target and the order of execution.
Using XML as the definition language for your build logic results in overly large and verbose build scripts compared to build tools with a more succinct definition language.
Complex build logic leads to long and unmaintainable build scripts.
Ant doesn’t give you any guidelines on how to set up your project.
In an enterprise setting, this often leads to a build file that looks different every time.
Listing 1.1 Ant script with targets for compiling source code and assembling JAR file.
Sets global properties for this build, like source, output, and distribution directories.
Every new developer on the project needs to understand the individual structure of a build.
You want to know how many classes have been compiled or how many tasks have been executed in a build.
Ant doesn’t expose an API that lets you query information about the in-memory model at runtime.
Using Ant without Ivy makes it hard to manage dependencies.
Oftentimes, you’ll need to check your JAR files into version control and manage their organization manually.
Using Ant across many projects within an enterprise has a big impact on maintainability.
With flexibility comes a lot of duplicated code snippets that are copied from one project to another.
The Maven team realized the need for a standardized project layout and unified build lifecycle.
Maven picks up on the idea of convention over configuration, meaning that it provides sensible default values for your project configuration and its behavior.
The project automatically knows what directories to search for source code and what tasks to perform when running the build.
You can set up a full project with a few lines of XML as long as your project adheres to the default values.
As an extra, Maven also has the ability to generate HTML project documentation that includes the Javadocs for your application.
Maven’s core functionality can be extended by custom logic developed as plugins.
The community is very active, and you can find a plugin for almost every aspect of build support, from integration with other development tools to reporting.
If a plugin doesn’t exist for your specific needs, you can write your own extension.
For example, Java application source code sits in the directory src/main/java.
Every project knows exactly which steps to perform to build, package, and distribute an application, including the following functionality:
Every step in this build lifecycle is called a phase.
The phase you want to execute is defined when running the build on the command line.
Java build tools you call the phase for packaging the application, Maven will automatically determine that the dependent phases like source code compilation and running tests need to be executed beforehand.
Figure 1.13 shows the predefined phases of a Maven build and their order of execution.
For example, if your project requires the popular Java library Hibernate, you simply define its unique artifact coordinates, such as organization, name, and version, in the dependencies configuration block.
The following code snippet shows how to declare a dependency on version 4.1.7
Figure 1.12 Maven’s default project layout defines where to find Java source code, resource files, and test code.
At runtime, the declared libraries and their transitive dependencies are downloaded by Maven’s dependency manager, stored in the local cache for later reuse, and made available to your build (for example, for compiling source code)
Maven preconfigures the use of the repository, Maven Central, to download dependencies.
Subsequent builds will reuse an existing artifact from the local cache and therefore won’t contact Maven Central.
Maven Central is the most popular binary artifact repository in the Java community.
You can also declare a dependency on other Maven projects.
This need arises if you decompose software into modules, which are smaller components based on associated functionality.
Figure 1.15 shows an example of a traditional three-layer modularized architecture.
In this example, the presentation layer contains code for rendering data in a webpage, the business layer models real-life business objects, and the integration layer retrieves data from a database.
All dependencies of project must be declared within <dependencies> tag.
Group identifier of dependency, usually an organization or company name.
Version of a dependency, usually consisting of classifiers like minor and major version.
Figure 1.14 Maven’s interaction with Maven Central to resolve and download dependencies for your build.
Keep in mind that you stick to the default conventions here, so Maven will look for the source code in the directory src/ main/java instead of src.
Maven proposes a default structure and lifecycle for a project that often is too restrictive and may not fit your project’s needs.
You’ll need to learn about Mojos (Maven’s internal extension API), how to provide a plugin descriptor (again in XML), and about specific annotations to provide the data needed in your extension implementation.
In the last section, we examined the features, advantages, and shortcomings of the established build tools Ant and Maven.
It became clear that you often have to compromise on the supported functionality by choosing one or the other.
Either you choose full flexibility and extensibility but get weak project standardization, tons of boilerplate code, and no support for dependency management by picking Ant; or you go with Maven, which offers a convention over configuration approach and a seamlessly.
Declared dependency on Apache Commons Lang library with version 3.1; scope of a dependency determines lifecycle phase it’s applied to.
Wouldn’t it be great if a build tool could cover a middle ground? Here are some features that an evolved build tool should provide:
This will matter if you have long-running builds (for example, two hours or longer), which is the case for some big enterprise projects.
This book will introduce you to a tool that does provide all of these great features: Gradle.
Together, we’ll cover a lot of ground on how to use it and exploit all the advantages it provides.
Life for developers and QA personnel without project automation is repetitive, tedious, and error-prone.
Every step along the software delivery process—from source code compilation to packaging the software to releasing the deliverable to test and production environments—has to be done manually.
Project automation helps remove the burden of manual intervention, makes your team more efficient, and leads the way to a push-button, fail-safe software release process.
You learned that the different types of project automation are not exclusive.
A build tool is one of the enablers for project automation.
It allows you to declare the ordered set of rules that you want to execute when initiating a build.
We discussed the moving parts of a build tool by analyzing its anatomy.
The build engine (the build tool executable) processes the rule set defined in the build script and translates it into executable tasks.
Each task may require input data to get its job done.
The dependency manager is an optional component of the build tool architecture that lets you declare references to external libraries that your build process needs to function correctly.
We saw the materialized characteristics of build tools in action by taking a deeper look at two popular Java build tool implementations: Ant and Maven.
Ant provides a very flexible and versatile way of defining your build logic, but doesn’t provide guidance on a standard project layout or sensible defaults to tasks that repeat over and over in projects.
It also doesn’t come with an out-of-the-box dependency manager, which requires you to manage external dependencies yourself.
Maven, on the other hand, follows the convention over configuration paradigm by supporting sensible default configuration for your project as well as a standardized build lifecycle.
Automated dependency management for external libraries and between Maven projects is a built-in feature.
Maven falls short on easy extensibility for custom logic and support for nonconventional project layouts and tasks.
You learned that an advanced build tool needs to find a middle ground between flexibility and configurable conventions to support the requirements of modern-day software projects.
In the next chapter, we’ll identify how Gradle fits into the equation.
For years, builds had the simple requirements of compiling and packaging software.
But the landscape of modern software development has changed, and so have the needs for build automation.
Today, projects involve large and diverse software stacks, incorporate multiple programming languages, and apply a broad spectrum of testing strategies.
With the rise of agile practices, builds have to support early integration of code as well as frequent and easy delivery to test and production environments.
Established build tools continuously fall short in meeting these goals in a simple but customizable fashion.
How many times have your eyes glazed over while looking at XML to figure out how a build works? And why can’t it be easier to add custom logic to your build? All too often, when adding on to a build script, you can’t Next-generation builds with Gradle.
Why Gradle? Why now? shake the feeling of implementing a workaround or hack.
There has to be a better way of doing these things in an expressive and maintainable way.
Gradle is the next evolutionary step in JVM-based build tools.
It draws on lessons learned from established tools like Ant and Maven and takes their best ideas to the next level.
Because Gradle is a JVM native, it allows you to write custom logic in the language you’re most comfortable with, be it Java or Groovy.
In the Java world, an unbelievably large number of libraries and frameworks are available.
Dependency management is used to automatically download these artifacts from a repository and make them available to your application code.
Having learned from the shortcomings of existing dependency management solutions, Gradle provides its own implementation.
Not only is it highly configurable, it also strives to be as compatible as possible with existing dependency management infrastructures (like Maven and Ivy)
Gradle’s ability to manage dependencies isn’t limited to external libraries.
As your project grows in size and complexity, you’ll want to organize the code into modules with clearly defined responsibilities.
Gradle provides powerful support for defining and organizing multiproject builds, as well as modeling dependencies between projects.
I know, all of this sounds promising, but you’re still stuck with your legacy build.
Gradle doesn’t leave you in the dust, but makes migration easy.
Ant gets shipped with the runtime and therefore doesn’t require any additional setup.
Gradle provides teams with the ability to apply their accumulated Ant knowledge and investment in build infrastructure.
Imagine the possibilities of using existing Ant tasks and scripts directly in your Gradle build scripts.
To get started with Gradle, all you need to bring to the table is a good understanding of the Java programming language.
If you’re new to project automation or haven’t used a build tool before, chapter 1 is a good place to start.
This book will teach you how to effectively use Gradle to build and deliver real-world projects.
In this chapter, we’ll compare existing JVM-language build tools with the features Gradle has to offer.
Later, you’ll learn how Gradle can help you automate your software delivery process in the context of a continuous delivery deployment pipeline.
To get a first taste of what it’s like to use Gradle, you’ll install the runtime, write a simple build script, and run it on the command line.
Join me on an exciting journey as we explore the world of Gradle.
Let’s say you want to copy a file to a specific location under the condition that you’re building the release version of your project.
To identify the version, you check a string in the metadata describing your project.
If it matches a specific numbering scheme (for example, 1.0-RELEASE), you copy the file from point A to point B.
From an outside perspective, this may sound like a trivial task.
If you have to rely on XML, the build language of many traditional tools, expressing this simple logic becomes a nightmare.
The build tool’s response is to add scripting functionality through nonstandard extension mechanisms.
You end up mixing scripting code with XML or invoking external scripts from your build logic.
It’s easy to imagine that you’ll need to add more and more custom code over time.
As a result, you inevitably introduce accidental complexity, and maintainability goes out the window.
Wouldn’t it make sense to use an expressive language to define your build logic in the first place?
Maven follows the paradigm of convention over configuration by introducing a standardized project layout and build lifecycle for Java projects.
That’s a great approach if you want to ensure a unified application structure for a greenfield project—a project that lacks any constraints imposed by prior work.
However, you may be the lucky one who needs to work on one of the many legacy projects that follow different conventions.
One of the conventions Maven is very strict about is that one project needs to produce one artifact, such as a JAR file.
But how do you create two different JAR files from one source tree without having to change your project structure? Just for this purpose, you’d have to create two separate projects.
Again, even though you can make this happen with a workaround, you can’t shake off the feeling that your build process will need to adapt to the tool, not the tool to your build process.
These are only some of the issues you may have encountered with existing solutions.
Often you’ve had to sacrifice nonfunctional requirements to model your enterprise’s automation domain.
But enough with the negativity—let’s see how Gradle fits into the build tool landscape.
Let’s look at how build tools have evolved over the years.
As I discussed in chapter 1, two tools have dominated building Java projects: Ant and Maven.
Over the course of years, both tools significantly improved and extended their feature set.
But even though both are highly popular and have become industry standards, they have one weak point: build logic has to be described in XML.
As a build script grows in complexity, maintaining the build code becomes a nightmare.
Each element of work (a target in Ant’s lingo) can be combined and reused.
Why Gradle? Why now? single units of work into full workflows.
For example, you might have one target for compiling Java source code and another one for creating a JAR file that packages the class files.
Building a JAR file only makes sense if you first compiled the source code.
In Ant, you make the JAR target depend on the compile target.
Ant doesn’t give any guidance on how to structure your project.
Though it allows for maximum flexibility, Ant makes each build script unique and hard to understand.
External libraries required by your project were usually checked into version control, because there was no sophisticated mechanism to automatically pull them from a central location.
Early versions of Ant required a lot of discipline to avoid repetitive code.
As a result, the bad coding practice of copying and pasting code was the only viable option.
It provided a standardized project and directory structure, as well as dependency management.
If you want to break out of Maven’s conventions, writing a plugin, called a Mojo, is usually the only solution.
The name Mojo might imply a straightforward, easy, and sexy way to extend Maven; in reality, writing a plugin in Maven is cumbersome and overly complex.
Later, Ant caught up with Maven by introducing dependency management through the Apache library Ivy, which can be fully integrated with Ant to declaratively specify dependencies needed for your project’s compilation and packaging process.
Maven’s dependency manager, as well as Ivy, support resolving transitive dependencies.
When I speak of transitive dependencies, I mean the graph of libraries required by your specified dependencies.
A typical example of a transitive dependency would be the XML parser library Xerces that requires the XML APIs library to function correctly.
Projects consisting of multiple modules could define their dependencies on each other.
These days a lot of people are looking for alternatives to established build tools.
We see a shift from using XML to a more expressive and readable language to define builds.
A build tool that carries on this idea is Gant, a DSL on top of Ant written in Groovy.
Using Gant, users can now combine Groovy’s language features with their existing knowledge of Ant without having to write XML.
Even though it wasn’t part of the core Maven project, a similar approach was proposed by the project Maven Polyglot that allows you to write your build definition logic, which is the project object model (POM) file, in Groovy, Ruby, Scala, or Clojure.
We’re on the cusp of a new era of application development: polyglot programming.
Many applications today incorporate multiple programming languages, each of which is best suited to implement a specific problem domain.
It’s not uncommon to face projects that use client-side languages like JavaScript that communicate with a mixed, multilingual backend like Java, Groovy, and Scala, which in turn calls off to a C++ legacy application.
JavaScript needs to be merged, minified, and zipped, and your server-side and legacy code needs to be compiled, packaged, and deployed.
Gradle fits right into that generation of build tools and satisfies many requirements of modern build tools (figure 2.1)
It provides an expressive DSL, a convention over configuration approach, and powerful dependency management.
It makes the right move to abandon XML and introduce the dynamic language Groovy to define your build logic.
Sounds compelling, doesn’t it? Keep reading to learn about Gradle’s feature set and how to get your boss on board.
If you’re a developer, automating your project is part of your day-to-day business.
Don’t you want to treat your build code like any other piece of software that can be extended, tested, and maintained? Let’s put software engineering back into the build.
Gradle build scripts are declarative, readable, and clearly express their intention.
It’s impressive to see how much less code you need to write in Gradle to achieve the same goal.
Where other build tools like Maven propose project layouts that are “my way or the highway,” Gradle’s DSL allows for flexibility by adapting to nonconventional project structures.
Figure 2.1 Gradle combines the best features from other build tools.
Why Gradle? Why now? Never change a running system, you say? Your team already spent a lot of time on establishing your project’s build code infrastructure.
Gradle doesn’t force you to fully migrate all of your existing build logic.
Good integration with other tools like Ant and Maven is at the top of Gradle’s priority list.
ThoughtWorks, a highly regarded software development consultancy, periodically publishes a report on emerging technologies, languages, and tools—their so-called technology radar.
The goal of the technology radar is to help decision makers in the software industry understand trends and their effect on the market.
Figure 2.2 Comparing build script size and readability between Maven and Gradle.
Gradle found adopters early on, even before a 1.0 version was released.
Popular open source projects like Groovy and Hibernate completely switched to Gradle as the backbone for their builds.
Every Android project ships with Gradle as the default build system.
Companies like Orbitz, EADS, and Software AG embraced Gradle as well, to name just a few.
VMware, the company behind Spring and Grails, made significant investments in choosing Gradle.
Many of their software products, such as the Spring framework and Grails, are literally built on the trust that Gradle can deliver.
Let’s take a closer look at what sets Gradle apart from its competitors: its compelling feature set (see figure 2.3)
To summarize, Gradle is an enterprise-ready build system, powered by a declarative and expressive Groovy DSL.
It combines flexibility and effortless extendibility with the idea of convention over configuration and support for traditional dependency management.
While syntax issues can be dealt with through generation, plug-in architectures severely limit the ability for build tools to grow gracefully as projects become more complex.
Gradle’s compelling feature set (Gradleware) and strong community involvement, Gradle is becoming the numberone choice build solution for many open source projects and enterprises.
The key to unlocking Gradle’s power features within your build script lies in discovering and applying its domain model, as shown in figure 2.4
As you can see in the figure, a build script directly maps to an instance of type Project in Gradle’s API.
Figure 2.4 Build scripts apply the Gradle DSL and have access to its deep API.
Like most APIs in the Java world, it’s available as HTML Javadoc documentation on Gradle’s website at http://www.gradle.org/docs/current/javadoc/index.html.
Without knowing it, you generate an object representation of your build logic in memory.
In chapter 4, we’ll explore many of Gradle’s API classes and how they’re represented in your build script.
Each element in a Gradle script has a one-to-one representation with a Java class; however, some of the elements have been sugarcoated with a sprinkle of Groovy syntax.
Having a Groovy-fied version of a class in many cases makes the code more compact than its Java counterpart and allows for using new language features like closures.
Gradle can’t know all the requirements specific to your enterprise build.
By exposing hooks into lifecycle phases, Gradle allows for monitoring and configuring your build script’s execution behavior.
Let’s assume you have the very unique requirement of sending out an email to the development team whenever a unit test failure occurs.
The way you want to send an email (for example, via SMTP or a third-party email service provider) and the list of recipients are very specific to your build.
Other builds using Gradle may not be interested in this feature at all.
By writing a custom test listener that’s notified after the test execution lifecycle event, you can easily incorporate this feature for your build.
Gradle establishes a vocabulary for its model by exposing a DSL implemented in Groovy.
When dealing with a complex problem domain, in this case the task of building software, being able to use a common language to express your logic can be a powerful tool.
Most common to builds is the notation of a unit of work that you want to get executed.
Part of Gradle’s standard DSL is the ability to define tasks very specific to compiling and packaging Java source code.
It’s a language for building Java projects with its own vocabulary that doesn’t need to be relevant to other contexts.
Another example is the way you can express dependencies to external libraries, a very common problem solved by build tools.
Out-of-the-box Gradle provides you with two configuration blocks for your build script that allow you to define the dependencies and repositories that you want to retrieve them from.
If the standard DSL elements don’t fit your needs, you can even introduce your own vocabulary through Gradle’s extension mechanism.
This may sound a little nebulous at first, but once you’re past the initial hurdle of learning the build language, creating maintainable and declarative builds comes easy.
You may want to change the behavior of an existing task or add your own idioms for describing your business domain.
Prominent build tools like Ant and Maven define their build logic through XML.
As we all know, XML is easy to read and write, but can become a maintenance nightmare.
Under the hood, Gradle’s DSL is written with Groovy providing syntactic sugar on top of Java.
Being able to use a programming language to express your build needs is a major plus.
You don’t have to be a Groovy expert to get started.
Because Groovy is written on top of Java, you can migrate gradually by trying out its language features.
You could even write your custom logic in plain Java—Gradle couldn’t care less.
Battlescarred Groovy veterans will assure you that using Groovy instead of Java will boost your productivity by orders of magnitude.
One of Gradle’s big ideas is to give you guidelines and sensible defaults for your projects.
Every Java project in Gradle knows exactly where source and test class file are supposed to live, and how to compile your code, run unit tests, generate Javadoc reports, and create a distribution of your code.
All of these tasks are fully integrated into the build lifecycle.
If you stick to the convention, there’s only minimal configuration effort on your part.
Figure 2.5 illustrates how Gradle introduces conventions and lifecycle tasks for Java projects.
Default tasks are provided that make sense in the context of a Java project.
For example, you can compile your Java production source code, run tests, and assemble a JAR file.
It defines where to find production source code, resource files, and test code.
The same concept applies to other project archetypes like Scala, Groovy, web projects, and many more.
The build script developer doesn’t need to know how this is working under the hood.
Instead, you can concentrate on what needs to be configured.
Gradle’s conventions are similar to the ones provided by Maven, but they don’t leave you feeling boxed in.
Maven is very opinionated; it proposes that a project only contains one Java source directory and only produces one single JAR file.
Gradle allows you to easily break out of the conventions.
On the opposite side of the spectrum, Ant never gave you a lot of guidance on how to structure your build script, allowing for a maximum level of flexibility.
Gradle takes the middle ground by offering conventions combined with the ability to easily change them.
All too often, your application code uses a third-party library providing existing functionality to solve a specific problem.
Why would you want to reinvent the wheel by implementing a persistence framework if Hibernate already exists? Within an organization, you may be the consumer of a component or module implemented by a different team.
External dependencies are accessible through repositories, and the type of repository is highly dependent on what your company prefers.
Options range from a plain file system to a full-fledged enterprise repository.
External dependencies may have a reference to other libraries or resources.
Gradle provides an infrastructure to manage the complexity of resolving, retrieving, and storing dependencies.
Once they’re downloaded and put in your local cache, they’re made available to your project.
Do you remember the last time your coworker said, “But it works on my box”? Builds have to produce the same result on different machines, independent of the contents of your local cache.
Dependency managers like Ivy and Maven in their current implementation cannot fully guarantee reproducibility.
Figure 2.5 In Gradle, Java projects are build by convention with sensible defaults.
Changing the defaults is easy and achieved through convention properties.
Gradle’s compelling feature set and stored in the local cache, it doesn’t take into account the artifact’s origin.
In situations where the repository is changed for a project, the cached dependency is considered resolved, even though the artifact’s content may be slightly different.
At worst, this will cause a failing build that’s extremely hard to debug.
Another common complaint specific to Ivy is the fact that dependency snapshot versions, artifacts currently under development with the naming convention –SNAPSHOT, aren’t updated correctly in the local cache, even though it changed on the repository and is marked as changing.
There are many more scenarios where current solutions fall short.
Gradle provides its own configurable, reliable, and efficient dependency management solution.
Large enterprise projects usually consist of multiple modules to separate functionality.
In the Gradle world, each of the submodules is considered a project that can define dependencies to external libraries or other modules.
Gradle figures out for you which of the subproject dependencies need to be rebuilt, without having to store a subproject’s artifact in the local cache.
For some companies, a large project with hundreds of modules is reality.
Building and testing minor code changes can consume a lot of time.
You may know from personal experience that deleting old classes and resources by running a cleanup task is a natural reflex.
All too often, you get burned by your build tool not picking up the changes and their dependencies.
What you need is a tool that’s smart enough to only rebuild the parts of your software that actually changed.
Gradle supports incremental builds by specifying task inputs and outputs.
It reliably figures out for you which tasks need to be skipped, built, or partially rebuilt.
The same concept translates to multimodule projects, called partial builds.
Because your build clearly defines the dependencies between submodules, Gradle takes care of rebuilding only the necessary parts.
Automated unit, integration, and functional tests are part of the build process.
It makes sense to separate short-running types of tests from the ones that require setting up resources or external dependencies to be run.
This feature is fully configurable and ensures that you’re actually taking advantage of your processor’s cores.
Gradle is going to support distributing test execution to multiple machines in a future version.
I’m sorry to tell you, but the days of reading your Twitter feed between long builds are gone.
That means starting a new Gradle process each time, loading all its internal dependencies, and running the build logic.
You’ll notice that it usually takes a couple of seconds before your script actually starts to execute.
To improve the startup performance, Gradle can be run in daemon mode.
In practice, the Gradle command forks a daemon process, which not only executes your build, but also keeps running in the background.
As a result, you’ll notice a far snappier initial build execution.
Most enterprise builds are not alike, nor do they solve the same problems.
Once you’re past the initial phase of setting up your basic scripts, you’ll want to implement custom logic.
Gradle is not opinionated about the way you implement that code.
Instead, it gives you various choices to pick from, depending on your specific use case.
The easiest way to implement custom logic is by writing a task.
Tasks can be defined directly in your build script without special ceremony.
If you feel like complexity takes over, you may want to explore the option of a custom task that allows for writing your logic within a class definition, making structuring your code easy and maintainable.
If you want to share reusable code among builds and projects, plugins are your best friend.
Representing Gradle’s most powerful extension mechanism, plugins give you full access to Gradle’s API and can be written, tested, and distributed like any other piece of software.
Writing a plugin is surprisingly easy and doesn’t require a lot of additional descriptors.
Wouldn’t it be a huge timesaver to be able to integrate with existing build tools? Gradle plays well with its predecessors Ant, Maven, and Ivy, as shown in figure 2.6
If you’re coming from Ant, Gradle doesn’t force you to fully migrate your build infrastructure.
Instead, it allows you to import existing build logic and reuse standard Ant tasks.
Gradle builds are 100% compatible with Maven and Ivy repositories.
Gradle provides a converter for existing Maven builds that can translate the build logic into a Gradle build script.
Existing Ant scripts can be imported into your Gradle build seamlessly and used as you’d use any other external Gradle script.
Gradle ships with the Ant libraries and exposes a helper class to your scripts.
Figure 2.6 Gradle provides deep integration with other build tools and opens the door to gradually migrate your existing Ant or Maven build.
Gradle’s compelling feature set called AntBuilder, which fully blends into Gradle’s DSL.
It still looks and feels like Ant’s XML, but without the pointy brackets.
Ant users will feel right at home, because they don’t have to transition to Gradle syntax right away.
You can take baby steps by reusing your existing Ant logic while using Gradle’s benefits at the same time.
Gradle aims to reach a similar depth of integration with Maven.
At the time of writing, this hasn’t been realized yet.
In the long run, Maven POMs and plugins will be treated as Gradle natives.
Maven and Ivy repositories have become an important part of today’s build infrastructure.
Imagine a world without Maven Central to help access specific versions of your favorite project dependencies.
Retrieving dependencies from a repository is only one part of the story; publishing to them is just as important.
With a little configuration, Gradle can upload your project’s artifact for companywide or public consumption.
Gradle is free to use and ships with the Apache License 2.0
After its first release in April 2008, a vibrant community quickly started to form around it.
Over the past five years, open source developers have made major contributions to Gradle’s core code base.
Being hosted on GitHub turned out to be very beneficial to Gradle.
Code changes can be submitted as pull requests and undergo a close review process by the core committers before making it into the code base.
If you’re coming from other build tools like Maven, you may be used to a wide range of reusable plugins.
Apart from the standard plugins shipped with the runtime, the Gradle community releases new functionality almost daily.
Throughout the book, you’ll use many of the standard plugins shipped with Gradle.
Appendix A gives a broader spectrum on standard as well as third-party plugins.
Every community-driven software project needs a forum to get immediate questions answered.
Gradle connects with the community through the Gradle forum at http://forums.gradle.org/gradle.
You can be sure you’ll get helpful responses to your questions on the same day.
Gradleware is the technical service and support company behind Gradle.
Not only does it provide professional advice for Gradle itself, it aims for a wide range of enterprise automation consulting.
The company is backed by high-caliber engineers very experienced in the domain.
Recently, Gradleware started to air free webinars to spark interest for newcomers and deepen knowledge for experienced Gradle users.
Don’t you hate having to install a new runtime for different projects? Gradle Wrapper to the rescue! It allows for downloading and installing a fresh copy of the Gradle runtime from a specified repository on any machine you run the build on.
This process is automatically triggered on the first execution of the build.
The Wrapper is especially useful for sharing your builds with a distributed team or running them on a CI platform.
Using command-line options, you can control everything from specifying the log level, to excluding tests, to displaying help messages.
Gradle allows for running commands in an abbreviated, camel-cased form.
In practice, a command named runMyAwesomeTask would be callable with the abbreviation rMAT.
Handy, isn’t it? Even though this book presents most of its examples by running commands in a shell, bear in mind that Gradle provides an out-of-the-box graphical user interface.
Being able to build your source code is only one aspect of the software delivery process.
More importantly, you want to release your product to a production environment to deliver business value.
Along the way, you want to run tests, build the distribution, analyze the code for quality-control purposes, potentially provision a target environment, and deploy to it.
First and foremost, delivering software manually is slow, error-prone, and nerve-wracking.
I’m sure every one of us hates the long nights due to a deployment gone wrong.
With the rise of agile methodologies, development teams are able to deliver software faster.
Release cycles of two or three weeks have become the norm.
Some organizations like Etsy and Flickr even ship code to production several times a day! Optimally, you want to be able to release software by selecting the target environment simply by pressing a button.
Practices like automated testing, CI, and deployment feed into the general concept of continuous delivery.
In this book, we’ll look at how Gradle can help get your project from build to deployment.
It’ll enable you to automate many of the tasks required to implement continuous delivery, be they compiling your source code, deploying a deliverable, or calling external tools that help you with implementing the process.
Continuous delivery introduces the concept of a deployment pipeline, also referred to as the build pipeline.
A deployment pipeline represents the technical implementation of the process for getting software from version control into your production environment.
The process consists of multiple stages, as shown in figure 2.7
Automated acceptance test stage : Asserts that functional and nonfunctional requirements are met by running automated tests.
Manual test stage : Verifies that the system is actually usable in a test environment.
Usually, this stage involves QA personnel to verify requirements on the level of user stories or use cases.
Release stage : Either delivers the software to the end user as a packaged distribution or deploys it to the production environment.
Let’s see what stages of the deployment pipeline can benefit from project automation.
It’s obvious that the manual test stage can be excluded from further discussion, because it only involves manual tasks.
This book mainly focuses on using Gradle in the commit and automated acceptance test stages.
Figure 2.8 shows the order of tasks within each of the stages.
While there are no hard rules that prevent you from skipping specific tasks, it’s recommended that you follow the order.
For example, you could decide to compile your code, create the distribution, and deploy it to your target environment without running any tests or static code analysis.
However, doing so increases the risk of undetected code defects and poor code quality.
Topics like infrastructure provisioning, automated deployment, and smoke testing can also be applied to the release stage.
In practice, applying these techniques to a production environment is more complex than in a controlled test environment.
In a production environment, you may have to deal with clustered and distributed server infrastructures, zero-downtime release rollouts, and automated rollbacks to the previous release.
Covering these advanced topics would go beyond the scope of this book.
But enough pure theory—let’s get your feet wet by installing Gradle on your machine and building your first project.
In chapter 3, we’ll go even further by exploring how to implement and run a complex Java project using Gradle.
As a prerequisite, make sure you’ve already installed the JDK with a version of 1.5 or higher.
Even though some operating systems provide you with an out-of-the-box Java installation, make sure you have a valid version installed on your system.
You can download the distribution directly from the Gradle homepage at http://gradle.org/downloads.
As a beginner to the tool, it makes sense to choose the ZIP file that includes the documentation and a wide range of source code examples to explore.
Unzip the downloaded file to a directory of your choice.
To reference your Gradle runtime in the shell, you’ll need to create the environment variable GRADLE_HOME and add the binaries to your shell’s execution path:
These instructions assume that you installed Gradle in the directory /opt/gradle:
You’ll verify that Gradle has been installed correctly and is ready to go.
To check the version of the installed runtime, issue the command gradle –v in your shell.
You should see meta-information about the installation, your JVM, and the operating system.
The following example shows the version output of a successful Gradle 1.7 installation.
Setting Gradle’s JVM options Like every other Java application, Gradle shares the same JVM options set by the environment variable JAVA_OPTS.
If you want to pass arguments specifically to the Gradle runtime, use the environment variable GRADLE_OPTS.
Let’s say you want to increase the default maximum heap size to 1 GB.
The preferred way to do that is to add the variable to the Gradle startup script under $GRADLE_HOME/bin.
Now that you’re all set, you’ll implement a simple build script with Gradle.
Even though most of the popular IDEs provide a Gradle plugin, all you need now is your favorite editor.
The default naming convention for a Gradle build script is build.gradle.
When executing the command gradle in a shell, Gradle looks for a file with that exact name.
If it can’t be located, the runtime will display a help message.
Let’s set the lofty goal of creating the typical “Hello world!” example in Gradle.
Within that script, define a single atomic piece of work.
To print the message “Hello world!” make use of Gradle’s lingua franca, Groovy, by adding the println command to the task’s action doLast.
As expected, you see the output “Hello world!” when running the script.
By defining the optional command-line option quiet with –q, you tell Gradle to only output the task’s output.
Gradle allows for specifying the same logic in a more concise way.
The left shift operator << is a shortcut for the action doLast.
The following snippet shows a modified version of the first example:
I’ll give you a taste of more advanced features in the example build script shown in the following listing.
Let’s strengthen our belief in Gradle by exercising a little group therapy session.
You may not notice it at first, but there’s a lot going on in this listing.
You introduced the keyword dependsOn to indicate dependencies between tasks d.
Gradle makes sure that the depended-on task will always be executed before the task that defines the dependency.
Under the hood, dependsOn is actually a method of a task.
Chapter 4 will cover the internals of tasks, so we won’t dive into too much detail here.
A feature we’ve talked about before is Gradle’s tight integration with Ant B.
Because you have full access to Groovy’s language features, you can also print your message in a method named chant()
Every script is equipped with a property called ant that grants direct access to Ant tasks.
In this example, you print out the message “Repeat after me” using the Ant task echo to start the therapy session.
A nifty feature Gradle provides is the definition of dynamic tasks, which specify their name at runtime.
Groovy automatically exposes an implicit variable named it to indicate the loop iteration index.
For the first iteration, the task would be called yayGradle0
As shown in figure 2.10 Gradle executed the tasks in the correct order.
You may have noticed that the example omitted the quiet command-line option, which gives more information on the tasks run.
Thanks to your group therapy, you got rid of your deepest fears that Gradle will be just another build tool that can’t deliver.
For now, let’s get more accustomed to Gradle’s command line.
In the previous sections, you executed the tasks helloWorld and groupTherapy on the command line, which is going to be your tool of choice for running most examples throughout this book.
Even though using an IDE may seem more convenient to newcomers, a deep understanding of Gradle’s command-line options and helper tasks will make you more efficient and productive in the long run.
In the last section I showed you how to run a specific task using the gradle command.
Running a task requires you to know the exact name.
Wouldn’t it be great if Gradle could tell you which tasks are available without you having to look at the source code? Gradle provides a helper task named tasks to introspect your build script and display each available task, including a descriptive message of its purpose.
Running gradle tasks in quiet mode produces the following output:
To see all tasks and more detail, run with --all.
Build setup tasks help you initialize the Gradle build (for example, generate the build.gradle file)
Help task group listing task names and their descriptions separated by hyphen.
Uncategorized tasks that are not assigned to a task group.
Tasks without descriptions aren’t self-expressive; in chapter 3 you’ll learn how to add an appropriate task description.
Gradle provides the concept of a task group, which can be seen as a cluster of tasks assigned to that group.
Out of the box, each build script exposes the task group Help tasks B without any additional work from the developer.
If a task doesn’t belong to a task group, it’s displayed under Other tasks c.
You may wonder what happened to the other tasks that you defined in your build script.
On the bottom of the output, you’ll find a note that you can get even more details about your project’s tasks by using the --all option.
The --all option is a great way to determine the execution order of a task graph before actually executing it.
To reduce the noise, Gradle is smart enough to hide tasks that act as dependencies to a root task B.
For better readability, dependent tasks are displayed indented and ordered underneath the root task.
In the previous examples, you told Gradle to execute one specific task by adding it as an argument to the command gradle.
Gradle’s command-line implementation will in turn make sure that the task and all its dependencies are executed.
You can also execute multiple tasks in a single build run by defining them as command-line parameters.
Indented names of dependent tasks listed in order of execution.
Tasks are always executed just once, no matter whether they’re specified on the command line or act as a dependency for another task.
You see the same output as if you’d just run gradle groupTherapy.
The correct order was preserved and each of the tasks was only executed once.
If you wanted to run the previous example in the abbreviated form, you’d just have to type gradle yG0 gT.
This is especially useful if you’re dealing with very long task names or multiple task arguments.
Keep in mind that the task name abbreviation has to be unique to enable Gradle to identify the corresponding task.
Using the abbreviation gT in a build that defines the tasks groupTherapy and generateTests causes Gradle to display an error:
Try: Run gradle tasks to get a list of available tasks.
Gradle excluded the task yayGradle0 and its dependent task startSession, a concept Gradle calls smart exclusion.
Now that you’re becoming familiar with the command line, let’s explore some more helpful functions.
In this section, we explore the most important general-purpose options, flags to control your build script’s logging level, and ways to provide properties to your project.
The gradle command allows you to define one or more options at the same time.
Let’s say you want to change the log level to INFO using the –i option and print out any stack trace if an error occurs during execution with the option -s.
To do so, execute the task groupTherapy command like this: gradle groupTherapy –is or gradle groupTherapy –i –s.
As you can see, it’s very easy to combine multiple options.
To discover the full set, run your build with the –h argument or see appendix A of this book.
I won’t go over all the available options, but the most important ones are as follows:
Use this option to execute a build script with a different name (for example, gradle –b test.gradle)
If these dependencies were not stored in your local cache yet, running a build without a network connection to these repositories would result in a failed build.
Use this option to run your build in offline mode and only check the local dependency cache for dependencies.
As with all Java processes, you can provide a system property like this: –Dmyprop=myvalue.
P, --project-prop: Project properties are variables available in your build script.
You can use this option to pass a property to the build script directly from the command line (for example, -Pmyprop=myvalue)
Use this option to get more informative messages by changing Gradle’s logger to INFO log level.
This is helpful if you want to get more information on what’s happening under the hood.
Other properties are user-defined properties originating from a property file or property command-line option, or directly declared in your build script.
When using Gradle on a day-to-day basis, you’ll find yourself having to run your build repetitively.
This is especially true if you’re working on a web application.
You change a class, rebuild the web application archive, bring up the server, and reload the URL in the browser to see your changes being reflected.
For continuous feedback on their code quality, they run their unit tests over and over again to find code defects early on.
Each time you initiate a build, the JVM has to be started, Gradle’s dependencies have to be loaded into the class loader, and the project object model has to be constructed.
Once started, the gradle command will reuse the forked daemon process for subsequent builds, avoiding the startup costs altogether.
On my machine, it takes about three seconds to successfully complete running the task groupTherapy.
It’s easy to start the Gradle daemon on the command line: simply add the option --daemon to your gradle command.
You may notice that we add a little extra time for starting up the daemon as well.
To verify that the daemon process is running, you can check the process list on your operating system:
Windows: Open the task manager with the keyboard shortcut Ctrl+Shift+Esc and click the Processes tab.
Subsequent invocations of the gradle command will now reuse the daemon process.
Give it a shot and try running gradle groupTherapy --daemon.
Wow, you got your startup and execution time down to about one second! Keep in mind that a daemon process will only be forked once even though you add the command-line option --daemon.
At any time you can choose to execute your build without using the daemon by adding the command-line option --no-daemon.
For a deep dive into all configuration options and intricacies, please refer to the Gradle online documentation at http://gradle.org/docs/current/userguide/gradle_daemon.html.
Existing tools can’t meet the build needs of today’s industry.
In this chapter, we explored how Gradle can be used to deliver in each of the phases of a deployment pipeline in the context of continuous delivery.
Throughout the book, we’ll pick up on each of the phases by providing practical examples.
Next, you got a first taste of Gradle’s powerful features in action.
You installed the runtime, wrote a first simple build script, and executed it.
By implementing a more complex build script, you found out how easy it is to define task dependencies using Gradle’s DSL.
Knowing the mechanics of Gradle’s command line and its options is key to becoming highly productive.
Gradle offers a wide variety of command-line switches for changing runtime behavior, passing properties to your project, and changing the logging level.
We explored how running Gradle can be a huge timesaver if you have to continuously execute tasks, such as during test-driven development.
In chapter 3, I’ll show how to build a full-fledged, web-enabled application with Gradle.
Starting out with a simple, standalone Java application, you’ll extend the code base by adding a web component and use Gradle’s in-container web development support to efficiently implement the solution.
I’m going to show how to enhance your web archive to make it enterprise-ready and make the build transferable across machines without having to install the Gradle runtime.
Chapter 2 introduced Gradle’s feature set and showed how it compared to other JVM build tools.
Some simple examples gave you a first impression of the tool’s expressive build language.
By running your first build script, you saw how easy it is to become productive on the command line.
Now it’s time to strengthen this newly acquired knowledge by building a real-world Java project.
When starting a brand-new application, Java doesn’t guide you toward a standardized project structure.
You may ask yourself where to put source, configuration, and library files.
Stuck with a legacy application that has a different directory structure? No problem! Gradle allows for adapting its conventions to your needs.
In this chapter, you’ll explore the inner workings of Gradle’s standardization paradigm by building a Java project and learning how to tailor it to nonconventional use cases.
In the next step, you’ll extend your application by a web component and introduce productivity tools for fast development turnarounds.
Then we’ll round out this chapter by looking at the Gradle wrapper, which allows you to create transferable and reproducible builds without having to install the Gradle runtime.
This section introduces a simple application to illustrate the use of Gradle: a To Do application.
Throughout the book, we’ll apply the content to demonstrate Gradle’s features in each phase of the build pipeline.
The use case starts out as a plain Java application without a GUI, simply controlled through console input.
Over the course of this chapter, you’ll extend this application by adding components to learn more advanced concepts.
The To Do application will act as a vehicle to help you gain a broad knowledge of Gradle’s capabilities.
You’ll learn how to apply Gradle’s standard plugins to bootstrap, configure, and run your application.
By the end of this chapter, you’ll have a basic understanding of how Gradle works that you can apply to building your own webbased Java projects with Gradle.
Many of us manage multiple projects simultaneously, both in our professional and private lives.
Often, you may find yourself in situations where you feel overwhelmed and out of control.
The key to staying organized and focused on priorities is a well-maintained to-do list.
Sure, you could always write down your tasks on a piece of paper, but wouldn’t it be convenient to be able to access your action items everywhere you go? Access to the internet is almost omnipresent, either through your mobile phone or publicly available access points.
You’re going to build your own web-based and visually appealing application, as shown in figure 3.1
Now that you know your end goal, let’s identify the use cases the application needs to fulfill.
Every task management system consists of an ordered list of action items or tasks.
A task has a title to represent the action needed to complete it.
Tasks can be added to the list and removed from the list, and marked active or completed to indicate their status.
The list should also allow for modifying a task’s title in case you want.
Figure 3.1 The To Do application is accessible through the internet and manages action items in a data store.
Changes to a task should automatically get persisted to a data store.
To bring order to your list of tasks, you’ll include the option to filter tasks by their status: active or completed.
For now, we’ll stick with this minimal set of features.
Figure 3.2 shows a screenshot of the user interface rendered in a browser.
Let’s take a step back from the user interface aspect and build the application from the ground up.
In its first version, you’ll lay out its foundation by implementing the basic functionality controlled through the command line.
In the next section, we’re going to focus on the application’s components and the interactions between them.
We found that a To Do application implements the typical create, read, update, and delete (CRUD) functionality.
For data to be persisted, you need to represent it by a model.
You’ll create a new Java class called ToDoItem, a plain old Java object (POJO) acting as a model.
To keep the first iteration of the solution as simple as possible, we won’t introduce a traditional data store like a database to store the model data.
Instead, you’ll keep it in memory, which is easy to implement.
The drawback is that you can’t persist the data after shutting down the application.
Later in the book, we’ll pick up this idea and show how to write a better implementation for it.
Every standalone Java program is required to implement a main class, the application’s entry point.
Your main class will be called ToDoApp and will run until the user decides to exit the program.
You’ll present users with a menu of commands through which they can manage their to-do list by typing in a letter triggering a specific action.
Each action command is mapped to an enum called CommandLineInput.
Figure 3.2 Web-based user interface of To Do application and its actions.
Figure 3.3 illustrates the object interaction arranged in a time sequence for the use case of listing all available tasks.
In the next section, we’ll dive right into the code.
In the last section, we identified the classes, their functions, and the interaction between them.
First, let’s look at the model of a todo action item.
The attribute id defines the item’s unique identity, enabling you to store it in the in-memory.
Figure 3.3 Listing all available tasks represented as a sequence diagram.
Additionally, the model class exposes the fields name and completed.
For brevity, the getter and setter methods as well as the compareTo method are excluded from the snippet:
Now let’s look at the repository implementation for reading and writing the model.
IN-MEMORY PERSISTENCE OF THE MODEL Storing data in memory is convenient and simplifies the implementation.
Later in the book, you may want to provide more sophisticated implementations like database or file persistence.
To be able to swap out the implementation, you’ll create an interface, the ToDoRepository, as shown in the following listing.
You can find all existing to-do items, look up a specific one by ID, insert new action items, and update or delete them.
Next, you’ll create a scalable and thread-safe implementation of this interface.
So far, you’ve seen the data structure of a to-do item and an in-memory implementation for storing and retrieving the data.
To be able to bootstrap the Java program, you’ll need to create a main class.
Only puts to-do item into Map if it doesn’t exist yet.
So far, we’ve discussed the components of the application and their interactions in the context of a specific use case: finding all to-do items of a user.
Listing 3.3 should give you a rough idea of the components responsibilities and how they work internally.
Don’t worry if you don’t understand every little implementation detail of the class definitions presented here.
We’ll look at specific concerns like setting up the project with Gradle, compiling the source code, assembling the JAR file, and running the application in the rest of the chapter.
In the last section, we identified the Java classes required to write a standalone To Do application.
To assemble an executable program, the source code needs to be compiled and the classes need to be packaged into a JAR file.
The Java Development Kit (JDK) provides development tools like javac and jar that help with implementing these tasks.
Unless you’re a masochist, you don’t want to run these tasks manually each and every time your source code changes.
A plugin extends your project by introducing domain-specific conventions and tasks with sensible defaults.
One of the plugins that Gradle ships with is the Java plugin.
The Java plugin goes far beyond the basic functionality of source code compilation and packaging.
It establishes a standard layout for your project and makes sure that tasks are executed in the correct order so they make sense in the context of a Java project.
It’s time to create a build script for your application and apply the Java plugin.
In chapter 1, you learned that every Gradle project starts with the creation of the build script named build.gradle.
Create the file and tell your project to use the Java plugin like this:
One line of code is enough to build your Java code, but how does Gradle know where to find your source files? One of the conventions the Java plugin introduces is the location of the source code.
By default, the plugin searches for production source code in the directory src/main/java.
You’ll take all the classes of your To Do application and put them under the appropriate directory.
After creating the build script and moving your source code into the correct location, your project structure should look like this:
One of the tasks the Java plugin adds to your project is named build.
The build task compiles your code, runs your tests, and assembles the JAR file, all in the correct order.
Running the command gradle build should give you an output similar to this:
Automatic project generation Wouldn’t it be great if you didn’t have to create the source directories manually? Maven has a concept called project archetypes, a plugin to generate a project structure from an existing template.
Unfortunately, at the time of writing this functionality hasn’t become a Gradle core feature.
The plugin Gradle Templates created by the Gradle community proposes a solution to this issue.
A first attempt at initializing a Gradle project is automatically made by the build setup plugin, which you can use even without a build script.
This plugin allows for generating the project file (and other related files you’ll learn about later)
To generate the Gradle build script, execute gradle setupBuild from the command line.
Each line of the output represents an executed task provided by the Java plugin.
You may notice that some of the tasks are marked with the message UP-TO-DATE.
Gradle’s incremental build support automatically identified that no work needed to be done.
Especially in large enterprise projects, this feature proves to be a real timesaver.
In chapter 4 you’ll learn how to apply this concept to your own tasks.
In the command-line output, you can see concrete examples of skipped tasks: compileTestJava and testClasses.
As you provide any unit tests in the default directory src/test/java, Gradle happily moves on.
Assembled JAR file containing Java class files and manifest; name of file is derived from project’s directory name.
On the root level of your project, you’ll now also find a directory named build, which contains all output of the build run, including class files, test reports, the assembled JAR file, and temporary files like a manifest needed for the archive.
If you’ve previously used the build tool Maven, which uses the standard output directory target, the structure should look familiar.
The name of the build output directory is a configurable standard property common to all Gradle builds.
You’ve seen how effortless it is to build a Java project by convention without any additional configuration from your side.
The JAR file was created under build/libs and is ready for execution.
It’s important to understand that the name of the JAR file is derived from the project name.
As long as you don’t reconfigure it, the directory name of your project is used, which in this case is todo-app.
For now, you’ll just use the JDK’s java command from the root directory of your project:
To Do Application --Please make a choice: (a)ll items (f)ind a specific item (i)nsert a new item (u)pdate an existing item (d)elete an existing item (e)xit >
The Java program starts up, prints a list of all available to-do actions, and awaits your input from the command prompt.
That’s it—you effortlessly implemented a Java application and built it with Gradle.
All it took was a one-liner in your build script as long as you stuck to the standard conventions.
Java standalone application support Gradle can simplify building a standalone Java application even further.
Another standard Gradle extension worth mentioning is the application plugin.
The plugin provides tasks for simplifying the act of running and bundling an application.
It assumes sensible default values for many aspects of your project, like its layout.
If your view of the world is different, Gradle gives you the option of customizing the conventions.
How do you know what’s configurable? A good place to start is Gradle’s Build Language Reference, available at http://www.gradle.org/docs/current/dsl/
Remember the command-line option properties from chapter 2? Running gradle properties gives you a list of configurable standard and plugin properties, plus their default values.
You’ll customize the project by extending the initial build script.
Previously, you ran the To Do application using the java command.
You told the Java runtime where to find the classes by assigning the build output directory to the classpath command-line option via -cp build/classes/ main.
To be able to start the application from the JAR file, the manifest MANIFEST.MF needs to contain the header Main-Class.
The following listing demonstrates how to configure the default values in the build script and add a header attribute to the JAR manifest.
After assembling the JAR file, you’ll notice that the version number has been added to the JAR filename.
Next, we’ll look at how to retrofit the project structure to a legacy layout.
All too often, you’ll have to integrate with a legacy system, migrate the technology stack of an existing project, or adhere to internal standards or limitations.
A build tool has to be flexible enough to adapt to external constraints by configuring the default settings.
In this section we’ll explore examples that demonstrate the customizability of the To Do application.
Let’s assume you started the project with a different directory layout.
Instead of putting source code files into src/main/java, you chose to use the directory src.
The same concept applies if you want to change the default test source directory.
Additionally, you’d like to let Gradle render its build output into the directory.
Identifies project’s version through a number schemeSets Java version compilation compatibility to 1.6
Building a Java project out instead of the standard value build.
The next listing shows how to adapt your build to a custom project layout.
The key to customizing a build is knowledge of the underlying properties and DSL elements.
Next, we’ll look at how to use functionality from external libraries.
Let’s think back to the main method in the class ToDoApp.
You wrote some code to read the user’s input from the console and translate the first character into a to-do command.
To do so, you needed to make sure that the entered input string had a length of only one digit.
I bet you can improve on this implementation by reusing a library that wraps this logic.
The perfect match is the class CharUtils from the Apache Commons Lang library.
It provides a method called toChar that converts a String to a char by using just the first character, or a default character if the string’s value is empty.
The following code snippet shows the improved version of your input parsing code:
So how do you tell Gradle to reference the Apache Commons Lang library? We’ll look at two DSL configuration elements: repositories and dependencies.
Many libraries are available in a repository, such as a file system or central server.
Gradle requires you to define at least one repository to use a dependency.
Replaces conventional source code directory with list of different directories.
Replaces conventional test source code directory with list of different directories.
With a repository in place, you’re ready to declare the library.
You’ll use version 3.1 of the library, as shown in this code snippet:
One of the configurations that the Java plugin introduces is compile.
You can probably tell by the configuration’s name that it’s used for dependencies needed for compiling source code.
If the dependency hasn’t been resolved successfully, it downloads it with the next task invocation that requires it to work correctly—in this case, task compileJava:
Chapter 5 will give a deeper coverage of the topic of dependency management.
I know that the To Do application in its current form doesn’t knock your socks off.
It’s time to modernize it by adding a visually attractive user interface.
How to find a dependency Finding out detailed information about a dependency on Maven Central is straightforward.
In Java, server-side web components of the Enterprise Edition (Java EE) provide the dynamic extension capabilities for running your application within a web container or application server.
As the name Servlet may already indicate, it serves a client request and constructs the response.
The response of a Servlet is rendered by the view componentthe Java Server Page (JSP)
Figure 3.4 illustrates the MVC architecture pattern in the context of a Java web application.
A WAR (web application archive) file is used to bundle web components, compiled classes, and other resource files like deployment descriptors, HTML, JavaScript, and CSS files.
To run a Java web application, the WAR file needs to be deployed to the server environment, a web container.
Gradle provides out-of-the-box plugins for assembling WAR files and deploying web applications to a local Servlet container.
Before we look at how to apply and configure these plugins, you’ll need to turn your standalone Java application into a web application.
We focus next on the web components we’re going to introduce and how they interact with each other.
The Java enterprise landscape is dominated by a wide range of web frameworks, such as Spring MVC and Tapestry.
Web frameworks are designed to abstract the standard web components and reduce boilerplate code.
Despite these benefits, web frameworks can introduce a steep learning curve as they introduce new concepts and APIs.
To keep the example as simple and understandable as possible, we’ll stick to the standard Java enterprise web components.
Before jumping into the code, let’s see how adding web components changes the interaction between the existing classes from the previous section.
The Servlet class you’re going to create is called ToDoServlet.
It’s responsible for accepting HTTP requests, executing a CRUD operation mapped to a URL endpoint, and forwarding the request to a JSP.
To present the user with a fluid and comfortable experience, you’ll implement the to-do list as a single-page application.
Figure 3.4 Java EE provides components for building a web application based on the MVC architecture pattern.
The page knows how to dynamically render the list of to-do items and provides UI elements like buttons and links for initiating CRUD operations.
Figure 3.5 shows the flow through your new system for the use case of retrieving and rendering all to-do items.
Both classes work seamlessly with the controller and view components.
Let’s look at the inner workings of the controller component.
The following code snippet shows the most important parts of your controller web component, the class ToDoServlet:
Sets to-do items as request attributes and forwards to view.
Figure 3.5 Finding all to-do items use case: the user issues an HTTP request through the browser, which is served by a Servlet and renders the result through a JSP.
That’s it; you converted your task management program into a web application.
In the examples I only touched on the most important parts of the code.
For a deeper understanding, I encourage you to browse the full source code.
Gradle provides extensive support for building and running web applications.
In this section we’ll look at two plugins for web application development: War and Jetty.
The War plugin extends the Java plugin by adding conventions for web application development and support for assembling WAR files.
Running a web application on your local machine should be easy, enable rapid application development (RAD), and provide fast startup times.
Optimally, it shouldn’t require you to install a web container runtime environment.
Jetty is a popular, lightweight, open source web container supporting all of these features.
It comes with an embedded implementation by adding an HTTP module to your application.
Gradle’s Jetty plugin extends the War plugin, provides tasks for deploying a web application to the embedded container, and runs your application.
In case incoming request URL doesn’t match any handling, redirect to /all URL.
First, you’ll apply the plugins and use the default conventions, and then you’ll customize them.
In practice, this means that you don’t have to apply the Java plugin anymore in your build script.
Note that even if you applied the Java plugin as well, there would be no side effect on your project.
Applying plugins is an idempotent operation, and therefore is only executed once for a specific plugin.
When creating your build.gradle file, use the plugin like this:
What exactly does that mean to your project? In addition to the conventions provided by the Java plugin, your project becomes aware of a source directory for web application files and knows how to assemble a WAR file instead of a JAR file.
The default convention for web application sources is the directory src/main/webapp.
With all the web resource files in the right location, your project layout should look like this:
Alternative embedded container plugins The Jetty plugin works great for local web application development.
However, you may use a different Servlet container in your production environment.
To provide maximum compatibility between runtime environments early on in the software development lifecycle, look for alternative embedded container implementations.
A viable solution that works very similarly to Gradle’s standard Jetty extension is the third-party Tomcat plugin.
Directory storing style sheets that describe how to display HTML elements.
Before you run the build, you’ll need to make sure that you declare those external dependencies.
The configuration you’ll use for the Servlet dependency is providedCompile.
It’s used for dependencies that are required for compilation but provided by the runtime environment.
As a consequence, dependencies marked provided aren’t going to be packaged with the WAR file.
Runtime dependencies like the JSTL library aren’t needed for the compilation process, but are needed at runtime.
The following dependencies closure declares the external libraries you need for your application:
The assembled WAR file can be found in the directory build/libs after running the command gradle build.
By changing the nature of the project from a standalone application to a web application, the task jar was replaced by the task war, as shown in the following output:
The War plugin makes sure that the assembled WAR file adheres to the standard structure defined by the Java EE specification.
The war task copies the contents of the default web application source directory src/main/webapp to the root of the WAR file without modifying the structure.
Compiled classes end up in the directory WEB-INF/ classes, and runtime libraries, defined through the dependencies closure, get put in WEB-INF/lib.
Directory holding dynamic scripting view components in form of JSPs.
By default, the WAR filename is derived from the project’s directory name.
Even if your project doesn’t adhere to Gradle’s standard conventions, the plugin can be used to build a WAR file.
In the following example, we’re going to assume that all of your static files sit in the directory static, and that all of your web application content resides under the directory webfiles:
The following code snippet shows how to configure the convention properties.
The default value src/ main/webapp is easily switched to webfiles by assigning a new value.
Directories can be selectively added to the WAR file by invoking the from method, as follows:
The previous example only showed an excerpt of the War plugin’s configuration options.
You can easily include other external JAR files, use a web deployment descriptor from a nonstandard directory, or add another file set to the WEB-INF directory.
If you’re looking for a configuration parameter, the best place to check is the War plugin DSL guide.
You’ve seen how to build the WAR file from a web project with a standard structure or customized directory layout.
Now it’s time to deploy the file to a Servlet container.
In the next section, you’ll fire up Jetty to run the application on your local development machine.
Internally, the Jetty plugin does all this work for you.
As the War plugin exposes all this information, it can be accessed at runtime by the Jetty plugin.
This is a typical example of a plugin using another plugin’s configuration through the Gradle API.
The task you’re going to use to run the web application is jettyRun.
It’ll start the Jetty container without even having to create a WAR file.
The output of running the task on the command line should look similar to this:
On the last line of the output, the plugin gives you the URL that Jetty listens to for incoming requests.
Finally, you can see the To Do web application in action.
Gradle will leave the application running until you stop it by pressing Ctrl + C.
How did Jetty know what port and context to use for running the application? Again, it’s conventions.
The default port of a web application run by the Jetty plugin is 8080, and the context path todo-webapp-jetty is derived from your project name.
Adds directories css and jsp to root of WAR file archive.
Another application is already running on port 8080, and you got tired of typing in the long context path.
Starting the application with this configuration will expose the URL http://localhost:9090/todo.
There are many more options for configuring the Jetty plugin.
A great place to start is with the API documentation of the plugin.
You put together a prototype of a task management web application.
After you show it to your coworker, Mike, he says he wants to join forces and bring the application to the next level by adding more advanced features.
The code has been committed to a version control system (VCS), so he can go ahead, check out the code, and get started working on it.
Mike has never worked with the build tool Gradle, so he asks you how to install the runtime on his machine and which version to use.
Because he didn’t go through the motions of initially setting up Gradle, he’s also concerned about potential differences between setting up Gradle on his Windows machine versus installing it on a Mac.
From experience with other build tools, Mike is painfully aware that picking the wrong version of the build tool distribution or the runtime environment may have a detrimental effect on the outcome of the build.
All too often, he’s seen that a build completes successfully on his machine but fails on another for no apparent reason.
After spending hours troubleshooting, he usually discovers that the cause was an incompatible version of the runtime.
Gradle provides a very convenient and practical solution to this problem: the Gradle wrapper.
The wrapper is a core feature and enables a machine to run a Gradle build script without having to install the runtime.
It also ensures that the build script is run with a specific version of Gradle.
It does so by automatically downloading the Gradle runtime from a central location, unpacking it to your local file system, and using it.
Rapid application development Having to restart the container for every single change you make to your application code is cumbersome and time-consuming.
The Jetty plugin allows you to change static resources and JSP files on the fly without having to restart the container.
Additionally, bytecode swap technologies like JRebel can be configured to perform hot deployment for class file changes.
The ultimate goal is to create reliable and reproducible builds independent of the operating system, system setup, or installed Gradle version.
Let’s look at how to set up the wrapper for Mike and any other developer who wants to join the team.
To set up the wrapper for your project, you’ll need to do two things: create a wrapper task and execute the task to generate the wrapper files (figure 3.6)
To enable your project to download the zipped Gradle runtime distribution, define a task of type Wrapper and specify the Gradle version you want to use through the property gradleVersion:
It’s not required to name the task wrapper—any name will do.
However, wrapper is used throughout the Gradle online documentation and serves as a helpful convention.
When to use the wrapper Using the wrapper is considered best practice and should be mandatory for every Gradle project.
Gradle scripts backed by the wrapper are perfectly prepared to run as part of automated release processes like continuous integration and delivery.
Figure 3.6 Set up the wrapper in two easy steps: add the wrapper task and execute it.
As a result, you’ll find the following wrapper files alongside your build script:
Keep in mind that you’ll only need to run gradle wrapper on your project once.
From that point on, you can use the wrapper’s script to execute your build.
The downloaded wrapper files are supposed to be checked into version control.
For documentation reasons it’s helpful to also keep the task in your project.
It’ll help you to upgrade your wrapper version later by changing the gradleVersion and rerunning the wrapper task.
Instead of creating the wrapper task manually and executing it to download the relevant files, you can use the build setup plugin mentioned earlier.
Executing the command gradle wrapper will generate the wrapper files with the current version of your Gradle runtime:
Next you’ll use the generated wrapper scripts to bootstrap the Gradle script.
As part of the wrapper distribution, a command execution script is provided.
For *nix systems, this is the shell script gradlew; for Windows operating systems, it’s gradlew.bat.
You’ll use one of these scripts to run your build in the same way as you would with the installed Gradle runtime.
Figure 3.7 illustrates what happens when you use the wrapper script to execute a task.
Included in the source code tree of the project, he’ll find the wrapper files.
As Mike develops his code on a Windows box, he’ll need to run the wrapper batch file to execute a task.
The following console output is produced when he fires up the local Jetty container to run the application:
Gradle wrapper microlibrary contains logic to download and unpack distribution.
Wrapper metadata like storage location for downloaded distribution and originating URL.
The Gradle wrapper also takes care of unpacking the distribution and setting the appropriate permissions to execute the batch file.
Subsequent build runs reuse the unpacked installation of the runtime located in your Gradle home directory:
What are the key takeaways? A build script executed by the Gradle wrapper provides exactly the same tasks, features, and behavior as it does when run with a local Gradle installation.
Again, you don’t have to stick with the default conventions the wrapper gives you.
Some enterprises have very restrictive security strategies, especially if you work for a government agency, where access to servers outside of the network is prohibited.
Figure 3.7 When a wrapped task is executed, the Gradle runtime is downloaded, unpacked, and used.
How do you enable your project to use the Gradle wrapper in that case? It’s all in the configuration.
You’ll change the default properties to target an enterprise server hosting the runtime distribution.
And while you’re at it, you’ll also change the local storage directory:
Pretty straightforward, right? There are many more options to explore.
Make sure to check out the Gradle wrapper DSL documentation for detailed information at http://gradle.org/docs/current/dsl/org.gradle.api.tasks.wrapper.Wrapper.html.
In chapter 2, you learned how to express and execute simple logic by adding tasks to your project.
You implemented a full-stack Java application and used Gradle to build it.
They need to compile Java source code, run tests, and bundle a JAR file containing the classes.
Luckily, you didn’t have to write these tasks yourself to make this happen in your project.
Through the use of Gradle plugins, you merely had to write code in your build script.
You started out by using the Java plugin that ships with Gradle.
Applying the plugin to your project added preconfigured tasks and a standardized project structure wrapped by an opinionated framework.
We explored the option of customizing the default conventions introduced by the plugin.
Key to knowing your options are the Gradle DSL and API documentation.
After a short recap of the fundamentals of Java web application development, we discussed how to extend the example project by Java EE–compatible web components.
Gradle helps you with simplifying web development through the War and Jetty plugins.
The War plugin assists in assembling a WAR file, and the Jetty plugin provides efficient deployment to a lightweight Servlet container.
You also learned that using the wrapper is a best practice for every Gradle project.
Not only does it allow you to run the project on a machine that doesn’t have Gradle installed, it also prevents version compatibility issues.
The presented plugins provide far more functionality than we’ve discussed.
In part 2, we delve into many of Gradle’s core concepts.
The next chapter will focus on Gradle’s building blocks, the ins and outs of tasks, and the build lifecycle.
Path where wrapper will be unzipped relative to Gradle home directory.
In part 1, you learned Gradle’s core concepts and features by example.
We’ll look at more advanced topics like dependency management, testing an application with Gradle, extending your build with plugins, and many more.
Chapter 4 covers Gradle’s quintessential building blocks for modeling your build.
You’ll learn how to declare new tasks, manipulate existing ones, and implement proper abstractions for complex logic.
In chapter 5, you’ll learn how to declare and organize dependencies in a build script.
Modularized software projects pose an additional layer of complexity for modeling your build.
In chapter 7, we’ll turn our attention to the important topic of testing.
You’ll see how easy it is to write, organize, and execute unit, integration, and functional tests, while at the same time picking and choosing the tooling of your choice.
You’ll learn how to abstract complex logic for deploying your sample application to a cloud service.
We’ll touch on all facets of the extension model, from custom tasks and script and binary plugins, to exposing your own configuration language.
Because Gradle goes hand in hand with popular build tools like Ant and Maven, chapter 9 is dedicated to helping you translate existing.
Once you’ve finished this part of the book, you’ll be able to apply Gradle’s core concepts to a real-world project.
In part 3, we’ll discuss the use of Gradle with other tools of the build and delivery ecosystem.
In chapter 3, you implemented a full-fledged Java web application from the ground up and built it with the help of Gradle’s core plugins.
You learned that the default conventions introduced by those plugins are customizable and can easily adapt to nonstandard build requirements.
Preconfigured tasks function as key components of a plugin by adding executable build logic to your project.
In this chapter, we’ll explore the basic building blocks of a Gradle build, namely projects and tasks, and how they map to the classes in the Gradle API.
Properties are exposed by methods of these classes and help to control the build.
You’ll also learn how to control the build’s behavior through properties, as well as the benefits of structuring your build logic.
At the core of this chapter, you’ll experience the nitty-gritty details of working with tasks by implementing a consistent example.
Step by step, you’ll build your knowledge from declaring simple tasks to writing custom task classes.
Along the way, we’ll touch on topics like accessing task properties, defining explicit and implicit task dependencies, adding incremental build support, and using Gradle’s built-in task types.
We’ll also look at Gradle’s build lifecycle to get a good understanding of how a build is configured and executed.
Your build script can respond to notifications as the build progresses through the lifecycle phases.
In the last part of this chapter, we’ll show how to write lifecycle hooks as closure and listener implementations.
Every Gradle build consists of three basic building blocks: projects, tasks, and properties.
Each build contains at least one project, which in turn contains one or more tasks.
Projects and tasks expose properties that can be used to control the build.
Gradle applies the principles of domain-driven design (DDD) to model its own domain-building software.
As a consequence, projects and tasks have a direct class representation in Gradle’s API.
Let’s take a closer look at each component and its API counterpart.
In Gradle’s terminology a project represents a component you’re trying to build (for example, a JAR file), or a goal you’re trying to achieve, like deploying an application.
If you’re coming from Maven, this concept should sound pretty familiar.
Figure 4.2 shows the API interface and its most important methods.
Figure 4.1 Two basic concepts of a Gradle build are projects and tasks.
A project can depend on other projects in the context of a multiproject build.
Similarly, tasks can form a dependency graph that guarantees their execution order.
A project can create new tasks, add dependencies and configurations, and apply plugins and other build scripts.
Many of its properties, like name and description, are accessible via getter and setter methods.
So why are we talking about Gradle’s API early on? You’ll find that after getting to know Gradle’s basics, you’ll want to go further and apply the concepts to your realworld projects.
The API is key to getting the most out of Gradle.
The Project instance gives you programmatic access to all Gradle features in your build, like task creation and dependency management.
You’ll use many of these features throughout the book by invoking their corresponding API methods.
Keep in mind that you’re not required to use the project variable when accessing properties and methods of your project—it’s assumed you mean the Project instance.
The following code snippet illustrates valid method invocations on the Project instance:
Figure 4.2 Main entry point of a Gradle build—the Project interface.
Using Groovy syntax to access name and description properties with and without using project variable.
In the previous chapters, you only had to deal with single-project builds.
One of the most important principles of software development is separation of concerns.
The more complex a software system becomes, the more you want to decompose it into modularized functionality, in which modules can depend on each other.
Each of the decomposed parts would be represented as a Gradle project with its own build.gradle script.
For the sake of simplicity, we won’t go into details here.
If you’re eager to learn more, feel free to jump to chapter 6, which is fully devoted to creating multiproject builds in Gradle.
Next, we’ll look at the characteristics of tasks, another one of Gradle’s core building blocks.
Even though the use cases I presented were trivial, you got to know some important capabilities of a task: task actions and task dependencies.
An action defines an atomic unit of work that’s executed when the task is run.
Many times a task requires another task to run first.
This is especially true if the task depends on the produced output of another task as input to complete its own actions.
For example, you’ve seen that you need to compile Java sources first before they can be packaged into a JAR file.
The Task interface provides even more methods than are shown in the figure.
You’ll use them one by one as you apply them to concrete examples throughout the book.
Now that we’ve discussed projects and tasks, let’s look at different types of properties.
Tasks can define dependencies on other tasks, a sequence of actions, and conditional execution.
Each instance of Project and Task provides properties that are accessible through getter and setter methods.
A property could be a task’s description or the project’s version.
Later in this chapter, you’ll read and modify these values in the context of a practical example.
For example, you may want to declare a variable that references a file that’s used multiple times within the same build script.
Internally, these properties are stored as key-value pairs in a map.
To add properties, you’re required to use the ext namespace.
The following code snippet demonstrates that a property can be added, read, and modified in many different ways:
Similarly, additional properties can be fed through a properties file.
Any property declared in the properties file will be available to all of your projects.
You can access both variables in your project as follows:
Gradle offers many other ways to provide properties to your build, such as.
Only initial declaration of extra property requires you to use ext namespace.
I won’t show you concrete examples for these alternative ways of declaring properties, but you can use them if needed.
The online Gradle user guide provides excellent usage examples if you want to go further.
For the rest of this chapter, you’ll make extensive use of tasks and Gradle’s build lifecycle.
This means that they can only be accessed through their public getter and setter methods.
Thankfully, Groovy provides you with some syntactic sugar, which allows you to use fields by their name.
In this section, we’ll explore the most important features of a task by example.
Now that you have the general build infrastructure in place, features can easily be added.
To identify each release, a unique version number is added to the deliverable.
Many enterprises or open source projects have their own versioning strategy.
Think back to some of the projects you’ve worked on.
Usually, you assign a specific version numbering scheme (for example, a major and minor version number separated by a dot, like 1.2)
You may also encounter a project version that appends a SNAPSHOT designator to indicate that the built project artifact is in the state of development.
You’ve already assigned a version to your project in chapter 3 by setting a string value to the project property version.
Using a String data type works great for simple use cases, but what if you want to know the exact minor version of your project? You’ll have to parse the string value, search for the dot character, and filter out the substring that identifies the minor version.
Wouldn’t it be easier to represent the version by an actual class?
You could easily use the class’s fields to set, retrieve, and modify specific portions of your numbering scheme.
By externalizing the version information to persistent data storage, such as a file or database, you’ll avoid having to modify the build script itself to change the project version.
Figure 4.4 illustrates the interaction among the build script, a properties file that holds the version information, and the data representation class.
You’ll create and learn how to use all of these files in the upcoming sections.
Being able to control the versioning scheme programmatically will become a necessity the more you want to automate your project lifecycle.
Here’s one example: your code has passed all functional tests and is ready to be shipped.
Before building the final WAR file, you’ll want to make it a release version 1.2 and automatically deploy it to the production server.
Each of these steps can be modeled by creating a task: one for modifying the project version and one for deploying the WAR file.
Let’s take your knowledge about tasks to the next level by implementing flexible version management in your project.
An action is the appropriate place within a task to put your build logic.
The Task interface provides you with two relevant methods to declare a task action: doFirst(Closure) and doLast(Closure)
When a task is executed, the action logic defined as closure parameter is executed in turn.
You’re going to start easy by adding a single task named printVersion.
The task’s purpose is to print out the current project version.
Define this logic as the last action of this task, as shown in the following code snippet:
When executing the task with gradle printVersion, you should see the correct version number:
Figure 4.4 The project version is read from a properties file during runtime of the build script.
Each of the version classifiers is translated into a field value of the data class.
The instance of ProjectVersion is assigned to the version property of the project.
The same result could be achieved as the first action of the task by using the doFirst method instead:
But you’re not limited to a single action per task.
In fact, you can add as many actions as you need even after the task has been created.
Let’s look at a modified version of your example task:
As shown in the listing, an existing task can be manipulated by adding actions to them.
This is especially useful if you want to execute custom logic for tasks that you didn’t write yourself.
For example, you could add a doFirst action to the compileJava task of the Java plugin that checks if the project contains at least one Java source file.
Next you’ll improve the way you output the version number.
Gradle provides a logger implementation based on the logging library SLF4J.
Apart from implementing the usual range of logging levels (DEBUG, ERROR, INFO, TRACE, WARN), it adds some extra levels.
The logger instance can be directly accessed through one of the task’s methods.
For now, you’re going to print the version number with the log level QUIET:
See how easy it is to access one of the task properties? There are two more properties I want to show you: group and description.
Initial declaration of a task can contain a first and last action.
Additive doFirst closures are inserted at beginning of actions list.
The description property represents a short definition of the task’s purpose, whereas the group defines a logic grouping of tasks.
You’ll set values for both properties as arguments when creating the task:
Alternatively, you can also set the properties by calling the setter methods, as shown in the following code snippet:
When running gradle tasks, you’ll see that the task shows up in the correct task bucket and is able to describe itself:
Even though setting a task’s description and grouping is optional, it’s always a good idea to assign values for all of your tasks.
It’ll make it easier for the end user to identify the task’s function.
Next, we’ll review the intricacies of defining dependencies between tasks.
The method dependsOn allows for declaring a dependency on one or more tasks.
You’ve seen that the Java plugin makes extensive use of this concept by creating task graphs to model full task lifecycles like the build task.
The following listing shows different ways of applying task dependencies using the dependsOn method.
You’ll execute the task dependency chain by invoking the task third from the command line:
If you take a close look at the task execution order, you may be surprised by the outcome.
The task printVersion declares a dependency on the tasks second and first.
Wouldn’t you have expected that the task second would get executed before first? In Gradle, the task execution order is not deterministic.
The method call dependsOn only defines that the dependent tasks need to be executed beforehand.
Gradle’s philosophy is to declare what should be executed before a given task, not how it should be executed.
This concept is especially hard to grasp if you’re coming from a build tool that defines its dependencies imperatively, like Ant does.
In Gradle, the execution order is automatically determined by the input/output specification of a task, as you’ll see later in this chapter.
On the one hand, you don’t need to know the whole chain of task dependencies to make a change, which improves code maintainability and avoids potential breakage.
On the other hand, because your build doesn’t have to be executed strictly sequentially, it’s been enabled for parallel task execution, which can significantly improve your build execution time.
In practice, you may find yourself in situations that require a certain resource to be cleaned up after a task that depends on it is executed.
A typical use case for such a resource is a web container needed to run integration tests against a deployed application.
Gradle’s answer to such a scenario is finalizer tasks, which are regular Gradle tasks scheduled to run even if the finalized task fails.
The following code snippet demonstrates how to use a specific finalizer task using the Task method finalizedBy:
You’ll find executing the task first will automatically trigger the task named second:
Chapter 7 covers the concept of finalizer tasks in more depth with the help of a realworld example.
In the next section, you’ll write a Groovy class to allow for finergrained control of the versioning scheme.
It’s time to come back to my statement about Gradle’s ability to define general-purpose Groovy code within a build script.
In practice, you can write classes and methods the way you’re used to in Groovy scripts or classes.
In this section, you’ll create a class representation of the version.
In Java, classes that follow the bean conventions are called plain-old Java objects (POJOs)
By definition, they expose their fields through getter and setter methods.
Over time it can become very tiresome to write these methods by hand.
POGOs, Groovy’s equivalent to POJOs, only require you to declare properties without an access modifier.
Their getter and setter methods are intrinsically added at the time of bytecode generation and therefore are available at runtime.
In the next listing, you assign an instance of the POGO ProjectVersion.
When running the modified build script, you should see that the task printVersion produces exactly the same result as before.
Unfortunately, you still have to manually edit the build script to change the version classifiers.
Next, you’ll externalize the version to a file and configure your build script to read it.
At first sight, the task may look like any other task you defined before.
But if you look closer, you’ll notice that you didn’t define an action or use the left shift operator.
If you run printVersion now, you’ll see that the new task loadVersion is executed first.
Despite the fact that the task name isn’t printed, you know this because the build output prints the logging statement you added to it:
You may ask yourself why the task was invoked at all.
Granted, you didn’t declare a dependency on it, nor did you invoke the task on the command line.
The key to fully understanding this behavior is the Gradle build lifecycle.
Let’s take a closer look at each of the build phases.
File method is provided by Project interface; it creates an instance of java.io.File relative to project directory.
If version file doesn’t exist th a GradleExcep with an appropriate error message.
Groovy’s file implementation adds methods to read it with newly created InputStream.
In Groovy you can omit the return keyword if it’s last statement in method.
GRADLE’S BUILD LIFECYCLE PHASES Whenever you execute a Gradle build, three distinct lifecycle phases are run: initialization, configuration, and execution.
Figure 4.5 visualizes the order in which the build phases are run and the code they execute.
During the initialization phase, Gradle creates a Project instance for your project.
In the context of a multiproject build, this build phase becomes more important.
Depending on which project you’re executing, Gradle figures out which of the project dependencies need to participate in the build.
Note that none of your currently existing build script code is executed in this build phase.
This will change in chapter 6 when you modularize the To Do application into a multiproject build.
The build phase next in line is the configuration phase.
Internally, Gradle constructs a model representation of the tasks that will take part in the build.
The incremental build feature determines if any of the tasks in the model are required to be run.
This phase is perfect for setting up the configuration that’s required for your project or specific tasks.
As you can see, Gradle’s incremental build feature is tightly integrated in the lifecycle.
In chapter 3 you saw that the Java plugin made heavy use of this feature.
The task compileJava will only run if any of the Java source files are different from the last time the build was run.
Keep in mind that any configuration code is executed with every build of your project—even if you just execute gradle tasks.
Figure 4.5 Order of build phases in Gradle’s build lifecycle.
In the next section, I’ll show how to use the incremental build feature for your own tasks.
Gradle determines if a task is up to date by comparing a snapshot of a task’s inputs and outputs between two builds, as shown in figure 4.6
A task is considered up to date if inputs and outputs haven’t changed since the last task execution.
Therefore, the task only runs if the inputs and outputs are different; otherwise, it’s skipped.
An input can be a directory, one or more files, or an arbitrary property.
A task’s output is defined through a directory or 1...n files.
Inputs and outputs are defined as fields in class DefaultTask and have a direct class representation, as shown in figure 4.7
Imagine you want to create a task that prepares your project’s deliverable for a production release.
To do so, you’ll want to change the project version from SNAPSHOT to release.
The following listing defines a new task that assigns the Boolean value true to the version property release.
The task also propagates the version change to the property file.
Figure 4.6 Gradle determines if a task needs to be executed though its inputs/ outputs.
Figure 4.7 The class DefaultTask defines task inputs and outputs.
Ant task propertyfile provides a convenient way of modifying the property file.
The task makeReleaseVersion may be part of another lifecycle task that deploys the WAR file to a production server.
You may be painfully aware of the fact that a deployment can go wrong.
The network may have a glitch so that the server cannot be reached.
After fixing the network issues, you’ll want to run the deployment task again.
Because the task makeReleaseVersion is declared as a dependency to your deployment task, it’s automatically rerun.
Wait, you already marked your project version as production-ready, right? Unfortunately, the Gradle task doesn’t know that.
To make it aware of this, you’ll declare its inputs and outputs, as shown in the next listing.
You moved the code you wanted to execute into a doLast action closure and removed the left shift operator from the task declaration.
With that done, you now have a clear separation between the configuration and action code.
Task inputs/outputs evaluation Remember, task inputs and outputs are evaluated during the configuration phase to wire up the task dependencies.
That’s why they need to be defined in a configuration block.
To avoid unexpected behavior, make sure that the value you assign to inputs and outputs is accessible at configuration time.
In contrast to the regular inputs/outputs evaluation, this method is evaluated at execution time.
If the closure returns true, the task is considered up to date.
As the version file is going to be modified it’s declared as output file property.
Now, if you execute the task twice you’ll see that Gradle already knows that the project version is set to release and automatically skips the task execution:
If you don’t change the release property manually in the properties file, any subsequent run of the task makeReleaseVersion will be marked up to date.
So far you’ve used Gradle’s DSL to create and modify tasks in the build script.
Every task is backed by an actual task object that’s instantiated for you during Gradle’s configuration phase.
However, sometimes you may want to have full control over your task implementation.
In the next section, you’ll rewrite the task makeReleaseVersion in the form of a custom task implementation.
The action logic within the task makeReleaseVersion is fairly simple.
Code maintainability is clearly not an issue at the moment.
However, when working on your projects you’ll notice that simple tasks can grow in size quickly the more logic you need to add to them.
The need for structuring your code into classes and methods will arise.
You should be able to apply the same coding practices as you’re used to in your regular production source code, right? Gradle doesn’t suggest a specific way of writing your tasks.
The programming language you choose, be it Java, Groovy, or any other JVM-based language, and the location of your task is up to you.
Custom tasks consist of two components: the custom task class that encapsulates the behavior of your logic, also called the task type, and the actual task that provides the values for the properties exposed by the task class to configure the behavior.
Maintainability is only one of the advantages of writing a custom task class.
Because you’re dealing with an actual class, any method is fully testable through unit tests.
Testing your build code is out of the scope of this chapter.
Another advantage of enhanced tasks over simple tasks is reusability.
The properties exposed by a custom task can be set individually from the build script.
With the benefits of enhanced tasks in mind, let’s discuss writing a custom task class.
When creating a custom task, you do exactly that—create a class that extends DefaultTask.
The following listing demonstrates how to express the logic from makeReleaseVersion as the custom task class ReleaseVersionTask written in Groovy.
In the listing, you’re not using the DefaultTask’s properties to declare its inputs and outputs.
Not only do they have the same effect as the method calls to TaskInputs and TaskOutputs, they also act as automatic documentation.
At first glance, you know exactly what data is expected as input and what output artifact is produced by the task.
When exploring the Javadocs of this package, you’ll find that Gradle provides you with a wide range of annotations.
Applying input and output annotations to fields isn’t the only option.
You can also annotate the getter methods for a field.
But how do you actually use it? In your build script, you’ll need to create a task of type ReleaseVersionTask and set the inputs and outputs by assigning values to its properties, as shown in the next listing.
Think of it as creating a new instance of a specific class and setting the values for its fields in the constructor.
Task input validation The annotation @Input will validate the value of the property at configuration time.
To allow null values, mark the field with the @Optional annotation.
Writing a custom task that extends Gradle’s default task implementation.
As expected, the enhanced task makeReleaseVersion will behave exactly the same way as the simple task if you run it.
One big advantage you have over the simple task implementation is that you expose properties that can be assigned individually.
The version POGO exposes different fields to represent the versioning scheme, as shown in the next listing.
How does the enhanced task adapt to these requirements? You simply assign different values to the exposed properties, as shown in the following listing.
Gradle ships with a wide range of out-of-the-box custom tasks for commonly used functionality, like copying and deleting files or creating a ZIP archive.
In the next section we’ll take a closer look at some of them.
Working with tasks the boss knocking on your door asking about what went wrong, and your coworkers frantically trying to figure out the root cause of the stack trace being thrown when starting up the application.
Forgetting a single step in a manual release process can prove fatal.
Let’s be professionals and take pride in automating every aspect of the build lifecycle.
Being able to modify the project’s versioning scheme in an automated fashion is only the first step in modeling your release process.
To be able to quickly recover from failed deployments, a good rollback strategy is essential.
Having a backup of the latest stable application deliverable for redeployment can prove invaluable.
You’ll use some of the task types shipped with Gradle to implement parts of this process for your To Do application.
Before deploying any code to production you want to create a distribution.
It’ll act as a fallback deliverable for future failed deployments.
A distribution is a ZIP file that consists of your web application archive, all source files, and the version property file.
After creating the distribution, the file is copied to a backup server.
The backup server could either be accessible over a mounted shared drive or you could transfer the file over FTP.
Because I don’t want to make this example too complex to grasp, you’ll just copy it to the subdirectory build/backup.
Figure 4.8 illustrates the order in which you want the tasks to be executed.
As such, they can be used from an enhanced task within the build script.
Gradle provides a broad spectrum of task types, but for the purposes of this example you’ll use only two of them.
The following listing shows the task types Zip and Copy in the context of releasing the production version of your software.
You can find the complete task reference in the DSL guide.
Listing 4.10 Using task types to back up a zipped release distribution.
Implicit reference to output of War taskTakes all source files.
In this listing there are different ways of telling the Zip and Copy tasks what files to include and where to put them.
Many of the methods used here come from the superclass AbstractCopyTask, as shown in figure 4.9
For a full list of available options, please refer to the Javadocs of the classes.
The task types you used offer far more configuration options than those shown in the example.
Again, for a full list of available options, please refer to the DSL reference or the Javadocs.
Next, we’ll take a deeper look at their task dependencies.
However, some of the tasks don’t model a direct dependency to other tasks (for example, createDistribution to war)
How does Gradle know to execute the dependent task beforehand? By using the output of one task as input for another task, dependency is inferred.
Figure 4.9 Inheritance hierarchy for the task types Zip and Copy.
You can easily assign a different distribution output directory by setting the property destinationDir.
The following directory tree shows the relevant artifacts generated by the build:
Running the tasks multiple times in a row will mark them as up-to-date if you don’t change any of the source files.
Next, you’ll learn how to define a task on which the behavior depends on a flexible task name.
Sometimes you may find yourself in a situation where you write multiple tasks that do similar things.
For example, let’s say you want to extend your version management functionality by two more tasks: one that increments the major version of the project and another to do the same work for the minor version classifier.
Both tasks are also supposed to persist the changes to the version file.
If you compare the doLast actions for both tasks in the following listing, you can tell that you basically duplicated code and applied minor changes to them.
Using Ant task propertyfile to increment a specific property with a property file.
Run it on the INFO log level to see more detailed output information:
Having two separate tasks works just fine, but you can certainly improve on this implementation.
In the end, you’re not interested in maintaining duplicated code.
The pattern consists of two parts: the static portion of the task name and a placeholder.
Once you have the reference, you can call the method addRule(String, Closure)
The first parameter provides a description (for example, the task name pattern), and the second parameter declares the closure to execute to apply the rule.
Unfortunately, there’s no direct way of creating a task rule through a method from Project as there is for simple tasks, as illustrated in figure 4.10
With a basic understanding of how to add a task rule to your project, you can get started writing the actual closure implementation for it.
Task rules in practice Some of Gradle’s core plugins make good use of task rules.
One of the task rules the Java plugins define is clean<TaskName>, which deletes the output of a specified task.
For example, running gradle cleanCompileJava from the command line deletes all production code class files.
Using Ant task propertyfile to increment a specific property within a property file.
After adding the task rule in your project, you’ll find that it’s listed under a specific task group called Rules when running the help task tasks:
Figure 4.10 Simple tasks can be directly added by calling methods of your project instance.
Task rules can only be added through the task container, so you’ll need to get a reference to it first by invoking the getTasks() method.
Dynamically add a task named after provided pattern with a doLast action.
Task rules can’t be grouped individually as you can do with any other simple or enhanced task.
A task rule, even if it’s declared by a plugin, will always show up under this group.
You’ve seen how quickly your build script code can grow.
In this chapter you already created two Groovy classes within your build script: ProjectVersion and the custom task ReleaseVersionTask.
These classes are perfect candidates to be moved to the buildSrc directory alongside your project.
The buildSrc directory is an alternative location to put build code and a real enabler for good software development practices.
You’ll be able to structure the code the way you’re used to in any other project and even write tests for it.
Gradle standardizes the layout for source files under the buildSrc directory.
Java code needs to sit in the directory src/main/java, and Groovy code is expected to live under the directory src/main/groovy.
Any code that’s found in these directories is automatically compiled and put into the classpath of your regular Gradle build script.
The buildSrc directory is a great way to organize your code.
Because you’re dealing with classes, you can also put them into a specific package.
The following directory structure shows the Groovy classes in their new location:
Keep in mind that extracting the classes into their own source files requires some extra work.
The difference between defining a class in the build script versus a separate source file is that you’ll need to import classes from the Gradle API.
The following console output shows the compilation tasks that are run before the task you invoked on the command line:
The buildSrc directory is treated as its own Gradle project indicated by the path :buildSrc.
Because you didn’t write any unit tests, the compilation and execution tasks for tests are skipped.
Chapter 7 is fully dedicated to writing tests for classes in buildSrc.
In the previous sections, you learned the ins and outs of working with simple tasks, custom task classes, and specific task types provided by Gradle’s API.
We examined the difference between task action and configuration code, as well as their appropriate use cases.
An important lesson you learned is that action and configuration code is executed during different phases of the build lifecycle.
The rest of this chapter will talk about how to write code that’s executed when specific lifecycle events are fired.
As a build script developer, you’re not limited to writing task actions or configuration logic, which are evaluated during a distinct build phase.
Sometimes you’ll want to execute code when a specific lifecycle event occurs.
A lifecycle event can occur before, during, or after a specific build phase.
An example of a lifecycle event that happens after the execution phase would be the completion of a build.
Suppose you want to get feedback about failed builds as early as possible in the development cycle.
A typical reaction to a failed build could be that you send an email to all developers on the team to restore the sanity of your code.
There are two ways to write a callback to build lifecycle events: within a closure, or with an implementation of a listener interface provided by the Gradle API.
The big advantage you have with a listener implementation is that you’re dealing with a class that’s fully testable by writing unit tests.
To give you an idea of some of the useful lifecycle hooks, see figure 4.11
An extensive list of all available lifecycle hooks is beyond the scope of this book.
Many of the lifecycle callback methods are defined in the interfaces Project and Gradle.
Gradle’s Javadocs are a great starting point to find the appropriate event callback for your use case.
In the following two sections, I’ll demonstrate how to receive notifications immediately after the task execution graph has been populated.
To fully understand what’s happening under the hood when this graph is built, we’ll first look at Gradle’s inner workings.
As noted in chapter 1, the internal structure that represents these task dependencies is modeled as a directed acyclic graph (DAG)
Each task in the graph is called a node, and each node is connected by directed edges.
You’ve most likely created these connections between nodes by declaring a dependsOn relationship for a task or by leveraging the implicit task dependency interference mechanism.
It’s important to note that DAGs never contain a cycle.
In other words, a task that has been executed before will never be executed again.
Figure 4.12 demonstrates the DAG representation of the release process modeled earlier.
Don’t be afraid of making good use of lifecycle hooks.
Instead, they’re provided intentionally because Gradle can’t predict the requirements for your enterprise build.
Now that you have a better idea of Gradle’s internal task graph representation, you’ll write some code in your build script to react to it.
Recall the task makeReleaseVersion you implemented that was automatically executed as a dependency of the task release.
Because the build knows exactly which tasks will take part in the build before they get executed, you can query the task graph to check for its existence.
Figure 4.13 shows the relevant interfaces and their methods to access the task execution graph.
Listing 4.13 extends the build script by the method call whenReady to register a closure that’s executed immediately after the task graph has been populated.
Because you know that the logic is run before any of the tasks in the graph are executed, you can completely remove the task makeReleaseVersion and omit the dependsOn declaration from createDistribution.
Alternatively, you can implement this logic as a listener, which you’ll do next.
Hooking into the build lifecycle via a listener is done in two simple steps.
First, you implement the specific listener interface by writing a class within your build script.
At the time of writing, you only need to implement one.
Figure 4.13 TaskExecutionGraph provides the method whenReady that’s called when the task graph has been populated.
Registers lifecycle hook that gets called when task graph is populated.
Keep in mind that you don’t have direct access to the Project instance if you add the listener to your build script.
The following listing shows how to access the project by calling the getProject() method on the release task.
A listener can be registered through the generic addListener method or through a specific method that only takes an instance of a specialized listener type.
You’re not limited to registering a lifecycle listener in your build script.
Lifecycle logic can be applied to listen to Gradle events even before any of your project’s tasks are executed.
In the next section, we’ll explore options for hooking into the lifecycle via initialization scripts to customize the build environment.
Let’s say you want to be notified about the outcome of a build.
Whenever a build finishes, you’ll want to know whether it was successful or failed.
You also want to be able to identify how many tasks have been executed.
The plugin automatically picks the correct notification system based on your OS.
You could apply the plugin to every project individually, but why not use the powerful mechanisms Gradle provides? Initialization scripts are run before any of your build script logic has been evaluated and executed.
You’ll write an initialization script that applies the plugin to any of your projects without manual intervention.
Gradle will execute every initialization script it finds under init.d as long as the file extension matches .gradle.
An important lesson to learn in this context is that some lifecycle events are only fired if they’re declared in the appropriate location.
Every Gradle build script consists of two basic building blocks: one or more projects and tasks.
Both elements are deeply rooted in Gradle’s API and have a direct class representation.
At runtime, Gradle creates a model from the build definition, stores it in memory, and makes it accessible for you to access through methods.
You learned that properties are a means of controlling the behavior of the build.
Additionally, you can define extra properties on many of Gradle’s domain model objects (for example, on the project and task level) to declare arbitrary user data.
Later in the chapter, you learned the ins and outs of tasks.
As an example, you implemented build logic to control your project’s version numbering scheme stored in an external properties file.
You started out by adding simple tasks to the build script.
Build logic can be defined directly in the action closure of a task.
As such, it comes loaded with functionality accessible through methods of its superclass.
Understanding the build lifecycle and the execution order of its phases is crucial to beginners.
Gradle makes a clear distinction between task actions and task configurations.
Task actions, defined through the closures doFirst and doLast or its shortcut notation <<, are run during the execution phase.
Any other code defined outside of a task action is considered a configuration and therefore executed beforehand during the configuration phase.
Next, we turned our attention to implementing nonfunctional requirements: build execution performance, code maintainability, and reusability.
You added incremental build support to one of your existing task implementations by declaring its input and output data.
If the data doesn’t change between the initial and subsequent builds task, execution is skipped.
If done right, it can significantly improve the execution time of your build.
Complex build logic is best structured in custom task classes, which give you all the benefits of objectoriented programming.
You practiced writing a custom task class by transferring the existing logic into an implementation of DefaultTask.
You also cleaned up your build script by moving compilable code under the buildSrc directory.
Gradle comes with a whole range of reusable task types like Zip and Copy.
You incorporated both types by modeling a chain of task dependencies for releasing your project.
Access to Gradle’s internals is not limited to the model.
You can register build lifecycle hooks that execute code whenever the targeted event is fired.
As an example, you wrote a task execution graph lifecycle hook as a closure and listener implementation.
Initialization scripts can be used to apply common code like lifecycle listeners across all of your builds.
You already got a first taste of the mechanisms that enable you to declare a dependency on an external library.
In the next chapter, we’ll deepen your knowledge with a detailed discussion of working with dependencies and how dependency resolution works under the hood.
In chapter 3, you learned how to declare a dependency on the Servlet API to implement web components for the To Do application.
Gradle’s DSL configuration closures make it easy to declare dependencies and the repositories to retrieve them from.
First, you define what libraries your build depends on with the dependencies script.
Second, you tell your build the origin of these dependencies using the repositories closure.
With this information in place, Gradle automatically resolves the dependencies, downloads them to your machine if needed, stores them in a local cache, and uses them for the build.
We’ll take a close look at key DSL configuration elements for grouping dependencies and targeting different types of repositories.
Dependency management sounds like an easy nut to crack, but can become difficult when it comes to dependency resolution conflicts.
Transitive dependencies, the dependencies a declared dependency relies on, can be a blessing and a curse.
Complex dependency graphs can cause a mix-up of dependencies with multiple versions resulting in unreliable, nondeterministic builds.
You’ll learn how to find answers to questions like “Where does a specific dependency come from?” and “Why was this specific version picked?” to resolve version conflicts.
Having learned from the shortcomings of other dependency managers like Ivy and Maven, Gradle’s special concern is performance, build reliability, and reproducibility.
Almost all JVM-based software projects depend on external libraries to reuse existing functionality.
For example, if you’re working on a web-based project, there’s a high likelihood that you rely on one of the popular open source frameworks like Spring MVC or Play to improve developer productivity.
Libraries in Java get distributed in the form of a JAR file.
The JAR file specification doesn’t require you to indicate the version of the library.
You’ve seen small projects grow big very quickly, along with the number of third-party libraries and modules your project depends on.
Because the Java language doesn’t provide or propose any tooling for managing versioned dependencies, teams will have to come up with their own strategies to store and retrieve them.
This is the most primitive, nonautomated, and error-prone approach to handle dependencies.
Using a shared storage for JAR files (for example, a folder on a shared network drive), which gets mounted on the developer’s machine, or retrieving binaries over FTP.
This approach requires the developer to initially establish the connection to the binary repository.
New dependencies will need to be added manually, which potentially requires write permissions or access credentials.
Checking JAR files that get downloaded with the project source code into the VCS.
This approach doesn’t require any additional setup and bundles source code and all dependencies as one consistent unit.
Your team can retrieve changes whenever they update their local copy of the repository.
On the downside, binary files unnecessarily use up space in the repository.
Changing working copies of a library requires frequent check-ins whenever there’s a change to the source code.
This is especially true if you’re working with projects that depend on each other.
While all of these approaches work, they’re far from being sufficient solutions, because they don’t provide a standardized way to name and organize the JAR files.
At the very least, you’ll need to know the exact version of the library and the dependencies it depends on, the transitive dependencies.
If not documented meticulously, you can never be sure which features are actually supported by the library version in your project.
Upgrading a library to a newer version becomes a guessing game, because you don’t know exactly what version you’re upgrading from.
In fact, you may actually be downgrading without knowing it.
These are the libraries your first-level dependencies require in order to work correctly.
Popular Java development stacks like the combination of Spring and Hibernate can easily bring in more than 20 additional libraries from the start.
A single library may require many other libraries in order to work correctly.
Figure 5.1 shows the dependency graph for Hibernate’s core library.
Trying to manually determine all transitive dependencies for a specific library can be a real time-sink.
Many times this information is nowhere to be found in the library’s documentation and you end up on a wild-goose chase to get your dependencies right.
As a result, you can experience unexpected behavior like compilation errors and runtime class-loading issues.
I think we can agree that a more sophisticated solution is needed to manage dependencies.
Optimally, you’ll want to be able to declare your dependencies and their respective versions as project metadata.
As part of an automated process, they can be retrieved from a central location and installed for your project.
Let’s look at existing open source solutions that support these features.
The Java space is mostly dominated by two projects that support declarative and automated dependency management: Apache Ivy, a pure dependency manager that’s mostly used with Ant projects, and Maven, which contains a dependency manager as part of its build infrastructure.
I’m not going to go into deep details of any of these solutions.
Instead, the purpose of this section is to explain the concepts and mechanics of automated dependency management.
In Ivy and Maven, dependency configuration is expressed through an XML descriptor file.
The configuration consists of two parts: the dependency identifiers plus their respective versions, and the location of the binary repositories (for example, an HTTP address you want to retrieve them from)
The dependency manager evaluates this information and automatically targets those repositories to download the dependencies onto your local machine.
Libraries can define transitive dependencies as part of their metadata.
The dependency manager is smart enough to analyze this information and resolve those dependencies as part of the retrieval process.
If a dependency version conflict is recognized, as demonstrated by the example of Hibernate core, the dependency manager will try to resolve it.
Once downloaded, the libraries are stored in a local cache.
Now that the configured libraries are available on your developer machine, they can be used for your build.
Subsequent builds will first check the local cache for a library to avoid unnecessary requests to a repository.
Figure 5.2 illustrates the key elements of automated dependency management.
Using a dependency manager frees you from the burden of manually having to copy or organize JAR files.
Gradle provides a powerful out-of-the-box dependency management implementation that fits into the architecture just described.
It describes the dependency configuration as part of Gradle’s expressive DSL, has support for transitive dependency management, and plays well with existing repository infrastructures.
Before we dive into the details, let’s look at some of the challenges you may face with dependency management and how to cope with them.
Even though dependency management significantly simplifies the handling of external libraries, at some point you’ll find yourself dealing with certain shortcomings that may compromise the reliability and reproducibility of your build.
Many of these projects publish their releases to a centrally hosted repository.
One of the most widely used repositories is Maven Central.
If Maven Central is the only repository your build relies on, you’ve automatically created a single point of failure for your system.
In case the repository is down, you’ve stripped yourself of the ability to build your project if a dependency is required that isn’t available in your local cache.
You can avoid this situation by configuring your build to use your own custom in-house repository, which gives you full control over server availability.
If you’re eager to learn about it, feel free to directly jump to chapter 14, which talks about how to set up and use open source and commercial repository managers like Sonatype Nexus and JFrog’s Artifactory.
A dependency manager analyzes this information, builds a dependency graph from it, and resolves all nested dependencies for you.
Using transitive dependency management is a huge timesaver and enables traceability for your dependency graph.
Unfortunately, neither the metadata nor the repository guarantees that any of the artifacts declared in the metadata actually exist, are defined correctly, or are even needed.
You may encounter problems like missing dependencies, especially on repositories that don’t enforce any quality control, which is a known issue on Maven Central.
Figure 5.3 demonstrates the artifact production and consumption lifecycle for a Maven repository.
Gradle allows for excluding transitive dependencies on any level of the dependency graph.
Alternatively, you can omit the provided metadata and instate your own transitive dependency definition.
You’ll find that popular libraries will appear in your transitive dependency graph with different versions.
This is often the case for commonly used functionality like logging frameworks.
The dependency manager tries to find a smart solution for this problem by picking one of these versions based on a certain resolution strategy to.
To do so, you’ll first want to find out which dependencies bring in what version of a transitive dependency.
Now let’s see how Gradle implements these ideas with the help of a full-fledged example.
In chapter 3, you saw how to use the Jetty plugin to deploy a To Do application to an embedded Jetty Servlet container.
With its lightweight container implementation, it provides fast startup times.
Many enterprises use other web application container implementations in their production environments.
Let’s assume you want to build support for deploying your web application to a different container product, such as Apache Tomcat.
The open source project Cargo (http://cargo.codehaus.org/) provides versatile support for web application deployment to a variety of Servlet containers and application servers.
Cargo supports two implementations you can use in your project.
On the one hand, you can utilize a Java API, which gives you fine-grained access to each and every aspect of configuring Cargo.
On the other hand, you can choose to execute a set of preconfigured Ant tasks that wrap the Java API.
Because Gradle provides excellent integration with Ant, our examples will be based on the Cargo Ant tasks.
Let’s revisit figure 5.1 and see how the components change in the context of a Gradle use case.
In chapter 3 you learned that dependency management for a project is configured with the help of two DSL configuration blocks: dependencies and repositories.
The names of the configuration blocks directly map to methods of the interface Project.
For your use case, you’re going to use Maven Central because it doesn’t require any additional setup.
Figure 5.4 shows that dependency definitions are provided through Gradle’s DSL in the build.gradle file.
The dependency manager will evaluate this configuration at runtime, download the required artifacts from.
Figure 5.3 Bad metadata complicates the use of transitive dependencies.
Dependency metadata in Maven repositories is represented by a project object model (POM) file.
If the library developer provides incorrect metadata, the consumer will inherit the problems.
Dependency configurations a central repository, and store them in your local cache.
You’re not using a local repository, so it’s not shown in the figure.
The following sections of this chapter discuss each of the Gradle build script configuration elements one by one.
Not only will you learn how to apply them to the Cargo example, you’ll also learn how to apply dependency management to implement the requirements of your own project.
Let’s first look at a concept that will become more important in the context of our example: dependency configurations.
In chapter 3, you saw that plugins can introduce configurations to define the scope for a dependency.
The Java plugin brings in a variety of standard configurations to define which bucket of the Java build lifecycle a dependency should apply to.
For example, dependencies required for compiling production source code are added with the compile configuration.
In the build of your web application, you used the compile configuration to declare a dependency on the Apache Commons Lang library.
To get a better understanding of how configurations are stored, configured, and accessed, let’s look at responsible interfaces in Gradle’s API.
Configurations can be directly added and accessed at the root level of a project; you can decide to use one of the configurations provided by a plugin or declare your own.
You can control whether transitive dependencies should be part of the dependency resolution, define the resolution strategy (for example, how to respond to conflicting artifact versions), and even make configurations extend to each other.
Figure 5.5 shows the relevant Gradle API interfaces and their methods.
Figure 5.4 Declaring a dependency on the Cargo libraries in a Gradle build.
Another way of thinking of configurations is in terms of a logical grouping.
Grouping dependencies by configuration is a similar concept to organizing Java classes into packages.
The Java plugin already provides six configurations out of the box: compile, runtime, testCompile, testRuntime, archives, and default.
Couldn’t you just use one of those configurations to declare a dependency on the Cargo libraries? Generally, you could, but you’d mix up dependencies that are relevant to your application code and the infrastructure code you’re writing for deploying the application.
Adding unnecessary libraries to your distribution can lead to unforeseen side effects at runtime and should be avoided at all costs.
For example, using the compile configuration will result in a WAR file that contains the Cargo libraries.
Next, I’ll show how to define a custom configuration for the Cargo libraries.
To clearly identify the dependencies needed for Cargo, you’ll need to declare a new configuration with the unique name cargo, as demonstrated in the following listing.
Figure 5.5 Configurations can be added and accessed through the Project instance.
For now, you’re only dealing with a single Gradle project.
Limiting the visibility of this configuration to this project is a conscious choice in preparation for a multiproject setup.
You don’t want to let configurations spill into other projects if they’re not needed.
The description that was set for the configuration is directly reflected when you list the dependencies of the project:
After adding a configuration to the configuration container of a project, it can be accessed by its name.
Next, you’ll use the cargo configuration to make the third-party Cargo Ant task public to the build script.
Essentially, Ant tasks are Java classes that adhere to Ant’s extension endpoint for defining custom logic.
To add a nonstandard Ant task like the Cargo deployment task to your project, you’ll need to declare it using the Taskdef Ant task.
To resolve the Ant task implementation class, the Cargo JAR files containing them will need to be assigned.
The next listing shows how easy it is to access the configuration by name.
The task uses the resolved dependencies and assigns them to the classpath required for the Cargo Ant task.
Uses Cargo Ant task to automatically download a Tomcat 7 distribution, deploy a WAR file, and run it in container.
Don’t worry if you don’t understand everything in the code example.
The important part is that you recognize the Gradle API methods that allow you access to a configuration.
The rest of the code is mostly Ant-specific configurations expressed through Gradle’s DSL.
Chapter 9 will give you the inside scoop on using Ant tasks from Gradle.
With the deployment task set up, it’s time to assign the Cargo dependencies to the cargo configuration.
Chapter 3 gave you a first taste of how to tell your project that an external library is needed for it to function correctly.
The DSL configuration block dependencies is used to assign one or more dependencies to a configuration.
External dependencies are not the only dependencies you can declare for your project.
Table 5.1 gives you an overview of the various types of dependencies.
In this book we’ll discuss and apply many of these options.
Some of the dependency types are explained in this chapter, but others will make more sense in the context of another chapter.
In this chapter we’ll cover external module dependencies and file dependencies, but first let’s see how dependency support is represented in Gradle’s API.
Every Gradle project has an instance of a dependency handler, which is represented by the interface DependencyHandler.
You obtain a reference to the dependency handler by using the project’s getter method getDependencies()
Each of the dependency types presented in table 5.1 is declared through a method of the dependency handler within the project’s dependencies configuration block.
External module dependency A dependency on an external library in a repository including its provided metadata.
Project dependency A dependency on another Gradle project Section 6.3.3
File dependency A dependency on a set of files in the file system.
Client module dependency A dependency on an external library in a repository with the ability to declare the metadata yourself.
Gradle runtime dependency A dependency on Gradle’s API or a library shipped with the Gradle runtime.
Figure 5.6 illustrates the relationship between the project, the dependency handler, and the actual dependencies.
Let’s first look at how to declare external module dependencies, their notation, and how to configure them to meet your needs.
In Gradle’s terminology, external libraries, usually in the form of JAR files, are called external module dependencies.
They represent a dependency on a module outside of the project hierarchy.
This type of dependency is characterized by attributes that clearly identify it within a repository.
In the following section, we’ll discuss each attribute one by one.
At a minimum, a dependency needs to provide a name.
Let’s review the dependency attributes with the help of the Hibernate core library we examined in section 5.1.2:
The group may use a dot notation, but it’s not mandatory.
In the case of the Hibernate library, the group is org.hibernate.
Many times the version string consists of a major and a minor version.
Figure 5.6 Different types of dependencies can be added on the project level.
Now that we’ve reviewed some dependency attributes, we can look more closely at how Gradle expects them to be declared in the build script.
You first state the name of the configuration you want to assign the dependencies to and then a list of dependencies in the notation of your choice.
You can either provide a map of attribute names and their values, or the shortcut notation as a string that separates each attribute by a colon (see figure 5.7)
After defining the configuration, you can easily use it to assign the relevant Cargo dependencies.
To use Cargo in your project, you’ll need to provide JAR files containing the Cargo API, the core container implementations, and the Cargo Ant tasks.
Thankfully, Cargo provides an UberJar, a single JAR file that packages the API and container functionality, which will make the dependency management easier.
The following listing shows how to assign the relevant Cargo dependencies to the cargo configuration.
If you deal with a lot of dependencies in your project, it’s helpful to break out commonly used dependency attributes as extra properties.
You do that in the example code by creating and using properties for Cargo’s dependency group and version attributes.
Dependency declaration using map containing group, name, and version attributes.
So far, we haven’t talked about different types of repositories and how to configure them.
For the sake of getting this example running, add the following repositories configuration block:
There’s no need to fully understand the intricacies of this code snippet.
The important point is that you configured your project to use Maven Central to download the Cargo dependencies.
Later in this chapter, you’ll learn how to configure other repositories.
The tree shows the top-level dependencies you declared in the build script, as well as their transitive dependencies:
Indicates both requested and selected version to resolve version conflict of library.
If you examine the dependency tree carefully, you’ll see that dependencies marked with an asterisk have been omitted.
That means that the dependency manager selected either the same or another version of the library because it was declared as a transitive dependency of another top-level dependency.
Interestingly, this is the case for the UberJar, so you don’t even have to declare it in your build script.
The Ant tasks library will automatically make sure that the library gets pulled in.
Gradle’s default resolution strategy for version conflicts is newest first—that is, if the dependency graph contains two versions of the same library, it automatically selects the newest.
As you can see, it’s very helpful to analyze the information exposed by the dependency report.
When you want to find out which top-level dependency declares a specific transitive dependency and why a specific version of a library has been selected or omitted, the dependency report is a good place to start.
Gradle gives you full control over transitive dependencies, so you can decide to either fully exclude all transitive dependencies or selectively exclude specific dependencies.
Let’s say you explicitly want to specify a different version of the library xml-apis instead of using the transitive dependency provided by Cargo’s UberJar.
In practice, this is often the case when some of your own functionality is built on top of a specific version of an API or framework.
The next listing shows how to use the exclude method from ModuleDependency to exclude a transitive dependency.
Notice that the exclusion attributes are slightly different from the regular dependency notation.
Gradle doesn’t allow you to exclude only a specific version of a dependency, so the version attribute isn’t available.
Sometimes the metadata of a dependency declares transitive dependencies that don’t exist in the repository.
This is only one of the situations when you want to have full control over transitive dependencies.
Gradle lets you exclude all transitive dependencies using the transitive attribute, as shown in the following listing.
So far, you’ve only declared dependencies on specific versions of an external library.
Let’s see how to resolve the latest version of a dependency or the latest within a range of versions.
Alternatively, you can declare the part of the version attribute you want to be dynamic by demarcating it with a plus sign (+)
The following listing shows how to resolve the latest 1.x version of the Cargo Ant library.
Gradle’s dependencies help task clearly indicates which version has been picked:
Exclusions can be declared in a shortcut or map notation.
Another option is to select the latest within a range of versions for a dependency.
To learn more about the syntax, feel free to check Gradle’s online manual.
As described earlier, projects that don’t use automated dependency management organize their external libraries as part of the source code or in the local file system.
Especially when migrating your project to Gradle, you don’t want to change every aspect of your build at once.
Gradle makes it easy for you to configure file dependencies.
You’ll emulate this for your project by referencing the Cargo libraries in the local file system.
The following listing shows a task that copies the dependencies resolved from Maven Central to the subdirectory libs/cargo under your user home directory.
After running the task, you’ll be able to declare the Cargo libraries in your dependencies configuration block.
The next listing demonstrates how to assign all JAR files to the cargo configuration as a file dependency.
Because you’re not dealing with a repository that requires you to declare dependencies with a specific pattern, you also don’t need to define a repositories configuration block.
Next, we’ll focus on the various repository types supported by Gradle and how they’re configured.
When should I use dynamic versions? The short answer is rarely or even never.
Choosing the latest version of a library may cause your build to fail.
Even worse, without knowing it, you may introduce incompatible library versions and side effects that are hard to find and only occur at runtime of your application.
Therefore, declaring the exact version of a library should be the norm.
Listing 5.7 Copying the Cargo dependencies to your local file system.
Gradle puts a special emphasis on supporting existing repository infrastructures.
You’ve already seen how to use Maven Central in your build.
By using a single method call, mavenCentral(), you configured your build to target the most popular Java binary repository.
Apart from the preconfigured repository support, you can also assign an arbitrary URL of a Maven or Ivy repository and configure it to use authentication if needed.
Alternatively, a simple file system repository can be used to resolve dependencies.
If metadata is found for a dependency, it will be downloaded from the repository as well.
Table 5.2 shows the different types of repositories and what section to go to next to learn more about it.
Feel free to jump to the section that describes the repository you want to use in your project.
In the next section, we’ll look at Gradle’s API support for defining and configuring repositories before we apply each of them to practical examples.
Central to defining repositories in your project is the interface RepositoryHandler, which provides methods to add various types of repositories.
From the project, these methods are invoked within your repositories configuration block.
When the dependency manager tries to download the dependency and its metadata, it checks the repositories in the order of declaration.
Subsequent repository declarations won’t be checked further for the specific dependency.
As shown in figure 5.8, each of the repository interfaces exposes different methods specific to the type of repository.
It’s up to your project’s needs to declare the repository most fitting.
In the next section, we’ll look at the syntax to declare Maven repositories.
Maven repositories are among the most commonly used repository types in Java projects.
The library is usually represented in the form of a JAR file.
Maven repository A Maven repository on the local file system or a remote server, or the preconfigured Maven Central.
Ivy repository An Ivy repository on the local file system or a remote server with a specific layout pattern.
Flat directory repository A repository on the local file system without metadata support.
Both artifacts are stored in a predefined directory structure in the repository.
When you declare a dependency in your build script, its attributes are used to derive the exact location in the repository.
The dot character in the group attribute of a dependency indicates a subdirectory in the Maven repository.
Figure 5.9 shows how the Cargo Ant dependency attributes are mapped to determine the location of the JAR and POM files in the repository.
The interface RepositoryHandler provides two methods that allow you to define preconfigured Maven repositories.
The method mavenCentral() adds a reference to Maven Central to the list of repositories, and the method mavenLocal() refers to a local Maven repository in your file system.
Let’s review both repository types and discuss when you’d use them in your project.
Gradle wants to make it as easy for the build developer as possible, and therefore provides you with a shortcut to declare Maven Central.
Figure 5.8 Relevant interfaces in Gradle’s API for configuring various types of repositories.
Gradle supports repository implementations for flat directories, Maven, and Ivy.
Using and configuring repositories each and every time, you can just call the method mavenCentral(), as shown in the following code snippet:
The location of this cache in your local file system is different than the directory in which Maven stores artifacts after downloading them.
You may wonder when you’d want to use a local Maven repository now that you’re dealing with Gradle.
This is especially the case if you work in a mixed environment of build tools.
Imagine you’re working on one project that uses Maven to produce a library, and another project operating with Gradle wants to consume the library.
Especially during development, you’ll go through cycles of implementing changes and trying out the changes on the consuming side.
To prevent you from having to publish the library to a remote Maven repository for every little change, Gradle provides you with the option to target a local Maven repository, as shown in the following repository declaration:
Figure 5.9 How a dependency declaration maps to artifacts in a Maven repository.
Be aware that using a local Maven repository should be limited to this specific use case, as it may cause unforeseen side effects.
You’re explicitly dependent on artifacts that are only available in the local file system.
Running the script on other machines or a continuous integration server may cause the build to fail if the artifacts don’t exist.
Perhaps a specific dependency is simply not available, or you want to ensure that your build is reliable by setting up your own enterprise repository.
One of the options a repository manager gives you is to configure a repository with a Maven layout.
This means that it adheres to the artifact storage pattern we discussed before.
Additionally, you can protect access to your repository by requiring the user to provide authentication credentials.
Gradle’s API supports two ways of configuring a custom repository: maven() and mavenRepo()
The following listing shows how to target an alternative public Maven repository if an artifact isn’t available in Maven Central.
I can’t discuss every available configuration option in this chapter, so please refer to the online documentation for more information.
Let’s see how an Ivy repository is different from a Maven repository and its configuration.
Artifacts in a Maven repository have to be stored with a fixed layout.
On the other hand, even though an Ivy repository proposes a default layout, it’s fully customizable.
In Ivy, repository dependency metadata is stored in a file named ivy.xml.
Gradle provides a wide variety of methods to configure Ivy repositories and their specific layout in your build.
It goes beyond the scope of this book to cover all options, but let’s look at one example.
Imagine you want to resolve the Cargo dependencies from an Ivy repository.
The following listing demonstrates how to define the repository base URL, as well as the artifact and metadata layout pattern.
As with the POM in Maven repositories, you’re not forced to use the Ivy metadata to resolve transitive dependencies.
The Ivy repository is perfect for resolving dependencies that don’t necessarily follow the standard Maven artifact pattern.
For example, you could decide to place JAR files into a specific directory of a web server and serve it up via HTTP.
To complete our discussion about repositories, we’ll look at flat directories.
The simplest and most rudimentary form of a repository is the flat directory repository.
It’s a single directory in the file system that contains only the JAR files, with no metadata.
If you’re used to manually maintaining libraries with your project sources and planning to migrate to automated dependency management, this approach will interest you.
When you declare your dependencies, you can only use the attributes name and version.
The group attribute is not evaluated and leads to an unresolved dependency if you try to use it.
The next listing shows how to declare the Cargo dependencies as a map and shortcut notation retrieved from a flat directory repository.
This listing also perfectly demonstrates how useful it is to be able to use metadata that automatically declares transitive dependencies.
Listing 5.11 Cargo dependencies declaration retrieved from a flat directory repository.
So far we’ve discussed how to declare dependencies and configure various types of repositories to resolve those artifacts.
Gradle automatically determines whether a dependency is needed for the task you want to execute, downloads the artifacts from the repositories, and stores them in the local cache.
In this section, we’ll dig deeper by analyzing the cache structure, identifying how the cache works under the hood and how to tweak its behavior.
Let’s explore the local cache structure through the example of your Cargo libraries.
You know Gradle downloaded the JAR files when you ran the deployment task, but where did it put them? If you check the Gradle forum, you’ll find that many users frequently ask for it.
The following listing shows how to print out the full, concatenated path of all dependencies assigned to the configuration cargo.
This path will probably look slightly different on your machine.
Let’s dissect this path even more and give it some more meaning.
The next part of the path, artifact-15, is an identifier that’s specific to the Gradle version.
It’s needed to differentiate changes to the way metadata is stored.
Bear in mind that this structure may change with newer versions of Gradle.
The subdirectory filestore contains the raw binaries downloaded from the repository.
Additionally, you’ll find some binary files that store metadata about the downloaded artifacts.
Listing 5.12 Printing the concatenated file path of all Cargo dependencies.
The following directory tree shows the contents from the root level of a local dependency cache:
The filestore directory is a natural representation of a dependency.
The attributes group, name, and version directly map to subdirectories in the file system.
In the next section, we’ll discuss the benefits Gradle’s cache brings to your build.
The real power of Gradle’s cache lies in its metadata.
It enables Gradle to implement additional optimizations that lead to smarter, faster, and more reliable builds.
While running the build for the first time, the dependency gets downloaded and stored in the cache.
Subsequent builds will happily use the dependency available in the cache.
What would happen if the structure of the repository were changed (for example, one of the attributes was renamed or the dependency moved or was simply deleted)—something you as an artifact consumer have no control over? With many other dependency managers like Maven and Ivy, the build would work just fine, because the dependency exists in the local cache and can be resolved.
However, for any other developer that runs the build on a different machine, the build would fail.
It knows the location a dependency originates from and stores this information in the cache.
This is not only the case for dependencies that were already downloaded.
Gradle uses this information to avoid having to check the repository every time the build runs.
The same is true for artifacts that were stored with other versions of Gradle.
Gradle detects if an artifact was changed in the repository by comparing its local and remote checksum.
Unchanged artifacts are not downloaded again and reused from the local cache.
Imagine the artifact was changed on the repository but the checksum is still the same.
This could happen if the administrator of the repository replaces an artifact with the same version.
Ultimately, your build will use an outdated version of the artifact.
Gradle’s dependency manager tries to eliminate this situation by taking additional information into consideration.
For example, it can ensure an artifact’s uniqueness by comparing the value of the HTTP header parameter contentlength or the last modified date.
This is an advantage Gradle’s implementation has over other dependency managers like Ivy.
Sometimes this behavior is undesirable; for example, if you’re traveling and don’t have access to the Internet.
You can tell Gradle to avoid checking remote repositories by running in offline mode with the --offline command-line option.
Instead of performing dependency resolution over the network, only dependencies from the local cache will be used.
If a required dependency doesn’t exist in the cache, the build will fail.
If your project deals with many dependencies and you choose to use automatic resolution for transitive dependencies, version conflicts are almost inevitable.
Gradle’s default strategy to resolve those conflicts is to pick the newest version of a dependency.
The dependency report is an invaluable tool for finding out which version was selected for the dependencies you requested.
In the following section, I’ll show how to troubleshoot version conflict and tweak Gradle’s dependency resolution strategy to your specific use case.
Gradle won’t automatically inform you that your project dealt with a version conflict.
Having to constantly run the dependency report to find out isn’t a practical approach to the problem.
Instead, you can change the default resolution strategy to fail the build whenever a version conflict is encountered, as shown in the following code example:
Failing can be helpful for debugging purposes, especially in the early phases of setting up the project and changing the set of dependencies.
Running any of the project’s tasks will also indicate the version conflict, as shown in the following sample output:
The more projects you have to manage, the more you may feel the need to standardize the build environment.
You’ll want to share common tasks or make sure that all projects use a specific version of a library.
For example, you want to unify all of your web projects to be deployed with Cargo version 1.3.0, even though the dependency declaration may request a different version.
With Gradle, it’s really easy to implement such an enterprise strategy.
It enables you to enforce a specific version of a top-level dependency, as well as a transitive dependency.
The following code snippet demonstrates how to reconfigure the default resolution strategy for the configuration cargo to force a dependency on version 1.3.0 of the Ant tasks:
Now when you run the dependency report task, you’ll see that the requested Cargo Ant version was overruled by the globally enforced module version:
Gradle’s resolution result API gives you an even more fine-grained control over the requested and selected dependencies.
A good place to start geting familiar with the API is the interface ResolutionResult.
A change to the resolution strategy of a configuration, as shown previously, is perfectly placed in an initialization script so it can be enforced on a global level.
The build script user may not know why this particular version of the Cargo Ant tasks has been picked.
The only thing they saw was that the dependency report indicated that a different version was selected.
Sometimes you may want to know what forced this version to be selected.
Gradle provides a different type of report: the dependency insight report, which explains how and why a dependency is in the graph.
To run the report, you’ll need to provide two parameters: the name of the configuration (which defaults to the compile configuration) and the dependency itself.
The following invocation of the help task dependencyInsight shows the reason, as well as the requested and selected version of the dependency xml-apis:xml-apis:
While the dependency report starts from the top-level dependencies of a configuration, the insight report shows the dependency graph starting from the particular dependency down to the configuration.
As such, the insight report represents the inverted view of the regular dependency report, as shown in figure 5.10
To avoid having to hit a repository over and over again for specific types of dependencies, Gradle applies certain caching strategies.
This is the case for snapshot versions of a dependency and dependencies that were declared with a dynamic version pattern.
Once resolved, they’re cached for 24 hours, which leads to snappier, more efficient builds.
After the artifact caching timeframe is expired, the repository is checked again and a new version of the artifact is downloaded if it has changed.
Reason why a particular dependency was selected is shown in brackets.
This flag forces a check for changed artifact versions with the configured repositories.
If the checksum changed, the dependency will be downloaded again and replace the existing copy in the cache.
Having to add the command-line options can become tiring after a while, or you may forget to tag it on.
Alternatively, you can configure a build to change the default behavior of your cache.
You can set the cache timeout for dynamic dependency versions to 0 seconds, as shown in the following code snippet:
You may have good reasons for not wanting to cache a SNAPSHOT version of an external module.
For example, another team in your organization works on a reusable library that’s shared among multiple projects.
During development the code changes a lot, and you always want to get the latest and (hopefully) greatest additions to the code.
The following code block modifies the resolution strategy for a configuration to not cache SNAPSHOT versions at all:
Most projects, be they open source projects or an enterprise product, are not completely self-contained.
They depend on external libraries or components built by other projects.
While you can manage those dependencies yourself, the manual approach doesn’t fulfill the requirements of modern software development.
The more complex a project becomes, the harder it is to figure out the relationships between dependencies, resolve potential version conflicts, or even know why you need a specific dependency.
Figure 5.10 View of dependency graph with different report types.
With automated dependency management, you declare dependencies by unique identifiers within your project without having to manually touch the artifacts.
At runtime, the dependent artifacts are automatically resolved in a repository, downloaded, stored in a local cache, and made available to your project.
We discussed potential pitfalls and how to cope with them.
You learned how to declare different types of dependencies, group them with the help of configurations, and target various types of repositories to download them.
The local cache is an integral part of Gradle’s dependency management infrastructure and is responsible for high-performance and reliable builds.
Knowing how to troubleshoot dependency version conflicts and fine-tune the cache is key to a stable and reliable build.
You used Gradle’s dependency reporting to get a good understanding of the resolved dependency graph, as well as why a specific version of a dependency was selected and where it came from.
I showed strategies for changing the default resolution strategy and cache behavior, as well as appropriate situations that make them necessary.
In the next chapter, you’ll take your To Do application to the next level by modularizing the code.
You’ll learn how to use Gradle’s multiproject build support to define dependencies between individual components and make them function as a whole.
The code base of every active software project will grow over time.
What started as a small project with a handful of classes may quickly become a collection of packages and classes with different responsibilities.
To improve maintainability and prevent tight coupling, you’ll want to group code into modules based on particular functionality and logical boundaries.
Modules are usually organized hierarchically and can define dependencies on each other.
The build tool needs to be able to cope with these requirements.
Because every module in Gradle is a project, we call them multiproject builds (as opposed to Maven’s use of multimodule builds)
By the end of the chapter, you’ll know how to apply the technique that best fits the needs of your own project and model your build appropriately.
Gradle support for multimodule builds will be explained with the help of your To Do web application.
You’ll start by deconstructing the existing project structure and break out individual, functional subprojects.
This newly created project layout will serve as a basis for modeling the build.
Then we’ll go over the options for organizing your build logic and you’ll get to know the part of the Gradle API that helps define individual and common project behavior.
Finally, you’ll learn how to control the project execution order by declaring project dependencies, and how to execute a single subproject or the full build for all participating subprojects from the root project.
Not only will this chapter teach you the structure of a multiproject build, but you’ll also learn how to bring down your build’s execution time, something everyone can appreciate.
You’ll start by refactoring the existing To Do application project structure into a modularized architecture.
In enterprise projects, the package hierarchy and class relationships can become highly complex.
Separating code into modules is a difficult task, because it requires you to be able to clearly identify functional boundaries—for example, separating business logic from data persistence logic.
Two major factors will determine how easy it is to implement separation of concerns for a project: coupling and cohesion.
Coupling measures the strength of relationships between specific code artifacts like classes.
Cohesion refers to the degree to which components of a module belong together.
The less coupled and the higher the cohesion of your code, the easier it will be to perform the restructuring of your project.
Teaching good software design practices is beyond the scope of this book, but there are two guidelines you should keep in mind: minimize coupling and maximize cohesion.
An example of a modularized architecture done right is the Spring framework.
Spring is an open source framework that provides a wide range of services needed in many enterprise Java applications.
For example, the functionality of a service support for a simplified MVC web application development or transaction management is distributed as a JAR file.
Services depend on each other if they need the functionality provided by a different module.
It defines a lot of components that depend on each other.
But in practice, you won’t need to import the whole framework with all components into your project.
You can pick and choose which service of the framework you want to use.
Using Gradle’s dependency management makes resolving these transitive dependencies a piece of cake.
In the following sections, you’ll modularize the To Do application and use Gradle’s multiproject features to build it.
With the limited code base you have at the moment, this will be a much easier task than it is for developers of the Spring framework.
We’ll get started by identifying the modules for your application.
Let’s review the code you already wrote for the To Do application to find its natural boundaries.
These boundaries will help you break the application code into modules.
JEE-specific web componen and static content like images and CSS files.
You already did a good job of separating the concerns of the application by grouping classes with specific functionality into packages.
You’re going to use these packages as guidelines for finding the functional boundaries of your application:
Even in your fairly simple application, these modules have relationships between each other.
For example, the classes in the Repository module use the Model data classes to transport the data in and out of the data storage.
Figure 6.2 gives the full picture of all proposed modules and their relationships.
With the identified modules and their relationships in mind, you can get started breaking them out of the single project.
It’s easy to refactor the existing project structure into the identified modules.
For each of the modules, you’ll create a subdirectory with the appropriate name and move the relevant files underneath it.
The default source directory src/main/java will stay intact for each of the modules.
The only module that requires the default web application source directory src/main/webapp is the Web module.
Now it’s time to take care of the build infrastructure.
In the last section, you defined a hierarchical directory structure for your project.
The full project consists of a root directory and one subdirectory per module.
In this section, you’ll learn how to build such a project structure with Gradle.
Your starting point will be the obligatory build.gradle file on the root level of the directory tree.
Create an empty build script and check the projects participating in your build by running gradle projects:
Repository module containing classes for storing and retrieving to-do items.
Gradle reports that you’re only dealing with a single project.
When setting up a build with more than one project, Gradle’s vocabulary speaks of a multiproject build.
The reason is that you’ll represent each of the modules as a distinct Gradle project.
From this point forward, we won’t use the term module anymore in keeping with Gradle’s syntax; we’ll only talk about projects.
The overarching project located in the top-level directory is called the root project, and it has its own right to exist in a multiproject build.
It coordinates building the subprojects and can define common or specific behavior for them.
Figure 6.3 gives you a graphical overview of the hierarchical project structure you’re going to achieve.
So far we’ve only dealt with the Gradle configuration of single-project builds.
You saw that separating your code into multiple projects wasn’t all that hard.
What’s missing is the build support that represents the root project and its subprojects.
The declaration of subprojects in a multiproject build is done via the settings file.
The settings file declares the configuration required to instantiate the project’s hierarchy.
By default, this file is named settings.gradle and is created alongside the build.gradle file of the root project.
The following listing shows the contents of the settings file.
For each of the subprojects you want to be part of the build, you call the method include with the argument of the project’s path.
The supplied project path in this snippet is the project directory relative to the root directory.
Keep in mind that you can also model a deeper project hierarchy.
A colon character (:) separates each level of the subproject hierarchy.
For example, if you wanted to map the directory structure model/todo/items, you’d add the subproject via the path model:todo:items.
Executing the help task projects after adding the settings file will produce a different result:
Figure 6.3 Hierarchical multiproject structure for To Do application, which defines three subprojects.
Adds given subproject to build; argument passed to include method is project path, not file path.
Instead of calling method include for each project individually, pass a String[] of projects to a single call.
Root project 'todo' +--- Project ':model' +--- Project ':repository' \--- Project ':web'
By adding a single settings file, you created a multimodule build containing a root project and three subprojects.
Let’s go deeper into the details of the settings file.
You may have guessed already that there’s an API representation for it that you can use to query and modify the configuration of your build.
Before Gradle assembles the build, it creates an instance of type Settings.
The interface Settings is a direct representation of the settings file.
Its main purpose is to add the Project instances that are supposed to participate in a multiproject build.
In addition to assembling your multiproject build, you can do everything you’re used to in your build.gradle script because you have direct access to the Gradle and Project interfaces.
Figure 6.4 shows the relevant methods of the Settings interface and its associations.
The important takeaway here is that you’re coding toward an instance of the interface Settings in your settings.gradle file.
Any method of the interface Settings can be directly invoked as you did by calling include.
Next, we’ll discuss when the settings file is executed during the build lifecycle and what rules are applied for resolving the file.
Subprojects are displayed in the form of an indented, hierarchical tree.
You can use the Settings instance to retrieve the project descriptor or project instance through the interface Gradle.
Think back to chapter 4 when we discussed the three distinct lifecycle phases of a build.
You may already have an idea during what phase the code of the settings file is evaluated and executed.
It needs to happen during the initialization phase before any of the Project instances can be configured, as shown in figure 6.5
When executing a build, Gradle automatically figures out whether a subproject is part of a single- or multiproject build.
Let’s examine the set of rules Gradle uses to determine the existence of a settings file.
Gradle allows you to run your build from the root project directory or any of the subproject directories as long as they contain a build file.
How does Gradle know that a subproject is part of a multiproject build? It needs to find the settings file, which indicates whether the subproject is included in a multiproject build.
Figure 6.6 shows the two-step process Gradle uses to find a settings file.
In step 1, Gradle searches for a settings file in a directory called master with the same nesting level as the current directory.
If no settings file is found in step 1, Gradle searches for a settings file in the parent directories, starting from the current directory.
If one of the steps finds a settings file and the project is included in its definition, the project is considered part of a multiproject build.
Step 2 in the settings file resolution process applies to a hierarchical project layout you set up earlier.
Accessing Settings from the build file If you need access to the Settings instance from your build.gradle file after the settings have been loaded and evaluated, you can register a lifecycle closure or listener.
Figure 6.5 The settings file is evaluated and executed during the initialization phase.
Gradle projects can be structured hierarchically or with a flat layout, as shown in figure 6.7
We speak of a flat multiproject layout if the participating projects exist on the same directory level as the root project.
As a consequence, this means that the nesting level for subprojects can only be one level deep.
The layout you choose for your project is up to you.
Personally, I prefer the hierarchical project layout, as it gives you a more fine-grained control to model your components.
Figure 6.7 compares the differences between setting up the To Do application project with a hierarchical and a flat layout.
Instead of putting the build and settings file on the root level of the project, you’ll have to create a dedicated directory alongside.
Controlling the settings file search behavior There are two command-line parameters that are helpful in determining the search behavior for a settings file:
This option is useful if you want to avoid the performance hit of searching all parent directories in a deeply nested project structure.
You may want to use this option if your settings filename deviates from the standard naming convention.
Settings file Figure 6.6 Settings file resolution is a two-step process.
Choose the directory name master so you can execute the build from your subprojects, as discussed in the previous section.
To indicate that you want to include projects on the same project nesting level as the root project, use the method includeFlat in the settings file.
In the next section, you’ll configure the build logic for each of the projects in your build.
So far, you’ve split your application code based on functional responsibilities and rearranged it into individual subprojects.
Now, you’ll take a similar approach to organizing your build logic in a maintainable fashion.
The following points represent requirements common to real-world multiproject builds:
The root project and all subprojects should use the same group and version property value.
All subprojects are Java projects and require the Java plugin to function correctly, so you’ll only need to apply the plugin to subprojects, not the root project.
Figure 6.7 Comparison of hierarchical and flat project layouts and their settings file configurations.
In this section, you’ll learn how to define specific and common behaviors for projects in a multiproject build, a powerful way to avoid having to repeat configuration.
Some of the subprojects may depend on the compiled source code of other projects—in your application, the code from the project model is used by the repository project.
By declaring project dependencies, you can make sure imported classes are available on the classpath.
Before you fill your empty build.gradle file with life, we’ll review methods of the Project API I haven’t shown you yet but that are relevant in the context of multiproject builds.
In chapter 4, I explained the properties and methods of the Project API that you’ll probably use the most in your day-to-day business.
For implementing multiproject builds, you’ll need to get to know some new methods, as shown in figure 6.8
For declaring project-specific build code, the method project is used.
At the very least, the path of the project (for example, :model) has to be provided.
Many times, you’ll find yourself wanting to define common behavior for all your projects or only the subprojects of your build.
For each of these use cases, the Project API provides a specialized method: allprojects and subprojects.
Let’s say you want to apply the Java plugin to all of your subprojects because you need to compile Java source code.
You can do so by defining the code within the subprojects closure parameter.
The default evaluation order of projects in a multiproject build is based on their alphanumeric name.
Figure 6.8 Important methods of the Project API for implementing multiproject builds.
We won’t discuss these methods in this chapter; for specific use cases, refer to Gradle’s online manual.
In this chapter, you’ll use all of the presented methods to configure your multiproject build.
First, you’ll take the existing build code and apply it to only specific subprojects.
To set up the build infrastructure for your three subprojects—model, repository, and web—you’ll create a project configuration block for each of them.
The following listing shows the project definition within your single build.gradle file.
You can see that the solution is far from perfect.
Even though you defined an extra property for assigning the group and version for each subproject, you’re still left with duplicated code and the Java plugin has to be applied for each subproject individually.
Declaration of extra property projectIds as a map that holds the key-value pairs for group and version; property can be used in subprojects.
Configures each subproject by project path; actual configuration happens in the closure.
From the root directory of the multiproject build, you can execute tasks for individual subprojects.
All you’ll need to do is name the concatenated project path and task name.
Remember that paths are denoted by a colon character (:)
For example, executing the task build for the subproject model can be achieved by referencing the full path on the command line:
This works great for the self-contained subproject model, because it has no dependencies on code from other subprojects.
If you executed the same task for the subproject repository, you’d end up with a compilation error.
Why is that? The subproject repository uses code from the subproject model.
To function correctly, you’ll need to declare a compile-time dependency on the project.
Declaring a dependency on another project looks very similar to declaring a dependency on an external library.
In both cases, the dependency has to be declared within the closure of the dependencies configuration block.
Project dependencies have to be assigned to a particular configuration—in your case, the configuration compile provided by the Java plugin.
The following listing outlines the project dependency declarations for all of your subprojects.
Property inheritance Properties defined in a project are automatically inherited by its subprojects, a concept available in other build tools like Maven.
In listing 6.2, the extra property projectIds declared in the root project is available to the subprojects model, repository, and web.
The subproject repository depends on the subproject model, and the subproject web depends on the sibling project repository.
The actual dependency of a project dependency is the library it creates.
In the case of the subproject model, it’s the JAR file.
That’s why a project dependency is also called a lib dependency.
Depending on another project also adds its transitive dependencies to the classpath.
That means external dependencies and other project dependencies are added as well.
During the initialization phase of the build lifecycle, Gradle determines the execution order of projects.
Depending on another subproject means that it has to be built first.
It knows that the subproject repository depends on model and the subproject web depends on repository.
You don’t have to execute a task from a particular subproject—you can execute one for all projects of the build.
Let’s say you want to execute the task build from the root project.
Given the fact that Gradle knows the order in which the subprojects need to be executed, you’d expect the build to play out as shown in figure 6.9
Figure 6.9 Multiproject task execution order when running the task build from the root project.
You can prove the hypothesis by executing the task on the root level of your project:
Executing a task from the root project is a real timesaver.
Gradle executes the tasks that are required from all subprojects including the support for incremental builds.
As much as this behavior is convenient and ensures that you’ll always have the latest class files in your classpath, you may want finer-grained control over when to build all dependent subprojects.
Complex multiproject builds with tens or even hundreds of dependent subprojects will significantly influence the average execution time.
Gradle will go through all project dependencies and make sure that they’re up to date.
During development, oftentimes you know which source files have been changed in what subproject.
Technically, you don’t need to rebuild a subproject that you didn’t change.
For these situations, Gradle provides a feature called partial builds.
Partial builds are enabled through the command-line option –a or --no-rebuild.
By using partial builds, you can avoid the cost of checking the subproject model and bring down your build execution time.
If you’re working on an enterprise project with hundreds of subproject dependencies, you’ll be grateful for every second you can save when executing the build.
The following command-line output shows the usage of this option:
The --no-rebuild option works great if you’re only changing files in a single project.
As part of your day-to-day development practices, you’ll want to pull the latest version of the source code from the repository to integrate changes made by your teammates.
To ensure that code didn’t break by accident, you’ll want to rebuild and test the projects your current project depends on.
The regular build task only compiles the code of dependent projects, and assembles the JAR files and makes them available as project dependencies.
To run the tests as well, execute the task buildNeeded, as shown in the following command-line output:
Any change you make to your project may have side effects on other projects that depend on it.
With the help of the task buildDependents, you can verify the impact of your code change by building and testing dependent projects.
In the last section, you saw that executing a specific task from the root project invokes all tasks with the same name across all subprojects, with the execution order for the task build determined by the declared compile-time project dependencies.
If your project doesn’t rely on project dependencies, or defines a task with the same name for the root project and one or more subprojects, the story is different.
In each of the doLast actions, you print out a message on the console to indicate the project you’re in.
If you run the task hello from the root project, you’ll see the following output:
None of the tasks declares a dependency on another task.
So how does Gradle know in which order to execute the tasks? Simple: the task on the root level of the multiproject build is always executed first.
For the subprojects, execution order is solely determined by the alphanumeric order of the names of the projects: model comes before repository.
Keep in mind that the declaration order of the subprojects within the settings files doesn’t play any role in the execution order.
To do so, you need to reference the path to the task from a different project.
The next listing demonstrates how to ensure that the hello task from the subproject repository gets executed before the one from the subproject model.
Declares a task with same name for root project and all subprojects.
If you run the task hello from the root project, you’ll notice that the dependent task is executed in the correct order:
Controlling the execution order between tasks across different projects isn’t limited to tasks with identical names.
The same mechanics apply if you need to control the execution order for tasks with different names.
All you need to do is reference the full path when declaring the task dependency.
You have a basic multiproject build running and a general understanding of how to control the task execution order.
Next, we’ll discuss methods for defining common behavior to improve your code’s readability and reusability.
In listing 6.2, you needed to apply the Java plugin to each of the subprojects individually.
You also created an extra property named projectIds to define the group and version.
You used that extra property to assign its values to the Project properties of the root project and its subprojects.
This may not seem like a big problem in this fairly small project, but having to do this in larger projects with more than 10 subprojects can become very tedious.
In this section, you’ll improve the existing code by using the allprojects and subprojects methods.
Figure 6.10 provides a visual representation of how each method applies to a multiproject build.
Figure 6.10 Defining common project behavior with the Project API.
What does this mean for your project? You’ll want to use the allprojects method for setting the group and version properties of the root project and subprojects.
Because the root project doesn’t define any Java code, you don’t need to apply the Java plugin.
You can use the subprojects method to apply the plugins to just the subprojects.
The following listing demonstrates the usage of the methods allprojects and subprojects in your multiproject build.
Executing this build script will produce the same result as the previous build.
However, it’ll rapidly become clear that being able to define common project behavior has the potential to reduce duplicated code and improve the build’s readability.
The multiproject build you’ve defined so far only consists of a single build.gradle file and the settings.gradle file.
As you add new subprojects and tasks to your build.gradle file, code maintainability will suffer.
Having to wade through pages and pages of code to extend or modify your build logic is no fun.
You can drive the separation of concerns even further by creating individual build.gradle files for each of the projects.
Sets group and version properties for the root project and all subprojects.
For each of the subprojects, you’ll need to create a build file with the default naming convention.
With the project files in place, you can now split up the build logic from the master build file and move it to the appropriate location.
All you need to keep is the allprojects and subprojects configuration blocks, as shown in the following listing.
The remainder of the code will be moved to the build files of your subprojects.
Next, we’ll focus on defining the build logic of your subprojects.
Remember that the model subproject didn’t define any project-specific build logic.
In fact, you didn’t even have to declare a project configuration block.
As a consequence, you won’t have to declare any code in the subproject’s build.gradle file.
Gradle knows that the subproject is part of the multiproject build because you included it in the settings file.
The build files for the subprojects repository and web won’t introduce any new code.
You can simply take the existing project configuration blocks and copy them into the correct location.
Having a dedicated Gradle file per project indicates that you’re dealing with a specific project.
Therefore, enclosing your code into a project closure becomes optional.
The following listing shows the contents of the build file for the repository subproject.
The build.gradle file for the subproject web should look equally familiar, as shown in the next listing.
Running this multiproject build produces the same result as having the same code in one master build file.
On the upside, you significantly improved the readability and maintainability of the build code.
In the next section, I’ll discuss some examples of customizing your projects even more.
In a multiproject build with many subprojects, you may want to be more expressive when it comes to naming your build files.
Editing multiple build.gradle files in parallel and constantly switching between them easily becomes confusing when you’re using an IDE.
This section will explain how to configure your project to use custom build filenames.
Let’s assume you want to build the following project structure: each of the subproject directory names is constructed of the prefix todo- and a speaking name for the project.
For example, the directory for the subproject repository would be named todo-repository.
The build filename, however, should only be represented by the actual project responsibility.
The following directory tree shows the final result you want to accomplish:
The key to making this project structure work again lies in the settings file.
It provides more functionality than just telling your build which of the subprojects should be included.
In fact, it’s a build script itself that’s executed during the evaluation phase of the build lifecycle.
With the help of the Settings API outlined in section 6.2.2, you have direct access to the root project and its children.
The following listing shows how to iterate over all subprojects to assign a custom build filename.
In addition, you also set a custom name for the root project.
Though this example may not apply to your real-world projects, the possibilities of configuring a multiproject build to your liking are endless.
In most cases it can be achieved without much effort.
Keep in mind that the Settings API is your best friend.
Modularizing a project improves the quality attributes of your system—that is, reusability, maintainability, and separation of concerns.
Two guidelines make it easy to achieve that for your software: minimize coupling and maximize cohesion.
In this chapter, you split the To Do application code base into modules.
You created one module that holds the model classes, one that deals with the persistence of the data, and one that exposes the web application capabilities.
You learned that the settings file, executed during the initialization phase of the build lifecycle, determines which of the projects should be part of the build.
The Project API provides methods for declaring project-specific build code.
You learned that dependencies between projects in the current project hierarchy are declared using the same dependency mechanism as external dependencies.
The organization of your multiproject build code is very flexible.
You can choose to use a single master build script, individual build scripts per project, or a mixed approach.
The route you take depends on the requirements of your project.
However, organizing build logic into individual scripts improves maintainability of your code the more subprojects you add to your build.
The Settings API available in your settings file can be used to adapt to unconventional multiproject layouts.
The example demonstrated how easy it is to use custom build script names that deviate from the standard naming convention.
The next chapter is fully devoted to Gradle’s test support.
We’ll explore the use of different test frameworks for writing unit, integration, and functional tests.
We’ll also discuss how to write test code for your own build scripts.
In the previous chapters, you implemented a simple but fully functional web application and learned how to build and run it using Gradle.
Testing your code is an important activity of the software development lifecycle.
It ensures the quality of your software by checking that it works as expected.
In this chapter, we’ll focus on Gradle’s support for organizing, configuring, and executing test code.
In particular, you’ll write unit, integration, and functional tests for your To Do application and integrate them into your build.
Gradle integrates with a wide range of Java and Groovy unit testing frameworks.
By the end of this chapter, you’ll write tests with JUnit, TestNG, and Spock, and execute them as part of the build lifecycle.
You’ll also tweak the default test execution Testing with Gradle.
You’ll learn how easy it is to control the test logging output and to add a hook or listener to react to test lifecycle events.
We’ll also explore how to improve the performance of big test suites through forked test processes.
Integration and functional tests require a more complex tooling setup.
You’ll learn how to use the thirdparty tools H2 and Geb to bootstrap your test code.
Before you start exercising tests with your build, let’s do a quick refresher on the different types of testing as well as their individual advantages and disadvantages.
We’re not going to cover the details of why an automated testing approach is beneficial to the quality of your project.
Long story short: if you want to build reliable, high-quality software, automated testing is a crucial part of your development toolbox.
Additionally, it’ll help reduce the cost of manual testing, improve your development team’s ability to refactor existing code, and help you to identify defects early in the development lifecycle.
They usually differ in scope, implementation effort, and execution time.
We categorize three types of automated tests—unit tests, integration tests, and functional tests:
Unit testing is performed as a task alongside the implementation of your production code and aims for testing the smallest unit of your code.
In a unit test you want to avoid interacting with other classes or external systems (for example, the database or file system)
References to other components from within the code under test are usually isolated by test doubles, which is a generic term for a replacement of a component for testing purposes, like a Stub or Mock.
Unit tests are easy to write, should execute quickly, and provide invaluable feedback about your code’s correctness during development.
Integration testing is used to test an entire component or subsystem.
You want to make sure that the interaction between multiple classes works as expected.
A typical scenario for an integration test is to verify the interaction between production code and the database.
As a result, dependent subsystems, resources, and services have to be accessible during test execution.
Integration tests usually take longer to execute than unit tests and are harder to maintain, and the cause of a failure may be harder to diagnose.
Functional testing is used to test the end-to-end functionality of an application, including the interaction with all external systems from a user’s perspective.
When we talk about the user’s perspective, we usually mean the user interface.
Functional tests are the hardest to implement and the slowest to run, because they require emulating user interaction.
In the case of a web application, the tooling for functional.
Testing Java applications tests will need to be able to click a link, enter data into form fields, or submit a form within a browser window.
Because user interfaces can change a lot over time, maintaining functional test code can become tedious and time-consuming.
You may wonder which type of testing is the most appropriate for your project and to what extent.
In a perfect world, you’d have a good mixture of all of these tests to ensure that your code is working correctly on different layers of architecture.
However, the number of tests you write should be driven by the time and effort it takes to implement and maintain them.
The easier a test is to write and the quicker it is to execute, the higher the return on investment (ROI)
To optimize your ROI, your code base should contain many unit tests, fewer integration tests, and still fewer functional tests.
This distribution of tests and their correlation to ROI is best illustrated by the test automation pyramid, introduced by Mike Cohn in his book Succeeding with Agile: Software Development Using Scrum (Addison Wesley, 2009)
Figure 7.1 shows an adapted version of Cohn’s test automation pyramid.
In the rest of this chapter, we’ll explore how to automate unit, integration, and functional tests with Gradle.
Many of Gradle’s out-of-the-box testing capabilities are provided by the Java plugin.
Let’s start by having a closer look at these features.
Popular open source testing frameworks like JUnit and TestNG help you write repeatable and structured tests.
To execute these tests, you’ll need to compile them first, as you do with your production source code.
The purpose of test code is solely to exercise its test cases.
Because you don’t want to ship the compile test classes to production systems, mingling production source and test code isn’t a good idea.
Optimally, you’ll have a dedicated directory in your project that holds test source code and another that acts as a destination directory for compiled test classes.
Gradle’s Java plugin does all of this heavy lifting for you.
It introduces a standard directory structure for test source code and required resource files, integrates test code compilation and its execution into the build’s lifecycle, and plays well with almost all of the popular testing frameworks.
This is a significant improvement over implementing the same functionality in an imperative build tool like Ant.
If that wasn’t enough, you’d have to copy the same code for every project that wants to use it.
In chapter 3 we talked about the default directory structure for placing production source code: src/main/java and src/main/resources.
You put test source files into the directory src/test/java, and required resources files consumed by your test code into src/test/resources.
After compiling test source code, the class files end up in the output directory build/ classes/test, nicely separated from the compiled production class files.
All testing frameworks produce at least one artifact to indicate the results of the test execution.
Many testing frameworks allow for transforming the results into a report.
Figure 7.2 gives a visual overview of the standard test directories provided by the Java plugin.
With all this talk about testing frameworks, how do you tell Gradle to use a particular one? You’ll need to declare a dependency on an external library.
The Java plugin introduces two new configurations that can be used to declare dependencies on libraries required for test code compilation or execution: testCompile and testRuntime.
Let’s look at an example that declares a compile-time dependency on JUnit:
The other test configuration, testRuntime, is used for dependencies that aren’t needed during the compilation phase of your tests, but are needed at runtime during test execution.
Keep in mind that dependencies assigned to test configurations don’t influence the classpath of your production code.
In other words, they’re not used for the compilation or packaging process.
However, the test configurations extend specific configurations for handling dependencies needed for your production source code, as shown in figure 7.3
The configuration testCompile is automatically assigned the dependencies of the configuration compile.
The configuration testRuntime extends the runtime and testCompile and their configuration parents.
When executing earlier examples, you may have noticed that the task graph contained four tasks that were always up to date and therefore skipped.
This is because you hadn’t written any test code that Gradle would need to compile or execute.
Figure 7.4 shows the test tasks provided by the Java plugin and how they fit into the existing order of tasks.
Figure 7.4 Test tasks seamlessly integrate into the build lifecycle.
As shown in the figure, test compilation and execution happen after the production code is compiled and packaged.
If you want to avoid executing the test phase, you can run gradle jar on the command line or make your task definition depend on the task jar.
The annotation can either be the JUnit or TestNG implementation.
If none of these rules apply or the scanned class is abstract, it won’t be executed.
It’s time to apply what you’ve learned in the context of a full example.
In the following section, you’ll write unit tests with the help of different testing frameworks and execute them with Gradle.
As a Java developer, you can pick from a wide range of testing frameworks.
In this section, you’ll use the traditional tools JUnit and TestNG, but also look at the new kid on the block, Spock.
If you’re new to any of these testing frameworks, refer to their online documentation, because we won’t cover the basics of how to write a test.
To highlight commonalities and differences between the testing frameworks and their integration with Gradle, all unit tests will verify the functionality of the same class.
However, you’ll adapt the test and build code to fit the needs of the particular testing framework.
The correct location to put this test is the standard test source directory.
In the spirit of test-driven development, you formulate the assertions in such a way that they’ll fail first.
This gives you confidence later that your assumptions are correct.
The following listing shows the JUnit test case implementation that verifies the correctness of the insert functionality.
With the test class in place, let’s look at adding test support to your build.
The following listing shows how to assign the JUnit dependency with version 4.11 to the configuration.
Methods marked with this annotation are always executed before every test method of class.
Methods marked with this annotation will be run as test case.
Wrong assertion pu there on purpose to provoke a failed tes.
You enabled your build to use JUnit as a test framework in your project.
Next, you’ll prove your hypothesis about the failing assertion by executing the task test.
The following command-line output indicates a failed build due to a test assertion error:
Listing 7.2 Declaring a dependency on JUnit in subproject repository.
Test file an line of code where the exception occurred.
Summary of test result including number of completed, failed, and skipped test cases.
In the console output, you can see that one of the assertions failed.
If you had a huge suite of tests, finding out the cause of any failed test would require you to open the test report.
You can make the test output a bit chattier by running the task on the INFO logging level:
Changing the logging level through a command-line option isn’t the only way to control the test log output.
Later in this chapter, we’ll cover options in your build script for configuration test logging.
You created the assumption that the value of newId should be null.
The reality is that every record in a data store should be uniquely identifiable, so the field needs to have a value.
You’ll fix the assertion in your test method by expecting a non-null ID value:
Running the task test again shows that all tests are passing:
As you learned earlier, you can find the HTML report under build/reports/test.
Opening the index HTML page should render something like the screenshot shown in figure 7.5
The report gives you a summary of the number of run tests, the failure rate, and the execution duration.
You can switch the view between test packages and classes by clicking the tabs.
In the case of at least one failed test, another tab is shown that gives you the full stack trace of the unfulfilled assertion.
JUnit is the standard unit testing framework in Gradle; however, Gradle doesn’t stand in the way of giving you the option of picking a different solution.
Let’s discuss how to integrate other unit testing frameworks or even use multiple frameworks together in a single project.
In your project, you may prefer to use a different unit testing framework than JUnit.
The reasons for your choice might vary, but are usually based on the feature set, like out-of-the-box mocking support or the language you use to write the test.
Clickable report URLs Navigating to the reports directory and double-clicking the HTML index file can become tedious over time.
Sure, you could always bookmark the URL, but Gradle gives you a great shortcut for this manual task.
On some operating systems, the outputted file URL in the console is clickable, which opens the HTML report in your primary browser:
This feature is not only available to failed test execution.
Any task that produces a report file offers a clickable URL in the console.
Unit testing section, we’ll cover how to use two alternatives in your build: TestNG and Spock.
We won’t go into detail about how to write the test classes with different unit testing frameworks.
You’ll be able to find examples in the source code of the book, as well as online.
Instead, let’s focus on the nuts and bolts of integrating these frameworks into your build.
To enable your build to execute TestNG tests, you’ll need to do two things:
The following listing demonstrates TestNG integration in the context of the full build script.
After running gradle :repository:test on the example, you’ll notice that the task execution order is the same as in the JUnit example.
Earlier versions of Gradle produced a different look and feel of the test report than the JUnit report.
Starting with version 1.4, the test report looks exactly the same.
A test case written in a BDD style has a clear title and is formulated in a given/when/then narrative.
The result is a very readable and expressive test case.
This class is marked with the annotation @RunWith that allows running the tests with a specialized JUnit runner implementation.
To be able to compile Groovy classes in the default source directory src/test/groovy, your project will need to apply the Groovy plugin.
The Groovy plugin requires you to declare the version of Groovy you’d like to use in your project as a dependency.
Because you need to use Groovy for test source code compilation, you’ll assign the library to the testCompile configuration.
In addition to the Groovy library, you’ll also declare the version of the Spock library.
The next listing illustrates the setup required for compiling and executing Spock tests.
The produced HTML test report aligns with the look and feel of reports generated for JUnit and TestNG tests.
Gradle presents you with a homogeneous reporting approach, no matter which testing framework you pick.
You don’t have to make one determining decision about which unit testing framework you want to use.
All of these frameworks can be incorporated into one project.
It’s not unusual for a team to switch from one testing framework to another.
Clearly, you don’t want to rewrite all of your existing test classes with the new and shiny testing framework you’re planning to use.
You want to keep them and run them as part of your build.
On top of that, you want to generate a single test report that aggregates all test results.
Let’s assume you want to support the ability to write unit tests in all the frameworks we discussed before.
One additional requirement you’ll introduce to the project is a naming convention for test classes:
Listing 7.4 Using Spock to write and execute unit tests.
You’ve seen that TestNG support needs to be configured by calling the useTestNG() method.
However, the default test task executes either JUnit or TestNG tests.
To enable support for both, you’ll have to add a new task of class type Test.
This new task can easily be integrated into the test lifecycle by making the test task depend on it, as shown in figure 7.6
The result is a build that executes all three test class types.
JUnit and Spock tests are executed by the test task, and TestNG tests are executed by the testNG task.
The following listing demonstrates the minor change to your existing build that provides support for multiple frameworks in one build.
This happens because the second run of the report generation overwrites the first one.
Therefore, it only contains the JUnit and Spock test results.
This can easily be fixed by merging the test results of both test tasks.
To integrate this task into the build lifecycle, you added it as a dependent task to the verification task check, as shown in figure 7.7
Executing the task build will automatically aggregate the test reports.
After running gradle build, you’ll find the aggregated HTML test report under the directory build/reports/test.
It should look similar to the screenshot in figure 7.8
In practice, you’ll find yourself tweaking the test execution behavior to fit your needs.
The next section explores available configuration options and how to apply them to your build.
Test execution is an essential and important phase in the lifecycle of your build.
Gradle gives you a wide variety of configuration options in your build script, as well as command-line parameters to control the runtime behavior.
Configuring test execution these options depends on what you need in your build.
This section will give you a short and sweet overview of frequently used functionality and the API classes behind these options.
Projects with huge test suites call for fine-grained control of the tests you’d like to execute.
Every so often, you’ll want to run just a single test or tests of a particular package or project.
This situation quickly arises if one or more tests fail, and you’d like to fix and rerun them without taking the hit of executing the full test suite.
Let’s say you’d like to execute Spock tests in all packages.
This is just one simple example of defining the test name pattern.
For the full breadth of pattern options, refer to the online documentation of the Java plugin.
Being able to remotely debug your tests with an IDE is an invaluable tool to have in your toolbox.
Using this startup parameter will start a server socket on port 5005 and block task execution until you actually connect to it with your IDE:
In the meantime, you can bring up the IDE of your choice, set break points in your code, and connect to the port.
Once you’re connected, task execution will resume and you’ll be able to step through your code.
The steps for connecting the remote debugger vary from IDE to IDE.
While these command-line options come in handy during day-to-day business, you may want to configure test execution in a more permanent way: in your build script.
The class Test extends DefaultTask and can be used to create particular test tasks in your build script.
You can change the default behavior through its exposed API.
Figure 7.9 shows the primary test API and its associated classes.
The class diagram mainly shows the methods we’ll touch on in this chapter.
For a deeper dive into the API, consult the DSL guide or Javadocs.
With this class diagram in mind, you’ll start by using some of these configuration options.
The following scenarios will give you an idea of how to apply them.
By doing so, you get all the benefits you usually have when starting up a Java process.
You can pass in options to tweak garbage collection and performance tuning, or provide system properties for use in your code.
Let’s say you made some minor changes to your test method.
Instead of inserting a single to-do item, you make the number of insertable to-do items configurable through a system property named items.
Now, how do you tell Gradle to consume a system property that drives the creation of to-do items in your test? You can simply call the method systemProperty on Test and provide a name and value as parameters.
You can imagine that the higher the number of items, the easier you’ll fill up your memory.
While you’re at it, you’ll also fine-tune the JVM memory settings by calling the method jvmArgs to avoid potential OutOfMemoryErrors.
The following listing demonstrates method calls on the test task.
Depending on the number you provide for the system property items, the time it takes to complete the test might vary.
You don’t get direct feedback on whether the provided value is actually evaluated correctly.
If you examine listing 7.7 closely, you may notice that you print out the provided number of items to the standard output.
However, when you run the tests, you won’t see that output.
Let’s see how to change this by taking control of test logging.
Being able to control logging can be tremendously helpful when trying to diagnose problems during test execution.
I encourage you to explore the class even further, because we won’t cover all options.
One of Gradle’s Test configuration options is to flip a Boolean flag that prints standard output and error messages to the terminal, as shown in the following listing.
The drawback to this approach is that your terminal will also fill up with other messages that are irrelevant for diagnosing the cause of a failed test.
You can permanently change the format for logging test exceptions via the method exceptionFormat.
The next listing provides the value full, which tells Gradle to print the full stack exception traces independent of the fact that you run your build on the INFO logging level.
Only if at least one of your tests fails will a summary be printed.
The method events allows you to pass in a list of event types you’d like to be logged.
The following listing demonstrates how to log a message to the terminal every time a test is started, passed, skipped, or failed.
Executing tests with logging turned on for the events started, passed, skipped, and failed will produce the following result:
Each event is logged on a single line and is color-coded.
Events that didn’t occur—in this case, skipped and failed—aren’t logged.
Refer to the online documentation to learn about all available options.
Executing huge test suites with thousands of test cases may take minutes if not hours, because they run sequentially.
Given that today’s computers have blazingly fast multicore processors, you should use their computing powers to their fullest.
Gradle provides a convenient way to execute your tests in parallel.
All you need to specify is the number of forked JVM processes.
In addition, you can set the number of maximum test classes to execute per forked test process.
The next listing uses a simple formula to calculate the number of forks by available processors on your machine.
Maximum number of test classes to execute in a forked test process.
Let’s visualize the execution behavior based on a test suite with 18 test classes.
The listing shows that the number of parallel test processes is calculated based on the number of logical cores available to your JVM, either virtual or physical.
With the property forkEvery set to 5, each forked test process will execute a group of five test classes.
Figure 7.10 demonstrates how the test execution will play out at runtime.
The assigned numbers in this example are not set in stone.
How you configure parallel test execution in your project depends on the target hardware and the type of tests (CPU or I/O bound)
Try experimenting with these numbers to find the sweet spot.
For more information on how to find the optimal balance on your machine, I recommend reading Programming Concurrency on the JVM by Venkat Subramaniam (The Pragmatic Programmers, 2011)
In chapter 4, you learned that you can easily hook into the build lifecycle to execute code whenever an event occurs.
Gradle exposes lifecycle methods for any task of type Test.
Figure 7.11 shows how these events fit into the build lifecycle when they’re registered for the default test task provided by the Java plugin.
Let’s assume you want to find out how long it takes for the tests in your suite to finish.
To figure this out, you’ll hook into the test lifecycle via the afterSuite method.
The following listing demonstrates how to use the parameters passed into the closure to calculate the elapsed execution time and send this information as a notification to the desktop.
Figure 7.10 Test execution with two forked processes at a time.
This is a simple and straightforward approach to displaying the test suite execution time.
However, you won’t have any track record of previously run test suites.
You could easily send this data to a database and visualize it in a graph over time.
Registering test event methods is great for ad hoc functionality.
The drawback is that you can’t easily share it between projects.
TestListener is an interface for listening to test execution events.
You’ll implement the same functionality as discussed in the last section.
The only method you’ll need to fill with life is afterSuite.
The following listing shows the full listener implementation and how to register the class with the test task.
Listing 7.14 Adding a test listener to the default test task.
Adds closure to be notified after test suite has executed.
In chapter 4, you learned that you can easily share classes between projects if you put them into the buildSrc directory.
In this section, we took a quick flight over the most relevant configuration options.
These options aren’t specific to tasks that handle unit tests.
They can also be applied to integration and functional tests.
Next, we’ll discuss how to write integration tests for your To Do application and integrate them into your build.
A unit test verifies that the smallest unit of code in your system, a method, works correctly in isolation.
This allows you to achieve a fast-running, repeatable, and consistent test case.
They usually integrate other components of your system or external infrastructure like the file system, a mail server, or a database.
As a result, integration tests usually take longer to execute.
Oftentimes they also depend on the correct state of a system—for example, an existing file with specific content—making them harder to maintain.
A common scenario for integration tests is verifying that your persistence layer works as expected.
TestListener implementation that notifies user about test suite execution time.
H2 is easy to set up and provides fast startup times, which makes it a perfect fit for this example.
In chapter 3, you provided an interface for your persistence layer.
That makes it easy to provide different implementations of ToDoRepository and interchange them in the web layer.
Any interaction with the database is implemented in the new class H2ToDoRepository that implements the interface.
I don’t want to bore you with repetitive details of a class that uses JDBC, so I won’t discuss the code in detail.
The downloadable code example contains all the relevant code if you want to dig deeper.
It’s not unusual for projects to put all types of tests in the same source directory.
Given that integration tests usually take longer to execute than unit tests, you’ll want to be able to separate them from each other by a naming convention.
Developers will now be able to rerun unit tests on their local machine and get fast feedback about their code changes.
As the naming convention for integration tests in your project, let the test class names end with the suffix IntegTest.
The only big difference is that the class under test is H2ToDoRepository, as shown in the following listing.
The integration test code will live alongside the existing unit test class in the same package.
This test class verifies the same assertions as in the unit tests.
In practice, you’d have many more test cases to test the interaction with the database.
The test you wrote focuses on testing the integration point between the code and the database.
That means you’ll need to have an accessible H2 database up and running that hosts the correct schema.
It’s considered good practice to provide a database instance per environment (for example, development, QA, and production)
In the following section, we’ll assume that you don’t have to deal with managing and configuring the database.
In your build, you want to support three basic requirements:
In the previous sections, you already learned the skills to achieve this goal.
The following listing demonstrates how to include or exclude test class names with a specific naming pattern.
You also override the default directory for test results and report.
Adds JDBC driver as runtime dependency to be able to connect to H2 database.
Running gradle :repository:build on the command line will invoke the test tasks that run unit and integration tests.
Mixing different types of tests in the same source folder might sound like a good idea at first.
With an increasing number of test classes, the test sources in your project will become hard to navigate and differentiate.
You’ll also have to teach every developer on your team to stick to the test class naming convention.
If this pattern isn’t followed meticulously, a test class might be executed in an unintended phase of the build.
As you learned,  integration tests usually take longer to execute than unit tests, so this would be a significant drawback.
You can actually enforce your own conventions by separating unit and integration tests into different source sets.
Let’s say you want to leave all your unit tests in the directory src/test/java but move the integration tests into the directory src/integTest/java.
After creating the new test source directory for integration tests, the project structure should look like this:
Gradle provides a clean solution for separating different types of tests into source directories.
In chapter 3, you learned how to reconfigure the default source directories.
That’s what you’re going to do for your integration tests.
Remember the days when you had to implement a similar requirement with Ant? You’d have to write code similar to what you would write for compiling and executing your unit tests.
How did you solve this? Usually by copying and pasting the code and modifying the targeting directories, a generally bad practice.
In your project, you define what you want do: add a new test source code directory.
The how, compiling the source code, you’ll leave to Gradle.
In fact, Gradle automatically makes this decision for you and implicitly adds a new compilation task just for that new source set.
The following listing shows how to define the new integration test source set in your project.
You can see in the source code example that the source set needs some additional configuration.
It’ll require you to assign the compilation classpath, which consists of the production code classes and all dependencies assigned to the configuration testRuntime.
You’ll also need to define the runtime classpath consisting of the compiled integration test classes directly accessible through the variable output and the compilation classpath.
Because the class output directory of your integration test source set deviates from the default directory, you need to point the integrationTest task to it.
You also need to take care of reconfiguring the task’s classpath.
The following code snippet shows the integrationTest task and the assigned property values:
Points test task to directory in which to find the test classes.
The following output shows the result of running the build task on the command line:
With this build code in place, you have a nice separation of concerns between unit and integration tests.
Next, we’ll touch on the topic of automatically setting up the database on your local machine as part of the build.
They need to be accessible from the machine you’re executing the build on.
If that isn’t the case, your integration tests will fail.
To ensure a stable testing environment, you can bootstrap the required resources from your build.
H2 provides lightweight, Java-based tools to manage and control your database, which you can easily integrate into your build.
The tasks you need to create could look similar to figure 7.12
Bootstrapping your test environment in your build is very diverse, product-specific, and tailored to the requirements of your project.
You may need to stand up a mail server or bring up another application to expose its web services.
The important takeaway is that you can make this work if you need to.
It goes beyond the scope of this book to discuss the details of how to make this happen for your H2 database.
However, the source code of the book provides a working sample that you can try out and explore.
If you run the example, you’ll find that the output of the command line looks similar to this:
By adding a new source set, Gradle automatically adds required tasks to compile and process integration test source code; task names are derived from source set name.
Figure 7.12 Starting, preparing, and stopping the database for integration testing.
With a basic understanding on how to write integration tests, we’ll turn our attention to the top part of the test automation pyramid: functional testing.
Functional testing is ideal for verifying that the software meets the requirements from the end user’s perspective.
In the context of your web application, this means simulating the user’s interactions with the browser, such as entering values into text fields or clicking links.
Historically, functional tests have been hard to write and costly to maintain.
You need a tool that automates bringing up the browser, manipulates the data object model (DOM) of the web page, and supports running these tests against different browsers.
On top of that, you also need to integrate the functional tests into your build to be able to run them in an automated and repeatable fashion.
Let’s look at a specific use case and an automation tool that can help you test-drive the tests.
When designing a functional test on the UI-level, it’s helpful to ask yourself the following questions:
What’s the high-level user workflow? For example, the user has to insert 11 to-do items before the list offers pagination.
What are the technical steps to reach this goal? For example, the user opens a browser and enters the URL /all to view the list of to-do items.
To insert a new to-do item, they enter a name for a new to-do Item and press Enter.
This UI interaction calls the URL /insert, which adds the to-do item to the list.
Repeat this 11 times, and verify that pagination is displayed.
For our purposes, we’ll pick a simple use case: open the URL to show the list of to-do items.
Insert a new to-do item named “Write functional tests” into the text field and press Enter.
Verify that it was successfully added to the list by inspecting its items.
Functional testing test assumes that the list will start with zero to-do items.
Figure 7.13 demonstrates the page workflow you’ll need to script.
With these UI interactions in mind, let’s look at a tool that can help implement these requirements.
An open source tool that can stand up to the challenge of these requirements is Geb (http://www.gebish.org/)
Geb is built on top of the popular browser automation framework Selenium and allows you to define your tests with a very readable Groovy DSL.
Test classes can be written with frameworks such as JUnit, TestNG, or Spock.
This means that if you know any of these testing frameworks and poke through the Geb’s DSL documentation, you’re perfectly set up to write your first functional tests.
For now, you can assume that test classes for the business workflow described earlier are built with Geb using the test framework JUnit.
This book will not teach how to write tests with Geb, as that could easily fill another one or two chapters.
I’ll leave it to you to explore the provided code examples.
All tests are configured to work exclusively against Mozilla Firefox.
If you don’t have Firefox installed on your machine, now is a good time to do so.
It’s worth mentioning that Geb allows for executing tests against other browsers as well.
For more information, check the Geb online documentation and source code examples.
Next, you’ll prepare the build for organizing and executing the tests.
Your functional tests require an up-and-running instance of the web application.
Many organizations provide a specific runtime environment solely for this purpose.
Let’s say you want to modify your build to support functional tests, given that you have access to such an environment.
At first sight, the requirements for the build look similar to the ones you defined for integration tests:
For compiling the test code, you’ll need to assign the Geb JUnit implementation and the Selenium API if needed in the tests.
For running the tests, you’ll need to provide the Selenium driver for Firefox to remote-control the browser.
You could easily assign these dependencies to the existing testCompile and testRuntime configurations.
The drawback is that you’ll convolute the classpath for your unit and integration tests, which might cause version conflicts.
To keep the classpath for functional tests as clean as possible and separated from other test types, we’ll look at two new configurations: functTestCompile and functTestRuntime.
Figure 7.14 shows how they fit into the existing configuration hierarchy introduced by the Java plugin.
The Geb tests work against the UI of your application.
Therefore, it makes the most sense to add the test definitions to the web project.
The following listing shows the basic setup for defining the configurations needed for functional test dependencies.
Geb tests are written in Groovy and need the assigned external library for compiling and running the tests.
The following directory tree shows where the tests live in the example project:
This can be achieved similarly to the way you did this for the integration test source set.
The big difference is that you have to assign the custom configurations to the relevant classpath properties, as shown in the next listing.
After defining the new source set, you can use its class output directory and runtime classpath in a new enhanced task of class type Test.
The following listing shows the functionalTest task, which writes its results and report to a custom directory.
Geb also requires you to set some mandatory system properties.
One of these properties is the name of the browser you want to run tests against.
If you want to run your tests against multiple browsers, you’ll need to create individual tests tasks and pass in the appropriate value.
With these additions to your build script, you can run the functional test against a network-reachable instance of your web application.
Next, you’ll go the extra mile and provide a way to run the tests exclusively on your local machine.
A benefit of this is that you don’t have to rely on a server to run your tests.
You already know how to use the Jetty plugin to deploy your application.
By default, the task jettyRun will block further build execution until the user stops the process with the keystroke Ctrl + C.
Thankfully, the Jetty plugin can be configured to execute the embedded container in a background thread with the property daemon.
For this purpose, you’ll create enhanced tasks for starting and stopping the Servlet container, as shown in the next listing.
Listing 7.20 Using the source set in the functionalTest task.
Listing 7.21 Declaring enhanced Jetty tasks for use with functional tests.
Now that you have two dedicated tasks for controlling the web application runtime environment, you can sandwich the functionalTest task in between.
Figure 7.15 illustrates the order of Gradle tasks you need to model to fully integrate functional tests into the build.
Task dependency chaining is your best friend to help you achieve this goal.
The last task in the chain should be the verification task check, as shown in listing 7.22
The check task is a lifecycle task provided by the Java plugin that depends on any of the verification tasks like test.
This task is convenient if you want to automatically execute the whole chain of test tasks.
Executing the command gradle build for the web project commences the following actions: the functional test classes are compiled first, an embedded Jetty container is brought up in the background, and Firefox is automatically started and remotecontrolled based on your functional test definitions.
The console output of the command should look as follows:
Determines that container should be run in background (task will not be blocked)
Automated testing is an essential instrument for ensuring the correctness of your application’s functionality, and is a direct enabler for effective refactorings.
Unit, integration, and functional tests differ in scope, implementation effort, and execution time.
You saw how the test automation pyramid, introduced by Mike Cohn, shows these criteria in relation to the ROI for your project.
The easier tests are to implement and the faster they can be executed, the higher the ROI ratio.
And the higher the ROI of a test type, the more test cases of this type you should have.
By applying the plugin, your project automatically knows where to search for test classes, compiles and executes them as part of the build lifecycle, exposes configurations for assigning required test dependencies, and produces a visually attractive HTML report.
In this chapter, you learned how to implement unit tests with the help of three popular testing frameworks: JUnit, TestNG, and Spock.
Gradle’s Test API plays a significant role in configuring the test execution to your needs.
The two examples we discussed in great detail can be directly applied to a real-world project.
Being able to have fine-grained control over your test logging is a huge benefit when trying to identify the root cause of a failed test.
Test classes that are part of large test suites can be run in parallel to minimize their execution time and utilize your hardware’s processing power to its full capacity.
Integration and functional tests are harder to write and maintain than unit tests.
Integration tests usually involve calling other components, subsystems, or external services.
We discussed how to test an application’s data persistence layer in combination with a running SQL database.
Functional tests verify the correctness of your application from the user’s perspective.
With the help of a test automation framework, you remote-controlled the browser and emulated user interaction.
You configured your build to provide a source set for different types of tests, provided new test tasks, fully integrated them into the build lifecycle, and even bootstrapped the test environment where needed.
The next chapter will talk about how to extend your build script with a plugin.
Not only will you implement a fully functional, real-world plugin and use it in your build, you’ll also expand on the topic of testing by verifying its functionality.
You added custom logic by declaring simple tasks and custom task classes within your build script.
Often, a task becomes so useful that you’ll want to share it among multiple projects.
Gradle provides various approaches for reusing code, each with its own unique advantages and drawbacks.
Plugins take the concept of reusability and extensibility even further.
They enhance your project with new capabilities by introducing conventions and patterns for a specific problem domain.
Earlier in the book, you saw how powerful plugins are.
In chapter 3, you used the Java, War, and Jetty plugins to implement a task management web application.
Applying these plugins to your project was as simple as adding a single line of code and enhanced your build with new capabilities.
The War plugin allows for building a WAR file, whereas the Jetty plugin deploys in an embedded Servlet container.
All of these plugins are small, opinionated frameworks that introduce default conventions and project layouts.
However, your view of the world may be different when it comes to building Java or web applications.
All core plugins allow for changing the default conventions, which makes it easy to adapt to nonstandard projects.
In this chapter, you’ll learn how to structure, implement, test, and build your own custom plugin.
Our discussion will touch on topics such as development practices for reusable code, writing flexible tasks, and introducing the concept of convention over configuration.
Let’s start with a look at the practical application for your plugin.
The running example in this book is a web-based To Do application in Java.
Through Gradle’s out-of-the-box plugin support, you were able to create a WAR file, deploy it to an embedded Servlet container, and test the application’s functionality in the browser.
I bet you’re eager to show off your hard work to end users by deploying it to an internetaccessible web container.
The traditional approach to hosting a web application is to manage your own web servers.
Though you have full control over the infrastructure, buying and maintaining servers is expensive.
Remember the last time you had to ask your infrastructure team to provide you with a server and the compatible runtime environment for your application? Provisioning the hardware and software delayed your time to market, and in the end, you didn’t even have root access to tweak your application’s runtime parameters.
A quick and easy way to host an application is to use a platform as a service (PaaS), a combination of a deployment platform and a solution stack that in many cases is free of charge.
A PaaS combines traditional application server functionality with support for scalability, load balancing, and high availability.
Manually deploying an application to a server is a repetitive and error-prone task.
Thankfully, many PaaS providers expose an API for managing their platform services and resources programmatically.
With the help of Gradle, you can automate the deployment to remote containers and make it part of the project’s build lifecycle.
Every web application needs to be launched to a runtime environment at some point in the development process, so it makes sense to write the code in a reusable fashion.
Unfortunately, you can’t fall back to an existing Gradle core plugin, so you’ll need to roll your own implementation.
This is a great way to practice your build tool’s extension mechanism.
Before you get started writing code, let’s pick a PaaS provider that fulfills your requirements.
In the last couple of years many JVM PaaS providers have sprung up.
Some vendors propose a programming model that requires you to conform to their specific software.
Introducing the plugin case study stack and APIs, such as a proprietary data storage implementation.
You don’t want to lock yourself in, because you’d like to be able to transfer the web application to a different server environment later on.
CloudBees provides a Java-based client library to communicate with runtime services over HTTP.
Using the library within a Gradle script is straightforward: define it as a classpath dependency and write tasks to use the API.
The CloudBees API provides an HTTP-based programming interface for managing services and applications on the RUN@cloud platform.
But what if your coworker wants to use the same tasks in their project? Avoid the urge to just duplicate the code! Friends don’t let friends copy and paste code—this maxim is true for build logic as well.
The right approach is to formalize the code into a Gradle plugin.
You’ll start your journey by creating an account on CloudBees.
Before you can interact with CloudBees’ PaaS through its API, you need to provision an account.
In this section, I’ll walk you through the signup and application setup process.
It requires you to fill out your email address, name, and password, as well as a username and domain.
Figure 8.1 Managing CloudBees runtime services through HTTP from a Gradle build script.
Upon successful submission, you’ll receive an email confirming your registration.
On the CloudBees landing page, you can find the services available to use.
In this chapter we’ll only concentrate on the application services.
Before you can access any of the application runtime services, you need to select a subscription plan.
Click the Applications link shown in figure 8.3 and select the free.
On the following page add the application service to your account.
To create a new application, choose the Apps menu item on top of the page or the Applications link on the landing page.
Both links will bring you to the application management page.
Before you can deploy any application, you’ll need to provision it.
Clicking the Create Application button will open a dialog, which should look similar to figure 8.4, that lets you enter the application name and supported runtime environment.
Because you’ll want to deploy a WAR file, choose the value JVM Web Application (WAR) from the dropdown box and enter todo into the input field to represent the application name.
You just have to enter the appropriate URL in the browser.
Because I chose the account name gradle-in-action in the registration form, my application URL is http://todo.gradle-in-action.cloudbees.net.
Give it a try! The application URL will already resolve even though you haven’t deployed a WAR file yet.
In the management page, you can now configure the application, deploy new versions, get an overview of incoming requests, monitor memory usage and server load, and view the log files, all in one place accessible over a central dashboard.
Feel free to familiarize yourself with the management functionality by browsing through the tabs.
Even though the application management dashboard is easy to use, you’d probably like to avoid manual labor at any cost.
To this end, you’ll use the CloudBees API, which enables you to fully automate the communication with the services backend.
The API key is unique to your account and clearly identifies the caller.
The secret key is used to securely sign the HTTP web request to the CloudBees services.
You’ll want to avoid checking this file into version control.
If you haven’t created the file yet, now is a good time to do so.
The following terminal commands show how to do this on *nix systems:
In the properties file, add the following keys and replace the placeholders with the actual values of your account:
Setting up an account on CloudBees and provisioning an application is really painless.
It probably took you less than five minutes to complete the whole process.
Imagine how much work it would be to set up a similar runtime environment on a self-hosted server.
Next, we’ll discuss the step-by-step game plan for building the plugin.
You’ll need to get to know some new concepts while at the same time applying techniques you’ve already learned in previous chapters.
Gradle distinguishes two types of plugins: script plugins and object plugins.
A script plugin is nothing more than a regular Gradle build script that can be imported into other build scripts.
With script plugins, you can do everything you’ve learned so far.
The source code for object plugins usually lives in the buildSrc directory alongside your project or a standalone project and is distributed as a JAR file.
In this chapter, you’ll learn how to use both approaches.
In the spirit of agile development, you’ll iteratively build the functionality in digestible pieces.
The goal is to get a first version up and running quickly to collect early feedback.
From a high-level view, you’ll plan to build the CloudBees plugin in three major steps, as shown in figure 8.5
With each of the following iterations, we’ll identify the drawbacks of the previous approach and discuss how to improve on it as we go along.
In the first step, I want you to become familiar with the CloudBees API and experience its functionality firsthand.
You’ll write two simple tasks in a script plugin: one for retrieving information on a provisioned application in your CloudBees account, and another one for deploying a WAR file to the cloud.
In step two, you’ll transfer the logic you’ve written in task action closures and encapsulate it into custom task classes.
By exposing properties, the behavior of the task classes will become highly configurable and reusable.
The property values will be provided by an enhanced task, the consumer of a task class.
In the final step, you’ll learn how to create a full-fledged object plugin.
You’ll set up a standalone Groovy project to produce the plugin JAR file.
With the master plan in place, you’ll get started by writing some tasks to interact with the CloudBees client SDK.
A script plugin is no different from your ordinary build.gradle file.
You’ll create a new script named cloudbees.gradle that will contain the future CloudBees tasks.
Because the build script’s filename deviates from the default naming convention, you’ll need to use the –b command-line option to invoke it.
Executing gradle –b cloudbees.gradle tasks should only present you with the default help tasks.
Before you can write the first task, the build script needs to become aware of the CloudBees API client library.
To use an external library directly in a build script, you’ll need to declare it in its classpath.
The method expects a single parameter, a closure that defines the dependencies you want to resolve denoted by the classpath configuration.
The following listing demonstrates how to add the latest version of the library to the build script’s classpath.
Whenever the script is executed for the first time, the CloudBees library is downloaded and put into your local dependency cache.
You’ll now be able to import and use any of the CloudBees SDK classes directly in your build script.
Next, you’ll write your first task to interact with your CloudBees account.
Each of the exposed methods gives you access to a specific RUN@cloud platform service.
Upon instantiation, the class expects you to provide the account credentials as well as the API’s URL, format, and version.
For now, you’ll define them as extra properties, as shown in the following code block:
You don’t want to share the API key and secret or check it into version control.
Read these values and store them in the properties apiKey and secret:
Listing 8.1 Adding the CloudBees API library to the build script’s classpath.
The CloudBees API can be used to remotely retrieve information about an application without having to log on to the dashboard, as shown in the following listing.
Before executing the task, you’ll create another Gradle script file: build.gradle.
The following code snippet demonstrates how an external script can be reused:
Note that the value of the from property in the apply method call can be any kind of URL, such as an HTTP address like http://my.scripts.com/shared/cloudbees.gradle.
Script plugins exposed over HTTP(S) are perfect candidates to be shared among departments in an organization.
Listing 8.2 Writing a task to list an available application on CloudBees account.
Declares input properties for task; if any properties aren’t provided the task execution fails.
CloudBees SDK client implementation that expos access to all services.
Any execution failure (e.g., authenticati errors from client) bubbles up as exception, which is caught and rethrow as specific Gradle exception.
The output gives the application’s title, when it was created, under what URL it can be reached, and its current status.
Applications on a free plan will be put to sleep if they have been idle for too long to save resources for other applications.
Upon the next request, the application will automatically be reactivated.
By successfully querying information about the application, you know that it exists under the given ID.
Now, you’ll actually deploy a WAR file to it so you can enjoy the fruits of your hard work.
The only difference is that you’ll need to provide other input parameters, like the WAR file itself and an optional message.
Listing 8.3 Writing a task for deploying a WAR file.
As shown in the console output, the deployment was successful.
In the CloudBees application dashboard you should see the newly deployed version, as shown in figure 8.6
Of course, you don’t want to miss out on actually trying the application.
The application is ready to be shown to your customers.
You’ve seen how easy it is to write tasks to interact with the CloudBees API.
Because you wrote the code in a shared script, you can manage applications from any other project that applies the script.
Let’s go a step further and see how to improve your design by turning the simple tasks into custom task classes.
In the last section, you saw how to create a shared script for interacting with a PaaS provider.
By applying a script plugin, you provided your project with tasks for managing and deploying your web application in a cloud environment.
A simple task is a great solution for developing one-off implementations.
Even though you took it to the extreme and provided configurable properties for your tasks, code maintainability and testability fell by the wayside.
If you want to go one step further, your best bet is to implement your logic in a custom task.
When using the custom task, you define how the task should behave by providing values for the properties.
If you see your task code grow, custom tasks can help to structure and encapsulate your build logic.
In fact, many tasks of Gradle’s standard plugins inherit from DefaultTask.
There are multiple options for where your custom task class can be defined.
The easiest way is to put it side by side with existing build code in your build script.
The custom task gets compiled automatically and put into the classpath when invoking a task of your script.
Another option is to put it under the buildSrc directory in the root directory of your project.
Make sure you stick to the source directory convention defined by the language plugin.
Gradle treats this directory as a default source directory and automatically tries to compile all source files whenever you run your build.
Keep in mind that Gradle’s incremental build feature is supported here as well.
Custom task classes that live under buildSrc are shared among all build scripts of your project and are automatically available in the classpath.
To make custom tasks transferable among projects, you can package them into a JAR file and declare it in your build script’s classpath.
Figure 8.7 shows the various implementation options for custom tasks.
You’ll create custom task source files in the buildSrc directory, which is the optimal setup for using them later with an object plugin.
The build script in the buildSrc directory declares the dependency on the CloudBees library.
The final directory structure of your project will look as follows:
The behavior of the task is encapsulated in the task action.
To indicate which action to execute, mark the method start() with the annotation @TaskAction.
The name of the task execution method can be picked arbitrarily as long as you don’t override the method with signature void execute() from the parent class.
The behavior of the task action can be configured through properties—for example, appId for defining the application identifier.
In the custom task implementation you import classes from the CloudBees library.
To make sure the classes can be compiled correctly, you’ll create a build script dedicated to your buildSrc project and declare the CloudBees library, as shown in the following listing.
To use and configure the behavior defined by the custom task, you’ll need to create an enhanced task.
The enhanced task declares the type of task it uses, in this case CloudBeesAppInfo, as shown in the next listing.
Parse the value from a project property provided as command parameters.
The API options and credentials are defined as extra properties within the build script.
Executing the enhanced task will first compile all custom classes in the buildSrc project and then retrieve the application information from CloudBees:
We discussed how to implement and use a single custom task for interacting with the CloudBees backend.
You’ll find it in the provided code examples for the book.
One of the benefits of dealing with actual classes for your task implementation is that you can make good use of principles of object-oriented programming.
By creating a parent class, you can significantly simplify the code you have to write for any given CloudBees custom task.
The following listing shows that all common characteristics just mentioned became a concern of the parent class.
You’ll use the parent CloudBees task for one of your custom tasks.
Listing 8.8 demonstrates how easy it is to deal with the CloudBees API.
No more repetitive creation of the API client or handling of exceptions.
Listing 8.7 Simplifying CloudBees interaction by introducing a parent task class.
You already executed this task and know that it works.
The more custom tasks you add to your project, the less you’ll feel inclined to manually rerun them each time you change the code to verify that they work.
Next, you’ll build confidence in your code by writing tests to be prepared for future refactorings.
The idea is to hand you a dummy instance of a Gradle Project that exposes the same methods and properties as the one you use in your build scripts.
You’ll see the ProjectBuilder in action by writing a test for the custom task CloudBeesAppInfo with the help of the Spock framework, as shown in listing 8.9
As you can see in the listing, you use the same package as the class under test.
Whenever you run the build, this class will automatically be compiled and the test cases will be executed.
Implements task action; the already-created CloudBees API client instance is provided.
The ProjectBuilder opens new doors to developing your build code with a testdriven approach, though it’s limited in functionality.
The Project instance produced by the ProjectBuilder doesn’t behave 100% like the real-world object.
Certain behaviors, like the up-to-date checks for input/output annotation or the actual loading of Gradle properties from your home directory, aren’t implemented.
In most cases, you can work around these shortcomings by writing additional code in your test class.
Deep integration testing with a sophisticated toolkit is one of the items on the roadmap and will be available in a future version of Gradle.
In the next section, we’ll discuss how to turn existing code into an object plugin and apply it from a different project.
Implementing your logic as custom tasks produces a maintainable and testable solution.
Bundled as a JAR file, tasks are fully reusable among independent projects.
Let’s review the advantages and shortcomings of a packaged custom task implementation:
Custom logic is self-contained in a class and can be configured through enhanced tasks.
Providing additional boilerplate code, conventions, and lifecycle integration isn’t straightforward.
A custom task can only be configured through an enhanced task.
It’s lacking an expressive extension mechanism through a self-defined DSL.
Functionality from other plugins can’t easily be used or extended.
Object plugins give you the most flexibility to encapsulate highly complex logic and provide a powerful extension mechanism to customize its behavior within your build script.
As with custom task classes, you have full access to Gradle’s public API and your project model.
Gradle ships with out-of-the-box plugins, called standard plugins, but can be extended by third-party plugins as well.
This means that they either rely on Gradle’s core API or deliver functionality through its packaged code.
More complex plugins may depend on features from other libraries, tools, or plugins.
Figure 8.8 shows how plugins fit into the overall architecture of Gradle.
In the previous chapters, you used various standard plugins covering support for programming languages and smooth integration with software development tools.
Think back to chapter 3 and remember how applying the Java plugin extended your.
As shown in figure 8.9, the plugin can provide a new set of tasks integrated into the execution lifecycle, introduce a new project layout with sensible defaults, add properties to customize its behavior, and expose configurations for dependency management.
With the addition of a single line of code, your project was able to compile source code, run unit tests, generate a report, and package the project into a JAR file.
All of this functionality came with minimal configuration effort from your side.
Standard plugins provide an impressive set of commonly used functionality.
Third-party plugins, contributed by the Gradle community or developed and shared among members of an enterprise, can be used to enhance your build scripts with nonstandard capabilities.
You may be used to sophisticated plugin portals that let you search for existing plugins, view their documentation, and even rate them.
At the time of writing, Gradle doesn’t provide a centralized repository for community plugins.
How do you know what’s out there, you may ask? Gradle provides a curated list of available community plugins on a wiki page: http://wiki.gradle.org/display/GRADLE/Plugins.
Feels pretty clunky, doesn’t it? Gradleware recognizes the fact that a plugin portal is an important prerequisite for sharing and distributing plugins and has added it as a planned feature to Gradle’s development roadmap.
For more information on its timeline, please refer to the roadmap’s dashboard (http://www.gradle.org/roadmap)
In this section, we’ll revisit how to use standard and third-party plugins in your build script.
Next, we’ll study a plugin’s internals to get a deep understanding of its building blocks and mechanics.
Finally, you’ll apply your knowledge by writing your own object plugin with all the bells and whistles.
Let’s revisit how to use an object plugin in a project.
You’ve seen that a project can be configured to use a standard plugin by using the apply method.
The method defines one parameter of type java.util.Map called options.
Using and building object plugins option you want to use here is plugin.
A plugin can be applied to a build script by using its name or type.
To apply the Java plugin to a project, pass in the key plugin with a value of java: apply plugin: 'java'
This is useful if the plugin doesn’t expose a name or if there’s a naming conflict between two different plugins.
Applying a plugin by type makes it explicit but feels a bit more cumbersome:
A convenient side effect of using standard plugins is that they’re part of Gradle’s runtime.
In most cases, the user doesn’t have to know about the libraries or the versions the plugin depends on.
The Gradle distribution makes sure that all standard plugins are compatible.
If you’re curious where to find these libraries, look at the directory lib/plugins of your Gradle installation.
You can do this by using the buildscript method that defines the location of the plugin, the repository, and the plugin dependency.
The order in which the buildscript and apply methods are declared is irrelevant.
During the configuration phase, Gradle will build the model of your project and connect the dots between the plugin and the build logic.
An external plugin is treated like every other dependency in Gradle.
Once it’s downloaded and put into the local dependency cache, it’s available for subsequent runs of the build.
The following listing shows how to apply the external plugin tomcat for deploying web applications to an embedded Tomcat container.
The build script only needs to define the plugin dependency and its originating repository.
In the following section, we’ll dissect the internals of a plugin to get a better understanding of its anatomy.
Listing 8.10 Applying the tomcat plugin available on Maven Central.
Figure 8.10 shows a high-level overview of the options you have when implementing an object plugin.
There are four basic elements that are important for implementing an object plugin:
Gradle gives you full flexibility over the location in which to place your plugin implementation.
The code can live in the build script or the buildSrc directory, or it can be developed as a standalone project and in turn distributed as a JAR file.
Every plugin needs to provide an implementation class, which represents the plugin’s entry point.
Plugins can be written in any JVM language that compiles down to bytecode.
I prefer Groovy because you can benefit from its dynamic language features and conciseness.
However, you can also use Java or Scala to implement your build logic.
A plugin applied to a project can be customized through its exposed extension objects.
This is especially useful if the user wants to override the plugin’s default configuration from the consuming build script.
The plugin descriptor is a property file containing meta-information about the plugin.
Usually it contains a mapping between the plugin’s short name and the plugin implementation class.
Plugin bundled in JAR file containing the compiled implementation class and a plugin descriptor.
The plugin you’re about to create is going to use the custom tasks you implemented before.
In the early phases of developing a plugin, you’ll want to have a quick feedback loop.
Because you don’t have to package your plugin code, you can fully concentrate on implementing business logic while having full access to Gradle’s API.
To represent the intent of the plugin, name the class CloudBeesPlugin, as shown in the following listing.
For all CloudBees tasks we automatically assign the API URL, key and secret.
If the warFile property is not provided, we assign path to the WAR file produce by the WAR plugin.
As shown in the listing, you integrated your custom tasks and preconfigured them with default values.
Your plugin makes certain assumptions about the nature of the project that consumes the plugin.
To do so, use the plugin implementation type in build.gradle, as shown in the following code snippet:
To verify that the task has been created, run gradle tasks.
Currently, you’re retrieving the inputs for your custom tasks from the command line.
You can improve on this design by obtaining this configuration from the consuming build script.
Parsing command-line parameters to feed your tasks with inputs may not always be desirable.
You can establish your own build language by exposing a DSL with its own unique namespace.
The code shows a closure named cloudBees that allows for setting values for properties you need as required input values for tasks from the consuming build script.
An extension can be added to many Gradle objects like the Project or a Task, as long as they’re extension-aware.
Plugin capabilities versus conventions As a plugin developer, you often walk the fine line between capabilities and conventions provided by a plugin.
On the one hand, you may want to enhance another project’s functionality; for example, through tasks.
On the other hand, you may want to introduce conventions that make meaningful decisions for the user; for example, standardized project layouts.
If the conventions impose a strong, opinionated view on the structure of the consuming project, it makes sense to separate basic functionality from conventions by creating two different plugins: a base plugin that contains the capabilities, and another one that applies the base plugin and preconfigures these capabilities by convention.
This approach was taken by the Java plugin, which derives from the Java base plugin.
For more information on their characteristics, please see appendix B or the online documentation.
Listing 8.12 Providing a plugin DSL for capturing user input.
Every extension needs to be backed by a model that captures the values provided in the user’s build script.
The model can be a simple plain old Java or Groovy Bean.
The next listing shows the extension model for the CloudBees plugin that you create in the package as your object plugin implementation.
As shown in listing 8.12, you need to extend the backing Project of the build script that applied the CloudBees plugin.
Extension-aware objects expose the method extensions() that returns a container for registering extension models with a name.
That method takes in a name and the model type as parameters.
Once an extension is registered, you can query for the model values and assign them to custom task properties.
Using extension values for feeding input properties of custom tasks can be a bit tricky.
Remember that custom task properties are set during the configuration phase of the build lifecycle.
At that point of time extension, values haven’t been populated.
You can solve the problem of evaluation order by using the concept of convention mapping.
Extensions versus extra properties Extensions are used to extend the DSL of an object that is extension-aware.
A registered extension model can expose properties and methods that can be used to establish new build language constructs for your build script.
The typical use case for an extension is a plugin.
Extra properties, on the other hand, are simple variables that can be created through the ext namespace.
They’re meant to be used in the user space, the build script.
Every task of your plugin has a property named conventionMapping.
To be more specific, every task derived from DefaultTask owns this property.
You use this property to assign the extension model values to a task’s input or output fields.
By wrapping the extension model value into a closure, you lazily set these values.
This means that the value is only calculated when the task is executed.
To retrieve the values of a property stored in convention mapping, you’ll need to explicitly use getter methods, as shown in the next listing.
Keep in mind that trying to access a field directly will result in a null value.
Adds tasks after project is evaluated to ensure that extension values are set.
Assigning the extension property value wrapped in a closure to the task’s convention mapping.
Properties set by convention mapping need to explicitly use getter methods.
Convention mapping is a powerful concept used by many Gradle core plugins to ensure that extension properties are evaluated at runtime.
Even though the conventionMapping property isn’t part of the public Task API, it’s your best bet to set a task’s input/output property values in combination with extensions.
Next, you’ll equip your plugin with a more descriptive name.
Even though the namespace is less susceptible to naming clashes with other plugins, it would be handy to be able to pick a shorter, more expressive plugin name.
The name of the property file automatically determines the plugin name.
The next listing demonstrates how to apply the plugin with its short identifier in your build script.
From now on, you’ll only use the short identifier when you want to apply the CloudBees plugin.
A plugin can be tested on the level of the implementation class as well.
Testing your plugin code is as easy as testing custom tasks.
The Project instance produced by ProjectBuilder provides the perfect setup for verifying your plugin’s.
Other options for setting configuration-time properties There are other approaches to dealing with these kinds of situations, each with their own advantages and drawbacks.
Usually, they’re highly dependent on your use case and the language you use to implement your plugin.
This topic is heavily discussed on the Gradle online forum.
In the following listing, you apply the plugin, set the extension values, and test for the correct behavior of the created tasks.
As the next step, you’ll set up a standalone project for the plugin so you can build a JAR distribution that can be shared among independent projects.
Implementing a plugin in the buildSrc project is convenient if the code is supposed to be used from the build scripts of the main build; for example, in a multiproject.
Listing 8.18 Writing a test for the plugin implementation class.
If you want to share a plugin across builds, you’ll need to develop it as a standalone project and publish the produced artifact to a repository.
Each time you want to release a new version of the plugin, the produced JAR file will be published to a local Maven repository named repo.
The repository will live on the same directory level as the plugin project.
The To Do web application will act as a plugin consumer.
Its build script will define the local repository, declare the plugin as a dependency, and use the plugin tasks to interact with the CloudBees backend services.
You’ll start by creating a new project for the plugin with the directory named plugin.
You’ll copy the existing structure from the buildSrc directory to the new project.
The todo project is a one-to-one copy from your existing To Do application multiproject build.
You won’t need to create the directory for the local repository—it’s automatically generated at publishing time.
The project doesn’t have access to the buildSrc infrastructure anymore, so you’ll need to declare dependencies on the Groovy and Gradle API libraries.
You’ll configure the Maven deployer to upload both files to a local directory.
To clearly identify the artifact, assign a value to the plugin’s group, name, and version.
Before the plugin can be consumed by the To Do web application, you’ll upload it with the help of a task from the Maven plugin called uploadArchives.
Executing the task should produce a similar output to the following:
After publishing the artifact, you’ll find a new directory named repo.
It contains the plugin’s JAR and POM files and is ready for consumption.
In chapter 14, we’ll discuss the Maven plugin in more detail, as well as how to publish artifacts to publicly available repositories.
Configures Maven deployer to upload plugin artifact to a local directory.
The artifact automatically knows its own dependencies, which are declared in the POM file.
Alternatively, the consuming project can also refer to the JAR file directly by declaring a file dependency.
If you go with that option, you’ll need to handle the plugin’s transitive dependencies yourself.
The next listing demonstrates how easy it is to let your build script depend on the plugin, available in the local Maven repository.
That’s it—you’ve gotten to know all the important development practices that optimally prepare you for writing your own Gradle plugins.
Gradle provides a rich plugin ecosystem for reusing functionality through out-of-thebox standard plugins and third-party plugins contributed by the community.
There are two types of plugins: script plugins and object plugins.
A script plugin is a regular Gradle build script with full access to Gradle’s API.
Writing a script plugin is very easy, lowers the bar for sharing code, and can be applied to another project by a URL.
Object plugins usually contain more complex logic that requires appropriate structuring into packages and classes.
The entry point of every object plugin is the interface Plugin that provides direct access to Gradle’s Project model.
Listing 8.20 Using the object plugin from the web project.
References Maven Central to retr the plugin’s dependencies (name CloudBees API client library)
In this chapter, you built a Gradle plugin for interacting with the CloudBees backend through an API library.
For this purpose, we discussed two useful functionalities: deploying a WAR file to a CloudBees web container and retrieving runtime information about this application.
You wrote simple tasks in a script plugin, translated these tasks into custom tasks located in the buildSrc project, and later turned this code into a full-fledged object plugin.
A plugin can expose its own DSL for configuring functionality.
Extensions are powerful API elements for introducing the concept of convention over configuration into your plugin.
You experienced a typical scenario by registering an extension that serves as a model for capturing user input for overriding default configuration values.
Writing test code for your plugin is as important as writing it for application code.
Gradle’s ProjectBuilder allows for creating a Project dummy representation that can be used to test custom components.
Having tools like this removes impediments to writing tests for build code and encourages developers to aim for high code coverage.
The next chapter will be particularly helpful for users who have existing build infrastructure developed with Ant or Maven and who plan to migrate to Gradle.
We’ll also talk about upgrading Gradle versions in your project and how to verify its success by comparing the outcomes of the builds before and after the migration.
Long-running development projects are usually heavily invested in established build tool infrastructure and logic.
As one of the first build tools, Gradle acknowledges that moving to a new system requires strategic planning, knowledge transfer, and acquisition, while at the same time ensuring an unobstructed build and delivery process.
Gradle provides powerful tooling to integrate existing build logic and alleviate a migration endeavor.
If you’re a Java developer, you likely have at least some experience with another build tool.
Many of us have worked with Ant and Maven, either by choice or because the project we’re working on has been using it for years.
If you decide to move to Gradle as your primary build tool, you don’t have to throw your existing knowledge overboard or rewrite all the existing build logic.
We’ll also explore migration strategies to use if you decide to go with Gradle long-term.
Ant users have the best options by far for integrating with Gradle.
Through the helper class AntBuilder, which is available to all Gradle build scripts, any standard Ant task can be used with a Groovy builder-style markup, similar to the way you’re used to in XML.
Gradle can also be pointed to an existing Ant build script and can reuse its targets, properties, and paths.
This allows for smooth migrations in baby steps as you pick and choose which of your existing Ant build logic you want to reuse or rewrite.
The migration path from Maven to Gradle isn’t as easy.
At the time of writing, a deep integration with existing Maven project object model (POM) files isn’t supported.
To get you started, Gradle provides a conversion tool for translating a Maven pom.xml into a build.gradle file.
Whether you’re migrating an existing Maven build or starting from scratch, Maven repositories are ubiquitous throughout the build tool landscape.
Gradle’s Maven plugin allows for publishing artifacts to local and remote Maven repositories.
Migrating a build process from one build tool to another is a strategic, missioncritical endeavor.
The end result should be a comparable, functional, and reliable build, without disruption to the software delivery process.
On a smaller scale, migrating from one Gradle version to another can be as important.
Gradle provides a plugin that compares the binary output of two builds—before and after the upgrade—and can make a deterministic statement about the result before the code is changed and checked into version control.
In this chapter, you’ll learn how to use the plugin to upgrade your To Do application from one Gradle version to another.
Let’s start by taking a closer look at Gradle’s Ant capabilities.
On the one hand, it can import an existing Ant script and directly use Ant constructs from a Gradle build script as if they’re native Gradle language elements.
This type of integration between Gradle and Ant doesn’t require any additional change to your Ant build.
On the other hand, familiar Ant tasks (for example, Copy, FTP, and so on) can be used within your Gradle build script without importing any Ant script or additional dependency.
Long-term Ant users will find themselves right at home and can reuse familiar Ant functionality with a convenient and easy-to-learn Groovy DSL notation.
Chapter 2 demonstrated how to use the Echo task within a Gradle build.
It allows for using Ant capabilities directly from Groovy in a concise fashion.
Gradle augments the Groovy AntBuilder implementation by adding new methods.
In regular class files that don’t have access to a project or task, you can create a new instance.
The following code snippet demonstrates how to do that in a Groovy class:
You’re going to take the Ant build script from chapter 1, import it into a Gradle build script, reuse its functionality, and even learn how to manipulate it.
Importing an Ant script and reusing its functionality from Gradle is dead simple.
All you need to do is use the method importBuild from Gradle’s AntBuilder and provide it with the target Ant build script, as shown in figure 9.2
To see the import functionality, you’ll take the directory structure of your Ant build from chapter 1 and put it under the directory name ant.
Parallel to this directory, create another directory named gradle, which holds your Gradle build script responsible for importing the Ant script.
The end result should look similar to the following directory tree:
Figure 9.1 Access to Ant functionality from Gradle is provided through the class AntBuilder.
Let’s look at the Ant build script shown in listing 9.1
It’s a simple script for compiling Java source code and creating a JAR file from the class files.
This directory only holds a single library, the Apache Commons language API.
The script also defines a target for initializing the build output directory.
Another target named clean makes sure that existing class and JAR files can be deleted.
Use the implicit property named ant, call the method importBuild, and provide the path to the Ant build script, as shown in the following code snippet:
Listing the tasks available to the Gradle build script reveals that Ant targets are treated as Gradle tasks:
Ant properties for defining directories and the version of your JAR file.
Other tasks ----------clean - clean up dist - generate the distribution compile - compile the source init.
As shown in the command-line output, Gradle inspects the available Ant targets, wraps them with Gradle tasks, reuses their description, and even keeps their dependencies intact.
You can now execute the translated Ant targets as you’re used to in Gradle:
After executing the dist Ant target, you’ll find that the source code was compiled and the JAR file was created—exactly as if running the script directly from Ant.
Importing an Ant script into Gradle can be a first step toward a full migration to Gradle.
In section 9.1.3, we’ll discuss various approaches in more detail.
The commandline output of Gradle tasks wrapping Ant targets is pretty sparse.
As an Ant user, you may want to see the information you’re used to seeing when executing the targets from Ant.
As always, this commandline parameter renders more information than you actually want to see.
Instead of using this command-line parameter, you can directly change the logging level for the Gradle task that wraps the Ant target.
The following assignment changes the logging level to INFO for all Ant targets:
Listing imported Ant targets manually can be tedious and error-prone.
Unfortunately, there’s no easy way around it, because you can’t distinguish the origin of a task.
Run the dist task again to see the appropriate output from Ant:
You’ll see the output from Ant that you’re familiar with.
Often you’ll want to further modify the original Ant script functionality or even extend it.
This could be the case if you’re planning a gradual transition from your existing Ant script to Gradle.
Accessing imported Ant properties and paths from Gradle Ant targets translate one-to-one to Gradle tasks and can be invoked from Gradle with exactly the same name.
This makes for a very fluent interface between both build tools.
The same can’t be said about Ant properties or paths.
To access them, you’ll need to use the methods getProperties() and getReferences() from Gradle’s AntBuilder reference.
Keep in mind that the Gradle task properties won’t list any Ant properties.
In turn, you can make good use of all of their features.
Remember when we discussed adding actions to existing Gradle tasks in chapter 4? You can apply the same behavior to imported Ant targets by declaring doFirst and doLast actions.
The following listing demonstrates how to apply this concept by adding log messages to Ant target init before and after the actual Ant target logic is executed.
Now when you execute the task init, the appropriate messages are rendered in the terminal:
Importing Ant targets into a Gradle build is often only the starting point when working in the setting of a conjunct build.
You may also want to extend the existing model by functionality defined in the Gradle build script.
In listing 9.2, you saw how to access the Ant property build via the AntBuilder method getProperties()
You can even change the value of an Ant property to make it fit your needs.
You can also make changes to the task graph by hooking in new tasks.
With the regular instruments of Gradle’s API, a dependency can be defined between an Ant target and a Gradle task or vice versa.
Let’s look at code that pulls together all of these concepts in a concise example.
In the next listing, you’ll make heavy use of existing Ant properties, change the value of an existing Ant property, and let an imported Ant target depend on a new Gradle task.
Adds a Gradle action executed before any Ant target code is run.
Renders a message to inform the user about the directory you’re about to delete by accessing Ant property build.
Adds a Gradle action that’s executed after Ant target code is run.
It simply creates a new JAR file containing the Java source files in the destination directory ant/build/libs.
Because it should be part of your distribution, you declared a dependency on the Ant target dist.
Executing the build automatically invokes the task as part of the task graph:
The resulting JAR files can be found in a new distribution directory:
So far, you’ve learned how to apply Gradle’s feature set to simplify the integration with an existing Ant build script.
Next you’ll apply one of Gradle’s unique and powerful features: incremental builds.
Sure, you can always implement it yourself by applying homegrown (and potentially error-prone) techniques to prevent the execution of unnecessary targets (for example, with the help of time-stamped files)
But why put in all this effort if Gradle provides a built-in mechanism for it? The following listing demonstrates how to define inputs and outputs for the compilation target imported from the Ant script.
Listing 9.4 Defining inputs and outputs for imported Ant target.
First, clean up the existing class and JAR files and run through the whole generation process:
As expected, the Java source code is compiled and the JAR files are created.
When you run the dist task again without first deleting the files, Gradle realizes that the source files haven’t changed and that output files already exist.
The compilation task is automatically marked as UP-TO-DATE, as shown in the following command-line output:
Being able to add incremental build functionality to an imported Ant target is a big win for Ant users coming to Gradle.
It proves to be a huge timesaver, especially in enterprise builds with many dependencies and source files.
Even if you don’t import an existing Ant build script, Gradle allows for executing Ant tasks directly from your Gradle build script.
To round out your use case, let’s discuss how to incorporate one of Ant’s standard tasks into the build.
Gradle’s AntBuilder provides direct access to all standard Ant tasks within your build script—no additional configuration needed.
At runtime, Gradle checks the bundled Ant JAR files available on its classpath for the respective Ant task.
Figure 9.3 illustrates the interaction between the use of an Ant task within the Gradle build script, the Gradle runtime, and its included Ant tasks.
Using standard Ant tasks in Gradle comes in handy if you don’t want to import an existing Ant script, or if you feel more comfortable with the Ant syntax.
Using an Ant task in a Gradle build script isn’t hard, as long as you remember some basic rules:
Nested task elements don’t require the use of the implicit ant variable.
Let’s look at a concrete example: the Ant Get task.
The purpose of the task is to download remote files to your local disk via HTTP(S)
Assume that you want to download two files to the destination directory downloads.
Figure 9.4 shows how to express this logic using the AntBuilder DSL.
If you compare the task definition in Ant and Gradle, there are a few differences.
You got rid of the pointy brackets and ended up with a more readable task definition.
The important point is that you don’t have to rewrite existing Ant functionality, and its integration into Gradle is seamless.
Next, you’ll use the Get task in the context of your previous code examples.
As part of your distribution, you want to bundle a project description file and the release notes.
Each file resides on a different server and isn’t part of the source code in version control.
This is a perfect use case for the Get Ant task.
The following listing shows how to apply the Get Ant task to download both files and make them part of the generated JAR file.
All standard Ant tasks can be used with this technique because they’re bundled with the Gradle runtime.
Make sure to keep the Ant documentation handy when writing your logic.
Optional or third-party Ant tasks usually require you to add another JAR file to the build script’s classpath.
You already learned how to do that in chapter 5 when you used the external Cargo Ant tasks to deploy your To Do application to a web container.
Please refer to the code examples in chapter 5 for more information on how to use optional Ant tasks.
So far, you’ve learned many ways to interact with existing Ant build scripts or tasks from Gradle.
But what if you’re planning to move to Gradle long term? How do you approach a step-by-step migration?
Gradle doesn’t force you to fully migrate an existing Ant script in one go.
A good place to start is to import the existing Ant build script and get familiar with Gradle while using existing logic.
In this first step, you only have to invest minimal effort.
Let’s look at some other measures you may want to take.
Try to implement the logic of the targets “the Gradle way” instead of falling back to an implementation backed by the AntBuilder.
This approach automatically buys you incremental build functionality without actually having to explicitly declare inputs and outputs.
Over time, you’ll see that your Ant build script will get smaller and smaller while the logic in your Gradle build will grow.
If you’re an Ant user and aren’t already using Ivy’s dependency management, it will relieve you of the burden of having to manually manage external libraries.
When migrating to Gradle, dependency management can be used even if you’re compiling your sources within an Ant target.
All you need to do is move the dependency declaration into your Gradle build script and provide a new property to the Ant script.
The next listing demonstrates how to do this for a simple code example.
You can now also get rid of the lib directory in your Ant build, because Gradle’s dependency manager automatically downloads the dependencies.
Of course, you could also move the Javac task to Gradle.
But why do all of this work when you can simply use the Gradle Java plugin? Introducing the plugin would eliminate the need for the Ant build logic.
Your example Ant project resembles the typical tasks needed in a Java project: compile the source code, assemble a JAR file, and clean up existing artifacts.
Applying the Java plugin works like a charm, as long as your Ant script doesn’t define any targets that have the same name as any of the tasks exposed by the plugin.
Give it a shot by modifying your Gradle build to have the following content:
Sets a new Ant property to be used for compilation in.
Executing any Gradle task will indicate that you have a task namespace clash, as shown in the following command-line output:
What went wrong: A problem occurred evaluating root project 'gradle'
Cannot add task ':clean' as a task with that name already exists.
Either exclude the existing Ant target, or wrap the imported Ant target with a Gradle task with a different name.
The approach you take depends on your specific use case.
The following code snippet demonstrates how to trick AntBuilder into thinking that the task already exists:
As a result, the original Ant target is excluded; the clean task provided by the Java plugin is used instead.
Excluding some of the less complex Ant targets may work for you, but sometimes you want to preserve existing logic because it would require a significant amount of time to rewrite it.
In those cases, you can build in another level of indirection, as shown in figure 9.5
The consuming Gradle build script declares an enhanced task of type GradleBuild that defines the Ant target you want to use with a new name.
You can think of this technique as renaming an existing Ant target.
Figure 9.5 Wrapping an imported Ant target by exposing a Gradle task with a new name.
With this code in place, the exposed Gradle task name is cleanAnt:
If you want to let the standard Gradle clean task depend on the Ant clean-up logic, you can define a task dependency between them:
We discussed how to approach a migration from Ant to Gradle step by step without completely blocking the build or delivery process.
In the next section, we’ll compare commonalities and differences between Maven and Gradle concepts.
As we did in this section, we’ll also talk about build migration strategies.
It allows for importing existing Ant build scripts that translate targets in Gradle tasks, enable executing them transparently from Gradle, and provide the ability to enhance targets with additional Gradle functionality (for example, incremental build support)
With these features in place, you can approach migrating from Ant to Gradle through various strategies.
Unfortunately, the same cannot be said about the current Maven integration support.
At the time of writing, Gradle doesn’t provide any deep imports of existing Maven builds.
This means that you can’t just point your Gradle build to an existing POM file to derive metadata at runtime and to execute Maven goals.
But there are some counter strategies to deal with this situation, and we’ll discuss them in this section.
Before we dive into migrating from Maven to Gradle, let’s compare commonalities and differences between both systems.
Then, we’ll map some of Maven’s core concepts to Gradle functionality.
When directly comparing Maven and Gradle, you can find many commonalities.
Tasks from originating build script to be invoked when wrapped task is executed.
Wrapping task with name cleanAnt that directs the call to imported clean Ant target.
Let’s discuss some important differences frequently asked about on the Gradle forum.
Many of the Maven dependency scopes have a direct equivalent to a Gradle configuration provided by the Java or War plugin.
There’s one Maven scope that only finds a representation with the War plugin: provided.
Dependencies defined with the provided scope are needed for compilation but aren’t exported (that is, bundled with the runtime distribution)
The scope assumes that the runtime environment provides the dependency.
If you aren’t building a web application, you won’t have an equivalent configuration available in Gradle.
This is easily fixable—you can define the behavior of the scope as a custom configuration in your build script.
The following listing shows how to create a provided scope for compilation purposes and one for exclusive use with unit tests.
This is particularly true for Maven Central, the go-to location on the web for retrieving open source libraries.
Table 9.1 Maven dependency scopes and their Gradle configuration representation.
Being able to publish an artifact to a Maven repository is equally important because in an enterprise setting a Maven repository may be used to share a reusable library across teams or departments.
Maven ships with support for deploying an artifact to a local or remote Maven repository.
As part of this process, a pom.xml file is generated containing the meta-information about the artifact.
Gradle provides a 100% compatible plugin that resembles Maven’s functionality of uploading artifacts to repositories: the Gradle Maven Publishing plugin.
We won’t discuss the plugin any further in this chapter.
A typical use case for a profile is to define properties for use in a specific deployment environment.
If you’re coming to Gradle, you’ll want to either reuse an existing profile definition or to emulate this functionality.
I’m sorry to disappoint you, but Gradle doesn’t support the concept of profiles.
Let’s first look at how you can read an existing profile file.
Assume that you have the settings.xml file shown in the following listing located in your Maven home directory (~/.m2)
Listing 9.8 Maven profile file defining application server home directories.
The settings file declares two profiles for determining the application server home directory for deployment purposes.
Based on the provided environment value with the key env, Maven will pick the appropriate profile.
The next listing shows how to read the settings file, traverse the XML elements, and select the requested profile based on the provided property env.
To run this example, call the task name and provide the property as a commandline parameter:
This works great if you want to stick with the settings file.
However, at some point you may want to get rid of this artifact to cut off any dependency on Maven concepts.
You have many options for defining values for a specific profile:
The benefit of this approach is that you’re able to use any data type available to Groovy to declare properties.
Traverse XML elements to find the application server property value.
Maven’s site plugin allows for creating a set of HTML files by running a single goal.
A site usually exposes a unified view on general information about the project extracted from the POM, as well as aggregated reporting on test and static code analysis results.
At the time of writing, this functionality isn’t available to Gradle users.
A standard Gradle plugin that started down that road is build-dashboard.
The plugin was introduced with Gradle 1.5 and exposes a task for building a dashboard HTML report that contains references to all reports generated during the build.
Make sure to check the Gradle online documentation for more information.
In the previous section, we discussed major differences between the build tools Maven and Gradle.
The illustrated approaches for bridging the gap between certain functionality will give you a head start on a successful migration.
Because Gradle doesn’t offer importing Maven goals into the Gradle model at runtime, a migration will have to take place in parallel to the existing Maven setup.
This procedure ensures a smooth migration without disrupting the build and delivery process.
For a quick refresher on the code, look at the following listing.
Maven’s artifactId element value maps to a Gradle project name.
The plugin supports generating build.gradle and settings.gradle files by analyzing a Maven pom.xml file, as shown in figure 9.6
Let’s take a closer look at how to use the plugin.
There’s another task provided by the plugin that we haven’t looked at yet: maven2Gradle.
This task is only presented if your current directory contains a pom.xml file.
Create a new directory, create a pom.xml file, and copy the contents of listing 9.11 into it.
Now, navigate to this directory on the command line and list the available Gradle tasks:
Executing the maven2Gradle task will analyze the effective Maven POM configuration.
When I talk about the effective POM, I mean the interpolated configuration of the pom.xml file, any parent POM, and any settings provided by active profiles.
Even though the task is at an early stage of development, you can expect the following major conversion features:
In most cases, this converter task does a good job of translating the build logic from Maven to Gradle.
One thing to remember is that the converter doesn’t understand any third-party plugin configuration, and therefore can’t create any Gradle code for it.
After executing the task, you’ll find the expected Gradle files in the same directory as the POM file:
Let’s take a closer look at the generated build.gradle file, as shown in the next listing.
The file contains all necessary DSL elements you’d expect: plugins, project metadata, repositories, and dependencies.
The project name can only be set during Gradle’s initialization phase.
For that reason, the maven2Gradle task also generates the settings file shown in the following listing.
The generated Gradle files will give you a good start in migrating your complete Maven.
Without compromising your existing build, you can now add on to the generated Gradle logic until you’ve fully transferred all functionality.
At that point, you can flip the switch and continue to use Gradle as the primary build tool and start building confidence.
Should you encounter any impediments, you can always fall back to your Maven build.
Wouldn’t it be great if you could automatically determine that the build artifacts between the Maven and Gradle build are the same? That’s the primary goal of the Gradle build comparison plugin.
It has the high goal of comparing the outcomes of two builds.
When I speak of an outcome, I mean the binary artifact produced by a build—for example, a JAR, WAR, or EAR file.
Gradle build compared to an Ant or Maven build in the case of a build tool migration.
Comparing the same Gradle build with two different versions in the case of an upgrade.
Comparing a Gradle build with the same version after changing build logic.
I know you’re excited because this plugin could be extremely helpful in comparing the build outcomes after migrating from Ant or Maven.
At the time of writing, this functionality hasn’t been implemented.
What you can do, though, is compare a Gradle build after upgrading the version.
Figure 9.7 demonstrates such a use case in the context of your To Do application.
Your sample project creates three binary artifacts: two JAR files produced by the projects model and repository, and one WAR file produced by the web project.
The following listing shows the necessary setup required to compare the builds.
The task will fail if the outcome of the compared build is different.
Because the command-line output of this task is lengthy, I won’t show it here.
What’s more interesting is the reporting structure produced by the task.
The following directory tree shows the build outcome files used to compare the build and HTML report:
Figure 9.7 Comparing build outcomes of two different Gradle versions.
Target build definition pointing to root project of source build definition.
The report file index.html gives detailed information about the compared builds, their versions, the involved binary artifacts, and the result of the comparison.
If Gradle determines that the compared builds aren’t identical, it’ll be reflected in the reports, as shown in figure 9.8
I think it becomes apparent how helpful the functionality of this plugin can be in making a determination whether an upgrade can be performed without side effects.
The plugin has a lot of future potential, especially if you’re interested in comparing existing builds backed by other build tools within the scope of a migration.
In this chapter, we discussed how the traditional Java-based build tools Ant and Maven fit into the picture of integration and migration.
As an Ant user, you have the most options and the most powerful tooling.
Gradle allows for deep imports of an Ant build script by turning Ant targets into Gradle tasks.
Even if you don’t import existing Ant builds, you can benefit from reusing standard and third-party Ant tasks.
Figure 9.8 Sample build comparison HTML report for upgrading a Gradle version.
If you’re coming from Maven, the basic project layout and dependency management usage patterns should look strikingly familiar.
We discussed some Maven features that are missing from Gradle, such as the provided scope and the concept of profiles.
You saw that Gradle (and ultimately its underlying language Groovy) is flexible enough to find solutions to bridge the gap.
Unfortunately, Gradle doesn’t support importing Maven goals from a POM at runtime, which makes migration less smooth than for Ant users.
With Gradle’s maven2Gradle conversion task, you can get a head start on a successful migration by generating a build.gradle file from an effective POM.
Upgrading a Gradle build from one version to another shouldn’t cause any side effects or even influence your ability to compile, assemble, and deploy your application code.
The build comparison plugin automates upgrade testing by comparing the outcomes of a build with two different versions.
With this information in hand, you can mitigate the risk of a failed upgrade.
Congratulations, you got to know Gradle’s most essential features! This chapter concludes part 2 of the book.
In part 3, we’ll shift our focus to using Gradle in the context of a continuous delivery process.
First, let’s discuss how to use Gradle within popular IDEs.
Building an application on the developer’s machine is only part of the story.
In times of increased pressure to deliver software quickly and frequently, automating the deployment and release process is extremely important.
In part 3, you’ll learn how to use Gradle to its fullest in the context of continuous delivery.
Many developers live and breathe their IDEs, which are key to being highly productive.
Chapter 10 delves into the nitty-gritty details of generating IDE project files with the Gradle’s core plugins, importing a Gradle project from scratch, and using IDE features to manage and operate a build from the IDE.
Software projects in today’s world rarely concentrate on using a single programming language to get the work done.
Instead, developers bring in suited languages to make their lives easier.
In chapter 11, we’ll discuss the use of Gradle as a single solution to build polyglot projects.
Continuous delivery describes a build pipeline that orchestrates the end-to-end process for getting software from the developer’s machine into the production environment.
The last four chapters of the book are devoted to implementing a pipeline with Gradle and third-party tools.
We’ll start by looking at continuous integration, the backbone of every build pipeline.
In chapter 13, you’ll use Jenkins, an open source continuous integration server, to model a chain of build steps.
Later on, you’ll take this artifact and deploy it to different environments.
By the end of part 3, you’ll be able to use your Gradle skills to take your own projects to the next level.
Being able to automate the full delivery process will save your organization lots of money and vastly improve your time-to-market ratio.
Functionality like syntax highlighting, navigation through keyboard shortcuts, refactoring,  and code completion generation save valuable time during development and make implementing code a more enjoyable experience.
IDEs store the configuration data for projects in project files.
The format of these files, usually described in XML, is different from vendor to vendor.
Historically, popular IDEs didn’t incorporate integration support for Gradle, so Gradle had to provide plugins to generate these files.
In this chapter, we’ll look at the plugins provided by Gradle to generate project metadata for popular Java IDEs.
These plugins are effective tools in deriving most of this metadata from your build script definition.
We’ll discuss how to use the DSL exposed by these plugins to adjust this metadata to your individual needs.
With the increasing popularity of Gradle, IDE vendors have provided first-class, built-in support for managing Gradle projects.
We’ll explore how to import existing projects backed by Gradle build scripts, navigate these projects, and execute Gradle tasks directly in the IDE.
The breadth of this functionality is dependent on the IDE product.
Most IDE vendors use the tooling API to embed Gradle into their products.
The tooling API is part of Gradle’s public API and is used to execute and monitor a build.
While the API’s main focus is to embed Gradle into third-party applications, it can also be used in other situations.
You’ll get to know one of these use cases and apply the API in practice.
You’ll start by using Gradle’s IDE plugins to generate project files for your To Do application.
IDEs describe organizational units with the notation of a project, similarly to a Gradle project.
A project defines a type (for example, web versus desktop application), external dependencies (for example, JAR, source, and Javadoc files), plus individual settings.
With the help of project files, a project can be opened in an IDE and can allow for sharing configuration data with other developers.
Unfortunately, the formats of these project files are not unified across IDE products.
Gradle allows for generating IDE project files with the help of plugins.
The standard Gradle distribution provides two out-of-the-box plugins: Eclipse and IDEA.
The plugins also expose a powerful DSL for customizing the generated project settings.
To generate project files, you’ll need to apply the IDE plugin to your build script, execute the task provided by the plugin, and import the generated project files into your IDE, as shown in figure 10.1
To be able to share project files, you’d usually check them into your VCS alongside your source code.
When another developer checks out the project from the VCS, they.
Using IDE plugins to generate project files can directly open the project in the IDE and start working.
Using the Gradle IDE plugins makes this step redundant because you describe the project settings using the plugin’s DSL.
The project files can be regenerated at any time, similarly to the process of compiling source code to class files.
In this chapter, you’ll model project files to enable your To Do application to be loaded in the IDEs Eclipse, IntelliJ IDEA, and Sublime Text.
Therefore, Gradle’s IDE plugins need to be able to translate the individual needs of your build into the IDE’s configuration data.
In this book, you added very specific configuration elements that stand out from a simple Gradle project:
You’ll learn how to describe these rules with the help of the DSL exposed by the Gradle plugins.
Eclipse (http://www.eclipse.org/) is probably the most popular and widely used IDE for Java projects.
It’s completely open source and can be extended by plugins to add specialized functionality, such as support for Groovy projects and the use of version control systems like Git.
You need a good understanding of the format of Eclipse project files before you start generating them because the plugin’s DSL elements target specific files.
This means that every project participating in a multiproject build contains its own set of Eclipse project files and directories.
It contains the name, description, references of other projects or resources, and the type of project.
The file within the directory stores settings like the Java compiler version and source code version compliance.
In the setting of your To Do application, the generated project files will look similar to figure 10.2
Each project in the hierarchy, root project, and subprojects has its own set of project files.
Next, you’ll apply the relevant Gradle plugins to your project and generate the project files.
The eclipse plugin is responsible for generating standard Eclipse configuration data.
The plugin named eclipse-wtp builds on top of the eclipse plugin and generates configuration files to be used with Eclipse’s Web Tools Platform (WTP)
The WTP plugin simplifies the creation of typical web artifacts like web descriptors, Servlets, and JSP files.
It also provides support for deploying WAR files to various web containers, which can come in handy if you need to debug your running application from your IDE.
At its core, your To Do application is a web application, which makes it a good candidate to apply both Gradle Eclipse plugins.
First, apply the eclipse plugin to all Gradle projects of your application:
Applying the plugin to the allprojects configuration block will create project files for the root project and all subprojects.
The only project that you need to generate WTP configuration files for is the web project.
You can apply the plugin as shown in the following code snippet:
With these two plugins in place, you’re ready to generate Eclipse project files by executing their provided tasks.
The task eclipse generates all Eclipse project files, including .project, .classpath, and the settings files under the directory .settings.
Try executing the task eclipse on the root level of your multiproject hierarchy:
Each of these dependent tasks is responsible for generating a specific type of project file.
For example, the task eclipseClasspath generates the content of the file .classpath.
Creates .project, .classpath, and JDT settings files for model project.
Creates .project, .classpath, and JDT settings files for repository project.
Creates .project, .classpath, and JDT settings files for web project.
We already discussed the purpose of the configuration files .project and .classpath.
Let’s take a closer look at one of the generated settings files.
This file stores configuration data specific to Java projects provided by Eclipse’s Java development tools (JDT)
One example of a JDT setting is the Java compiler version.
If you open the generated project files in Eclipse right now, what would you have achieved with the default Gradle plugin configuration? All of the projects would be recognized as Java or Groovy projects (in Eclipse this is described as project nature), the correct source paths would be set, and dependencies for default configurations defined by the Gradle Java plugin would be linked.
The prep work for using Eclipse WTP tooling would be done.
This is a lot of configuration that the Gradle plugins provide out of the box without any additional customization from your side.
In this case, these are functTestCompile and functTestRuntime, two configurations you defined for declaring functional test dependencies.
You’ll fix this by customizing the generation of the project files.
Table 10.1 shows the important key properties that provide access to the Eclipse generation model.
Each concern is nicely separated and can be configured individually by using the dedicated property.
In the following section, you’ll see each of these properties in action.
It’s likely that you won’t find your use case covered by the example.
If you feel that you need to dig deeper, the best place to start is the Gradle DSL guide.
Also keep in mind that it’s a good idea to delete existing project files whenever you change the configuration and want to regenerate the project files.
You can easily achieve this by executing the task cleanEclipse:
The Gradle Eclipse plugin derives many configuration values for Eclipse project files from the implicit or explicit build script configuration.
For example, the Eclipse project name is taken from the Gradle property project.name.
You’ll start by fine-tuning the project details of your root project.
If you just want to assign properties to the subprojects of your application, you can put the specific configuration into the subprojects configuration block.
The next listing shows how to set values for JDT properties and how to influence the classpath generation.
Listing 10.2 Setting JDT and classpath properties for all subprojects.
Indicates whether source code and Javadocs for external dependencies should be downloaded and linked to project sources.
Eclipse configuration to individual subprojects is applied on top of already existing configuration data.
All you need to do is add another eclipse configuration block in the build script of the subproject.
This method can also be used if you want to redefine an already set configuration value.
The following listing demonstrates how to set the project description, add the custom Gradle configurations, and individualize WTP configuration data.
These examples give you an idea of how easy it is to customize the generated configuration by the properties exposed through the Gradle plugin DSL.
The exposed properties cover many of the commonly used configuration options.
However, it’s impossible for the DSL to reflect configuration options for third-party Eclipse plugins or personalized user customizations.
For that purpose, the DSL provides hooks into the model generation that allow you to manipulate the resulting XML project files.
One is to directly reach into the XML domain object model (DOM) using the withXml hook.
The other is to register the merge hook beforeMerged or whenMerged, which allows for working directly with the domain object representing the Eclipse metadata model.
Let’s see the usage of these hooks by implementing a practical example.
When applying the Gradle eclipse-wtp plugin to your web project, you can generate project files for basic functionality in Eclipse WTP.
Unfortunately, the basic plugin settings don’t preconfigure support for editing JavaScript files (including syntax highlighting and code completion)
In Eclipse, a unit of functionality is called a facet.
One example of a facet is the JavaScript editing functionality.
To configure the JavaScript facet, you’ll need to specify the value wst.jsdt.web as the facet key.
The next listing shows how to append a node to the XML DOM representation that adds the JavaScript facet with version 1.0
Listing 10.3 Fine-tuning the Eclipse properties of the web subproject.
The same configuration can be achieved using the merge hooks.
When applying the hook in the context of the WTP facet element, the closure is provided with an instance of WtpComponent.
In the next listing, you’ll instantiate a new instance of type Facet, provide the data you need in the constructor, and add it to the list of facets.
Listing 10.5 Adding the WTP JavaScript facet using a merge hook.
Choosing the “right” type of hook You may ask yourself, which of these hooks should be used in what situation, or is there even a best practice? In short, there isn’t.
The option you choose for your project is often subjective.
Adds hook for manipulating WTP component file after Gradle populates its build information.
It’s time to see the results of your hard work.
The package contains many useful plugins you’ll want to use.
If you haven’t already done so, download the package from the Eclipse homepage and install it on your machine.
The standard Eclipse distribution doesn’t provide an easy way of importing hierarchical projects.
To import a single project, choose the menu item File > Import...
Press the Next button and browse to the root directory of the project you want to import.
If Eclipse can find a valid project file, it’ll display it in the list of projects ready to be imported.
After pressing the Finish button, you’ll find the project in the Package Explorer tab.
If you repeat this process for the root project and all subprojects, your Eclipse workspace should look similar to figure 10.3
Congratulations—if you’re an Eclipse user, you successfully set up your project for yourself and your peers in a reproducible manner.
Later, we’ll also touch on a more sophisticated Eclipse distribution called the SpringSource Tool Suite (STS) that provides out-of-the-box support for importing hierarchical Gradle projects.
In the next section, you’ll learn how to achieve the same for users of IntelliJ IDEA.
IntelliJ IDEA (http://www.jetbrains.com/idea/) is a commercial IDE for working with JVM-based projects.
It provides extensive support for popular frameworks and is backed by a suite of integrated developer tools.
While many of its features are preinstalled with the core product, it can be extended by plugins.
Let’s start with a quick tour of IntelliJ’s project files.
These domain objects usually describe the data they need to work correctly.
If you’re unsure about what data needs to be provided, you can easily answer this question by looking at the Javadocs of the Eclipse plugin.
If you need to add a configuration that isn’t backed by domain objects available in the Gradle API, you can always fall back to the XML hook.
The XML hook doesn’t run any validation on the data you provide, which makes it easy to provide unconventional configuration data.
To find out the structure of available Eclipse XML configuration elements, please refer to the Eclipse documentation.
We differentiate between project files with the following file extensions:
A module file holds configuration data about source code, build scripts, and relevant descriptors.
Each Gradle project in a multiproject build maps to one module file.
There’s only one settings file per single project or multiproject setting.
Your To Do application would be based on the project files shown in figure 10.4
Next, you’ll apply and execute the Gradle IDEA plugin to generate these files.
You can use this plugin to generate all required IntelliJ project files previously discussed.
To generate the project files for all projects of your To Do application, apply it in the allprojects configuration block of the root build script:
The main task provided by the plugin is called idea.
On execution, it generates all project files shown in figure 10.4
The command-line output of the task looks like the following:
You can already open the full IntelliJ project with the generated project files.
For now, hold off on that, because the IDEA plugin doesn’t recognize custom source sets and configuration.
Next, you’ll learn how to fine-tune the generated configuration data.
Table 10.2 shows the relevant DSL configuration blocks that give you access to the internal model.
Module, project, and workspace files are created for root project.
For subprojects, only the task for generating module file is executed.
We’ll explore each of the configuration elements in the context of your To Do application.
If at any time you want to set a property that isn’t mentioned in the examples, make sure to have the Gradle DSL guide handy.
After changing the configuration, it’s a good idea to fully replace the existing project files with the cleanIdea command:
The usage of the main IDE tasks is uniform among the Eclipse and IDEA plugins.
This makes it easy for a user to switch between IDEs or provide support for both.
The following listing demonstrates how to set project properties on the root level of a multiproject build.
If you want to customize the configuration for all subprojects participating in a multiproject build, use the subprojects configuration block.
Next, you’ll provide instructions to download the source files for all external dependencies in your Java projects.
The following listing shows how to make the sufficient configuration for all module files.
As mentioned earlier, the IDEA plugin doesn’t recognize custom source sets right off the bat.
You can easily change that by modifying the build scripts for the repository.
Start with the source set integrationTest defined by the subproject repository.
Thankfully, you don’t have to repeat the directory path again.
Gradle allows for accessing the path directly via its API.
The following listing demonstrates a perfect example of how Gradle’s Java plugin DSL can be used to retrieve the required information to apply the IDEA module DSL.
You’ll want to apply the same type of customization to the web module.
Listing 10.9 shows how to add the test source set functionalTest.
In addition to this customization, the listing also demonstrates how to add dependencies defined by your custom configurations to the IDE’s test compilation scope.
After creating this configuration setup, you can regenerate the IntelliJ project files and open the project without any problems in the IDE.
Before you do, let’s also look at the XML and merge hooks provided by the plugin.
While the IDEA plugin does a good job of deriving a default configuration from the build script, it certainly doesn’t pick up every desirable setting you need to get started right away as an end user.
Gradle comes with the JetGradle plugin preinstalled to support executing Gradle build scripts.
Listing 10.8 Adding the custom source set for the repository subproject.
Listing 10.9 Adding custom source set and configurations for the web subproject.
Adds each of the directories to list of test source directories recognized by IntelliJ.
The IDEA plugin doesn’t make this selection for you when generating the project metadata.
Thankfully, you can use the XML and merge hooks to generate this configuration.
Let’s see how to use the XML hook for this purpose.
In the next listing, you first gain access to the XML root node, then append a new component XML tag as its child node, and finally assign the required configuration to point to your root project’s build script.
Unfortunately, you can’t achieve the same customization with a merge hook.
In comparison to the domain model exposed by the Eclipse plugin, the API of the IDEA plugin comes with fewer domain model representations.
Your best bet is to implement the desired functionality with the XML hook, which provides you with maximum flexibility.
To give you an idea of the usefulness of the merge hooks, let’s look at an example.
Instead of using the properties exposed by idea.project, you can also use the plugin’s project domain models.
The following listing demonstrates how to set the JDK name and language level of your IntelliJ project using the whenMerged hook.
If you don’t buy a license key for it, you can still test it for 30 days.
After installing IntelliJ on the local machine, you’re ready to import the project hierarchy.
In IntelliJ, you can directly import the full multiproject hierarchy by pointing it to the root-level project file.
You should end up with a project setup similar to figure 10.5
Eclipse and IntelliJ IDEA are among the major players when it comes to feature-rich IDEs for JVM language development.
At the time of writing, there’s no published plugin that allows for generating project files with a customizable DSL approach.
However, some developers prefer a more minimalistic editing environment: plaintext editors or power editors.
They’re lightweight and can be extremely effective with the right key bindings.
In recent times, the power editor Sublime Text has become extremely popular.
In the next section, we’ll discuss how to generate project files for this editor by using a third-party plugin.
Sublime Text (http://www.sublimetext.com/) is a sophisticated text editor with support for a wide range of programming languages including Java, Groovy, and Scala.
The basic functionality of the tool can be extended by custom plugins.
It differentiates between two types of files, each of which stores configuration data as JSON:
In the context of your To Do application, a typical project setup would consists of one .sublime-project file per Gradle project.
After working with the project in the editor, Sublime Text will automatically create the workspace file.
A workspace file can exist for any of the projects, not just the root project.
Figure 10.6 illustrates the project files in the directory tree.
Similar to the Eclipse and IDEA Gradle plugins bundled with the Gradle distribution, you can generate Sublime Text project files with the help of a third-party plugin.
Next, you’ll explore its functionality by using it for your application.
As an add-on, it allows for preparing the project to execute Gradle tasks for Java projects from within the IDE.
The following listing shows the content of your root project build script that declares the plugin and adds it to the classpath.
Because you want to generate Sublime Text project files for all projects of your application, apply the plugin within the allprojects configuration block.
Listing 10.12 Applying the Sublime Text plugin to all projects of your build.
If you execute the task sublimeText from your root project, you should end up with the following command-line output:
With the default settings of the plugin, the generated project files contain mere pointers to the directory path.
You can open the project and edit the files, but you won’t be able to compile the code with Gradle.
Listing 10.13 shows how to exclude unwanted directories and set up the project’s source directories and classpath.
By default, the plugin doesn’t make the selection for you, even though these directories aren’t necessarily relevant for working with your project within Sublime Text.
The Gradle plugin doesn’t define a clean task to delete existing project files.
To regenerate the changed metadata, run the task sublimeText again.
Because you’re working with a multiproject build, you’ll want to see the full project hierarchy in the user interface.
Listing 10.13 Tweaking the configuration of Sublime Text project files.
Declares plugin dependency that makes plugin available to build script’s classpath.
The rendered To Do application project hierarchy should look similar to figure 10.7
At the time of writing, only an installed Gradle runtime can be used; your provided Gradle Wrapper is omitted.
This concludes our discussion of generating project files for prominent IDEs and text editors.
In the next section, you’ll take the inverse approach by letting the IDE analyze your build scripts to generate the project files.
A couple of years ago, Gradle was the newcomer among the top dogs Ant and Maven.
None of the IDE vendors provided any significant Gradle support.
To import a project built by Gradle, you had to generate the project files using the provided Eclipse and IDEA plugins.
If you used a different IDE (for example, NetBeans), you had no tooling support.
Figure 10.8 shows how the IDE acts as the middleman between the build script and Gradle runtime.
You may ask yourself which method of generating project files should be preferred: generating project files via the build, or letting the IDE analyze your build code.
My personal preference is to let the IDE do the heavy lifting.
If you need fine-grained control over your configuration, you can bring in the Gradle IDE plugin support.
You may have to be careful, though, not to directly override the existing configuration produced by the IDE.
In practice, it sometimes takes a bit of playing around to get the desired result.
The following features are particularly important to IDE end users:
Opening a project by analyzing the existing Gradle build script and automatically setting up the correct source directories and classpath.
As you’ll learn, the feature sets and their effectiveness differ slightly.
Let’s start by taking a closer look at SpringSource’s Tool Suite.
Earlier in this chapter, you learned how to generate project files for Eclipse.
With the help of the generated project metadata, you were able to import the project into the IDE.
So far, we haven’t discussed how to work with a Gradle project after importing it.
The standard distribution of Eclipse doesn’t come with any Gradle integration.
You’ll need to manually install the Groovy and Gradle plugins.
But why install these plugins manually, when you can do it the easy and convenient way using the Spring Tool Suite (STS) (http://www.springsource.org/sts)? STS is an Eclipse-based development environment mainly targeted for building applications using the Spring framework.
The IDE also provides excellent support for importing and managing applications powered by Gradle.
Alternatively, you can modify your existing Eclipse instance by stacking STS’s features on top of it.
You’ll find the required installation instructions on SpringSource’s web page.
The dashboard contains sections for creating new projects, accessing tutorials and documentation, and installing extensions.
The core idea of an extension is to give you a preconfigured Eclipse plugin for a particular language, framework, or tool.
We’re mainly interested in installing extensions for Groovy and Gradle.
If you open the IDE for the first time, the dashboard is rendered in the main panel.
At any time, you can also render the dashboard panel from the menu with Help > Dashboard.
Click on the Extensions panel, scroll to the section Language and Framework Tooling, and tick the checkboxes next to the extensions named Gradle Support and Groovy-Eclipse, as shown in figure 10.9
You can initiate the installation process by pressing the Install button on the bottom of the dashboard panel.
After successfully installing both extensions, the STS must be restarted.
After the IDE restarts, you can now use full Gradle support, like importing multiproject builds, managing dependencies, DSL code completion, and integrated task execution.
The wizard doesn’t require an application to contain existing project files.
During the import process, STS analyzes the Gradle build script, derives the configuration data from the internal model, and generates the project files for you.
You can bring up the wizard by choosing the menu.
Figure 10.9 Installing the Groovy and Gradle plugins from the dashboard.
In the filter text input field, type “gradle” to indicate that you want to import a Gradle project.
Pressing the Next button will present you with the dialog shown in figure 10.11
In this dialog, you can select the root directory of your To Do project and build the STS project hierarchy from it by pressing the Build Model button.
Select all Gradle projects, as well as the relevant import options for enabling dependency management, enabling DSL support, and running Gradle Eclipse plugin tasks.
Press the Finish button, and the projects are imported and will show up in the workspace as a flat folder hierarchy.
Figure 10.12 shows some of the features you have at hand.
On the left side, you can see the Package Explorer.
If you open one of them, you can examine the source directories and external dependencies.
In the editor pane, you can modify your build script.
As you can see in the screenshot, you’re making good use of the DSL code completion feature (type in the first letters of a keyword and press CTRL + space)
Press the OK button and choose the task to be executed from the list of tasks for a project.
Starting with version 12.1, IntelliJ has a feature set for Gradle that’s comparable to STS’s feature set.
With the help of the preinstalled Gradle plugin, you can import a project by pointing to a Gradle build script, generate the IDE metadata, execute Gradle tasks directly from the IDE, and make use of code completion in the editor.
Let’s look at IntelliJ’s Gradle support by showing how to import your To Do application.
This is a big advantage over Eclipse, because the physical structure of your project is directly reflected in the IDE.
Pressing the OK button will bring you to the next dialog, shown in figure 10.14
In the dialog, you can choose the Gradle runtime and some other configuration parameters, like its home directory.
If the project comes with a Gradle wrapper, IntelliJ automatically recognizes it.
Not only should you use the wrapper on the command line, you also want to use it in the IDE to ensure a consistent runtime environment.
Move on to the next wizard dialog by pressing the Next button.
The dialog shown in figure 10.15 is all about the details for generating the project files.
On the left side, you can see the project structure that IntelliJ will generate for you.
On the right side of the dialog, you can change important settings of your project.
The initial values presented here are derived from the configuration of your build script.
If you decide to change the location of your project files or change the JDK, you can do it here.
Stick to the default settings and let IntelliJ generate the project files by pressing the Finish button.
You may be surprised by the format of the generated project files.
A closer look at the project structure reveals that IntelliJ created a new directory named .idea to store IDE metadata.
This is a different format for project files than the one we saw in section 10.1.1
Newer versions of IntelliJ favor the use of this directory-based format.
After importing the application, you can start working on the code.
In the project pane on the left, you can find the application hierarchy.
To indicate modules, IntelliJ marks every Gradle subproject with a particular icon (folder with a blue square)
Files with the extension .gradle are recognized as Gradle build scripts, as indicated by the Gradle icon.
In the screenshot, you can see the code completion feature for Gradle’s DSL.
If IntelliJ can identify a DSL keyword while typing text, the context menu will pop up and propose available configuration elements.
Alternatively, you can activate the code completion feature with the keyboard shortcut CTRL + space.
You can open the JetGradle functionality from the tab panel on the right side of the editor.
One of its features is the ability to analyze the project structure, display its dependencies, download them, and execute available tasks directly from the IDE.
At the time of writing, the plugin isn’t smart enough to automatically import newly defined dependencies whenever a change is made to the build script.
It supports implementing Java and Groovy applications out of the box, along with the typical functionality you can expect of a first-class IDE, like refactoring.
Managing Gradle projects in popular IDEs capabilities, code completion, and integration with popular frameworks and tools.
Download the distribution for your OS (at the time of writing, this is version 7.3) and run the installer.
After a couple of minutes, the IDE is set up and you’re ready to take care of adding Gradle support.
You can place the plugin file anywhere in your file system.
Select your downloaded Gradle plugin file, as shown in figure 10.17
After pressing the Open button, you’re presented with details about the plugin.
In figure 10.18, you can see the version you’re about to install, the source of the plugin, and a description of its functionality.
Make sure the checkbox next to the plugin name is checked and press the Install button to initiate the installation process.
After a successful install, NetBeans IDE needs to be restarted.
With the help of the menu item File > Open Project you can open a new dialog that allows you to select a Gradle project.
The displayed file browser conveniently displays the Gradle icon for every folder containing a Gradle build script.
Navigate to the To Do project and select the root folder, as shown in figure 10.19
After pressing the button Open Project, the project is imported.
For convenience, expand the Subprojects node and click each of the subproject names.
This will automatically add the subprojects to the top level of the Projects view, as shown in figure 10.20
Gradle tasks can be executed by clicking a project node, bringing up the context menu, and choosing a task from the list.
Every project node groups source and test packages, dependencies, and build scripts.
This is great if you prefer a logical grouping of important Gradle elements.
Embedding Gradle with the tooling API switch to the Files tab at any time.
In the screenshot, you can also see an opened Gradle build script in the editor.
Unfortunately, the Gradle plugin doesn’t come with any support for DSL code completion.
Next up, we’ll look at Gradle’s tooling API, the API that makes it possible for all of these IDEs to provide a smooth Gradle integration.
The main purpose of the tooling API is to embed Gradle into an application.
Without knowing it, you already used the tooling API, because it’s the main driver for Gradle support in IDEs.
The tooling API is good for achieving three main goals:
Executing the build with a specific version of Gradle (either via the wrapper or an installed Gradle runtime)
Querying the build for runtime information and its internal model (for example, tasks, dependencies, and the command-line output)
One example is automatically downloading external dependencies in an IDE whenever the declaration is changed.
Many developers aren’t working for tool vendors—we usually work on Enterprise or open source applications that use Gradle as the build system.
Why would we care about the tooling API? The tooling API is very versatile and can be applied to other use cases.
Let’s explore one of them to see the API in action.
In the following section, you’ll use the tooling API to implement integration testing by actively executing a build script.
To verify its correct behavior, you can query for the runtime information available through the tooling API.
When you integration-test a build script with the tooling API, you don’t have to use the ProjectBuilder class to emulate the creation of.
The build script is tested from the end-user perspective and behaves as if you ran the gradle command from the console.
In chapter 8, you developed various Gradle tasks to interact with the CloudBees backend.
The following listing shows the task that retrieves information about a deployed application on RUN@cloud and prints the details on the command line.
Let’s see how the tooling API can help you to verify its correct behavior by using the build script.
Put the CloudBees task into a regular Gradle build script in a directory named script-under-test.
It holds a Gradle build script that sets up Spock as its testing framework and pulls in the tooling API dependencies.
Build script containing task for retrieving application information from CloudBees.
Let’s take a quick peek at the integration test build script shown in listing 10.15
Because you’re using Spock to write your tests, you’ll need to apply the Groovy plugin.
All dependencies use the testCompile configuration because you’re only planning to write tests.
It defines a single test method that uses the Gradle tooling API to execute the CloudBees task named cloudBeesAppInfo.
Not only do you want to execute the task, you also want to make sure it behaves as expected.
The task only produces command-line output, so you’ll need to parse the standard output stream to see if the correct application information is printed.
This functionality is testable in a unit test because a Gradle task doesn’t allow for setting a mock object for the logger instance.
Let’s also analyze how you use the Gradle tooling API within the test class CloudBeesSpec.
Configure the GradleConnector instance by setting the expected Gradle version, installation location, and the project directory containing the build script to invoke.
Connect to the target build script by invoking the connect() method.
There are many more options to explore, but this example should give you an idea of how powerful the tooling API is.
You can find more information on usage patterns and configuration options in the user guide at http://www.gradle.org/docs/current/userguide/embedding.html.
While this feature makes the build portable, many developers are most productive in a visual editor or IDE.
Having to switch between a command-line tool and editor can slow down the development process.
With the help of plugins, you can generate project files for various IDEs and editors.
The generated project files contain the metadata needed to conveniently open a project in an IDE with the correct setup.
We discussed how to use these plugins in the context of your To Do application to create project files for Eclipse, IntelliJ IDEA, and Sublime Text.
This metadata is mainly derived from the default build script configuration but can be customized to fit individual needs.
Popular Java IDE vendors realized the need for first-class Gradle support.
The IDEs SpringSource STS, IntelliJ IDEA, and NetBeans provide sufficient tooling for opening a Gradle project by pointing them to a build script.
Once the project is opened in the tool, a developer is able to manage dependencies, use Gradle DSL code completion, and execute tasks from within the IDE.
The tooling API forms the foundation for integrating Gradle with many of the IDEs we discussed.
It allows for executing Gradle tasks, while at the same time monitoring and querying the running build.
You learned that the tooling API can be used for implementing integration tests for a Gradle build script under real-world conditions.
For web developers, the world can no longer do without a potpourri of JavaScript, corresponding frameworks, libraries, and CSS.
Backend code or desktop applications are no longer built with Java alone.
Other JVM-based languages like Groovy or Scala are added to the mix.
It’s all about picking the right tool for the job.
In this chapter, we’ll discuss how Gradle faces the challenge of organizing and building polyglot projects by using your To Do application as an example.
As Java developers, we’re spoiled by extensive tooling and build automation support.
Concepts like build-time dependency management of external libraries and convention over configuration for project layouts have been around for years Building polyglot projects.
Managing JavaScript with Gradle and are part of our day-to-day vocabulary.
In JavaScript land, commonly accepted patterns and techniques for build automation are in their early stages of development.
Gradle doesn’t provide first-class support for JavaScript and CSS, but we’ll discuss how to use its API to implement typical usage scenarios, like combining and minifying JavaScript files.
In past chapters, we mainly concentrated on building Java projects.
Software stacks that incorporate a wide range of languages are a reality for many organizations.
We’ll drill into the details of using the core language plugins for building Java, Groovy, and Scala code.
Along the way, we’ll touch on topics like compiler daemons and joint compilation.
In addition, many enterprises have to integrate with software written in non-JVM languages like C or C++
This chapter will give an overview of how to incorporate heterogeneous software infrastructure into a build.
Let’s start by looking at build automation for modern web architectures.
A rich user experience has become the key to success for any application.
Today, users demand the same look and feel for web applications as they do for native applications.
Long gone are the days when customers were happy with static HTML pages and synchronous server roundtrips.
JavaScript has become the most popular programming language for the web that can meet this demand.
Developers no longer have to depend on plain JavaScript to implement their features.
Many application development frameworks and libraries (like jQuery, Prototype, and Backbone, to name just a few) are available to simplify the use of JavaScript development and to smooth the rough edges this language introduces.
Platforms like Node.js (http://nodejs.org/) use JavaScript for implementing server-side applications as well.
As a result, JavaScript is ubiquitous and grows in relevance.
When dealing with JavaScript, various tasks come to mind that may be part of your workflow.
In fact, some of them should sound familiar, because we discussed them in the context of your Java web application:
Minification: JavaScript is often included in an HTML page as an external file.
This means it has to be downloaded to the browser before it can be used to render the page.
The smaller the byte footprint of such a file, the faster the page can be loaded.
Minification aims to decrease the size of an original JavaScript file by applying smart optimizations such as removing comments, spaces, and tabs.
Merging : The size of a JavaScript file isn’t the only factor playing a role in the loading performance of a page.
The more external JavaScript files requested, the longer the user has to wait until the page is rendered.
Combining multiple JavaScript files into one can solve this problem.
Testing : Similar to writing test code for server-side code, you also want to be able to verify the correct behavior of your JavaScript implementation.
Testing frameworks like Jasmine (http://pivotal.github.io/jasmine/) let you write test code in JavaScript.
Code analysis : JavaScript source code can be analyzed to find potential bugs and problems, structural issues, and abnormal style conventions.
Transpiling : Some client-side programming languages like CoffeeScript or Dart provide their own syntax and language constructs.
Unfortunately, these kinds of languages do not run in a browser.
Transpiling, a specific kind of compilation, translates this source code into JavaScript to make it executable in the target environment.
If you try to incorporate all of these tasks in your To Do project without proper automation, there are many manual and repetitive steps that need to be performed before you can assemble the WAR file.
For the same reasons laid out in chapter 1, you’ll want to avoid this situation under all circumstances.
In this chapter, we won’t cover solutions to all tasks presented, but you’ll learn how to generally automate them with Gradle.
Let’s assume you start to modernize your To Do application.
Instead of submitting the data to the server for each operation and rerendering the page, you change the functionality to exchange data via asynchronous JavaScript calls (AJAX) in the background.
If the HTML user interface needs to be updated, you directly modify the document object model (DOM)
As a result, the To Do application will behave similar to a desktop application with no page reload required.
To simplify the use of JavaScript, your application will use an external JavaScript library.
Out of the many available libraries, pick JQuery (http://jquery.com/), an established, feature-rich, easy-to-use API for handling AJAX calls and manipulating the HTML DOM.
With the help of JQuery, you’ll implement updating the name of an existing To Do item in your list.
Double-clicking the item’s name will bring the item name into edit mode by offering an input field to change its value.
Pressing the Enter key while in editing mode will send the modified item name to the server, update the value in the data store, and exit the edit mode.
These actions are implemented in the JavaScript files edit-action.js and update-action.js.
Managing JavaScript with Gradle in the book’s source code examples.
The following directory structure shows both JavaScript files, as well as the minified JQuery library in your web project:
Remember when we talked about dependency management for external Java libraries in chapter 5? You can apply the same concept for automatically resolving and downloading JavaScript files.
Historically, JavaScript developers didn’t bother with the concept of dependency management.
We used to throw in external libraries with our application sources.
As our code base grew, we had no way of knowing what JavaScript dependencies the project required and with what version.
If we were lucky, the filename of the JavaScript library would indicate that.
If you exclusively work with JavaScript, this problem can be tackled by using the node package manager (NPM)
Dependencies between JavaScript files can be modeled by using RequireJS (http://requirejs.org/)
But what if you want to use JavaScript dependency management from Gradle? No problem; you can just use the standard Gradle dependency declaration and resolution mechanism.
There are some challenges we face with this approach, though.
As JVM developers, we’re used to having access to all possible libraries we’d ever want to use.
We pointed our build script to Maven Central and that was that.
Alternatively, you can directly use the hosted versions of a specific library if the project provides them.
On top of an insufficient JavaScript hosting infrastructure, you can’t depend on a unified descriptor format for libraries, similar to the pom.xml file for Maven repositories.
This means that transitive JavaScript libraries, CSS, and even required images need to be declared explicitly.
Let’s look at how to retrieve the minified version of the library from JQuery’s download page.
First, you define a custom configuration, and then you declare the library with the specific version in the dependencies configuration block.
For downloading the library to your source code directory, create a new task of type Copy that uses your custom configuration.
The following listing demonstrates how to wire this task up with other important tasks of your project to ensure that you fetch the JavaScript before running the application in Jetty or assembling the WAR file.
You’ll run it from the root directory of your Gradle project:
You can now directly link to this library in your JSPs and HTML files.
After its initial download, the task fetchExternalJs automatically knows that it doesn’t have to run again:
Feel free to check out the working application by running gradle :web:jettyRun.
You may ask yourself why the downloaded JavaScript file was put directly in your project’s source tree.
Good point! Usually, you don’t want to mix versioned files with downloaded dependencies.
This was done to support in-place web application deployment with the Jetty plugin using the jettyRun task.
Unfortunately, the plugin doesn’t allow pointing to a secondary web application directory.
Therefore, the jQuery JavaScript file needs to sit in the target web application location.
For example, you could exclude the file from versioning (please consult your VCS documentation)
You laid the groundwork for implementing additional automation tasks for your JavaScript code.
In the next two sections, we’ll talk about how to build your own JavaScript automation with the help of standard Gradle tooling.
In particular, we’ll look at how to implement minification and how to generate code quality metrics.
There are two essential optimizations every large JavaScript project needs to embrace: merging multiple JavaScript files into one and minifying its content.
One of its distributions is a Java library that can be directly included in a Gradle build, as shown in the following listing.
Apart from the plain Java API classes, this library includes an Ant task wrapping the provided functionality.
It’s a matter of personal taste which of these approaches you use in your Gradle build—both of them work.
For further examples, you’ll use the Ant task because it conveniently wraps the API.
The following listing uses the project’s fileTree method to derive the list of your application’s JavaScript files as input.
The output file is defined as all-min.js and is created in the directory build/js.
Executing the task minifyJs will first compile your minification custom task under buildSrc, and then produce a single, optimized JavaScript file:
Being able to create an optimized JavaScript file is great, but now you have to change the references in your dynamic and static web page files.
Of course, this process should be fully automated and integrated in your development process.
In the next section, we’ll discuss one way to implement this.
That’s why many organizations choose to deploy optimized JavaScript files.
The downside is that single-line, optimized JavaScript files are hard to read and not very useful for debugging or even diagnosing an issue.
Therefore, you don’t want to package and run your application with only minified files.
During development or in testing environments, you still want to use plain old JavaScript files.
Obviously, at build time you need to be able to control whether your application JavaScript files need to be optimized and used.
For your project, we’ll discuss a new project property named jsOptimized.
If it’s provided on the command line, you indicate that optimizations should happen.
Here’s an example of how to use it: gradle :web:war –PjsOptimize.
Based on this optimization flag, you can configure your war task to trigger the minification, exclude the original JavaScript files, include the produced minified file, and change the JSP file to reference it.
The following listing shows how to make this happen for your To Do application.
Listing 11.5 Conditional packaging and use of optimized JavaScript files.
The trickiest part of this configuration is the replacement of the JavaScript import in the JSP file todo-list.jsp.
To make life easier, you created two new JSP files that reference either the original JavaScript files or the minified file.
At build time, you only need to replace the JSTL import statement if you decide to go with the optimized version.
Keep in mind that the original source file isn’t touched.
The file is changed on the fly when building up the WAR file.
This example covers one of the most common use cases for JavaScript developers.
Let’s also look at introspecting JavaScript source files for code quality purposes.
Detecting issues and potential problems before the code gets deployed to production should be of concern to every JavaScript developer.
JSHint is a popular tool written in JavaScript for detecting errors and potential issues in JavaScript code.
To run JSHint, you’ll need to be able to execute it from Gradle.
Rhino is an open source implementation of JavaScript written in Java.
With the help of this library, you can run JavaScript files and therefore use JSHint in your build.
In the next listing, you’ll retrieve Rhino from Maven Central and the JSHint JavaScript file from a hosted download URL.
Replace JSP import file with minified version for file todo-list.jsp.
You declared Rhino as a dependency—now let’s see it in action.
Instead of using an Ant task, invoke the main method by assigning it to an enhanced Gradle task of type JavaExec.
JSHint requires you to define a list of JavaScript files as arguments that it should examine.
Under the hood, a Java invocation of Rhino for your application looks as follows:
The following listing demonstrates how to build this argument list and write the enhanced task for invoking the Rhino main class.
Executing the task for your fully functional application doesn’t report any issues:
However, try emulating a bug in your JavaScript by removing a semicolon at the end of a line in the file edit-action.js.
You can see that JSHint rightfully complains about it and fails the build:
Puts together arguments for Rhino executable with JSHint, JavaScript being the first.
We saw that it’s really easy to incorporate third-party libraries to create sufficient tooling for JavaScript in your Gradle build.
This can be accomplished by reusing Ant tasks, by executing Java applications with the help of the task type JavaExec, or even by calling external shell scripts.
All it takes is a little bit of research and the eagerness to find the best tool for the job.
We all know that good developers are lazy—lazy because they hate doing monotonous and repetitive tasks.
There’s a community Gradle plugin that provides most of the functionality you’ve implemented and even more (for example, generating API documentation with JSDoc)
The plugin is a good starting point for every project that incorporates JavaScript and doesn’t require you to implement your own tasks.
In this section, we’ll discuss how to use the plugin and its exposed DSL to optimize your JavaScript—that is, by combining and minifying it.
First, pull in the plugin from Maven Central and apply it to your web project, as shown in the next listing.
The plugin describes JavaScript source directories with a dedicated name.
This named JavaScript source set can then be used as input for one of the tasks the plugin defines.
The following listing shows the JavaScript DSL in more detail.
This code should look similar to what you’ve built so far, but is more readable and easier to understand.
Listing 11.8 Adding the JavaScript plugin to the build script’s classpath.
The task usage is very similar to what you’ve experienced so far.
To run minification on your files, execute the task minifyJs.
This task first calls the combineJs task to merge your application JavaScript files before shrinking their size.
But what if your frontend team already implemented build automation that doesn’t use Gradle? Do you need to rewrite all their existing logic? Thankfully, you don’t.
You can directly invoke other build automation tools from Gradle.
If you’re deeply involved in the JavaScript community, you may know the build tool Grunt (http://gruntjs.com/)
Grunt is targeted toward implementing common tasks like minifying, merging, and testing of JavaScript code in an automated fashion.
With its plugin system and rapidly growing community, you’ll need to take into consideration existing JavaScript automation workflows.
To get started with Grunt, you first need to install the executable via NPM, the Node.js package manager.
Please check the Grunt documentation for detailed information on how to install it.
Based on your operating system, you’ll either end up with a batch script named grunt.cmd (Windows) or a shell script named grunt (*nix)
This executable is used to evaluate and run a Grunt build.
A Grunt build requires two mandatory files: the file Gruntfile.js defines the tasks and their configuration, and the file package.json defines the project metadata and required external dependencies (for example, plugins)
In this section, we’ll discuss how to call Grunt tasks from Gradle.
This is a valuable approach for two reasons: either you already have an automation process in place and.
What about CSS? Requirements for optimizing CSS are in many cases no different from JavaScript.
Similar tasks can be implemented to combine and minify CSS, except with other tools.
Uses JavaScript source set as input for combining JavaScript files.
Uses output from task combineJs as input for task minifyJs.
Figure 11.1 illustrates how executing the Grunt executable looks in practice.
If a user forgets about this step, the Grunt build will fail.
To install these dependencies, navigate to the directory that contains the file and run the NPM with the command npm install.
Of course, we, as automation specialists, want to make this happen automatically.
Before running a Grunt task from Gradle, run the Node install command.
The following listing demonstrates how to wrap this call with a Gradle task.
If you take a closer look at figure 11.1, you’ll notice a task of type Grunt.
All you need to provide to the enhanced task are the commands (the Grunt tasks) defined in the Grunt build file.
Let’s assume you have an existing Grunt build that applies the JSHint plugin.
To invoke this Grunt task, you create a new task of type Grunt and provide the Grunt task name as the command, as shown in the next listing.
The following command-line output shows the output from Grunt discovering an erroneous JavaScript file that’s missing a semicolon:
After this extensive discussion about JavaScript integration with Gradle, we’ll look at Gradle support for JVM languages other than Java.
The days when Java was the one and only language used for writing applications are over.
Today, when we think about Java, we also refer to it as a mature development platform: the Java Virtual Machine (JVM)
There’s a wide range of languages running on the JVM that are suitable for enterprise or server-side development.
Why would you even want to use a different language than Java or mix them within a single project? Again, it’s all about the right tool for the job.
You may prefer Java for its statically typed nature and library support to implement your business logic.
However, Java’s syntax isn’t a natural fit for producing DSLs.
This is where other, more suitable languages like Groovy come into play.
So far, we’ve discussed how to build Java applications, but there’s more to Gradle.
In this section, we’ll look at how to organize and build Groovy and Scala projects and how to bring them all under one umbrella within a single project.
Before we dive into the details, let’s take a step back and review some of the inner workings of the Java plugin, which to an extent builds the foundation for other language plugins.
In chapter 8, we talked about a guiding principle for separating concerns in plugins: capabilities versus conventions.
A plugin that provides a capability oftentimes introduces new concepts or tasks, and a plugin that provides conventions imposes opinionated defaults for these concepts.
What might sound very abstract becomes strikingly clear by dissecting the inner workings of the Java plugin.
Your project is automatically equipped with the concept of source sets and Java-specific configurations, and exposes tasks as well as properties.
This baseline Java support builds the foundation for every JVM language-based project.
Gradle also preconfigured your project with sensible default configurations, also called conventions—for example, production source code sits in the directory src/main/ java, and test source code in src/test/java.
These conventions aren’t set in stone and can be reconfigured, but they paint an initial opinioned view of your project.
Some of the tasks in a Java project are based on the preconfigured source sets.
The following list describes the opinionated bits of the Java plugin:
How does it do it? Capabilities are extracted into another plugin: the Java base plugin.
Internally, the Java plugin applies the Java base plugin and adds the conventions on top, as shown in figure 11.2
First of all, if your Java project doesn’t fit the default conventions introduced by the Java plugin, you can instead apply the Java base plugin and define your own conventions.
Second, the Java base plugin can be used as the basis for building other JVM language plugins because they require similar concepts.
This is a perfect example of code reuse and is applied to the core plugins for building Groovy and Scala projects.
Before we look at how these base capabilities are applied, let’s explore another useful optimization you can use when compiling Java sources.
Especially in large Java-based multiproject builds, this causes unnecessary overhead.
You can speed up the compilation process by reusing an already-forked instance of a Java compilation process.
In the following listing, you configure every task of type JavaCompile to run in fork mode, while at the same time using Gradle’s own Java compiler integration.
Figure 11.2 Java plugin automatically applies the Java base plugin.
When executing the multiproject build on the root level with gradle clean build, notice that the total time of your build is faster than without this modification.
Internally, Gradle reuses the compiler daemon process across tasks even if they belong to a different subproject.
After the build is finished, the compiler daemon process(es) are stopped.
Groovy works nicely in conjunction with its older brother Java.
It builds on top of Java, but has a more concise syntax and adds dynamic language features to the mix.
Introducing Groovy to a Java project is as simple as adding the Groovy JAR file to its classpath.
In this book, you already used Groovy to write project infrastructure code.
You used the Spock framework to write unit tests for your production application code.
We also looked at how Groovy can be used to implement custom Gradle tasks.
For conservative organizations, these are suitable use cases for incorporating Groovy, because the code isn’t deployed to a production environment.
There are many good reasons why you may also want to use Groovy instead of Java for implementing production code.
Groovy is a great match for writing DSLs with the help of its meta-programming features.
It uses Java to implement the core logic, but wraps it with Groovy to expose the powerful DSL you’re now accustomed to.
Groovy support in Gradle is provided by the Groovy plugin.
Let’s look at its features and see where the Java base plugin comes into play.
To get a quick overview on how they play together, look at figure 11.3
Central to the figure are the Groovy base plugin and the Groovy plugin.
The Groovy base plugin inherits all the capabilities of the Java base plugin and imposes its own conventions on top of it.
The Groovy plugin builds upon the Groovy base plugin and the Java plugin.
The additional benefit that the plugin adds to the equation is a task of type Groovydoc.
Executing this task generates the HTML API documentation for the Groovy source code.
Next, you’ll convert one of the projects in your To Do application from a Java project into a Groovy project.
We’ll examine the conversion process by the example of your model project.
Currently, this project only defines a single Java class file: ToDoItem.java.
By applying the Groovy plugin, you equipped your project with the capabilities to compile Groovy source code and the default source sets that define where the compiler looks for the source code files.
Remember that the Groovy plugin automatically applies the Java plugin as well.
This means that your project now contains a source set for Java sources and one for Groovy sources.
Figure 11.4 shows the default project layout with the applied Groovy plugin.
Figure 11.3 Groovy plugin inherits capabilities from the Java base plugin.
Keep in mind that this doesn’t mean that you’re required to have Java source files in your project.
However, it enables you to mix Java and Groovy files in a single project.
It’s time to convert the bulky POJO class implementation into Groovy.
The next listing demonstrates to what extreme the code could be condensed.
Which Groovy distribution do I pick? You may be aware that there are multiple distributions of the Groovy library.
The file groovy-all contains all external dependencies (for example, Antlr and Apache Commons CLI) in a single JAR file archive to ensure compatibility between them.
The file with the name groovy doesn’t contain these dependencies.
You’re responsible for declaring these dependencies in your build script.
No matter which distribution you choose, both will enable your project to compile Groovy source code.
Groovy test sources directory Figure 11.4 Default source directories for projects applying the Groovy plugin.
You can see that the task compileJava is marked UP-TO-DATE because you don’t have any Java source files under src/main/ java.
The task compileGroovy finds the file ToDoItem.groovy and compiles it.
The underlying concept that enables you to reconfigure the default source directories is a SourceSet.
In the following listing, you’ll say that the Groovy production source files should sit under src/groovy, and Groovy test source files should reside under test/groovy.
To a developer, it feels natural to use Java classes from Groovy classes and vice versa.
In the end, shouldn’t both source file types be compiled to bytecode? There’s a catch.
Groovy can depend on Java, but Java can’t depend on Groovy.
To demonstrate this behavior, you’ll apply the Groovy plugin to the repository project.
You’ll also turn the class ToDoRepository from a Java interface into a Groovy interface, as shown in the next listing.
Now, if you compile the repository project, you’ll end up with a compilation error:
Does this mean you can never depend on Groovy classes from Java classes? No.
The key to making this work is called joint compilation, which allows you to freely mix Java and Groovy source code with bidirectional dependencies on each other.
One way to address this issue is to put the Java source code together with the Groovy source code under the directory src/main/groovy.
Alternatively, you can configure the Groovy compiler to enable joint compilation.
The following listing shows what needs to be configured in your build to use this feature.
After you add this code snippet to the build.gradle file of the repository project, joint compilation works as expected:
For the compiler nerds among us, let’s look at what happens under the hood:
The compiler parses the Groovy source files and generates stubs for them.
With the Java stubs in the Groovy source path, the Groovy compiler can compile both.
With the techniques presented in this chapter, you should be able to set up your own project to incorporate Groovy, either as a standalone Groovy project or as a mixed Java and Groovy project.
Scala is another language running on the JVM that has become increasingly popular in the past few years.
The language is statically typed, combines object-oriented programming with support for functional programming, and is designed to express logic in an elegant, concise way.
As with Groovy, Scala can use all Java libraries that a Java developer already knows.
Twitter reimplemented their backend services with Scala, and Foursquare moved on to Scala as well.
Scala support in Gradle is as sophisticated as the support for Groovy.
In this section, we’ll explore the corresponding Scala language plugins.
You’ll take an approach similar to what you did for Groovy to transform parts of your To Do application to Scala and build them with Gradle.
In figure 11.5, you can see that Gradle provides two Scala plugins.
The Scala base plugin automatically applies the Java base plugin.
You learned before that the Java plugin provides the capabilities for building Java-based projects.
The Scala base plugin uses these capabilities and preconfigures default conventions for Scala projects, as shown in the following list:
The Scala plugin internally applies not only the Scala base plugin, but also the Java plugin.
This allows you to build Java and Scala source code within a single project.
The feature the Scala plugin provides on top of the Scala base plugin is the ability to generate HTML API documentation for Scala source code.
If you’re building a fullfledged Scala project, applying the Scala plugin is your best choice.
The project in your build with the least complex source code is the model project.
At the moment, this project’s build.gradle file doesn’t define any logic.
The following listing applies the Scala plugin and declares the Scala library to get a hold on the Scala compiler.
Figure 11.5 Scala plugin inherits capabilities from the Java base plugin.
After applying the Scala plugin, your project can compile any Scala source code found in the directories src/main/scala and src/test/scala.
Figure 11.6 shows the groovy source set directories alongside the source directory conventions introduced by the Java plugin.
Next, you’ll transform the existing Java file ToDoItem.java into a Scala file.
Go ahead and rename the file and move it under the directory src/main/scala.
It’s time to change the source code to Scala, as shown in the next listing.
Scala test sources directory Figure 11.6 Default source directories for projects applying the Scala plugin.
Even though you may not be well versed in Scala, the language syntax is relatively easy to decipher.
In fact, the class doesn’t introduce any additional logic and works exactly like its Java counterpart.
Executing the build for the model project produces the following output:
We can see that the compileScala task initiates the download of the Scala library to be able to invoke the Scala compiler.
As a result, your project compiles the Scala code, which can be used as a dependency for other projects in your build.
By now, you’ve used this concept various times, so the code in the next listing should come as no surprise.
In this example, you’ll point the production source directory to src/scala and the test source directory to test/scala.
What it boils down to is that Scala can depend on Java, but Java can’t depend on Scala.
Bidirectional dependencies between these source code file types can only be achieved by joint compilation.
You can either put the Java code into the Scala source directory (by default, src/main/scala) or reconfigure the source directory sets, as shown in the following listing.
What happens when you configure Scala joint compilation? Joint compilation for Scala code works slightly differently than for Groovy code.
The Scala compiler parses and analyzes the Scala source code to figure out the dependencies on Java classes.
The Scala compiler understands Java syntax, but doesn’t invoke the Java compiler.
The compiler also produces a class file for each Java source file.
They’re used for type checking between Java and Scala sources.
We covered the most basic skills required to build Scala projects.
Optimizations like incremental compilation can reduce the compilation time even more.
In the next section, we’ll go over the Gradle compilation and packaging support for some other languages, JVM-based and native.
There are far more languages out there than we could possibly discuss in this book.
Some of these languages are directly supported by a core plugin; others can be integrated with your build through a plugin developed by the Gradle community.
Table 11.1 lists some of the more popular language plugins.
Even if you don’t find an appropriate plugin for the language you’re trying to incorporate into your build, you’re not out of luck.
You learned that Ant tasks and Java APIs can be wrapped with a Gradle task.
The same technique is used by the Groovy plugin, for example, which internally invokes the groovyc Ant task.
Alternatively, you can always execute a compiler from the command line by creating an enhanced task of type Exec.
Today’s software projects embrace a wide range of programming languages, technologies, and libraries.
Whatever works best, increases productivity, or solves a problem in a more elegant way should be preferred.
This maxim became increasingly important for projects that incorporate more than a single programming language, so-called polyglot projects.
In this chapter, we discussed how to configure, manage, and build three different languages with Gradle: JavaScript and the two JVM-based languages, Groovy and Scala.
JavaScript has been the dominant language for creating dynamic web experiences for over a decade.
Obviously, page-loading times influenced by the size of the JavaScript files play a significant role for end users.
JavaScript files can be merged and minified to improve the page rendering performance.
You learned that Gradle can help automate otherwise manual steps to perform these actions and integrate them with the development lifecycle of your To Do application.
We also explored how to simplify the required configuration of your JavaScript build code by using the community JavaScript plugin.
Later, you wrapped existing Grunt build code with Gradle to provide a single, unified control unit for automating all components of your application.
The JVM-based languages Groovy and Scala are directly supported within a Gradle build.
Each language plugin builds on top of the Java base plugin to enforce a separation of capabilities from conventions.
You transformed some of the existing To Do application Java classes into Groovy and Scala equivalents to demonstrate the use of these plugins.
Summary them in case your project needs to adapt to a legacy project layout.
Groovy and Scala source code can coexist with Java source code in a single project.
Bidirectional dependencies between Java and Groovy or Scala require the use of joint compilation.
We discussed how to prepare the compiler for handling such a scenario.
In the last part of this chapter, we touched on other programming languages supported by Gradle.
The next chapter will focus on measuring the code quality of your project by integrating external tooling into the build process.
The end product of most commercial projects is a binary.
Unless your deliverable doesn’t ship with the source code or is the source code itself, users are usually not concerned about the quality of your code.
They’re happy as long as the software fulfills the functional requirements and has no defects.
So why would you as the software development and delivery team care? In a nutshell, high-quality code results in fewer bugs and influences nonfunctional requirements like maintainability, extensibility, and readability, which have a direct impact on the ROI for your business.
In this chapter, we’ll focus on tools that measure code quality and visualize the results to help you pinpoint problem areas in your code.
By the time you finish this chapter, you’ll know how to integrate code quality tools with your build.
Earlier, you learned how to write unit, integration, and functional tests to verify the correctness of your To Do application code.
Code coverage analysis (also called test coverage analysis) is the process of finding the areas in your code that are not exercised by test cases.
Empirical studies show that reasonable code coverage has an indirect impact on the quality of your code.
Coding standards define agreed-on source code conventions within a team or organization, and can range from simple code formatting aspects, such as the use of whitespaces and indentation, to programming best practices.
By following these guidelines, you’ll make the code base more readable for other team members, improve its maintainability, and prevent potential bugs.
But despite all these benefits, code analysis doesn’t replace a code review by an experienced peer; rather, it complements it.
It’s impossible for a single person to manually keep track of all of these metrics in a large and ever-changing software project.
Therefore, it’s essential to be able to easily identify problem areas from a 10,000-foot view and track the progress over time.
Code quality tools help you to automatically analyze your software and provide sufficient reporting.
In the Java space, you can choose from a wide range of open source and commercial solutions, such as Checkstyle, PMD, Cobertura, FindBugs, and Sonar.
Many of these tools are already available in the form of Gradle core or third-party plugins and can be seamlessly integrated into your build.
In this chapter, you’ll use many of these plugins to measure the code quality of your To Do application.
Where do we stand? So far you’ve learned how to compile your code and implement and execute various types of tests.
These tasks cover the first two phases of the commit stage.
If any of these tasks fail, the build will automatically fail.
Though the outcome of code compilation and testing gives you a basic idea of your project’s health status, it doesn’t provide you with any feedback about code quality characteristics such as maintainability and code coverage.
Code analysis tools help you produce metrics that make a statement about these characteristics.
In the context of a build pipeline, code analysis is performed after the first two phases of the commit stage, as shown in figure 12.1
Similar to running integration and functional tests, the process of performing code analysis can take a long time depending on the size of your code base and the number of exercised code quality tools.
For that reason, it’s helpful to create a dedicated set.
These tasks are usually provided by a plugin, so you won’t need to create them yourself.
In practice, you’ll want to run particular code quality tasks independently from others.
For example, during development you may want to know whether you’ve improved the code coverage of the class you’re currently refactoring without having to run other lengthy code quality processes.
Code quality tasks shouldn’t depend on each other, which makes them perfect candidates to be executed in parallel.
In the overall build lifecycle of a Java project, it’s helpful to make the verification task check depend on all the code quality tasks, as shown in figure 12.2
Bear in mind that the given task names in the figure are only representative names.
Projects may consist of multiple source sets for separating code with different concerns.
You learned how to do this in chapter 7, when you defined an additional source set solely for integration tests.
To get a clear, comprehensive picture of code quality, you’ll need to be able to selectively perform code analysis on individual source sets or even on all of them.
You’ll start by measuring the code coverage of your To Do application.
It uncovers execution paths in your code that aren’t exercised by tests.
A frequently discussed metric is the overall percentage of tests covering the production code.
While achieving 100% code coverage is an honorable goal, it rarely pays off, nor can it give you the ultimate confidence that the logic is correct or bug-free.
When I talk about code coverage metrics, what exactly do I mean? The following list should give you some basic coverage criteria:
Figure 12.1 Code analysis phase in the context of the deployment pipeline.
Figure 12.2 Code analysis tasks in relation to the standard Java plugin tasks.
Let’s look at available code coverage tools and their respective Gradle plugins.
The Java ecosystem created several tools, free as well as commercial, for producing code coverage metrics.
The feature set usually differs by performance to generate the metrics, the type of metric that can be produced, the coverage quality, and how these metrics are produced at runtime (called the instrumentation)
Many of these tools originated as Ant tasks, which makes them easily wrapped by Gradle tasks in your build.
A plugin abstraction is available for some of these tools.
At the time of writing, none of these plugins have made it into the Gradle core code base.
Table 12.1 gives an overview of some popular coverage tools and their support in Gradle.
With Clover, you have a good chance of getting support, because it’s a commercial tool that you pay for.
Among the free tools, JaCoCo is the most active, whereas Emma and Cobertura haven’t been updated in years.
Two other compelling reasons for choosing a tool are the produced metrics and their quality.
Usually, the coverage percentage doesn’t deviate very much among the tools (at a maximum by ~3–5%)
Table 12.1 Popular code coverage tools for Java and Groovy projects.
The job of instrumentation is to inject instructions that are used to detect whether a particular code line or block is hit during test execution.
You have a single class named ToDoItem that represents a To Do item:
For the ToDoItem class, you want to determine the test coverage.
For that purpose, you create a unit test class named ToDoItemTest.
The class defines a test method that verifies that the set priority of an item cannot be higher than the maximum priority:
If you generate code coverage for this class, you’ll see that all methods are covered, because you called all of them from your test class.
The only line of code that isn’t covered is priority++
This is because your test method assumed an initial priority of 3 before you tried to increase the priority of the To Do item.
To achieve 100% code coverage for the class ToDoItem, you’d have to write another test method that uses the initial priority.
As shown in table 12.1, this is achieved through source code, offline bytecode, or on-the-fly bytecode instrumentation.
What’s the difference between these methods? Source code instrumentation adds instructions to the source code before compiling it.
Priority of an item can only be increased if it isn’t already set to highest priority.
Measuring code coverage to trace which part of the code has been executed.
Offline bytecode instrumentation applies these instructions directly to the compiled bytecode.
On-the-fly instrumentation adds these same instructions to the bytecode, but does this when it’s loaded by the JVM’s class loader.
While all of these methods do their job and the produced metrics don’t exhibit a huge difference, why does it matter which we choose? In the context of a continuous delivery pipeline, where bundling the deliverable is done after executing the code analysis phase, you want to make sure that the source or bytecode isn’t modified after the compilation process to avoid unexpected behavior in the target environment.
In the next chapter, I’ll also demonstrate ways to avoid problems with other instrumentation methods.
In the following sections, you’ll learn how to use two plugins for generating code coverage: the JaCoCo and the Cobertura plugins.
JaCoCo (http://www.eclemma.org/jacoco/), short for Java Code Coverage, is an open source toolkit for measuring code coverage through on-the-fly bytecode instrumentation.
To achieve this, JaCoCo attaches a Java agent to the JVM class loader, which collects execution information and writes it to a file.
Separating different aspects of software functionality is a best practice.
To separate your main build logic from code coverage logic, you’ll create a new build script named jacoco.gradle in the directory gradle on the root level of your project hierarchy.
Later, you’ll add other Gradle build files to this directory.
After creating the script, your project’s directory tree should look like this:
This script will serve as a container for declaring and configuring the JaCoCo plugin for your project.
Remember when you added your own plugin to the classpath of the consuming build script in chapter 8? Here you do the same by assigning a repository and the plugin dependency.
Adds JaCoCo plugin to build script’s classpath by retrieving it from Maven Central.
You can now easily apply the script plugin jacoco.gradle to other projects.
In the context of your multiproject build, this is achieved by declaring a reference on it in the subprojects configuration block in the build script of the root project, as illustrated in the following listing.
In addition to applying the script plugin, you’ll also need to tell the JaCoCo plugin where to retrieve its transitive dependencies.
For now, you’ll settle on declaring Maven Central as the repository.
Your project is prepared for generating code coverage reports with JaCoCo.
When executing tasks of type Test, JaCoCo’s agent will collect runtime information based on the exercised test classes.
Keep in mind that you won’t see any additional tasks that would indicate that code coverage data is created.
Executing the full build will produce the execution data file with the extension exec in the directory build/jacoco of each subproject.
The following directory tree shows the produced execution data files for the subproject repository:
JaCoCo execution data is stored in a binary, non-human-readable file.
To be able to visualize the code coverage, you’ll need to generate an HTML report.
The JaCoCo plugin automatically registers a report task per test source set for each subproject.
Listing 12.2 Applying the JaCoCo script plugin to all subprojects.
Adds JaCoCo plugin to build script’s classpath by retrievin it from Maven Central.
Declares task name for integration tests to produce code coverage metrics for integration test source set.
Transitive dependencies needed by JaCoCo plugin are retrieved from Maven Central.
Figure 12.3 shows the HTML report for the test source set produced by the task jacocoTestReport.
Each method in the HTML report is clickable and brings you directly to a new page that marks the coverage line by line.
A line marked in green indicates that it was exercised by tests, and a line marked in red means that it lacks test coverage.
With the default setup of the plugin, you’ll have one report for unit test coverage and one for integration test coverage.
If you want to get a full picture of the project’s overall coverage across test source sets or even across all projects of a multiproject build, you can write a new task of type JaCoCoMerge.
To get an idea of how Cobertura’s report compares to the one produced by JaCoCo, you’ll configure Cobertura for your build.
Cobertura (http://cobertura.sourceforge.net/) is a Java code coverage tools that needs to instrument the bytecode after it’s compiled.
A report produced by Cobertura contains the percentage of line and branch coverage, as well as the cyclomatic complexity for each package.
You’ll prepare the same setup as you did for integrating the JaCoCo plugin.
Then, you’ll apply the script plugin to all subprojects of your build.
The following project structure shows the new script under the directory gradle:
The following listing shows the contents of the script plugin cobertura.gradle.
You’ll retrieve the plugin from Maven Central and apply it by type.
Similar to the JaCoCo script plugin, you can now apply the Cobertura script plugin to all subprojects of your To Do application.
The plugin adds one task for inserting instrumentation instructions into the compiled class files and another for generating the code coverage report.
These tasks are fully integrated into the build lifecycle of a Java project.
A build that executes the verification task check will perform the necessary work to generate the code coverage.
To do so, the plugin copies the class files from build/classes to build/cobertura, instruments the files, and serializes coverage information to a file named cobertura.ser.
The following directory tree shows the relevant files after executing the build:
Listing 12.4 Applying the Cobertura script plugin to all subprojects.
Adds Cobertura plugin to the build script’s classpath by retrieving it from Maven Central.
Transitive dependencies needed by Cobertura plugin are retrieved from Maven Central.
By default, the task produces an HTML report but can also be reconfigured to create code coverage formatted as XML.
Figure 12.4 shows a sample HTML report created for the unit tests of the repository project.
We’ll stop at this point in our discussion of code coverage tools.
Later in this chapter, you’ll reuse the reports you generated to track your code coverage quality over time with Sonar.
JaCoCo and Cobertura are the most widely used open source coverage tools today.
If you’re planning to use a different tool, please refer to the links provided in table 12.1
In the next section, we’ll explore various static code analysis tools.
Members of the software development team perform code reviews to identify architectural problems, security defects, and potential bugs.
While this kind of review is extremely helpful in mitigating the risk of technical debt, this process easily becomes expensive and unmanageable for large software projects.
How many of us have had to toil away at code reviews? Sure, they’re useful—you identify problems, security defects, and bugs—but it can easily get expensive and unwieldy.
A cheap and automated way of finding issues in code is static code analysis.
Static code analysis is the task of analyzing source code without actually executing the software to derive quality metrics.
Metrics that can be extracted range from potential bugs and adherence to coding standards, to unnecessary code complexity and bad coding practices.
Let’s look at a list of tools producing these metrics.
There are a lot of open source tools for identifying poor code quality.
In this section, we’ll concentrate on the ones that are directly supported by Gradle through a standard plugin.
Because they’re shipped with Gradle’s runtime distribution, it’s easy to integrate them into your build and make them part of the build pipeline.
Table 12.2 shows these code analysis tools, the metrics they produce, and the name of the Gradle plugins for applying them to the build.
This list of plugins and their feature sets may look overwhelming at first.
To be able to differentiate their metrics, you’ll apply and configure each of them to determine the code quality of your To Do application.
In the next couple of sections, you’ll work your way from the top to the bottom of the table, with the exception of CodeNarc.
Many of these Gradle plugins require a configuration file located in a directory config/<toolname> that defines the rules for the analysis.
To be optimally prepared, you’ll create the directory config on the root level of your multiproject build and define a property that can be used in all subprojects:
Checkstyle checkstyle Only XML Enforces coding standards; discovers poor design problems, duplicated code, and bug patterns.
FindBugs findbugs XML or HTML Discovers potential bugs, performance issues, and bad coding practices.
The dependencies of each tool aren’t included in the Gradle distribution.
The following listing shows the changes you’ll need to make to the root project’s build script.
While XML is a useful format for postprocessing the results from other tools like Sonar, it’s not convenient to read.
Unfortunately, not all of the plugins you’ll use provide such a report output.
However, you can easily produce them yourself via an XML-to-HTML transformation.
For that purpose, you’ll create a reusable custom task in the buildSrc directory of your project that uses the Ant XSLT task, as show in the next listing.
Listing 12.5 Preparing subprojects for the integration of static code analysis plugins.
Directory containing script plugins that apply and configure code quality plugins.
Extra property pointing to configuration directory (directory can have any arbitrary name)
Only run HTML generation if input file exists; if not, task will be skipped.
You’re prepared for applying the static code analysis tools to your build.
In enterprise projects, it’s helpful to introduce a coding standard to define how source code should be formatted, structured, and annotated to form a uniform picture.
As a by-product, you’ll receive more readable and maintainable source code.
The project started out as a tool for finding areas in your source code that don’t comply with your coding rules.
Over time, the feature set was expanded to also check for design problems, duplicate code, and common bug patterns.
In that script plugin, you’ll need to apply the standard Gradle Checkstyle plugin and configure the version of the tool you’d like to use and its runtime behavior.
With the basic Checkstyle definitions in place, you can apply the encapsulating script plugin to all subprojects of your multiproject build, as shown in the next listing.
Listing 12.7 Applying and configuring the Checkstyle plugin as script plugin.
Prevents Gradle from printing out every violation on command line.
The following command-line output shows the relevant bits for the repository subproject:
Of course, the default report directory can be reconfigured to point to a custom directory.
For more information on this and other configuration options, please see the DSL documentation of the Checkstyle plugin.
The Checkstyle plugin is one of the candidates that doesn’t currently provide the generation of HTML reports.
Listing 12.9 shows how to create report tasks of type XsltReport for each of your source sets.
By declaring a dependency on the default Checkstyle task, you can ensure that the HTML report task is executed after the initial XML Checkstyle report generation.
Listing 12.9 Generating a Checkstyle HTML report for all source sets.
Checkstyle task and violations notification for integration test source set.
Makes sure all source sets have been evaluated for a project.
For the transformation from XML to HTML, you used an XSL file provided with the Checkstyle distribution.
Executing the build with gradle build again will produce a report similar to figure 12.5
PMD’s distribution comes with a wide range of specialized rule sets—for.
Performing static code analysis example, JEE components, web frameworks like JSF, and mobile technologies such as Android.
You’ll take the same approach you took for setting up the Checkstyle plugin.
First, you’ll create a script plugin named pmd.gradle that applies the PMD plugin.
Then, you’ll configure it and apply it to all subprojects of your To Do application.
The following listing demonstrates how to use the default version and rule set of PMD.
We won’t explore exactly how to apply the script plugin to your subprojects, because it looks very similar to the way you integrated the Checkstyle plugin.
The PMD Gradle plugin generates an XML and HTML report out of the box, so you don’t need to do any extra work.
The following command-line output shows how PMD metrics are produced for all source sets of the project repository:
The rendered HTML report in build/reports/pmd should look similar to figure 12.6
Listing 12.10 Applying and configuring the PMD plugin as script plugin.
Sometimes you may just want to generate a single type of report.
This need arises especially if you want to verify your progress during development.
The PMD task can be configured to enable or disable particular types of report formats, as shown in the next listing.
The same method can be applied to other code quality tasks where applicable.
FindBugs (http://findbugs.sourceforge.net/) is a static code analysis tool for finding potential bugs and bad coding practices.
The bugs identified include problems like equals/hashCode implementations, redundant null checks, and even performance issues.
Unlike the other analyzers presented earlier, FindBugs operates on the Java bytecode, rather than the source code.
You’ll find that operating on bytecode makes the analysis slower than source code analysis.
For bigger projects, be prepared for it to take minutes.
Listing 12.12 shows that the FindBugs plugin is as easy to integrate into your build as the other code quality plugins.
For a good overview of the available configuration options, see the Gradle DSL guide.
Currently, the plugin only supports the generation of an XML or HTML report.
Executing the full build on all subprojects will produce the expected reports for all available source sets.
The following command-line output shows the representative tasks for the repository subproject:
Listing 12.12 Applying and configuring the FindBugs plugin as script plugin.
Disables the XML report generation, but enables HTML report generation.
Defines analysis effort level; the higher the precision, the more meticulous the analysis.
Figure 12.7 illustrates the report for the main source set.
The static code analysis tool JDepend (http://clarkware.com/software/JDepend.html) produces metrics that measure the design quality of your code.
It scans all packages of your Java code, counts the number of classes and interfaces, and determines their dependencies.
This information will help you identify hot spots of unwanted or strong coupling.
Listing 12.13 shows how to apply and configure the standard Gradle JDepend plugin.
You can choose a report that’s formatted either as XML or plain text.
The default value is XML and doesn’t require any additional configuration.
The listing also shows you how to easily switch between the report formats.
After the code is applied to the subprojects configuration block, an XML report is generated for all source sets of a project.
The following command-line output illustrates the executed JDepend tasks required to produce the reports:
Unfortunately, you can’t generate an HTML report out of the box, but you can use the custom XSTL task introduced in section 12.3.1 to produce the desired result.
You can find an example in the source code of the book.
After executing the HTML report generation, your build/reports/ jdepend directory will contain at least one file that renders similarly to figure 12.8
Another great feature of JDepend is the ability to visualize the dependencies between packages as a graph.
Part of the JDepend distribution is an XSL style sheet file that transforms an XML report into a Graphviz DOT file.
You’ll find a full-fledged example in the source code of the book.
You’ve seen how to generate code metrics for your project using different code analysis tools.
Each of these tools produces reports that need to be checked individually.
With every build, the existing report is potentially deleted and a new one is created, so you have no idea whether the code quality has improved or decayed over time.
What you need is a centralized tool that monitors, visualizes, and aggregates your metrics.
Sonar is an open source, web-based platform for managing and monitoring code quality metrics like coding rules compliance, unit tests and their coverage, and source.
Listing 12.13 Applying and configuring the JDepend plugin as script plugin.
Integrating with Sonar code documentation, as well as architectural aspects like maintainability and technical debt.
It integrates well with most of the tools we discussed earlier, including JaCoCo, Checkstyle, and PMD.
If the need arises to support an unconventional tool or language, Sonar can be extended by plugins.
Sonar uses a central database to collect code quality metrics over time.
In its default configuration, Sonar comes with an embedded H2 instance, which doesn’t require any additional setup.
While H2 is a great way to get started and explore Sonar’s functionality, it’s recommended to configure a more scalable solution for a production setting like MySQL or Oracle.
Gradle integrates well with Sonar through its Sonar Runner plugin.
The Sonar Runner is a launcher that analyzes your project and pushes the gathered metrics to the Sonar database via JDBC.
You can directly open your project’s dashboard and view aggregated quality metrics over time.
As the figure shows, Sonar can be fed metrics from the outside.
The rule set for these metrics is defined in quality profiles.
A quality profile, which is directly configured on Sonar, defines the code analysis tools you’d like to run on your source code and their acceptance criteria.
For example, you could say, “A class must have a documented API greater than 70%.”
This means that Sonar not only gives you the option to apply specific metric rule sets and thresholds per project, but also allows you to decide whether you want to use the analysis tools provided by Sonar to generate metrics.
Let’s take the default, preconfigured quality profile called Sonar way, for example.
It automatically analyzes your project through Checkstyle and PMD with 119 rules without having to configure the standard Gradle plugins.
Go to Sonar’s homepage, download the latest version (at the time of writing this is 3.5), and extract the ZIP file anywhere in your local file system.
Depending on your operating system, you can start up the Sonar web server through the provided startup scripts in the directory $SONARHOME/bin.
Let’s assume you want to start Sonar on a Mac OS X 64-bit machine.
From the command line, navigate to the dedicated bin directory and start up the server:
Reusing existing code analysis reports Earlier, you configured your build to produce metrics through Gradle’s static code analysis plugins.
You might ask yourself if you can configure the Sonar Runner plugin to reuse these reports.
At the time of writing, Sonar doesn’t provide a mechanism to publish these reports to Sonar’s database.
Instead, you’ll need to rely on the embedded Checkstyle, PMD, and FindBugs plugins that are configured by a quality profile.
The only exception to this rule is code coverage metrics.
You can find a discussion of this topic later in this chapter.
Starting up Sonar for the first time takes about 30 seconds.
In the upper-right corner of the screenshot, you’ll see a panel named Projects.
Because you didn’t publish any metrics for a project, the list doesn’t contain any data.
Next, you’ll change that by configuring your Gradle build to use the Sonar Runner plugin.
Listing 12.14 shows the relevant changes to your build from using Sonar Runner.
It applies the plugin to the root project of your build and configures basic properties like the project’s name and description, as well as the source code encoding for all subprojects.
Because this chapter will only discuss the Sonar Runner plugin, I’ll refer you to the Gradle documentation for more information: http://www.gradle.org/sonar_plugin.
This is all you need to get started with Sonar.
Analyze your project using the default quality profile and publish the reports to the Sonar database.
You can initiate this process by executing the task named sonarRunner provided by the plugin:
It gives you sufficient insight into which projects and directories have been analyzed and what code quality tools have been used.
If you refresh the Sonar dashboard after executing the task, you’ll find your project named todo, with some basic information like lines of code, a percentage of rules compliance, and the last date of analysis.
Clicking on the project name will bring you to a more detailed view of the project’s metrics, as shown in figure 12.11
The project dashboard gives you an informative, condensed summary of metrics like code complexity, rules violations, and code coverage.
You can drill into each of these metrics even further by clicking on them.
The plugin lets you set properties to change every aspect of your Sonar configuration.
One set of properties is dedicated to pointing to a different database to host your Sonar metrics.
I’ll leave it up to you to work with the plugin documentation to make this happen in your environment.
Another set of properties I want to discuss, though, is directly relevant to your project.
Integrating with Sonar source set, you’ll have to tell Sonar about it.
The following listing demonstrates how to add the source set directory to the appropriate Sonar property.
Code coverage is an extremely helpful metric to have in the Sonar dashboard.
If available, you can directly walk through each class and see a visual representation of coverage metrics.
Adds integration test source set to default analysis source sets.
Earlier in this chapter, we discussed how to generate code coverage reports with JaCoCo and Cobertura.
You can reuse these coverage reports by feeding them through the Sonar Runner plugin.
Sonar supports this functionality for coverage reports that comply with the JUnit XML format generated by the tools JaCoCo, Emma, Cobertura, and Clover.
All you need to do is to add some configuration to the Sonar Runner plugin configuration.
The only thing you need to do is tell Sonar Runner where to find the JaCoCo report files.
You’ll find that the command-line output of Sonar Runner will register the analyzed JaCoCo report files:
Now that the additional data has been sent to Sonar, you can refresh the project dashboard page and see the correct reporting on code coverage in the unit test coverage widget.
To get insight into the integration test coverage, you need to add the integration test widget to your dashboard.
Figure 12.13 shows both code coverage widgets side by side.
At the time of writing, Sonar doesn’t provide a dedicated widget for functional test coverage.
To work around this shortcoming, you could register your functional code coverage as integration test coverage.
To complete our discussion of reusing code coverage metrics, we’ll also look at the configuration for integrating Cobertura reports into Sonar.
Figure 12.13 Populated unit and integration test code coverage widgets in Sonar.
Sonar can only process XML-formatted report files, so you’ll need to reconfigure the Cobertura plugin.
Keep in mind that you’ll only need to provide this property if you wish to reuse reports produced by a tool other than JaCoCo.
In addition to this property, you’ll also need to point Sonar Runner to the Cobertura report files.
The following listing demonstrates how to reuse the Cobertura unit test report.
The task sonarRunner will give you concrete information about the parsed report files:
Integrating other third-party coverage reports produced by tools like Emma or Clover follows the same pattern.
The key is to look up the required property from the Sonar online documentation.
Sets Sonar property for declaring JaCoCo unit test report file.
Poor code quality and technical debt inevitably lead to developer productivity losses, missed deadlines, and a higher bug rate.
In addition to design and code reviews through peers, it’s important to introduce coding standards and start monitoring your project’s code quality with the help of static code analysis tools in the early stages of your product.
The Java ecosystem offers many open source tooling options to produce code quality metrics.
Gradle simplifies the integration of many of these tools into the build process by providing standard or third-party plugins.
Code coverage measures the percentage of code that’s exercised by tests and reveals obvious areas that aren’t covered by tests.
A high code coverage rate significantly improves your ability to refactor, maintain, and enhance your code base.
We looked at how to apply and configure two code coverage plugins: JaCoCo and Cobertura.
While both tools do their jobs in generating adequate coverage reports, JaCoCo shines through better flexibility, performance, and developer support.
Static code analysis tools help you enforce coding standards and uncover bad coding practices and potential bugs.
Gradle offers a wide range of standard plugins for you to pick and choose from.
We discussed how to apply, configure, and execute these plugins in a reusable way.
Tracking, evaluating, and improving code quality over time can be achieved with Sonar.
Sonar provides a set of static code analysis tools out of the box and defines quality rules and thresholds on a central platform.
Sonar should be the preferred solution if you have to manage more than one project and need a central place for aggregating quality metrics.
You saw that integrating Sonar into a build process required minimal effort.
In the next chapter, we’ll discuss how to install and configure a continuous integration server that automatically builds your project whenever a code change is pushed to the VCS.
While working on the code, other developers may commit changes to existing code or add new artifacts like resource files and dependencies.
The longer you wait to commit your source code into the shared code repository, the harder it’ll become to avoid merge conflicts and integration issues.
Continuous integration (CI) is a software development practice where source code is integrated frequently, optimally multiple times a day.
With each change, the source code is compiled and tested by an automated build, which leads to significantly less integration headaches and immediate feedback about the health status of your project.
If you’re working as a member of a software development team, you inevitably will have to interface with code written by your peers.
In this chapter, we’ll discuss the principles and architecture of continuous integration.
We’ll also explore the tooling that enables continuous integration, called CI servers or platforms.
After learning the basic mechanics of a CI server, you’ll put your knowledge into practice.
We’ll discuss how to install and use the popular open source CI server Jenkins to build your To Do application.
As with many development tools, CI server products have been moved to the cloud.
Breaking up a big, monolithic build job into smaller, executable steps leads to faster feedback time and increases flexibility.
A build pipeline orchestrates individual build steps by defining their order and the conditions under which they’re supposed to run.
Jenkins provides a wide range of helpful extensions to model such a pipeline.
This chapter will build the foundation for configuring the pipeline steps we’ve touched on so far in the book.
With each of the following chapters, you’ll add new steps to the pipeline until you reach deployment into production.
Let’s first get a basic understanding of how continuous integration ties into the development process.
Integrating source code committed to a central VCS by different developers should be a nonevent.
Continuous integration is the process of verifying these integrations by building the project in well-defined intervals (for example, every five minutes) or each time a commit is pushed to the VCS.
You’re perfectly set up with your Gradle build to make this determination.
With every commit, you can compile the code, run various types of tests, and even determine if the code quality for your project improved or degraded.
What exactly do you gain? Apart from the initial time investment of setting up and configuring a CI server, continuous integration provides many benefits:
Reduced risk: Code is built with every commit to the VCS.
This practice reduces the risk of discovering integration issues late in the project’s lifecycle; for example, every two to four weeks for a new release.
As a side effect, you can also be confident that your build process works because it’s constantly exercised.
While you can rule out general build tool runtime issues by using the Gradle Wrapper, you still have a dependency on the machine’s setup.
On a CI server, you can exercise the build independent of a particular machine setup or configuration.
Improved productivity: While developers run their builds many times a day, it’s reasonable for them to concentrate on executing tasks that are essential to their work: compiling the code and running selected tests.
Long-running tasks, like generating code quality reports, would reduce their productivity and are better off being run on a CI server.
Project visibility: Continuous integration will give you a good idea of the current health status of your project.
Many CI servers come with a web-based dashboard that renders successful and failed builds, aggregates metrics, and provides central reporting.
Despite all of these benefits, introducing continuous integration to a team or organization requires an attitude of transparency, and in extreme cases may even require a complete culture shift.
The health status of a project is always visible through a dashboard or notifications.
This means that a broken build won’t be a secret anymore.
To improve project quality, try to foster a culture of intolerance for defects.
You’ll see that it pays off in the long run.
With these benefits in mind, let’s see how continuous integration plays out in practice by playing through a typical scenario.
Three components are essential to a CI environment: a central VCS that all developers commit changes to, the CI server, and an executable build script.
Let’s go over a typical scenario of integrating code changes in a team of three developers:
Committing code : One or more developers commit a code change to the VCS within a certain timeframe.
Triggering the build: A CI server can be configured in two different modes to identify if there’s a code change in the VCS.
The server can either be scheduled to check the VCS for changes in predefined time intervals (pull mode), or it can be configured to listen for a callback from the VCS (push mode)
If a change is identified, the build is automatically initiated.
Alternatively, you schedule a predefined time interval for triggering a build.
Executing the build: Once a build is triggered, it executes a specific action.
An action could be anything from invoking a shell script, to executing a code snippet, to running a build script.
In our discussions, this will usually be a Gradle build.
Sending a notification : A CI server can be configured to send out notifications about the outcome of a build, whether it was successful or failed.
Notifications can include emails, IMs, IRC messages, SMS, and many more.
Depending on the configuration of your CI server, these steps are performed for a single code change or for multiple code changes at once.
The longer the scheduled intervals for a scheduled build, the more changes are usually picked up.
Over the past 10 years, many open source and commercial CI server products have sprung up.
Many of them are downloadable products that are installed and hosted within your company’s network.
Recently, there’s been a lot of hype about CI servers available in the cloud.
Cloud-based solutions relieve you from the burden of having to provision infrastructure and lower the barrier of entry.
They’re usually a good fit for your own open source project.
In this chapter, you’ll mainly use Jenkins to implement continuous integration for your To Do application because it has the biggest market share.
Before you can emulate a typical CI workflow on your local machine, you’ll have to set up your components.
Continuous integration is best demonstrated by seeing it in action.
All you need is a CI server installed on your local system, access to a central VCS repository, and a project you can build with Gradle.
This section assumes that you’ve already installed a Java version on your machine.
Its distribution can be downloaded and started in literally a minute.
For your convenience, I uploaded the sample To Do application to GitHub, an online hosting service for projects.
GitHub is backed by the free and open source VCS named Git.
Don’t be intimidated by this suite of tools if you haven’t used them yet.
You’ll install and configure each of them step by step.
You’ll start by signing up on GitHub if you don’t yet have an account.
That’s it; you don’t even need to confirm your account.
A successful signup will bring you to your account’s dashboard.
Feel free to explore the functionality or update your profile settings.
To establish a secure SSH connection between your computer and GitHub, you’ll need to generate SSH keys and add the public key to your GitHub account.
Because you’re not the owner of this repository, you won’t be able to commit changes to it.
The easiest way to get push permission on a repository is to fork it from your own account.
A fork is a local copy of the original repository that you can modify at will without harming the original repository.
To fork a repository, navigate to the sample repository URL and click the Fork button in the navigation bar shown in figure 13.3
After a few seconds, the project will be ready for use.
To interact with your remote GitHub repository you’ll need to install and configure the Git client.
You can download the client distribution from the Git homepage (http://git-scm.com/)
The page offers you installers for the most common operating systems.
After a successful installation, you should be able to execute Git on the command line.
You can verify the installed version with the following command:
Commits to a remote repository can be directly mapped to your GitHub account.
By setting your client’s username and email address, GitHub will automatically link the.
The following two commands show how to set both configuration values:
You’re all set; you’ve configured Git and the sample repository.
Next, you’ll install Jenkins and configure a build job to run the build for your To Do application.
Hudson started out as an open source project in 2004 at Sun Microsystems.
Over the years, it became one of the most popular CI servers with a huge market share.
When Oracle bought Sun in 2011, the community decided to fork the project on GitHub and call it Jenkins.
While Hudson still exists today, most projects switched to using Jenkins because it provides the best support for bug fixes and extensions.
Jenkins, which is entirely written in Java, is easy to install and upgrade and provides good scriptability and over 600 plugins.
On the Jenkins webpage, you can find native installation packages for Windows, Mac OS X, and various Linux distributions.
Alternatively, you can download the Jenkins WAR file and either drop it into your favorite Servlet container or directly start it using the Java command.
Download the WAR file and start up the embedded container with the Java command:
After it starts up successfully, open the browser and enter the URL http://localhost:8080/
For example, out of the box you can only configure a build job that pulls the source code from a project hosted on CVS or Subversion and invoke an Ant script.
If you want to build a project with Gradle hosted on a Git repository, you’ll need to install the relevant plugins.
To access the plugin manager, click Manage Jenkins on the main dashboard page.
You’ll end up on the Plugin Manager page, shown in figure 13.4
In the upper-right corner, you’ll find a search input box called Filter.
Enter the search criteria “git plugin” and tick the checkbox next to the plugin named Git Plugin, as shown in figure 13.5
After pressing the button Install Without Restart, the plugin is downloaded and installed.
Using this technique, you’ll also search for the Gradle plugin.
Enter “gradle plugin” into the search box, as shown in figure 13.6
You’ll see a screen similar to figure 13.7 that shows the downloaded and installed plugins.
After a few moments, Jenkins is restarted and the plugins are fully functional.
Jenkins defines the actual work steps or tasks in a build job.
A build job usually defines the origin of source code that you want to build, how it should be retrieved, and what action should be executed when the job is run.
For example, a build job can be as simple as compiling the source code and running the unit tests.
You’ll create a build job that does exactly that for your To Do application.
On the Jenkins main dashboard, click the link New Job.
This opens a screen that lets you enter the job name and select the type of project you want to build.
For the job name, enter “todo” and press the radio button Build a Free-style Software Project.
A free-style project allows you to control all aspects of a build job; for example, the VCS and build tool you want to use.
The build job is created and you’ll be presented with the job configuration page.
By configuring the repository, you ensure that Jenkins will know where to find the source code of your project when the job is executed.
If you scroll down a little in the configuration screen, you’ll find a section named Source Code Management.
You want to build your project stored in a Git repository.
Click the Git radio button and enter the repository URL, which is the SSH URL you’ll find in the forked repository of your GitHub account.
Now that you’ve told Jenkins where to retrieve the sources from, you’ll also want to define when to pull them.
In the next section, you’ll set up a build trigger.
It determines when a build should be executed or triggered.
Let’s say you want to poll your repository on GitHub in certain time intervals, such as every minute.
Not only does it create unnecessary load for your VCS and Jenkins server, it also delays the build after a change is pushed to the repository by the timeframe you defined in your cron expression (in your case this is one minute)
A better way is to configure your Jenkins job to listen for push notifications from the repository.
Every time a change is committed to the repository, the VCS will make a call to Jenkins to trigger a build.
Figure 13.10 Polling the repository for changes minute by minute.
You’ll find many examples online that describe the necessary setup for your VCS.
Next, we’ll define what a build means if it’s triggered.
Each task that should be executed is called a build step.
Build steps can be added in the configuration section Build.
The options you see in figure 13.11 are provided by the Gradle plugin you installed earlier.
Choose the radio button Use Gradle Wrapper and enter the tasks “clean test” into the Tasks input box.
This is one of the scenarios where the Gradle Wrapper really shines.
Your build provides the runtime and clearly expresses which version of Gradle should be used.
If you’re building the project on your developer machine, you’ll want to make good use of Gradle’s incremental build feature to save time and improve the performance of your build.
In a CI setting, the build should be run from a clean slate to make sure all tests are rerun and recorded appropriately.
That’s why you added “clean test” to the list of tasks.
Scroll down to the section Post-build Actions, click the dropdown box Add Post-build Action, and choose the option E-mail Notification.
The only thing you need to do to receive emails on a failed build is to enter your email address into the Recipients input box, as shown in figure 13.12
After adding this entire configuration, make sure you save the settings by pressing Save on the bottom of the screen.
After saving the build job, you can find it listed on Jenkins’ dashboard.
The gray ball on the left side of the job indicates that it hasn’t been built yet.
A successful build will turn it blue, and a failed build is indicated by a red ball.
You can either wait a minute until the job is triggered automatically or you can manually initiate the build by pressing the clock icon, which schedules the build.
You should see the ball turn blue and a sun icon will appear, which indicates the health status of your project.
The job also reports on the last duration of the build and displays a timestamp that tells you the last time the build was successfully run.
To get more information about the specifics of a build, you can click on the job name, which brings you to the project’s homepage.
You’ll find your first build at #1 in the build history.
Click on it to examine what happened under the hood when the job was executed.
One of the menu items on the left side is the console output.
The console output recorded the steps that were executed during the build.
First, the Git repository was checked out from the master branch.
After pulling down the source code, the Gradle build was initiated for the tasks you defined.
If you look closer, you can also see that the Gradle Wrapper and the dependencies were downloaded before the tasks were executed.
The console output is rendered in real time while a build is executing.
This feature provides invaluable information if you want to track down the root cause of a failed build.
Congratulations, you set up a CI job for your project! To trigger a subsequent build, you can either push a code change to your repository or manually initiate it on the project’s dashboard.
With minimal effort, you can configure your project to process the XML test results produced by testing frameworks like JUnit, TestNG, and Spock.
In turn, Jenkins generates a graphical test result trend over time and lets you drill into the details of successfully executed and failed tests.
Though limited in functionality, it can serve as an easy-to-set-up alternative to reporting provided by Sonar.
To create a clean separation between unit and integration test results, you reconfigured the project on GitHub to put the results for unit tests in the subdirectory unit and integration test results into the subdirectory integration.
After navigating back to the project configuration page, scroll down to the section Post-build Actions, click the dropdown box Add Post-build Action, and choose Publish JUnit Test Result Report.
For test results to be rendered on the project dashboard, you’ll need to execute the build at least once.
If you click on it, you can view statistical information on your executed test suite.
The test result trend determines the historical development over multiple data points.
After executing the job at least twice, a graph is rendered.
Successful tests are displayed in blue and failed tests in red.
Figure 13.17 shows the test result trend in the project’s dashboard.
Your unit test task is configured to produce code coverage metrics with JaCoCo.
Next, you’ll show the test coverage trend alongside the unit test results.
You already know how to install a plugin for Jenkins.
Go to the plugin manager page, search for jacoco plugin, and install the plugin.
Figure 13.18 shows how to configure the plugin to point to the correct exec file, as well as directories that hold the class and source files.
Another useful feature of the plugin is the ability to act as a quality gate.
Let’s assume you want to make sure that your unit tests have to cover at least 70% of all classes and methods.
In case your project’s code coverage is below the expected quality threshold, Jenkins will appropriately reflect that as poor health status.
In figure 13.19 you can see the code coverage trend below the test result trend after creating at least two data points.
You can directly drill into the coverage result by clicking on the graph or the menu item Coverage Trend on the left side of the project dashboard.
This concludes our discussion of setting up a basic build job with Jenkins.
You’ll now be able to use your knowledge to build your own projects.
Later in this chapter, you’ll expand your knowledge by chaining multiple build jobs to form a build pipeline.
Before you do that, we’ll discuss some cloud-based CI solutions.
First and foremost, you don’t need to provision your infrastructure and maintain the software.
Depending on the purpose of your build, the demand for hardware resources can be high.
Continuous integration in the cloud promises to provide a scalability solution when you need it.
Need more CPU power to satisfy the demand of multiple, concurrent compilation build jobs? Simply scale up by purchasing a plan that provides more hardware resources.
Many cloud-based CI servers directly integrate with your online repository account like GitHub.
Log into your account, select a project, and start building.
The following list gives an overview of some of the popular CI server solutions in the cloud with Gradle support:
The free version comes with limited server resources and plugin support.
The paid plan gives you full access to all standard Jenkins plugins.
DEV@cloud also allows you to limit the visibility of project reports and access to configuration options.
The service is backed by Jenkins with a limited feature set—for example, you can’t add more Jenkins plugins or repositories hosted outside of GitHub.
Build jobs are easy to set up and provide support for verifying pull requests before you merge them.
BuildHive is a good choice if you need basic compilation and testing support for open source projects.
The service provides its own homegrown CI product that lets you build projects hosted on GitHub.
Projects need to provide a configuration file checked in with the source code to indicate the language and the command you want to execute.
In the free version, you can only build public repositories.
Paid plans offer build support for private repositories as well.
While reporting is limited, drone.io allows you to automatically deploy your application to environments like Heroku or AppEngine.
Choosing a hosted CI server might sound like a no-brainer.
Continuous integration can consume a lot of hardware resources, especially if you have to build a whole suite of applications and want quick feedback.
If you’re playing with the idea of using a cloudbased CI solution, it’s a good idea to try out the free tier first and diligently evaluate the pros and cons.
You already learned how to use Jenkins to build tasks for your To Do application.
If you want to build a full pipeline that separates individual tasks into phases, you’ll need.
In the following section, you’ll learn how to achieve that with Jenkins.
While it may be convenient to run all possible tasks of your Gradle build in a single build job, it’s hard to find the root cause of a failed build.
It’s much easier to break up the build process into smaller steps with their own technical responsibility.
This leads to clear separation of concerns and faster, more specific feedback.
For example, if you create a step for exclusively executing integration tests and that step fails, you know two things.
On the one hand, you can be certain that the source code is compilable and the unit tests ran successfully.
On the other hand, the root cause for a failed integration test is either an unsuccessful test assertion or a misbehaving integration with other components of the system.
In this section, you’ll model the first steps of your build pipeline, as shown in figure 13.20
A build pipeline defines quality gates between each of the steps.
Only if the result of a build step fulfills the requirements of its quality gate will the pipeline then proceed to the next step.
What does this mean for your example? In case the suite of integration tests fails to run successfully, the pipeline won’t trigger the next build step that performs code analysis.
When modeling a build pipeline, you face certain challenges that call for adequate solutions.
Every build pipeline starts with a single initial build job.
During the job’s execution, the project’s source code is checked out or updated from the VCS repository.
Subsequent steps will work on the same revision of the code base to avoid pulling in additional, unwanted changes.
A unique build number or identifier is used to clearly identify a build.
This build number should be assigned by the first job of the pipeline and carried across all steps of the pipeline.
Produced artifacts (for example, JAR files, reports, and documentation) incorporate the build number to clearly identify their version.
If later steps require it (for example, for deployment), it should be reused and not rebuilt.
The build number is used to retrieve the artifact from a shared binary repository.
Figure 13.20 Modeling the first phases of a build pipeline.
At the time of writing, Jenkins doesn’t provide a standardized and easy-to-use solution to implement those needs.
The good news is that you can model a full-fledged build pipeline with the help of community plugins.
The next section will give you a highlevel overview of their features and use cases before you use them to configure your pipeline jobs.
Wading through the feature lists of more than 600 Jenkins plugins is no fun if you need particular functionality.
The following four plugins provide you with the most essential functionality to get started with a build pipeline.
All you need to do is add a new post-build action called Build Other Projects.
This action allows you to define the build job name that should automatically be triggered when the current build job completes.
The problem with this approach is that you can’t pass parameters from one job to another, a feature you need to clearly identify a build by an initial build number.
The Parameterized Trigger plugin extends the functionality of chaining build jobs with the ability to declare parameters for the triggered job.
After installing the plugin, you can add a new post-build action named Trigger Parameterized Build on Other Projects.
In the configuration section you can name the project to build, under what condition it should be triggered, and the parameters you want to pass along.
Keep in mind that you can also trigger multiple jobs by declaring a commaseparated list of job names.
As the value for this parameter, you can use the built-in Jenkins parameter BUILD_NUMBER.
BUILD_NUMBER is a unique number assigned to every Jenkins build job at runtime.
Figure 13.21 demonstrates how to define a build trigger on the build job running your integration tests from the job definition responsible for compilation/unit tests execution.
If you’re unsure about what parameters have been passed to a build job, you can install the Jenkins plugin Show Build Parameters Plugin.
It helps you verify parameters and their values by displaying them on the project page for a specific build.
We’ll expand on using the plugin’s functionality later when you model the full pipeline.
Subsequent build jobs should work on the same change set.
The Clone Workspace SCM plugin lets you reuse a project’s workspace in other jobs.
To achieve this, you’ll need to configure the initial build job to archive the checked-out change set, as shown in figure 13.23
In subsequent jobs, you can now select the new option Clone Workspace in the Source Code Management configuration section.
Figure 13.24 demonstrates how to reuse the workspace of the parent project todo-initial in one of the subsequent build jobs.
Figure 13.21 Passing a parameter from one build job to another when triggered.
Instead of checking out the source code again, you can now build on top of the already existing workspace.
This gives you access to previously created artifacts like compiled class files and project reports.
On the one hand, it offers a visualization of your whole pipeline in one single view.
On the other hand, it allows you to configure a downstream build job to only execute if the user initiates it manually.
We’ll explore this functionality in chapter 15 when discussing artifact deployments.
Creating a build pipeline view of the chained tasks is simple.
After installing the plugin, click the + tab in the main Jenkins dashboard to add a new view.
In the rendered page, select the radio button Build Pipeline View and enter an appropriate view name, as shown in figure 13.25
After pressing OK, you’re presented with just one more page.
Select the initial build job and you’re ready to create the build pipeline view.
Figure 13.26 shows an exemplary view produced by the plugin.
As shown in the figure, the pipeline consists of three build jobs.
The status of a build is indicated by the color.
Another option for creating a graphical representation of the pipeline is the Downstream Buildview plugin.
Starting from the initial project, it renders a hierarchical view of downstream projects.
The plugin you choose for your project is mostly a matter of taste.
In this book, we’ll stick to the Build Pipeline plugin.
After getting to know these plugins, you’re well equipped to build the first three steps of your pipeline.
Modeling a build pipeline for your To Do application doesn’t require any additional Gradle tasks.
With the help of Jenkins, you’ll orchestrate a sequence of build jobs that call off to your existing tasks.
The full pipeline consists of three build jobs in the following order:
Earlier in this chapter, you set up a build job for compiling your source code and running the unit tests.
With minor modifications, this job will serve as the initial step for your build pipeline.
To indicate that the job is the entry point for your pipeline, you’ll rename it todo-initial.
Declaring Jenkins build jobs can be a repetitive and tedious task.
To keep it short, I’ll stick to the most important points when explaining the configuration for each of the build steps.
To be able to use the same workspace in downstream projects, make sure to add the post-build action Archive for Clone Workspace SCM with the expression “**/*”
This is only the case if there were no compilation errors and all unit tests passed.
In the Source Code Management configuration section, choose the option Clone Workspace and then the parent project todo-initial.
As the build step, you want to trigger the execution of your integration tests.
You separated the test results between unit and integration tests by writing them to different directories.
Add a parameterized build action that defines a build trigger on the job running your static code analysis named todo-code-quality.
As far as parameters go, you’ll reuse the existing ones by choosing the option Current Build Parameters.
You’ll expand on your pipeline in the next two chapters by adding jobs for publishing the WAR file to a repository and deploying the artifact to different runtime environments.
Most of the configuration for this job looks similar to the previous job definition:
Quick setup of Jenkins jobs In its default configuration, Jenkins stores the definition of a build job in the directory ~/.jenkins/jobs on your local disk.
Don’t worry if you feel lost at any point of time when configuring your pipeline.
The source code of the book contains the job definition for each of the steps.
All you need to do is copy the job definitions to the jobs directory and restart the server.
As the build step, you want to trigger the execution of Sonar Runner to produce code quality metrics.
Add a build step for invoking your Gradle script using the wrapper and enter the task sonarRunner.
Perfect, you built your first build pipeline with Jenkins by setting up a chain of Jenkins jobs.
Make sure to configure at least one of the pipeline visualization plugins.
It’s exciting to see the job execution travel down the swim lane.
Continuous integration is a software development practice that delivers an instant payoff for your team and the project.
By automatically integrating shared source code multiple times a day, you make sure that defects are discovered at the time they’re introduced.
As a result, the risk of delivering low-quality software is reduced.
In this chapter, you experienced firsthand how easy it is to set up continuous integration for a project.
You installed the open source CI server Jenkins on your machine and created a build job for the To Do application.
In a first step, you learned how to periodically retrieve the source code from a GitHub repository and trigger a Gradle build.
You configured your build job to display the unit test results and code coverage metrics.
Hosting a Jenkins instance on a server requires hardware resources and qualified personnel to maintain it.
We explored popular, cloud-hosted CI solutions and compared their advantages and disadvantages.
While hosting a CI server in the cloud is convenient, it may become costly with an increasing number of build jobs and features.
A CI server is more than just a platform for compiling and testing your code.
You learned how to model such a build pipeline with Jenkins.
Though Jenkins doesn’t provide a standardized pipeline implementation out of the box, you can combine the features of various community plugins to implement a viable solution.
We discussed how to set up build jobs for the first three stages of a continuous delivery commit phase and tied them together.
In the next chapter, you’ll learn how to build the distribution for your project and how to publish it to private and public artifact repositories.
In later chapters, you’ll extend this pipeline by creating jobs for publishing the WAR file and deploying it to a target environment.
We’ve already seen examples of binary artifacts in this book, including JAR, WAR, and ZIP files.
Source code repositories provided by version control systems like Git or Subversion are designed to manage source code.
They provide features like storing the changes between two versions of a file, branching, tagging, and many more.
A source code file is usually small and can be handled well by a source code repository.
Larger files, typically binaries, can degrade the performance of your repository, slow down the developer’s check-out process, and consume a lot of network bandwidth.
Binary repositories like JFrog Artifactory and Sonatype Nexus are well suited for storing binary artifacts.
One of the most prominent binary repositives is Maven Artifact assembly and publishing.
As a developer, you mainly deal with two kinds of artifacts during the software.
They’re equipped to handle binary artifacts of large file sizes, provide a way to organize them, describe them with the help of metadata, and expose an interface (a user interface and/or API) to publish and download these artifacts.
In this chapter, we’ll look at how to define the artifacts your build is supposed to produce.
We’ll also discuss how to generate metadata for these artifacts and publish them to local and remote repositories.
The CloudBees plugin you wrote in chapter 8 is a perfect example to demonstrate this functionality.
By publishing the artifact of the plugin, you can make its capabilities available to other Gradle users within your organization or to anyone on the web interested in using the plugin.
In the context of continuous delivery, publishing your artifact plays a crucial role.
Once you package the delivery with a specific version, it’s ready to be deployed to various target environments for acceptance testing or to be given into the hands of the end user.
It’s a good practice to build the artifact only once, deploy it to a binary repository, and reuse it whenever needed.
We’ll discuss how to apply this concept to your To Do application as part of your build pipeline introduced in the last chapter.
Let’s start by bringing back your plugin code from chapter 8 and reviewing its assembly process.
By default, every project that applies the Java plugin generates a single JAR file when the lifecycle task assemble is executed.
In chapter 8, you made good use of this functionality when you created the plugin artifact for further distribution.
The artifact filename consists of a name, which is derived from the base name (usually the project name), and a version number if it was set via the version property.
The following directory tree shows the plugin artifact after generating it:
The file type of an archive might change based on the project type.
For example, if you apply the War plugin, the generated archive is turned into a web archive (WAR file) with all its specific packaging characteristics.
Gradle doesn’t impose any limitations on how many artifacts a project can produce.
If you’re coming from Maven, which gives you a hard time for wanting to create more than one artifact per project, you may find this feature a welcome change.
Chapter 4 presented an example of adding an enhanced task that packages a Zip file.
It may be helpful to have a quick peek at the example again to refresh your memory.
To produce the output of a custom archive task, you’ll need to execute it on the command line or add it as a task dependency to another task.
If you consider nonstandard artifacts part of your project’s delivery consumed by other users or projects, you’ll want to include them into the assembly process.
Gradle offers a convenient and declarative way to add artifacts.
Understanding how to declare additional project artifacts requires a bit of background information: every project that applies the Java plugin is equipped with the configuration archives.
You can check its existence by invoking the dependencies task, as shown in the following command-line output:
The archives configuration declares the outgoing artifacts of a project.
For Java projects, the artifact assigned to this configuration is the standard JAR file.
Whenever you execute the task assemble, all declared artifacts are built.
You’re going to enrich your plugin project by adding some more outgoing artifacts.
As the popularity of your plugin grows, you’ll want to give your users deeper insights into the inner workings of your code.
Plugin consumers are especially interested in learning about the exposed API.
What could better serve this purpose than to provide them with the source code and the Groovydocs of your code?
It’s common practice to deliver the source code and Groovydocs of a project in the form of JAR files.
For your plugin project, this means that you’ll have to create two new tasks of type Jar.
The source code JAR task needs to include the source files of all SourceSets.
The task groovydocJar creates a JAR file containing the API documentation of your Groovy classes.
To be able to include the project’s Groovydocs into a JAR file, you’ll need to generate them first.
This can easily be achieved by letting the jar task depend on the groovydoc task provided by the Java plugin.
Figure 14.1 shows the new archive tasks as part of the assemble task graph.
To clearly identify that the resulting JAR files belong to your plugin, it’s helpful to align the naming.
For that purpose, you’ll add the suffix (also called classifier) sources to the JAR file containing source code and the suffix groovydoc to the JAR file containing the Groovydocs.
When looking at some of the libraries on Maven Central, you’ll notice that this naming convention is a fairly common practice.
There are two ways to make the new archive tasks part of the assemble task graph.
You can go down the imperative route by adding the sourcesJar and groovydocJar as task dependencies.
This approach probably looks the most familiar and straightforward to battle-scarred Ant users:
This works fine and is a valid way to hook up the creation of all artifacts for a project.
You can express what outgoing artifacts a project produces without saying how they’re created.
On the other hand, projects participating in more complex builds can directly refer to the outgoing artifacts of another project with the help of Gradle’s API.
To declaratively make the new archive tasks part of the assemble task graph, you’ll need to register them with the project’s instance of ArtifactHandler.
Alternatively, you can also assign an instance of type java.io.File in case the artifact isn’t generated by an archive task.
After assigning your archive tasks to the archives configuration, executing the assemble task will automatically invoke the creation of your custom JAR files:
As expected, these files end up in the build/libs directory side by side with the standard JAR file of the project:
You learned how easy it is to declare tasks that produce additional project artifacts and how to register them with the assembly lifecycle process.
As part of your software delivery process, you may also want to create a single archive or even multiples archives containing a unique set of these artifacts.
This requirement usually arises if you need to assemble distributions targeted toward different operating systems, specific groups of end users, or diverse product flavors of your software.
For each archive you want to create, you can add an enhanced task of type Zip or Tar.
While this approach works great for a project with a small set of distributions, you’ll have to come up with a naming pattern for the tasks to clearly express their intent.
Gradle’s distribution plugin offers a more streamlined and declarative approach to solving this problem.
The plugin exposes an expressive language that lets you describe a number of distributions for a project without having to manually declare tasks.
The task creation is handled by the plugin under the hood.
The plugin lets you generate a distribution in the form of a ZIP or TAR file.
Let’s say you want to create a distribution for your CloudBees plugin project that bundles the plugin JAR file, the sources JAR file, and the Groovydocs JAR file into a new archive.
The following listing demonstrates how to apply the distribution plugin and specify the target directory content you want to bundle with the archive.
With this code in place, you can decide whether you want to create a ZIP or TAR file for the distribution.
The ZIP file can be generated by the task distZip, and the TAR file is built by the task distTar.
A TAR file is oftentimes a preferred format on UNIX operating systems.
For a crossplatform distribution, a ZIP file is usually the preferred format.
The following commandline output shows the creation of a ZIP distribution:
The distribution plugin is designed to support more than a single distribution.
For each additional distribution, you’ll need to add another named configuration block within the distributions closure.
On top of the standard distribution that bundles all JAR files from the directory build/libs, you want to create a distribution that solely contains documentation files.
As documentation files, you classify the source files JAR and the Groovydocs JAR.
You may have noticed that the configuration block of the distribution is named docs.
The plugin automatically derives the distribution task names for nonstandard distributions from the declared name.
In addition to the already existing tasks, you can now generate the documentation distribution by using the tasks docsDistZip and docsDistTar.
The distribution output directory now also contains the expected TAR file:
You saw how to declare and build distributions for your plugin project.
Though descriptive and powerful, the requirements in your project may call for more complex or platform-specific functionality; for example, creating a desktop installer or generating an RPM package.
If you feel like the distribution plugin doesn’t cut the mustard, make sure to look at third-party Gradle plugins.
Some of them may provide the functionality you’re looking for.
Let’s summarize what you’ve done so far: you enabled your project to produce the plugin JAR file and two documentation archives containing the project’s source code and Groovydocs.
Now you’re ready to share the plugin with the world.
Next, I’ll show how to publish these artifacts to a binary repository.
In chapter 5, we mainly talked about dependency management from a consumer’s perspective.
You learned how to declare dependencies and repositories within your build script.
Gradle’s dependency manager would in turn try to locate these dependencies, download and store them in a local cache, and make them available to your build.
In this chapter, you’ll take on the role of the artifact producer.
You’ll learn how to publish build artifacts to a local or remote repository.
An important step in the publishing process is to generate metadata for these artifacts.
This metadata, usually stored in an XML-based text file, can give sufficient information about the corresponding artifacts.
The following list should give you an idea of some common types of metadata:
General information about the artifact, like name, description, involved developers, and links to source code or documentation.
Gradle can help with producing customized metadata and uploading the artifacts of a build to different types of repositories.
Figure 14.2 demonstrates the interaction among a Gradle build, its produced artifacts, and some targeted binary repositories.
The most common repository formats you’ll find in the wild are based on either Maven or Ivy.
At the time of writing, Gradle doesn’t provide its own, specialized repository format.
For our examples, you’ll see how to publish to the most widely used repository format: Maven.
Remember earlier in this chapter when you modified your plugin build to produce three different artifacts? You ended up with these files in your build/libs directory:
It’s time to publish them to a Maven repository for later consumption.
We’ll look at how to publish them to three types of Maven repositories, as shown in figure 14.3:
Projects in an enterprise setting shouldn’t rely on Maven Central as their primary source of dependency management.
To be able to produce reliable and reproducible builds, an internal repository should be set up for catering to your projects.
We won’t discuss how to set up such a repository.
Please refer to the product’s installation manual for more information.
For now, let’s assume you already set up Artifactory as your manager for dependencies running on the URL http://localhost:8081/artifactory.
Publishing artifacts is an important step in the lifecycle of a software component.
Gradle came with support for publishing artifacts to a repository early on with the Upload task included as part of the Maven plugin.
While the Maven plugin is used by early Gradle adopters and works well in production environments, it became apparent that a more descriptive and elegant DSL is needed to describe publications of a project.
Starting with version 1.3 of Gradle, a new publishing mechanism was introduced by the plugins maven-publish and ivy-publish.
Even though it’s still an incubating feature, it coexists with the existing methods for publishing artifacts, but will supersede them in an upcoming version of Gradle.
Given this outlook, most of our discussions will be based on the new publishing mechanism, because it’ll make your build script future-proof.
In the following sections, you’ll use the maven-publish plugin for publishing the artifacts produced by your CloudBees plugin project to a Maven repository.
A similar approach can be taken with the ivy-publish plugin if you prefer to use the Ivy repository format.
The DSL exposed by both plugins looks fairly similar, so with minor modifications you can make it target Ivy as well.
Let’s jump right in and see how to publish the plugin JAR to a Maven repository.
Gradle projects that apply a specific plugin are preconfigured to produce a primary, outgoing artifact whenever the assemble task is executed.
You already got to know various examples of this behavior.
A project that applies the Java or Groovy plugin creates a JAR file, and a project that applies the War plugin packages a WAR file.
In the context of the publishing plugin, this artifact is called a software component.
The following listing applies the Maven publishing plugin to your CloudBees project and uses its DSL to declare a single publication: a Java software component.
When I speak of a Java software component, I mean the generated JAR file.
Declares publication name of type MavenPublication with the name plugin.
Adds JAR component of your project to list of publications.
This configuration is all you need to generate the metadata for your JAR file and publish the file to a Maven repository.
Because you haven’t yet defined a repository, you can only publish these files to the local Maven cache located in the directory .m2/ repository.
Check the list of publication tasks available to the project:
This may look confusing at first, but it makes a lot of sense if you need fine-grained control over the publication process.
By picking the right task, you can selectively say which artifact(s) you want to publish to what repository.
This makes for a very declarative way to construct a publication task name.
The local Maven cache is of particular relevance for Gradle users for two reasons.
In chapter 5, you learned that Gradle tries to reuse artifacts found in the local Maven cache.
This is a major benefit to migrating Maven users, because the artifacts don’t have to be downloaded again.
The other reason why you’d want to publish to the local Maven cache is if you work with a variety of projects that use different build tools.
One project may use Gradle to publish an artifact; another project may use Maven to consume it.
Let’s assume you want to publish your plugin JAR file, including the generated POM file.
You may wonder why the POM file isn’t listed as an uploaded artifact in the console output.
What you end up with are the following artifacts in the local Maven cache directory:
The uploaded artifacts follow the typical format for dependencies within a Maven repository.
The directory com/manning/gia is derived from the project’s group property, the artifact ID (in your case, cloudbees-plugin) follows the project name, and the version reflects the value of the version property in your project.
They’re used to check the integrity of the associated file.
How to change the artifact publication name By default, the name of the published artifact is derived from the project name.
You may remember from earlier chapters that the project name corresponds with the name of your project directory.
It’s important to understand that any change to properties like archivesBaseName won’t have an effect on the publication name, even though the assembled archive may have the naming you desire.
Sometimes you may want to use a different publication name.
The same naming pattern is reflected in the dependency attributes groupId, artifactId, and version of the generated POM file.
At the time of the POM generation, the publication plugin automatically determines the project’s dependencies from the configuration runtime and declares them as transitive dependencies.
Here, you can see that this is the case for the CloudBees API client library.
Keep in mind that the publication API will only declare the dependencies that are relevant to the consumers of the JAR file.
Dependencies that are used to build your project (for example, test libraries) aren’t included.
Though this is a minor shift from Maven’s default philosophy of including both types of dependencies in the POM, build, and runtime information, it makes for much cleaner metadata without any drawbacks.
Great, you were able to publish the default outgoing project artifact.
But what if you want to publish additional artifacts? Your plugin project has the specific need to provide the sources JAR and Groovydocs JAR for consumption as well.
Let’s look at how these files can be declared using the publishing DSL.
Alternatively, you can also set the project name in your settings.gradle file.
The following example demonstrates how to change the project name to my-plugin:
Additional artifacts can be registered for publication by using the method artifact provided by the API class MavenPublication.
The Javadocs of the publication API will give you examples that demonstrate the use of both types of declarations.
The next listing shows how to assign the archive tasks sourcesJar and groovydocJar as Maven publications.
There’s one important detail  to keep in mind when declaring custom artifacts.
The publication plugin only allows a single artifact to be published with an empty classifier attribute.
For your sources JAR file, the classifier is sources, and for the Groovydocs JAR file this is groovydoc.
Initiate the previously used publication task again and see if you can deliver your custom artifacts to the Maven cache:
The console output now shows that all declared publications were uploaded to the repository.
A quick check of the directory tree reveals the desired end result:
Perfect—all artifacts you wanted to make available to the Maven cache could be published.
Consumers of the artifacts rely on the default POM metadata to resolve the plugin JAR file and its transitive dependencies.
In section 14.2, we talked about the data a POM file can contain.
In the next section, we’ll discuss how to modify the generated POM.
The POM file that describes an artifact within a repository should be as informative as possible.
At the very least, you should provide details about the purpose of your artifact, the software license, and a pointer to the documentation so that end users can get a lead on what functionality is provided and how to use it within their project.
The best way to find out what information can be configured is to the check the POM reference guide at http://maven.apache.org/pom.html.
The guide describes the XML structure and available tags for configuring a POM.
The generated standard POM can be modified with the help of the hook pom.withXml.
By calling the method asNode(), you can retrieve the root node of that POM.
The following listing shows how to add more information to your plugin POM.
The next listing shows the regenerated POM file, which now reflects your changes to the metadata.
So far, you’ve installed the published artifacts to the local Maven cache.
What if you wanted to publish them to a Maven repository on your local file system or the Artifactory repository you set up earlier? Nothing easier than that—the publishing API allows for declaring repositories as well.
In chapter 8, you used the “old” Maven plugin to upload your project’s artifact to a local Maven repository so it could be consumed by another Gradle project.
You used this technique for testing your plugin functionality on your local machine without having to make it public.
The same functionality is provided by the Maven publishing plugin.
Every repository you want to target needs to be declared in a repositories configuration block exposed by the publishing DSL.
In the following listing, you’ll declare a single Maven repository located in the directory named repo parallel to the project directory.
If you assign a name, it becomes part of the corresponding publishing task.
This is especially useful when dealing with multiple repositories at the same time.
Gradle is smart enough to automatically create the local repository even though its root directory doesn’t exist yet.
As expected, you’ll find the uploaded artifacts in the correct location:
In the next section, we’ll also look at how to publish the same artifacts to a remote repository via HTTP.
Remote repositories are extremely helpful for making artifacts available to other teams or stakeholders within your organization.
For testing purposes, we set up an Artifactory instance on our local machine.
Keep in mind that this repository could sit anywhere within your corporate network as long as it’s accessible via HTTP(S)
Both of these repositories expose a dedicated HTTP URL and control how and who can upload artifacts.
A secured repository requires you to supply the configured authentication credentials.
If you don’t provide these properties, your build will fail because it couldn’t authenticate your upload.
Gradle will read the contents of this file automatically at runtime and make the properties available to your build script.
For now, you’re going to use the default Artifactory administration credentials:
Next, you’ll write some code to publish your artifacts to one of these repositories based on the version property of your project.
The following listing doesn’t look too different from previous examples.
The biggest difference is that you also provide the credentials to the repository to authenticate the upload.
The publishing plugin incorporates the repository name into the name of the task used to upload your project artifacts to Artifactory:
Sets credentials needed to upload artifacts to secured Artifactory repository.
After uploading the artifacts to Artifactory, you should be able to browse the repository by the provided group ID.
There’s a directory with the specified version containing the POM and JAR files of your CloudBees plugin (figure 14.4)
Publishing artifacts to a repository within your corporate network is a practical solution for sharing binaries among teams.
It also takes care of protecting intellectual property from the outside world.
Simplifying the publishing code with product-specific plugins Some binary repository products provide a Gradle plugin to simplify the process of publishing.
Such a plugin provides a standardized, product-specific DSL and can’t be used with other binary repositories.
Think of it as a higher-level API sitting on top of the Gradle publishing API, while at the same time adding more features (for example, build information)
Using a vendor plugin can reduce the code you need to write for publishing artifacts.
These plugins should only be used if you know that your project is sticking with a particular product long term.
Organizations with a stake in open source software like to contribute their hard work to the Gradle community or anyone interested in using their code.
In this chapter, you’ll learn how to publish your CloudBees plugin to both repositories.
Bintray is more than just a provider for hosting binary repositories.
It comes with a web dashboard, integrated search functionality, social features, artifact download statistics, and a RESTful API for managing your repositories and artifacts.
Bintray was launched in early 2013 but quickly grew in popularity within its first months of existence.
Currently, you see a lot of open source projects moving to Bintray.
Before you can publish any artifacts to Bintray, you’ll need to set up an account.
After completing the signup process, log into the newly created account.
For your purposes, name this repository gradle-plugins and choose the Maven repository format.
A package acts as a container for artifacts produced by a specific project.
You can store any number of artifacts in a package.
Any artifact you publish to this package can only be downloaded as a Gradle project dependency if you provide your credentials in the repository configuration.
To make this package public to any Gradle user, you can request it to be synced with Bintray’s JCenter, a free-of-charge public Maven repository.
Only after JFrog approves your request will you be able to make your artifacts public.
Don’t worry if you get lost anywhere during the setup procedure or have additional questions.
The main difference is that you can share an artifact located in a private repository with JCenter, a public repository.
Bintray doesn’t require you to sign your artifacts before you upload them, but gives you the option of signing them after the fact.
Therefore, you’ll only need some minor modifications to your original build script, as shown in the following listing.
You’ll need to define a new Maven repository, provide the repository URL of your package, and give the Bintray credentials of your account.
The contents of this file should look similar to the following properties:
You used an API key instead of a password in the credentials configuration block.
You’re all set; time to make the world a better place by publishing your plugin artifacts:
Looks like the upload worked! You can also view the uploaded artifacts in the Bintray dashboard.
Managing public artifacts Never underestimate the power of open source.
Once you put an artifact on a public repository, other developers may start to use it.
Bintray doesn’t prevent you from deleting already-published versions of an artifact.
Try to stay away from deleting existing versions of your artifacts because it might break other developer’s builds.
If you need to fix a bug in your code, make sure to release a new version of your artifact and communicate the bug fix.
Looks pretty straightforward, right? Let’s also discuss what steps are required to publish the same plugin to Maven Central.
Maven Central (http://repo1.maven.org/maven2/) is probably the most popular public repository for binary artifacts, particularly open source projects.
Defines a dependency on your previously uploaded plugin JAR file.
Sonatype OSS has more specific requirements for publishing artifacts than Bintray.
You’ll need to wait until your account has been approved.
Once your request is approved, you can start publishing with the requested groupId.
For publishing with a different groupId, you’ll need to create another JIRA ticket.
Generate GNU Privacy Guard (GPG) signatures for all artifacts you wish to publish (shown in figure 14.7 with the file extension .asc)
To generate these signatures, you can use the Gradle signing plugin.
One word of warning: at the time of writing, the new publishing API doesn’t provide out-of-the-box signing support.
You may have to fall back to the old publishing API to achieve this.
Navigate to the Staging Repositories page, choose your artifact, and press the Close button.
Before the promoted artifact can be accessed via Maven Central, Sonatype OSS needs to sync it with the central repository.
Once an artifact is published, it can’t be deleted or modified.
The publishing process to Sonatype OSS requires a lot of manual steps and even more patience to wait for approvals.
If you’re unsure which public repository to choose for your project, I recommend using Bintray over Sonatype OSS.
Next, you’ll learn how to apply the concepts we’ve discussed so far to your To Do application.
After getting to know the core concepts of creating a distribution and publishing it to a binary repository, you can now apply your knowledge to your web application.
First, you’ll make some extensions to your existing Gradle build, and then you’ll extend your build pipeline by configuring a Jenkins job.
In the context of continuous delivery, there are some important practices to discuss.
They ultimately determine how you’ll implement the artifact packaging and publishing.
Before an application can be deployed to a target environment—for example, a UAT (user acceptance test)—for manual testing by the QA team, or to production to give it into the hands of the end users, the deliverable, also called the deployable artifact, has to be built.
It’s not uncommon for teams to rebuild this deliverable for each environment individually.
While this approach works, it creates the unnecessary risk of introducing a difference to the deliverable.
For example, the dependency manager may pick a newer version of a third-party library that became available on a repository that you didn’t intend to include.
To avoid any side effects, you should only build your deliverable once and store it in a central location.
As you learned earlier, a binary repository is the perfect fit for this use case.
Binary repositories require you to publish an artifact with unique attributes.
Figure 14.8 Creating the distribution in the context of the build pipeline.
The main distinguishing difference between each artifact built by your pipeline is the version.
The two other attributes, groupId and artifactId, will likely never change once you’ve settled on a meaningful value.
Over time, you’ll notice that more and more versions of your artifact will be stored.
Uploaded artifacts will stay in your repository until you delete them.
Please refer to the product’s documentation on how to best achieve this.
Figure 14.9 illustrates the process of publishing to a repository with incremental versions.
Once the artifact is uploaded to a binary repository with a specific version, you can retrieve it by these attributes and reuse it for later steps in the pipeline.
A typical use case would be the deployment to various environments.
Many repository products expose a RESTful API for downloading artifacts via HTTP(S)
The URL includes the dependency attributes to uniquely identify the artifact.
Figure 14.10 shows the download of a published artifact with a particular version from Artifactory for successive deployment purposes.
With this background information in mind, you’ll start by defining the versioning scheme for your project.
Figure 14.9 Publishing a WAR file to Artifactory with different versions.
Figure 14.10 Retrieving a WAR file from Artifactory for deployment purposes.
Versioning your artifact becomes important at the time of assembly.
In this section, we’ll discuss which versioning scheme plays well with the core principles of continuous delivery and how to implement an appropriate strategy with Gradle.
Some build tools propose a standard format for your versioning scheme.
A snapshot version of an artifact indicates that it’s still under development and not ready to be released to production.
Whenever the artifact is published to a binary repository, it’s uploaded with the same version.
Any consumer of this artifact will only be able to retrieve the latest version of that snapshot version.
Because there’s no concrete version attached to an artifact, you can’t link it to a unique revision in the VCS.
This can become a major drawback when trying to debug an issue.
For that reason, snapshot versions of a deliverable should never be deployed to the production environment.
At some point in development, it’s determined that the software is feature-complete.
Once it passes the QA process, it’s ready to be released to production.
At that time, the -SNAPSHOT suffix is taken off the version and it’s released to production.
Now you’re dealing with a release version of the artifact.
Ultimately, this means having to modify the version attribute in the POM file and checking it into VCS.
A new development cycle starts by bumping up the major and/or minor version of your project; the -SNAPSHOT suffix is added again.
What’s wrong with this way of assigning a project version? This is best explained by one of the core principles of continuous delivery.
Of course, this will only happen if the software passes all phases defined by the build pipeline with the quality standards agreed on by the team.
Maven’s versioning philosophy is diametrically opposed to this release strategy.
It assumes that you work on a feature for a certain period of time until it’s actually released.
To be able to uniquely identify a version of an artifact during development, as well as in the live environment, you’ll need to set an appropriate version at the initial.
Figure 14.11 Applying Maven’s standard versioning scheme in a build pipeline.
This version will carry through all the way to the pipeline stage that deploys the deliverable to production (figure 14.12)
Next, we’ll look at how such a versioning scheme can be implemented with Gradle.
You can directly apply this knowledge to build the project version for your To Do application.
It consists of three attributes: major version, minor version, and a build number.
The major and minor attributes of the versioning scheme are configured in Gradle.
They’ll change less often (for example, to indicate a new feature) and have to be incremented manually.
The dynamic part of this versioning scheme is the build number.
It’s incremented every time a build is kicked off—that is, at the initial step of the build pipeline.
Let’s look at the actual implementation of this versioning scheme in Gradle.
Because you want to share this class among all projects of your build, you’ll create the class in the buildSrc directory, as shown in the following directory tree:
Figure 14.12 Setting a dynamic, incrementing version number at the initial phase of the build pipeline.
You know that Gradle automatically compiles every class under the directory buildSrc when the build is run.
This compiled class can now be used in any of your build scripts to implement the versioning scheme.
This variable is automatically available to your build when you execute it as part of your Jenkins build pipeline.
In the last chapter, you set it as part of your initial job configuration via the parameterized trigger plugin.
For a quick refresher, you may want to quickly jump back to section 13.5.2
With your versioning script plugin in place, you can now apply it to all projects of your build with the help of the allprojects configuration block, as shown in the following listing.
Sets a build timestamp property to determine when build was initiated.
Assigns a new instance of ProjectVersion to project property version.
You can easily emulate building your WAR artifact without having to execute the Jenkins build pipeline.
The following example demonstrates how to assign the build number 42:
If you assemble the WAR file now, you’ll see that the correct version number is applied to your artifact.
Being able to identify a unique version of your artifact is essential for two reasons.
It implies a releasable version and enables you to map the binary to the source code by tagging it in your VCS.
The same versioning information can be included into the artifact to make it self-describing.
On occasion, you may also want to know what version of your application is deployed to a given environment.
At runtime, this property file can be read and displayed anywhere in your web application.
The following listing demonstrates the required task configuration in the build file of your web subproject.
Listing 14.15 Providing the version to all projects of the build.
Adds a task dependency on task that writes build information file.
Bundles build information file in directory WEB-INF/ classes of WAR file.
To give you an idea of how the generated file may look, see the following example:
We won’t discuss the custom code required to read and display these properties in your To Do application.
Please refer to the book’s code examples to see how to achieve this with Java.
With your artifact well prepared, you’re ready to push it to an internal binary repository for later deployments.
Next, you’ll adapt the publishing code you wrote at the beginning of this chapter to upload the WAR file.
Publishing the WAR file to an internal Artifactory repository isn’t magic.
You already have the essential knowledge on how to do this.
The major difference here is that the software component you’re about to publish is a WAR file instead of a JAR file, as shown in the following listing.
Remember that this task name is put together by the publishing API based on the assigned publication and repository name.
If you check the generated POM file, you may notice that a WAR publication doesn’t define any external dependencies.
This is for a good reason: they’re already bundled with the WAR file in the directory WEB-INF/lib.
In the next section, you’ll wire up this phase of your build pipeline to the existing jobs in Jenkins.
Creating the distribution and publishing it to a binary repository is an essential step for delivering your To Do application.
You already implemented the Gradle side to support this functionality.
Now it’s time to extend your build pipeline by configuring the corresponding job on Jenkins.
From the last chapter, you may remember that you set up three Jenkins jobs executed in the following order:
Bring up the Jenkins dashboard and add a new job called todo-distribution.
To simplify its creation, feel free to clone it from the existing job named todo-code-quality.
After the job is created, you’ll need to make some additional changes to step 3 of your build pipeline.
As far as parameters go, you’ll reuse the existing ones by choosing the option Current Build Parameters.
Next, you’ll put in some finishing touches on your newly created job.
Have a quick look at this checklist to see if it’s set up correctly:
In the Source Code Management configuration section, choose the option Clone Workspace and choose the parent project todo-initial.
As the build step, you want to assemble the WAR file and publish it to Artifactory.
Add a build step for invoking your Gradle script using the wrapper and enter the tasks assemble publish.
With this configuration in place, your build pipeline view in Jenkins should look similar to figure 14.14
To verify its correct behavior, make sure to take the pipeline for a test drive.
Given the current status of your build pipeline, you’re close to deploying your To Do application to various target environments.
Keep on reading to learn how to approach this task.
Most software needs to be assembled before it can be deployed to a target environment or installed to a particular location.
During the assembly process, individual software components need to be put together in a meaningful, consumable format.
A software delivery doesn’t have to be limited to a single artifact.
Often it consists of multiple artifacts or distributions cut for a specific group of stakeholders or runtime environments.
Gradle supports creating artifacts for a wide range of archiving formats through core custom tasks like Jar or Zip, some of which are automatically preconfigured for your project if you apply a certain plugin.
In addition to this functionality, the Gradle distribution plugin can be used to describe custom distributions through an expressive DSL, without actually having to define tasks in your build script.
With these tools in your toolbox, it’s simple to produce the artifacts you need and flexibly react to new requirements for your delivery process.
Once the deliverable artifacts are built, they can be shared with other projects, teams, or literally every developer on the planet.
Binary repositories provide the infrastructure for uploading, managing, browsing, and consuming any number or type of artifacts.
In this chapter, you learned how to use Gradle’s publishing plugin to interact with a local or remote Maven repository.
You took your plugin project from chapter 8, assembled the plugin JAR file, generated individualized metadata, and uploaded it to Artifactory, a popular binary repository.
In the context of continuous delivery, assembling and publishing artifacts plays a crucial role.
Whenever possible, you’ll want to package the artifacts just once to avoid potential side effects.
After uploading the artifacts to a repository, they can be reused.
You learned how to implement a flexible versioning strategy to clearly identify a set of artifacts.
Later, you extended your build pipeline with a new job for packaging and publishing the WAR file produced by your To Do application.
In the last chapter of this book, you’ll finally roll out your To Do application to various target environments, write smoke tests to verify a successful deployment, and tag the release in version control.
Any server outage inflicted by a faulty deployment—with the biggest hit on production systems—results in money lost for your organization.
Automation is the next logical and necessary step toward formulating and streamlining the deployment process.
In this chapter, we’ll talk about how to automate the deployment process with Gradle by the example of your To Do application.
Before any deployment can be conducted, the target environment needs to be preconfigured with the required software infrastructure.
Historically, this has been the task of a system administrator, who would manually provision the physical server machine and install the software components before use.
This setup can be defined as real code with tools like Puppet and Chef, checked into version control, Infrastructure provisioning and deployment.
While Gradle doesn’t provide native tooling for this task, you can bootstrap other tools to do the job for you.
Deploying software to use as build masters means more than just copying a file to a server.
In your build, you’ll need to be able to configure and target different environments.
Automating the full deployment lifecycle often requires cleaning out previously deployed artifacts, as well as restarting remote runtime environments like web containers.
Once you deploy a new version of your software, you need to verify the outcome.
Automated smoke and acceptance testing can help to detect the correct functioning of the software.
You’ll set up a sufficient suite of tests and execute them with Gradle.
After setting up the supporting tasks in Gradle, you can invoke them from corresponding jobs in Jenkins.
Deploying software for authorized stakeholders within an organization should be as easy as pushing a button.
You’ll extend your build pipeline by deploying jobs for different environments.
Before we can dive into the details of deploying software, let’s review the tools that are helpful for provisioning an infrastructure.
Before any application can be deployed, the hosting infrastructure needs to be provisioned.
When I talk about infrastructure provisioning in the traditional sense, I mean setting up the hardware as well as installing and configuring the required operating system and software components.
Nowadays, we see a paradigm shift toward cloud provisioning of infrastructure.
Unlike the traditional approach, a cloud provider often allocates preconfigured hardware in the form of virtual servers.
Server virtualization is the partitioning of a physical server into smaller virtual servers to help maximize the server resources.
Depending on the service offer, the operating system and software components are managed by the cloud provider.
In this section, we’ll talk about automating the creation of virtual servers and infrastructure software components with the help of third-party open source tools.
These tools will help you set up and configure streamlined target environments for your To Do application.
Later, you’ll learn how Gradle can integrate with these tools.
Software infrastructure that’s needed to run an application has to be set up by hand.
If you think back to your To Do application, this includes the Java runtime, a web container, and a database.
What might sound unproblematic for a single developer can transform into a huge issue the moment the team grows in size.
Infrastructure provisioning needs to make sure that they install the same version of the same software packages with the same configuration (optimally on the same operating system)
A similar process has to be followed for setting up the hardware and software infrastructure for other environments (for example, UAT and production) that are part of the deployment pipeline.
In larger organizations, the responsibility for performing this task traditionally falls on the shoulders of the operations team.
Without proper communication and documentation, getting these environments ready ends up becoming a lengthy and nerve-wracking procedure.
Even worse, if any of the configuration settings need to be changed, they have to be propagated across all environments manually.
While shell scripting is a good first step to mitigate this pain, it doesn’t fully automate the infrastructure provisioning end-to-end across environments.
The paradigm of infrastructure as code aims to bridge the gap between software development and system administration.
With sufficient tooling, it’s possible to describe a machine’s configuration as executable code, which is then checked into version control and shared among different stakeholders.
Any time you need to create a new machine, you can build a new one based on the instructions of your infrastructure code.
Ultimately, this allows you to treat infrastructure code like any other software development project that can be versioned, tested, and checked for potential syntactical issues.
In the past couple of years, several commercial and open source tools have emerged to automate infrastructure provisioning.
We’ll focus on two of the most popular open source infrastructure automation tools: Vagrant and Puppet.
The next section will give you an architectural overview of how both tools can be used together to build a virtual machine from scratch.
The end result will be a runtime environment equipped to serve your To Do application.
Vagrant (http://www.vagrantup.com/) is an infrastructure tool for configuring and creating virtual environments.
A machine can be managed with the help of the Vagrant executable.
For example, you can start and stop a machine with a simple, oneline shell script.
Even better, you can directly SSH into it and control it like every other remote *nix server.
The software configuration of a machine is described through shell scripts or provisioning tools such as Chef and Puppet.
The provisioning provider you use often boils down to personal preference and knowledge of the tool.
If you think back to the runtime environment required for your To Do application, you can identify the following software packages and their configuration:
It’s beyond the scope of this book to fully describe the configuration needed to set up such a scenario.
However, you can find a working example in the source code of the book.
Let’s examine the basic structure of your Vagrant project to get a highlevel understanding:
Figure 15.1 illustrates how the individual components of a Vagrant project play together.
At a minimum, every Vagrant project needs to contain a Vagrantfile.
Based on this file, virtual machines are configured and created.
You’re going to go with Puppet as the configuration provider.
The configuration you want to apply to the virtual machine is set up in a Puppet manifest file, which is referenced in the Vagrantfile.
In this case, the name of the manifest file is tomcat.pp.
To be able to version and share the Vagrant project with other developers, you need to check it into version control like regular source code.
After a successful installation, you can invoke the Vagrant executable from the command line.
To bring up your Vagrant machine, navigate to the Vagrant project in your shell and execute the command vagrant up.
Vagrant is fairly wordy, so we won’t show the command-line output here.
After a few moments, you should see the notice that the virtual machine was brought up successfully.
Vagrant file that defines how to configure and provision virtual machines.
Puppet manifest file that defines how to configure a target virtual machine.
In the Vagrantfile, you configured the virtual machine to be accessible by the IP address 192.168.1.33
To verify a successful installation of the web container, open the browser of your choice and enter the URL http://192.168.1.33:8080/
To shut down the virtual machine, use the command vagrant destroy.
In the next section, you’ll learn how to bootstrap Vagrant commands from Gradle.
At this point, you may be thinking, “Why would I want to execute Vagrant commands from Gradle?” The short answer is automation.
Any workflow that incorporates interacting with a virtual machine provided by Vagrant needs to be able to call the corresponding command from Gradle.
To show you a simple workflow, let’s assume you want to execute functional tests on a virtual machine that mimics the infrastructure setup of a production server.
Shut down the virtual machine via the command vagrant destroy.
This use case is fairly advanced and requires some complex setup.
For now, you’ll start simple and enable your build to wrap Vagrant command calls with Gradle tasks.
The task defines the Vagrant commands you want to execute as an input parameter.
Additionally, you need to point the task to the Vagrant project you’d like to target.
The following listing demonstrates a reusable task that allows executing Vagrant commands.
Depending on the complexity of your configuration, some Vagrant commands (especially vagrant up) may need a few minutes to finish.
If you have a chain of tasks that build on each other, you need to make sure that task execution is delayed until Vagrant completes the actual work.
Your task implementation takes care of this requirement by letting the current thread wait until the Vagrant command responds with an exit value.
The following listing demonstrates the use of the custom task to expose important Vagrant commands to a Gradle build.
Stops a running Vagrant machine and destroys all its resources.
Outputs configuration of SSH configuration file (needed to SSH into a running machine)
Reports on state of a Vagrant machine (for example, running, suspended)
Suspends a running Vagrant machine by creating a snapshot of its current state.
Congratulations, you just implemented a way to integrate Vagrant into your build! Running Vagrant on a local machine is great for simulating production-like environments.
When it comes to interacting with existing environments other than your local machine, your build needs to have a way of configuring the connection information.
The main maxim of continuous delivery is to get the software from the developer’s machine into the hands of the end users as quickly and frequently as possible.
However, that doesn’t mean that you assemble your deliverable and deploy it in the production environment right away.
In between these steps, a build pipeline usually verifies functional and nonfunctional requirements in other environments, as shown in figure 15.2
At the beginning of this chapter, you created a virtual machine on your developer machine.
Though the virtual machine has a production-like setup, you use this environment solely for testing purposes.
The test environment brings together code changes from multiple developers of the team.
Therefore, it can be seen as the first integration point of running code.
On the deployed application in the test environment, you can run automated acceptance tests to verify functional and nonfunctional requirements.
The user acceptance test (UAT) environment typically exists for the purpose of exploratory, manual testing.
Once the QA team considers the current state of the software code to be satisfactory, it’s ready to be shipped to production.
The production environment directly serves to the end user and makes new features available to the world.
If you want to use the same code for deploying to all of these environments, you’ll need to be able to dynamically target each one of them at build time.
Naturally, the test, UAT, and production environments run on different servers with potentially different ports and credentials.
You could store the configuration as extra properties in your build script, but that would quickly convolute the logic of the file.
In both cases, you’d end up with a fairly unstructured list of properties.
At build time, you’d have to pick a set of properties based on a naming convention.
Doesn’t sound very flexible, does it? There’s a better way of storing and reading this configuration with the help of a standard Groovy feature.
Configuration, especially if you have a lot of it, should be as readable and structured as possible.
One of Groovy’s language features allows for defining properties with the.
Each of the environments that you want to define properties for is enclosed in the environments configuration block.
For each environment, you assigned a closure with a descriptive name.
For example, you can define the server hostname, SSH port, and username to log into the server.
You now have a Groovy-based configuration file in place, but how do you read its content from the Gradle build? Groovy provides a handy API class named groovy.util .ConfigSlurper that’s designed to parse a treelike data structure.
ConfigSlurper is a utility class for reading configuration in the form of Groovy scripts.
Once this configuration is parsed, the property graph can be navigated by dot notation.
You need to make sure that the configuration is read before any task action is executed.
The task that reads the Groovy script doesn’t need to contain a task action.
Instead, you’ll create an instance of the class ConfigSlurper in the configuration block of the task and provide the specific environment you want to read in its constructor.
The method parse points to the location of the configuration file.
The following listing demonstrates how to parse the Groovy script based on the provided property env.
The parsed configuration is made available to all projects of your build through the extra property config.
Next, you’ll actually use the property in another task that requires the parsed configuration.
Assigns read configuration to extra property config available to all projects of build.
A typical usage pattern for the configuration shown in listing 15.3 is the deployment of the To Do web application to a specific environment.
Key to targeting a specific environment is to provide a value for the property env on the command line.
For example, if you want to target the UAT environment, you’d provide the project property -Penv=uat.
The value of this project property directly corresponds to the closure named uat in the Groovy configuration script.
You’re going to emulate a deployment task to see if your mechanism works.
In the web project, you’ll create a new task named deployWar, as shown in listing 15.5
For now, you won’t bother with actually implementing deployment logic.
To verify that the settings appropriate to the targeted environment are parsed correctly, the task’s doLast action will use Gradle’s logger to print the read hostname and port.
When you run the task deployWar for the environment local, you can see that the proper settings are parsed and rendered on the command line:
Figure 15.4 Targeting specific environments by providing a project property.
In the next section, you’ll actually use these settings to deploy the To Do application WAR file to a server.
The end game of every build pipeline is to deploy the software to a production environment once it passes all automated and manual testing phases.
Under all circumstances, you want to avoid human error when interacting with a production environment to install a new version.
Failure to deploy the software properly will lead to unexpected side effects or downtime and actual money lost for your organization.
I think we can agree that the task of deploying software to production should be a nonevent.
Deployment automation is an important and necessary step toward this goal.
The code used to automate the deployment process shouldn’t be developed and exercised against the production environment right away to reduce the risk of breakages.
Instead, start testing it with a production-like environment on your local machine, or a test environment.
It uses infrastructure definitions that are fairly close to your production environment.
Mimicking a production-like environment using a virtual machine for developing deployment code is cheap, easy to manage, and doesn’t disturb any other environment participating in your build pipeline.
Once you’re happy with a working solution, the code should be used for deploying to the least-critical environment in your build pipeline.
After gaining more confidence that the code is working as expected, you can deploy it to more mission-critical environments like UAT and production.
It’s dependent on the type of software you write and the target environment you’re planning to deploy to.
For example, a deployment of a web application to a Linux machine has different requirements than client-installed software running on Windows.
At the time of writing, Gradle doesn’t offer a unified approach for deploying software.
The approach we’ll discuss in this chapter is geared toward deploying your web application to a Tomcat container.
In the last chapter, you learned how to upload an artifact to a binary repository.
Now that you have the Groovy configuration file in place, you can also add the Artifactory repository URL.
In this example, you only use a single repository that isn’t specific to an environment.
ConfigSlurper also reads any properties declared outside of the environments closure independent of the provided env property.
The following listing demonstrates how to declare common configuration—in this case, the binary repository.
In this example, Gradle’s dependency management does the heavy lifting of downloading the file from the repository.
Please note that it’s not mandatory to use Gradle’s dependency management for retrieving the file.
You could also use the Ant task Get and write a lower-level implementation in Groovy.
After executing the task, you’ll find the expected file in the download directory:
Of course, it makes sense to download the artifact just once, even though you’ll deploy it to different environments.
Executing the task a second time will mark it UP-TO-DATE, as shown in the following command-line output:
Listing 15.7 Downloading the WAR file from a remote repository.
Before you can take care of business by deploying the web application to Tomcat, you should plan out the necessary steps to implement the process.
The deployment process for a web application to a remote server needs to follow a workflow to ensure a smooth transition from the current version to a new one.
First, you need to make sure that all artifacts of the old version, like the exploded WAR file, are properly removed.
Under all circumstances, you need to avoid mixing up old artifacts with the new ones.
Some deployment solutions like Cargo (http://cargo.codehaus.org/) allow for deploying a web application while the container is running, a technique also known as hot deployment.
While it might sound attractive at first, because you don’t have to restart the server, hot deployment isn’t a viable solution for production systems.
Over time, long-running JVM processes will run into an OutOfMemoryError for their PermGen space, which will cause it to freeze up.
The reason is that the JVM will not garbagecollect class instances from previous deploys even if they’re now unused.
Therefore, it’s highly recommended to fully stop the web container JVM process before a new version is deployed.
An efficient deployment process can look like the following steps:
In the following section, you’ll implement this process with the help of Gradle.
The previously created Vagrant instance will act as a test bed for your deployment script.
We didn’t go into any specifics about the operating system of the virtual machine you set up before.
Assume that the box is based on the Linux distribution Ubuntu.
You may know that transferring a file to a remote machine running a SSH daemon can be achieved with Secure Copy (SCP)
For authentication purposes, SCP will ask for a password or a pass phrase.
Alternatively, the private key file can be provided to authenticate the user.
You could model the whole deployment process in a shell script and call it from Gradle by creating an enhanced task of type Exec.
That’s certainly a valid way of implementing the necessary steps.
However, in this section we’ll discuss how to model each step with a corresponding Gradle task.
If you’re familiar with Ant, you may have used the SCP task before.
The Ant task provides a nice abstraction on top of a pure Java SSH implementation named JSch (http://www.jcraft.com/jsch/)
The next listing shows how to wrap the Ant SCP task with a custom Gradle task declared in the buildSrc project.
You’ll use this SCP abstraction in the build script of your web project to copy the WAR file to the remote location.
In listing 15.9, you declare the JSch Ant task dependency with the help of a custom configuration named jsch.
This dependency is passed on to the classpath property of the enhanced task responsible for transferring the WAR file to the remote server.
You also incorporate the server settings read during the configuration phase of your build.
Listing 15.8 Custom task wrapping the optional Ant SCP task.
Listing 15.9 Transferring the WAR file to the server via SCP.
This listing only implements step one of the deployment process.
All of the other steps need to execute shell commands on the remote server itself.
Instead of running an interactive shell, SSH can run a command on the remote machine and render the output.
The following listing shows the custom task SshExec that internally wraps the SSH Ant task.
Listing 15.10 Custom task wrapping the optional Ant SSH task.
Before SCPing the WA file to server, make sure it’s downloaded from Artifactory.
In the next listing, you use this custom SSH task to run various shell commands on the Vagrant virtual box.
As a whole, this script implements the full deployment workflow we discussed earlier.
Listing 15.11 SSH commands for managing Tomcat and deploying WAR file.
Uses Ant SSH task to copy a file to server.
That’s all you need to implement a full deployment process.
Bring up the Vagrant box and give it a try.
The following command-line output shows the execution steps in action:
After restarting Tomcat again, it may take some seconds until your web application is up and running.
After giving Tomcat enough time to explode the WAR file and start its services, navigating to the URL http://192.168.1.33:8080/todo in a browser will present you with a To Do list ready to be filled with new tasks.
Running SSH commands isn’t the only approach for tackling deployment automation.
Due to the diversity of this topic, we won’t present them in this chapter.
As long as the process you choose is repeatable, reliable, and matches your organization’s needs, you’re on the right path.
Every deployment of an application should be followed by rudimentary tests that verify that the operation was successful and the system is in an expected, working state.
These types of tests are often referred to as deployment tests.
If for whatever reason a deployment failed, you want to know about it—fast.
In the worst-case scenario, a failed deployment to the production environment, the customer shouldn’t be the first to tell you that the application is down.
You absolutely need to avoid this situation because it destroys credibility and equates to money lost for your organization.
The harsh reality is that deployments can fail even with the best preparation.
Knowing about it as soon as possible is worth a mint.
As a result, you can take measures to bring back the system into an operational state; for example, by rolling back the application to the last “good” version.
In addition to these fail-fast tests, automated acceptance tests verify that important features or use cases of the deployed application are correctly implemented.
Instead of running them on your developer machine against an embedded Jetty container, you’ll configure them to target other environments.
Let’s first look at how to implement the most basic types of deployment tests: smoke tests.
Your deployment automation code should incorporate tests that check that your system is in a basic, functional state after the deployment is completed.
Why this name, you ask? Smoke tests make an analogy to hardware installation like electronic circuits.
If you turn on the power and see smoke coming out of the electrical parts, you know that the installation went wrong.
After a deployment, the target environment may need time to reach its fully functional state.
For example, if the deployment process restarts a web container, it’s obvious that it won’t be able to serve incoming requests right away.
If that’s the case for your environment setup, make sure to give some leeway before executing your suite of smoke tests.
How do smoke tests look for a web application like your To Do application? Simple—you can fire basic HTTP requests to see if the Tomcat server is up and running.
You’ll make your life even easier by using a Groovy.
HTTPBuilder wraps the functionality provided by HttpComponents with a DSL-style configuration mechanism, which boils down the code you need to write significantly.
You’ll use HTTPBuilder from a custom task named HttpSmokeTest that acts as an abstraction layer for making HTTP calls.
To make this task available to all projects of your build, the implementing class becomes part of the buildSrc project, as shown in the following directory tree:
Before any class under buildSrc can use the HTTPBuilder library, you need to declare it in a build script for that project.
As you can imagine, you may implement other types of smoke tests later; for example, for testing the functionality of the database.
Let’s have a closer look at the implementation in the next listing for smoke tests that fire an HTTP request.
Build script for buildSrc project declaring a dependency on the HTTPBuilder library.
In your build script, you can set up as many smoke tests as you need.
As with the URL, provide the HTTP endpoint of your web application that was read from the Groovy configuration file earlier.
Figure 15.5 shows how to use the env project property to target a particular environment.
Makes HTTP Get request for provided URL and checks if response code comes back OK.
Fail smoke test task if HTTP response code is anything other than OK.
The following listing shows two different smoke tests: one for verifying that Tomcat is up and running and another for checking if your web application was deployed successfully.
I’m sure you can imagine a whole range of smoke tests for your real-world applications.
It’s certainly worth experimenting with options, as long as they’re cheap to write and fast to execute.
If all smoke tests pass, you can rightfully assume that your application was deployed successfully.
But does it work functionally? As a next step, you should determine whether the provided application functionality works as expected.
Functional tests, also called acceptance tests, focus on verifying whether the end user requirements are met.
In chapter 7, you learned how to implement a suite of functional tests for your web application with the help of the browser automation tool Geb.
Of course, you want to be able to run these tests against a deployed application in other environments.
Acceptance tests are usually run during the automated acceptance test phase of the continuous delivery build pipeline.
This is the first time in the pipeline that you bring together the work of the development team, deploy it to a test server, and verify whether the functionality meets the needs of the business in an automated fashion.
In later phases of the build pipeline, acceptance tests can be run to get quick feedback about the success of a deployment on a functional level.
The better the quality of your tests, the more confident you can be about the determined result.
In listing 15.13, you added a new task of type Test for running functional tests against remote servers.
Geb allows for pointing to an HTTP endpoint by setting the.
The value you assign to this system property is derived from the read environment configuration, as shown in the following listing.
In the previous chapters, we discussed the purpose and practical application of phases during the commit stage.
We compiled the code, ran automated unit and integration tests, produced code quality metrics, assembled the binary artifact, and pushed it to a repository for later consumption.
For a quick refresher on previously configured Jenkins jobs, please refer to earlier chapters.
While the commit stage asserts that the software works at the technical level, the acceptance stage verifies that it fulfills functional and nonfunctional requirements.
To make this determination, you’ll need to retrieve the artifact from the binary repository and deploy it to a production-like test environment.
You use smoke tests to make sure that the deployment was successful before a suite of automated acceptance tests is run to verify the application’s end user functionality.
In later stages, you reuse the already downloaded artifact and deploy it to other environments: UAT for manual testing, and production environment to get the software into the hands of the end users.
As a template for these stages, you’ll duplicate an existing Jenkins job.
The end result will be the following list of Jenkins jobs:
Listing 15.15 Task for exercising functional tests against remote servers.
Task of typ Test for running functional tests agai different environme.
Together these jobs model the outstanding stages of your build pipeline, as shown in figure 15.6
In the next sections, we’ll examine each of these stages one by one.
Figure 15.7 illustrates this stage in the context of later deployment stages of the build pipeline.
Have a quick look at this checklist to see if it’s set up correctly:
In the Source Code Management configuration section, choose the option Clone Workspace and the parent project todo-distribution.
As with the build step, you want to download the WAR file from Artifactory and deploy it to the test environment.
Add a build step for invoking your Gradle script using the wrapper and enter the task deployWar.
In the field Switches, you’ll provide the appropriate environment property: -Penv=test.
As far as parameters go, you’ll reuse the existing ones by selecting the option Current Build Parameters.
Deployment testing should follow directly after deploying the application to the test environment (figure 15.8)
There are two important points you need to consider when configuring the corresponding Jenkins job.
The execution of the job has to be slightly delayed to allow the.
Figure 15.6 Acceptance, UAT, and production stages as part of the build pipeline.
Figure 15.7 Deploying the WAR file to a test server for acceptance testing.
Also, the downstream project (the deployment to UAT) may not be executed automatically.
In the Advanced Project Options configuration section, tick the checkbox Quiet Period and enter the value 60 into the input field.
This option will delay the execution of the job for one minute to ensure that the Tomcat server has been properly started.
Because this method can be kind of brittle, you may want to implement a more sophisticated mechanism to check whether the server is up and running.
As with the build step, you want to run smoke and acceptance tests against the test environment.
In the field Switches, you’ll provide the appropriate environment property: -Penv=test.
When executing the full pipeline in Jenkins, you’ll notice that the job for deploying to UAT requires manual intervention.
Only if you actively initiate the deployment will the pipeline execution resume—that is, until it hits another push-button, downstream job.
The same configuration needs to apply to the job that deploys the artifact to the production environment, as shown in figure 15.9
We won’t go into too much detail about the configuration of these jobs.
In fact, they look very similar to the jobs that you set up to implement the acceptance stage.
In the Gradle build step, the UAT deployment job needs to set the –Penv=uat switch.
The deployment job to the production environment applies the setting –Penv=prod.
The build pipeline view in Jenkins can be configured to keep a history of previously executed builds.
This is a handy option if you want to get a quick overview of failed and successful builds.
This view also enables the stakeholders of your build to deploy artifacts with a specific version.
Typical scenarios for this use case could be one of the following:
The product team decides to launch a new feature included in a specific version of your application.
Rolling back the application version in production to a known “good” state due to a failed deployment or broken feature.
Deploying a given feature set for manual testing by the QA team into the UAT environment.
Jenkins needs to know which version should be deployed when you hit the release button.
Thankfully, the parameterized build plugin helps you to provide the appropriate version to the job.
For each of the deployment jobs, make the following configuration.
Any server outage inflicted by a faulty deployment—with the biggest hit on production systems—results in money lost for your organization.
Automation is the next logical and necessary step toward formulating and streamlining the deployment process.
Deployable artifacts often look different by nature, follow custom project requirements, and demand distinct runtime environments.
While there’s no overarching recipe for deploying software, Gradle proves to be a flexible tool for implementing your desired deployment strategy.
A configured target environment is a prerequisite for any software deployment.
At the beginning of this chapter, we discussed the importance of infrastructure as code for setting up and configuring an environment and its services in an automated fashion.
Vagrant can play an instrumental role in creating and testing infrastructure.
Figure 15.9 Performing push-button releases to UAT and production environments.
You learned how to bootstrap a virtual machine by wrapping Vagrant management commands with Gradle tasks.
Later, you implemented an exemplary deployment process using SSH commands and exercised the functionality on a running Vagrant box.
To ensure repeatability for your deployments, the same code should be used across all environments.
This means that the automation logic needs to use dynamic property values to target a particular environment.
Groovy’s API class ConfigSlurper provides an easy-to-use mechanism for parsing these settings.
To have the property values available for consumption across all projects of your build, you coded a task that reads the Groovy script during Gradle’s configuration lifecycle phase.
Automated deployment tests, invoked after a deployment, can provide fast feedback.
Smoke tests are easy to implement and quickly reveal breakages.
Functional tests, also called acceptance tests, are the natural extension of smoke tests.
This type of test assesses whether functional and nonfunctional requirements are met.
By the end of this chapter, you extended your build pipeline by manual and pushbutton deployment capabilities.
In Jenkins, you set up three deployment jobs for targeting a test, UAT, and production environment, including their corresponding deployment tests.
With these last steps completed, you built a fully functional, end-to-end build pipeline.
Together, we explored the necessary tooling and methods that will enable you to implement your own build pipeline using Gradle and Jenkins.
Gradle’s command-line interface (CLI) is a user’s primary tool of choice for discovering available options, inspecting the project, and controlling execution behavior by providing configuration information.
It consists of three parts: discovery or help tasks, build setup tasks, and configuration input.
A.1 Discovery tasks Many discovery tasks provide information about the build.
If you’re new to a project, they’re a good place to start discovering the configuration.
Every Gradle build provides the discovery tasks shown in table A.1 from the get-go.
A.2 Build setup tasks At a minimum, every Gradle project requires a build.gradle file to define your build logic.
This file can be created manually or conveniently generated by tasks of the build setup plugin.
Table A.2 shows the build setup tasks for initializing a new Gradle build.
Table A.1 Discovery tasks available to all Gradle projects (continued)
The task requires you to provide the mandatory parameters --dependency to inspect a particular dependency.
To inspect a dependency assigned to a configuration other than compile, use the parameter --configuration.
If you run the gradle command without specifying a task, the help task is executed.
Some of these properties are provided by Gradle’s Project object, the build’s internal representation.
Other properties are user-defined properties originating from a property file, property command-line option, or directly declared in your build script.
To print additional information about the available tasks, this task can be run with the option --all.
Table A.2 Build setup tasks for initializing a new Gradle build.
If a pom.xml is found, Gradle tries to derive a Gradle project from the Maven metadata (see task maven2Gradle)
A.3 Configuration input Build configuration information can be provided through the CLI.
Options that don’t require a value can be combined; for example, -is for running the build on the INFO log level and printing out the stack trace if an error occurs.
Table A.3 describes common command-line options that don’t belong to a particular functional grouping.
These options may come in handy in your day-to-day use of Gradle, so feel free to explore them.
This task is only available if no pom.xml file can be found in the project directory.
This task is only available if no pom.xml file can be found in the project directory.
After running this task, build.gradle and settings.gradle files are created.
This task is only available if a pom.xml can be found.
By using partial builds, you can avoid the cost of checking the subproject model and bring down your build execution time.
Table A.2 Build setup tasks for initializing a new Gradle build (continued)
Use this option to execute a build script with a different name (for example,  gradle –b test.gradle build)
This option is particularly useful in a multiproject build with many subprojects.
It’ll discover all possible issues with a build, and allows you to fix them at once without having to fix problems one by one.
This option aims for optimizing the configuration time required to initialize a multiproject build.
The configuration on-demand mode attempts to configure only projects that are relevant for requested tasks.
Gradle’s default home directory is located in the directory .gradle under the user’s home directory.
Use this option if you want to point to a different directory.
I, --init-script Specifies an initialization script used for the build.
This script is executed before any of your project tasks are executed.
With this option, you can specify a different directory to execute the build.
Properties provide a way to configure your build from the command line.
Besides the standard Java system properties, Gradle defines project properties.
This option comes in handy if you want to quickly determine the task execution order of your build.
This option ignores any UP-TODATE status of previous task executions.
Tells Gradle to not search for a settings file in parent directories.
This option is useful if you want to avoid the performance hit of searching all parent directories in a deeply nested project structure.
A practical example for this option is if you want to execute a full build of a Java project without running the unit tests (for example, gradle –x test build)
As with all Java processes, you can provide a system property like this: –Dmyprop=myvalue.
Gradle allows access to all log messages produced by your build.
Depending on your use case, you can provide logging options to filter the relevant messages important to you, as shown in table A.5
Gradle uses caching on various levels to improve the performance of the build.
With the options presented in table A.6, you can change the default caching behavior.
P, --project-prop Project properties are variables available in your build script.
You can use this option to pass a property to the build script directly from the command line (for example, -Pmyprop=myvalue)
Use this option to get more informative messages by changing Gradle’s logger to the INFO log level.
This is helpful if you want to get more information on what’s happening under the hood.
Use this option if you want to troubleshoot a build problem.
The option –s prints out an abbreviated stack trace if an exception is thrown, making it perfect for debugging broken builds.
S, --full-stacktrace Prints out the full stack trace for all exceptions.
Once started, the gradle command will reuse the forked daemon process for subsequent builds, avoiding the startup costs altogether.
Table A.7 gives an overview of the available options for controlling the daemon process.
If these dependencies weren’t stored in your local cache, running a build without a network connection to these repositories would result in a failed build.
Use this option to run your build in offline mode and only check the local dependency cache for dependencies.
This option can be used to point to a different directory.
To flush the cache of compiled scripts, execute the build with this option.
This flag forces a check for changed artifact versions with the configured repositories.
If it doesn’t exist, a new daemon process is started.
On top of this functionality sits a domain-specific language (DSL) written in the dynamic programming language Groovy.
When writing a Gradle build script, you automatically use the language constructs exposed by this DSL to express the desired build instructions.
Gradle build scripts are executable Groovy scripts, but they can’t be run by the Groovy runtime.
When the need to implement custom logic arises, you can use Groovy’s language features to build out the desired functionality directly in the Gradle build script.
This appendix is a primer to Groovy and explains why it’s important for users of Gradle to learn the language.
Later, I’ll also demonstrate how some of Gradle’s configuration elements are implemented with Groovy.
Its syntax is close to the one provided by Java.
The language integrates with existing Java classes and libraries, which makes it easy for Java developers to learn it.
Not only does Groovy build upon the strengths of Java, it also provides powerful programming features inspired by those of Ruby, Python, and others.
Groovy can be used as a scripting language without having to compile the code.
While this appendix will give you a head start on Groovy’s most important language features, it’s highly recommended that you explore it further on your own.
There are great resources out there in the form of books and dedicated Groovy web pages.
The definitive guide to Groovy is the book Groovy in Action, Second Edition by Dierk Konig, et al (Manning, 2009)
The DZone reference guide on Groovy provides a cheat sheet that’s handy to every beginner of the lan429
B.2 How much Groovy do I need to know? If you’re new to Gradle you may wonder how much Groovy you actually need to know to write your first build scripts.
When implementing trivial task actions within your build script, you can choose to write plain Java code or use Groovy’s expressive language constructs.
Assume you want to determine all files within a directory and write their names to a new file named allfiles.txt.
Sounds pretty simple, right? The following listing demonstrates the Java version of the task action doLast.
Apart from the task definition itself, which is defined with Gradle’s DSL, no Groovy code was required to implement the scenario.
Next, we’ll compare this task implementation with the Groovy version.
In the following listing, the task action code is much shorter and more concise than the Java version.
A mixture of Java and Groovy works just fine for trivial builds, especially if you’re in the process of learning Groovy.
For more complex builds that include custom tasks and plugins, you’ll need to know more about the language.
And for a deep understanding of how Gradle works internally, knowing about advanced Groovy features is key.
B.3 Comparing Java and Groovy syntax In the last section, we compared a task implementation written in Java and Groovy.
In Groovy you can become extremely productive while at the same time writing less code.
To get a good understanding of how this plays out in practice, let’s talk about the main differentiators between both languages.
The class describes the major and minor version of a Gradle project.
The next listing shows a simplified version of the class.
As a Java developer, you probably see this kind of boilerplate code every day.
Plain old Java objects (POJOs), classes that don’t implement or extend third-party library classes, are often used to represent data.
The class exposes two private fields via getter and setter methods.
The following listing demonstrates how to express the same logic in Groovy.
I think we can agree that the Groovy version of the class looks far less noisy.
Groovy assumes sensible defaults for any class you write, particularly the following optimizations:
If you compare two instances of a class with ==, Groovy automatically calls the method equals() under the hood.
B.4 Essential Groovy features So far, we’ve only scratched the surface of the feature set Groovy provides.
In this section, we’ll explore language aspects used on a regular basis when programming in Groovy.
The features we’ll discuss in the following sections aren’t listed in a particular order.
Feel free to follow along and try out the examples one by one on your computer.
There are two tools that are extremely helpful in running code snippets.
The Groovy console is automatically available after installing the Groovy runtime.
It provides an interactive user interface for entering and executing Groovy scripts.
The Groovy web console (http://groovyconsole.appspot.com/) provides an even easier way to try out Groovy code snippets.
You can run Groovy scripts on the web without installing the Groovy runtime.
Keep in mind that the language features used in your script are bound to the bundled Groovy version of the web console indicated on the page.
If you’re coming from Java, you may know the keyword assert.
Unlike Java’s pendant, which only works if you enable assertion checking by setting a runtime flag (-ea or –enableassertion), Groovy’s assert statement is always evaluated.
Groovy’s assert statement, also called power assert, provides helpful output to identify the root cause of an issue.
The following output demonstrates a sample output for the listing:
You’ll use the assert statement in the upcoming code examples as a tool to verify and document the desired behavior.
Groovy doesn’t force you to explicitly declare a type of variable, method parameter, or return type.
At runtime, Groovy figures out the type based on the assigned value.
This feature is helpful for certain use cases, but you may still prefer explicit, strong typing.
Particularly in projects that expose a public API, strong typing automatically improves documentation, makes it more obvious what parameter types need to be provided, and enables IDE code completion.
For the same reasons, declaring void instead of def as a return type should be used if a method doesn’t return a value.
Method calls in Groovy can omit the parentheses if the method signature requires at least one parameter.
Without going into too much detail, this feature is often used to create more natural-looking DSLs, a human-readable language understandable by domain experts.
The following listing compares two method calls with and without the parentheses.
There are three different ways to declare Strings in Groovy.
A single-quoted String always creates the Java equivalent of a String.
The second form follows the Java way of creating a String.
Multiline Strings, wrapped by triple double quotes, are helpful if you want to assign wordy text or impose formatting (for example, multiline SQL statements)
The next listing shows the full range of creating Strings in Groovy.
They can interpolate embedded variables or expressions denoted by a dollar sign and curly braces.
At runtime, the expression is evaluated and forms a String.
In Groovy, these types of Strings are also known as GStrings.
The following listing gives an example of their practical use.
Groovy offers a concise syntax for implementations of the Collections API, which makes them easier to use than their Java equivalent.
Groovy also adds syntactic sugar to simplify the use of a List.
A perfect example is the left shift operator (<<) that allows for adding a new element to the List.
To initialize a new Map with a new value, create a comma-separated list of key-value pairs in square brackets.
The next listing shows usage examples of a Groovy Map.
The class exposes a constructor to initialize its fields with values.
Groovy provides another handy way of setting property values, called named parameters.
This mechanism first calls the default constructor of the class and then calls the setter methods for each of the provided parameters.
The following listing shows how to set the values for the fields major and minor through named parameters.
Closures can be assigned to variables, passed to methods as parameters, and called like regular methods.
The parameter it refers to the first parameter passed to a closure when calling it.
If no parameter is provided, the value of the parameter is null.
Let’s look at an example to make this concept less abstract.
The following listing shows the definition and invocation of a closure, including the implicit parameter it.
In the next listing, you define a parameter named version of type ProjectVersion.
You could have used the identifier version without the type.
Groovy doesn’t limit the number of parameters a closure can define.
The following listing shows how to declare multiple, untyped parameters for a closure.
This is either the last statement of the closure if no explicit return statement is declared, or the value of the executed return statement.
If the last statement of a closure doesn’t have a value, null is returned.
The closure shown in the following listing returns the value of the last statement, the project’s minor version.
By default, the delegate is the owner of the closure.
The implicit variable delegate of a closure allows for redefining the default owner.
The Groovy Development Kit (GDK) extends the Java Development Kit (JDK) by providing convenience methods to the standard JDK classes.
Internal method of class ProjectVersion that takes a closure as parameter Only resolve.
Invokes method that internally executes closure on instance of ProjectVersion.
Many of these methods use a closure as a parameter to add a functional flavor to the language.
You used the method to iterate over the elements of a Collection.
Let’s look at some other examples in the following listing.
Within the build script, you can use every Groovy feature the language provides.
This means that the code has to strictly adhere to Groovy’s syntax.
Invalid code will automatically result in a runtime error when executing the build.
Gradle comes with a DSL written in Groovy for modeling typical build concerns.
In most cases, properties and methods you invoke in the build script automatically delegate to the Project instance.
Let’s examine the sample build script shown in listing B.20
After getting to know some of Groovy’s language features by example, you may have a hunch how they work internally.
I hope this example can demystify some of the “magic” beginners to Gradle and Groovy encounter.
As parameter, provides a Map with a single key–value pair (see section B.4.6)
Sets a new value for property version by calling setter method of Project under the hood (see section B.3)
Calls method repositories on Project with a single closure parameter (see section B.4.8)
Calls method dependencies on Project with a single closure parameter (see section B.4.8).Calls method compile with a String as parameter on.
It extends the usage patterns established by its forerunners Ant and Maven and allows builds that are expressive, maintainable, and easy to understand.
Using a fl exible Groovy-based DSL, Gradle provides declarative and extendable language elements that let you model your project’s needs the way you want.
Gradle in Action is a comprehensive guide to end-to-end project automation with Gradle.
Starting with the basics, this practical, easy-to-read book discusses how to establish an effective build process for a full-fl edged, real-world project.
Along the way, it covers advanced topics like testing, continuous integration, and monitoring code quality.
You’ll also explore tasks like setting up your target environment and deploying your software.
The book assumes a basic background in Java but no knowledge of Groovy.
Benjamin Muschko is a member of the Gradleware engineering team and the author of several popular Gradle plugins.
Front cover brief contents contents foreword preface acknowledgments about this book Roadmap Who should read the book? Code conventions and downloads Author Online About the author.
