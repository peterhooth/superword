Java Peformance Tuning, 2nd edition provides a comprehensive and indispensable guide to eliminating all types of performance problems.
Using many real-life examples to work through the tuning process in detail, JPT shows how tricks such as minimizing object creation and replacing strings with arrays can really pay off in improving your code's performance.
For this reason, Java Performance Tuning, Second Edition includes four new chapters: a new chapter on J2EE application tuning in general followed by chapters on tuning JDBC, servlets and JSPs, and EJBs.
Java Peformance Tuning, 2nd edition provides a comprehensive and indispensable guide to eliminating all types of performance problems.
Using many real-life examples to work through the tuning process in detail, JPT shows how tricks such as minimizing object creation and replacing strings with arrays can really pay off in improving your code's performance.
For this reason, Java Performance Tuning, Second Edition includes four new chapters: a new chapter on J2EE application tuning in general followed by chapters on tuning JDBC, servlets and JSPs, and EJBs.
O'Reilly & Associates books may be purchased for educational, business, or sales promotional use.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O'Reilly & Associates, Inc.
The association between the image of a serval and the topic of Java performance tuning is a trademark of O'Reilly & Associates, Inc.
Java and all Java-based trademarks and logos are trademarks or registered trademarks of Sun Microsystems, Inc., in the United States and other countries.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
This book provides all the details a developer needs to performance-tune any type of Java program.
I give step-by-step instructions on all aspects of the performance-tuning process, from early considerations such as setting goals, measuring performance, and choosing a compiler, to detailed examples on using profiling tools and applying the results to tune the code.
This is not an entry-level book about Java, but you do not need any previous tuning knowledge to benefit from reading it.
Many of the tuning techniques presented in this book lead to an increased maintenance cost, so they should not be applied arbitrarily.
Change your code only when a bottleneck has been identified, and never change the design of your application for minor performance gains.
If you do not yet have a tuning strategy, this chapter provides a methodical tuning process.
Chapter 2 covers the tools you need to use while tuning.
Chapter 3 looks at the SDK, including Virtual Machines (VMs) and compilers.
Chapter 12 looks at tuning techniques specific to distributed applications.
Chapter 13 steps back from the low-level code-tuning techniques examined throughout most of the book and considers tuning at all other stages of the development process.
Chapter 14 is a quick look at some operating system-level tuning techniques.
Finally, Chapter 19 covers a wide range of additional resources where you can learn more about Java performance tuning.
Use these lists to ensure that you have not missed any core tuning techniques while you are tuning.
All test results have been updated using the latest versions of the VMs available, including the VMs released with SDK 1.4
Table P-1 provides an overview of material that is new or significantly expanded in this edition.
I have focused on the Sun VMs, as there is enough variation within these to show interesting results.
I have shown the time variation across different VMs for many of the tests.
However, your main focus should be on the effects that tuning has on any one VM, as this identifies the usefulness of a tuning technique.
Differences between VMs are interesting, but are only indicative and need to be verified for your specific application.
Where I have shown the results of timed tests, the VM versions I have used are:
The default is with a JIT, and this is the mode used for all measurements in the book.
The default is with a JIT, and this is the mode used for all measurements in the book.
Version 1.3 supports running in interpreted mode, with HotSpot technology for the client (mixed mode) and for the server (server mode)
For the server-mode tests, I recorded the test times after the JIT generated code, i.e., the second run of tests.
These times are more representative of how server-mode JVMs are used.
In addition, I've used the -Xbatch option to ensure that all the HotSpot compilation takes place in the first run through any tests, and I've recorded the second run times.
These times are more representative of how server-mode JVMs are used.
I do not recommend using -Xbatch in general, unless you know definitely that the first runthrough of the application can be much slower without affecting user expectations.
Names and keywords in Java programs, including method names, variable names, and class names.
Please send any errors you find, as well as suggestions for future editions, to:
There is a web page for this book, which lists errata, examples, and additional information.
To comment or ask technical questions about this book, send email to:
For more information about books, conferences, Resource Centers, and the O'Reilly Network, see the O'Reilly web site at:
A huge thank you to my wonderful wife, Ava, for her unending patience with me.
This book would have been considerably poorer without her improvements in clarity and consistency throughout.
I am also very grateful to Mike Loukides and Kirk Pepperdine for the enormously helpful assistance I received from them while writing this book.
Their many notes have helped to make this book much clearer and more complete.
Thanks also to my reviewers for the first edition, Patrick Killelea, Ethan Henry, Eric Brower, and Bill Venners, who provided many useful comments.
They identified several errors and added good advice that makes this book more useful.
This edition has benefited from feedback from many, many people, and I'm very grateful to everyone who took the time to give me their opinions and helpful suggestions.
I am, of course, responsible for the final text of this book, including any erroors tthat rremain.
The trouble with doing something right the first time is that nobody appreciates how difficult it was.
There is a general perception that Java programs are slow.
Part of this perception is pure assumption: many people assume that if a program is not compiled, it must be slow.
Part of this perception is based in reality: many early applets and applications were slow, because of nonoptimal coding, initially unoptimized Java Virtual Machines (VMs), and the overhead of the language.
In earlier versions of Java, you had to struggle hard and compromise a lot to make a Java application run quickly.
More recently, there have been fewer reasons why an application should be slow.
With good designs and by following good coding practices and avoiding bottlenecks, applications usually run fast enough.
However, the truth is that the first (and even several subsequent) versions of a program written in any language are often slower than expected, and the reasons for this lack of performance are not always clear to the developer.
This book shows you why a particular Java application might be running slower than expected, and suggests ways to avoid or overcome these pitfalls and improve the performance of your application.
In this book I've gathered several years of tuning experiences in one place.
I hope you will find it useful in making your Java application, applet, servlet, and component run as fast as you need.
Throughout the book I use the generic words "application" and "program" to cover Java applications, applets, servlets, beans, libraries, and really any use of Java code.
Where a technique can be applied only to some subset of these various types of Java programs, I say so.
Otherwise, the technique applies across all types of Java programs.
This question is always asked as soon as the first tests are timed: "Where is the time going? I did not expect it to take this long." Well, the short answer is that it's slow because it has not been performance-tuned.
In the same way the first version of the code is likely to have bugs that need fixing, it is also rarely as fast as it can be.
When debugging, you have to fix bugs throughout the code; in performance tuning, you can focus your effort on the few parts of the application that are the bottlenecks.
The longer answer? Well, it's true that there is overhead in the Java runtime system, mainly due to its virtual machine layer that abstracts Java away from the underlying hardware.
It's also true that there is overhead from Java's dynamic nature.
These overhead s can cause a Java application to run slower than an equivalent application written in a lower-level language (just as a C program is generally slower than the equivalent program written in assembler)
For example, hierarchical method invocation requires an extra computation for every method call because the runtime system has to work out which of the possible methods in the hierarchy is the actual target of the call.
Most modern CPU s are designed to be optimized for fixed call and branch targets and do not perform as well when a significant percentage of calls need to be computed on the fly.
On the other hand, good objectoriented design actually encourages many small methods and significant polymorphism in the method hierarchy.
Compiler inlining is another frequently used technique that can significantly improve compiled code.
However, this technique cannot be applied when it is too difficult to determine method calls at compile time, as is the case for many Java methods.
Of course, the same Java language features that cause these overheads may be the features that persuaded you to use Java in the first place.
The important thing is that none of them slows the system down too much.
Naturally, "too much" differs depending on the application, and the users of the application usually make this choice.
But the key point with Java is that a good round of performance tuning normally makes your application run as fast as you need it to run.
There are already plenty of nontrivial Java applications, applets, and servlets that run fast enough to show that Java itself is not too slow.
So if your application is not running fast enough, chances are that it just needs tuning.
Your target is to get a better score (lower time) than the last score after each attempt.
You are playing with, not against, the computer, the programmer, the design and architecture, the compiler, and the flow of control.
You can complete this list better than I can for your particular situation.
I once worked with a customer who wanted to know if there was a "go faster" switch somewhere that he could just turn on and make the whole application go faster.
Of course, he was not really expecting one, but checked just in case he had missed a basic option somewhere.
There is no such switch, but very simple techniques sometimes provide the equivalent.
Techniques include switching compilers, turning on optimizations, using a different runtime VM, finding two or three bottlenecks in the code or architecture that have simple fixes, and so on.
I have seen all of these yield huge improvements to applications, sometimes a 20-fold speedup.
Order-of-magnitude speedups are typical for the first round of performance tuning.
When tuning an application, the first step is to determine which of these is causing your application to run too slowly.
If your application is CPU-bound, you need to concentrate your efforts on the code, looking for bottlenecks, inefficient algorithms, too many short-lived objects (object creation and garbage collection are CPU-intensive operations), and other problems, which I will cover in this book.
On the other hand, external data access or writing to the disk can be slowing your application.
In this case, you need to look at exactly what you are doing to the disks that is slowing the application: first identify the operations, then determine the problems, and finally eliminate or change these to improve the situation.
For example, one program I know of went through web server logs and did reverse lookups on the IP addresses.
A simple analysis of the activity being performed determined that the major time component of the reverse lookup operation was a network query.
These network queries do not have to be done sequentially.
Consequently, the second version of the program simply multithreaded the lookups to work in parallel, making multiple network queries simultaneously, and was much, much faster.
In this book we look at the causes of bad performance.
Identifying the causes of your performance problems is an essential first step to solving those problems.
There is no point in extensively tuning the disk-accessing component of an application because we all know that "disk access is much slower than memory access" when, in fact, the application is CPU-bound.
Once you have tuned the application's first bottleneck, there may be (and typically is) another problem, causing another bottleneck.
It is not uncommon for an application to have its initial "memory hog" problems solved, only to become disk-bound, and then in turn CPU-bound when the disk-access problem is fixed.
After all, the application has to be limited by something, or it would take no time at all to run.
This may seem obvious, but I frequently encounter teams that tackle the main identified problem, and then instead of finding the next real problem, start applying the same fix everywhere they can in the application.
One application I know of had a severe disk I/O problem caused by using unbuffered streams (all disk I/O was done byte by byte, which led to awful performance)
After fixing this, some members of the programming team decided to start applying buffering everywhere they could, instead of establishing where the next bottleneck was.
In fact, the next bottleneck was in a data-conversion section of the application that was using inefficient conversion methods, causing too many temporary objects and hogging the CPU.
Rather than addressing and solving this bottleneck, they instead created a large memory-allocation problem by throwing an excessive number of buffers into the application.
Here's a strategy I have found works well when attacking performance problems:
Identify the main bottlenecks (look for about the top five bottlenecks, but go higher or lower if you prefer)
Choose the quickest and easiest one to fix, and address it (except for distributed applications where the top bottleneck is usually the one to attack: see the following paragraph)
The advantage of choosing the "quickest to fix" of the top few bottlenecks rather than the absolute topmost problem is that once a bottleneck has been eliminated, the characteristics of the application change, and the topmost bottleneck may not need to be addressed any longer.
However, in distributed applications I advise you target the topmost bottleneck.
The characteristics of distributed applications are such that the main bottleneck is almost always the best to fix and, once fixed, the next main bottleneck is usually in a completely different component of the system.
Although this strategy is simple and actually quite obvious, I nevertheless find that I have to repeat it again and again: once programmers get the bit between their teeth, they just love to apply themselves to the interesting parts of the problems.
After all, who wants to unroll loop after boring loop when there's a nice juicy caching technique you're eager to apply?
You should always treat the actual identification of the cause of the performance bottleneck as a science, not an art.
Measure the performance by using profilers and benchmark suites and by instrumenting code.1
Think of a hypothesis for the cause of the bottleneck.3
Create a test to isolate the factor identified by the hypothesis.5
Test that the alteration improves performance, and measure the improvement (include regression-testing the affected code)
Looking at the code, you find a complex loop and guess this is the problem (hypothesis).3
You see that it is not iterating that many times, so possibly the bottleneck could be outside the loop (confounding factor)
You could vary the loop iteration as a test to see if that identifies the loop as the bottleneck.
However, you instead try to optimize the loop by reducing the number of method calls it makes: this provides a test to identify the loop as the bottleneck and at the same time provides a possible solution.
Although this is frequently the way tuning actually goes, be aware that this can make the tuning process longer: if there is no speedup, it may be because your optimization did not actually make things faster, in which case you have neither confirmed nor eliminated the loop as the cause of the bottleneck.
Rerunning the profile on the altered application finds that this method has shifted its percentage time down to just 4%
This method may still be a candidate for further optimization, but nevertheless it's confirmed as the bottleneck and your change has improved performance.
It is important to understand that the user has a particular view of performance that allows you to cut some corners.
The user of an application sees changes as part of the performance.
A browser that gives a running countdown of the amount left to be downloaded from a server is seen to be faster than one that just sits there, apparently hung, until all the data is downloaded.
People expect to see something happening, and a good rule of thumb is that if an application is unresponsive for more than three seconds, it is seen as slow.
Some Human Computer Interface authorities put the user patience limit at just two seconds; an IBM study from the early '70s suggested people's attention began to wander after waiting for more than just one second.
For performance improvements, it is also useful to know that users are not generally aware of response time improvements of less than 20%
This means that when tuning for user perception, you should not deliver any changes to the users until you have made improvements that add more than a 20% speedup.
A few long response times make a bigger impression on the memory than many shorter ones.
Consequently, as long as you reduce the variation in response times so that the 90th percentile value is smaller than before, you can actually increase the average response time, and the user will still perceive the application as faster.
For this reason, you may want to target variation in response times as a primary goal.
Unfortunately, this is one of the more complex targets in performance tuning: it can be difficult to determine exactly why response times are varying.
If the interface provides feedback and allows the user to carry on other tasks or abort and start another function (preferably both), the user sees this as a responsive interface and doesn't consider the application as slow as he might otherwise.
If you give users an expectancy of how long a particular task might take and why, they often accept this and adjust their expectations.
Modern web browsers provide an excellent example of this strategy in practice.
People realize that the browser is limited by the bandwidth of their connection to the Internet and that downloading cannot happen faster than a given speed.
Good browsers always try to show the parts they have already received so that the user is not blocked, and they also allow the user to terminate downloading or go off to another page at any time, even while a page is partly downloaded.
Generally, it is not the browser that is seen to be slow, but rather the Internet or the server site.
In fact, browser creators have made a number of tradeoffs so that their browsers appear to run faster in a slow environment.
I have measured browser display of identical pages under identical conditions and found browsers that are actually faster at full page display but seem slower because they do not display partial pages, download embedded links concurrently, and so on.
Modern web browsers provide a good example of how to manage user expectations and perceptions of performance.
However, one area in which some web browsers have misjudged user expectation is when they give users a momentary false expectation that operations have finished when in fact another is to start immediately.
This frustrates users who initially expected the completion time from the first download report and had geared themselves up to do something, only to have to wait again (often repeatedly)
A better practice would be to report on how many elements need to be downloaded as well as the current download status, giving the user a clearer expectation of the full download time.
It is better to provide the option to choose between faster performance and better functionality.
When users have made the choice themselves, they are often more willing to put up with actions taking longer in return for better functionality.
When users do not have this control, their response is usually less tolerant.
This strategy also allows those users who have strong performance requirements to be provided for at their own cost.
But it is always important to provide a reasonable default in the absence of any choice from the user.
Where there are many different parameters, consider providing various levels of user-controlled tuning parameters, e.g., an easy set of just a few main parameters, a middle level, and an expert level with access to all parameters.
This must, of course, be well documented to be really useful.
A lot of time (in CPU cycles) passes while the user is reacting to the application interface.
This time can be used to anticipate what the user wants to do (using a background low-priority thread), so that precalculated results are ready to assist the user immediately.
Similarly, ensuring that your application remains responsive to the user, even while it is executing some other function, makes it seem fast and responsive.
For example, I always find that when starting up an application, applications that draw themselves on screen quickly and respond to repaint requests even while still initializing (you can test this by putting the window in the background and then bringing it to the foreground) give the impression of being much faster than applications that seem to be chugging away unresponsively.
Starting different word-processing applications with a large file to open can be instructive, especially if the file is on the network or a slow (removable) disk.
Some act very nicely, responding almost immediately while the file is still loading; others just hang unresponsively with windows only partially refreshed until the file is loaded; others don't even fully paint themselves until the file has finished loading.
This illustrates what can happen if you do not use threads appropriately.
In Java, the key to making an application responsive is multithreading.
Use threads to ensure that any particular service is available and unblocked when needed.
Of course, this can be difficult to program correctly and manage.
Handling interthread communication with maximal responsiveness (and minimal bugs) is a complex task, but it does tend to make for a very snappily built application.
When you display the results of some activity on the screen, there is often more information than can fit on a single screen.
For example, a request to list all the details on all the files in a particular large directory may not fit on one display screen.
The usual way to display this is to show as much as will fit on a single screen and indicate that there are more items available with a scrollbar.
Other applications or other information may use a "more" button or have other ways of indicating how to display or move on to the extra information.
In these cases, you initially need to display only a partial result of the activity.
For activities that take too long and for which some of the results can be returned more quickly than others, it is certainly possible to show just the first set of results while continuing to compile more results in the background.
This gives the user an apparently much quicker response than if you were to wait for all the results to be available before displaying them.
The general case is when you have a long activity that can provide results in a stream.
For distributed applications, sending all the data is often what takes a long time; in this case, you can build streaming into the application by sending one screenful of data at a time.
Also, bear in mind that when there is a really large amount of data to display, the user often views only some of it and aborts, so be sure to build in the ability to stop the stream and restore its resources at any time.
Caching is an optimization technique I return to in several different sections of this book when appropriate to the problem under discussion.
For example, in the area of disk access, there are several caches that apply: from the lowest-level hardware cache up through the operating-system disk read and write caches, cached filesystems, and file reading and writing classes that provide buffered I/O.
Some caches cannot be tuned at all; others are tuneable at the operating-system level or in Java.
Where it is possible for a developer to take advantage of or tune a particular cache, I provide suggestions and approaches that cover the caching technique appropriate to that area of the application.
In cases where caches are not directly tuneable, it is still worth knowing the effect of using the cache in different ways and how this can affect performance.
For example, disk hardware caches almost always apply a readahead algorithm: the cache is filled with the next block of data after the one just read.
This means that reading backward through a file (in chunks) is not as fast as reading forward through the file.
Caches are effective because it is expensive to move data from one place to another or to calculate results.
If you need to do this more than once to the same piece of data, it is best to hang onto it the first time and refer to the local copy in the future.
This applies, for example, to remote access of files such as browser downloads.
The browser caches the downloaded file locally on disk to ensure that a subsequent access does not have to reach across the network to reread the file, thus making it much quicker to access a second time.
It also applies, in a different way, to reading bytes from the disk.
If you are going to read more than one byte from a particular disk area, it is better to read in a whole page (or all the data if it fits on one page) and access bytes through your local copy of the data.
General aspects of caching are covered in more detail in Section 11.7
Caching is an important performancetuning technique that trades space for time, and it should be used whenever extra memory space is available to the application.
Before diving into the actual tuning, there are a number of considerations that will make your tuning phase run more smoothly and result in clearly achieved objectives.
Any application must meet the needs and expectations of its users, and a large part of those needs and expectations is performance.
Before you start tuning, it is crucial to identify the target response times for as much of the system as possible.
The performance should be specified for as many aspects of the system as possible, including:
Multiuser response times depending on the number of users (if applicable)
Systemwide throughput (e.g., number of transactions per minute for the system as a whole, or response times on a saturated network, again if applicable)
The maximum number of users, data, files, file sizes, objects, etc., the application supports.
Any acceptable and expected degradation in performance between minimal, average, and extreme values of supported resources.
Agree on target values and acceptable variances with the customer or potential users of the application (or whoever is responsible for performance) before starting to tune.
Otherwise, you will not know where to target your effort, how far you need to go, whether particular performance targets are achievable at all, and how much tuning effort those targets may require.
But most importantly, without agreed targets, whatever you achieve will tend to become the starting point.
The following scenario is not unusual: a manager sees horrendous performance, perhaps a function that was expected to be quick, but takes 100 seconds.
Agreeing on targets before tuning makes everything clear to everyone.
After establishing targets with the users, you need to set benchmarks.
These are precise specifications stating what part of the code needs to run in what amount of time.
Without first specifying benchmarks, your tuning effort is driven only by the target, "It's gotta run faster," which is a recipe for a wasted return.
You must ask, "How much faster and in which parts, and for how much effort?" Your benchmarks should target a number of specific functions of the application, preferably from the user perspective (e.g., from the user pressing a button until the reply is returned or the function being executed is completed)
You should specify ranges: for example, best times, acceptable times, etc.
These times are often specified in frequencies of achieving the targets.
You should also have a range of benchmarks that reflect the contributions of different components of the application.
If possible, it is better to start with simple tests so that the system can be understood at its basic levels, and then work up from these tests.
In a complex application, this helps to determine the relative costs of subsystems and which components are most in need of performance-tuning.
The following point is critical: Without clear performance objectives, tuning will never be completed.
This is a common syndrome on single or small group projects, where code keeps being tweaked as better implementations or cleverer code is thought up.
Your general benchmark suite should be based on real functions used in the end application, but at the same time should not rely on user input, as this can make measurements difficult.
Any variability in input times or any other part of the application should either be eliminated from the benchmarks or precisely identified and specified within the performance targets.
There may be variability, but it must be controlled and reproducible.
There are tools for testing applications in various ways.[2] These tools focus mostly on testing the robustness of the application, but as long as they measure and report times, they can also be used for performance testing.
However, because their focus tends to be on robustness testing, many tools interfere with the application's performance, and you may not find a tool you can use adequately or cost-effectively.
If you cannot find an acceptable tool, the alternative is to build your own harness.
Your benchmark harness can be as simple as a class that sets some values and then starts the main( ) method of your application.
A slightly more sophisticated harness might turn on logging and timestamp all output for later analysis.
GUI-run applications need a more complex harness and require either an alternative way to execute the graphical functionality without going through the GUI (which may depend on whether your design can support this), or a screen event capture and playback tool (several such tools exist[3])
In any case, the most important requirement is that your harness correctly reproduce user activity and data input and output.
Normally, whatever regression-testing apparatus you have (and presumably are already using) can be adapted to form a benchmark harness.
The benchmark harness should not test the quality or robustness of the system.
The harness should support the different configurations your application operates under, and any randomized inputs should be controlled, but note that the random sequence used in tests should be reproducible.
You should use a realistic amount of randomized data and input.
It is helpful if the benchmark harness includes support for logging statistics and easily allows new tests to be added.
The harness should be able to reproduce and simulate all user input, including GUI input, and should test the system across all scales of intended use up to the maximum numbers of users, objects, throughputs, etc.
You should also validate your benchmarks, checking some of the values against actual clock time to ensure that no systematic or random bias has crept into the benchmark harness.
For the multiuser case, the benchmark harness must be able to simulate multiple users working, including variations in user access and execution patterns.
Without this support for variations in activity, the multiuser tests inevitably miss many bottlenecks encountered in actual deployment and, conversely, do encounter artificial bottlenecks that are never encountered in deployment, wasting time and resources.
It is critical in multiuser and distributed applications that the benchmark harness correctly reproduce user-activity variations, delays, and data flows.
Each run of your benchmarks needs to be under conditions that are as identical as possible; otherwise, it becomes difficult to pinpoint why something is running faster (or slower) than in another test.
The benchmarks should be run multiple times, and the full list of results retained, not just the average and deviation or the ranged percentages.
Also note the time of day that benchmarks are being run and any special conditions that apply, e.g., weekend or after hours in the office.
It is essential that you always run an initial benchmark to precisely determine the initial times.
This is important because, together with your targets, the initial benchmarks specify how far you need to go and highlight how much you have achieved when you finish tuning.
It is more important to run all benchmarks under the same conditions than to achieve the end-user environment for those benchmarks, though you should try to target the expected environment.
It is possible to switch environments by running all benchmarks on an identical implementation of the application in two environments, thus rebasing your measurements.
But this can be problematic: it requires detailed analysis because different environments usually have different relative performance between functions (thus your initial benchmarks could be skewed compared with the current measurements)
Each set of changes (and preferably each individual change) should be followed by a run of benchmarks to precisely identify improvements (or degradations) in the performance across all functions.
A particular optimization may improve the performance of some functions while at the same time degrading the performance of others, and obviously you need to know this.
Each set of changes should be driven by identifying exactly which bottleneck is to be improved and how much of a speedup is expected.
Rigorously using this methodology provides a precise target for your effort.
You need to verify that any particular change does improve performance.
It is tempting to change something small that you are sure will give an "obvious" improvement, without bothering to measure the performance change for that modification (because "it's too much trouble to keep running tests")
Jon Bentley once discovered that eliminating code from some simple loops can actually slow them down.[4] If a change does not improve performance, you should revert to the previous version.
An empty loop in C ran slower than one that contained an integer increment operation.
Be on the lookout for artificial performance problems caused by the benchmarks themselves.
This is very common if no thought is given to normal variation in usage.
Be careful not to measure artificial situations, such as full caches with exactly the data needed for the test (e.g., running the test multiple times sequentially without clearing caches between runs)
There is little point in performing tests that hit only the cache, unless this is the type of work the users will always perform.
When tuning, you need to alter any benchmarks that are quick (under five seconds) so that the code applicable to the benchmark is tested repeatedly in a loop to get a more consistent measure of where any problems lie.
By comparing timings of the looped version with a single-run test, you can sometimes identify.
Optimizing code can introduce new bugs, so the application should be tested during the optimization phase.
A particular optimization should not be considered valid until the application using that optimization's code path has passed quality assessment.
It is often useful to retain the previous code in comments for maintenance purposes, especially as some kinds of optimized code can be more difficult to understand (and therefore to maintain)
It is typically better (and easier) to tune multiuser applications in single-user mode first.
Many multiuser applications can obtain 90% of their final tuned performance if you tune in single-user mode, and then identify and tune just a few major multiuser bottlenecks (which are typically a sort of give-and-take between single-user performance and general system throughput)
Occasionally, though, there will be serious conflicts that are revealed only during multiuser testing, such as transaction conflicts that can slow an application to a crawl.
These may require a redesign or rearchitecting of the application.
For this reason, some basic multiuser tests should be run as early as possible to flush out potential multiuser-specific performance problems.
Tuning distributed applications requires access to the data being transferred across the various parts of the application.
At the lowest level, this can be a packet sniffer on the network or server machine.
One step up from this is to wrap all the external communication points of the application so that you can record all data transfers.
These are small applications that just reroute data between two communication points.
Most useful of all is a trace or debug mode in the communications layer that allows you to examine the higher-level calls and communication between distributed parts.
You should use this measurement to specify almost all benchmarks, as it's the real-time interval that is most appreciated by the user.
There are certain situations, however, in which system throughput might be considered more important than the wall-clock time, e.g., for servers, enterprise transaction systems, and batch or background systems.
The number of runnable processes waiting for the CPU (this gives you an idea of CPU contention)
You need to be careful when running tests with small differences in timings.
The first test is usually slightly slower than any other tests.
Try doubling the test run so that each test is run twice within the VM (e.g., rename main( ) to maintest( ), and call maintest( ) twice from a new main( ))
There are almost always small variations between test runs, so always use averages to measure differences and consider whether those differences are relevant by calculating the variance in the results.
For distributed applications , you need to break down measurements into times spent on each component, times spent preparing data for transfer and from transfer (e.g., marshalling and unmarshalling objects and writing to and reading from a buffer), and times spent in network transfer.
Each separate machine used on the networked system needs to be monitored during the test if any system parameters are to be included in the measurements.
Timestamps must be synchronized across the system (this can be done by measuring offsets from one reference machine at the beginning of tests)
Taking measurements consistently from distributed systems can be challenging, and it is often easier to focus on one machine, or one communication.
The most efficient tuning you can do is not to alter what works well.
As they say, "If it ain't broke, don't fix it." This may seem obvious, but the temptation to tweak something just because you have thought of an improvement has a tendency to override this obvious statement.
The second most efficient tuning is to discard work that doesn't need doing.
It is not at all uncommon for an application to be started with one set of specifications and to have some of the specifications change over time.
Many times the initial specifications are much more generic than the final product.
However, the earlier generic specifications often still have their stamps in the application.
I frequently find routines, variables, objects, and subsystems that are still being maintained but are never used and never will be used because some critical aspect is no longer supported.
These redundant parts of the application can usually be chopped without any bad consequences, often resulting in a performance gain.
In general, you need to ask yourself exactly what the application is doing and why.
Then question whether it needs to do it in that way, or even if it needs to do it at all.
If you have third-party products and tools being used by the application, consider exactly what they are doing.
Try to be aware of the main resources they use (from their documentation)
For example, a zippy DLL (shared library) that is speeding up all your network transfers is using some resources to achieve that speedup.
You should know that it is allocating larger and larger buffers before you start trying to hunt down the source of your mysteriously disappearing memory.
Then you can realize that you need to use the more complicated interface to the DLL that restricts resource usage rather than a simple and convenient interface.
And you will have realized this before doing extensive (and useless) object profiling because you would have been trying to determine why your application is being a memory hog.
When benchmarking third-party components, you need to apply a good simulation of exactly how you will use those products.
Determine characteristics from your benchmarks and put the numbers into your overall model to determine if performance can be reached.
Be aware that vendor benchmarks are typically useless for a particular application.
Break your application down into a hugely simplified version for a preliminary benchmark implementation to test third-party components.
You should make a strong attempt to include all the scaling necessary so that you are benchmarking a fully scaled usage of the components, not some reduced version that reveals little about the components in full use.
Specify target response times for as much of the system as possible.
Specify and use a benchmark suite based on real user behavior.
Agree on all target times with users, customers, managers, etc., before tuning.
Make your benchmarks long enough: over five seconds is a good target.
Use elapsed time (wall-clock time) for the primary time measurements.
Ensure the benchmark harness does not interfere with the performance of the application.
Run benchmarks before starting tuning, and again after each tuning exercise.
Take care that you are not measuring artificial situations, such as full caches containing exactly the data needed for the test.
Break down distributed application measurements into components, transfer layers, and network transfer times.
Tune systematically: understand what affects the performance; define targets; tune; monitor and redefine targets when necessary.
Approach tuning scientifically: measure performance; identify bottlenecks; hypothesize on causes; test hypothesis; make changes; measure improved performance.
Determine which resources are limiting performance: CPU, memory, or I/O.
Accurately identify the causes of the performance problems before trying to tune them.
Use the strategy of identifying the main bottlenecks, fixing the easiest, then repeating.
Improve a CPU limitation with faster code, better algorithms, and fewer short-lived objects.
Improve a system-memory limitation by using fewer objects or smaller long-lived objects.
Improve I/O limitations by targeted redesigns or speeding up I/O, perhaps by multithreading the I/O.
Work with user expectations to provide the appearance of better performance.
Hold back releasing tuning improvements until there is at least a 20% improvement in response times.
Avoid giving users a false expectation that a task will be finished sooner than it will.
Bear in mind that users perceive the mean response time as the actual 90th percentile value of the response times.
The interface should not be dead for more than two seconds when carrying out tasks.
Provide the ability to abort or carry on alternative tasks.
Provide partial data for viewing as soon as possible, without waiting for all requested data to be received.
Cache locally items that may be looked at again or recalculated.
If you only have a hammer, you tend to see every problem as a nail.
Before you can tune your application, you need tools that will help you find the bottlenecks in the code.
I have used many different tools for performance tuning, and so far I have found the commercially available profilers to be the most useful.
You can easily find several of these, together with reviews, by searching the Web using "java+optimi" and "java+profile" as your search term or by checking various computer magazines.
These tools are usually available free for an evaluation period, and you can quickly tell which you prefer using.
If your budget covers it, it is worth getting several profilers: they often have complementary features and provide different details about the running code.
All profilers have some weaknesses, especially when you want to customize them to focus on particular aspects of the application.
Another general problem with profilers is that they frequently fail to work in nonstandard environments.
Nonstandard environments should be rare, considering Java's emphasis on standardization, but most profiling tools work at the VM level, and there is not currently a VM profiling standard,[1] so incompatibilities do occur.
Even if a VM profiling standard is finalized, I expect there will be some nonstandard VMs you may have to use, possibly a specialized VM of some sort—there are already many of these.
The Java Virtual Machine Profiler Interface (JVMPI) was introduced in 1.2, but it is only experimental and subject to change, and looks like it will stay that way officially.
Where a particular VM offers extra APIs that tell you about some running characteristics of your application, these custom tools are essential to access those extra APIs.
Using a professional profiler and the proprietary tools covered in this chapter, you will have enough information to figure out where problems lie and how to resolve them.
When necessary, you can successfully tune without a professional profiler, as the Sun VM contains a basic profiler, which I cover in this chapter.
However, this option is not ideal for the most rapid tuning.
From JDK 1.2, Java specifies a VM-level interface, consisting of C function calls, that allows some external control over the VM.
These calls provide monitoring and control over events in the VM, allowing an application to query the VM and to be notified about thread activity, object creation, garbage collection, method call stack, etc.
The interface is intended to standardize the calls to the VM made by a profiler, so any profiler works with any VM that supports the JVMPI standard.
However, in JDK 1.2, the JVMPI is experimental and subject to change.
In addition to Java-specific profilers, there are other more generic tools that can be useful for profiling:
J2EE server-side monitors, useful to monitor server performance both in development and in production.
Network packet sniffers (both hardware and software types, e.g., netstat)
System performance measuring utilities (vmstat, iostat, sar, top on Unix; the task manager and performance monitor on Windows)
When looking at timings, be aware that different tools affect the performance of applications in different ways.
The degree of slowdown can vary from a few percent to a few hundred percent.
Another variation on timing the application depends on the underlying operating system.
The operating system can allocate different priorities for different processes, and these priorities determine the importance the operating system applies to a particular process.
This in turn affects the amount of CPU time allocated to a particular process compared to other processes.
Furthermore, these priorities can change over the lifetime of the process.
It is usual for server operating systems to gradually decrease the priority of a process over that process's lifetime.
This means that the process has shorter periods of the CPU allocated to it before it is put back in the runnable queue.
An adaptive VM (like Sun's HotSpot) can give you the reverse situation, speeding up code shortly after it has started running (see Section 3.7)
Whether or not a process runs in the foreground can also be important.
This ensures that the window currently being worked on is maximally responsive.
However, if you start a test and then put it in the background so that you can do something else while it runs, the measured times can be very different from the results you would get if you left that test running in the foreground.
This applies even if you do not actually do anything else while the test is running in the background.
Similarly, on server machines, certain processes may be allocated maximum priority (for example, Windows NT and 2000 server version, as well as most Unix server configured machines, allocate maximum priority to network I/O processes)
Any other configuration implies some overhead added to timings, and you must be aware of this.
As long as you are aware of any extra overhead, you can usually determine whether a particular measurement is relevant or not.
Most profiles provide useful relative timings, and you are usually better off ignoring the absolute times when looking at profile results.
Be careful when comparing absolute times run under different conditions, e.g., with and without a profiler, in the foreground versus in the background, on a very lightly loaded server (for example, in the evening) compared to a moderately loaded one (during the day)
There will be effects from caches in the hardware, in the operating system, across various points in a network, and in the application.
Starting the application for the first time on a newly booted system usually gives different timings as compared to starting for the first time on a system that has been running for a while, and both give different timings compared to an application that has been run several times previously on the system.
All these variations need to be considered, and a consistent test scenario used.
Typically, you need to manage the caches in the application, perhaps explicitly emptying (or filling) them for each test run to get repeatable results.
The other caches are difficult to manipulate, and you should try to approximate the targeted running environment as closely as possible, rather than testing each possible variation in the environment.
The Java runtime system normally includes a garbage collector.[2] Some of the commercial profilers provide statistics showing what the garbage collector is doing.
You can also use the -verbosegc option with the VM.
This option prints out time and space values for objects reclaimed and space recycled as the reclamations occur.
Printing directly to a file is slightly more efficient than redirecting the VM output to a file because the direct file write buffering is slightly more efficient than the piped redirect buffering.
The printout includes explicit synchronous calls to the garbage collector (using System.gc( ) ) as well as asynchronous executions of the garbage collector, as occurs in normal operation when free memory available to the VM gets low.
All objects may have to fit into memory without any garbage collection for these runtimes.
System.gc( ) does not necessarily force a synchronous garbage collection.
Instead, the gc( ) call is really a hint to the runtime that now is a good time to run the garbage collector.
The runtime decides whether to execute the garbage collection at that time and what type of garbage collection to run.
It is worth looking at some output from running with -verbosegc.
The following code fragment creates lots of objects to force the garbage collector to work, and also includes some synchronous calls to the garbage collector:
Note that -verbosegc can also work with applets by using this command line:
As you can see, each time the garbage collector kicks in, it produces a report of its activities.
Any one garbage collection reports on the times taken by the various parts of the garbage collector and specifies what the garbage collector is doing.
Note that the internal times reported by the garbage collector are not the full time taken for the whole activity.
However, these times include the times taken to output the printed statements from the garbage collector and are therefore higher times than those for the garbage collection alone.
To see the pure synchronous garbage-collection times for this code fragment, you need to run the program without the verbosegc option.
In the previous examples, the garbage collector kicks in either because it has been called by the code fragment or because creating an object from the code fragment (or the runtime initialization) encounters a lack of free memory from which to allocate space for that object.
Some garbage-collector versions appear to execute their garbage collections faster than others.
But be aware that this time difference may be an artifact: it can be caused by the different number of printed statements when using the -verbosegc option.
When run without the -verbosegc option, the times may be similar.
The garbage collector from JDK 1.2 executes a more complex scavenging algorithm than earlier JDK versions to smooth out the effects of garbage collection running in the background.
The garbage-collection algorithm is discussed in Chapter 3 along with tuning the heap.
The garbage collection algorithm can be altered a little, and garbage-collection statistics can give you important information about objects being reclaimed, which helps you tune your application.
Overall, Java 2 applications do seem to have faster object recycling in application contexts than previous JDK versions did.
It is worthwhile running your application using the -verbosegc option to see how often the garbage collector kicks in.
At the same time, you should use all logging and tracing options available with your application so that the output from the garbage collector is set in the context of your application activities.
Garbagecollection statistics can be collected and summarized in a useful way.
The 1.2 output is relatively easy to understand; the important lines are those summarizing the statistics, e.g.:
The numbers before and after the arrow show the amount of space taken by objects before and after the garbage collection.
The following number in parentheses is the total available heap space, and the remaining number shows the time taken to execute the garbage collection.
The important items that all -verbosegc output has in common are:
The number of bytes reclaimed by the garbage collection (either listed directly, or deduced by subtracting the before and after used values)
Having the number of objects reclaimed would also be nice, but not all output lists that.
First, the heap size gives you a good idea of how much memory the application needs and helps you to tune the heap.
By running the output through a pattern matcher to extract the GC times and amount freed and totalling those values, you can identify the cost of GC to your application.
I like to send some output from the application to indicate when the application finished the initialization stage and started running; then I can filter lines from that point until when I terminate my test.
Let's look at an example now of how to calculate the GC impact on an application.
This particular test is a server application, which I run for 40 minutes, then stop.
Taking the logged -verbosegc output, I eliminate any log statements before and after the test run (identified by lines emitted by the application)
Then I run a pattern matcher against the -verbosegc logs.
The pattern matcher can be Awk, Perl, Java regular expressions, or any pattern matcher you prefer.
The matching is very simple; for example, here is a simple matcher in Perl for the 1.2 output:
If you don't know the exact number of objects being churned, you can estimate it using a standard average object size of 50 bytes.
In this case we have an object churn rate of 2.8 million objects per minute.
For comparison, the actual recorded number of objects churned was 107 million.
Of course, these values need to be taken in the context of the application.
The other churn values can be considered only in the context of the application, taking into account what you are doing.
The calculation I just made of GC percentage time is actually only partially complete.
To be completely accurate, you also need to factor in how much load the application and GC put on the CPU, which would.
In this case, the server ran on a single-processor system.
Again, a few simple calculations show exactly what is happening:
These results reveal that GC is taking way too much time, but at least the metric is accurate.
The calculation outlined here is fairly simple and can be made with a minimum of tools.
It is also easily altered to handle different output formats of -verbosegc.
In addition to performing these calculations yourself, several tools are available to analyze -verbosegc output:
Ken Gottry's JavaWorld article[4] gives a nice description of the HotSpot generational garbage collection and includes an awk script for generating an analysis of -verbosegc logs.
A Sun Wireless Developer article[5] from Nagarajayya and Mayer provides a very detailed discussion of fine-tuning a heap.
It includes a GC analyzer tool[6] for generating reports from -verbosegc logs.
GCViewer is a graphical tool that allows you to visualize GC logging output.
GCViewer can also export data in the proper format for further manipulation in a spreadsheet.
Most profiling tools provide a profile of method calls, showing where the bottlenecks in your code are and helping you decide where to target your efforts.
By showing which methods and lines take the most time, a good profiling tool can quickly pinpoint bottlenecks.
Most method profilers work by sampling the call stack at regular intervals and recording the methods on the stack.[7] This regular snapshot identifies the method currently being executed (the method at the top of the stack) and all the methods below, to the program's entry point.
By accumulating the number of hits on each method, the resulting profile usually identifies where the program is spending most of its time.
However, this is a sampling technique , so it is not foolproof: methods can be missed altogether or have their weighting misrecorded if some of their execution calls are missed.
Any reasonably long test (i.e., seconds rather than milliseconds) normally gives correct results.
A variety of profiling metrics, including the way different metrics can be used, are reported in "A unifying approach to performance analysis in the Java environment" by Alexander, Berry, Levine, and Urquhart, IBM Systems Journal, Vol.
The profiler must also ensure that it has a coherent stack state, so the call must be synchronized across stack activities, possibly by temporarily stopping the thread.
The profiler also needs to make sure that multiple threads are treated consistently and that the timing involved in its activities is accounted for without distorting the regular sample time.
Also, too short a sample interval causes the program to become extremely slow, while too long an interval results in many method calls being missed and misrepresentative profile results being generated.
The JDK comes with a minimal profiler, obtained by running a program using the java executable with the Xrunhprof option (-prof before JDK 1.2, -Xprof with HotSpot)
This option produces a profile data file called java.hprof.txt (java.prof before 1.2)
When using a method profiler, the most useful technique is to target the top five to ten methods and choose the quickest to fix.
The reason for this is that once you make one change, the profile tends to be different the next time, sometimes markedly so.
This way, you can get the quickest speedup for a given effort.
However, it is also important to consider what you are changing so you know what your results are.
Similarly, if you have a method that takes 10% of the time but is called a huge number of times, with each individual method call being quite short, you are less likely to speed up the method.
On the other hand, if you can eliminate some significant fraction of the calling methods (the methods that call the method that takes 10% of the time), you might gain speed that way.
Let's look at the profile output from a short program that repeatedly converts some numbers to strings and inserts them into a hash table:
The methods are ranked according to the percentage of time they take.
Note that the trace does not identify actual method signatures, only method names.
The samples that count toward a particular method's execution time are those where the method itself is executing at the time of the sample.
If method foo( ) were calling another method when the sample was taken, that other method would be at the top of the stack instead of foo( )
So you do not need to worry about the distinction between foo( )'s execution time and the time spent executing foo( )'s callees.
Only the method at the top of the stack is tallied.
The fourth method in the list takes 3.10% of the time, so clearly you need look no further than the top three methods to optimize the program.
The methods ranked first, third, and fourth are the same method, possibly called in different ways.
Trace 214 is a truncated entry, and is probably the same stack as the other two (these differences highlight the fact that the JDK profiler sometimes loses information)
All three entries refer to the same stack: an inferred call from the StringBuffer to append a double, which calls String.valueOf( ), which calls Double.toString( ), which in turn creates a FloatingDecimal object.
These are also the actual names for constructors and static initializers in the class file.
FloatingDecimal is private to the java.lang package, which handles most of the logic involved in converting floating-point numbers.
FloatingDecimal stores the digits of the floating-point number as an array of chars when the FloatingDecimal is created; no strings are created until the floating-point number is converted to a string.
Since this stack includes a call to a constructor, it is worth checking the object-creation profile to see whether you are generating an excessive number of objects; object creation is expensive, and a method that generates many new objects is often a performance bottleneck.
I show the object-creation profile and how to generate it later in this chapter under Section 2.4
The object-creation profile shows that a large number of extra objects are being created, including a large number of FDBigInt objects that are created by the new FloatingDecimal objects.
Almost any improvement in this one method translates directly to a similar improvement in the overall program.
However, normally only Sun can modify this method, and even if you want to modify it, it is long and complicated and takes an excessive amount of time to optimize unless you are already familiar with both floating-point binary representation and converting that representation to a string format.
The trace corresponding to this second entry in the summary example turns out to be another truncated trace, but the example shows the same method in 14th position, and the trace for that entry identifies the Double.equals( ) call as coming from the Hashtable.put( ) call.
Unfortunately for tuning purposes, the Double.equals( ) method itself is already quite fast and cannot be optimized further.
When methods cannot be directly optimized, the next best choice is to reduce the number of times they are called or even avoiding the methods altogether.
In fact, eliminating method calls is actually the better tuning choice, but it is often considerably more difficult to achieve and so is not a first-choice tactic for optimization.
The object-creation profile and the method profile together point to the FloatingDecimal class as being a huge bottleneck, so avoiding this class is the obvious tuning tactic here.
In Chapter 5, I employ this technique, avoiding the default call through the FloatingDecimal class for the case of converting floatingpoint numbers to Strings, and I obtain an order-of-magnitude improvement.
Basically, the strategy is to create a more efficient routine to run the equivalent conversion functionality and then replace the calls to the underperforming FloatingDecimal methods with calls to more efficient optimized methods.
The 1.1 profiling output is quite different and much less like a standard profiler's output.
Since the "callee" for these methods is listed as System.gc( ), this also indicates that the methods are significantly involved in memory creation, and suggests that the next.
The best way to avoid the Double.equals( ) method is to replace the hash table with another implementation that stores double primitive data types directly rather than requiring the doubles to be wrapped in a Double object.
This allows the = = operator to make the comparison in the put( ) method, thus completely avoiding the Double.equals( ) call.
This is another standard tuning tactic, replacing a data structure with one more appropriate and faster for the task.
The default profile output gained from executing with -Xrunhprof in Java 2 is not useful for method profiling.
The default output generates object-creation statistics from the heap as the dump (output) occurs.
By default, the dump occurs when the application terminates; you can modify the dump time by typing Ctrl-\ on Solaris and other Unix systems, or Ctrl-Break on Windows.
To get a useful method profile, you need to modify the profiler options to specify method profiling.
The full list of options available with -Xrunhprof can be viewed using the -Xrunhprof:help option.
This can be quite confusing if you are not expecting it.
The profiling option in JDKs 1.2/1.3/1.4 can be pretty flaky.
Several of the options can cause the runtime to crash (core dump)
The output is a large file because huge amounts of trace data are written rather than summarized.
Since the profile option is essentially a Sun engineering tool, it is pretty rough, especially since Sun has a separate (not free) profile tool for its engineers.
Another tool that Sun provides to analyze the output of the profiler is the Heap Analysis Tool (search http://www.java.sun.com for "HAT")
But this tool analyzes only the object-creation statistics output gained with the default profile output, so it is not that useful for method profiling (see Section 2.4 for slightly more about this tool)
Nevertheless, I expect the free profiling option to stabilize and be more useful in future versions.
The output when run with the options already listed (cpu=samples, thread=y) already results in fairly usable information.
Each unique stack trace provides a TRACE entry in the second section of the file, describing the method calls on the stack for that trace.
Multiple identical samples are not listed; instead, the number of their "hits" is summarized in the third section of the file.
The profile output file in this mode has three sections:
A standard header section describing possible monitored entries in the file.
For example: WARNING!  This file format is under development, and is subject to change without notice.
Individual entries describing monitored events, i.e., threads starting and terminating, but mainly sampled stack traces.
A summary table of methods ranked by the number of times the unique stack trace for that method appears.
Section 3 is the place to start when analyzing this profile output.
More accurately, this field reports the percentage of samples that have the stack given by the trace field.
It indicates that this method was probably executing for about 11.55% of the application execution time because the samples are at regular intervals.
This TRACE entry clearly identifies the exact method and its caller.
Note that the stack is reported to a depth of four methods.
This field is a running additive total of all the self field percentages as you go down the table.
This field indicates how many times the unique stack trace that gave rise to this entry was sampled while the program ran.
This field shows the unique trace identifier from the second section of profile output that generated this entry.
The trace is recorded only once in the second section no matter how many times it is sampled; the number of times that this trace has been sampled is listed in the count field.
This summary table lists only the method name and not its argument types.
Therefore, it is frequently necessary to refer to the stack itself to determine the exact method if the method is an overloaded method with several possible argument types.
The stack is given by the trace identifier in the trace field, which in turn references the trace from the second section of the profile output.
If a method is called in different ways, it may also give rise to different stack traces.
Sometimes the same method call can be listed in different stack traces due to lost information.
Each of these different stack traces results in a different entry in the third section of the profiler's output, even though the method field is the same.
For example, it is perfectly possible to see several lines with the same method field, as in the following table segment:
Note that the trace does not identify actual method signatures, only method names.
Line numbers are given if the class was compiled so that line numbers remain.
The profiler in this mode (cpu=samples) suffices when you have no better alternative.
It does have an effect on real measured times, slowing down operations by variable amounts even within one application run.
But it normally indicates major bottlenecks, although sometimes a little extra work is necessary to sort out multiple identical method-name references.
Using the alternative cpu=times mode, the profile output gives a different view of application execution.
In this mode, the method times are measured from method entry to method exit, including the time spent in all other calls the method makes.
This profile of an application provides a tree-like view of where the application is spending its time.
Some developers are more comfortable with this mode for profiling the application, but I find that it does not directly identify bottlenecks in the code.
HotSpot does not support the standard Java 2 profiler detailed in the previous section; it supports a separate profiler using the -Xprof option.
The HotSpot profiler has no further options available to modify its behavior; it works by sampling the stack every 10 milliseconds.
The output, printed to standard output, consists of a number of sections.
Each section lists entries in order of the number of ticks counted while the method was executed.
The various sections include methods executing in interpreted and compiled modes, and VM runtime costs as well:
A list of methods sampled while running in interpreted mode.
The methods are listed in order of the total number of ticks counted while the method was at the top of the stack.
A list of methods sampled while running in compiled mode.
The methods are listed in order of the total number of ticks counted while the method was at the top of the stack.
A list of external (non-Java) method stubs, defined using the native keyword.
Listed in order of the total number of ticks counted while the method was at the top of the stack.
Listed in order of the total number of ticks counted while the method was at the top of the stack.
This includes ticks from the garbage collector, thread-locking overhead, and other miscellaneous entries:
The entries at the top of Section 3 are the methods that probably need tuning.
Such methods may still need to be optimized, but it is more likely that the methods at the top of Section 3 need optimizing.
The ticks for the two sections are the same, so you can easily compare the time taken up by the top methods in the different sections and decide which to target.
This output format is supported in Java 2, using the cpu=old variation of the -Xrunhprof option.
The method profile table showing cumulative times spent in each method executed.
The line reports the number of handles and the number of bytes used by the heap memory storage over the application's lifetime.
The number of handles used is the maximum number of objects that existed at any one time in the application (handles are recycled by the garbage collector, so over its lifetime the application could have used many more objects than are listed)
Reports the number of primitive data type arrays left at the end of the process, just before process termination.
The first field is the primitive data type (array dimensions and data type given by letter codes listed shortly), the second field is the number of arrays, and the third is the total number of bytes used by all the arrays.
The reported data does not include any arrays that may have been garbage-collected before the end of the process.
You could use the -noasyncgc option to try to eliminate garbage collection (if you have enough memory; you may also need -mx with a large number to boost the maximum memory available)
If you do, also use -verbosegc so that if garbage collection is forced, you at least know that it has occurred and can get the basic number of objects and bytes reclaimed.
The fourth section of the profile output is the per-object memory dump.
Again, this includes only objects left at the end of the process just before termination, not objects that may have been garbage-collected before the end of the process.
This dump is a snapshot of the actual object table.
The fields in the first line of an entry are:
Internal memory locations for the instance and class; of no use for performance tuning.
Internal memory locations for the instance and class; of no use for performance tuning.
The number of instances of the class reported on the next line.
The number of instances of arrays of the class reported on the next line.
The total number of array elements for all the arrays counted in the previous (ac) field.
That leaves in the object table all the objects that are rooted[10] by the system and by your application (from static variables)
If this snapshot shows significantly more objects than you expect, you may be referencing more objects than you realized.
Objects rooted by the system are objects that the JVM runtime keeps alive as part of its runtime system.
Rooted objects generally cannot be garbage-collected because they are referenced in some way from other objects that cannot be garbagecollected.
The first section of the profile output is the most useful.
The first line of this section specifies the four fields in the profile table in this section: count, callee, caller, and time.
The total number of times the callee method was called from the caller method, accumulating multiple executions of the caller method.
The actual reported numbers may be less than the true number of calls: the profiler can miss calls.
The callee can be listed in other entries as the callee method for different caller methods.
If each of the count calls in one line took exactly the same amount of time, then one call from caller to callee took time divided by count milliseconds.
However, for this profiler, the time spent in methods tends to be more useful.
Because the times in the time field include the total time that the callee method was anywhere on the stack, interpreting the output of complex programs can be difficult without processing the table to subtract subcall times.
The lines in the profile output are unique for each callee-caller pair, but any one callee method and any one caller method can (and normally do) appear in multiple lines.
This is because any particular method can call many other methods, and so the method registers as the caller for multiple callee-caller pairs.
Any particular method can also be called by many other methods, and so the method registers as the callee for multiple callee-caller pairs.
The methods are written out using the internal Java syntax listed in Table 2-1
There are free viewers, including source code, for viewing this file:
The biggest drawback to the 1.1 profile output is that threads are not shown at all.
This means that it is possible to get time values for method calls that are longer than the total time spent in running the application, since all the call times from multiple threads are added together.
It also means that you cannot determine from which thread a particular method call was made.
Nevertheless, after re-sorting the section on the time field rather than the count field, the profile data is useful enough to suffice as a method profiler when you have no better alternative.
One problem I've encountered is the limited size of the list of methods that can be held by the internal profiler.
Technically, this limitation is 10,001 entries in the profile table, and there is presumably one entry per method.
There are four methods that help you avoid the limitation by profiling only a small section of your code:
The -Xhprof option seems to be simply an alternative name for the -Xrunhprof option.
I believe that originally it was called -Xhprof, and then the option was left in although all subsequent documentation used -Xrunhprof.
It prints the number and total size of instances allocated per class, including array classes, accumulating instances across all threads and creation points.
In fact, it seems to be very similar to the tool I describe in the next section.
Nevertheless, it is useful when it works, and it was introduced with 1.3
The primitive arrays are listed using the one-character labels from Table 2-1
All instances created at any time by the VM are included, whether they have been garbage-collected or not.
The first column is the total size in bytes taken up by all the instances summed together; the second column provides the number of those instances created; and the third divides the first column by the second column to give an average size per object in bytes.
The only disadvantage seems to be that you cannot take a snapshot.
Otherwise, this is another useful tool to add to your armory.
Most profile tool vendors provide much better object-creation statistics, determining object numbers and identifying where particular objects are created in the code.
My recommendation is to use a better (probably commercial) tool in place of the SDK profiler.
The Heap Analysis Tool, which can analyze the default profiling mode with Java 2, provides a little more information from the profiler output, but if you are relying on this, profiling object creation will require a lot of effort.
To use this tool, you must use the binary output option:
This is not a supported feature, but it does seem to work on most systems because all constructors chain up to the Object class's constructor, and any explicitly created nonarray object calls the constructor in Object as its first execution point after the VM allocates the object on the heap.
Objects that are created implicitly with a call to clone( ) or by deserialization do not call the Object class's constructor, and so are missed when using this technique.
Under the terms of the license granted by Sun, it is not possible to include or list an altered Object class with this book.
The change requires adding a line in the Object constructor to pass this to some object-creation monitor you are using.
For any class other than Object, that is all you need to do.
But there is an added problem in that Object does not have a superclass, and the compiler has a problem with this: the compiler cannot handle an explicit super( ) from the Object class, nor the use of this, without an explicit super( ) or this( ) call.
This trick works for the compiler that comes with the JDK; other compilers may be easier or more difficult to satisfy.
Generating the bytecodes without the extra constructor is perfectly legal.
Recursive calls to the Object constructor present an additional difficulty.
You must ensure that when your monitor is called from the constructor, the Object constructor does not recursively call itself as it creates objects for your object-creation monitor.
It is equally important to avoid recursive calls to the Object constructor at runtime initialization.
The simplest way to handle all this is to have a flag on which objects are conditionally passed to the monitor from the Object constructor and to have this flag in a simple class with no superclasses so that classloading does not impose extra calls to superclasses.
Actually, the object is passed immediately after it has been created by the runtime system but before any constructors have been executed, except for the Object constructor.
You should not set the monitoring variable to true before the core Java classes have loaded; a good place to set it to true is at the start of the application.
Unfortunately, this technique does not catch any of the arrays that are created: array objects do not chain through the Object constructor (although Object is their superclass) and so do not get monitored.
But you typically populate arrays with objects (except for data type arrays such as char arrays), and the objects populating the arrays are caught.
In addition, objects that are created implicitly with a call to clone( ) or by deserialization do not call the Object class's constructor, and so these objects are also missed when using this technique.
Deserialized objects can be included using a similar technique by redefining the ObjectInputStream class.
When I use this technique, I normally first get a listing of the different object types and how many of each are created.
If you prefer, you can make the technique more focused by altering other constructors to target a specific hierarchy below Object.
Or you could focus on particular classes within a more general monitoring class by filtering interesting hierarchies using instanceof.
In addition, you can get the stack of the creation call for any object by creating an exception or filling in the stack trace of an existing exception (but not throwing the exception)
As an example, I define a monitoring class that provides many of the possibilities you might want to use for analysis.
The files listed in the -Xbootclasspath option can be listed with relative or absolute paths; they do not have to be in the current directory.
Note that classloaders seem to be changed in almost every version of the SDK.
Some readers have had problems using the tool described here with Version 1.3
If you have problems, try unpacking everything and putting it all in the bootclasspath, including the application classes, for the purposes of running this monitoring tool.
That way, there are no issues of classpath or classloading.
Even the String objects are overheads: there is no requirement for the numbers to be converted to Strings before they are appended to the Stringbuffer.
In Chapter 5, I show how to convert numbers and avoid creating all these intermediate objects.
Implementing the optimizations mentioned at the end of Section 2.3.1 allows the program to avoid the FloatingDecimal class (and consequently the FDBigInt class too) and also to avoid the object wrappers for the doubles and longs.
This results in a program that avoids all the temporary FloatingDecimal, Double, Long, FDBigInt, and String objects generated by the original version: over a quarter of a million objects are eliminated from the object-creation profile, leaving just a few dozen objects! So the order-of-magnitude improvement in speed attained is now more understandable.
Within this memory allocation, the VM manages its objects and data.
Some of this allocated memory is held in reserve for creating new objects.
When the currently allocated memory gets filled and the garbage collector cannot allocate sufficiently more memory, the VM requests more memory from the underlying system.
If the underlying system cannot allocate any further memory, an OutOfMemoryError error is thrown.
Total memory can go up and down; some Java runtimes return sections of unused memory to the underlying system while still running.
The free memory increases when a garbage collection successfully reclaims space used by dead objects, and also increases when the Java runtime requests more memory from the underlying operating system.
The free memory reduces each time an object is created and when the runtime returns memory to the underlying system.
This method simply gives the -Xmx value, and is of no use to monitor heap usage.
It can be useful to monitor memory usage while an application runs: you can get a good feel for the hotspots of your application.
You may be surprised to see steady decrements in the free memory available to your application when you were not expecting any change.
This can occur when you continuously generate temporary objects from some routine; manipulating graphical elements frequently shows this behavior.
Monitoring memory with freeMemory( ) and totalMemory( ) is straightforward, and I include a simple class that does this graphically.
It creates three threads: one to periodically sample the memory, one to maintain a display of the memory usage graph, and one to run the program you are monitoring.
Figure 2-1 shows the memory monitor after monitoring a run of the ProfileTest class.
The total memory allocation is flat because the class did not hold on to much memory at any one time.
The free memory shows the typical sawtooth pattern of an application cycling through temporary objects: each upstroke is where the garbage collector kicked in and freed up the space being taken by the discarded dead objects.
Here are the classes for the memory monitor, together with comments:
To tune client/server or distributed applications, you need to identify all communications that occur during execution.
The most important factors to look for are the number of transfers of incoming and outgoing data and the amount of data transferred.
Generally, if the amount of data per transfer is less than about one kilobyte, the number of transfers is the factor that limits performance.
If the amount of data being transferred is more than about a third of the network's capacity, the amount of data is the factor limiting performance.
Between these two endpoints, either the amount of data or the number of transfers can limit performance, although in general, the number of transfers is more likely to be the problem.
As an example, web surfing with a browser typically hits both problems at different times.
A complex page with elements from multiple sites can take longer to display completely than one simple page with 10 times more data.
Many different sites are involved in displaying the complex page; each site must have its server name converted to an IP address, which can take many network transfers.[12] Each site then needs to be connected to and downloaded from.
The simple page needs only one name lookup and one connection, and this can make a huge difference.
On the other hand, if the amount of data is large compared to the connection bandwidth (the speed of the Internet connection at the slowest link between your client and the server machine), the limiting factor is bandwidth, so the complex page may display more quickly than the simple page.
The DNS name lookup is often hierarchical, requiring multiple DNS servers to chain a lookup request to resolve successive parts of the name.
Although there is only one request as far as the browser is concerned, the actual request may require several serverto-server data transfers before the lookup is resolved.
Several generic tools are available for monitoring communication traffic, all aimed at system and network administrators (and quite expensive)
I know of no general-purpose profiling tool targeted at application-level communications monitoring; normally, developers put their own monitoring capabilities into the application or use the trace mode in their third-party communications package, if they use one.
If you are using a third-party communications package, your first step in profiling is to make sure you understand how to use the full capabilities of its tracing mode.
Most communications packages provide a trace mode to log various levels of communication details.
Some let you install your own socket layer underlying the communications; this feature, though not usually present for logging purposes, can be quite handy for customizing communications tracing.
For example, RMI (remote method invocation) offers very basic call tracing enabled by setting the java.
The RMI framework also lets you install a custom RMI socket factory.
This socket customization support is provided so that the RMI protocol is abstracted away from actual communication details, and it allows sockets to be replaced by alternatives such as nonsocket communications or encrypted or compressed data transfers.
For example, here is the tracing from a small client/server RMI application.
The client simply connects to the server and sets three attributes of a server object using RMI.
An alternative way to trace communications is to replace the sockets (or other underlying communication classes) directly, providing your own logging.
In the next section, I provide details for replacing socket-level communication for basic Java sockets.
In addition to Java-level logging, you should be familiar with system- and network-level logging facilities.
The most ubiquitous of these is netstat, a command-line utility that is normally executed from a Unix shell or Windows command prompt.
For example, using netstat with the -s option provides a full dump of most network-related structures (cumulative readings since the machine was started)
By filtering this, taking differences, and plotting various data, you get a good idea of the network traffic background and the extra load imposed by your application.
Using netstat with this application shows that the connection, the resolution of the server object, and the three remote method invocations require four TCP sockets and 39 packets of data (frames) to be transferred.
These include a socket pair opened from the client to the registry to determine the server location, then a second socket pair between the client and the server.
The frames include several handshake packets required as part of the RMI protocol, and other overhead that RMI imposes.
The socket pair between the registry and server are not recorded because the pair lives longer than the interval that measures differences recorded by netstat.
However, some of the frames are probably communication packets between the registry and the server.
This is a hardware device you plug into the network line that views (and can save) all network traffic that is passed along that wire.
If you absolutely must know every detail of what is happening on the wire, you may need one of these.
More detailed information on network utilities and tools can be found in system-specific performance tuning books (see Chapter 14 for more about system-specific tools and tuning tips)
Occasionally, you need to be able to see what is happening to your sockets and to know what information is passing through them and the size of the packets being transferred.
It is usually best to install your own trace points into the application for all communication external to the application; the extra overhead is generally small compared to network (or any I/O) overhead and can usually be ignored.
The application can be deployed with these tracers in place but configured so as not to trace (until required)
However, the sockets are often used by third-party classes, and you cannot directly wrap the reads and writes.
A more useful possibility I have employed is to wrap the socket I/O with my own classes.
You can almost do this generically using the SocketImplFactory, but if you install your own SocketImplFactory, there is no protocol to allow you to access the default socket implementation, so another way must be used.
You could add a SocketImplFactory class into java.net, which then gives you access to the default PlainSocketImpl class, but this is no more generic than the previous possibility, as it too cannot normally be delivered with an application.
This is simpler than the previous alternatives and can be quite powerful.
Only two methods from the core classes need changing, namely those that provide access to the input stream and output stream.
You need to create your own input stream and output stream wrapper classes to provide logging.
Use system- and network-level monitoring utilities to assist when measuring performance.
Run tests on unloaded systems with the test running in the foreground.
Never use the timings obtained from a profiler as absolute times.
The better your tools, the faster and more effective your tuning.
Pinpoint the bottlenecks in the application: with profilers, by instrumenting code (putting in explicit timing statements), and by analyzing the code.
Target the top five to ten methods, and choose the quickest to fix.
Speed up the bottleneck methods that can be fixed the quickest.
Improve the method directly when the method takes a significant percentage of time and is not called too often.
Reduce the number of times a method is called when the method takes a significant percentage of time and is also called frequently.
Use an object-creation profiler together with garbage-collection statistics to determine which objects are created in large numbers and which large objects are created.
See if the garbage collector executes more often than you expect.
Check whether your communication layer supports the addition of customized layers.
Identify the number of incoming and outgoing transfers and the amounts of data transferred in distributed applications.
Throughout the progressive versions of Java, improvements have been made at all levels of the runtime system: in the garbage collector, in the code, in the VM handling of objects and threads, and in compiler optimizations.
It is always worthwhile to check your own application benchmarks against each version (and each vendor's version) of the Java system you try out.
Any differences in performance need to be identified and explained; if you can determine that a compiler from one version (or vendor) together with the runtime from another version (or vendor) speeds up your application, you may have the option of choosing the best of both worlds.
Standard Java benchmarks tend to be of limited use in deciding which VMs provide the best performance for your application.
You are always better off creating your own application benchmark suite for deciding which VM and compiler best suit your application.
The following sections identify some points to consider as you investigate different VMs, compilers, and JDK classes.
If you control the target Java runtime environment, i.e., with servlet and other server applications, more options are available to you.
The effects of the garbage collector can be difficult to determine accurately.
It is worth including some tests in your performance benchmark suite that are specifically arranged to identify these effects.
You can do this only in a general way, since the garbage collector is normally not under your control.
Sun does intend to introduce an API that will allow a pluggable garbage collector to replace the one delivered with the VM, but building your own garbage collector is not a realistic tuning option.
Using a pluggable third-party garbage collector doesn't give you control over the garbage collector either.
The basic way to see what the garbage collector is up to is to run with the -verbosegc option.
This prints out time and space values for objects reclaimed and space recycled.
The printout includes explicit synchronous calls to the garbage collector (using System.gc( )) as well as asynchronous executions of the garbage collector, as occurs in normal operation when free memory available to the VM gets low.
You can try to force the VM to execute only synchronous garbage collections by using the -noasyncgc option to the Java executable (no longer available from JDK 1.2)
The -noasyncgc option does not actually stop the garbage-collector thread from executing; it still executes if the VM runs out of free memory (as opposed to just getting low on memory)
Output from the garbage collector running with -verbosegc is detailed in Section 2.2
The garbage collector usually works by freeing the memory that becomes available from objects that are no longer referenced or, if this does not free sufficient space, expanding the available memory space by asking the operating system for more memory (up to a maximum specified to the VM with the -Xmx/-mx option)
The garbage collector's space-reclamation algorithm tends to change with each version of the JDK.
Sophisticated generational garbage collectors, which smooth out the impact of garbage collection, are now being used; HotSpot uses a state-of-the-art generational garbage collector.
Analysis of object-oriented programs has shown that most objects are short-lived, fewer have medium lifespans, and very few objects are long-lived.
Generational garbage collectors move objects through multiple spaces, each time copying live objects from one space to the next and reclaiming the space used by objects that are no longer alive.
By concentrating on short-lived objects—the early spaces—and spending less time recycling space where older objects live, the garbage collector frees the maximum amount of space for the lowest impact.[1]
One book giving more details on garbage collection is Inside the Java 2 Virtual Machine by Bill Venners (McGraw-Hill)
The garbage collection chapter is also available online at http://www.artima.com.
Because the garbage collector is different in different VM versions, the output from the -verbosegc option is also likely to change across versions, making it difficult to compare the effects of the garbage collectors across versions (not to mention between different vendors' VMs)
But you should still attempt this comparison, as the effect of the garbage collector can make a difference to the application.
Looking at garbage-collection output can tell you that parts of your application are causing significantly more work for the garbage collector, suggesting that you may want to alter the flow of objects in those parts.
Garbage collection is also affected by the number of threads and whether objects are shared across threads.
Expect to see improvements in threaded garbage collection over different VM versions.
Garbage-collection times may be affected by the size of the VM memory.
A larger memory implies there will be more objects in the heap space before the garbage collector needs to kick in.
This in turn means that the process of sweeping dead objects takes longer, as does the process of running through a larger object table.
Different VMs have optimal performance at different sizes, and the optimal size for any particular applicationVM pairing must be determined by trial and error.
First, we'll look at the big picture, with gross tuning steps that optimize the size of the heap, followed by advice for finetuning the heap.
Next, we'll look at the impact of shared memory on tuning the heap.
The VMs provided by most vendors include the two main heap tuning parameters: -mx/-Xmx and -ms/-Xms.
Respectively, these parameters set the maximum and starting sizes of the heap in bytes.
VMs vary as to whether they accept the -mx and -ms parameters or the -Xmx and -Xms parameters, or both.
They also vary about accepting a space between the number following the parameter and accepting shorthand notations of K and M for kilobytes and megabytes, e.g., -Xmx32M.
Tuning the heap with these two parameters requires trial and error, but is relatively simple.
You don't need to consider the exact garbage-collection algorithm or how different parameters might affect each other.
You can then simply alter the two parameters and remeasure using the same technique.
Typically, you might want to use a range of values for the maximum heap size, keeping the starting heap size either absolutely constant (e.g., 1 megabyte) or relatively constant (e.g., half the maximum heap), and graph the result, looking for where garbage collection has the minimum cost.
Note that GC activity can take hours to settle into a regular pattern.
If you are tuning a long-lived application, bear this in mind when looking at the GC output.
Gross heap tuning is fairly stable, in that moving to a different VM or tweaking the application usually won't invalidate the tuning parameters.
They may no longer be the most optimal sizes after such changes, but they should still be reasonable.
The heap size should not become so large that physical memory is swamped and the system has to start paging.
So keep the maximum heap size below the size of the physical memory (RAM) on the machine.
Also, subtract from the RAM the amount of memory required for other processes that will be running at the same time as the Java application, and keep the maximum heap size below that value.
A larger heap allows more objects to exist in memory before garbage collection is required to reclaim space for new objects.
However, a larger heap also makes the garbage collection last longer, as it needs to work through more objects.
In the absence of concurrent garbage collection (see the information about -Xconcgc in Section 3.4), a larger heap causes longer individual perceptible pauses, which may be undesirable.
You need to balance the pause times against the overall garbage-collection cost.
Using -Xincgc is an alternative that is also described in the section on fine tuning.
There are many different suggestions about what the starting heap should be compared to the maximum.
Set the starting heap size the same as the maximum heap size.
Set the starting heap size to the size needed by the maximum number of live objects (estimated or measured in trials), and set the maximum heap size to about four times this amount.
Set the starting heap size to half the maximum heap size.
Although there is no conclusive evidence that any of these suggestions represents the best approach in all situations, each has been shown to be appropriate in different applications.
Assuming you've worked out what the maximum heap size should be, then growing the JVM memory can be considered as pure overhead, requiring multiple system calls and resulting in segmented system memory allocation.
If you figure that you are going to get to the maximum heap anyway, then there is a good argument for simply starting out at the maximum heap (the first suggestion), thus avoiding the growth overhead as well as getting a heap that is less segmented.
However, this can mean longer pauses when garbage collection kicks in, so the system load might not be smoothed out as much as you'd want.
But a generational garbage collector will not necessarily suffer from this longer pause problem, as it specifically smooths out the GC load.
An alternative view is that there is this lovely garbage-collection system in the JVM, which will grow the JVM to be just as big as needed and no more, so why not let it do its job? This way, despite the overhead in growing the JVM, you will end up using the minimum resources and the GC should be optimizing what it does best, i.e., handling and maintaining memory.
With this argument, you set the starting heap to 1MB (the last suggestion) and the maximum as high as reasonable.
A combination of these two rationales might lead you to one of the intermediate recommendations.
For example, assuming that the maximum heap is an overestimate of the ultimate JVM size, then half the maximum could be a good starting point to minimize memory allocation and memory segmentation overhead while still giving the GC space to optimize memory usage.
When running benchmarks, some engineers try to manipulate the benchmark and heap size so that no garbage collection needs to occur during the run of the benchmark.
This is an idealized situation, but it may be appropriate if your application is expected to run for only a short period.
In any case, be aware that this may apply to benchmarks presented to you.
If you are loading classes indirectly (e.g., through Class.forName( ) or by J2EE automatic classloading), then classes can be repeatedly garbage-collected and reloaded.
In addition to the gross heap-tuning factors, a host of other parameters can be used for fine-tuning the VM heap.
These other factors are usually strongly dependent on the garbage-collection algorithm being used by the VM, and the parameters vary for different VMs and different versions of VMs.
In this section, I'll cover a few examples to give you an idea of the possibilities.
Note that every VM and every version of the VM is different, and you need to retune the system with any change for this level of fine-tuning.
Fine-tuning is probably worth doing only where every last microsecond is needed or for a really stable deployed system, i.e., one that needs no more development.
Note that the following sections refer to some of the internal heap areas used by the HotSpot generational garbage collector.
Generally, the total VM heap consists of permanent space (Perm), old space (Old), young space (Young), and scratch or survivor space (Scratch)
Parameters referring to "new" space (New), such as XX:NewSize, refer to the combination Young+Scratch.
Most garbage-collection algorithms do not immediately expand the heap if they need space to create more objects, even when the heap has not yet been expanded to the maximum allowable size.
Instead, there is usually a series of attempts to free up space by reclaiming objects, compacting objects, defragmenting the heap, and so on.
If all these attempts are exhausted, then the heap is expanded.
Several GC algorithms try to keep some free space available so that temporary objects can be created and reclaimed without expanding the heap.
Similarly, the XXMaxHeapFreeRatio parameter specifies when the heap should be contracted.
The Sun default is to try to keep the proportion of free space to living objects at each garbage collection within the 40%-70% range.
Otherwise, the next garbage collection will likely occur sooner than desired.
Once an application reaches a steady state, it has a fairly constant churn of objects (created and released)
If the minimum free heap ratio is a small value (e.g., 10%), then there is not much space for objects to churn in, and garbage collection occurs frequently.
On the other hand, if the minimum free heap ratio is a large value (e.g., 60%), then GC runs much less frequently, but the pause when it runs is longer.
Pauses that last too long create bad user perceptions of performance.
The primary technique to eliminate pauses is to identify which objects are being churned, causing the GC to activate, and try to minimize those objects.
The secondary option is to reduce the heap size so that garbage collection runs more often, but for shorter periods.
As an alternative, -Xincgc changes the garbage-collection algorithm to use an incremental or "train" garbage-collection algorithm.
The intention is to minimize individual pause time at the expense of overall GC time.
The train algorithm clusters objects that reference each other and collects these clusters individually.
The idea is to garbage-collect only a small fraction of the heap at a time, thus causing shorter pauses.
But the algorithm is more costly in CPU time and results in longer total GC time.
The concurrent garbage collector tries to avoid stopping the application threads by asynchronously executing as much of the garbage collection algorithm as possible.
Once again, this has a higher cost on the CPU, and total GC time is increased.
The concurrent garbage collector should be especially effective on multiprocessor machines.
Note that the garbage collector has always run in its own thread, but in order to access memory areas to run the garbage collection, it pauses application threads.
It is this pausing that the concurrent garbage collector aims to minimize.
Finally, if you need a large heap and want to decrease pause times, you can try altering the size of the "new" space with generational garbage collectors (all Java 2 VMs)
The "new" space (also called "Eden" or "the young generation") is the heap space where new objects are created.
If the objects are short-lived, they are also garbage-collected rapidly in the new space.
The longer pauses are usually caused by the garbage collections that run across the spaces outside the "new" space, i.e., garbage collection of the full heap, so the more objects that are churned in the new space, the better.
If you need a large heap, try increasing the new space.
Remember that new space is not one space, but internally consists of heap space plus scratch space (working areas for the algorithm)
The option XX:SurvivorRatio= sets the ratio of sizes between scratch space and new space (Eden)
As you can see, fine-tuning the heap is complex, and changes with every VM.
The promoteall option forces the GC to move any object that survives a GC in young space to be immediately moved to old space; otherwise, the GC may leave the object in young space for longer.
Since young-space garbage collection is faster, you might think that promoteall would decrease performance, but at least one in-depth test found that using promoteall improved performance (see http://wireless.java.sun.com/midp/articles/garbage/)
In addition, an additional -bestFitFirst option seems to improve concurrent GC.
The "best fit" refers to an internal free-list allocation policy that helps to.
Prior to Java 2, explicit calls to System.gc( ) could assist an application.
Garbage collection was pretty much an all-or-nothing affair, and often you knew better than the garbage collector when it was a good time to start garbage collecting.
But with the introduction of generational garbage collection, explicit calls to System.gc( ) become disruptive, possibly forcing a full mark-sweep of the heap when the generational garbage collector was doing just fine.
The default is one collection per minute (property values of 6000)
This may or may not be a good tuning option; it is intended for very large servers.
There is also an option to lock the heap in physical memory (see Section 14.3 and http://java.sun.com/docs/hotspot/ism.html for further details)
HotSpot stores as Java objects some of its own internal data structures, things like the internal representation of classes, methods, and fields.
These are stored in a separate area called the Perm Space.
If you are loading a truly huge number of classes, you may need to enlarge this space by using the -XX:MaxPermSize parameter.
Finalizers force objects to be promoted to old space and degrade the performance of the garbage collector.
Finalizers postpone garbage collection until the finalizer is run, adding yet more overhead to the GC algorithm.
There is no way to avoid this overhead apart from minimizing the use of finalization methods in the application.
If you are running multiple VMs on the same machine, you have the option of sharing some of the memory between them.
There is a proposal for VMs to share system memory automatically, and this is likely to happen in the future.
But currently (as of the 1.4 release), if you want to share memory between VM processes, you need to run multiple pseudoprocesses within one VM process.
The necessary techniques are actually quite complicated, as many subtle problems can arise when trying to run several applications in the same VM while keeping them independent of each other.
Fortunately, there is a free open source library called Echidna (available from http://www.javagroup.org/echidna/) that takes care of all the subtleties involved in running multiple applications independently within the same VM system process.
The library also provides several management tools to help use Echidna effectively.
If you want to know how Echidna works or need to use parts of the library within your project, I have written an article that covers the technology in some detail.[2]
The shared-memory advantages from combining multiple applications into one VM are significant for applications with small memory requirements where the VM memory overhead is significant by comparison.
But for applications that require large amounts of memory, there may be little benefit.
A shared-memory VM also provides a faster startup time, as the VM can already be running when the application is started.
For example, a VM using the Echidna library can be a running system process with no Java application running (except for the Echidna library)
The Echidna library can start any Java application in exactly the same way the VM would have started it, but without all the VM startup overhead.
It is possible for you to replace JDK classes directly.
Unfortunately, you can't distribute these altered classes with any application or applet unless you have complete control of the target environment.
The alterations would not be supported by the vendor (Sun in this case) and may violate the license, so contact the vendor if you need to do this.
In addition, altering classes in this way can be a significant maintenance problem.[3]
If your application has its classes localized in one place on one machine, for example with servlets, you might consider deploying changes to the core classes.
The upshot is that you can easily alter JDK-supplied classes for development purposes, which can be useful for various reasons including debugging and tuning.
But if you need the functionality in your deployed application, you need to provide classes that are used instead of the JDK classes by redirecting method calls to your own classes.
Replacing JDK classes indirectly in this way is a valid tuning technique.
Some JDK classes, such as StreamTokenizer (see Section 5.4), are inefficient and can be replaced quite easily since you normally use them in small, well-defined parts of a program.
Other JDK classes, like Date , BigDecimal, and String, are used all over the place, and it can take a large effort to replace references with your own versions of these classes.
The best way to replace these classes is to start from the design stage so that you can consistently use your own versions throughout the application.
StrictMath provides bitwise consistency across platforms; earlier versions of Math used platform-specific native functions that were not identical across all platforms.
Unfortunately, StrictMath calculations are somewhat slower than the corresponding native functions.
My colleague Kirk Pepperdine, who first pointed out the performance problem to me, puts it this way: "I've now got a bitwise-correct but excruciatingly slow program." The potential workarounds to this performance issue are all ugly: using an earlier JDK version, replacing the JDK class with an earlier version, or writing your own class to manage faster alternative floating-point calculations.
For optimal performance, I recommend developing with your own versions of classes rather than the JDK versions whenever possible.
Given that, perhaps the single most significant class to replace with your own version is the String class.
Most other classes can be replaced inside identified bottlenecks when required during tuning without affecting other parts of the application.
But String is used so extensively that replacing String references in one location tends to have widespread consequences, requiring extensive rewriting in many parts of the application.
But the String class tends to be used most often.
See Chapter 5 for details on why the String class can be a performance problem and why you might need to replace it.
The latter usage of Strings can be replaced more easily than the former.
Note that Generics are due to be introduced in Version 1.5
Generics allow instances of generic classes like Vector to be specified as aggregate objects that hold only specified types of objects.
However, the implementation of Generics is to insert casts at all the access points and to analyze the updates to ensure that the update type matches the cast type.
There is no specialized class generation, so there is no performance benefit, and there may even be a slight performance degradation from the additional casts.
If you are using your own classes, you can extend them with specific functionality you require, with direct access to the internals of the class.
Using Vector as an example, if you want to iterate over the collection (e.g., to select a particular subset based on some criteria), you need to access the elements through the get( ) method for each element, with the significant overhead that implies.
If you are using your own (possibly derived) class, you can implement the specific action you want in the class, allowing your loop to access the internal array directly with the consequent speedup:
More and more optimizations are finding their way into both VMs and compilers.
Many possible compiler optimizations are considered in later sections of this chapter.
Some VMs are intended purely for development and are highly suboptimal in terms of performance.
These VMs may have huge inefficiencies, even in such basic operations as casting between different numeric types, as was the case with one development VM I used.
It provided the foundation of an excellent development environment (actually my preferred environment) but was all but useless for performance testing.
Any data type manipulation other than with ints or booleans produced highly varying and misleading times.
It is important to run any tests involving timing or profiling in the same VM that will run the application.
You should test your application in the current "standard" VMs if your target environment is not fully defined.
There is, of course, nothing much you can do about speeding up any one VM (apart from heap tuning or upgrading the CPUs)
But you should be aware of the different VMs available, whether or not you control the deployment environment of your application.
If you control the target environment, you can choose your VM appropriately.
If you do not control the environment on which your application runs, remember that performance is partly user expectation.
If you tell your user that VM "A" gives such and such a performance for your application, but VM "B" gives much slower performance, then you at least inform your user community of the implications of their choice of VM.
This might also pressure vendors with slower VMs to improve them.
The basic bytecode interpreter VM executes by decoding and executing bytecode.
This is slow and is pure overhead, adding nothing to the functionality of the application.
A just-in-time (JIT) compiler in a virtual machine eliminates much of this overhead by doing the bytecode fetch and decode just once.
The first time the method is loaded, the decoded instructions are converted into machine code native for the CPU the system is running on.
After that, future invocations of a particular method no longer incur the interpreter overhead.
However, a JIT must be fast at compiling to avoid slowing the runtime, so extensive optimizations within the compile phase are unlikely.
This means that the compiled code is often not as fast as it could be.
A JIT also imposes a significantly larger memory footprint to the process.
Without a JIT, you might have to optimize your bytecodes for a particular platform.
Optimizing the bytecode for one platform can conceivably make that code run slower on another platform (though a speedup is usually reflected to some extent on all platforms)
A JIT compiler can theoretically optimize the same code differently for different underlying CPUs, thus getting the best of all worlds.
In a direct comparison of method call times for this JIT VM compared to a compiled C++ program, the Java method call time was found to be just one clock cycle slower than C++: fast enough for almost any application.
However, object creation is not speeded up by anywhere near this amount, which means that with a JIT VM, object creation is relatively.
The time your application takes to start depends on a number of factors.
First, there is the time taken by the operating system to start the executable process.
This time is mostly independent of the VM, though the size of the executable and the size and number of shared libraries needed to start the VM process have some effect.
But the main time cost is mapping the various elements into system memory.
This time can be shortened by having as much as possible already in system memory.
The most obvious way to have the shared libraries already in system memory is to have recently started a VM.
If the VM was recently started, even for a short time, the operating system is likely to have cached the shared libraries in system memory, so the next startup is quicker.
A better but more complicated way of having the executable elements in memory is to have the relevant files mapped onto a memory-resident filesystem; see Section 14.1.3 for more on how to manage this.
Yet another option is to use a prestarted VM; see the earlier section Section 3.5
A prestarted VM also partially addresses the startup overhead discussed in the next paragraph.
The second component in the startup time of the VM is the time taken to manage the VM runtime initializations.
Interpreter VMs generally have faster startup times than JIT VMs because the JIT VMs need to manage extra compilations during the startup and initial classloading phases.
Starting with SDK 1.3, Sun tried to improve VM startup time.
HotSpot has the more leisurely startup time acceptable for long-running server processes.
In the future you can expect to see VMs differentiated by their startup times even more.
Finally, the application architecture and class file configuration determine the last component of startup time.
The application may require many classes and extensive initializations before the application is started, or it may be designed to start up as quickly as possible.
It is useful to bear in mind the user perception of application startup when designing the application.
For example, if you can create the startup window as quickly as possible and then run any initializations in the background without blocking windowing activity, the user sees this as a faster startup than if you waited for initializations to finish before creating the initial window.
The number of classes that need to be loaded before the application starts are part of the application initializations, and again the application design affects this time.
In the later section Section 3.12, I discuss the effects of class file configuration on startup time.
Section 13.3 also includes an example of designing an application to minimize startup time.
On the VM side, improvements are possible using JIT compilers to compile methods to machine code, using algorithms for code caching, applying intelligent analyses of runtime code, etc.
Some bytecodes allow the system to bypass table lookups that would otherwise need to be executed.
But these bytecodes take extra effort to apply to the VM.
Using these techniques, an intelligent VM could skip some runtime steps after parts of the application have been resolved.
Generally, a VM with a JIT compiler gives a huge boost to a Java application and is probably the quickest and simplest way to improve performance.
The most optimistic predictions say that using optimizing compilers to generate bytecodes, together with VMs with intelligent JIT (re)compilers, puts Java performance on a par with or even above that of an equivalent natively compiled C++ application.
Having a runtime environment adapt to the running characteristics of a program should, in theory at.
A similar argument runs in CPU design circles where dynamic rescheduling of instructions to take pipelining into account allows CPUs to process instructions out of order.
But at the time of writing this book, we are not particularly close to proving this theory for the average Java application.
The time available for a VM to do something other than the most basic execution and bytecode translation is limited.
The following quote about dynamic scheduling in CPUs also applies to adaptive VMs:
At runtime, the CPU knows almost everything, but it knows everything almost too late to do anything about it.
As an example of an "intelligent" VM, Sun's HotSpot VM is targeted precisely to this area of adaptive optimization.
This VM includes some basic improvements (all of which are also present in VMs from other vendors) such as using direct pointers instead of Java handles (which may be a security issue),[5] improved thread synchronization, a generational garbage collector, speedups to object allocation, and an improved startup time (by not JIT-compiling methods initially)
In addition to these basic improvements, HotSpot includes adaptive optimization, which works as follows: HotSpot runs the application initially in interpreted mode (as if there is no JIT compiler present) while a profiler identifies the bottlenecks in the application.
Then, an optimizing JIT compiler compiles into native machine code only those hotspots in the application that are causing the bottlenecks.
Because only a small part of the application is targeted, the JIT compiler (which might in this case be more realistically called an "after-a-while" compiler rather than a "just-in-time" compiler) can spend extra time compiling those targeted parts of the application, thus allowing more than the most basic compiler optimizations to be applied.
Java uses handles to ensure security so that one object cannot gain direct access to another object without the security capabilities of Java being able to intervene.
The HotSpot compiler ignores the nonbottlenecked code, instead focusing on getting the 40% of the original time.
For example, HotSpot can speculatively optimize on the basis of guessing the type of particular objects.
If that guess turns out to be wrong, HotSpot has to deoptimize the code, which results in some wildly variable timings.
So far, I have no evidence that optimizations I have applied in the past (and detailed in this book) have caused any problems after upgrading compilers and VMs.
However, it is important to note that the performance characteristics of your application may change with different VMs and compilers, and not necessarily always for the better.
Be on the lookout for any problems a new compiler and VM may bring.
The technique of loading classes explicitly from a new thread after application startup can conflict with a particular JIT VM's caching mechanism and actually slow down the startup sequence of your application.
I have no evidence for this; I am just speculating on possible conflicts.
Java code compilers that specifically target performance optimizations are increasingly available.
Of course, all compilers try to optimize code, but some are better than others.
Some companies put a great deal of effort into making their compiler produce the tightest, fastest code, while others tend to be distracted by other aspects of the Java environment and put less effort into the compile phase.
For example, the JAVAR compiler (http://www.extreme.indiana.edu/hpjava/) is a prototype compiler that automatically parallelizes parts of a Java application to improve performance.
It is possible to write preprocessors to automatically achieve many of the optimizations you can get with optimizing compilers; in fact, you can think of an optimizing compiler as a preprocessor together with a basic compiler (though in many cases it is better described as a postprocessor and recompiler)
Even if you ignore the Java code parsing or bytecode parsing required,[6] any one preprocessor optimization can take months to create and verify.
Getting close to the full set of optimizations listed in the following sections could take years of development.
Such parsing is a one-off task that can then be applied to any optimization.
There are several free packages available for parsing class files, including CFParse from the IBM alphaWorks site, http://www.alphaworks.ibm.com/tech/cfparse.
Optimizing compilers cannot change your code to use a better algorithm.
If you are using an inefficient search routine, there may be a vastly better search algorithm that would give an order-of-magnitude speedup.
But the optimizing compiler only tries to speed up the algorithm you are using (probably a small incremental speedup)
It is still important to profile applications to identify bottlenecks even if you intend to use an optimizing compiler.
It is important to start using an optimizing compiler from the early stages of development in order to tailor your code to its restrictions.
Integrating an optimizing compiler at a late stage of development can mean restructuring core routines and many disparate method calls, and may even require some redesign to work around limitations imposed by being unable to correctly handle reflection and runtime class resolution.
Optimizing compilers have difficulty dealing with classes that cannot be identified at compile time (e.g., building a string at runtime and loading a class of that name)
Basically, using Class.forName( ) is not (and cannot be) handled in any complete way, though several compilers try to manage as best they can.
In short, managers with projects at a late stage of development are often reluctant to make extensive changes to either the development environment or the code.
While code tuning can be targeted at bottlenecks and so normally affects only small sections of code, integrating an optimizing compiler can affect the entire project.
If there are too many problems in this integration, most project managers decide that the potential risks outweigh the possible benefits and prefer to take the safe route of carrying on without the optimizing compiler.
Compilers can apply many "classic" optimizations and a host of newer optimizations that apply specifically to object-oriented programs and languages with virtual machines.
But usually you should not, as it makes the code more complicated to read and maintain.
Individually, each of these optimizations improves performance only by small amounts.
Collectively (as applied by a compiler across all the code), they can make a significant contribution to improving performance.
This is important to remember: as you look at each individual optimization, in many cases you may think, "Well, that isn't going to make much difference." This is correct.
The power of optimizing compilers comes in automatically applying many small optimizations that would be annoying or confusing to apply by hand.
The combination of all those small optimizations can add up to a big speedup.
Most applications in serious need of optimization are looking for speedups even greater than this, but don't ignore the optimizing compiler for that reason: it may be doubling the speed of your application for a relatively cheap investment.
As long as you do not need to restructure much code to take advantage of them, optimizing compilers can give you "the biggest bang for your buck" after JIT VMs in terms of performance improvements.
The next sections list many of the well-known optimizations these compilers may apply.
This list can help you when selecting optimizing compilers or applying some of these optimizations by hand.
When all application classes are known at compile time, an optimizing compiler can analyze the full runtime code-path tree, identifying all classes that can be used and all methods that can be called.
Most method calls in Java necessarily invoke one of a limited set of methods, and by analyzing the runtime path, you can eliminate all but one of the possibilities.
The compiler can then remove unused methods and classes, including removing superclass methods that have been overridden in a subclass and are never called in the superclass.
The optimization makes for smaller download requirements for programs sent across a network and, more usefully, reduces the impact of method lookup at runtime by eliminating unused alternative methods.
Section 14.9 of the Java specification requires compilers to carry out flow analysis on the code to determine.
The only valid unreachable code is the consequence of an if statement (see the later section Section 3.9.1.4)
Invalid unreachable code must be flagged as a compile error, but the valid code from an if statement is not a compile error and can be eliminated.
The if statement test can also be eliminated if the boolean result is conclusively identified at compile time.
In fact, this is a standard capability of almost all current compilers.
This flow analysis can be extended to determine if other sections and code branches that are syntactically valid are actually semantically unreachable.
Some null tests can be eliminated by establishing that the variable has either definitely been assigned to or definitely never been assigned to before the test is reached.
Similarly, some bytecode instructions that can be generated may be unnecessary after establishing the flow of control, and these can also be eliminated.
An optimizing compiler should determine if there is a computationally cheaper alternative to a set of instructions and replace those slower instructions with the faster alternative.
The classic version of this technique, termed "strength reduction," replaces an operation with an equivalent but faster operation.
One of the technical reviewers for this book, Ethan Henry, pointed out to me that there is no actual guarantee that these strength reductions are more efficient in Java.
However, they seem to work for at least some VMs.
In addition, compilers producing native code (including JIT compilers) should produce faster code, as these techniques do work at the machinecode level.
An optimizing compiler can identify code that requires runtime execution if bytecodes are directly generated, but can be replaced by computing the result of that code during the compilation phase.
And it can be extended to other structures by adding some semantic input to the compiler.
Analysis of the application can identify fields of objects that are never used, and these fields can then be removed.
This makes the runtime take less memory and improves the speeds of both the object creation and the garbage collection of these objects.
The type of analysis described in the earlier section Section 3.8.2.1 improves the identification of unused fields.
Removing some unnecessary parts of compiled files is standard with most optimizing compilers.
This option removes line number tables and local variable tables.
The Java .class file structure allows extra information to be inserted, and some optimizing compilers make an effort to remove everything that is not necessary for runtime execution.
This can be useful when it is important to minimize the size of the .class files.
Frequently, compilers with this capability can remove unnecessary parts of files that are already compiled, e.g., from third-party .class files you do not have the source for.
Some optimizing compilers can reduce the necessary parts of compiled files.
For example, the .class file includes a pool of constants, and an optimizing compiler can minimize the size of the constant pool by combining and reducing entries.
At least one optimizing compiler (the DashO optimizer by PreEmptive) provides the option to alter the access control to methods.
The rationale for this is that any non-public method has access control on that method since it is access restricted (i.e., the runtime system must verify at some point that the caller to a method has access to calling that method)
The thinking is that any non-public method must have some overhead compared to an identical method declared as public.
The result is that the compiler supports normal compilation (so that any incorrect accesses are caught at the compilation stage), and the subsequent compiled class can have all its methods changed to public.
However, the degree of inlining supported can vary enormously, as different compilers are more or less aggressive about inlining (see the extended discussion in the later section Section 3.9.2)
Inlining is the technique in which a method call is directly replaced with the code for that method; for example, the code as written may be:
Every compiler removes dynamic type checks when the compiler can establish they are unnecessary.
Loop unrolling makes the loop body larger by explicitly repeating the body statements while changing the amount by which the loop variable increases or decreases.
This reduces the number of tests and iterations the loop requires to be completed.
Code motion moves calculations out of loops that need calculating only once.
Both this technique and the next one are good coding practices.
In this case, though, the compiler identifies an expression that is common to more than one statement and does not need to be calculated more than once.
The following example uses the same calculation twice to map two pairs of variables:
Some compilers rename classes, fields, and methods for various reasons, such as to obfuscate the code (making it difficult to understand if it were decompiled)
For example, the DashO optimizer renames everything possible to one-character names.
An optimizing compiler can reorder or change bytecode instructions to make methods faster.
Normally, this reduces the number of instructions, but sometimes making an activity faster requires increasing the number of instructions.
An example is where a switch statement is used with a list of unordered, nearly consecutive values for case statements.
An optimizing compiler can reorder the case statements so that the values are in order, insert extra cases to make the values fully consecutive, and then use a faster switch bytecode to execute the switch statement.
The Java bytecode specification provides support for optional extra information to be included with class files.
This information can be VM-specific: any VM that does not understand the codes must ignore them.
Consequently, it is possible that a particular compiler may be optimized (in the future) to generate extra information that allows particular VMs to run code faster.
For example, it would be possible for the compiler to add extra information that tells the VM the optimal way in which a JIT should compile the code, thus removing some of the JIT workload (and overhead)
A more extreme example might be where a compiler generates optimized native code for several CPUs in addition to the bytecode for methods in the class file.
This would allow a VM to execute the native code immediately if it were running on one of the supported CPUs.
Unfortunately, this particular example would cause a security loophole, as there would be no guarantee to the VM that the natively compiled method was the equivalent of the bytecode-generated one.
All the optimizations previously listed are optimizations that compilers should automatically handle.
Unfortunately, you are not guaranteed that any particular compiler actually applies any single optimization.
The only way I have found to be certain about the optimizations a particular compiler can make is to compile code with lines such as those shown previously, then decompile the bytecodes to see what comes out.
There are several decompilers available on the Net: a web search for "java+decompile" should fetch a few.
My personal favorite at the time of this writing is jad by Pavel Kouznetsov, which currently resides at http://kpdus.tripod.com/jad.html.
Several Java compilers are targeted at optimizing bytecode, and several other compilers (including all mainstream ones) have announced the intention to roll more optimizations into future versions.
This highlights another point: ensure that you use the compiler's latest version.
It may be that, for robustness, you do not want to go into production with the very latest compiler, as that will have been less tested than an older version, and your own code will have been more thoroughly tested on the classes generated by the older compiler.
Nevertheless, you should at least test whether the latest compiler gives your application a boost (using whatever standard benchmarks you choose to assess your application's performance)
Finally, the compiler you select to generate bytecode may not be the same compiler you use while developing code.
You may even have different compilers for different parts of development and even for different optimizations (though this is unlikely)
In any case, you need to be sure the deployed application is using the bytecodes generated by the specific compilers you have chosen for the final version.
At times in large projects, I have seen some classes recompiled with the wrong compiler.
This alternate recompilation does not affect the correctness of the application because all compilers should be generating correct bytecodes, which means that such a situation allows the application to pass all regression test suites.
But you can end up with the production application not running as fast as you expect for reasons that are difficult to track down.
As you can see from the previous sections, knowing how the compiler alters your code as it generates bytecodes is important for performance tuning.
Some compiler optimizations can be canceled out if you write your code so that the compiler cannot apply its optimizations.
In this section, I cover what you need to know to get the most out of the compilation stage if you are using the JDK compiler (javac)
Several optimizations occur at the compilation stage without your needing to specify any compilation options.
These optimizations are not necessarily required because of specifications laid down in Java.
The JDK compiler always applies them, and consequently almost every other compiler applies them as well.
You should always determine exactly what your specific compiler optimizes as standard, from the documentation provided or by decompiling example code.
This optimization is a concrete implementation of the ideas discussed in Section 3.8.2.5 earlier.
In this implementation, multiple literal constants[9] in an expression are "folded" by the compiler.
The result is as if the line read: int foo = 90; This optimization allows you to make your code more readable without having to worry about avoiding runtime overhead.
With the Java 2 compiler, string concatenations to literal constants are folded.
The optimization applies where the statement can be resolved into literal constants concatenated with a literal string using the + concatenation operator.
In this last case, all compilers fold two (or more) strings since that action is required by the Java specification.
Primitive constant fields (those primitive data type fields defined with the final modifier) are inlined within a class and across classes, regardless of whether the classes are compiled in the same pass.
For example, if class A has a public static final field, and class B has a reference to this field, the value from class A is inserted directly into class B, rather than a reference to the field in class A.
Strictly speaking, this is not an optimization, as the Java specification requires constant fields to be inlined.
Another type of optimization automatically applied at the compilation stage is to cut code that can never be reached because of a test in an if statement that can be completely resolved at compile time.
The discussion in the earlier section Section 3.8.2.3 is relevant to this section.
A problem is frequently encountered with this kind of code.
The constant value is set when the class with the constant, say class A, is compiled.
Any other class referring to class A's constant takes the value that is currently set when that class is being compiled, and does not reset the value if A is recompiled.
So you can have the situation where A is compiled with A.DEBUG set to false, then B is compiled and the compiler inlines A.DEBUG as false, possibly cutting dead code branches.
Then if A is recompiled to set A.DEBUG to true, this does not affect class B; the compiled class B still has the value false inlined, and any dead code branches stay eliminated until class B is recompiled.
You should be aware of this possible problem if you compile your classes in more than one pass.
You should use this pattern for debug and trace statements and assertion preconditions, postconditions, and invariants.
The only standard compile-time option that can improve performance with the JDK compiler is the -O option.
Check your compiler's documentation to find out what other options are available and what they do.
Some compilers allow you to make the tradeoff between optimizing the compiled code for speed or minimizing the size.
The standard -O option does not currently apply a variety of optimizations in the Sun JDK (up to JDK 1.4)
In future versions it may do more, though the trend has actually been for it to do less.
Currently, the option makes the compiler eliminate optional tables in the class files, such as line number and local variable tables.
This gives only a small performance improvement by making class files smaller and therefore faster to load.
You should definitely use this option if your class files are sent across a network.
The main performance improvement of using the -O option used to come from the compiler inlining methods.
When using the -O option with javac prior to SDK 1.3, the compiler considered inlining methods defined with any of the following modifiers: private, static, or final.
Some methods, such as those defined as synchronized, are never inlined.
If a method can be inlined, the compiler decides whether or not to inline it depending on its own unpublished considerations.
These considerations seem mainly to be the simplicity of the method: in JDK 1.2 the compiler inlined only fairly simple methods.
For example, one-line methods with no side effects, such as accessing or updating a variable, are invariably inlined.
From 1.3, the -O option does not even inline methods.
Instead, inlining is left to the HotSpot compiler, which can speculatively inline and is far more aggressive.
The sidebar Why There Are Limits on Static Inlining discusses one of the reasons why optimizations such as inlining have been pushed back to the HotSpot compiler.
Choosing simple methods to inline does have a rationale behind it.
The larger the method being inlined, the more the code gets bloated with copies of the same code inserted in many places.
This has runtime costs in extra code being loaded and extra space taken by the runtime system.
A JIT VM would also have the extra cost of compiling more code.
At some point, there is a decrease in performance from inlining too much code.
In addition, some methods have side effects that can make them quite difficult to inline correctly.
The static compiler applies its methodology for selecting methods to inline, irrespective of whether the target method is in a bottleneck: this is a machine-gun strategy of many little optimizations in the hope that some inline calls may improve the bottlenecks.
A performance tuner applying inlining works the other way around, first finding the bottlenecks, then selectively inlining methods inside bottlenecks.
This latter strategy can result in good speedups, especially in loop bottlenecks.
This is because a loop can be speeded up significantly by removing the overhead of a repeated method call.
If the method to be inlined is complex, you can often factor out parts of the method so that those parts can be executed outside the loop, gaining even more speedup.
HotSpot applies the latter rationale to inlining code only in bottlenecks.
I have not found any public document that specifies the actual decision-making process that determines whether or not a method is inlined, whether by static compilation or by the HotSpot compiler.
The only reference given is to Section 13.4.21 of the Java language specification that specifies only that binary compatibility with preexisting binaries must be maintained.
It does specify that the package must be guaranteed to be kept together for the compiler to allow inlining across classes.
The specification also states that the final keyword does not imply that a method can be inlined since the runtime system may have a differently implemented method.
The HotSpot documentation does state that simple methods are inlined, but again no real details are provided.
Prior to JDK 1.2, the -O option used with the Sun compiler did inline methods across classes, even if they were not compiled in the same compilation pass.
Primarily methods that accessed private or protected variables were incorrectly inlined into other classes, leading to runtime authorization exceptions.
Unfortunately, there is no way to specify directly which methods should be inlined rather than relying on some compiler's internal workings.
Possibly in the future, some compiler vendors will provide a mechanism that supports specifying which methods to inline, along with other preprocessor options.
In the meantime, you can implement a preprocessor (or use an existing one) if you require tighter control.
Opportunities for inlining often occur inside bottlenecks (especially in loops), as discussed previously.
Selective inlining by hand can give an order-of-magnitude speedup for some bottlenecks, and no speedup at all in others.
Relying on HotSpot to detect these kinds of situations is an option.
The speedup obtained purely from inlining is usually only a small percentage: 5% is fairly common.
Some static optimizing compilers are very aggressive about inlining code.
They apply techniques such as analyzing the entire program to alter and eliminate method calls in order to identify methods that can be coerced into being statically bound.
Then these identified methods are inlined as much as possible according to the compiler's analysis.
This technique has been shown to give a 50% speedup to some applications.
Some runtime options can help your application to run faster.
Options that allow the VM to have a bigger footprint (-Xmx/-mx is the main one, which allows a larger heap space; but see the comments in the following paragraph)
The various alternative garbage-collection strategies like -Xincgc and -Xconcgc are aimed at minimizing some aspect (pause times for these two), but the consequence is that total GC is slower.
Some options can be both detrimental to performance and help make a faster application, depending on how they are used.
Xcomp, which forces HotSpot to compile 100% of the code with maximum optimization.
This makes the first pass through the code very slow indeed, but subsequent passes should be faster.
Xbatch, which forces HotSpot to compile methods in the foreground.
Normally methods are compiled in the foreground if they compile quickly.
Compilation is moved to the background if it is taking too long (the method carries on executing in interpreted mode until the compilation is finished)
This makes the first execution of methods slower, but subsequent executions can be faster if compilation would not have otherwise finished.
Increasing the maximum heap size beyond the default usually improves performance for applications that can use the extra space.
Increasing the heap size actually causes garbage collection to take longer since it needs to examine more objects and a larger space.
Up to now, I have found no better method than trial and error to determine optimal maximum heap sizes for any particular application.
This is covered in more detail earlier in this chapter.
I once had a customer who had a sudden 40% decrease in performance during tests.
Their performance harness had a configuration file that set up how the VM could be run, and this was accidentally set to include the -prof option on the standard tests as well as for the profiling tests.
That was the cause of the sudden performance decrease, but it was not discovered until time had been wasted checking software versions, system configurations, and other things.
If you know the target environments of your application, you have the option of taking your Java application and compiling it to a machine-code executable.
A variety of these compilers target various platforms, and the list continues to grow.
Check the computer magazines or follow the compiler links on good Java web sites.
See also the compilers listed in Chapter 19 and at http://www.JavaPerformanceTuning.com/resources.shtml.
These compilers often work directly from the bytecode (i.e., the .class files) without the source code, so any third-party classes and beans you use can normally be included.
If you use this option, a standard technique to remain multiplatform is to start the application from a batch file that checks the platform and installs (or even starts) the application binary appropriate for that platform, falling back to the standard Java runtime if no binary is available.
Of course, the batch file also needs to be multiplatform, but then you could build it in Java.
Prepare to be disappointed with the performance of a natively compiled executable compared to the latest JIT-enabled runtime VMs.
The compiled executable still needs to handle garbage collection, threads, exceptions, etc., all within the confines of the executable.
These runtime features of Java do not necessarily compile efficiently into an executable.
The performance of the executable may well depend on how much effort the compiler vendor has made in making those Java features run efficiently in the context of a natively compiled executable.
The latest adaptive VMs have been shown to run some applications faster than the equivalent natively compiled executable.
Advocates of the "compile to native executable" approach feel that the compiler optimizations will improve with time so that this approach will ultimately deliver the fastest applications.
Luckily, this is a win-win situation for the performance of Java applications: try out both approaches if appropriate to you, and choose the one that works best.
There are also several translators that convert Java programs into C.
I only include a mention of these translators for completeness, as I have not tried any of them.
They presumably enable you to use a standard C compiler to compile to a variety of target platforms.
However, most source code-to-source code translations between programming languages are suboptimal and do not usually generate fast code.
For that extra zing in your application (but probably not applet), try out calls to native code.
Wave goodbye to 100% pure Java certification, and say hello to added complexity in your development environment and deployment procedure.
If you are already in this situation for reasons other than performance tuning, there is little overhead to taking this route in your project.
I've seen native method calls used for performance reasons in earlier Java versions when doing intensive number-crunching for a scientific application and parsing large amounts of data in a restricted time.
In these and other cases, the runtime application environment at the time could not get to the required speed using Java.
I should note that a parsing application would now be able to run fast enough in pure Java, but the original application was built with quite an early version.
In addition, some number crunchers find that the latest Java runtimes and optimizing compilers give them sufficient performance in Java without resorting to any native calls.[11]
Serious number crunchers spend a large proportion of their time performance-tuning their code, whatever language it is written in.
To gain sufficient performance in Java, they of course need to tune the application intensively.
But this is also true if the application is written in C or Fortran.
The amount of tuning required is now, apparently, similar for these three languages.
The JNI interface itself has its own overhead, which means that if a pure Java implementation comes close to the native call performance, the JNI overhead probably cancels any performance advantages from the native call.
However, on occasion the underlying system can provide an optimized native call that is not available from Java and cannot be implemented to work as fast in pure Java.
In this kind of situation, JNI is useful for tuning.
Another case in which JNI can be useful is reducing the number of objects created, though this should be less common: you should normally be able to do this directly in Java.
I once encountered a situation where JNI was needed to avoid excessive objects.
This was with an application that originally required the use of a native DLL service.
The vendor of that DLL ported the service to Java, which the application developers would have preferred using, but unfortunately the vendor neglected to tune the ported code.
This resulted in a native call to a particular set of services producing just a couple dozen objects, but the Java-ported code producing nearly 10,000 objects.
Apart from this difference, the speeds of the two implementations were similar.[12] However, the overhead in garbage collection caused a significant degradation in performance, which meant that the native call to the DLL was the preferred option.
This increase in object creation normally results in a much slower implementation.
However, in this particular case, the methods required synchronizing to a degree that gave a larger overhead than the object creation.
Nevertheless, the much larger number of objects created by the untuned Java implementation needed reclaiming at some point, and this led to greater overhead in the garbage collection.
If you are following the native function call route, there is little to say.
You write your routines in C, plug them into your application using the native keyword, profile the resultant application, and confirm that it provides the required speedup.
You can also use C (or C++ or whatever) profilers to profile the native code calls if it is complicated.
If you are calling the native routines from loops, you should move the loops down into the native routines and pass the loop parameters to the routine as arguments.
In a similar but more generic vein, try to avoid crossing the JNI.
If it is necessary to pass objects such as arrays, try to do as much data movement as possible in one transfer to minimize transfer overhead.
From 1.4, native ByteBuffer s (available with the java.nio packages) allow you to pass data to native libraries without necessarily passing the data through the JNI, which can be a significant gain.
You can allocate a native ByteBuffer in the C code and pass the pointer through the JNI, avoiding the JNI data transfer overhead.
At least one animation application has actually allocated memory on the graphics card as a native ByteBuffer, and manipulated that ByteBuffer from the Java side.
One other recommendation, which is not performance tuning-specific, is that it is usually good practice to provide a fallback methodology for situations when the native code cannot be loaded.
This requires extra maintenance (two sets of code, extra fallback code) but is often worth the effort.
You can manage the fallback at the time when the DLL library is being loaded by catching the exception when the load fails and providing an alternative path to the fallback code, either by setting boolean switches or by instantiating objects of the appropriate fallback classes as required.
It is better to deliver your classes in a ZIP or JAR file than to deliver them one class at a time over the network or load them individually from separate files in the filesystem.
The benefits gained from packaging class files come from reducing I/O overhead such as repeated file opening and closing, and possibly improving seek times.[14] Within the ZIP or JAR file, the classes should not be compressed unless network download time is a factor for the application.
The best way to deliver local classes for performance reasons is in an uncompressed ZIP or JAR file.
In the context here, I use "clustering" to mean the close grouping of files.
With operating system-monitoring tools, you can see the system temporarily stalling when the operating system issues a diskcache flush if lots of files are closed around the same time.
If you use a single packed file for all classes (and resources), you avoid this potential performance hit.
It is possible to further improve the classloading times by packing the classes into the ZIP/JAR file in the order in which they are loaded by the application.
You can determine the loading order by running the application with the -verbose option, but note that this ordering is fragile: slight changes in the application can easily alter the loading order of classes.
A further extension to this idea is to include your own classloader that opens the ZIP/JAR file itself and reads in all files sequentially, loading them into memory immediately.
This is similar to the ZIP filesystem, but it is better if you read the header in one block, and read in and load the files directly rather than going through the java.util.zip classes.
One further optimization to this classloading tactic is to start the classloader running in a separate (lowpriority) thread immediately after VM startup.
Many of these suggestions apply only after a bottleneck has been identified:
Test your benchmarks on each version of Java available to you (classes, compiler, and VM) to identify any performance improvements.
Test performance using the target VM or "best practice" VMs.
Include some tests of the garbage collector appropriate to your application, so that you can identify changes that minimize the cost of garbage collection in your application.
Run your application with both the -verbosegc option and with full application tracing turned on to see when the garbage collector kicks in and what it is doing.
Vary the -Xmx/-Xms option values to determine the optimal memory sizes for your application.
Fine-tuning the heap is possible, but requires knowledge of the GC algorithm and the many parameter options available.
Sharing memory between multiple VMs is easy with the Echidna library.
Replace generic classes with more specific implementations dedicated to the data type being manipulated, e.g., implement a LongVector to hold longs rather than using a Vector object with Long wrappers.
Extend collection classes to access internal arrays for queries on the class.
Replace collection objects with arrays where the collection object is a bottleneck.
Look for compilers targeted at optimizing performance: these provide the cheapest significant speedup for all runtime environments.
Use the -O option (but always check that it does not produce slower code)
Identify the optimizations a compiler is capable of so that you do not negate the optimizations.
Use a decompiler to determine precisely the optimizations generated by a particular compiler.
Consider using a preprocessor to apply some standard compiler optimizations more precisely.
Remember that an optimizing compiler can only optimize algorithms, not change them.
A better algorithm is usually faster than an optimized slow algorithm.
Make sure that the deployed classes have been compiled with the correct compilers.
Make sure that any loops using native method calls are converted so that the native call includes the loop instead of running the loop in Java.
Any loop iteration parameters should be passed to the native call.
Deliver classes in uncompressed format in ZIP or JAR files (unless network download time is significant, in which case files should be compressed)
Use a customized classloader running in a separate thread to load class files.
The biggest difference between time and space is that you can't reuse time.
I thought that I didn't need to worry about memory allocation.
Java handles low-level memory allocation and deallocation and comes with a garbage collector.
Further, it prevents access to these low-level memoryhandling routines, making the memory safe.
So memory access should not cause corruption of data in other objects or in the running application, which is potentially the most serious problem that can occur with memory-access violations.
In a C or C++ program, problems of illegal pointer manipulations can be a major headache (e.g., deleting memory more than once, runaway pointers, bad casts)
They are very difficult to track down and are likely to occur when changes are made to existing code.
Java deals with all these possible problems and, at worst, will throw an exception immediately if memory is incorrectly accessed.
However, Java does not prevent you from using excessive amounts of memory nor from cycling through too much memory (e.g., creating and dereferencing many objects)
Contrary to popular opinion, you can get memory leaks (or, more accurately, object retention) by holding onto objects without releasing references.
This stops the garbage collector from reclaiming those objects, resulting in increasing amounts of memory being used.[1] In addition, Java does not provide for large numbers of objects to be created simultaneously (as you could do in C by allocating a large buffer), which eliminates one powerful technique for optimizing object creation.
Creating objects costs time and CPU effort for an application.
Garbage collection and memory recycling cost more time and CPU effort.
The difference in object usage between two algorithms can make a huge difference.
In Chapter 5, I cover algorithms for appending basic data types to StringBuffer objects.
These can be an order of magnitude faster than some of the conversions supplied with Java.
A significant portion of the speedup is obtained by avoiding extra temporary objects used and discarded during the data conversions.[2]
Up to SDK 1.4, data-conversion and object-lifecycle performance has been targeted by Sun.
In 1.4, the core SDK int conversion is faster, but all other data type conversions are still significantly slower.
Here are a few general guidelines for using object memory efficiently:
Because these routines are called frequently, you will likely be creating objects frequently, and consequently adding heavily to the overall burden of object cycling.
By rewriting such routines to avoid creating objects, possibly by passing in reusable objects as parameters, you can decrease object cycling.
Try to presize any collection object to be as big as it will need to be.
It is better for the object to be slightly bigger than necessary than to be smaller than it needs to be.
This recommendation really applies to collections that implement size increases in such a way that objects are discarded.
For example, Vector grows by creating a new larger internal array object, copying all the elements from the old array, and discarding it.
Most collection implementations have similar implementations for growing the collection beyond its current capacity, so presizing a collection to its largest potential size reduces the number of objects discarded.
When multiple instances of a class need access to a particular object in a variable local to those.
This reduces the space taken by each object (one fewer instance variable) and can also reduce the number of objects created if each instance creates a separate object to populate that instance variable.
Reuse exception instances when you do not specifically require a stack trace (see Section 6.1)
This chapter presents many other standard techniques to avoid using too many objects and identifies some known inefficiencies when using some types of objects.
Objects need to be created before they can be used, and garbage-collected when they are finished with.
The more objects you use, the heavier this garbage-cycling impact becomes.
General object-creation statistics are actually quite difficult to measure decisively, since you must decide exactly what to measure, what size to pregrow the heap space to, how much garbage collection impacts the creation process if you let it kick in, etc.
For example, on a medium Pentium II, with heap space pregrown so that garbage collection does not have to kick in, you can get around half a million to a million simple objects created per second.
If the objects are very simple, even more can be garbage-collected in one second.
On the other hand, if the objects are complex, with references to other objects, and include arrays (like Vector and StringBuffer) and nonminimal constructors, the statistics plummet to less than a quarter of a million created per second, and garbage collection can drop way down to below 100,000 objects per second.
Each object creation is roughly as expensive as a malloc in C, or a new in C++, and there is no easy way of creating many objects together, so you cannot take advantage of efficiencies you get using bulk allocation.
There are already runtime systems that use generational garbage collection, minimize object-creation overhead, and optimize native-code compilation.
By doing this they reach up to three million objects created and collected per second (on a Pentium II), and it is likely that the average Java system should improve to get closer to that kind of performance over time.
But these figures are for basic tests, optimized to show the maximum possible object-creation throughput.
In a normal application with varying size objects and constructor chains, these sorts of figures cannot be obtained or even approached.
Also bear in mind that you are doing nothing else in these tests apart from creating objects.
In most applications, you are doing something with all those objects, making everything much slower but significantly more useful.
Avoidable object creation is definitely a significant overhead for most applications, and you can easily run through millions of temporary objects using inefficient algorithms that create too many objects.
In Chapter 5, we look at an example that uses the StreamTokenizer class.
This class creates and dereferences a huge number of objects while it parses a stream, and the effect is to slow down processing to a crawl.
As we saw in the last section, objects are expensive to create.
Where it is reasonable to reuse the same object, you should do so.
You need to be aware of when not to call new.
One fairly obvious situation is when you have already used an object and can discard it before you are about to create another object of the same class.
You should look at the object and consider whether it is possible to reset the fields and then reuse the object, rather than throw it away and create another.
This can be particularly important for objects that are constantly used and discarded: for example, in graphics processing, objects such as Rectangles, Points, Colors, and Fonts are used and discarded all the time.
Recycling can also apply to the internal elements of structures.
For example, a linked list has nodes added to it as it grows, and as it shrinks, the nodes are discarded.
Holding onto the discarded nodes is an obvious way to recycle these objects and reduce the cost of object creation.
Most container objects (e.g., Vectors, Hashtables) can be reused rather than created and thrown away.
Of course, while you are not using the retained objects, you are holding onto more memory than if you simply discarded those objects, and this reduces the memory available to create other objects.
You need to balance the need to have some free memory available against the need to improve performance by reusing objects.
But generally, the space taken by retaining objects for later reuse is significant only for very large collections, and you should certainly know which ones these are in your application.
Note that when recycling container objects, you need to dereference all the elements previously in the container so that you don't prevent them from being garbage-collected.
Because there is this extra overhead in recycling, it may not always be worth recycling containers.
As usual for tuning, this technique is best applied to ameliorate an object-creation bottleneck that has already been identified.
In their HotSpot FAQ, Sun engineering states that pooling should definitely no longer be used because it actually gives worse performance with the latest HotSpot engines.
Object pools are still useful even with HotSpot, but presumably not as often as before.
Certainly for shared resources pooling will always be an option if the overhead associated with creating a shareable resource is expensive.
Various recent tests have shown that the efficiency of pooling objects compared to creating and disposing of objects is highly dependent on the size and complexity of the objects.
And in some applications where deterministic behavior is important, especially J2ME applications, it is worth noting that object pools have deterministic access and reclamation costs for both CPU and memory, whereas object creation and garbage collection can be less deterministic.
A good strategy for reusing container objects is to use your own container classes, possibly wrapping other containers.
This gives you a high degree of control over each collection object, and you can design them specifically for reuse.
You can still use a pool manager to manage your requirements, even without reusedesigned classes.
Reusing classes requires extra work when you've finished with a collection object, but the effort is worth it when reuse is possible.
The code fragment here shows how you could use a vector pool manager:
If we got here, then all the Vectors are in use.
We will //increase the number in our pool by 10 (arbitrary value for //illustration purposes)
The capacity is the number of elements the collection can hold before that collection needs to resize its internal memory to be larger.
The size is the number of externally accessible elements the collection is actually holding.
The capacity is always greater than or equal to the size.
By holding spare capacity, elements can be added to collections without having to continually resize the underlying memory.
The previous example of a pool manager can be used by multiple threads in a multithreaded application, although the getVector( ) and returnVector( ) methods first need to be defined as synchronized.
This may be all you need to ensure that you reuse a set of objects in a multithreaded application.
Sometimes, though, there are objects you need to use in a more complicated way.
It may be that the objects are used in such a way that you can guarantee you need only one object per thread, but any one thread must consistently use the same object.
Singletons (see Section 4.2.4) that maintain some state information are a prime.
In this case, you might want to use a ThreadLocal object.
ThreadLocals have accessors that return an object local to the current thread.
ThreadLocal use is best illustrated using an example like this, which produces:
The vector obtained by each thread is always the same vector for that thread: the ThreadLocal object always returns the threadspecific vector.
As the following code shows, each vector has the same string added to it repeatedly, showing that it is always obtaining the same thread-specific vector from the vector access method.
The VectorPoolManager is really an object with behavior and state.
My colleague Kirk Pepperdine insists that this choice is more than just a preference.
He states that holding onto an object as opposed to using statics provides more flexibility should you need to alter the use of the VectorPoolManager or provide multiple pools.
This activity of replacing multiple copies of an object with just a few objects is often referred to as canonicalizing objects.
The Booleans provide an existing example of objects that should have been canonicalized in the JDK.
They were not, and no longer can be without breaking backward compatibility.
For Booleans, only two objects need to exist, but by allowing a new Boolean object to be created (by providing public constructors), you lose canonicalization.
The JDK should have enforced the existence of only two objects by keeping the constructors private.
Note that canonical objects have another advantage in addition to reducing the number of objects created: they also allow comparison by identity.
Deserializing Booleans would have required special handling to return the canonical Boolean.
All canonicalized objects similarly require special handling to manage serialization.
Java serialization supports the ability, when deserializing, to return specific objects in place of the object that is normally created by the default deserialization mechanism.
You are probably better off not canonicalizing all objects that could be canonicalized.
For example, the Integer class can (theoretically) have its instances canonicalized, but you need a map of some sort, and it is more efficient to allow multiple instances, rather than to manage a potential pool of four billion objects.
In this case, you can canonicalize a few integer objects, improving performance in several ways: eliminating the extra Integer creations and the garbage collections of these objects when they are discarded, and allowing comparison by identity.
There can be some confusion about whether Strings are already canonicalized.
There is no guarantee that they are, although the compiler can canonicalize Strings that are equal and are compiled in the same pass.
The String.intern( ) method canonicalizes strings in an internal table.
This is supposed to be, and usually is, the same table used by strings canonicalized at compile time, but in some earlier JDK versions (e.g., 1.0), it was not the same table.
In any case, there is no particular reason to use the internal string table to canonicalize your strings unless you want to compare Strings by identity (see Section 5.5)
Using your own table gives you more control and allows you to inspect the table when necessary.
To see the difference between identity and equality comparisons for Strings, including the difference that String.intern( ) makes, you can run the following class:
Canonicalizing objects is best for read-only objects and can be troublesome for objects that change.
If you canonicalize a changeable object and then change its state, then all objects that have a reference to the canonicalized object are still pointing to that object, but with the object's new state.
If that object has its date value changed, all objects pointing to that Date object now see a different date value.
This result may be desired, but more often it is a bug.
If you want to canonicalize changeable objects, one technique to make it slightly safer is to wrap the object with another one, or use your own (sub)class.[5] You then control all accesses and updates.
If the object is not supposed to be changed, you can throw an exception on any update method.
Alternatively, if you want some objects to be canonicalized but with copy-on-write behavior, you can allow the updater to return a noncanonicalized copy of the canonical object.
Beware that using a subclass may break the superclass semantics.
Note that it makes no sense to build a table of millions or even thousands of strings (or other objects) if the time taken to test for, access, and update objects in the table is longer than the time you are saving canonicalizing them.
One technique for maintaining collections of objects that can grow too large is the use of WeakReferences (from the java.lang.ref package in Java 2)
If you need to maintain one or more pools of objects with a large number of objects being held, you may start coming up against memory limits of the VM.
In this case, you should consider using WeakReference objects to hold onto your pool elements.
Objects referred to by WeakReferences can be automatically garbage-collected if memory gets low enough (see Section 4.3 later in this chapter)
A WeakReference normally maintains references to elements in a table of canonicalized objects.
If memory gets low, any of the objects referred to by the table and not referred to anywhere else in the application (except by other weak references) are garbage-collected.
This does not affect the canonicalization because only those objects not referenced anywhere else are removed.
The canonical object can be re-created when required, and this new instance is now the new canonical object: remember that no other references to the object exist, or the original could not have been garbage-collected.
For example, a table of canonical Integer objects can be maintained using WeakReferences.
I present it only as a clear and simple example to illustrate the use of WeakReferences.
The example has two iterations: one sets an array of canonical Integer objects up to a value set by the command-line argument; a second loops through to access the first 10 canonical Integers.
If the first loop is large enough (or the VM memory is constrained low enough), the garbage collector kicks in and starts reclaiming some of the Integer objects that are all being held by WeakReferences.
The second loop then reaccesses the first 10 Integer objects.
Even if the reference has not been garbage-collected, you have to access the underlying object and cast it to the desired type:
Another canonicalization technique often used is replacing constant objects with integers.
For example, rather than use the strings "female" and "male", you should use a constant defined in an interface:
In many ways, you can think of Reference objects as normal objects that have a private Object instance variable.
You can access the private object (termed the referent) using the Reference.get( ) method.
However, Reference objects differ from normal objects in one hugely important way.
The garbage collector may be allowed to clear Reference objects when it decides space is low enough.
For example, say you assign an object to a Reference.
Later you test to see if the referent is null.
It could be null if, between the assignment and the test, the garbage collector kicked in and decided to reclaim space:
WeakReferences and SoftReferences differ essentially in the order in which the garbage collector clears them.
Simplistically, the garbage collector does not clear SoftReference objects until all WeakReferences have been cleared.
PhantomReferences (not addressed here) are not cleared automatically by the garbage collector and are intended for use in a different way.
Sun's documentation suggests that WeakReferences could be used for canonical tables, whereas SoftReferences would be more useful for caches.
In the previous edition, I suggested the converse, giving the rationale that caches take up more space and so should be the first to be reclaimed.
But after a number of discussions, I have come to realize that both suggestions are simply misleading.
What we have are two reference types, one of which is likely to be reclaimed before the other.
So you should use both types of Reference objects in a priority system, using the SoftReference objects to hold higher-priority elements so that they are cleared later than low-priority elements.
For both caches and canonical tables, priority would probably be best assigned according to how expensive it is to recreate the object.
Prior to Version 1.3.1, SoftReferences and WeakReferences were treated fairly similarly by the VM, simply being cleared whenever they were no longer strongly (and weakly) reachable, with only a slight ordering difference.
However, from 1.3.1 on, the Sun VM started treating SoftReferences differently.
Now, SoftReferences remain alive for some time after the last time they were referenced.
The default length of time value is one second of lifetime per free megabyte in the heap.
This provides more of a differentiation between SoftReference and WeakReference behavior.
For example, to change the value to 3 seconds per free heap megabyte, you would use:
The server mode VM and client mode VM use slightly different methods to calculate the free megabytes in the heap.
The server mode VM assumes that the heap can expand to the -Xmx value and uses that as the full heap size to calculate the available free space.
The client mode VM simply uses the current heap size, deriving the actual free space in the current heap.
This means that the server VM has an increased likelihood of actually growing the heap space rather than clearing SoftReferences, even where there are SoftReferences that could otherwise be reclaimed.
This behavior is not part of any specification, so it could change in a future version.
But it is likely that some difference in behavior between WeakReferences and SoftReferences will remain, with SoftReferences being longer lived.
To complete our picture on references and how they work, we'll look in detail at the implementation and performance effects of the WeakHashMap class.
WeakHashMap is a type of Map that differs from other Maps in more than just having a different implementation.
WeakHashMap uses weak references to hold its keys, making it one of the few classes able to respond to the fluctuating memory requirements of the JVM.
This can make WeakHashMap unpredictable at times, unless you know exactly what you are doing with it.
The object passed as the key to a WeakHashMap is stored as the referent of the WeakReference object, and the value is the standard Map value.
The object returned by calling Reference.get( ) is termed the referent of the Reference object.
The key could be obtained by iterating over the keys of the HashMap.
Iterating over the keys of the WeakHashMap might obtain the key, but might not if the key has been garbagecollected.
However, when the key is collected by the garbage collector, the WeakReference object is subsequently removed from the WeakHashMap as a key, thus making the value garbagecollectable too.
The 1.4 version implements a hash table directly in the class, for improved performance.
The WeakHashMap uses its own ReferenceQueue object so that it is notified of keys that have been garbage-collected, thus allowing the timely removal of the WeakReference objects and the corresponding values.
In the 1.4 version, the queue is also checked whenever any key is accessed from the WeakHashMap.
If you have not worked with Reference objects and ReferenceQueues before, this can be a little confusing, so I'll work through an example.
The following example adds a key-value pair to the WeakHashMap, assumes that the key is garbage-collected, and records the subsequent procedure followed by the WeakHashMap:
This results in the addition of a WeakReference key added to the WeakHashMap, with the original key held as the referent of the WeakReference object.
You could do the equivalent using a HashMap like this:
For the equivalence code, I've used a subclass of WeakReference, as I'll need to override the.
At this point, you could access the value from the WeakHashMap using the original key, or another key that is equal to the original key.
Note that in order to get this equivalence, we need to implement equals( ) and hashcode( ) in the MyWeakReference class so that equal referents make equal MyWeakReference objects.
This is necessary so that the MyWeakReference wrapped keys evaluate as equal keys in Maps.
The equals( ) method returns true if the MyWeakReference objects are identical or if their referents are equal.
We now null out the reference to the original key:3
Maintaining a reference to the WeakReference object (in the RefKey variable) does not affect clearing the referent.
In the WeakHashMap, the WeakReference object key is also strongly referenced from the map, but its referent, the original key, is cleared.
The garbage collector adds the WeakReference that it recently cleared into its ReferenceQueue: that queue is the ReferenceQueue object that was passed in to the constructor of the WeakReference.
Trying to retrieve the value using a key equal to the original key would now return null.
To try this, it is necessary to use a key equal to the original key since we no longer have access to the original key; otherwise, it could not have been garbage-collected.
However, at the moment the WeakReference and the value objects are still strongly referenced by the Map.
Recall that when the garbage collector clears the WeakReference, it adds the WeakReference into the ReferenceQueue.
Now that it is in the ReferenceQueue, we need to have it processed.
Once one of the mutator methods has been called, the WeakHashMap runs through its ReferenceQueue, removing all WeakReference objects from the queue and also removing each WeakReference object as a key in its internal map, thus simultaneously dereferencing the value.
From the 1.4 version, accessing any key also causes the WeakHashMap to run through its ReferenceQueue.
In the following example, I use a dummy object to force queue processing without making any real changes to the WeakHashMap:
The equivalent code using the HashMap does not need a dummy object, but we need to carry out the equivalent queue processing:
As you can see, we take each WeakReference out of the queue and remove it from the Map.
This also releases the corresponding value object, and both the WeakReference object and the value object can now be garbage-collected if there are no other strong references to them.
Note that if you use a string literal as a key to a WeakHashMap or the referent to a Reference object, it will not necessarily be garbage-collected when the application no longer references it.
Similarly, other objects that the JVM could retain a strong reference to, such as Class objects, may also not be garbage-collected when there are no longer any strong references to them from the application, and so also should not be used as Reference object keys.
Consequently, there is no need to worry about achieving some sort of corrupt state if you try to access an object and the garbage collector is clearing keys at the same time.
Specifically, one of the mutator methods, put( ), remove( ), or clear( ), needs to be called directly or indirectly (e.g., from putAll( )) for the values to be released by the WeakHashMap.
If you do not call any mutator methods after populating the WeakHashMap, the values and WeakReference objects will never be dereferenced.
This does not apply to 1.4 or, presumably, to later versions.
However, even with 1.4, the WeakReference keys and values are not released in the background.
With 1.4, the WeakReference keys and values are only released when some WeakHashMap method is executed, giving the WeakHashMap a chance to run through the reference queue.
This means that practically every call to the WeakHashMap has one extra level of indirection it must go through (e.g., WeakHashMap.get( ) calls HashMap.get( )), which can be a significant performance overhead.
Although these are small, short-lived objects, if get( ) is used intensively this could generate a heavy performance overhead.
Unlike many other collections, WeakHashMap cannot maintain a count of elements, as keys can be cleared at any time by the garbage collector without immediately notifying the WeakHashMap.
This means that seemingly simple methods such as isEmpty( ) and size( ) have more complicated implementations than for most collections.
Consequently, size( ) is an operation that takes time proportional to the size of the WeakHashMap.
In the 1.4 implementation, size( ) processes the reference queue, then returns the current size.
This produces the perverse result that a WeakHashMap that had all its keys cleared and is therefore empty requires more time for isEmpty( ) to return than a similar WeakHashMap that is not empty.
The canonicalization techniques I've discussed are one way to avoid garbage collection: fewer objects means less to garbage-collect.
Similarly, the pooling technique in that section also tends to reduce garbage-collection requirements, partly because you are creating fewer objects by reusing them, and partly because you deallocate memory less often by holding onto the objects you have allocated.
Of course, this also means that your memory requirements are higher, but you can't have it both ways.
Another technique for reducing garbage-collection impact is to avoid using objects where they are not needed.
The primitive data types in Java use memory space that also needs reclaiming, but the overhead in reclaiming data-type storage is smaller: it is reclaimed at the same time as its holding object and so has a smaller impact.
Temporary primitive data types exist only on the stack and do not need to be garbage-collected at all; see Section 6.4
For example, an object with just one instance variable holding an int is reclaimed in one object reclaim.
If it holds an Integer object, the garbage collector needs to reclaim two objects.
Reducing garbage collection by using primitive data types also applies when you can hold an object in a primitive data-type format rather than another format.
For example, if you have a large number of objects, each with a String instance variable holding a number (e.g., "1492", "1997"), it is better to make that instance variable an int data type and store the numbers as ints, provided that conversion overhead does not swamp the benefits of holding the values in this alternative format.
Similarly, you can use an int (or long) to represent a Date object, providing appropriate calculations to access and update the values, thus saving an object and the associated garbage overhead.
Of course, you have a different runtime overhead instead, as those conversion calculations may take up more time.
A more extreme version of this technique is to use arrays to map objects: for example, see Section 11.10
Toward the end of that example, one version of the class gets rid of node objects completely, using a large array to map and maintain all instances and instance variables.
This leads to a large improvement in performance at all stages of the object life cycle.
Of course, this technique is a specialized one that should not be used generically throughout your application, or you will end up with unmaintainable code.
It should be used only when called for (and when it can be completely encapsulated)
Finally, here are some general recommendations that help to reduce the number of unnecessary objects being generated.
Reduce the number of temporary objects being used, especially in loops.
It is easy to use a method in a loop that has side effects such as making copies, or an accessor that returns a copy of some object you need only once.
Use StringBuffer in preference to the String concatenation operator (+)
This is really a special case of the previous point, but needs to be emphasized.
Be aware of which methods alter objects directly without making copies and which ones return a copy of an object.
For example, any String method that changes the string (such as String.trim( )) returns a new String object, whereas a method like Vector.setSize( ) does not return a copy.
If you do not need a copy, use (or create) methods that do not return a copy of the object being operated on.
Avoid using generic classes that handle Object types when you are dealing with basic data types.
For example, there is no need to use Vector to store ints by wrapping them in Integers.
Instead, implement an IntVector class that holds the ints directly.
All chained constructors are automatically called when creating an object with new.
Chaining more constructors for a particular object causes extra overhead at object creation, as does initializing instance variables more than once.
Be aware of the default values that Java initializes variables to:
There is no need to reinitialize these values in the constructor (although an optimizing compiler should be able to eliminate the redundant statement)
Generalizing this point: if you can identify that the creation of a particular object is a bottleneck, either because it takes too long or because a great many of those objects are being created, you should check the constructor hierarchy to eliminate any multiple initializations to instance variables.
You can avoid constructors by unmarshalling objects from a serialized stream because deserialization does not use constructors.
However, serializing and deserializing objects is a CPU-intensive procedure and is unlikely to speed up your application.
There is another way to avoid constructors when creating objects, namely by creating a clone( ) of an object.
You can create new instances of classes that implement the Cloneable interface using the clone( ) method.
These new instances do not call any class constructor, thus allowing you to avoid the constructor initializations.
Cloning does not save a lot of time because the main overhead in creating an object is in the creation, not the initialization.
However, when there are extensive initializations or many objects generated from a class with some significant initialization, this technique can help.
If you have followed the factory design pattern,[6] it is relatively simple to reimplement the original factory method to use a clone.
The factory design pattern recommends that object creation be centralized in a particular factory method.
This is actually detrimental for performance, as there is the overhead of an extra method call for every object creation, but the pattern does provide more flexibility when it comes to tuning.
If you identify a particular factory method as a bottleneck when performance tuning, you can relatively easily inline that method using a preprocessor.
For example, the original factory method can be defined similar to:
If you have not followed the factory design pattern, you may need to track down all calls that create a new instance of the relevant class and replace those calls.
Note that the cloned object is still initialized, but the initialization is not the constructor initialization.
Instead, the initialization consists of assigning exactly once to each instance variable of the new (cloned) object, using the instance variables of the object being cloned.
This allows you to manage a similar trick when it comes to initializing arrays.
But first let's see why you would want to clone an array for performance reasons.
When you create an array in code, using the curly braces to assign a newly created array to an array variable like this:
The final two sections of this chapter discuss two seemingly opposing tuning techniques.
The first section, Section 4.6.1, presents the technique of creating objects before they are needed.
This technique is useful when a large number of objects need to be created at a time when CPU power is needed for other routines and where those objects could feasibly be created earlier, at a time when there is ample spare CPU power.
The second section, Section 4.6.2, presents the technique of delaying object creation until the last possible moment.
This technique is useful for avoiding unnecessary object creation when only a few objects are used even though many possible objects can be created.
In fact, these techniques represent two sides of the same coin: moving object creation from one time to another.
Preallocating moves object creation to a time earlier than you would normally create those objects; lazy initialization moves object creation to a later time (or never)
There may be situations in which you cannot avoid creating particular objects in significant amounts: perhaps they are necessary for the application and no reasonable amount of tuning has managed to reduce the objectcreation overhead for them.
If the creation time has been identified as a bottleneck, it is possible that you can still create the objects, but move the creation time to a part of the application when more spare cycles are available or there is more flexibility in response times.
The idea here is to choose another time to create some or all of the objects (perhaps in a partially initialized stage) and store those objects until they are needed.
Again, if you have followed the factory design pattern, it is relatively simple to replace the return new Something( ) statement with an access to the collection of spare objects (presumably testing for a nonempty collection as well)
If you have not followed the factory design pattern, you may need to track down all calls that create a new instance of the relevant class and replace them with a call to the factory method.
For the real creation, you might want to spawn a background (low-priority) thread to churn out objects and add them into the storage collection until you run out of time, space, or necessity.
This is a variation of the "read-ahead" concept, and you can also apply this idea to:
Lazy initialization means that you do not initialize objects until the first time they are used.
Typically, this comes about when you are unsure of what initial value an instance variable might have but want to provide a default.
Rather than initialize explicitly in the constructor (or class static initializer), it is left until access time for the variable to be initialized, using a test for null to determine if it has been initialized.
On the other hand, there are particular design situations in which it is appropriate to use lazy initialization.
A good example is classloading, where classes are loaded dynamically as required.
This is a specific design situation in which it is clear there will be a performance impact on running applications, but the design of the Java runtime merited this for the features that it brought.
As usual, you should be tuning after functionality is present in your application, so I am not recommending using lazy initialization before the tuning stage.
But there are places where you can change objects to be lazily initialized and make a large gain.
Specifically, these are objects or variables of objects that may never be used.
For example, if you need to make available a large choice of objects, of which only a few will actually be used in the application (e.g., based on a user's choice), then you are better off not instantiating or initializing these objects until they are actually used.
An example is the char-to-byte encoding provided by the JDK.
Only a few (usually one) of these are used, so you do not need to provide every type of encoding, fully initialized, to the application.
When you have thousands of objects that need complex initializations but only a few will actually be used, lazy initialization provides a significant speedup to an application by avoiding exercising code that may never be run.
A related situation in which lazy initialization can be used for performance tuning is when there are many objects that need to be created and initialized, and most of these objects will be used, but not immediately.
In this case, it can be useful to spread out the load of object initialization so you don't get one large hit on the application.
It may be better to let a background thread initialize all the objects slowly or to use lazy initialization to take many small or negligible hits, thus spreading the load over time.
This is essentially the same technique as for preallocation of objects (see the previous section)
It is true that many of these kinds of situations should be anticipated at the design stage, in which case you could build lazy initialization into the application from the beginning.
But this is quite an easy change to make (usually affecting just the accessors of a few classes), and so there is usually little reason to over-engineer the application prior to tuning.
Most of these suggestions apply only after a bottleneck has been identified:
Reduce the number of temporary objects being used, especially in loops.
Use custom conversion methods for converting between data types (especially strings and streams) to reduce the number of temporary objects.
Define methods that accept reusable objects to be filled in with data, rather than methods that return objects holding that data.
Create only the number of objects a class logically needs (if that is a small number of objects)
Use primitive data types instead of objects as instance variables.
Avoid creating an object that is only for accessing a method.
Preallocate storage for large collections of objects by mapping the instance variables into multiple arrays.
Create or use specific classes that handle primitive data types rather than wrapping the primitive data types.
Consider using a ThreadLocal to provide threaded access to singletons with state.
Use the final modifier on instance-variable definitions to create immutable internally accessible objects.
Use WeakReferences to hold elements in large canonical lookup tables.
Use the clone( ) method to avoid calling any constructors.
Create copies of simple arrays faster by initializing them; create copies of complex arrays faster by cloning them.
Eliminate object-creation bottlenecks by moving object creation to an alternative time.
Create objects early, when there is spare time in the application, and hold those objects until required.
Use lazy initialization when there are objects or variables that may never be used, or when you need to distribute the load of creating objects.
Use lazy initialization only when there is a defined merit in the design, or when identifying a bottleneck that is alleviated using lazy initialization.
Everyone has a logger and most of them are string pigs.
A literal form (characters surrounded by double quotes, e.g., "hello")
Their own externally accessible collection in the VM and class files (i.e., string pools, which provide uniqueness of String objects if the string sequence can be determined at compile time)
Strings are immutable and have a special relationship with StringBuffer objects.
Applying a method that looks like it changes the String (such as String.trim( )) doesn't actually do so; instead, it returns an altered copy of the String.
These points have advantages and disadvantages so far as performance is concerned.
For fast string manipulation, the inability to subclass String or access the internal char array can be a serious problem.
Let's first look at the advantages of the String implementation:
At compile time, strings are resolved as far as possible.
This includes applying the concatenation operator and converting other literals to strings.
You can always check your compiler (e.g., by decompiling some statements involving concatenation) and change it if needed.
Because String objects are immutable, a substring operation doesn't need to copy the entire underlying sequence of characters.
Instead, a substring can use the same char array as the original string and simply refer to a different start point and endpoint in the char array.
This means that substring operations are efficient, being both fast and conserving of memory; the extra object is just a wrapper on the same underlying char array with different pointers into that array.[1]
Strings are implemented in the JDK as an internal char array with index offsets (actually a start offset and a character count)
This basic structure is extremely unlikely to be changed in any version of Java.
The close relationship with StringBuffers allows Strings to reference the same char array used by the StringBuffer.
For typical practice, when you use a StringBuffer to manipulate and append characters and data types, and then convert the final result to a String, this works just fine.
The StringBuffer provides efficient mechanisms for growing, inserting, appending, altering, and other types of String manipulation.
The resulting String then efficiently references the same char array with no extra character copying.
This is very fast and reduces the number of objects being used to a minimum by avoiding intermediate objects.
However, if the StringBuffer object is subsequently altered, the char array in that StringBuffer is copied into a new char array that is now referenced by the StringBuffer.
The String object retains the reference to the previously shared char array.
This means that copying overhead can occur at unexpected points in the application.
Instead of the copying occurring at the toString( ) method call, as might be expected, any subsequent alteration of the StringBuffer causes a new char array to be created and an array copy to be performed.
This allows StringBuffers to be reused with more predictable performance.
Not being able to subclass String means that it is not possible to add behavior to String for your own needs.
The previous point means that all access must be through the restricted set of currently available String methods, imposing extra overhead.
The only way to increase the number of methods allowing efficient manipulation of String characters is to copy the characters into your own array and manipulate them directly, in which case String is imposing an extra step and extra objects you may not need.
The tight coupling with StringBuffer can lead to unexpectedly high memory usage.
Obviously, this process can continue indefinitely, using vast amounts of memory where not expected.
Most methods expect a String object rather than a char array, and String objects are returned by many methods.
With extra work, most things you can do with String objects can be done faster and with less intermediate object-creation overhead by using your own set of char array manipulation methods.
For most performance tuning, you pinpoint a bottleneck and make localized changes to objects and methods that speed up that bottleneck.
But String tuning often involves converting to char arrays, whereas you rarely come across public methods or interfaces that deal in char arrays.
This makes it difficult to switch between Strings and char arrays in any localized way.
The consequences are that you either have to switch back and forth between Strings and char arrays, or you have to make extensive modifications that can reach across many application boundaries.
Sun recognizes that Strings are not the optimal solution in many cases and has added a CharSequence interface in JDK 1.4 that String and other classes implement.
New methods have been added that operate on CharSequence objects rather than requiring Strings.
This doesn't necessarily help your particular bottleneck, and CharSequences still access the char elements through a charAt( ) method, but it does at least increase the options available for optimizing applications.
But in many cases, internationalized Strings form a specific subset of String usage in an application, mainly in the user interface, and that subset of Strings rarely causes bottlenecks.
Note also that internationalized Strings can be treated as char arrays for some types of processing without any problems; see Section 5.4.2 later in this chapter.
My editor Mike Loukides summarized this succinctly with the statement, "Avoid using String objects if you don't intend to represent text."
For optimized use of Strings, you should know the difference between compile-time resolution of Strings and runtime creation.
At compile time, Strings are resolved to eliminate the concatenation operator if possible.
However, when an expression involving String concatenation cannot be resolved at compile time, the concatenation must execute at runtime.
Generally, the JDK methods that convert objects and data types to strings are suboptimal, both in terms of performance and the number of temporary objects used in the conversion procedure.
In this section, we consider how to optimize these conversions.
In the JDK, this is achieved with the Long.toString( ) method.
Bear in mind that you typically add a converted value to a StringBuffer (explicitly, or implicitly with the + concatenation operator)
So it would be nice to avoid the two intermediate temporary objects created while converting the long, i.e., the one char array inside the conversion method, and the returned String object that is used just to copy the chars into the StringBuffer.
Avoiding the temporary char array is difficult to do because most fast methods for converting numbers start with the low digits in the number, and you cannot add to the StringBuffer from the low to the high digits unless you want all your numbers coming out backwards.
However, with a little work, you can get to a method that is fast and obtains the digits in order.
The following code works by determining the magnitude of the number first, then successively stripping off the highest digit, as shown.
There are several things to note about possible variations of this algorithm.
First, although the algorithm here is specifically radix 10 (decimal), it is easy to change to any radix.
To do this, the reduction in magnitude in the loop has to go down by the radix value, and the l_magnitude( ) method has to be altered.
Finally, if you want formatting added in, the algorithm is again suitable because you proceed through the number in written order, and also because you have the magnitude at the start.
You can easily create another method, similar to magnitude( ), that returns the number of digits in the value.
You can put in a comma every three digits as the number is being written (or apply whatever internationalized format is required)
This saves you having to write out the number first in a temporary object and then add formatting to it.
For example, if you are using integers to fake fixed-place floating-point numbers, you can insert a point at the correct position without resorting to temporary objects.
While the previous append( ) version is suitable to use for ints by overloading, it is much more efficient to create another version specifically for ints.
This is because int arithmetic is optimal and considerably faster than the long arithmetic being used.
To better this already optimized performance, you need every optimization available.
There are three changes you can make to the long conversion algorithm already presented.
This gives a significant speedup (more than a third faster than the long conversion)
And finally, you can unroll the loop that handles the digit-by-digit conversion.
In this case, the loop can be completely unrolled since there are at most 10 digits in an int.
However, the comparison against the latest versions of the various VMs now shows a completely different story (see Table 5-3)
Sun has continued to optimize, especially object creation and garbage collection in the VM, as well as the conversion algorithm.
The improvement in garbage collection is obvious if you run the comparison test with the -verbosegc parameter.
With garbage collection being reported, the much larger volume of garbage slows down the JDK conversion relative to the proprietary algorithm.
Without -verbosegc, the extra temporary objects are still overhead, but not as significant as with earlier VMs.
It is also instructive to see what Sun has done to the algorithm to make the conversion faster.
One optimization is to reduce the number of temporary objects created by using a privileged String constructor that accepts a passed char array rather than creating a new one.
But the major algorithmic optimization is that multiplications have been changed to bit-shifts.
For example, instead of multiplying by 100, three bit-shifts are used:
Time taken to append an int to a StringBuffer (from the first edition)
Time taken to append an int to a StringBuffer (current version)
You can use the int conversion method for bytes and shorts (using overloading)
You can make byte conversion even faster using a String array as a lookup table for the 256 byte values.
This means that the int conversion algorithm shown previously, when applied to bytes and shorts, is significantly faster than the JDK conversions and does not produce any temporary objects.
Converting floating-point numbers to strings turns out to be hideously under-optimized in every version of the JDK up to 1.4 (and maybe beyond)
Looking at the JDK code and comments, it seems that no one has yet got around to tuning these conversions.
Floating-point numbers can be converted using similar optimizations to the number conversions previously addressed.
You need to check for and handle the special cases separately.
You then scale the floats into an integer value and use the previously defined int conversion algorithm to convert to characters in order, ensuring that you format the decimal point at the correct position.
Finally, it would be possible to overload the float and double case, but it turns out that if you do this, the float does not convert as well (in correctness or speed), so it is necessary to duplicate the algorithms for the float and double cases.
Note that the printed values of floats and doubles are, in general, only representative of the underlying value.
This is true both for the JDK algorithms and the conversions here.
There are times when the string representation comes out differently for the two implementations, and neither is actually more accurate.
The algorithm used by the JDK prints the minimum number of digits possible, while maintaining uniqueness of the printed value with respect to the other floating-point values adjacent to the value being printed.
The algorithm presented here prints the maximum number of digits (not including trailing zeros) regardless of whether some digits are not needed to distinguish the number from other numbers.
Note that the code that follows shortly uses the previously defined append( ) method for appending longs to StringBuffers.
The double conversion (see the next section) is similar to the float conversion, with all the same advantages.
In addition, both algorithms are several times faster than the JDK conversions.
Normally, when you print out floating-point numbers, you print in a defined format with a specified number of digits.
The default floating-point toString( ) methods cannot format floating-point numbers; you must first create the string, then format it afterwards.
The algorithm presented here could easily be altered to handle formatting floating-point numbers without using any intermediate strings.
This algorithm is also easily adapted to handle rounding up or down; it already detects which side of the "half" value the number is on:
Converting Objects to Strings is also inefficient in the JDK.
For a generic object, the toString( ) method is usually implemented by calling any embedded object's toString( ) method, then combining the embedded strings in some way.
For example, Vector.toString( ) calls toString( ) on all its elements and combines the generated substrings with the comma character surrounded by opening and closing square brackets.
Although this conversion is generic, it usually creates a huge number of unnecessary temporary objects.
If the JDK had taken the "printOn: aStream" paradigm from Smalltalk, the temporary objects used would be significantly reduced.
This paradigm basically allows any object to be appended to a stream.
With this framework in place, I find that performance is generally improved because the application uses more efficient conversion algorithms and reduces the number of temporary objects.
In almost every respect, this framework is better than the simpler framework, which supports only the toString( ) method.
In one of my first programming courses, in the language C, our instructor made an interesting comment.
He said, "C has lightning-fast string handling because it has no string type." He went on to explain this oxymoron by pointing out that in C, any null-terminated sequence of bytes can be considered a string: this convention is supported by all string-handling functions.
The point is that since the convention is adhered to fairly rigorously, there is no need to use only the standard string-handling functions.
Any string manipulation you want to do can be executed directly on the byte array, allowing you to bypass or rewrite any string-handling functions you need to speed up.
Because you are not forced to run through a restricted set of manipulation functions, it is always possible to optimize code using your own hand-crafted functions.
This can be a source of bugs, but is another reason speed can be optimized.
In Java, the inability to subclass String or access its internal char array means you cannot use the techniques applied in C.
Even if you could subclass String, this does not avoid the second problem: many other methods operate on or return copies of a String.
Generally, there is no way to avoid using String objects for code external to your application classes.
But internally, you can provide your own char array type that allows you to manipulate strings according to your needs.
As an example, let's look at a couple of simple text-parsing problems: first, counting the words in a body of text, and second, using a filter to select lines of a file based on whether they contain a particular string.
Let's look at the typical Java approach to counting words in a text.
I use the StreamTokenizer for the word count, as that class is tailor-made for this kind of problem.
The only difficulty comes in defining what a word is and coaxing the StreamTokenizer to agree with that definition.
To keep things simple, I define a word as any contiguous sequence of alphanumeric characters.
This means that words with apostrophes and numbers with decimal points count as two words, but I'm more interested in the performance than the niceties of word definitions here, and I want to keep the implementation simple.
I suspect the curious results and huge differences may have something to do with StreamTokenizer being a severely underoptimized class, as well as being too generic a tool for this particular test.
Object-creation differences of this order of magnitude impose a huge overhead on the StreamTokenizer implementation, explaining why the StreamTokenizer is so much slower than the char array implementation.
The object-creation overhead also explains why both the JIT and non-JIT tests took similar times for the StreamTokenizer.
The times also show that the VMs are getting much much better at reducing object creation and garbage collection overhead.
Object monitoring is easily done using the monitoring tools from Chapter 2, both the object-creation monitor and the verbosegc option with an explicit System.gc( ) at the end of the test.
This contrasts with the previous methodology using a dedicated class (StreamTokenizer), which turned out to be extremely inefficient.
The readline( ) method should present us with more of a performance-tuning challenge, as it is relatively much simpler and so should be more efficient.
I'll also add case-independence to the filtering, i.e., the lines will be selected even if the case of the characters in the line do not exactly match the case of the characters in the filter.
In fact, searches in some languages allow words to match even if they are spelled differently.
For example, when searching for a French word that contains an accented letter, the user might expect a nonaccented spelling to match.
This is similar to searching for the word "color" and expecting to match the British spelling "colour."
Such sophistication depends on how extensively the application supports this variation in spelling.
The full commented listing for the char array implementation is shown shortly.
Obviously you have to work a lot harder to get the performance you want.
The line lengths of the test files make a big difference, hence the variation in results.
Filter timings using filter or cfilter method on a short-line file.
Filter timings using filter or cfilter method on a long-line file.
I have implemented a relatively straightforward class for the char array parsing.
Tuning like this takes effort, but you can see that it is possible to use char arrays to very good effect for most types of String manipulation.
If you are an object purist, you may want to encapsulate the char array access.
Otherwise, you may be content to expose external access through static methods.
In any case, it is worth investing some time and effort in creating a usable char-handling class.
If the classes are well constructed, you can use them consistently within your applications, and this effort pays off handsomely when it comes to tuning (or, occasionally, the lack of a need to tune)
Here is the commented char array implementation that executes a line-by-line string-matching filter on a file:
String methods were also added as shortcuts to using the regular-expression objects.
Regular expressions are a pattern-matching language that provides a powerful mechanism to determine if sequences of characters contain particular patterns and to extract those patterns.
Almost every type of parsing is much easier and more flexible using regular expressions.
However, be aware that the methods in the String class are adequate for one-off uses of regular expressions but are inefficient for repeated application of a regular expression.
The line-filtering example in the previous section is a fairly simple problem and doesn't need the full power of regular expressions, but since we have already seen the equivalent functionality in alternative implementations it is worth looking at the cost of using regular expressions to handle the filtering.
The method required is straightforward, but I'll walk through it in case you are unused to regular expressions.
First, the regular-expression pattern needs to be compiled into a Pattern object.
The .* pattern simply indicates that anything can match between the beginning and the end of the line as long as the filter string is included.
In addition, the Pattern object needs to know that we are searching line-by-line, as it also supports searching text while treating line endings simply as any other characters, so we use the MULTILINE flag.
We also need the CASE_INSENSITIVE flag to make the match case-insensitive.
We could use a String, but if we were actually reading from a file the most efficient mechanism would be to use a CharBuffer on a FileChannel, so I'll stay with the CharBuffer.
Finally, we simply loop, repeatedly matching the regular expression against the text using the Matcher.find( ) method.
The Matcher.group( ) method provides us with the previously matched line of text if we need it for printing:
String comparison performance is highly dependent on both the string data and the comparison algorithm (this is really a truism about collections in general)
The methods that come with the String class have a performance advantage in being able to directly access the underlying char collection.
So if you need to make String comparisons, String methods usually provide better performance than your own methods, provided that you can make your desired comparison fit in with one of the String methods.
Another necessary consideration is whether comparisons are case-sensitive or -insensitive, and I will consider this in more detail shortly.
To optimize for string comparisons, you need to look at the source of the comparison methods so you know exactly how they work.
It checks for null, and then for strings being the same size (the String type check is not needed, since this method accepts only String objects)
Immediately, you see that the more differences there are between the two strings, the faster these methods return.
This behavior is common for collection comparisons, and the order of the comparison is crucial.
In these two cases, the strings are compared starting with the first character, so the earlier the difference occurs, the faster the methods return.
However, equals( ) returns faster if the two String objects are identical.
It is unusual to check Strings by identity, but there are a number of situations where it is useful (for example, when you are using a set of canonical Strings; see Chapter 4)
Another example is when an application has enough time during string input to intern( ) [5] the strings, so that later comparisons by identity are possible.
String.intern( ) returns the String object that is being stored in the internal VM string pool.
In any case, equals( ) returns immediately if the two strings are identical, but equalsIgnoreCase( ) does not even check for identity (which may be reasonable given what it does)
This results in equals( ) running an order of magnitude faster than equalsIgnoreCase( ) if the two strings are identical; identical strings is the fastest test case resolvable for equals( ), but the slowest case for equalsIgnoreCase( )
On the other hand, if the two strings are different in size, equalsIgnoreCase( ) has only two tests to make before it returns, whereas equals( ) makes four tests before it returns.
This can make equalsIgnoreCase( ) run 20% faster than equals( ) for what may be the most common difference between strings.
In almost every possible case of string data, equals( ) runs faster (often several times faster) than equalsIgnoreCase( )
However, in a test against the words from a particular dictionary, I found that over 90% of the words were different in size from a randomly chosen word.
When comparing the performance of these two methods for a comparison of a.
The many cases in which strings had different lengths compensated almost exactly for the slower comparison of equalsIgnoreCase( ) when the strings were similar or equal.
This illustrates how the data and the algorithm interplay with each other to affect performance.
Even though String methods have access to the internal chars, it can be faster to use your own methods if there are no String methods appropriate for your test.
You can build methods that are tailored to the data you have.
One way to optimize an equality test is to look for ways to make the strings identical.
An alternative that can actually be better for performance is to change the search strategy to reduce search time.
For example, a linear search through a large array of Strings is slower than a binary search through the same size array if the array is sorted.
This, in turn, is slower than a straight access to a hashed table.
Note that when you are able and willing to deploy changes to JDK classes (e.g., for servlets), you can add methods directly to the String class.
Several of my colleagues have emphasized their view that changes to the JDK sources lead to severe maintenance problems.
When case-insensitive searches are required, one standard optimization is to use a second collection containing all the strings uppercased.
This second collection is used for comparisons, obviating the need to repeatedly uppercase each character in the search methods.
For example, if you have a hash table containing String keys, you need to iterate over all the keys to match keys case-insensitively.
But, if you have a second hash table with all the keys uppercased, retrieving the key simply requires you to uppercase the element being searched for:
This means that the Unicode character set is the lingua franca in Java.
Unfortunately, because Unicode uses two-byte characters, many string libraries based on one-byte characters that can be ported into Java do not work so well.
Most string-search optimizations use tables to assist string searches, but the table size is related to the size of the character set.
For example, a traditional Boyer-Moore string search takes a great deal of memory and a long initialization phase to use with Unicode.
The Boyer-Moore string search uses a table of characters to skip comparisons.
The "abcd" is aligned against the first four characters of the string.
If that fourth character is none of a, b, c, or d, the "abcd" can be skipped to be matched against the fifth to eighth characters, and the matching proceeds in the same way.
If instead the fourth character of the string is b, the "abcd" can be skipped to align the b against the fourth character, and the matching proceeds as before.
For optimum speed, this algorithm requires several arrays giving skip distances for each possible character in the character set.
Furthermore, sorting international Strings requires the ability to handle many kinds of localization issues, such as the sorted location for accented characters, characters that can be treated as character pairs, and so on.
In these cases, it is difficult (and usually impossible) to handle the general case yourself.
There is also a useful StringSearch class available at the IBM alphaWorks site (http://alphaworks.ibm.com/tech/stringsearch)
You can use this when sorting an array of Strings, for example.
It is probably easiest to see how to use collation keys with a particular example.
For this, I use a standard quicksort algorithm (the quicksort implementation can be found in Section 11.9)
The only modification to the standard quicksort is that for each optimization, the quicksort needs to be adjusted to use the appropriate comparison method and the appropriate data type.
Only the comparison method changes (and in general the data type too, though not in these examples where the data type was Object)
The obvious first test, to get a performance baseline, is the straightforward internationalized sort:
The kind of investment made in building such global support is beyond most projects; it is almost always much cheaper to buy the support.
In this case, Taligent put a huge number of man years into the globalization you get for free with the JDK.
But is this test incompatible with the desired internationalized sort? Well, maybe not.
Sort algorithms usually execute faster if they operate on a partially sorted array.
Of course, these optimizations have improved the situation only for the particular locale I have tested (my default locale is set for US English)
However, running the test in a sampling of other locales (European and Asian locales), I find similar relative speedups.
Without using locale-specific dictionaries, this locale variation test may not be fully valid, but the speedup will likely hold across all Latinized alphabets.
You can also create a simple partial-ordering class-specific sort to some locales, which provides a similar speedup.
But, as shown here, you can certainly improve the performance.
Most of these suggestions apply only after a bottleneck has been identified:
Regular expressions provide acceptable performance compared with using String searching methods and String character iteration tokenizing techniques.
Create and optimize your own framework to convert objects and primitives to and from strings.
Use the string concatenation operator to create Strings at compile time.
Specify when the underlying char array is copied when reusing StringBuffers.
Improve access to the underlying String char array by copying the chars into your own array.
Manipulate characters in char arrays rather than using String and StringBuffer manipulation.
Optimize the string comparison and search algorithm for the data being compared and searched.
Apply the standard performance optimization for case-insensitive access (maintaining a second collection with all strings uppercased)
For every complex problem, there is a solution that is simple, neat, and wrong.
This chapter describes the costs of various programmatic elements, including exceptions, assertions (new in 1.4), casts, and variables.
It also describes how to optimize your use of these elements.
In this section, we examine the cost of exceptions and consider ways to avoid that cost.
First, we look at the costs associated with try-catch blocks, which are the structures you need to handle exceptions.
Then, we go on to optimizing the use of exceptions.
The following test determines whether a VM imposes any significant overhead for try-catch blocks when the catch block is not entered.
The test runs the same code twice, once with the try-catch entered for every loop iteration and again with just one try-catch wrapping the loop.
Because we're testing the VM and not the compiler, you must ensure that your compiler has not optimized the test away; use an old JDK version to compile it if necessary.
To determine that the test has not been optimized away by the compiler, you need to compile the code, then decompile it:
Extra cost of the looped try-catch test relative to the nonlooped try-catch test.
Throwing an exception and executing the catch block has a significant overhead.
This overhead seems to be due mainly to the cost of getting a snapshot of the stack when the exception is created (the snapshot allows the stack trace to be printed)
The cost is large: exceptions should not be thrown as part of the normal code path of your application unless you have factored it in.
Generating exceptions is one place where good design and performance go hand in hand.
You should throw an exception only when the condition is truly exceptional.
For example, an end-of-file condition is not an exceptional condition (all files end) unless the end-of-file occurs when more bytes are expected.[1] Generally, the performance cost of throwing an exception is equivalent to several hundred lines of simple code executions.
If your application is implemented to throw an exception during the normal flow of the program, you must not avoid the exception during performance tests.
Any time costs coming from throwing exceptions must be included in performance testing, or the test results will be skewed from the actual performance of the application after deployment.
To find the cost of throwing an exception, compare two ways of testing whether an object is a member of a class: trying a cast and catching the exception if the cast fails, versus using instanceof.
In the code that follows, I have highlighted the lines that run the alternative tests:
The 1.4 JVM JIT compiler in server mode identified that the test was effectively a repeated constant expression and collapsed the loop to one call, thus eliminating the test.
The costs of using exceptions are still present in 1.4.0 server mode, but this test cannot show those costs.
For VMs not running a JIT, or using HotSpot technology, the relative times for test2( ) are different depending on the object passed.
This difference for a false result indicates that the instanceof operator is faster when the instance's class correctly matches the tested class.
A negative instanceof test must also check whether the instance is from a subclass or interface of the tested type before it can definitely return false.
Given this, it is actually quite interesting that with a simple JIT, there is no difference in times between the two instanceof tests.
Because it is impossible to add methods to classes that are compiled (as opposed to classes you have the source for and can recompile), there are necessarily places in Java code where you have to test for the type of object.
Where this type of code is unavoidable, you should use instanceof, as shown in test2( ), rather than a speculative class cast.
There is no maintenance disadvantage in using instanceof, nor is the code any clearer or easier to alter by avoiding its use.
I strongly advise you to avoid the use of the speculative class cast, however.
It is a real performance hog and ugly as well.
You may decide that you definitely require an exception to be thrown, despite the disadvantages.
Most of the cost of throwing an exception is incurred in actually creating the new exception, which is when the stack trace is filled in.
Reusing an existing exception object without resetting the stack trace avoids the exception-creation overhead.
Throwing and catching an existing exception object is two orders of magnitude faster than doing the same with a newly created exception object:
To get the exception object to hold the stack trace that is current when it is thrown, rather than created, you must use the fillInStackTrace( ) method.
Of course, this is what causes the large overhead that you are trying to avoid.
However, this technique cannot eliminate all types of code blocks.
For example, you cannot use this technique to eliminate trycatch blocks from the code they surround.
You can achieve that level of control only by using a preprocessor.
Assertions allow you to add statements into your code of the form:
If the boolean expression evaluates to false, an AssertionError is thrown.
More precisely, assertions can be enabled or disabled at runtime, and you can specify separately for each class whether its assertions are enabled or disabled using the -ea and -da parameters of the java command.
The assertion status of a class is fixed once that class has been loaded.
The only limitation of the assert keyword is that it must be used in an executable block.
An assertion cannot be used with class variable declarations, for instance, but can be placed within any method.
Using assertions generally improves the quality of code and assists in diagnosing problems in an application.
Application code should be written so that it is functionally the same when running with assertions disabled.
Assertions that cannot be disabled without altering the functionality of the code should not be defined as assertions.
The assert keyword is not recognized by compilers prior to 1.4
Unfortunately, the resulting bytecode can not be run under pre-1.4 JVMs.
It is useful to understand how the assertion mechanism works to see how assertion statements can affect performance.
The assertion itself is compiled into a statement of the form:
There are two potential optimizations that could completely eliminate this assertion overhead.
First, the classloader itself could strip out the assertion status test and the subsequent if statement when assertions are disabled.
In practice, however, Sun has not enabled the classloader to strip the assertion status test.
This one uses exactly the same procedure outlined for the classloader optimization but, instead of the classloader, the JIT compiler would strip out the unnecessary statements.
In fact, this simply applies the standard compiler optimization of eliminating dead code.
This is the approach taken by Sun, which has left the optimization to the HotSpot JIT compiler.
This means that, at least for the initial 1.4 release, the overhead of assert statements is dependent on whether a JVM strips those statements.
For example, the 1.4 JVM running in client (default) mode is not sufficiently aggressive in JIT compilation optimizations to eliminate assertion overhead.
However, when running the same JVM in server mode (with the -server parameter), the JIT compiler effectively eliminates disabled assertion statements.
As the overhead of the assertion depends on what it is being compared against, I used two separate baselines, comparing assertion cost against a very quick test, essentially just a return statement, and against a more complex, slower test.
The test class code is listed shortly after the results.
Overhead from an assertion statement in a very simple method.
The server mode JIT compiler inlined the quick test into the test loop, resulting in no method call overhead at all and a legible measurement for test time.
Interestingly, the server mode fails to do this when the assert is disabled.
You may want to avoid adding assertions willy-nilly to setters, getters, and other short, frequently called methods.
However, there is no need to become paranoid about whether or not to add assertions.
It is probably better initially to add assertions as desired irrespective of performance considerations, then catch and eliminate any expensive assertions using a.
For longer methods, assertions can be added without too much concern.
But do note that when enabled, any assertion takes at least as long to run as its boolean_expression evaluation takes.
Consequently, code running with assertions enabled will definitely be slower than code running with assertions disabled, even if only a few percent slower.
If possible, you should run the application with as many assertions disabled as possible.
Similarly, since assertions can be turned off but explicit checks cannot, you should consider changing all explicit checks for incorrect parameters and state in your code to use assertions instead of explicitly using if...throw statements.
The decision about whether any particular test can be changed to an assertion ultimately comes down to whether the test should always be present (don't make it an assertion), or whether the test is optional and provides extra robustness, especially during development and testing (definitely an assertion candidate)
Finally, remember to profile the application as it will be run in practice, with the same mixture of assertions turned on or off.
Don't make the mistake of profiling the application with all assertions turned off or turned on if that is not the way the application will be run when deployed.
Casts that can be resolved at compile time can be eliminated by the compiler (and are eliminated by the JDK compiler)
But a primitive data type cast is still a runtime operation and has an associated cost.
Object type casts basically confirm that the object is of the required type.
It appears that a VM with a JIT compiler is capable of reducing the cost of some casts to practically nothing.
The following test, when run under JDK 1.2 without a JIT, shows object casts having a small but measurable cost.
With the JIT compiler running, the cast has no measurable effect (see Table 6-5):
However, the cost of an object type cast is not constant: it depends on the depth of the hierarchy and whether the casting type is an interface or a class.
Interfaces are generally more expensive to use in casting, and the further back in the hierarchy (and ordering of interfaces in the class definition), the longer the cast takes to execute.
Remember, though: never change the design of the application for minor performance gains.
It is best to avoid casts whenever possible; for example, use type-specific collection classes instead of generic collection classes.
Rather than use a standard List to store a list of Strings, you gain better performance with a StringList class.
You should always try to type the variable as precisely as possible.
In Chapter 9, you can see that by rewriting a sort implementation to eliminate casts, the sorting time can be halved.
If a variable needs casting several times, cast once and save the object into a temporary variable of the cast type.
Use that temporary variable instead of repeatedly casting; avoid the following kind of code:
Local (temporary) variables and method-argument variables are the fastest variables to access and update.
Local variables remain on the stack, so they can be manipulated directly; the manipulation of local variables depends on both the VM and the underlying machine implementation.
Heap variables (static and instance variables) are manipulated in heap memory through the Java VM-assigned bytecodes that apply to these variables.
There are special bytecodes for accessing the first four local variables and parameters on a method stack.
Arguments are counted first; then, if there are fewer than four passed arguments, local variables are counted.
Theoretically, this means that methods with no more than three parameters and local variables combined (four for static methods) should be slightly faster than equivalent methods with a larger number of parameters and local variables.
It also means that any variables allocated the special bytecodes should be slightly faster to manipulate.
In practice, I have found any effect to be small or negligible, and it is not worth the effort involved to limit the number of arguments and variables.
Instance and static variables can be up to an order of magnitude slower to operate on when compared to method arguments and local variables.
You can see this clearly with a simple test comparing local and static loop counters:
The cost of nonlocal loop variables relative to local variables.
The 1.4 JVM JIT compiler in server mode identified that the test was effectively a repeated constant expression and collapsed the loop to one call, thus eliminating the test.
Other tests have shown that the costs of static and array elements compared to local variables are still present in 1.4.0 server mode, but this test cannot show those costs.
If you are making many manipulations on an instance or static variable, it is better to execute them on a temporary variable, then reassign to the instance variable at the end.
This is true for instance variables that hold arrays as well.
Arrays also have an overhead, due to the range checking Java provides.
So if you are manipulating an element of an array many times, again you should probably assign it to a temporary variable for the duration.
For example, the following code fragment repeatedly accesses and updates the same array element:
The Java specification allows longs and doubles to be stored in more than one action.
If you have one specific target environment, you can test it to determine its implementation.
When executing arithmetic with the primitive data types, ints are undoubtedly the most efficient.
They then require a cast back if you want to end up with the data type you started with.
For example, adding two bytes produces an int and requires a cast to get back a byte.
Note that temporary variables of primitive data types (i.e., not objects) can be allocated on the stack, which is usually implemented using a faster memory cache local to the CPU.
Temporary objects, however, must be created from the heap (the object reference itself is allocated on the stack, but the object must be in the heap)
This means that operations on any object are invariably slower than on any of the primitive data types for temporary variables.
Also, as soon as variables are discarded at the end of a method call, the memory from the stack can immediately be reused for other temporaries.
But any temporary objects remain in the heap until garbage collection reallocates the space.
The result is that temporary variables using primitive (nonobject) data types are better for performance.
As I said at the beginning of the last section, method parameters are low-cost, and you normally don't need to worry about the cost of adding extra method parameters.
But it is worth being alert to situations in which there are parameters that could be added but have not been.
This is a simple tuning technique that is rarely considered.
Typically, the parameters that could be added are arrays and array lengths.
For example, when parsing a String object, it is common not to pass the length of the string to methods because each method can get the length using the String.length( ) method.
But parsing tends to be intensive and recursive, with lots of method calls.
Most of those methods need to know the length of the string.
Although you can eliminate multiple calls within one method by assigning the length to a temporary variable, you cannot do that when many methods need that length.
Passing the string length as a parameter is almost certainly cheaper than repeated calls to String.length( )
Similarly, you typically access the elements of the string one at a time using String.charAt( )
But again, it is better for performance purposes to copy the String object into a char array and pass this array through your methods (see Chapter 5)
To provide a possible performance boost, try passing extra values and arrays to isolated groups of methods.
As usual, you should do this only when a bottleneck has been identified, not throughout an implementation.
Finally, you can reduce the number of objects used by an application by passing an object into a method, which then fills in the object's fields.
This is almost always more efficient than creating new objects within the method.
See Section 4.2.3 for a more detailed explanation of this technique.
Most of these suggestions apply only after a bottleneck has been identified:
Include all error-condition checking in blocks guarded by if statements.
Avoid throwing exceptions in the normal code path of your application.
Investigate whether a try-catch in the bottleneck imposes any extra cost.
Use instanceof instead of making speculative class casts in a try-catch block.
Consider throwing exceptions without generating a stack trace by reusing a previously created instance.
Include any exceptions generated during the normal flow of the program when running performance tests.
Assertions add overhead even when disabled, though an optimizing JIT compiler can eliminate the overhead (only HotSpot server mode succeeded in 1.4.0)
Use temporary variables of the cast type, instead of repeatedly casting.
Use local variables rather than instance or static variables for faster manipulation.
Use temporary variables to manipulate instance variables, static variables, and array elements.
Use primitive data types instead of objects for temporary variables.
Consider accessing instance variables directly rather than through accessor methods.
Add extra method parameters when that would allow a method to avoid additional method calls.
I have made this letter longer than usual because I lack the time to make it shorter.
This chapter describes performance-tuning a variety of common code structures: loops, switches, and recursion.
Some of the tuning hints here are straightforward (for example, remove code from a loop if it is executed only once), but many are more esoteric, particularly given the subtleties of optimizations performed by HotSpot VMs and JIT compilers.
There are many optimizations that can speed up loops, as detailed in the following sections.
Take out of the loop any code that does not need to be executed on every pass.
This includes assignments, accesses, tests, and method calls that need to run only once.
Method calls are more costly than the equivalent code without the call, and by repeating method calls again and again, you just add overhead to your application.
Move any method calls out of the loop, even if this requires rewriting.
Array access (and assignment) always has more overhead than temporary variable access because the VM performs bounds-checking for array-element access.
Array access is better done once (and assigned to a temporary) outside the loop rather than repeated at each iteration.
This optimized loop is significantly better (twice as fast) than the original loop:
Using int data types for the index variable is faster than using any other numeric data types.
Operations on bytes, shorts, and chars are normally carried out with implicit casts to and from ints.
In the latter case, using your own for loop may be slightly faster.
Comparison to 0 is faster than comparisons to most other numbers.
Only non-JIT VMs and HotSpot showed improvements by rewriting the loop.
Note that HotSpot does not generate native code for any method executed only once or twice.
For example, convert equality comparisons to identity comparisons whenever possible.
You may be able to achieve this by canonicalizing your objects (see Section 4.2.4)
You can compare Strings by identity if you String.intern( ) them to ensure you have a unique String object for every sequence of characters, but obviously there is no performance gain if you have to do the interning within the loop or in some other time-critical section of the application.
But they impose a heavy overhead in requiring a method call for every comparison and may be better avoided in special situations (see Chapter 9)
It is more efficient to store an instance of the class in a static variable and test directly against that instance (there is only one instance of any class):
When several boolean tests are made together in one expression in the loop, try to phrase the expression so that it " short-circuits" as soon as possible by putting the most likely case first (see the sidebar Short-Circuit Operators)
Ensure that by satisfying earlier parts of the expression, you do not cause the later expressions to be evaluated.
Their left side is evaluated first, and their right side is not evaluated at all if the result of the left side produces a conclusive result for the expression.
Specifically, the conditional-And operator, &&, evaluates its right side only if the result of its left operand is true.
The conditional-Or operator, ||, evaluates its right side only if the result of its left operand is false.
The following example illustrates the differences between these two types of logical operators by testing both boolean And operators:
In this case, the given ordering of tests is the worst possible ordering for the expression.
Using reflection to execute a method is much slower than direct execution (as well as being bad style)
When reflection functionality is necessary within a loop, change any implementation so that you can achieve the same effect using interfaces and type overloading.
For the 1.4 VMs, Sun targeted reflection as one of the areas to be speeded up.
Some reflection operations are significantly faster than before 1.4, but reflection is still slower than using an interface to call a method.
Note that it is not just the resolution of a method that causes overhead when using reflection.
Invoking method calls using Method.invoke( ) is also more expensive than using the plain method call.
Handling method references can be complicated, especially with VMs supporting natively compiled code.
It can be necessary to manage artificial stack frames that impose overhead to the method calls.
In the java.io package, the Reader (and Writer) classes provide character-based I/O (as opposed to byte-based I/O)
The InputStreamReader provides a bridge from byte to character streams.
It reads bytes and translates them into characters according to a specified character encoding.
If no encoding is specified, a default converter class is provided.
For applications that spend a significant amount of time reading, it is not unusual to see the convert( ) method of this encoding class high up on a profile of how the application time is spent.
It is instructive to examine how this particular conversion method functions and to see the effect of a tuning exercise.
Examining the bytecodes of the convert( ) method[2] where most of the time is being spent, you can see that the bytecodes correspond to the following method (the Exception used is different; I have just used the generic Exception class):
The convert method is a method in one of the sun.* packages, so the source code is not available.
I have chosen the convert method from the default class used in some ASCII environments, the ISO 8859_1 conversion class.
Array-lookup speeds are highly dependent on the processor and the memory-access instructions available from the processor.
The lookup speed is also dependent on the compiler taking advantage of the fastest memory-access instructions available.
It is possible that other processors, VMs, or compilers will produce lookups faster than the cast.
But we have gained an extra option from these two tests.
It is now clear that we can map all the bytes to chars through an array.
Cleaning up the method slightly, we can see that the temporary variable, i1, which was previously required for the test, is no longer needed.
Being assiduous tuners and clean coders, we eliminate it and retest so that we have a new baseline to start from.
Astonishingly (to me at least), this speeds up the test measurably in some VMs.
The average test time is now even better, though again, a couple of VMs are still slower than the original method.
Some VMs incurred a definite overhead from the redundant temporary variable in the loop: a lesson to keep in mind for general tuning.
It may be worth testing to see if an int array performs better than the char array (MAP3) previously used, since ints are the faster data type.
And indeed, changing the type of this array and putting a char cast in the loop improves times slightly for some but not all VMs, and on average times are worse.
More to the point, after this effort, we have not really managed a speedup consistent enough or good enough to justify the time spent on this tuning exercise.
Now I'm out of original ideas, but one of my readers, Jesper Larsson from Sweden, has thought of a better way to map the chars to bytes.
Jesper noticed that the conversion corresponds to a simple bitwise operation, guaranteed by the Java language specification to work.
Start by eliminating expressions from the loop that do not need to be repeatedly called, and move the other boolean test (the one for the out-of-range Exception) out of the loop.
I am taking the trouble to make the method functionally identical to the original.
The original version filled in the array until the actual out-of-range exception is encountered, so I do the same.
If you throw the exception as soon as you establish the index is out of range, the code will be slightly more straightforward.
Other than that, the loop is the same as before, but without the out-of-range test and without the temporary assignment.
The average test result is now the fastest we've obtained on any tests on all VMs.
We've shaved off a third to a half of the time spent in this loop.
This is mainly down to eliminating tests that were originally being run on each loop iteration.
Loop unrolling is another standard optimization that eliminates some more tests.
Let's partially unroll the loop and see what sort of a gain we get.
In practice, the optimal amount of loop unrolling corresponds to the way the application uses the convert( ) method, for example, the size of the typical array that is being converted.
But in any case, we use a particular example of 10 loop iterations to see the effect.
Optimal loop unrolling depends on a number of factors, including the underlying operating system and hardware.
Loop unrolling is ideally achieved by way of an optimizing compiler rather than by hand.
HotSpot interacts with manual loop unrolling in a highly variable way: sometimes HotSpot makes the unoptimized loop faster, sometimes the manually unrolled loop comes out faster.
Of all the VMs tested, only the HotSpot VM produces inconsistent results, with a speedup when processing the long-line files but a slowdown when processing the short-line files.
The last two lines of each table show the difference between the original loop and the manually unrolled loop.
It's good news that this kind of optimization is finally being applied efficiently by the VM.
But from a performance-tuning point of view, this means that it is difficult to know whether to unroll the loop manually or not.
Obviously, if you know exactly which VM your application runs on, you can establish whether the unrolling optimization produces faster code.
But if your application could be used under any VM, the decision is more complex.
The slower VMs benefit from manual unrolling, whereas the faster, server-mode VMs still remain.
This suggests that, at least for the time being, manual loop unrolling is worth considering.
It is worth repeating that the speedup we have obtained is mainly a result of eliminating tests that were originally run in each loop iteration.
For tight loops (i.e., loops that have a small amount of actual work that needs to be executed on each iteration), the overhead of tests is definitely significant.
It is also important during the tuning exercise to run the various improvements under different VMs and determine that the improvements are generally applicable.
My tests indicate that these improvements are generally valid for all runtime environments.
One development environment with a very slow VM—an order of magnitude slower than the Sun VM without JIT—showed only a small improvement.
However, it is not generally a good idea to base performance tests on development environments.
For a small Java program that does simple filtering or conversion of data from text files, this convert( ) method could take 40% of the total program time.
Improving this one method as shown can shave 20% from the time of the whole program, which is a good gain for a relatively small amount of work (it took me longer to write this section than to tune the convert( ) method)
This is a technique for squeezing out the very last driblet of performance from loops.
With this technique, instead of testing on each loop iteration to see whether the loop has reached its normal termination point, you use an exception generated at the end of the loop to halt the loop, thus avoiding the extra test on each run through the loop.
The results of my test runs (summarized in Table 7-1) were variable due to differences in memory allocation, disk paging, and garbage collection.
The VMs using HotSpot technology could show quite variable behavior.
The plain JDK 1.2 VM had a huge amount of trouble reclaiming memory for the later tests, even when I put in pauses and ran explicit garbage-collection calls more than once.
For each set of tests, I tried to increase the number of loop iterations until the timings were over one second.
For the memory-based tests, it was not always possible to achieve times of over a second: paging or out-of-memory errors were encountered.
In all test cases, I found that the number of iterations for each test was quite important.
On some Java systems, try-catch blocks may have enough extra cost associated with them to make this technique slower.
The actual improvement (if any) in performance depends on the test case that runs in the loop and the code that is run in the body of the loop.
The basic consideration is the ratio of the time taken in the loop test compared to the time taken in the body of the loop.
The simpler the loop-body execution is compared to the termination test, the more likely that this technique will give a useful effect.
This technique works because the termination test iterated many times can have a higher cost than producing and catching an Exception once.
Execute the array assignment tests only if there is no second //argument to allow for large SIZE values on the first test //that would give out of memory errors in the second test.
The Java bytecode specification allows a switch statement to be compiled into one of two different bytecodes.
Given a particular value passed to the switch block to be compared, the passed value is successively compared against the value associated with each case statement in order.
If, after testing all cases, no statements match, then the default label is matched.
When a case statement that matches is found, the body of that statement and all subsequent case bodies are executed (until one body exits the switch statement, or the last one is reached)
The operation of this switch statement is equivalent to holding an ordered collection of values that are compared to the passed value, one after the other in order, until a match is determined.
This means that the time taken for the switch to find the case that matches depends on how many case statements there are and where in the list the matched case is.
If no cases match and the default must be used, that always takes the longest matching time.
The other switch bytecode works for switch statements where the case values all lie (or can be made to lie) in a particular range.
Given a particular value passed to the switch block to be compared, the passed value is tested to see if it lies in the range.
If it does not, the default label is matched; otherwise, the offset of the case is calculated and the corresponding case is matched directly.
The body of that matched label and all subsequent case bodies are executed (until one body exits the switch statement, or the last one is reached)
For this latter switch bytecode, the time taken for the switch statement to match the case is constant.
The time is not dependent on the number of cases in the switch, and if no cases match, the time to carry out the matching and go to the default is still the same.
This switch statement operates as an ordered collection with the switch value first being checked to see if it is a valid index into the ordered collection, and then that value is used as the index to arrive immediately at the matched location.
Clearly, the second type of switch statement is faster than the first.
Sometimes compilers can add dummy cases to a switch statement, converting the first type of switch into the second (faster) kind.
A compiler is not obliged to use the second type of switch bytecode at all, but generally it does if it can easily be used.
You can determine which switch a particular statement has been compiled into using javap, the disassembler available with the JDK.
Using the -c option so that the code is disassembled, examine the method that contains the switch statement.
It contains either a "tableswitch" bytecode identifier or a "lookupswitch" bytecode identifier.
The tableswitch keyword is the identifier for the faster (second) type of switch.
If you identify a bottleneck that involves a switch statement, do not leave the decision to the compiler.
You are better off constructing switch statements that use contiguous ranges of case values, ideally by inserting dummy case statements to specify all the values in the range, or possibly by breaking up the switch into multiple switches that each use contiguous ranges.
You may need to apply both of these optimizations as in the next example.
The first method, switch1( ), contains some noncontiguous values for the cases, with each returning a particular integer value.
This method is not directly comparable to the first two methods; it is present as a control test.
The first set of tests, labeled "varying," passes in a different integer for each call to the switches.
This means that most of the time, the default label is matched.
Interestingly, my original test passed in only the integer 8, but HotSpot server mode optimized that to call the method only once and reuse the result, hence the need for the alternation.
The results are shown in Table 7-2 for various VMs.
There is a big difference in optimizations gained depending on whether the VM has a plain JIT or uses HotSpot technology.
From the variation in timings, it is not clear whether the HotSpot technology fails to compile the handcrafted switch in an optimal way or whether it does optimally compile all the switch statements but adds overhead that cancels some of the optimizations.
From the first line, it is clear that HotSpot does a great job of compiling the original unoptimized switch.
Comparing the times across the different VMs in the second line, for the optimized switch, we can see that client-mode HotSpot does really badly.
It appears that the way you optimize your switch statement is heavily dependent on which VM runs your application.
For the JIT results, the first and second lines of output show the speedup you can get by recrafting the switch statements.
The first and third lines of output show the worst-case comparison for the two types of switch statements.
In this test, switch1( ) almost always fails all its comparison tests.
The average case for switch3( ) in this test is only a pair of checks followed by a return statement.
Both checks fail in most of the calls for this "varying" case.
In this example, the switch merely returns an integer, so the conversion to an array access is feasible; in general, it may be difficult to convert a set of body statements into an array access and subsequent processing:
Recursive algorithms are used because they're often clearer and more elegant than the alternatives, and therefore have a lower maintenance cost than the equivalent iterative algorithm.
However, recursion often (but not always) has a cost; recursive algorithms are frequently slower.
So it is useful to understand the costs associated with recursion and how to improve the performance of recursive algorithms when necessary.
Recursive code can be optimized by a clever compiler (as is done with some C compilers), but only if presented in the right way (typically, it needs to be tail-recursive: see the sidebar Tail Recursion)
For example, Jon Bentley[3] found that a functionally identical recursive method was optimized by a C compiler if he did not use the ? : conditional operator (using if statements instead)
However, it was not optimized if he did use the ?: conditional operator.
He also found that recursion can be very expensive, taking up to 20 times longer for some operations that are naturally iterative.
Bentley's article also looks briefly at optimizing partialmatch searching in ternary search trees by transforming a tail recursion in the search into an iteration.
See Chapter 11 for an example of tuning a ternary search tree, including an example of converting a recursive algorithm to an iterative one.
A tail-recursive function is a recursive function for which each recursive call to itself is a reduction of the original call.
A reduction is the situation where a problem is converted into a new problem that is simpler, and the solution of that new problem is exactly the solution of the original problem, with no further computation necessary.
This is a subtle concept, best illustrated with a simple example.
I will take the factorial example used in the text.
The tail-recursive version of this function requires two functions: one to set up the recursive call (to keep compatibility) and the recursive call itself.
Generally, the advice for dealing with methods that are naturally recursive (because that is the natural way to code them for clarity) is to go ahead with the recursive solution.
You need to spend time counting the cost (if any) only when your profiling shows that this particular method call is a bottleneck in the application.
At that stage, it is worth pursuing alternative implementations or avoiding the method call completely with a different structure.
In case you need to tune a recursive algorithm or convert it into an iterative one, I provide some examples here.
I start with an extremely simple recursive algorithm for calculating factorial numbers, as this illustrates several tuning points:
Since this function is easily converted to a tail-recursive version, it is natural to test the tail-recursive version to see if it performs any better.
For this particular function, the tail-recursive version does not perform any better, which is not typical.
Here, the factorial function consists of a very simple fast calculation, and the extra function-call overhead in the tail-recursive version is enough of an overhead that it negates the benefit that is normally gained.
Let's look at other ways this function can be optimized.
Start with the classic conversion for recursive to iterative and note that the factorial method contains just one value that is successively operated on to give a new value (the result), along with a parameter specifying how to operate on the partial result (the current input to the factorial)
A standard way to convert this type of recursive method is to replace the parameters passed to the method with temporary variables in a loop.
In this case, you need two variables, one of which is passed into the method and can be reused.
The 1.4.0 server HotSpot VM optimized the recursive version sufficiently to make it faster than the iterative version.
Consider a linked list, with singly linked nodes consisting of a next pointer to the next node, and a value instance variable holding (in this case) just an integer.
A simple linear search method to find the first node holding a particular integer looks like:
Before looking at general techniques for converting other types of recursive methods to iterative ones, I will revisit the original factorial method to illustrate some other techniques for improving the performance of recursive methods.
To test the timing of the factorial method, I put it into a loop to recalculate factorial(20) many times.
Otherwise, the time taken is too short to be reliably measured.
When this situation is close to the actual problem, a good tuning technique is to cache the intermediate results.
This technique can be applied when some recursive function is repeatedly being called and some of the intermediate results are repeatedly being identified.
This technique is simple to illustrate for the factorial method:
In this particular situation, you can make one further improvement, which is to compile the values at implementation and hardcode them in:
My editor Mike Loukides points out that a variation on hardcoded values, used by state-of-the-art high-performance mathematical functions, is a partial table of values together with an interpolation method to calculate intermediate values.
The techniques for converting recursive method calls to iterative ones are suitable only for methods that take a single search path at every decision node when navigating through the solution space.
For more complex recursive methods that evaluate multiple paths from some nodes, you can convert a recursive method into an iterative method based on a stack.
I'll use here the problem of looking for all the files with names ending in some particular string.
The following method runs a recursive search of the filesystem, printing all nondirectory files that end in a particular string:
In the cases of these particular search methods, the time-measurement comparison shows that the iterative method actually takes 5% longer than the recursive method.
This is due to the iterative method having the overhead of the extra stack object to manipulate, whereas filesystems are generally not particularly deep (the ones I tested on were not), so the recursive algorithm is not particularly inefficient.
This illustrates that a recursive method is not always worse than an iterative one.
Note that the methods here were chosen for illustration, using an easily understood problem that could be managed iteratively and recursively.
Since the I/O is actually the limiting factor for these methods, there would not be much point in actually making the optimization shown.
For this example, I eliminated the I/O overhead, as it would have swamped the times and made it difficult to determine the difference between the two implementations.
To do this, I mapped the filesystem into memory using a simple replacement of the java.io.File class.
This stored a snapshot of the filesystem in a hash table.
Actually, only the full pathnames of directories as keys, and their associated string array list of files as values, need be stored.
This kind of trick—replacing classes with another implementation to eliminate extraneous overhead—is quite useful when you need to identify exactly where times are going.
Most of these suggestions apply only after a bottleneck has been identified:
Remove from the loop any execution code that does not need to be executed on each pass.
Move any code that is repeatedly executed with the same result, and assign that code to a temporary variable before the loop ("code motion")
Avoid method calls in loops when possible, even if this requires rewriting or inlining.
Multiple access or update to the same array element should be done on a temporary variable and assigned back to the array element when the loop is finished.
Avoid using a method call in the loop termination test.
Use int data types preferentially, especially for the loop variable.
Phrase multiple boolean tests in one expression so that they "short circuit" as soon as possible.
Try unrolling the loop to various degrees to see if this improves speed.
Rewrite any switch statements to use a contiguous range of case values.
Try caching recursively calculated values to reduce the depth of recursion.
Use temporary variables in place of passed parameters to convert a recursive method using a single search path into an iterative method.
Use temporary stacks in place of passed parameters to convert a recursive method using multiple search paths into an iterative method.
I/O to the disk or the network is hundreds to thousands of times slower than I/O to computer memory.
Disk and network transfers are expensive activities and are two of the most likely candidates for performance problems.
Two standard optimization techniques for reducing I/O overhead are buffering and caching.
For a given amount of data, I/O mechanisms work more efficiently if the data is transferred using a few large chunks of data, rather than many small chunks.
Buffering groups of data into larger chunks improves the efficiency of the I/O by reducing the number of I/O operations that need to be executed.
Where some objects or data are accessed repeatedly, caching those objects or data can replace an I/O call with a hugely faster memory access (or replace a slow network I/O call with faster local disk I/O)
For every I/O call that is avoided because an item is accessed from a cache, you save a large chunk of time equivalent to executing hundreds or thousands of simple operations.[1]
Caching usually requires intercepting a simple attempt to access an object and replacing that simple access with a more complex routine that accesses the object from the cache.
Caching is easier to implement if the application has been designed with caching in mind from the beginning, by grouping external data access.
If the application is not so designed, you may still be lucky, as there are normally only a few points of external access from an application that allow you to add caching easily.
There are some other general points about I/O at the system level that are worth knowing.
First, I/O buffers throughout the system typically use a read-ahead algorithm for optimization.
This normally means that the next few chunks are read from disk into a low-level buffer somewhere.
Consequently, reading sequentially forward through a file is usually faster than other orders, such as reading back to front through a file or random access of file elements.
The next point is that at the system level, most operating systems support mmap( ) , memcntl( ), and various shared-memory options.
Using these can improve I/O performance dramatically, but they also increase complexity.
Portability is also compromised, though not as much as you might think.
If you need to use these sorts of features and also maintain portability, you may want to start with the latest Perl distribution.
Perl has been ported to a large number of systems, and these features are mapped consistently to system-level features in all ports.
Since the Perl source is available, it is possible to extract the relevant system-independent mappings for portability purposes.
Java editions prior to the 1.4 release require you to use either polling across the handles, which is system-intensive; a separate thread per handle, which is also system-intensive; or a combination of these two, which in any case is bad for performance.
However, almost all operating systems support an efficient multiplexing function call, often called select( ) or sometimes poll( )
This function provides a way to ask the system in one request if any of the (set of) open handles are ready for reading or writing.
For versions prior to 1.4, you could again use Perl, which provides a standardized mapping for this function if you need hints on maintaining portability.
For efficient complex I/O performance, select( )/poll( ) functionality was probably the largest single missing piece of functionality in Java.
SDKs prior to 1.4 do provide nonblocking I/O by means of polling.
Polling means that every time you want to read or write, you first test whether there are bytes to read or space to write.
If you cannot read or write, you go into a loop, repeatedly testing until you can perform the desired read/write operation.
Polling of this sort is extremely system-intensive, especially because in order to obtain good performance, you must normally put I/O into the highest-priority thread.
Polling solutions are usually more system-intensive than multithreaded I/O and do not perform as well.
Multiplexed I/O, as obtained with the select( ) system call, provides far superior performance to both.
If you are building a server, you are well advised to add support for the select( ) system call.
Here are some other general techniques to improve I/O performance:
Decoupling the application processes from the I/O operations means that, ideally, your application does not spend time waiting for I/O.
In practice, it can be difficult to completely decouple the I/O, but usually some reads can be anticipated and some writes can be run asynchronously without the program requiring immediate confirmation of success.
Try to replace multiple smaller I/O calls with a few larger I/O calls.
Because I/O is a slow operation, executing in a loop means that the loop is normally bottlenecked on the I/O call.
When actions need to be performed while executing I/O, try to separate the I/O from those actions to minimize the number of I/O operations that need to be executed.
For example, if a file needs to be parsed, instead of reading a bit, parsing a bit, and repeating until finished, it can be quicker to read in the whole file and then parse the data in memory.
If you repeatedly access different locations within the same set of files, you can optimize performance by keeping the files open and navigating around them instead of repeatedly opening and closing the files.
This often requires using random-access classes (e.g., RandomAccessFile) rather than the easier sequential-access classes (e.g., FileReader)
Preallocate files to avoid the operating-system overhead that comes from allocating files.
This can be done by creating files of the expected size, filled with any character (0 is conventional)
The bytes can then be overwritten (e.g., with the RandomAccessFile class)
Using multiple files simultaneously can improve performance because of disk parallelism and CPU availability during disk reads and writes.
However, this technique needs to be balanced against the cost of extra opens and closes and the extra resources required by multiple open streams.
Sequentially opening and closing multiple files is usually bad for performance (e.g., when loading unpacked class files from the filesystem into the Java runtime)
Typically, an application generates output to System.out or System.err, if only for logging purposes during development.
It is important to realize that this output can affect performance.
Any output not present in the final deployed version of the application should be turned off during performance tests; otherwise, your performance results can get skewed.
This is also true for any other I/O: to disk, pipes, other processes, or the network.
It is best to include a framework for logging output in your design.
You want a framework that centralizes all your logging operations and lets you enable or disable certain logging features (perhaps by setting a "debug level")
You may want to implement your own logging class, which decides whether to send output at all and where to send it.
The Unix syslog utility provides a good starting point for designing such a framework.
It has levels of priority (emergency, alert, critical, error, warning, notice, info, debug) and other aspects that are useful to note.
This includes logging levels and output redirection, as I show briefly in the next section.
If you are already well into development without this kind of framework but need a quick fix for handling unnecessary output, it is still possible to replace System.out and System.err.
It is simple to replace the print stream in System.out and System.err.
It is useful to retain a reference to the original print-stream objects you are replacing, since these retain access to the console.
For example, the following class simply eliminates all output sent to System.out and System.err if TUNING is true; otherwise, it sends all output to the original destination.
This class illustrates how to implement your own redirection classes:
The penalty you pay depends to some extent on how logging is done.
One possibility is using a final static variable to enable logging, as in the following code:
I recommend deploying applications with a simple set of logging features in place.
But first ensure that the logging features do not slow down the application.
So far we have looked only at general points about I/O and logging.
Now we look at an example of tuning I/O performance.
The example consists of reading lines from a large file.
This section was inspired by an article from Sun Engineering,[4] though I go somewhat further along the tuning cycle.
The initial attempt at file I/O might be to use the FileInputStream to read through a file.
Note that DataInputStream has a readLine( ) method (now deprecated because it is byte-based rather than charbased, but ignore that for the moment), so you wrap the FileInputStream with the DataInputStream, and run.
The first test in absolute times is really dreadful because you are executing I/O one byte at a time.
This performance is the result of using a plain FileInputStream without buffering the I/O, because the process is completely I/O-bound.
For this reason, I expected the absolute times of the various VMs to be similar, since the CPU is not the bottleneck.
Possibly the underlying native call implementation may be different between VM versions, but I am not interested enough to spend time deciding why there should be differences for the unbuffered case.
Everyone knows you should buffer your I/O (except when memory is really at a premium, as in an embedded system)
You can buffer I/O directly from the FileInputStream class and other low-level classes by passing arrays to the read( ) and write( ) methods.
The lesson is clear, if you haven't already had it drummed home somewhere else: buffered I/O performs much better than unbuffered I/O.
Having established that buffered I/O is better than unbuffered, you renormalize your times on the buffered I/O case so that you can compare any improvements against the normal case.
The variations are large, but there is a mostly consistent pattern.
The 8K buffer doesn't seem to be significantly better than the default.
Let's get back to the fact that we are using a deprecated method, readLine( )
You should really be using Readers instead of InputStreams, according to the Javadoc, for full portability, etc.
Let's move to Readers and ascertain what this change costs us:
These results tell us that someone at Sun spent time optimizing Readers.
You can reasonably use Readers in most situations where you would have used an InputStream.
Some situations can show a performance decrease, but generally there is a performance increase.
Note that if you are running your own versions of these tests, you need to repeat some measurements within the VM, even in plain JIT VMs, to eliminate the JIT compiler overhead.
So far we have just been working from bad coding to good working practice.
The final version so far uses buffered Reader classes for I/O, as recommended by Sun.
Can we do better? Well of course, but now let's get down and dirty.
You know from general tuning practices that creating objects is overhead you should try to avoid.
Up until now, we have used the readLine( ) method, which returns a string.
Suppose you work on that string and then discard it, as is the typical situation.
You would do better to avoid the String creation altogether.
Also, if you want to process the String, then for performance purposes you are better off working directly on the underlying char array.
Working on char arrays is quicker since you can avoid the String method overhead (or, more likely, the need to copy the String into a char array buffer to work on it)
Basically, this means that you need to implement the readLine( ) functionality with your own buffer while passing the buffer to the method that does the string processing.
It reads in characters to fill the buffer, then runs through the buffer looking for ends of lines.
Each time the end of a line is found, the buffer, together with the start and end index of the line in that buffer, is passed to the doSomething( ) method for processing.
This implementation avoids both the String-creation overhead and the subsequent String-processing overhead, but these are not included in any timings here.
The only complication comes when you reach the end of the buffer and you need to fill it with the next chunk from the file, but you also need to retain the line fragment from the end of the last chunk.
It is unlikely your 8192-char chunk will end exactly on an end of line, so there are almost always some characters left to be carried over to the next chunk.
If you were running this code under a 1.3.1 server VM, you would need to track down what produced the anomalous times by changing a little bit of the code at a time, until you produced a workaround.
Note that the HotSpot timings are, once again, for the second run of the repeated tests.
No other VMs exhibited consistent variations between the first and second run tests.
Subsecond times are notoriously variable, although in my tests the results were fairly consistent.
This shows that HotSpot is quite variable with its optimizations.
We have, however, hardcoded in the ISO 8859_1 type of byte-to-char conversion rather than supporting the generic case (where the conversion type is specified as a property)
But this conversion represents a common class of character-encoding conversions, and you could fall back on the method used in the previous test where the conversion is specified differently (in the System property file.encoding)
Often, you will read from files you know and whose format you understand and can predict.
In those cases, building in the appropriate encoding is not a problem.
But we have seen that it is possible to speed up I/O even further if you're willing to spend the effort.
Avoiding the creation of intermediate Strings gives you a good gain.
This is true for both reading and writing and allows you to work on the char arrays directly.
Working directly on char arrays is usually better for performance, but also more work.
In specialized cases, you might want to consider taking control of every aspect of the I/O right down to the byte-to-char encoding, but for this you need to consider how to maintain compatibility with the JDK.
Timings of the long-line tests normalized to the JDK 1.2.2 buffered input stream test.
Timings of the short-line tests normalized to the JDK 1.2.2 buffered input stream test.
Objects are serialized in a number of situations in Java.
The two main reasons to serialize objects are to transfer objects and to store them.
There are several ways to improve the performance of serialization and deserialization.
First, fields that are transient do not get serialized, saving both space and time.
Overriding the default serialization routine in this way is generally only worth doing for large or frequently serialized objects.
The tight control this gives you may also be necessary to correctly handle canonicalized objects (to ensure objects remain canonical when deserializing them)
To transfer objects across networks, it is worth compressing the serialized objects.
For large amounts of data, the transfer overhead tends to swamp the costs of compressing and decompressing the data.
For storing to disk, it is worth serializing multiple objects to different files rather than to one large file.
The granularity of access to individual objects and subsets of objects is often improved as well.
It is also possible to serialize objects in a separate thread for storage and network transfers, letting the serialization execute in the background.
For objects whose state can change between serializations, consider using transaction logs or change logs (logs of the differences in the objects since they were last fully serialized) rather than reserializing the whole object.
You need to maintain the changes somewhere, of course, so it makes the objects more complicated, but this complexity can have a really good payback in terms of performance: consider how much faster an incremental backup is compared to a full backup.
It is worthwhile to spend some time on a basic serialization tuning exercise.
I chose a couple of fairly simple objects to serialize, but they are representative of the sorts of issues that crop up in serialization.
Test measurements are easily skewed by rewriting previously written objects.
Previously written objects are not converted and written out again; instead, only a reference to the original object is written.
Writing this reference can be faster than writing out the object again.
The speed is even more skewed on reading since only one object gets created.
All the other references refer to the same uniquely created object.
Early in my career, I was given the task of testing the throughput of an object database.
The first tests registered a fantastically high throughput until we realized we were storing just a few objects once, and all the other objects we thought we were storing were only references to those first few.
The Foo objects each contain two Bar objects in an array to make the overall objects slightly more representative of real-world objects.
As you can see, I provide for running tests either to disk or purely in memory.
This allows you to break down the cost into separate components.
The actual values revealed that 95% of the time is spent in the serialization.
Less than 5% is the actual write to disk (of course, the relative times are system-dependent, but these results are probably representative)
The results of this first test for JDK 1.2.2[9] are shown in the following chart:
Table 8-3 lists the full results of tests with a variety of VMs.
I have used the 1.2 results for discussion in this section, and the results are generally applicable to the other VMs tested.
I have normalized the baseline measurements to 100% for the byte array output (i.e., serializing the collection of Foos)
This is not what I expected, because I am used to the idea that "writing" takes longer than "reading." Thinking about exactly what is happening, you can see that for the serialization you take the data in some objects and write that data out to a stream of bytes, which basically accesses and converts objects into bytes.
But for the deserializing, you access elements of a byte array and convert these to other object and data types, including creating any required objects.
Added to the fact that the serializing procedures are much more costly than the actual (disk) writes and reads, it is now understandable that deserialization is likely to be the more intensive, and consequently slower, activity.
Considering exactly what the ObjectInputStream and ObjectOutputStream must do, I realize that they are accessing and updating internal elements of the objects they are serializing, without knowing beforehand anything about those objects.
This means there must be a heavy usage of the java.reflect package, together with some internal VM access procedures (since the serializing can reach private and protected fields and methods).[10] All this suggests that you should improve performance by taking explicit control of the serializing.
The actual code is difficult and time-consuming to work through.
It was written in parts as one huge iterated/recursed switch, probably for performance reasons.
Alert readers might have noticed that Foo and Bar have constructors that initialize the object and may be wondering if deserializing could be speeded up by changing the constructors to avoid the unnecessary overhead there.
In fact, the deserialization uses internal VM access to create the objects without going through the constructor, similar to cloning the objects.
Although the Serializable interface requires serializable objects to have no-arg constructors, deserialized objects do not actually use that (or any) constructor.
We have improved the reads but made the writes worse.
I expected an improvement for both, and I cannot explain why the writes are worse (other than perhaps that the ObjectOutputStream class may have suboptimal performance for this method-overriding feature; the 1.4 VM does show a speedup for both the writes and reads, suggesting that the class has been optimized in that version)
Instead of analyzing what the ObjectOutputStream class may be doing, let's try further optimizations.
Now this gives a clearer feel for the costs of dynamic object examination and manipulation.
Given the overhead the serializing I/O classes incur, it has now become obvious that the more serializing you handle explicitly, the better off you are.
This being the case, the next step is to ask the objects explicitly to serialize themselves rather than going through the ObjectInputStream and ObjectOutputStream to have them in turn ask the objects to serialize themselves.
The readObject( ) and writeObject( ) methods must be defined as private according to the Serializable interface documentation, so they cannot be called directly.
You must either wrap them in another public method or copy the implementation to another method so you can access them directly.
The Externalizable interface also provides support for serializing objects using ObjectInputStream and ObjectOutputStream.
But Externalizable defines two public methods rather than the two private methods required by Serializable.
You must also redefine Foo as implementing Externalizable instead of Serializable.
All of these are simple changes, but to be sure that nothing untoward has happened as a consequence, rerun the tests (as good tuners should for any changes, even minor ones)
This probably reflects the improvement you get from being able to compile and execute a line such as:
This example also shows that you are better off making your.
The drawback to controlling your own serializing is a significantly higher maintenance cost, as any change to the class structure also requires changes to the two Externalizable methods (or the two methods supported by Serializable)
In some cases (as in the example presented in this tuning exercise), changes to the structure of one class actually require changes to the Externalizable methods of another class.
The example presented here requires that if the structure of Bar is changed, the Externalizable methods in Foo must also be changed to reflect the new structure of Bar.
Here, you can avoid the dependency between the classes by having the Foo serialization methods call the Bar serialization methods directly.
But the general fragility of serialization, when individual class structures change, still remains.
If you test serializing to and from the disk, you find that the disk I/O now takes nearly one-third of the total test times.
Because disk I/O is now a significant portion of the total time, the CPU is now underworked, and you can even gain some speedup by serializing in several threads, i.e., you can evenly divide the collection into two or more subsets and have each subset serialized by a separate thread (I leave that as an exercise for you)
Normally you should move any Bar initialization out of the no-arg constructor to avoid redundant assignments.
Determine how many bytes I wrote size = out.written(  ) - size;
We have lost out on the writes because of the added complexity, but improved the reads considerably.
The cost of the Foo.convert( ) method has not been factored in, but the strategy illustrated here is for cases where you need to run only that convert method on a small subset of the deserialized objects, and so the extra overhead should be small.
This technique works well when transferring large groups of objects across a network.
For the case in which you need only a few objects out of many serialized objects that have been stored on disk, another strategy is even more efficient.
This strategy uses techniques similar to the example just shown.
A second file (the index file) holds the offset of the starting byte of each serialized object in the first file.
A moment's thought should convince you that this provides the byte offset into the data file.
You enter the index file and skip to the correct index for.
The serialized int at that location holds the byte offset into the data file, so you deserialize that int.
Then you enter the data file, skipping to the specified offset, and deserialize the object there.
This is also the first step in building your own database: the next steps are normally to waste time and effort before realizing that you can more easily buy a database that does most of what you want.
This "index file-plus-data file" strategy works best if you leave the two files open and skip around the files, rather than repeatedly opening and closing the files every time you want to deserialize an object.
The strategy illustrated in this paragraph does not work as well for transferring serialized objects across a network.
You could transfer index files across the network, then use those index files to precisely identify the objects required and limit transfers to only those identified objects.
Timings (in write/read pairs) of the serialization tests with various VMs.
Foo as last test, but write/read called directly in test (test4c)
Clustering is a technique that takes advantage of locality (usually on the disk) to improve performance.
It is useful when you have objects stored on disk and can arrange where objects are in relation to each other.
For example, suppose you store serialized objects on disk, but need to have fast access to some of these objects.
The most basic example of clustering is arranging the serialization of the objects in such a way as to selectively deserialize them to get exactly the subset of objects you need, in as few disk accesses, file openings, and object deserializations as possible.
Perhaps they cannot all fit into memory at the same time, or they are persistent, or there are other reasons for serialization.
In this scenario, rather than serializing the whole table, you may be better off serializing the 10% of frequently used objects into one file (which can be deserialized in one long call) and the other 90% into one or more other files with an object table index allowing individual objects to be read in as needed.
Alternatively, it may be that objects are grouped in some way in your application so that whenever one of the table objects is referenced, this also automatically requires certain other related objects.
In this case, you want to cluster these groups of objects so they are deserialized together.
If you need to manage objects on disk for persistency, sharing, memory, or whatever reason, you should consider using an object-storage system (such as an object database)
The serialization provided with Java is very basic and provides little in the way of simple systemwide customization.
For example, if you have a collection of objects on disk, typically you want to read into memory the collection down to one or two levels (i.e., only the collection elements, not any objects held in the instance variables of the collection elements)
With serialization, you get the transitive closure[12] of the collection in general, which is almost certainly much more than you want.
Serialization supports reading to certain levels in only a very rudimentary way: basically, it says you have to do the reading yourself, but it gives you the hooks that let you customize on a per-class basis.
The ability to tune to this level of granularity is really what you need for any sort of disk-based object storage beyond the most basic.
And you usually do get those extra tuning capabilities in various objectstorage systems.
The transitive closure is the set of all objects reachable from any one object, i.e., an object and its data variables and their data variables, etc.
This means that if you cluster data (of whatever type) on the disk so that the data that needs to be together is physically close together on disk, then the reading of that data into memory is also speeded up.
Typically, the most control you have over clustering objects is by putting data into the same file near each other and hoping that the filesystem is not too fragmented.
Clustering should reduce the number of disk I/O operations you need to execute.
Consequently, measuring the number of disk I/O operations that are executed is essential to determine if you have clustered usefully.[13] The simplest technique to measure I/O is to monitor the number of reads, writes, opens, and closes that are performed.
This gets complicated by using different I/O classes wrapped one around the other.
But you can always find the lowest-level class that is actually doing the I/O (usually one of FileInputStream, FileOutputStream, and RandomAccessFile in the java.io package)
You can determine all actual methods that execute I/O fairly easily if you have the JDK source: simply find all source files with the word "native." If you look in java.io for these and look at the actual method names of the.
Now the difficult part is wrapping these calls so that you can monitor them.
Native methods that are declared private are straightforward to handle: just redefine the java.io class to count the times they are called internally.
Native methods that are protected or have no access modifier are similarly handled: just ensure you do the same redefinition for subclasses and package members.
But the methods defined with the public modifier need to be tracked for any classes that call these native methods, which can be difficult and tiresome, but not impossible.
Ultimately, it is the number of low-level I/O operations that matter.
But if you reduce the high-level I/O operations, the lowlevel ones are generally reduced by the same proportion.
The simplest alternative would be to use the debug interface to count the number of hits on the method.
Unfortunately, you cannot set a breakpoint on a native method, so this is not possible.
The result is that it takes some effort to identify every I/O call in an application.
If you have consistently used your own I/O classes, the java.io buffered classes, and the java.io Reader and Writer classes, it may be enough to wrap the I/O calls to FileOutputStream and FileInputStream from these classes.
If you have done nonstandard things, you need to put in more effort.
Unfortunately, this is optional functionality in the JDK (Java specifies that the traceMethodCalls( ) method must exist in Runtime, but it does not have to do anything), so you are lucky if you use a system that supports it.
The only one I am aware of is the Symantec development environment, and in that case, you have to be in the IDE and running in debug mode.
Running the Symantec VM outside the IDE does not seem to enable this feature.
Some profilers may also help to produce a trace of all I/O operations.
I would recommend that all basic I/O calls have logging statements next to them, capable of reporting the amount of I/O performed (both the number of I/O operations and the number of bytes transferred)
I/O is typically so costly that one null call or if statement (when logging is not turned on) is not at all significant for each I/O performed.
On the other hand, it is incredibly useful to be able to determine at any time whether I/O is causing a performance problem.
Typically, I/O performance depends on the configuration of the system and on resources outside the application.
So if an unusual configuration causes I/O to be dramatically more expensive, this can be easily missed in testing and difficult to determine (especially remotely) unless you have an I/O-monitoring capability built into your application.
A colleague of mine once installed a compression utility on his desktop machine that compressed the entire disk.
The utility worked as a type of disk driver: accesses to the disk went through the utility, and every read and write was decompressed or compressed transparently to the rest of the system, and to the user.
My colleague was expecting the system to run slower, but needed the extra disk space and was willing to put up with a slower system.
What he actually found was that his system ran faster! It turned out that the major bottleneck to his system was disk throughput, and by making most files smaller (averaging half the previous size), everything was moving between memory and disk much quicker.
This illustrates how the overhead of compression can be outweighed by the benefits of reducing I/O.
The system described obviously had a disk that was relatively too slow in comparison to the CPU processing power.
Disk throughput has not improved nearly as fast as CPUs have increased in speed, and this divergent trend is set to continue for some time.
Although networks do tend to have a huge jump in throughput with each generation, this jump tends to be offset by the much larger volumes of data being transferred.
Furthermore, network-mounted disks are also increasingly common, and the double performance hit from accessing a disk over a network is surely a prime candidate for increasing speed using compression.
On the other hand, if a system has a fully loaded CPU, adding compression can make things worse.
When the environment is unknown, the situation is more complex.
One suggestion is to write I/O wrapper classes that handle compressed and uncompressed I/O automatically on the fly.
Your application can then test whether any particular I/O destination has better performance using compression, and then automatically use compression when called for.
One final thing to note about compressed data is that it is not always necessary to decompress the data in order to work with it.
Have you any eggs? No, we haven't any eggs" is compressed into "LO.
One such sketch involved a restaurant scene where all the characters spoke only in letters and numbers, joining the letters up in such a way that they sounded like words.
The mapping for some of the words to letters was as follows: have = F, you = U, any = NE, eggs = X, hello = LO, no = 9, yes = S, we = V, have = F, haven't = FN, ham = M, and = N.
Now, if I want to search the text to see if it includes the phrase "any eggs," I do not actually need to decompress the compressed text.
Instead, I compress the search string "any eggs" using 2-Ronnies compression into "NE X", and I can now use that compressed search string to search directly on the compressed text.
When applied to objects or data, this technique requires some effort.
You need to ensure that any small data chunk compresses in the same way both on its own and as part of a larger volume of data containing that data chunk.
If this is not the case, you may need to break objects and searchable data into fields that are individually compressed.
There are several advantages to this technique of searching directly against compressed data:
There is no need to decompress a large amount of data.
Searches are actually quicker because the search is against a smaller volume of data.
More data can be held in memory simultaneously (since it is compressed), which can be especially important for searching through large volumes of disk-stored data.
It is rarely possible to search for compressed substrings directly in compressed data because of the way most compression algorithms use tables covering the whole dataset.
However, this scheme has been used to selectively query for data locations.
For this usage, unique data keys are compressed separately from the rest of the data.
This produces a compressed index table that can be searched without decompressing the keys.
This scheme allows compressed keys to be searched directly to identify the location of the corresponding data.
The NIO classes hold I/O functionality that is available from most modern operating systems but that was missing from Java.
Much of the NIO functionality is needed for highly scalable efficient server technology, but some aspects are useful for many applications, as we'll see in the next few sections.
For a good introduction to NIO, consider Learning Java (O'Reilly) as well as a basic introduction in Michael Nygard's excellent article in JavaWorld.[15]
When you create a client socket to connect to a server, the underlying TCP connection procedure involves a two-phase acknowledgment.
The Socket.connect( ) call invokes a blocking procedure, which normally stops the thread from proceeding until the connection is complete.
The NIO package allows you to initiate the socket connection procedure and carry on processing in the thread while the connection is being established.
To achieve this, you use a SocketChannel set to nonblocking mode.
Applying compression would actually slow down the read and write, though the overall transfer might be speeded up if the connection was slow with significant fragmentation.
I'll assume we are on a LAN, in which case compression would only slow down the transfer.
They yield optimal I/O efficiency by allowing system I/O operations to operate directly between system memory and an external medium (e.g., the disk or network)
In contrast, nondirect Buffers require an extra copy operation to move the data from the Java heap to and from the external medium.
The NIO I/O operations are optimized for dealing with direct Buffers.
However, note that the "old" I/O (java.io) classes are also optimized, but for operating on Java arrays.
The InputStream and OutputStream classes that operate directly on external media (for example, FileInputStream) also require no extra copy operations to move data to and from Java arrays.
So we can see that nondirect Buffers are at a disadvantage when compared to both the other options, but it is not obvious which other combination of data structure and I/O operation is the most efficient.
I'll use a simple file-copying operation to test the various options.
I've chosen file copying because NIO includes an extra operation for enabling file copies in the FileChannel class, which gives us one further optimization option.
First, we have the good old java.io technique of reading chunks from the file into a byte[ ] array buffer and writing those chunks out.
For most operating systems, mapping a file into memory is more expensive than reading or writing a few tens of kilobytes of data via the usual read and write methods.
Finally, as I said earlier, FileChannel also provides transferTo( ) and transferFrom( ) methods.
Once again, these methods are intended for maximal efficiency in transferring bytes between FileChannels and other Channels, by using the underlying operating system's filesystem cache.
Clearly, though, there is no huge advantage to NIO in this situation compared with using the "old" I/O.
But bear in mind that the NIO Buffers are not specifically designed to replace the old I/O classes.
So, for example, we haven't tested Buffer classes with scatter-gather operations, which work on multiple Buffers simultaneously.
When is this useful? A common example is an HTTP server.
When an HTTP server downloads a page (file) to a browser, it writes out a header and then the page.
The header itself consists of several different sections that need to be amalgamated.
It is efficient to write the parts of the headers to the stream in multiple separate I/O operations, followed by the page body, allowing the network to buffer the response.
Unfortunately, this turns out to be suboptimal because you are increasing the I/O operations and allowing the network stack to set the pace.
Acme's THTTPD developers ran a series of performance tests of various HTTP servers, and identified that the amount of data sent in the first network packet was crucial to optimal performance:
Turns out the change that made the difference was sending the response headers and the first load of data as a single packet, instead of as two separate packets.
Possibly the most important features of NIO from a performance standpoint are the nonblocking channels and the ability to multiplex channels.
Multiplexing I/O allows you to handle multiple I/O channels from one thread without having the thread block on any one channel.
Without NIO, you have no certain way to know that a read or write will block the thread.
Currently, NIO does not support the multiplexing of FileChannels (though most operating systems do), so multiplexing with JDK 1.4 is primarily for socket communications.
It is useful to understand nonblocking mode in a little more detail.
But at the operating-system level, I/O write operations do return a value, normally the number of bytes that were written by the write call.
If there are too many bytes being written by the call, then the buffer is filled with those bytes it can take, the remaining bytes are not written, and the number of bytes written is returned.
The buffer is then emptied by sending on the data, and it is ready for the next chunk of bytes.
The buffer emptying is done at I/O speeds, which is typically several orders of magnitude slower than the write to the in-memory buffer.
Instead, the buffer is filled, emptied, and so on, until all the bytes have been written.
But because the write can block for so long, you need to give it a separate thread of its own until it completes.
You are probably familiar with doing this for reads, but it may not have occurred to you that Java writes were in the same category.
For example, in all cases, where the data fits into the network buffer, a write to the socket should return immediately.
The actual network I/O proceeds asynchronously, leaving the Java thread to do other operations.
In fact, typically the thread has time for thousands more operations before the buffer is ready to accept more data (i.e., the next write can succeed without blocking)
But for effective multiplexing, you also need to know which Channels are ready to be written to or read from.
And it is the Selector class that will reliably tell you when any channel is ready to perform its next I/O operation.
The Selector class determines from the operating system which subset, from a set of Channels, is ready to perform I/O.
The Selector class differentiates between different types of I/O: there are currently four types.
The first two types are where Selector can inform you when any Channel is ready to be read or written to.
In addition, client sockets can be trying to connect to a server socket: the Selector can tell you when the connection attempt has completed.
And lastly, server sockets can accept new connections: the Selector can tell you when there is a connection pending that will allow the server socket to execute an accept( ) call without blocking.
Note that multiplexed asynchronous I/O does not necessarily make I/O any faster.
What you get is the ability to handle many I/O channels in one thread.
For most Java applications, which have only a few open I/O streams at one time, there is no need to multiplex because a few extra blocked threads are easily managed by the VM.
If you have many threads blocked on I/O, then multiplexing your I/O can significantly reduce your resource requirements.
Ten I/O threads are probably okay; a hundred is too many.
Multiplexed I/O is a definite requirement for scalable high-performance server applications, but most other applications do not need it.
Links to detailed examples, including full code for a high-performance NIO-based HTTP server, can be found at http://www.JavaPerformanceTuning.com/tips/nio.shtml.
Most of these suggestions apply only after a bottleneck has been identified:
Ensure that performance tests are run with the same amount of I/O as the expected finished application.
Specifically, turn off any extra logging, tracing, and debugging I/O.
Redefine the I/O classes to count I/O calls if necessary.
Include logging statements next to all basic I/O calls in the application.
Buffer to reduce the number of I/O operations by increasing the amount of data transfer each I/O operation executes.
Cache to replace repeated I/O operations with much faster memory or local disk access.
Replace System.out and System.err with customized PrintStream classes to control console output.
Use logger objects for tight control in specifying logging destinations.
Keep files open and navigate around them rather than repeatedly opening and closing the files.
Use change logs for small changes, rather than reserializing the whole object.
Consider partitioning objects into multiple sets and serializing each set concurrently in different threads.
Use lazy initialization to move or spread the deserialization overhead to other times.
Consider indexing an object table for selective access to stored serialized objects.
Optimize network transfers by transferring only the data and objects needed, and no more.
Cluster serialized objects that are used together by putting them into the same file.
Put objects next to each other if they are required together.
Consider using an object-storage system (such as an object database) if your object-storage requirements are at all sophisticated.
Use compression when the overhead of compression is outweighed by the benefit of reducing I/O.
Avoid compression when the system has a heavily loaded CPU.
Consider using "intelligent" I/O classes that can decide to use compression on the fly.
Nondirect Buffers provide an efficient mechanism for converting arrays of one primitive data type to another primitive data type.
Direct Buffers provide options for optimizing I/O, especially when using multiple Buffers with scatter-gather I/O operations.
High-performance scalable servers should use NIO multiplexing and asynchronous I/O.
These sorts are usually adequate for all but the most specialized applications.
To optimize a sort, you can normally get enough improvement by reimplementing a standard sort (such as quicksort) as a method in the class being sorted.
Comparisons of elements can then be made directly, without calling generic comparison methods.
Only the most specialized applications usually need to search for specialized sorting algorithms.
And it is often the case that your array is already in an Object array, hence the more generic (but slower) support in the JDK.
The first quicksort with the Object[ ] signature gives a baseline at 100%
I am sorting a randomized array of Sortable objects, using the same randomized order for each test.
Switching to a quicksort that specifies an array of Comparable objects (which means you avoid casting every object for each comparison) is faster for every VM I tested (see Table 9-1)
Timings of the various sorting tests normalized to the initial JDK 1.2 test.
For comparison, I have included in Table 9-1 the timings for using the Arrays.sort( ) method, applied to the same randomized list of Sortable objects used in the example.
The Arrays.sort( ) method uses a merge sort that performs better on a partially sorted list.
Merge sort was chosen for Arrays.sort( ) because, although quicksort provides better performance on average, the merge sort provides sort stability.
A stable sort does not alter the order of elements that are equal based on the comparison method used.[1]
The standard quicksort algorithm also has very bad worst-case performance.
For more specialized and optimized sorts, there are books (including Java-specific ones) covering various sort algorithms, and a variety of sort implementations are available on the Web.
The computer literature is full of articles providing improved sorting algorithms for specific types of data, and you may need to run a search to find specialized sorts for your particular application.
A good place to start is the classic reference The Art of Computer Programming by Donald Knuth (Addison-Wesley)
In the case of nonarray elements such as linked-list structures, a recursive merge sort is the best sorting algorithm and can be faster than a quicksort on arrays with the same dataset.
If you need your sort algorithm to run faster, optimizing the comparisons in the sort method is a good place to start.
These are best used when the comparison method requires a calculation for each object being compared, and that calculation can be cached.
Partially presorting the array with a faster partial sort, followed by the full sort.
Only when the performance is still short of your target do you need to start looking for alternatives.
The sorting methods provided by the JDK are perfectly adequate for most situations.
When they fall short, the techniques illustrated in the previous section often speed things up as much as is required.
However, if you work on a project where varied and flexible sorting capabilities are needed, sorting is one area of performance tuning where it is sensible to create a framework early in the development cycle.
Providing support for arbitrary sorting algorithms is straightforward: just use sorting interfaces.
There needs to be a sorting interface for each type of object that can be sorted.
Arrays and collection objects should be supported by any sorting framework, along with any other objects that are specific to your application.
Here are two interfaces that define sorting objects for arrays and collections:
For generic support, you need the Comparator interface to have an additional method that checks whether it supports optimized comparison wrappers (which I will now call ComparisonKeys)
Unfortunately, you cannot add a method to the Comparator interface, so you have to use the following subinterface:
An easy way to change the sorting algorithm being used at any specific point of the application.
An easy way to change the pair-wise comparison method, by changing the Comparator object.
Comparison keys are optimal to use in sorts where the comparison method requires a calculation for each object being compared, and that calculation could be cached.
An optimized integer key comparison class, which doesn't require method calls when used for sorting.
This outline should provide a good start to building an efficient sorting framework.
Many further generic optimizations are possible, such as supporting a LongComparisonKey class and other special classes appropriate to your application.
The point is that the framework should handle optimizations automatically.
The most the application builder should do is decide on the appropriate Comparator or ComparisonKey class to build for the object to be sorted.
The last version of our framework supports the fastest sorting implementation from the previous section (the last implementation with no casts and direct access to the ordering field)
It's worth looking at ways to reduce the cost of object creations for comparison keys.
By adding another interface, you can support the needed mapping:
HotSpot is variable in how well it manages to optimize the framework sort.
The 1.4.0 client is almost as fast as the direct field access sort.
This indicates that HotSpot technology is theoretically capable of similarly optimizing the framework sort.
Computer-science analysis of sorting algorithms show that, on average, no generic sorting algorithm can scale faster than O(nlogn) (see the sidebar Orders of Magnitude)
You often have additional information that can help you to improve the speed of a particular sort.
When discussing the time taken for particular algorithms to execute, it is important to know not just how long the algorithm takes for a particular dataset, but also how long it takes for differentsized datasets, i.e., how it scales.
One common way to indicate the behavior of algorithms across different scales of datasets is to describe the algorithm's scaling characteristics by the dominant numerical function relating to the scaling behavior.
The notation used is "O(function)," where function is replaced by the dominant numerical scaling function.
It is common to use the letter "n" to indicate the number of data items being considered in the function.
For example, O(n) indicates that the algorithm under consideration increases in time linearly with the size of the dataset.
O(n2) indicates that the time taken increases according to the square of the size of the dataset.
These orders of magnitude do not indicate absolute times taken by the algorithm.
Instead, they indicate how much longer the algorithm takes when the dataset increases in size.
Note that the scaled times are approximate; order-of-magnitude statistics include only the dominant scaling function, and there may be other, smaller terms that adjust the actual time taken.
The order of magnitude does not indicate the relative speeds of two different algorithms for any specific dataset size.
Instead, order-of-magnitude statistics indicate how expensive one particular algorithm may be as your dataset grows.
To take a concrete example, hash tables have an O(1) order of magnitude for accessing elements.
This means that the time taken to access elements in a hash table is mostly independent of the size of the hash table.
Accessing elements in an array by linearly searching through that array takes O(n)
In absolute times, it might be quicker to execute the linear array search on a small array than to access from a hash table.
But as the number of elements becomes larger, at some point the hash table will always become quicker.
Of course, typically you can't map your elements so neatly.
But if you can map items to integer keys that are more or less evenly distributed, you can still take advantage of improved sorting characteristics.
Bear in mind that an array of partially sorted items can be sorted faster than a typical unsorted array.
When you can guess the approximate final position of the items in the collection to be sorted, you can use this knowledge to improve sorting speed.
Items can be given an ordering value that can be mapped to integer keys.
The distribution of the keys is regular, or any one of the following is true:
The distribution of the keys is fairly even, so that when mapped into array indexes, ordering is approximately kept.
The distribution of the keys has a mapping into one of these other distributions.
A regular distribution allows them to be mapped straightforwardly into array indexes.
But if you have an uneven distribution and can specify a mapping that allows you to flatten out the keys in some way, it may still be possible to apply this methodology.
For example, if you know that your keys will have a normal (bell-curve) distribution, you can apply an inverse bell-curve function to the keys to flatten them out to array indexes.
For this technique to work, the mapped keys do not need to be unique.
Several keys or groups of keys can map to the same value or values.
Indeed, it is quite difficult to make the index mapping unique.
You need to be aware of this and handle the resulting collisions.
Normally, you can map clusters of keys into subsections of the sorted array.
This way, the problem has been modified to sort multiple smaller subsections (which is faster than sorting the whole array), and hence the array is sorted more quickly.
Note that Object.hashCode( ) provides a mechanism for generating an integer for any object.
However, the resulting hash code is not guaranteed to be evenly distributed or even unique, nor is it at all guaranteed to be consistent across different VMs or even over multiple runs of one VM.
Consequently, the hash code is of little use for any kind of mapping.
Karl-Dietrich Neubert[4] gives a detailed implementation of this approach, where the algorithm provides O(n) sorting behavior and also minimizes the extra memory needed to manage the sort.
I also implemented Neubert's sort for a plain int array rather than for an array of objects.
The results were the same as for the object array when the JIT was turned off.
But with any type of JIT turned on, the two simpler reference-sort algorithms were optimized much better by the native code compiler and were faster for all sizes of arrays I tested (up to several million elements)
Their absolute sort times were sufficiently fast that their bad scaling behavior didn't matter.
This curious difference in relative speeds applied only to sorting int[ ] arrays, not arrays of objects.
For arrays of objects, Neubert's sort seems to be faster both with and without a JIT.
I include here a Java implementation of Neubert's algorithm with comments in the code.
Timings of the various sorting tests normalized to the Flashsort JDK 1.2 test.
Note that the sort at the end of the Neubert algorithm is an insertion sort running over the entire array.
Insertion sorts provide better performance than quicksorts for partially ordered arrays.
This final insertion sort ensures that keys incorrectly classified by the group distribution end up in the right location:
Count the number of elements in each group int[  ] groups = new int[num_groups];
We need to make 'arr.length' moves at most, //but if we have one move left in the outer loop //then the remaining element is already in the right place, //so we need test for only 'arr.length-1' moves.
Most of these suggestions apply only after a bottleneck has been identified:
Reimplement a standard sort (such as quicksort) directly in the class being sorted.
Use comparison keys to replace objects where the comparison method requires a calculation for each object being compared, and that calculation could be cached.
Partially presort the array with a faster partial sort; then re-sort using the full comparison method.
Comparison key objects that cache calculations that would otherwise need to be repeatedly executed.
Comparison key objects that hold the ordering value in a directly accessible public field.
Improved object creation by mapping arrays of comparison key objects into multiple arrays.
Use specialized sorting algorithms for faster times and better scaling behavior.
Use specialized sorting algorithms when the sorting order of objects can be mapped directly to integer keys.
Minor Premise: One man can dig a posthole in sixty seconds.
Conclusion: Sixty men can dig a posthole in one second.
Multithreading allows an application to do multiple things at the same time.
While it is often possible to get the same effect with clever programming in a single thread, Java's extensive support of threads makes it easier to use multiple threads.
In addition, single-threaded applications cannot take advantage of multiprocessor machines.
Multithreading improves performance in many cases, but it also has drawbacks if the default mechanisms for cooperation between threads are used simplistically.
In this chapter, we look at the benefits and the disadvantages threads offer to performance.
We examine the likely problems that may be encountered and discuss how to minimize the performance downside while still gaining the benefits of multiple threads.
Synchronization can be confusing, so I felt it was worth including a short reminder of its subtleties here.
Two or more threads accessing and updating the same data variables have no way of knowing when a particular accessor update will occur relative to any other thread accesses.
Synchronization ensures that a group of statements (a synchronized block) will execute atomically as far as all synchronized threads are concerned.
Synchronization does not address the problem of which thread executes the statements first: it is first come, first served.
Every object can have a monitor associated with it, so any object can synchronize blocks.
Before a synchronized block can be entered, a thread needs to gain ownership of the monitor for that block.
Once the thread has gained ownership of the monitor, no other thread synchronized on the same monitor can gain entry to that block (or any other block or method synchronized on the same monitor)
The thread owning the monitor gets to execute all the statements in the block, and then automatically releases ownership of the monitor on exiting the block.
At that point, another thread waiting to enter the block can acquire ownership of the monitor.
Note, however, that threads synchronized on different monitors can gain entry to the same block at any time.
For example, a block defined with a synchronized(this) expression is synchronized on the monitor of the this object.
If this is an object that is different for two different threads, both threads can gain ownership of their own monitor for that block, and both can execute the block at the same time.
This won't matter if the block affects only variables specific to its thread (such as instance variables of this), but can lead to corrupt states if the block alters variables that are shared between the threads, such as static variables.
When tuning threads, it is easy to make a little change here, and a little change there, and end up with total confusion, race conditions, and deadlock.
Before you start tuning threads, it is important to have a good understanding of how they interact and how to.
If you are not comfortable with Java synchronization and how it works, I strongly advise you to spend some time studying how to use threads and synchronization.
Be sure you understand how race conditions and deadlocks occur (many articles and books on Java go into this in detail, and there are brief examples in the later sections of this chapter)
Be sure you know how to correctly use the various wait( ) and notify( ) methods in the Object class as well as the synchronized keyword, and understand which monitor objects are used and how they are used when execution reaches a synchronized block or method.
The user's impression of the performance of an application is greatly affected by its responsiveness.
Putting the user interface into a separate thread from any other work makes the application feel far more responsive to the user and ensures that an unexpectedly long operation doesn't freeze the application's screen.
This user-interface thread is quite important in applets, where it is simple to use the screen-update thread to execute other tasks because you can easily call code from the paint( ) method.
Although more effort is required to spawn a thread to execute other tasks, it is much better to do so, as otherwise you can easily block repainting the screen or other GUI responses.
In Figure 10-1, the clock on the left has been resized to a quarter of its original size, but the paint( ) method has been unable to resize the clock drawing, as the paint( ) method is busy keeping the correct time.
The clock on the right has been resized to a wide rectangular shape, and it keeps perfect time while also responding to the resize request because its paint( ) method always completes quickly.
If you are able to separate operations that slow processing (such as I/O) into specialized threads, your application will run more smoothly.
It can carry on its main work while another thread anticipates the need for data, saves data to disk, etc.
However, you should not pass work to another thread while your main thread just sits and waits until that other thread completes.
In fact, doing this is likely to hurt performance rather than improve it.
You should not use extra threads unless you have good design or performance reasons for doing so.
One useful technique is to use a separate thread to monitor the rest of the application and, when necessary, interrupt threads that are running beyond their expected execution time.
This is more often a technique that ensures robustness, but it can apply to performance, too, when a calculation provides successively better approximations to the required result.
It may be reasonable to interrupt the calculation after a certain length of time, assuming you have a good approximation calculated.
This technique does not specifically require a supervising thread, as the timeout checking could be done within the calculation.
It is often used in animation; the frame-display rate can be adjusted according to the time taken to display the frames, which in turn depends on picture resolution and the system environment.
All in all, using multiple threads needs careful consideration, and should be planned for in the design stage.
Retrofitting an application to use threads at an intermediate or advanced stage can sometimes be done quite simply in some sections of the application, but is not usually possible throughout the application.
In any case, care should be taken when changing the design to use more threads so that the problems illustrated in the next sections are avoided.
A race condition occurs when two threads attempt to use the same resource at the same time.
If each thread can complete the increment( ) method in its entirety without the other thread executing, then all is fine, and the counter monotonically increases.
Otherwise, the thread context switcher has the opportunity to interrupt one thread in the middle of executing the increment( ) method and let the other thread run through this method.
Note that the thread can actually be interrupted anywhere, not necessarily in the middle of the increment( ) method, but I've greatly increased the likelihood of an interruption in the increment( ) method by including a print statement there: package tuning.threads;
On a multiprocessor machine, the situation is even more confused.
Synchronizing the increment( ) method ensures the correct behavior of a monotonically increasing counter, as this gives exactly the desired behavior: the method is forced to complete before another call to it from any thread can be started.
In this test, because the counter is static, the increment( ) method needs to be static for synchronization to work correctly.
If the increment( ) method is not static, synchronizing it locks the monitor for each this object rather than for the class.
In the example I used a different object in each thread.
A non-static increment( ) method is synchronized separately on each this object, so the updates remain unsynchronized across the two threads.
It is not simply that the num variable is static (though it needs to be for this particular example to work)
The critical point is that the monitor that locks the method must be the same monitor for the two threads; otherwise, each thread gains its own separate lock with no synchronization occurring.
Generally, deciding what to synchronize can be quite subtle, and you need to keep in mind which monitor is going to be locked by any particular thread.
Ensuring that resources are used correctly between threads is easy in Java.
Usually, it just takes the use of the synchronized keyword before a method.
Because Java makes it seem so easy and painless to coordinate thread access to resources, the synchronized keyword tends to get used liberally.
Up to and including Java 1.1, this was the approach taken even by Sun.
From JDK 1.2, the engineers at Sun became more aware of performance and are now careful to avoid synchronizing willy-nilly.
Instead, many classes are built unsynchronized but are provided with synchronized wrappers (see the later section Section 10.4.1)
Synchronizing methods liberally may seem like good safe programming, but it is a sure recipe for reducing performance at best, and creating deadlocks at worst.
The following Deadlock class illustrates the simplest form of a race condition leading to deadlock.
The run( ) method just has a short half-second delay and then calls hello( ) on another Deadlock object.
The problem comes from the combination of the following three factors:
The sequence of execution does not guarantee that monitors are locked and unlocked in correct order.
The main( ) method accepts one optional parameter to set the delay in milliseconds between starting the two threads.
With a parameter of 1000 (one second), there should be no deadlock.
Table 10-1 summarizes what happens when the program runs without deadlock.
With a parameter of 0 (no delay between starting threads), there should be deadlock on all but the most heavily loaded systems.
A heavily loaded system can delay the startup of d2Thread enough that the behavior executes in the same way as the first sequence.
This illustrates an important issue when dealing with threads: different system loads can expose problems in the application and also generate different performance profiles.
The situation is typically the reverse of this example, with a race condition not showing deadlocks on lightly loaded systems, while a heavily loaded system alters the application behavior sufficiently to change thread interaction and cause deadlock.
First, there is the operational cost of managing the monitors.
This overhead can be significant: acquiring and testing for locks on the monitor for every synchronized method and block can impose a lot of overhead.
Attempting to acquire a lock must itself be a synchronized activity within the VM; otherwise, two threads can simultaneously execute the lock-acquisition code.
This overhead can be reduced by clever techniques in the VM, but never completely eliminated.
The next section addresses this overhead and looks at ways to avoid it whenever possible.
Attempts to lock on different objects in two threads must still be synchronized to ensure that the object identity check and granting of the lock are handled atomically.
This means that even attempting to get a lock on any object by two or more threads at the same time can still cause a performance degradation, as the VM grants only one thread at a time access to the lock-acquisition routine.
In some VMs, synchronizing static methods takes significantly longer than synchronizing nonstatic methods, suggesting that code is global in these VMs for the static synchronizations.
This is not strictly speaking a bug, but certainly not optimal for performance.
The second cost of synchronization is in what it actually does.
Synchronization serializes execution of a set of statements so that only one thread at a time executes that set.
Whenever multiple threads simultaneously try to execute the same synchronized block, those threads are effectively run together as one single thread.
This completely negates the purpose of having multiple threads and is potentially a huge bottleneck in any program.
On machines with multiple CPUs, you can leave all but one CPU idle when serialized execution occurs.
The later section "Avoiding Serialized Execution" addresses techniques for avoiding serialized execution where possible.
In fact, for short methods, using a synchronized method can mean that the basic time involved in calling the method is significantly larger than the time for actually running it.
The overhead of calling an unsynchronized method can be much smaller than that of calling a synchronized method.
You should be aware of when you do not need to synchronize.
Stateless objects (including no-static state) almost never need synchronization on their methods.
There are certain unusual implementations when methods may be altering state directly in another shared object, where synchronization would be required.
Some objects with state may have no need for synchronization because access to the object is highly restricted, and the synchronization is handled by other objects.
Some objects can implement a copy-on-write mechanism (StringBuffer uses this; see Chapter 5)
You can define copyon-write in such a way to allow multithreaded updates of that object.
Many multithreaded applications actually use most of their objects in a single-threaded manner.
Each individual thread maintains its own references to most objects, with just a few data or utility objects actually being used by multiple threads.
From a performance standpoint, it seems a shame to have the overhead of synchronized objects on many classes where synchronization is not needed or used.
On the other hand, when you design and build a particular class, it is seldom possible to anticipate that it will never be shared among several threads, so to be on the safe side, typically the class is built with synchronization.
When you have identified a bottleneck that uses synchronized objects, you can sometimes remove.
This is especially easy to achieve when you use objects that have an unsynchronized implementation held in a synchronized wrapper.
The idea behind synchronized wrappers is that you build your class completely unsynchronized, as if it is to be used single-threaded.
But you also provide a wrapper class with exactly the same interface.
The difference in the wrapper class is that all methods that require synchronization are defined with the synchronized modifier.
The wrapper could be a subclass with methods reimplemented, but more typically, it is a separate class that holds an internal reference to an instance of the unsynchronized class and wraps all the methods to call that internal object.
Using synchronized wrappers allows you the benefits of thread-safe objects by default, while still retaining the capability to selectively use unsynchronized versions of those classes in bottlenecks.
From Java 2, the framework of using synchronized wrappers has become standard.
All the new collection classes in java.util are now defined unsynchronized, with synchronized wrappers available.
Old collection classes (e.g., Hashtable, Vector) that are already synchronized remain so for backward compatibility.
The wrappers are usually generic, so you can actually create wrapped synchronized objects from any object of the right type.
Using unsynchronized classes gives a performance advantage, but it is a maintenance drawback.
There is every likelihood that initial implementation of any application will use the unsynchronized classes by default, leading to many subtle threading bugs that can be a debugging and maintenance nightmare.
Typical development scenarios then try to identify which objects need to be synchronized for the application, and then wrap those objects in their synchronized wrappers.
Under the stress of project milestones, I know of one project where the developers went through all their code with a recursive routine, chopping out every synchronized keyword in method declarations.
This seemed quicker than carefully tuning the code, and did in fact give a performance improvement.
They put a few synchronized keywords back in after the regression tests.
This type of tuning is exactly the opposite of what I recommend.
Instead, you should use synchronized wrapped objects throughout the application by default, but ensure that you have the capability to easily replace these with the unsynchronized underlying objects.
Remember, tuning is better done after the application works correctly, not at the beginning.
When you come to tune the application, identify the bottlenecks.
Then, when you find that a particular class needs to be speeded up, determine whether that class can be used unsynchronized.
If so, replace it with its unsynchronized underlying object, and document this thoroughly.
Any changes in the application must reexamine these particular tuning changes to ensure that these objects do not subsequently need to become synchronized.[1]
When the design indicates that a class or a set of methods should definitely be synchronized or definitely does not need synchronization, then of course you should apply that design decision.
For example, stateless objects can often be specified with no synchronization.
However, there are many classes and methods where this decision is uncertain, and this is where my recommendation applies.
Be aware, though, that there is no win-win situation here.
If you tend toward unsynchronized classes by default, you leave your application open to corruption.
If you prefer my recommended "synchronized by default" approach, your application has an increased chance of encountering deadlocks.
On the basis that deadlocks are both more obvious and easier to fix than corrupt objects, I prefer the "synchronized by default" option.
Implementing with interfaces and synchronized wrappers gives you an easy way to selectively back out of synchronization problems.
The next test gives you an idea of the relative performance of synchronized and unsynchronized methods, and of synchronized wrappers.
The test compares synchronized (Vector ), unsynchronized (ArrayList), and synchronized wrapper (ArrayList wrapped) classes.
Timings of the various array-manipulation tests, normalized to the JDK 1.2 Vector test.
Note that the Vector.set( ) method implementation is slightly more efficient (faster) than the ArrayList.set( ) implementation, so if there were no effect from the synchronization, the Vector test could be slightly faster than the ArrayList test.
There are some reports that the latest VMs have negligible overhead for synchronized methods; however, my own tests show that synchronized methods continue to incur some overhead (VMs up to and including JDK 1.4)
The 1.4 server-mode test is the only VM that shows negligible overhead from synchronized methods.
This comes from server mode's aggressive inlining together with being able to analyze the requirement for acquiring the lock.
In this case, the test is fairly simple, and it looks like the 1.4 server mode is able to establish that the lock does not need acquiring on each pass of the loop and to correctly optimize the situation.
In more complex real-world situations, server mode is not always able to optimize away the lock acquisition so well.
One way of completely avoiding the requirement to synchronize methods is to use separate objects and storage structures for different threads.
Care must be taken to avoid calling synchronized methods from your own methods, or you will lose all your carefully built benefits.
For example, Hashtable access and update methods are synchronized, so using one in your storage structure can eliminate any desired benefit.
Prior to JDK 1.2, there is no unsynchronized hash table in the JDK, and you have to build or buy your own unsynchronized version.
From JDK 1.2, unsynchronized collection classes are available, including Map classes.
As an example of implementing this framework, I look at a simple set of global counters, keyed on a numeric identifier.
Basically, the concept is a global counter to which any thread can add a number.
This concept is extended slightly to allow for multiple counters, each counter having a different key.
String keys are more useful, but for simplicity I use integer keys in this example.
To use String keys, an unsynchronized Map replaces the arrays.
The simple, straightforward version of the class looks like this:
On other test runs, the final value is different, but it is almost never the correct value (40 million)
If I use a faster CPU or a lower total count, the threads can get serialized by the operating system (by finishing quickly enough), leading to consistently correct results for the total count.
But those correct results are an artifact of the environment and are not guaranteed.
To get the correct behavior, you need to synchronize the update methods in the class.
ThreadLocal variables might be appropriate here, but not in JDK 1.2, which used an underlying implementation of a synchronized map to allocate pre-thread objects.
That implementation would defeat our intention to avoid synchronization completely.
We use the num instance variable of the CounterTest //object to determine which array we are going to increment.
The serialized execution avoidance class is a significant improvement on the synchronized case.
This variation is generated from the nature of multithreaded context switching, together with the fact that the activity taking much of the time in this test is lock management.
Switching is essentially unpredictable, and the amount of switching and where it occurs affects how often the VM has to release and reacquire locks in different threads.
Consider what happens on a multiprocessor machine where the threads can run on different CPUs (i.e., where the Java runtime and operating system support preemptive thread scheduling on separate CPUs)
Counter3 (the serialized execution avoidance class) is parallelized automatically and scales very nicely.
This same test with Counter3 running on a fourCPU machine tends towards one-quarter of the single-CPU time, assuming that the four CPUs have the same power as the single CPU we tested earlier.
On the other hand, the synchronized version of the counter, Counter2, always has serialized execution (that's what synchronized does)
Consequently, it does not scale and generally performs no better than in the single-CPU test (except for the advantage of running the OS on another CPU)
I measured timings of the three Counter classes in the previous section using another class, CounterTest.
This timing class illustrates some pitfalls you need to avoid when timing multithreaded applications, so I'll go into a little detail about the CounterTest definition.
Just create a Thread subclass with the run( ) method running timed tests of the classes you are measuring.
You need an extra instance variable for the Counter3 class, so the class can be defined as: package tuning.threads;
The timings are for each thread running its own threaded update to the Counter class.
But we should be measuring the time from the first update in any thread to the last update in any thread.
One way to avoid the first pitfall is to synchronize the tests so that they are not started until all the threads are ready.
Then all threads can be started at the same time.
The second pitfall can be avoided by setting a global time at the start of the first update, then printing the time difference when the last thread finishes.
Variables shared between multiple threads (e.g., instance variables of objects) have atomic assignment guaranteed by the Java language specification for all data types except longs and doubles.
Actually, the storing of a value into a variable takes two primitive operations, a store and a write.
However, the language specification also states that once a store operation occurs on a particular variable, no other store operation is allowed on that variable until the write operation has occurred.
The (original[5]) specification allows longs and doubles to be stored in two separate sets of store+write operations, hence their exception to atomicity.
I'm very grateful to Brian Goetz for clarifying several points in this section.
This means that access and update of individual variables does not need to be synchronized simply to avoid corruption (as long as they are not longs or doubles)
If a method consists solely of a single variable access or assignment, there is no need to make it synchronized for thread-safety, and every reason not to do so for performance.
Note that I'm talking about using synchronization for thread-safety here, not synchronization for visibility of updates and accesses of variables.
First, read the later Synchronization Ordering sidebar to ensure that you understand that synchronization provides no guarantees about the ordering of execution among different threads.
Bearing that in mind, atomic access and update once again do not provide any guarantees about ordering of variables.
Furthermore, unlike synchronization, atomic access and update do not provide any guarantee about timely synchronization between values of a variable held in different threads.
When a synchronized block is passed, all the variables in a thread have been updated to the values in the "master" memory: they are synchronized.
However, that synchronization does not occur with a simple atomic access or update.
This means that for a variable that is atomically assigned to outside of a synchronized block, theoretically a thread could see a different value from the "master" memory for that variable for an extended period of time.
Some other Java authors have suggested that this is dangerous.
And while this book is full of dangerous advice, I don't believe this particular performance-tuning technique is any more dangerous than many other techniques.
Specifically, if two or more variables don't need to be consistent with each other or vary consistently among themselves over time, you can trade some delay in propagation between threads for some extra performance.
If you have two variables that have to be consistent with each other at all times across threads, such as the X and Y values of a shared point or a variable that has to vary consistently over time between multiple threads, then you need to synchronize.
And in the case of variables needing to remain consistent with each other, atomic assignment is still fine for the individual variables, but the combined assignment needs to be synchronized (see the examples discussed shortly)
The lack of guarantee of any ordering between threads means that this can occur even with synchronized variables because the read can occur prior to the update.
But note that the synchronization does take place as soon as any synchronized block is passed in thread1
For the vast majority of programs, there is no issue with using atomic access and update.
The only place I have found where atomic access and update might easily be used but could cause a problem is where a Runnable thread has a stopping condition dependent on a variable being set from another thread, and the thread never enters a synchronized block.
The volatile keyword specifies that the variable declared volatile must always have its value updated to the "main" memory value.
So threads are not allowed to have a value for that variable that is different from the main memory value at any time.
As a consequence, volatile variables can be slower than non-volatile variables because of the extra requirement to always synchronize the variable, but faster than synchronized access and updates because volatile synchronizes only one variable whereas synchronization synchronizes all variables.
The thread-safety of atomic accesses and updates extends further to any set of statements that are accessing or assigning to a variable independently of any other variable values.
The exclusion here precludes setting a variable that depends on the value of another variable being thread-safe; this would be two separate operations, which is inherently not thread-safe.
Combining several calls to methods that atomically assign variables is the same problem as combining several calls to synchronized methods.
The individual calls are executed atomically, but the combination is not necessarily atomic:
Specifically, if two threads call the setBoth( ) method simultaneously, the outcome is not predictable unless setBoth( ) is synchronized.
Even the simple example of setInt(getInt( )+1) is not thread-safe; without synchronizing the whole statement you could lose the increment.
Synchronization ensures that a set of statements executes exclusively for a particular monitor.
Synchronization does not guarantee the relative order of execution of synchronized blocks between threads.
If two threads try to execute a synchronized block simultaneously, one succeeds first, but there is no guarantee about which one that is.
Atomic assignment is similar to the case where the set of synchronized statements is one statement, and the synchronization is set by the VM.
When considering atomic assignment, you might ask the question, "What if a context switch occurs during the method call setup or tear down? When does the synchronization happen, and what happens with the context switch?" The actual moment when the synchronization occurs does not matter.
It does not matter if a context switch happens at any time before or after a set of synchronized statements.
Either the synchronized set has not been entered, or it has been completed.
Only the actual granting of the lock matters, and that is atomic with respect to all interested threads.
Until you reach an atomic assignment statement, it makes no difference whether another atomic assignment on the same variable occurs.
This is purely the ordering of assignments, which is not guaranteed with synchronization anyway.
A context switch hitting the method tear down does not matter.
The usual reason to synchronize a simple updator is to avoid a corrupt assignment (two threads simultaneously updating the same variable, and the resulting value being neither of the updated values)
This can indeed occur for doubles and longs, but not for other data types.
For serious number crunching involving doubles and longs, I recommend using separate data structures for each thread or using a VM that guarantees atomic assignment for doubles and longs.
The VM is optimized for creating threads, so you can usually create a new thread when you need to without having to worry about performance.
But in some circumstances, maintaining a pool of threads can improve performance.
For example, in a case where you would otherwise create and destroy many short-lived threads, you are far better off holding onto a (variable-sized) pool of threads.
Here, the tasks are assigned to an already created thread, and when a thread completes its task, it is returned to the pool, ready for the next task.
This improves performance because thread creation (and, to some extent, destruction) does have a significant overhead that is better avoided for short-lived threads.
A second situation is where you want to limit the number of threads in your application.
In this case, your application needs to make all thread requests through a centralized pool manager.
Although a pool manager does not prevent other threads from being started, it is a big step toward that goal.
Strictly speaking, limiting threads does not require a pool of threads, just a centralized pool manager, but the two usually come together.
Every system has a response curve with diminishing returns after a certain number of threads are running on it.
This response curve is different for different systems, and you need to identify values for your particular setup.
A heavy-duty server needs to show good behavior across a spectrum of loads, and at the high end, you don't want your server crashing when 10,000 requests try to spawn 10,000 threads; instead, you want the server response to degrade (e.g., by queuing requests) and maintain whatever maximum number of threads is optimal for the server system.
When deciding which thread to run next, there may be a slight gain by choosing the available thread that ran most recently.
This thread is most likely to have its working set still fully in memory: the longer it has been since a thread was last used, the more likely it is that the thread has been paged or swapped out.
Also, any caches (at any level of the system and application) that may apply are more likely to contain elements from the most recently used thread.
By choosing the most recently used thread, paging and cache overhead may be minimized.
Load balancing is a technique for improving performance when many activities are processed concurrently.
These activities could be in separate processes on different machines, in separate processes on the same machine, or in separate threads within the same process.
To support load balancing, a standard design is to have:
One point of entry for all requests (the request queue)
A mechanism for the queue to decide which processor to hand a particular request to.
You also need communication lines between the queue and processors and a way to internally identify requests, but this is an obvious part of the infrastructure.
The decision mechanism is typically a simple loadbalancing system that distributes requests to those available processors.
The request processors specify when they are available or busy.
When the queue has a request to process, it chooses the first available request processor.
Some applications need more complex decision-making, and use a decision mechanism that allocates requests depending on the type of request.
Our main concern with this architecture is that the queue is a potential bottleneck, so it must pass on requests quickly and be ready fairly continually.[7] The pool of request processors behind the queue can be running in one or more threads or processes, usually one request processor per thread.
The pool of threaded request processors can be prestarted or started on demand, or you can have a combination of these.
Typically for this kind of setup, there are configuration options that specify how many prestarted request processors there should be, the maximum number of request processors to have running simultaneously, and how long to wait before terminating a request processor since it last processed a request.
For this reason, an advanced load-balancing design does not rely on a single queue.
Instead, any queues in the application are distributed, redundantly copied, and monitored so that any queue failure results in only a small performance degradation at worst.
Some designs use persistent queue elements so that a critical failure does not lose queued elements.
Note that there is always a point of diminishing returns on response time versus the number of threads in a system.
If you have too many threads running concurrently, the system's overall response time gets worse.
The operating-system thread scheduler (or Java-system thread scheduler, if you're not using OS threads) spends more and more time managing threads, and this overhead takes up the CPU time rather than allowing the threads to run.
You also need to consider whether the queue object handles the responses (collecting them from the request processes and handing them back to the clients) or whether the request-processor objects can hand the responses back directly.
The former design has the advantage that the client cannot get any direct access to the request-processor objects, but the disadvantage that you are introducing an unnecessary bottleneck in processing terms.
The latter option (handing responses back directly), of course, has the opposite characteristics: no extra bottleneck, but access to client objects is enabled.
If you use sockets to handle incoming requests within one process, the operating system provides some loadbalancing support.
If you want, the operating system will provide the queue for free.
A connectionless TCP server (such as a web server) performs the following process:
Whenever a client connects to the server socket, the operating-system TCP stack hands the connection off to only one thread that is blocked on the accept( ) call.
The thread that returns from the accept( ) call gets the client connection (Socket object), reads the request, processes it, and writes the request back (directly to the client)
The thread goes back into the accept( ) call, waiting for the next connection.6
At any time, you can start further threads to scale up the server as long as each thread has access to the previously created ServerSocket object.
It is therefore not possible to have multiple separate processes (i.e., independent operating-system processes, rather than threads within one operating-system process) serving on the same server socket.
Strictly speaking, it is possible to fork a process into multiple system processes after the socket has been opened.
Multiprocess TCP servers have some small disadvantages over multithreaded TCP servers, mainly when they need to communicate between themselves or use other expensive resources.
However, multiprocess TCP servers do have one big advantage over multithreaded servers, which is that if one server process crashes, the others can continue running independently, unaffected by the crash.
With UDP sockets, the architecture can be slightly different, as you can open a UDP server socket on a port that already has a server socket bound to it.
Instead, all the threads (from potentially multiple system processes) sit on a read( ) call on the UDP socket, and the UDP stack hands off each incoming packet to just one of the threads that are waiting on the read( )
The server then has to use the information from the packet (either at the application level or the protocol level) to determine the return address to send the result of the processed request (again, directly back to the client)
If you need to implement your own queuing system, you have to consider whether the queue controls the request processors, or whether the processors access the queue.
The latter model is how the socket model works: each request processor sits on the queue and waits for it to pass a request.
Data for the private (internal) queues //the RequestProcessor RequestProcessor requestProcessor; //Retain a handle on my thread so that we can easily access //it if we need control Thread myThread; //Keep a handle on the 'public' queue - the one that //actually holds the objects ActiveRequestQueue queueServer; //Availability boolean isAvailable = true;
It may help to look at a concrete implementation of load balancing.
I'll consider the task of downloading many pages from a web server as quickly as possible.
It is impolite to batch-download at high speeds from a single web server.
Automated programs that download multiple pages from web servers have a voluntary protocol they should adhere to.
One point of the protocol is to avoid overloading web servers by downloading many pages at a high access rate.
Automated download programs that are polite specifically stagger downloads over a long period in order to minimize the load on the web server.
Open a URL, read the data, and dump it into a local file:
For my tests, I downloaded a large number of pages.
I validated the tests over the Internet, but not surprisingly, Internet access times are extremely variable.
For detailed repeatable tests, I used a small local HTTP server that allowed me to control all the parameters to the tests very precisely.
This range is different depending on the full environment, and probably needs to be experimentally determined.
But you can make a decent first guess by considering the bottlenecks of the system.
In this case, the bottlenecks are CPU, system memory, disk throughput, network-connection latency, server-download rates, and network throughput.
In my tests, system memory and CPU limit the number of threads and download speed for the massively parallel case, but you are using a delay specifically to reduce the load on those resources.
System memory constrains the number of threads you can use, but again, the delay avoids overloading this resource (provided that the delay is not too short)
Disk throughput can be significant, but network and server throughput are far more likely to limit data-transfer rates.
So we are left with networktransfer rates and network-connection latency to consider.
Now you can make a good guess as to a starting point for the delay.
You can evaluate the average number of bytes to transfer for each download, and work out the amount of time this takes based on the available network throughput.
You can also estimate the average time taken to make a connection (by measuring some real connections)
A straightforward guess is to set the delay at a value below the higher of these two averages.
In my tests, the files being downloaded are not large, and the average connection time is the larger time.
I started with a delay of about half the average connection time and ran tests increasing and decreasing.
Figure 10-3 shows the results of varying the delay times.
An optimum choice for the delay in this particular test environment is approximately 70% of the average connection time.
The flat line in the middle of the graph shows the relative time taken for the massively parallel test.
The results show that for this environment there are several advantages to running with a delay.
A decisive advantage is that you never run out of system resources.
There are never so many threads running simultaneously that you run out of memory and completely lose URLs, as occurred with the massively parallel test.
In fact, the system doesn't even need to run to full capacity for most of the test.
Another significant advantage is that by tuning the delay, you can run the test faster.
What about our nice load-balancing architecture classes? Let's test these to see how they compare to the last simple optimization you made.
You need to add a few support classes so that your load-balancing architecture is running the same download test.
Basically, there are three classes to define: Request, RequestProcessor, and RequestResult.
Request needs to hold only a URL and a local file for storing the downloaded page.
RequestResult does not need any extra behavior for the test.[9]
RequestResult does need extra state and behavior in order to make timing measurements, and RequestProcessor is similarly a bit more complicated for timing purposes.
This time is also the same as that achieved with the previous most optimal test.
Both tests optimized the downloads enough that they have reached the same network-throughput bottleneck.
This bottleneck cannot be optimized any further by either test.
Time taken for the load-balanced download versus number of threads.
The load-balancing architecture is more complex than adding a delay between threads, but it is much more flexible and far more controlled.
If you want to vary your download in a number of ways, such as prioritizing URLs or repeating certain failed ones, it is much easier to do so with the load-balancing architecture.
By looking at the CPU utilization graphs for the load-balancing architecture compared to the other tests in Figure 10-5, you can easily see how much more controlled it is and how it uses resources in a far more consistent manner.
There are many techniques that reduce the time taken to solve intensive problems by using multiple threads to farm out parts of the problem.
Start multiple threads running to solve the whole of a particular problem, each starting from a different point in the solution space.
This technique has, for instance, been used to speed up a graph-coloring problem.
The specific strategy followed[10] was to run several problem solvers at the same time, one at a higher priority than the others (the main thread)
Normally the main thread would win, but on occasion, one of the background threads was lucky due to its starting point and finished quickly.
By stopping the main thread if it looked to be far behind in the solution, the problem was solved in one-tenth of the time, on average, compared to the time taken when the main thread was not terminated.
This improvement occurred in spite of the fact that this was a single-processor machine.
The improvement comes about because the problem can be solved much quicker from some starting points than from others.
There would be similar improvements if you also used several different problem-solving strategies in the various threads, where some of the strategies are sometimes quicker than others.
This article reports on the technique used by Bernardo Huberman of Xerox Parc.
In the same article, a variation of this strategy was applied to network connections for bypassing congestion.
By opening multiple connections to download the same large data source on a highly congested network (the Internet), some connections were less likely than others to be slowed significantly or broken.
Of course, if everyone on the network used this technique, downloads would be slower for everyone.
Break up the search space into logically parallelized search spaces.
This does not work too well if the problem is entirely CPU-bound, but if there is any significant I/O or if multiple processors are available, this technique works nicely.
An example would be searching a disk-based database for items.
If the database is partitioned into multiple segments, then having one thread searching per segment makes the search faster (both on average and in the worst case)
The classic blackboard architecture approach, in which multiple different solvers work on the parts of a problem in which they have expertise, independently of other solver threads.
The threads use a "blackboard" (a sort of globally accessible hash table with published keys) to communicate.
This allows a thread to pick up any (intermediate or full) results other threads may publish that help that particular thread with its own problem-solving routines.
Many of these suggestions apply only after a bottleneck has been identified:
Run slow operations in their own threads to avoid slowing down the main thread.
Keep the interface in a separate thread from other work so that the application feels more responsive.
Avoid designs and implementations that force points of serialized execution.
Use multiple resolution strategies racing in different threads to get quicker answers.
Build classes with synchronized wrappers, and use synchronized versions except when unsynchronized versions are definitely sufficient.
Avoid synchronized blocks by using thread-specific data structures, combining data only when necessary.
Load-balance the application by distributing tasks among multiple threads, using a queue and threadbalancing mechanism for distributing tasks among task-processing threads.
Use thread pools to reuse threads if many threads are needed or if threads are needed for very short tasks.
Use a thread pool manager to limit the number of concurrent threads used.
What in essence it consists of is a horizontal rectilinear plane surface maintained by four vertical columnar supports, which we call legs.
The tables in this laboratory, ma'am, are as advanced in design as one will find anywhere in the world.
In this chapter, we look at the performance problems that can stem from using an inappropriate or nonoptimal data structure.
Instead, my focus is on how to performance-tune structures and associated algorithms.
Those structures I do cover are provided as examples to give you an idea of how the tuning procedure looks.
For performance-tuning purposes, be aware of alternative structures and algorithms, and always consider the possibility of switching to one of these alternatives rather than tuning the structure and algorithm that is already being used.
Being aware of alternative data structures requires extensive reading of computer literature.[1] One place to start is with the JDK code.
Look at the structures that are provided and make sure that you know all about the available classes.
There are already several good books on data structures and algorithms in Java, as well as many packages available from the Web with extensive documentation and often source code too.
Some popular computer magazines include articles about structures and algorithms (see Chapter 19).[2]
An interesting analysis of performance-tuning a "traveling salesman" problem is made by Jon Bentley in his article "Analysis of Algorithms," Dr.
When tuning, you often need to switch one implementation of a class with a more optimal implementation.
Switching data structures is easier because you are in an object-oriented environment, so you can usually replace one or a few classes with different implementations while keeping all the interfaces and signatures the same.
When tuning algorithms, one factor that should pop to the front of your mind concerns the scaling characteristics of the algorithms you use.
For example, bubblesort is an O(n2) algorithm while quicksort is O(nlogn)
This tells you nothing about absolute times for using either of these algorithms for sorting elements, but it does clearly tell you that quicksort has better scaling characteristics, and so is likely to be a better candidate as your collections increase in size.
Similarly, hash tables have an O(1) searching algorithm where an array requires O(n) searching.
Collections are the data structures that are most easily altered for performance-tuning purposes.
Using the correct or most appropriate collection class can improve performance with little change to code.
For example, if a large ordered collection has elements frequently deleted or inserted throughout it, it usually can provide better performance if based on a linked list rather than an array.
On the other hand, a static (unchanging) collection that needs to be accessed by index performs better with an underlying implementation that is an array.
If the data is large and insertions are allowed (for example, a text buffer), then a common halfway measure is to use a linked list of arrays.
This structure copies data within a single array when data is inserted or deleted.
When an array gets filled, the collection inserts a new empty array immediately after the full array and moves some data from the full to the empty array so that both old and new arrays have space.
A converse structure provides optimized indexed access to a linked-list structure by holding an array of a subset of the link nodes (e.g., every 20th node)
This structure allows for quick navigation to the indexed nodes, and then slower nodal access to nodes in between.[3] The result is a linked-list implementation that is much faster at index access, though it occupies more space.
It is sometimes useful to provide two collections holding the same data so that the data can be accessed using the most appropriate (and fastest) procedure.
This is common for indexed data (database-type indexes as opposed to array indexes), but entails extra overhead at the build stage.
In a similar way, it may be that a particular data set is best held in two (or more) different collections over its lifetime, but with only one collection being used at any one time.
For example, you may use a linked-list implementation of a vector type collection during building because your collection requires many insertions while it is being built.
After the build is completed, the collection can be converted into one based on an array, thus speeding up access.
It can be difficult to identify optimal algorithms for particular data structures.
This is detrimental to performance, and it would be significantly faster to sort a linked list directly using a merge sort.[4] In any case, frequently converting between collections and arrays is likely to cause performance problems.
The drawback to using these directly is the lack of object-oriented methodology you can apply.
However, I occasionally find that there are situations when I want to pass these raw arrays directly between several classes rather than wrap the arrays in a class with the behavior required.
This is unfortunate in design terms, but does provide speed.
Here, there are several layers of protocols you need to pass your message through before it is transmitted, for example, a compression layer and an encryption layer.
If you use an object as a message being passed through these layers, each layer has to request the message contents (copying it), change the contents, and then assign back the new contents (copying again)
Assuming that you use an array to hold the contents, you can allow the message-contents array itself to be passed directly to the other compression and encryption layer objects.
If you want to iterate over the characters in a String, you must either repeatedly call String.charAt( ) or copy the characters into your own array using.
Depending on the size of the String and how many times you iterate through the characters, one or the other of these methods is quicker, but if you could iterate directly on the underlying char array, you would avoid the repeated method calls and the copy (see Chapter 5)
A final point is that the collections that come with Java and other packages are usually not type-specific.
If you reimplement the Vector class yourself using an underlying String[ ] array, and then change signature parameters and return types of methods from Object to String, the reimplemented class is faster.
It is also clearer to use: you get rid of all those casts from your code.
It is straightforward to test the performance costs of generalized collections compared to specialized collections.
Access that does not involve a cast takes place at essentially the same speed.
Even updating a typed object array with objects of the given type (e.g., Strings into an underlying String[ ] array of an array list) seems to be faster by about 10%
The only reason I can think of for this is that the JIT compiler manages to optimize the update to the specialized array.
Note that Generics are due to be introduced in SDK 1.5
Generics allow instances of generic classes like Vector to be specified as aggregate objects that hold only specified types of objects.
However, the implementation of Generics is to insert casts at all the access points and to analyze the updates to ensure that the update type matches the cast type.
There is no specialized class generation, so there is no performance benefit, and there may even be a slight performance degradation from the additional casts.
The Java 2 Collections framework provides a set of collection classes.
Each class has its own performance strengths and weaknesses, which I cover here.
Collection classes wrapped in synchronized wrappers are always slower than unwrapped, unsynchronized classes.
Nevertheless, my recommendation is generally to use objects within synchronized wrappers.
You can selectively "unwrap" objects when they have been identified as part of a bottleneck and when the synchronization is not necessary.
Synchronized wrappers are also discussed in that chapter in Section 10.4.1
Table 11-1 summarizes the performance attributes of the collection classes.
Based on a LinkedHashMap, so provides iteration of elements according to insertion order.
TreeSet No Slower than HashSet; provides iteration of keys in order.
Special-purpose HashMap based on identity (= =) instead of equality (.equals)
Faster than HashMap for high-performance mapping where the identity semantics are acceptable.
A hash map implementation that also maintains an ordered linked list of entries.
LinkedList No Slower than other Lists, but may be faster for some types ofqueues.
Implementations of Set are slower to update than most other collection objects and should be avoided unless you need Set functionality.
Of the three available Set implementations, HashSet is definitely faster than TreeSet, with LinkedHashSet, available from SDK 1.4, somewhere in between.
Objects are added to the set as the keys to the HashMap, so there is no need to search the set for the elements.
If you need Set functionality but not specifically a Set implementation, it is faster to use a HashMap directly.
Map has four general-purpose implementations: Hashtable , HashMap, TreeMap, and, added in SDK 1.4, LinkedHashMap.
IdentityHashMap is based on identity comparisons rather than equality comparisons (equality comparisons form the basis for all general-purpose maps), making it the fastest useful Map.
IdentityHashMap has one tuning parameter, the expected maximum size, which can help avoid rehashing by setting the number of buckets in the initial map.
Attributes simply wraps a HashMap, and restricts the keys to be ASCII-character alphanumeric Strings, and values to be Strings.
WeakHashMap can maintain a cache of elements that are automatically garbage-collected when memory gets low.
Properties is a Hashtable subclass specialized for maintaining key-value string pairs in files.
In the case of the general-purpose Maps, TreeMap is significantly slower than the other Maps and should not be used unless you need the extra functionality of iterating ordered keys.
LinkedHashMap also provides the ability to iterate its keys in order, with the default order being key-insertion order.
LinkedHashMap should normally be faster than TreeMap, but slower than HashMap.
Hashtable is a synchronized Map, and HashMap is an unsynchronized Map.
Hashtable is present for backward compatibility with earlier versions of the JDK.
Nevertheless, if you need to use a synchronized Map, a Hashtable is faster than using a HashMap in a synchronized wrapper.
Hashtable, HashMap, and HashSet are all O(1) for access and update, so they should scale nicely if you have the available memory space.
LinkedHashMap is based on a hash table but also maintains a linked list of entries so it can use the linked list to iterate through the entries in a particular order: the default order is the insertion order of keys.
The class provides a method called removeEldestEntry( ) that is intended to be overriden in a subclass to provide a policy for automatically removing stale mappings when new mappings are added to the Map.
Vector, Stack, and ArrayList have underlying implementations based on arrays.
LinkedList has an underlying implementation consisting of a doubly linked list.
As such, LinkedList's performance is worse than any of the other three Lists for most operations.
For very large collections that you cannot presize to be large enough, LinkedList provides better performance when adding or deleting elements toward the middle of the list, if the array-copying overhead of the other Lists is higher than the linear access time of the LinkedList.
Otherwise, LinkedList's only likely performance advantage is as a first-in-first-out queue or double-ended queue.
A circular array-list implementation provides better performance for a FIFO queue.
I discuss the performance differences between LinkedLists and ArrayLists in much more detail later in this chapter.
Vector is a synchronized List, and ArrayList is an unsynchronized List.
Vector is present for backward compatibility with earlier versions of the JDK.
Nevertheless, if you need to use a synchronized List, a Vector is faster than using an ArrayList in a synchronized wrapper.
Stack is a subclass of Vector with the same performance characteristics, but with additional functionality as a last-in-first-out queue.
Because Hashtables and HashMaps are the most commonly used nonlist structures, I will spend a little extra time discussing them.
Hashtables and HashMaps are pretty fast and provide adequate performance for most purposes.
I rarely find that I have a performance problem using Hashtables or HashMaps, but here are some points that will help you tune them or, if necessary, replace them:
That's fine if you are using it to share data across threads, but if you are using it single-threaded, you can replace it with an unsynchronized version to get a small boost in performance.
Hashtables and HashMaps are resized whenever the number of elements reaches the [capacity x loadFactor]
This requires reassigning every element to a new array using the rehashed values.
This is not simply an array copy; every element needs to have its internal table position recalculated using the new table size for the hash function.
You are usually better off setting an initial capacity that handles all the elements you want to add.
This initial capacity should be the number of elements divided by the loadFactor (the default load factor is 0.75)
Hashtables and HashMaps are faster with a smaller loadFactor, but take up more space.
You have to decide how this tradeoff works best for you.
The hashing function for most implementations should work better with a capacity that is a prime number.
However, the 1.4 HashMap implementation (but not the Hashtable implementation) uses a different implementation that requires a power-of-two capacity so that it can use bit shifting and masking instead of the % operator.
If you specify a non-power-of-two capacity, the HashMap will automatically find the nearest power-of-two value higher than the specified capacity.
For other hash maps, always use a prime (preferably) or odd number capacity.
But note also that speedups from prime number capacities are small at best.
Access to the Map requires asking the key for its hashCode( ) and also testing that the key equals( ) the key you are retrieving.
You can create a specialized Map class that bypasses these calls if appropriate.
Alternatively, you can use specialized key classes that have very fast method calls for these two methods.
Note, for example, that Java String objects have hashCode( ) methods that iterate and execute arithmetic over a number of characters to determine the value, and the String.equals( ) method checks that every character is identical for the two strings being compared.
Considering that strings are used as the most common keys in Hashtables, I'm often surprised to find that I don't have a performance problem with them, even for largish tables.
From JDK 1.3, Strings cache their hash code in an instance variable, making them faster and more suited as Map keys.
If you are building a specialized Hashtable, you can map objects to array elements to preallocate HashtableEntry objects and speed up access as well.
The technique is illustrated in the "Search Trees" section later in this chapter.
The fewer entries that map to the same internal table entry, the more efficient the map.
There are techniques for creating more efficient hash maps, for instance, those discussed in my article "Optimizing Hash Functions For a Perfect Map" (see http://www.onjava.com/pub/a/onjava/2001/01/25/hash_functions.html)
Here is a specialized class to use for keys in a Hashtable.
This example assumes that I am using String keys, but all my String objects are nonequal, and I can reference keys by identity.
I compare the access times against all the keys using two different Hashtables, one using the plain String objects as keys, the other using my own StringWrapper objects as keys.
The StringWrapper objects cache the hash value of the string and assume that equality comparison is the same as identity comparison.
These are the fastest possible equals( ) and hashCode( ) methods.
The access speedups are illustrated in the following table of measurements (times normalized to the JDK 1.2 case):
The limited speedup from JDK 1.3 reflects the improved performance of Strings having their hash code cached in the String instance.
If you create a hash-table implementation specialized for the StringWrapper class, you avoid calling the hashCode( ) and equals( ) methods completely.
Instead, the specialized hash table can access the hashinstance variable directly and use identity comparison of the elements.
The speedup is considerably larger, and for specialized purposes, this is the route to follow:
Other than accessing and updating elements of collections, the most common thing you want to do is query the collection.
I'll use a list (indexable collection) of strings as my collection.
For the query I'll use a simple test that checks whether any particular string includes one of a set of specified substrings, and the query will simply return the count of how many strings include those substrings.
For my actual collection, I'll generate multicharacter strings using the lowercase characters of the Latin alphabet (a to z)
I'll simply query this collection for the count of strings that contain any of the substrings "ie" or "xy" or "pq"
The query is representative of the types I've seen in applications.
I change the query block to use the shortcut operator:
To avoid repeating the method call in the loop test, we can simply replace:
Let's push on and try eliminating the unnecessary String casts.
This is done simply by holding the first casted object in a variable of the appropriate type:
We have been using a Vector object to hold the collection so far.
In most applications, bottleneck queries tend to be read-only or single-threaded.
In either case, you can normally use a nonsynchronized object to hold the collection.
To do so here requires using an ArrayList object instead of the Vector object we initially used.
Once again, we see the best times yet for all the VMs.
Another standard optimization is to avoid repeatedly using a method accessor to access elements of a collection if it is possible to access the elements directly in some way.
For the example here, we could manage this by implementing our own java.util.List class and implementing the query in that class, thus gaining access to the internal collection.
Vector is implemented with its internal element collection defined as protected, so we can subclass Vector to gain access to the internal element collection and implement the query in our subclass as in the following code.
I'm frequently asked about how LinkedLists compare in performance to ArrayLists.
To fully consider the performance ramifications of these two classes, we need to know how they are implemented.
So I'll start with brief descriptions of the most important aspects of their implementations from the point of view of performance.
Vector and ArrayList are both implemented with an underlying Object[ ] array that stores the elements.
We access the elements in the internal array by index:
The Vector and ArrayList implementations have excellent performance for indexed access and update of elements because there is no overhead beyond range checking.
Adding elements to or deleting elements from the end of the list also gives excellent performance, except when the capacity is exhausted and the internal array has to be expanded.
Inserting and deleting elements always requires an array copy (two copies when the internal array must be grown first)
The number of elements to be copied is proportional to [size - index], i.e., to the distance between the insertion/deletion index and the last index in the collection.
For insertions, inserting at the front of the collection (index 0) yields the worst performance, and inserting at the end of the collection (after the last element) yields the best performance.
The array-copying overhead grows significantly as the size of the collection increases, because the number of elements that need to be copied with each insertion increases.
LinkedList is implemented using a list of doubly linked nodes.
To access elements by index, you need to traverse all the nodes until the indexed node is reached:
There are many different functions of the classes that could be tested.
LinkedLists are frequently used because of their supposedly better performance for random index insertion and deletion, so I decided to focus on insertion performance, i.e., building collections.
The insertion speed is critically dependent on the size of the collection and the position where the element is to be inserted.
All the best- and worst-case performances arise when inserting either at one of the ends or at the exact middle point of the collection.
Table 11-3 shows that for short collections, ArrayList and LinkedList are performance rivals.
ArrayLists have the edge when inserting at the end of the collection (appending elements)
But then appending elements is the operation that ArrayList is optimized for: if you just want a statically sized collection, a Java array (e.g., Object[ ]) gives better performance than any collection object.
Beyond the append operation, measured timings are mixed and reflect various VM optimization capabilities more than anything else.
What I have not measured here are other advantages that ArrayList has over LinkedList, namely the ability to presize collections and the reduced garbage-collection overhead.
Specifically, ArrayLists can be created with a particular size (e.g., in this test the ArrayList could be created with a capacity of 100 elements), thus avoiding all the growth overhead.
When running this same test with presized ArrayLists, the ArrayLists times are roughly twice as fast as those recorded in the table! LinkedLists (up to SDK 1.4) cannot be presized.
Additionally, the ArrayList generates only a few extra objects for garbage collection, i.e., the internal array object that holds the elements and one extra internal array object each time the ArrayList capacity is.
The LinkedList generates one node object for every insertion, irrespective of any deletions that might take place.
Consequently, LinkedLists can give considerably more work to the garbage collector (many more objects to collect)
Taking these added factors into account, my inclination would be to use an ArrayList rather than a LinkedList for any small- to medium-sized collection.
We can see from Table 11-4 that we begin to get a severe performance penalty when we encounter large insertion overhead.
The worst case for LinkedList is, as predicted, inserting in the middle of the collection.
We can also see that this has worse performance than the ArrayList worst case of insertion at the start of the collection.
Insertion at the middle of the ArrayList has significantly better performance than those two worst cases.
Overall, ArrayList again gives better performance for most cases, including index insertion to random locations.
If you always need to insert toward the beginning of the collection, LinkedList is a good choice, but you can achieve even better performance using a reversed ArrayList, i.e., either with a dedicated implementation or by flipping indexes using a [size-index] mapping.
The results for very large collections (not shown), indicates very similar conclusions to those of Table 11-4
However, times are so long that they emphasize that very large collections need particularly close matches between data, collection types, and data-manipulation algorithms.
Otherwise, you can end up with performance that is essentially unusable.
For optimum performance, you should build a dedicated collection class implementation specific to the problem.
This is often a necessary step for very large collections.
Querying is most efficiently achieved by implementing the query inside the class (see Section 11.4)
The time needed to iterate over all the elements is the limiting factor for queries on these lists.
A query implemented in the ArrayList/Vector classes iterates over the array elements.
These iterators would be necessary if the query could not be implemented in the List class.
Once again, ArrayList shows superior performance, though not as dramatically as with Table 11-5
Iterating through all the elements of the collection using internal access.
Iterating through all the elements of the collection using a ListIterator.
The measurements and the other factors we've considered clearly indicate that ArrayLists and Vectors usually provide better performance than LinkedLists and synchronized wrapped LinkedLists.
Even in cases where you might have thought that the LinkedList would provide better performance, you may be able to coax superior performance from ArrayList by altering how elements are added, for example by reversing the collection order.
There are situations where LinkedLists provide better performance, for example with very large collections where many elements need to be added to both the beginning and end of the collection.
But in general, I recommend using ArrayList/Vector as the default and using LinkedList only where there is an identified performance problem that a LinkedList improves.
RandomAccess is a marker interface, like the Serializable and Cloneable interfaces.
Instead, they identify a class as having a particular capability.
So now that we know what RandomAccess means, how do we use it? There are two aspects to using the.
Of course, we still need to decide whether any particular class implements it, but the possible classes are severely restricted: RandomAccess should be implemented only in java.util.List classes.
The SDK provides the most frequently used implementations, and subclasses of the SDK classes do not need to implement RandomAccess because they automatically inherit the capability where appropriate.
The second aspect, using the RandomAccess capability, is also different.
Instead, you need to explicitly check whether a class implements RandomAccess using the instanceof operator: if (listObject instanceof RandomAccess) ...
You must then explicitly choose the appropriate access method, List.get( ) or Iterator.next( )
Clearly, if we test for RandomAccess on every loop iteration, we would be making a lot of redundant calls and probably losing the benefit of RandomAccess as well.
So the pattern to follow in using RandomAccess makes the test outside the loop.
I tested the four code loops shown in this section, using the 1.4 release, separately testing the -client (default) and -server options.
ArrayList has an underlying implementation consisting of an array with constant access time for any element, so using the ArrayList iterator is equivalent to using the ArrayList.get( ) method but with some additional overhead.
LinkedList has an underlying implementation consisting of linked node objects with access time proportional to the shortest distance of the element from either end of the list, whereas iterating sequentially through the list can shortcut the access time by traversing one node after another.
Times shown are the average of three runs, and all times have been normalized to the first table cell, i.e., the time taken by the ArrayList to iterate the list using the List.get( ) method in client mode.
The most important results are in the last two rows.
The last line shows the times obtained by making full use of the RandomAccess interface, and the line before that shows the most optimal general technique for iterating lists if RandomAccess is not available.
The size of the lists I used for the test (and consequently the number of loop iterations required to access every element) was sufficiently large that the instanceof test had no measurable cost in comparison to the time taken to run the loop.
Consequently, we can see that there was no cost (but also no benefit) in adding the instanceof RandomAccess test when iterating the LinkedList, whereas the ArrayList was iterated more than 20% quicker when the instanceof test was included.
Can you use RandomAccess and maintain backward compatibility with VM versions prior to 1.4? There are three aspects to using RandomAccess:
You may want to include code referencing RandomAccess without moving to 1.4
You will want to make your code forward-compatible so that it automatically takes advantage of RandomAccess when running in a 1.4+ JVM.
Caches use local data when present and thus don't need to access nonlocal data.
If the data is not present locally, the nonlocal data must be accessed or calculated; it is then stored locally as well as being returned.
After the first access, the data is available locally, and access is quicker.
Most caches have to maintain the consistency of the data held in the cache: it is usually important for the data in the cache to be up to date.
When considering the use of a cache, bear in mind the expected lifetime of the data and any refresh rate or time-to-live values associated with the data.
Similarly, for output data, consider how long to keep data in the cache before it must be written out.
You may have differing levels of priority for writing out different types of data.
For example, some filesystems keep general written data in a write cache, but immediately write critical system data that ensures system consistency in case of crashes.
Also, as caches cannot usually hold all the data you would like, a strategy for swapping data out of the cache to overcome cache space limitations is usually necessary.
The memory used by the cache is often significant, and it is always better to release the resources used by it explicitly when it is no longer needed, or reduce resources being used by the cache when possible, even if the cache itself is still required.
Caching can apply to data held in single objects or groups of objects.
For single objects, it is usual to maintain a structure or instance variable that holds cached values.
For groups of objects, there is usually a structure maintained at the point of access to the elements of the group.
In addition, caching applies generally to two types of locality of access, usually referred to as spatial and temporal.
Spatial locality refers to the idea that if something is accessed, it is likely that something else nearby will be accessed soon.
This is one of the reasons buffering I/O streams works so well.
If every subsequent byte read from disk were in a completely different part of the disk, I/O buffering would be no help at all.
Temporal locality refers to the idea that if you access something, you are likely to access it again in the near future.
This is the principle behind browsers holding files locally once downloaded.
There is a lot of research into the use of caches, but most of it is related to CPU or disk hardware caches.
Nevertheless, any good article or book chapter on caches should cover the basics and the pitfalls, and these are normally applicable (with some extra thought) to caches in applications.
One thing you should do is monitor cache-hit rates, i.e., the number of times that accessing data retrieves data from the cache, compared to the total number of data accesses.
This is important because if the cache-hit rate is too low, the overhead of having a cache may be more than any actual gain in performance.
It is frequently useful to build-in the option of disabling and emptying the cache.
First, you can make direct comparisons of operations with and without the cache, and second, there are times when you want to measure the overhead in filling an empty cache.
In this case, you may need to repeatedly fill an empty cache to get a good measurement.
When accessing elements from sets of data, some elements are accessed much more frequently than others.
In these cases, it is possible to apply caching techniques to speed up access to frequently accessed elements.
Consider a CacheTest class that consists mainly of a Map populated with Integer objects.
I use Integer objects for convenience to populate the Map with many elements, but the actual object type is of no significance because you use only the hashCode( ) and equals( ) methods, just as the Map does.
Basically, you provide two ways to access the elements of the Map.
The first, plain_access( ), just calls the Map.get( ) method as usual.
The second method, cached_access( ), uses the lower bits of the hash code of the object to obtain an index value into an array.
This index is then checked to see whether the object is there.
If it is, the corresponding value in a parallel value array is returned.
If it's not, the object is placed there with the value in the corresponding value array.
This is about the simplest example of general cached access.
I have selected 10 integers that do not map to the same indexes for the example.
Running the class gives a straightforward comparison between the two access methods, and I get the result that the cached access varies significantly depending on the VM used.
The access speedups are illustrated in the following table of measurements.
Times have been normalized to the JDK 1.2.2 case for using a HashMap.
The first time of each entry is the measurement using a HashMap, and the second is the measurement using a Hashtable.
This test is artificial, in that I chose integers where no two map to the same index.
If there is more than one integer that maps to the same cache array index, this is called a collision.
Clearly, with collisions, performance is not as good because you are constantly entering the code that puts the objects into the cache.
Collisions are a general problem with cached data, and you need to minimize them for optimal performance.
This can be done by choosing an appropriate mapping function to generate indexes that minimize collisions:
The index to use for the next object added to the cache static int freeIndex = 0;
The current index in the cache referenced by this object int cacheRef = -1; //Unique integer for each object, can be used as hash code int value;
Access index into the cache is quick and easy to get int access = i.cacheRef; Object o;
The problem considered here concerns a large number of string keys that need to be accessed by full or partial match.
The partial-match access needs to collect all objects that have string keys starting with a particular substring.
Of course, I am considering here a large collection of strings.
Alternatives are not usually necessary for a few (or even a few thousand) strings.
To tune, look for data structures that quickly match any partial string.
The task is somewhat simpler than the most generic version of this type of problem because you need to match only the first few consecutive characters.
This means that some sort of tree structure is probably ideal.
Of the structures available from the JDK, TreeMap looks like it can provide exactly the required functionality; it gives a minimal baseline and, if the performance is adequate, there is no more tuning to do.
The target is to obtain HashMap access speed for single-key access.
Don't get carried away searching for the perfect data structure.
If you have the strings in a sorted collection, you can apply a binary search to find the index of the string that is greater than or less than the partial string, and then obtain all the strings (and hence corresponding objects) in between.
More specifically, you can construct a sorted array of keys from the hash table.
Then, if you want to find all strings starting with "h", you can run a binary search for the strings "h" and "h\uFFFF"
This gives all the indexes of the band for all the keys that start with "h"
Note that a binary search can return the index where the string would be even if it is not actually in the array.
The correct solution actually goes from "h" inclusive to "i" exclusive, but this solution will do for strings that don't include character \uFFFF.
Having parallel collections can lead to all sorts of problems in making sure both collections contain the same elements.
Solutions that involve parallel collections should hide all accesses and updates to the parallel collections through a separate object to ensure that all accesses and updates are consistent.
The solution here is suitable mainly when the collections are updated infrequently, e.g., when they are built once or periodically and read from often.
Here's an alternate solution to the problem presented in the last section.
I looked for a more obvious solution, another tree structure that would handle the search, provide full keyed access, and give plenty of scope for tuning.
Jon Bentley and Bob Sedgewick[9] detail a potential solution that offers an interesting structure and provides a good tuning exercise, so I will use it here.
The structure is a halfway point between binary trees of strings (one string per node) and digital tries.
Digital tries are lightning-fast to search, but have exorbitant storage costs that typically rule them out as a solution.
The ternary tree node searches by comparing the current character with the current node's character.
If equal, the next character in the string becomes the current character, and the node at the "equal" pointer becomes the current node.
Otherwise, the current character in the string remains the current character, and the node at the "higher" or "lower" pointer becomes the current node.
If you compare update and access times against HashMap , you'll find that the TernarySearchTree is much slower.
We are expecting this slowdown, because the referred article does the same comparison and indicates that many optimizations are necessary to achieve similar times to the HashMap.
Since they do achieve similar times after optimizing, assume that you can too, and run through a tuning phase to see what improvements you can make.
The target is always the HashMap times, since you already have TreeMap if you need the partial-matching functionality without HashMap access speed.
If TreeMap does not exist, or you tune another structure with no counterpart, you still need a goal for the performance.
In the absence of application requirements, you should aim for the performance of some other existing class that provides similar or partial functionality.
For this example, it is still sensible to use HashMap to provide the performance target because the full key-match access time for the structure will probably be compared to HashMap access times by most developers.
Knowing that tree access and update are susceptible to the order of keys added, you are testing both for randomized order of insertion and mostly-sorted order, so that you know the near worst case and (probable) average case.
Take a HashMap that is presized (i.e., large enough to avoid rehashing after addition of all keys), using the case where the keys are mostly sorted as a baseline.
The following chart shows the times using Sun VM Version 1.2 with JIT (the ratios vary under other VMs):
You can see that you need to gain an order of magnitude to catch up to the HashMap performance.
Profiling is not a huge help; it says only that you need to improve the times on these few methods you have.
Well, it's a little better, but it should be a lot better.
Bentley and Sedgewick suggest one obvious improvement: changing the recursive algorithms to iterative ones.
This is a standard tuning technique, but it can be difficult to achieve.
You can use the implementations here as a sort of template: look at how the recursion has been converted into iteration by having a "current" node, p, which is changed on each pass.
But why is there such a discrepancy between the average and worst (mostly-sorted) case now, and why is there an improvement in the access times when the change should have altered only the insertion times? The discrepancy between the average and worst cases may indicate that the worst times are a result of the time spent in the stack due to the depth of the insert/search loops rather than node creation.
The improvement in the access times may be due to internal memory management of the VM: all the nodes being created one after the other may be the reason for the improved access.
Although since everything is in memory, I'm not quite sure why this would be so.
Possibly, the heap space is segmented according to requisition from the OS, and you may have paged slightly to disk, which could explain the discrepancy.
But I have discarded any timings that had any significant paging, so the discrepancy is not entirely clear.
There is an extra issue now, as you have not been measuring the time taken in creating the tree.
The last optimization has increased this time, as the nodes were previously created during the insert, but are now all created at tree creation.
Consequently, you might now need to consider this creation time, depending on how the data structure is used.
It would be nice if there were a way to create all nodes required in one VM call, but there is none provided in Java (at least up to JDK 1.4)
You can finesse this shortcoming by implementing your own memory management using arrays, and it is certainly worth doing so as an exercise to see if the technique gives any advantages.
Another possibility is to create objects at initialization in a separate thread, but this requires previous knowledge of many things, so I will not consider that option here.
A buffer for the string char[  ] buff = new char[5000]; //the array of node 'object's int[  ] nodes; //the array of Object values, one for each node Object[  ] objects;
This alternative implementation does not affect the external interface to the class, so the complexity remains hidden from any other class.
This implementation is much closer to the C implementation provided by Bentley and Sedgewick, where nodes were allocated in a similar large chunk.
Now the question is, have we improved performance? The next table shows the current measurements (previous measurements in parentheses):
Overall, these are the best results so far, and the worst case is much closer to the average case.
Also, the object-creation time is much better: it is essentially as fast as possible in a VM since you are creating just two new significant objects (which are very large arrays), and so the limitation is purely down to how quickly the VM can allocate the memory space.
You might be satisfied to stop at this point, even though your structure is slower than a HashMap by a factor of two.
It does provide the extra required functionality of partial matching, as well as the full matching that HashMaps provide, and relative performance is acceptable.
You know that digital search tries are extremely fast, but inefficient in space.
If you are prepared to accept the extra space taken, you can still consider using digital tries to achieve improved performance.
If you are using strings that contain mainly the ASCII character set, consider using a digital search trie for the first couple of characters.
So you have a quarter of a megabyte before you even start to use this structure.
However, if you use this structure for a large amount of string data, you may find this memory usage small compared to the final overall size.
Assuming this is acceptable, let's look at how it is implemented and how it performs.
Basically, you implement a trie for the first two characters, but each two-character node then points to the root of a ternary tree.
The two-digit trie needs a parallel Object structure to store the Object values that.
This is, of course, occupying a lot of space, and there are methods for reducing the space requirements (for example, you can optimize for just the alphanumeric characters, mapping them into smaller arrays), but for this exercise, let's keep it simple.
The results are the best yet for all values except the random access, which is roughly the same as before.
Perhaps the most interesting aspect is that you now get better times on the mostly-sorted input than on the randomized input (which is also the case for the HashMap)
The result is still slower than a HashMap, but has the extra capability to identify partial matches efficiently.
For more specialized versions, such as those needed for a particular application, you can make an implementation that is significantly faster (just as for the hashtable structures earlier in this chapter)
All in all, we've taken a particular structure in its initial form, optimized it using various techniques, and made it two to eight times faster accessing and updating elements.
Note that there are also costs beyond the extra space costs for this last hybrid structure.
The implementation before this last one is still a pure ternary tree.
That pure implementation has some elegant and simple recursive algorithms for iterating through the tree in order and for identifying partial matches.
However, implementing the equivalent algorithms for the last hybrid structure is quite a bit more complicated, as you have to jump between the two structures it uses.
There is not much educational value in proceeding further with these classes here.
We've covered the uses of different structures and how to reimplement classes to use different underlying structures for the purpose of improving performance.
This book is not intended to provide finished components, so if you feel that this structure may be useful to you in some situation, you'll need to complete it yourself.
Just a few final performance notes about the optimized class.
So note that its size is given by the high-water mark, which is easily determined.
And if you want to make the class dynamically growable at large sizes, you may be better off catching the exception thrown when the high-water mark grows past the end of the nodes array and then copying to a larger array, rather than making a test on each insertion.
Most of these suggestions apply only after a bottleneck has been identified:
Test using either the target size for collections or, if this is not definite, various sizes of collections.
Test updating collections using the expected order of the data or, if this is not definite, various orders of data, including sorted data.
Match the scaling characteristics of the structures against the volumes of data likely to be applied.
Test for the RandomAccess interface where applicable for faster list iteration.
Consider using two collections with different performance characteristics to hold the same data.
Use specialized collections that avoid casts or unnecessary method calls.
Consider wrapping the elements of the collection in a specialized class that improves access times (e.g., Hashtable key class with faster hashCode( ) and equals( ) methods)
Add caching or specialized accessors to collections where some elements are accessed more often than others.
Access the underlying collection structure when iterating over the elements of a collection.
Copy the elements into an array rather than access many elements one at a time through the collection element accessors.
Preallocate memory for element storage rather than allocating at update time.
Map elements into one or more arrays of data variables to allocate the whole collection in as few calls as possible.
Test if changing a recursive algorithm to an iterative one provides a useful speedup.
This type of bottleneck is essentially independent of the distributed nature of the application, and the other chapters in this book deal with how to tune this type of bottleneck.
In this chapter, we deal with the second type of bottleneck, which occurs within the distribution infrastructure.
This latter type of bottleneck is specific to the distributed nature of the application, and can be tuned using a number of techniques:
When an application repeatedly distributes the same data, a significant gain in performance can be obtained by caching the data, thus changing some distributed requests to local ones.
If the volume of data being transferred is large or causes multiple chunks to be transferred, then compressing the transferred data can improve performance by reducing transfer times.
Most distributed applications have their performance limited by the latency of the connections.
Each distributed message incurs the connection-latency overhead, and so the greater the number of messages, the greater the cumulative performance delay due to latency.
Reducing the number of messages transferred by a distributed application can produce a large improvement in the application performance.
The performance of any distributed function in a distributed application normally has at least two factors involved.
These two factors are the location for the function to execute and the location where the data for the function resides.
Typically, the application developers are faced with the choice of moving the function to the location of the data, or moving the data to the location of the function.
These decisions depend on the volume and nature of the data to be processed, the relative power and availability of the CPUs in the different locations, and how much of the data will be transferred after the function completes.
If the function's result is to transfer a relatively small amount of data, it should be located on the machine where the data used by the function resides.
There are several ways that batching can improve the performance of a distributed application.
First, the number of messages can be reduced by combining multiple messages into fewer batched messages.
Second, data can be split up and transferred in shorter batches if waiting for all the data is the cause of the delay in response times.
Third, data requirements can be anticipated, and extra data can be transferred in batches together with the data that is needed at that moment, in anticipation of the extra data that will be needed soon.
Further batching variations can be used by extending these strategies.
When data needs to be transferred across a distributed application, the distribution infrastructure often uses general mechanisms for transfers.
This results in transferring more data than is actually required.
By selectively "stubbing out" data links, only the data that is needed is transferred.
Instance variables of objects can be replaced with "stub" objects that respond to messages by transferring the required data (if the fields are defined using an interface)
Java also supports the transient modifier, which can be used to eliminate unnecessary data transfers.
Fields defined as transient are not transferred when serialization is used, but this is a rather blunt technique that leads to all-or-nothing transfers of fields.
Distributed systems should make maximum use of asynchronous activities wherever possible.
No part of the application should be blocked while waiting for other parts of the application to respond, unless the application logic absolutely requires such blocked activities.
In the following sections, we look at examples of applying some of these techniques to optimize performance.[1]
In addition, there is one other monitoring tool I often find useful when dealing with distributed applications: a relay server.
This is a simple program that accepts incoming socket connections and simply relays all data on to another outgoing socket.
Normally, I customize the server to identify aspects of the application being monitored, but having a generic relay server as a template is useful, so I present a simple one here:
A string message to printout for logging identification String message;
This instructs the browser to connect to the relay server, and the relay server acts like a web server at someserver (i.e., as if the URL had been http://someserver/some/path/)
Let's look at a simple example of reducing message calls.
In the example, I present a simple server object that supports only three instance variables and three methods to set those instance variables.
You can easily create your own communication mechanisms by connecting two processes using standard sockets.
Creating two-way connections with Sockets and ServerSockets is very straightforward.
For basic communication, you decide on your own communication protocol, possibly using serialization to handle passing objects across the communication channel.
However, using proprietary communications is not a wise thing to do and can be a severe maintenance overhead unless your communication and distribution requirements are simple.
I occasionally use proprietary communications for testing purposes and for comparison against other communications infrastructures, as I have done in this chapter.
In this chapter, I use a simple, generic communications infrastructure that automatically handles remotely invoking methods: basically, a stripped-down version of RMI.
I generate a server skeleton and client proxy using reflection to identify all the public methods of the distributable class.
Then I copy the RMI communication protocol (which consists of passing method identifiers and parameters from proxies to server objects identified by their own identifiers)
The only other item required is a lookup mechanism, which again is quite simple to add as a remotely accessible table.
My client simply resolves the server object from the name service, obtaining a proxy for the server, and then calls the three methods and sets the three instance variables.
For the test, I repeat the method calls a number of times to obtain average measurements.
The optimization to reduce the number of method calls is extremely simple.
Just add one method, which sets all three instance variables in one call in the following IDL definition:
All the support classes are generated using the rmic utility.
In addition, I define a main( ) method that sets a security manager and installs an instantiation of the server object in the name service.
Once again, the result is that the single method call requires one-third of the network transfers and takes one-third of the time, compared to the triple method calls (see Section 12.3)
A distributed system can be defined with sockets and serialization.
The server object is defined as before, with the interface:
Yet again, the result is that the single method call requires one-third of the network transfers and takes onethird of the time, compared to the triple method calls (see the next section)
In the previous sections, we saw how reducing the number of messages led to a proportional reduction in the time taken by the application to process those messages.
Table 12-1 compares the performance between the different communications layers used in those sections.
Executing three separate methods Executing one combined method Time taken Bytes written Overhead time Time taken Bytes written Overhead time.
Here, I detail the measurements made for the three communications layers using the tests defined in the "Message Reduction" section.
The first three columns list measurements taken while executing the three updating methods together.
The second three columns list the measurements taken when the single updating method updates the server object.
Within each set of three columns, the first column lists the round-trip time taken for executing the methods, with all times normalized to the proprietary communications layer time in the combined method case.
The network round-trip overhead is a 10-millisecond ping time in these tests.
The second column lists the number of bytes written from the client to the server to execute one set of methods, and the third column lists the time taken to run the test with no latency (i.e., client and server on the same machine), using the same time scale as the first column.
As you can see, CORBA has more overhead than RMI, which in turn has more overhead than the proprietary system.
If you include optimized serialization, which can be easily done only for the proprietary layer, the advantages would be even greater.
However, the proprietary layer requires more work to support the distribution mechanisms, and the more complicated the application becomes, the more onerous the support is.
Given this difference, it is probably not surprising that CORBA has the better scaling characteristics.
It appears that for simple distributed applications, a proprietary communications layer is most efficient and can be supported fairly easily.
For distributed applications of moderate complexity and scale, RMI and CORBA are similar in cost, though it is easier to develop with RMI.
For large-scale or very complex distributed applications, CORBA appears to win out in performance.
To illustrate caching, I extend the server object used in the previous sections.
I add three accessor methods to access the three server instance variables:
The client code needs to be changed in only one place; once the server object is resolved, the resolved proxy is wrapped in this caching proxy.
One form of batching optimization is to combine multiple messages into one message.
For the examples we've examined so far, this is easily illustrated.
Simply add a method to access all attributes of the server object in one access:[2]
This book is not a tutorial on CORBA or RMI, so I have elected to show a standard Java representation of the required classes.
A simple but dramatic example of the benefits of application partitioning is to run two identical queries on a collection.
One query runs on the server and returns only the result of the query; the second query runs on the client, requiring the collection to be copied to the client.[3]
This example is based on a demonstration originally created by GemStone to show the benefits of application partitioning using their application server.
It's pretty obvious that since the only difference between the two queries is the amount of data being copied across the network, the second query that copies much more data is slower.
For the example, I use a large array of strings and create a query that returns that subset of strings that includes the query string, e.g., "el" is included in "hello" but not in "hi."
The client query method is logically defined in the client stub or the client proxy object defined for the application.
But technically, it is not forced to be defined in these classes and can be defined in any client class that has access to the server proxy object.
Application partitioning similarly applies to moving some of the "intelligence" of the server to the client to reduce messaging to the server.
A simple example is a client form where various fields need to be filled in.
Often, some of the fields need to be validated according to data format or ranges.
Any such validation logic should be executed on the client; otherwise, you are generating a lot of unnecessary network transfers.
Most applications have a widget customized to handle their date-field entries.
But the general area of user-interface presentation logic is one in which the logic should reside mostly on the client.
To illustrate a second type of batching, we modify the test query from the last section.
The only difference is in the choice of string to pass into the query so that the result of the query is a large set.
In this test, the result set is over 25,000 strings.
The client query is still significantly longer than the server query, but even the server query now takes several seconds in absolute time.
There is no reason to make the user wait for the whole result set to be transferred before displaying some of the results.
Altering the application to send results in batches is quite easy.
You need to add an intermediate object to hold the results on the server, which can send the results in batches as required.
There are a number of optimizations you can make to the low-level communications infrastructure.
These optimizations can be difficult to implement, and it is usually easier to buy these types of optimizations than to build them.
Where the distributed application is transferring large amounts of data over a network, the communications layer can be optimized to support compression of the data transfers.
In order to minimize compression overhead for small data transfers, the compression mechanism should have a filter size below which compression is not used for data packets.
The JDK documentation includes an extended example of installing a compression layer in the RMI communications layer (the main documentation index page leads to RMI documentation under the "Enterprise Features" heading)
The following code illustrates a simple example of adding compression into a communications layer.
Caching at the low-level communications layer is unusual and often a fallback position where the use of the communications layer is spread too widely within the application to retrofit low-level caching in the application itself.
But caching is generally one of the best techniques for speeding up client/server applications and should be used whenever possible, so you could consider low-level caching when caching cannot be added directly to the application.
Caching at the low-level communications layer cannot be achieved generically.
The following code illustrates an example of adding the simplest low-level caching in the communications layer.
Batching can be useful when your performance analysis indicates there are too many network transfers occurring.
The standard batching technique uses two cutoff values: a timeout and a data limit.
The batched transfer is triggered either when the timeout is reached or when the data limit (which is normally the batch buffer size) is exceeded.
The following code illustrates a simple example of adding batching to the communications layer.
Multiplexing is a technique where you combine multiple pseudo-connections into one real connection, intertwining the actual data transfers so that they use the same communications pipe.
This reduces the cost of having many communications pipes (which can incur a heavy system load) and is especially useful when you would otherwise be opening and closing connections a lot: repeatedly opening connections can cause long delays in responses.
Multiplexing can be managed in a similar way to the transfer-batching example in the.
If a client holds a proxy to an object in the server, it is important that the server does not garbage-collect that object until the client releases the proxy (and it can be validly garbage-collected)
Most third-party distributed systems, such as RMI, handle distributed garbage collection, but that does not necessarily mean it will be done efficiently.
The overhead of distributed garbage collection and remote reference maintenance in RMI can slow network communications by a significant amount when many objects are involved.
Of course, if you need distributed reference maintenance, you cannot eliminate it, but you can reduce its impact.
You can do this by reducing the number of temporary objects that may have distributed references.
The issue is considerably more complex in a multiuser distributed environment, and here you typically need to apply special optimizations related to the products you use in order to establish your multiuser environment.
However, in all environments, reducing the number and size of the objects being used is typically the most effective optimization.
The techniques described in Chapter 4 are relevant to reducing the number of objects in a distributed system, and should be applied where possible.
Usually, the database documentation includes a section on optimizing performance, and that is the place to start.
Here are some hints applicable to many databases (note that JDBC optimizations are covered in Chapter 16):
Object databases are usually faster than relational databases for applications with strongly objectoriented designs, especially when navigating object networks[5] is a significant part of the application.
By "navigating object networks," I mean the activity of repeatedly accessing objects from one object's instance variables to another's.
The structure formed by the graph of objects reachable through nested instance variable access is a network.
Relational databases are generally faster than object databases when dealing with large amounts of basic data types, e.g., for objects whose object types are easily mapped into relational tables.
Reducing the amount of data transferred over the network is often the key to good performance with databases.
Stored procedures are precompiled SQL code that can be executed by the database server.
Database queries are often faster if they are statically defined, i.e., defined and precompiled.
For relational databases, these take the form of prepared statements that can usually accept parameters.
Many object databases also support statically defined queries that can navigate object networks more quickly using internal nodal access rather than executing methods.
Many databases support batching queries to reduce the number of network round trips, and these batching features should be used to improve performance.
Transactional access to databases is slower than nontransactional access, so use the nontransactional form whenever possible.
As such, all of the general guidelines for efficient client/server systems from previous sections also apply to improving the performance of Web Services.
The simplicity of the Web Services model has both advantages and disadvantages for performance (see Table 12-3)
Web Services is too simple for many distributed application requirements.
The many additional features in CORBA and RMI are not whimsical; they are there in response to recognized needs.
This implies that as these needs are transferred to Web Services, the Web Services standards will evolve to support additional functionality.
Typically, the more functionality that is added to the standard, the worse performance becomes because the architecture needs to handle more and more options.
So consider the performance impact of each function added to the Web Services standards.
Reduces communication overhead and resource management otherwise required to keep track of connected objects and signal reclaimable objects.
Objects have to time out (which means they are around longer than necessary) or are created for each request (which means they are created more often than necessary)
Transactional overhead can be one of the highest costs of short, distributed communications, equivalent to the network communication latency.
If transactions are required, they have to be built on top of Web Services, which means they will be less efficient than transactions supported within Web Services.
Stateless protocols scale much better than stateful protocols, as the success of the Web proves.
State must be maintained in the server, complicating server processes and making them less efficient, or be transferred with every request, increasing the communication cost.
Security can be efficiently added by wrapping the Web Services server interface with an authentication layer.
None really, as long as security is easy to add when required.
As I write this, there is a market opportunity for Web Services profiling and measurement tools.
You can use web measurement tools, such as load-testing tools and web-server monitoring tools, but these provide only the most basic statistics for Web Services, and are not normally sufficient to determine where bottlenecks lie.
For developers, this means that you cannot easily obtain a Web Services profiling tool, and consequently breaking down end-to-end performance of a Web Service and finding bottlenecks may be challenging.
Currently the best way to measure the component parts of Web Services seems to be to explicitly add logging points (see, for example, Steve Souza's Java Application Monitor at http://www.JavaPerformanceTuning.com/tools/jamon/index.shtml)
The major Web Services component times to measure are the time taken by the server service, the time taken by the server marshalling, the time taken by the client marshalling, and the time taken to transport the message.
It is important (but difficult to determine) the time taken in marshalling and unmarshalling and the time taken for network transportation, so that you know where to focus your tuning effort.
Note that I include these points because the client perception of your service is affected not only by how long the server takes to process it but also by any delays in the server receiving the message, and because the time taken to receive the message depends on the size of the returned message.
Specifically, if the TCP data has arrived at the server (or starts to arrive at the server if it requires several TCP packets) but the server does not start reading because it is busy, this service wait time is an overhead that adds to the time taken to service the request.
In the same way, the larger the size of the returned data, the more time it may take to be assembled on the client side before unmarshalling can begin, which again adds overhead to the total service time.
In practice, what tends to get measured is either the full round-trip time (client to server and back) with no breakdown, or only the server-side method call.
But there are a number of different ways to infer some of the intermediate measurements.
The following sections detail various ways to directly measure or infer some Web Service request times.
To measure the full round-trip time, employ the wrapping technique that we just described, but this time, in the client.
To infer round-trip overhead, simply measure the time taken to execute a call to an "echo" Web Service, i.e., the Web Service implemented as:
You can infer the combined time taken to transfer the data to and from the server by executing the Web Service in two configurations: across the network, and with both client and server executing on the local machine.
Note that since this is likely to be communication over the Internet, you can measure only average times or daily profile times.
You should repeat the measurements many times and either take the average or generate a profile of transport times at different times of the day.
From the previous measurements, you can subtract network communication time, DNS time, and server-side method execution time from the total round-trip time to obtain the remaining overhead time, which includes marshalling and other actions such as object resolution, proxy method invocation, etc.
The majority of this overhead time is expected to come from marshalling.
If your Web Service is layered behind a web server that runs a Java servlet, you can add logging to the web server layer in the doGet( ) and doPost( ) methods.
Since these servlet methods are called before any marshalling is performed, they provide more direct measurements of marshalling and unmarshalling times.
In addition to measuring individual calls, you should also load-test the Web Service, testing it as if multiple, separate clients were making requests.
It is not difficult to create a client to run multiple requests to the Web Service, but there are also free load-testing utilities that you can use, such as Load (available from http://www.pushtotest.com)
At the core, this is true, but Web Services standards target a simpler type of architecture and are already more widely accepted and used.
It is worth emphasizing that the previous sections of this chapter, as well as other chapters in this book, also apply to performance-tuning Web Services.
As with all distributed computing, caching is especially important and should be applied to data and metadata such as WSDL (Web Services Description Language) files.
The generation and parsing of XML is a Web Service overhead that you should try to minimize by using specialized XML processors.
Additionally, a few techniques are particularly effective for high-performance Web Services:
If you read the "Message Reduction" section, it should come as no surprise that Web Service methods should have a large granularity.
A Web Service should provide monolithic methods that do as much work as possible rather than many methods that perform small services.
The intention is to reduce the number of client/server requests required to satisfy the client's requirements.
For example, the classic example of a Web Service is providing the current share price of a company quoted on a stock exchange:
First, as already explained, I have changed the methods to accept and return an array of Strings so that multiple prices for multiple companies can be obtained in one request.
Second, I have not retained the previous interfaces that handle only one company at a time.
This is a deliberate attempt to influence the thinking of developers using the service.
I want developers of clients using this Web Service to immediately think in terms of multiple companies per request so that they build their client more efficiently.
As the server Web Services manager, this benefits me twice over: once by influencing clients to be more efficient, ultimately giving my service a better reputation, and again by reducing the number of requests sent to my Web Service.
Note that if a client is determined to be inefficient, he can still send one request per company, but at least I've tried my best to influence his thinking.
The third change I've made is to add a new method.
The original interface had two methods: one to get quotes using the company symbol and the other to get the company symbol using the company name.
In case you are unfamiliar with stock market exchanges, I should explain that a company may have several recognizable names (for example, Big Comp., Big Company, Big Company Inc., The Big Company)
The stock exchange assigns one unique symbol to identify the company (for example, BIGC)
The getSymbol( ) method provides a mechanism to get the unique symbol from one of the many alternative company names.
With only the two methods, if a client has a company name without the symbol, it needs to make two requests to the server to obtain the share price: a request for the unique symbol and a request for the price.
By adding a third method that gives a price directly from one of the various valid company names, I've provided the option to reduce requests for those clients that need this service.
Think through the service you provide, and try to design a service that minimizes client requests.
Similarly, if you are writing a Web Services client and the service provides alternative ways to get the information you need, use the methods that minimize the number of requests required.
Think in terms of individual methods that do a lot of work and return a lot of information rather than the recommended object-oriented methodology of many small methods that each do a little bit and combine to do a lot.
Unfortunately, you also need to be aware that if the interface is too complex, developers may use a competing Web Service provider with a simpler (but less efficient) interface that they can more easily understand.
The most efficient architecture for maximal scalability is a load-balanced server system.
This architecture allows the client to connect to a frontend load balancer, which performs the minimum of activity and whose main job is to pass the request onto one of several backend servers (or cluster of servers) that perform the real work.
Since Web Services already leverages the successful HTTP protocol, you can immediately use a web-server load balancer without altering any other aspect of the Web Service.
A typical load-balancing Web Service would have the client connect to a frontend load balancer, which is a proxy web server, and have that load balancer pass on requests to a farm of backend Web Services.
The main alternative to this architecture is to use round-robin DNS, where the DNS server supplies a different IP address from a list of servers for each request to resolve a hostname.
The client automatically connects to a random server in a farm of replicated Web Services.
A different load-balancing scheme is possible by controlling the WSDL document and sending WSDL containing different binding addresses (that is, different URLs for the Web Service location)
In fact, all three of the load-balancing schemes mentioned here can be used simultaneously if necessary to scale the loadbalancing and reduce failure points in the system.
Where even load balancing is insufficient to provide the necessary throughput to efficiently handle all Web Service requests, priority levels should be added to Web Service requests.
Higher-priority requests should be handled first, leaving lower-priority requests queued until server processing power is available.
There are a number of characteristics of Web Services that suggest that asynchronous messaging may be required to use Web Services optimally.
This means that requests can be dropped, typically for network congestion or server overload.
The client Web Service will get an error in this situation, but nevertheless needs to handle it and retry.
Traffic on the Internet follows a distinct usage pattern and regularly provides better service at certain times.
Web Service usage is likely to follow this pattern, as times of peak congestion are also likely to be peak Web Service usage (unless your service is targeted at an off-peak activity)
This means that at peak times the average Web Service gets a double hit of a congested network and a higher number of requests reaching the service.
Many client/server projects over the years have shown that if your application can put up with increased latency, asynchronous messaging maximizes the throughput of the system.
Requiring synchronous processing over the Internet is a heavy overhead.
Consider that synchronous calls are most likely to fail from congestion when other synchronous calls are also failing.
The response for a synchronous protocol, such as TCP, is simply to send more attempts to complete the synchronous call.
The repeated attempts only increase congestion, as they occur in addition to all the new synchronous calls that are now starting up.
Consequently, supporting asynchronous requests, especially for large, complicated services, is a good design option.
You can do this using an underlying messaging protocol, such as JMS, or independently of the transport protocol using the design of the Web Service.
The latter option means that you need to provide an interface that accepts requests and stores the results of processing the request for later retrieval by the client.
Similarly, the client of the Web Service should strive to use an asynchronous model where possible.
Finally, some Web Services combine other Web Services in some value-added way to provide what are called aggregation services.
Aggregation services should try to retrieve the data they require from other services during off-peak hours in large, coarse-grained requests.
Cache data and objects to change distributed requests to local ones.
Partition the application so that methods execute where their data is held.
Stub out data links to reduce the amount of data required to be transferred.
Design the various components so that they can execute asynchronously from each other.
Split up data so that partial results can be displayed.
Use JDBC optimizations such as prepared statements, specific SQL requests, etc.
Try to break down the time to execute a Web Service into client, server, and network processing times, and extract the marshalling and unmarshalling times from client and server processing.
Don't forget about DNS resolution time for a Web Service.
Try to load-balance high-demand Web Services or provide them asynchronously.
For more scalable and better performing Web Services, create coarser services that require fewer network requests to complete.
When developing an application, it is important to consider performance optimizations and apply them where appropriate in the development cycle.
Forgetting these optimizations (or getting them wrong) can be expensive to correct later in development.[1] In this chapter, we follow the various stages of the full product life cycle and consider when and why you might need to include some performance optimizations.
When I talk about expense, I mean cost in both time and money.
Occasionally, you have the wonderful situation that a change to the application is better in every way: it provides better performance, cleaner code, and a more maintainable product.
But more often, the performance of parts of an application are interrelated.
Tuning one part of the application affects other parts, and not necessarily for the better.
The more complicated the application, the more often this is true.
You should always consider how a particular performance change will affect other parts of the application.
This means that tuning can be a lengthy process simply because it must be iterative.
The full performance-tuning sequence (identifying the bottleneck, tuning, and then benchmarking) is necessary to make sure that tuning one part of the application is not too detrimental to another part.
Performance tuning at the analysis and design phases differs from performance tuning at the implementation phase.
Designing-in a performance problem usually results in a lot of trouble later on, requiring a large effort to correct.
On the other hand, coding that results in poor performance simply requires a tuning phase to eliminate bottlenecks and is much simpler (and cheaper) to correct.
As a rule of thumb, a performance problem created (or left uncorrected) in one phase requires roughly five times as much effort to correct in the following development phase.
Leaving the problem uncorrected means that the effort required to correct it snowballs, growing fivefold through each development phase (planning, analysis, schematic design, technical design, construction, deployment, production).[2]
Studies of the costs of fixing uncorrected problems have found that some phases have a higher cost than others.
Before discussing when to optimize, I'll start with when you should not optimize.
At the code-writing stage, your emphasis should not be on optimizing: it should be entirely on functionality and producing correct bug-free code.
Apart from optimizations (such as canonicalizing objects) that are good design, you should normally ignore performance while writing code.
Performance tuning should be done after the code is functionally correct.
If testing and documentation are inadequate, most people won't notice or care how fast a particular list box updates.
They'll have given up on the program before they ever got to that window.[3]
This is a nice article about when and why to performance-tune.
But make sure you have planned for a tuning phase.
I am not saying that you should create the whole application without considering performance until just before deployment.
Performance should be considered and planned for at all phases of the development process (especially design and architecture)
You need to rule out designs that lead to a badly performing application.
Optimizations that are good design should be applied as early as possible.
When parts of the application are complete, they should be tuned.
And benchmarks should be run as soon as possible: they give a good idea of where you are and how much effort will be needed for the tuning phase after code writing is mostly complete.
Most code can be categorized into one of two general types:
If this code is reused, it usually provides only a skeleton that needs reimplementing.
This type of code is usually referred to as class libraries, frameworks, components, and beans.
I refer to all of these together as reusable code.
You can run the application as it is intended to be used, determine any bottlenecks, and successively tune away those bottlenecks.
The second type of code, reusable code, is much more difficult to tune.
This code may be used in many situations that could never be foreseen by the developers.
Without knowing how the code will be used, it is very difficult to create tests that appropriately determine the performance of reusable code.
There is no truly valid technique that can be applied.
Even exhaustively testing every method is of little use (not to mention generally impractical), since you almost certainly cannot identify useful performance targets for every method.
I have not seen any studies that show this cost.
Instead, I base it on my own impression from examining early versions of various class libraries and comparing these classes with later versions.
I find that most methods in a random selection of classes are altered in some way that I can identify as giving performance improvements.
The standard way to tune reusable code is to tune in response to identified problems.
Usually the development team releases alpha and beta versions to successively larger groups of testers: other team developers, demo application developers, the quality-assurance team, identified beta testers, general beta testers, and customers of the first released version (some of these groups may overlap)
Each of these groups provides feedback in identifying both bugs and performance problems.
In fact, as we all know, this feedback process continues throughout the lifetime of any reusable code.
But the majority of bugs and performance problems are identified by this initial list of users.
This reactive process is hardly ideal, but any alternative makes tuning reusable code very expensive.
This is unlike bug testing, in which the quality of the test suite and quality-assessment process makes a big difference to the reliability of the released version, and is fully cost-effective.
First, from the viewpoint of the developer using reusable components, you need to be aware that first versions frequently have suboptimal performance.
Note that this does not imply anything about the quality of the software: it may have wonderfully comprehensive features and be delightfully bug-free.
But even in a large beta testing program with plenty of feedback, there is unlikely to be sufficient time to tune the software and repeat the test and release procedures.
Getting rid of identified bugs rightfully takes precedence, and developers normally focus on the (next) released version being as bug-free as possible.
Second, for developers creating reusable code, the variety of applications testing the reusable code is more important than the absolute number of those applications.
Ten people telling you that method X is slow is not as useful as two telling you that method X is slow and two telling you that method Y is slow.
Finally, perhaps the most significant way to create reusable code that performs well is for developers to be well-versed in performance tuning.
After any significant amount of performance tuning, many of the techniques in this book can become second nature.
Developers experienced in performance tuning can produce reusable code that is further along the performance curve right from the first cut.
Writing reusable code is one of the few situations in which it is sometimes preferable to consider performance when first writing the code.
The analysis phase of development encompasses a variety of activities that determine what functionality you are going to build into your application.
Identifying major functions and business areas (e.g., compression, display; targeted to the area of graphics files)
Deciding whether to build or buy (e.g., are there available beans or classes to handle compression and display? How much are they? How much will building our own cost? Do the purchasable ones provide all essential features?)
The analysis phase does not usually specify either the structure of the application or the technology (e.g., you might specify that the application uses a database, but probably not which database or even which type of database)
The analysis phase specifies what the application will do (and might do), not how it is done, except in the most general terms.
Determining general characteristics of objects, data, and users (e.g., number of objects in the application)
Specifying expected or acceptable performance boundaries (e.g., functionality X should take less than M seconds)
Identifying probable performance limitations from the determined specifications (e.g., function Y is an HTTP connection, and so is dependent on the quality of the network path and the availability of the server)
Eliminating any performance conflicts by extending, altering, or restating the specifications (e.g., the specification states query Z must always respond within N seconds, but this cannot be guaranteed without altering the specification to provide an alternative default result)
Performance goals should be an explicit part of the analysis and should form part of the specification.
The analysis phase should include time to analyze the performance impacts of the requirements.
The general characteristics of the application can be determined by asking the following questions about the application:
How many objects will there be, and what are their sizes (average and distribution)? What is the total.
How many simultaneous users will use the application, and what level of concurrency is expected for those simultaneous users? (Are they accessing the same resources, and if so, how many resources and what type of access?)
What is the expected distribution of the application? This is, of course, mainly relevant for distributed applications.
This applies back to the last point, but focuses on the distributed resources that are necessarily used simultaneously.
You can use the answers to these questions to provide an abstract model of the application.
Applying this abstract model to a generalized computer architecture allows you to identify any performance problems.
For example, if the application is a multiplayer game to be played across a network, a simple model of a network together with the objects (numbers and sizes) that need to be distributed, the number of users and their expected distributions, and possible patterns of play provide the information you need to identify whether the specified application can run over the network.
If, after including safety factors, the network can easily cope with the traffic, that section of the application is validated.
This type of analysis is part of software performance engineering.
This is a scientific technique referred to as "successive approximation by the application of empirically derived data." Another name for it is "educated guessing."
One of the most significant aspects to examine at the analysis phase is the expected performance gains and drawbacks of distributed computing.
Good design usually emphasizes decoupling components, but good performance often requires close coupling.
These are not always conflicting requirements, but you do need to bear in mind this potential conflict.
For distributed applications, distributed components should be coupled in such a way as to minimize the communication between those components.
The goal is to limit the number of messages that need to be sent back and forth between components, as too many network message transfers can have a detrimental effect on performance.
Components engaged in extended conversations over a network spend most of their time sitting idle, waiting for responses.
For this type of situation, the network latency tends to dominate the performance.
A simple example, showing the huge difference that distribution performance can make to even a standalone applet, indicates how important this aspect is.
You might have thought that a standalone applet does not need much analysis of its distributed components.
Table 13-1 shows two development paths that might be followed and illustrates how ignoring performance at the analysis stage can lead to performance problems later.
Distribution analysis: Applet is distributed using a compressed JAR file.
Applet2 is distributed using one or more compressed JAR files.
Because the download time may be significant for the expected number of classes, the analysis indicates that the applet should be engineered from the ground up, with minimizing download time as a high priority.
To this end, the specification is altered to state that a small entry point functionality of the applet, with a small isolated set of classes, will be downloaded initially to allow the applet to start as quickly as possible.
This initial functionality should be designed to engage the user while the remainder of the applet is downloaded to the browser in the background.
The applet could be downloaded in several sections, if necessary, to ensure the user's waiting time is kept to a minimum.
A secondary priority is for the user to have no further explicit download waiting time.
Applet2 design: Requires careful thought about which classes require the presence of other classes.
Applet1 performance testing: Applet takes far too long to download.
User testing indicates that 99% of users abandon the web page before download is complete and the applet can start.
Unpacking the JAR file and having classes download individually makes the situation even worse.
Project may be terminated, or a major (and costly) rewrite of the applet design may be undertaken to allow the applet to start faster at the user's desktop.
Applet2 performance testing: Applet downloads and starts in adequate time.
The analysis on the right saves a huge amount on development costs.
Of course, if not identified at the analysis phase, this aspect of performance may be picked up later in some other phase of development, but the further away from the analysis phase it is identified, the more expensive it is to correct.
Another consideration at the analysis stage is the number of features being specified.
Sometimes "nice to have" features are thrown into the requirements at the analysis phase.
Features seem to have an inverse relationship to performance: the more features there are, the worse the performance or the more effort is required to improve the performance.
For good performance, it is always better to minimize the features in the requirements or, at the very least, to specify that the design should be extensible to incorporate certain nice-to-have features rather than to simply go ahead and include the features in the requirements.
One other important aspect that you should focus on during the analysis phase is the application's use of shared resources.
Try to identify all shared resources and the performance costs associated with forcing unique access of shared resources.
When the performance cost is shown to be excessive, you need to specify.
For example, if several parts of the application may be simultaneously updating a file, then to avoid corruption, the updates may need to be synchronized.
If this potentially locks parts of the application for too long, an alternative, such as journaling, might be specified.
Journaling allows the different parts of the application to update separate dedicated log files, and these logs are reconciled by another asynchronous part of the application.
These include how long a transaction will be, how often data or objects need to be updated, where objects will be located, whether they are persistent and how persistency is achieved, how data is manipulated, how components interact, and how tightly coupled subsystems are, as well as determining responses to errors, retry frequencies, and alternative routes for solving tasks.
As I mentioned in the last section, the general technique for performance tuning during the analysis and design phases is to predict performance based on the best available data.[6] During the design phase, a great deal of prototype testing is possible, and all such tests should feed data back to help predict the performance of the application.
Any predictions indicating a problem with performance should be addressed at the design phase, prior to coding.
If necessary, it is better to revisit the analysis and alter specifications rather than leave any indicated performance issues unresolved.
At each stage, part of the design objective should be to predict the performance of the application.
Note that when I refer to the design phase, I include both logical and physical design; physical design is often called architecture.
The design phase usually includes determining the target platforms, and any predictions must be tailored to the limitations of those platforms.
This is especially important for embedded Java systems (e.g., applets and servlets), environments where a specific nonstandard target VM must be used, and where the target VM may be highly variable (i.e., is unknown)
In all these cases, the target Java runtime system performance cannot be inferred from using the latest standard VM, and performance prediction must be targeted at the known system or at the worst-performing Java runtime system.
Alternatively, the design phase may rule out some runtime systems as being unsupported by the application.
Any decoupling, indirection, abstraction, or extra layers in the design are highly likely to be candidates for causing performance problems.
You should include all these elements in your design if they are called for.
But you need to be careful to design using interfaces in such a way that the concrete implementation allows any possible performance optimizations to be incorporated.
Design elements that block, copy, queue, or distribute also frequently cause performance problems.
These elements can be difficult to optimize, and the design should focus attention on them and ensure that they can either be replaced or that their performance is targeted.[7] Asynchronous and background events can affect times unpredictably, and their effects need to be clearly identified by benchmark testing.
For example, in Chapter 10, we considered a load-balancing solution that included a queue.
The queue is a potential bottleneck, and care must be taken to ensure that the queue does not unnecessarily delay requests as they pass through.
Resources that must be shared by several users, processes, or threads are always a potential source of bottlenecks.
When a resource is shared, the resource usually requires its various sharers to use it one at a time to avoid a conflict of states and corruption.
During the design phase, you should try to identify all shared resources, and predict what performance limitations they impose on the application.
Be careful to consider the fully scaled version of the application, i.e., with as many users, objects, files, network connections, etc., as are possible according to the application specifications.
Considering fully scaled versions of the application is important because shared resources are highly nonlinear in performance.
They usually impose a gently decreasing performance at their bottleneck as the number of sharers increases, up to a point at which there is a sudden and catastrophic decrease in performance as the number of sharers increases further.
If the performance prediction indicates that a particular shared resource is likely to impose too high a performance cost, alternative designs that bypass or reduce the performance cost of that shared resource need to be considered.
For example, multiple processes or threads updating a shared collection have to synchronize their updates to avoid corrupting the collection.
If this synchronized update is identified as a performance problem, an alternative is to allow each process or thread to update its own collection, and wrap the collections in another collection object that provides global access to all the collections.
Failing to identify a shared resource at the design phase can be expensive.
In some cases, a simple class substitution of a redesigned class can reduce the performance drawback of the shared resource to acceptable performance levels.
But in many cases, a complete redesign of part or all of the application may be needed to achieve adequate performance.
The purpose of a transaction is to ensure consistency when using shared resources.
If there are no possible conflicts across sharers of those resources, there is no need for a transaction.
Removing unnecessary transactions is the simplest and most effective performance optimization for applications that include transactions.
So, if you do not need a transaction, do not use one.
Most systems that provide transactions usually have a "transactionless" mode, i.e., a way to access any shared resources without entering a defined transaction.
This mode normally has better performance than the transaction mode.
When transactions are absolutely necessary, your design goal should be to minimize the time spent in the transaction.
If transactions extend for too long and cause performance problems, a complete redesign of a significant part of the application is often needed.
You also need to be aware of the shared resources used by the transacting system itself.
Any system providing transaction semantics to your application uses an internal set of shared resources: this is necessary to ensure that the transactions are mutually consistent.
These transaction-system internal shared resources invariably have some product-specific idiosyncrasies that result in their being used more or less efficiently.
Many products have a performance-tuning section within their documentation, detailing how best to take advantage of idiosyncrasies in their product (more usually termed "features")
Even where short transactions are designed into an application, the application may enter unexpectedly long transactions in two common situations.
The first situation is when bugs occur in the transaction, and the second, when the user has control over the transaction.
Because unintended long transactions can occur, transactions should always have a timeout imposed on them: this is usually fairly easy to incorporate in Java using a separate high-priority timeout thread.
A standard way to convert naturally long transactions into short ones is to maintain sets of changes, rather like undo/redo logs.
In this design pattern, changes are abstracted into separate objects, and ordered lists of these changes can be "played." With this design pattern, the changes that occur in what would be a long transaction are leisurely collected without entering the transaction, and then a short transaction rapidly "plays" all the changes.
This pattern cannot be used exactly as described if the precise time of a particular change is important.
However, variations of this pattern can be applied to many cases.
Locking is a technique for ensuring access to a shared resource while maintaining coherence of state within the shared resource.
For example, one type of write-lock allows read access by multiple sharers to the shared resource while restricting write access to just one sharer.
Overhead includes the locking and unlocking overhead itself; the fact that locks must be shared resources implying extra shared-resource considerations; the explicit serialization of activities that result from using locks; and the possibility of deadlock when two sharers are simultaneously trying to obtain a lock held by the sharer, causing both sharers to "freeze" activity (see Chapter 10 for a concrete example)
These drawbacks mean you should consider locking only when the design absolutely requires it.
For instance, locking must be used when there is a requirement for definite deterministic noncorrupted access to a shared resource.
To illustrate: a bank account with no overdraft facilities must serialize access to the account and ensure that each deposit and withdrawal takes place without any other activities legally occurring at the same time.
The balance accessed for display also needs to be accurate.
From the bank's point of view, both transactions might go though, and the bank is owed $5 from someone it did not want to lend to.
Or the customer is given the wrong information and suffers frustration.
In fact, the setInUse( ) method probably needs to be synchronized, so this pattern is useful only for avoiding synchronizing methods that might take a long time.
The long synchronization is replaced by a short synchronization and a possible thrown exception.
For performance reasons, you should try to design parallelism into the application wherever possible.
The general guideline is to assume that you parallelize every activity.
It is always easy to move from a parallelized design back to the nonparallelized version, since the nonparallelized version is essentially a degenerate case of the more general version.
But retrofitting parallelism to the application is often considerably more difficult.
Starting with an application designed to work without any parallelism and trying to restructure it to add in parallelism can be extremely difficult and expensive.
Any parallelism designed into the application should take advantage of multiple processors.
This can be evaluated in the design phase by predicting the performance of the application on single- and multiple-CPU machines.
Once the application has been designed to run with parallelism, you can decide at the implementation stage, or possibly even at runtime, whether to use the parallelism.
The overhead comes from contention in trying to use shared resources and delays from the communication needed for synchronization.
Additional overhead comes from starting extra threads and distributing and coordinating work between the threads (there may also be overhead from caches that deal with twice the data throughput in the same space)
When designing the application to run activities in parallel, you need to focus on shared resources, especially the time spent using these resources.
Increasing the time spent exclusively using a shared resource adversely affects all other activities using that resource.
For example, the CPU is the most basic shared resource.
The more separate threads (and processes) using the CPU, the smaller the time slices allocated to each thread relative to the time spent waiting to acquire the CPU by each thread.
Consequently, the time actually taken for computing any particular activity becomes longer, since this is the sum of the time slices allocated to carry out the computation together with the sum of times waiting to gain a time slice.
You can see the need to keep spare capacity in the CPU to avoid an exponential degradation in performance.
You can also predict the effect threading can have if you can parallelize any particular calculation, even on a single-CPU machine.
If one thread does the calculation using 10% of the CPU in five seconds, and you can fully parallelize the calculation, then two threads (ideally) each take half the time to do their half of the calculation.
Both calculations run at the same time (that is why the CPU has double the utilization), so this is also the total time taken.
Time-slicing adds some additional overhead, but this will leave the expected time well below the three-second mark.
So even on a single-CPU machine, parallelizing this calculation enables it to run faster.
This can happen because the calculation considered here is not a pure CPU calculation: it obviously spends time doing some I/O (perhaps a database query), and thus it can be parallelized effectively.
If the calculation were number crunching of some sort, the CPU utilization would be 100%, and parallelizing the calculation would actually slow it down.
For example, suppose the number-crunching calculation took five seconds and caused a 100% CPU utilization on an otherwise unworked machine.
So theoretically, if you can parallelize this calculation into two equal halves running together on an otherwise unutilized machine, each half is allocated 50% of the CPU utilization.
So the total time taken is still five seconds, and there is no overall speedup.
If you add in the slight factor due to CPU time-slicing overhead, the total time increases beyond the five-second mark, so it is actually slower to parallelize this calculation.
This is what we should intuitively expect for any process that already takes up all the CPU's power.
Now what about the multiple CPU case: do we get a benefit here? Well, for a two-CPU machine, the CPU synchronization overhead may be 5% (this is normally an overestimate)
In this case, each part of the parallelized application effectively gets a 5% utilized CPU of its own.
And since the two threads are running in parallel, this is also the total expected time taken.
In the case of the number-crunching calculation, you have the exact same calculation resulting in 2.63 seconds.
So again, as you intuitively expect, the two-CPU machine lets the CPU-swamping parallelized calculation take just over half the time of the original unparallelized version.
Theoretical computation time depending on number of CPUs and non-CPU-bound threads.
However, CPU overhead is increased for each additional CPU, as they all have to synchronize with each other.
Theoretical computation time depending on number of CPUs and threads.
In fact, at some point, adding CPUs actually makes performance worse.
For example, let's suppose our number-crunching application is fully parallelizable to any number of CPUs, and that on a single unutilized CPU it takes 100 seconds.
In fact, for this particular sequence, 20 CPUs gives the minimum time.
Beyond that, the overhead of parallelizing CPUs makes things successively worse, and each additional CPU makes the fully parallelized calculation take longer.
A 10% reduction in time does not justify the cost of an extra CPU.
The general calculation presented here applies to other shared resources too.
In a similar way, you can determine the performance effects of adding other additional shared resources to the application and predict whether the advantages will outweigh the disadvantages.
Note that these are all general predictions, useful for estimating the benefits of adding shared resources.
The cutoff where adding a shared resource gives a useful speedup is usually quite small, so you can mostly assume that a little parallelizing is good, but a lot of parallelizing is too much of a good thing.
All the calculations we made in this section assumed full load-balancing.
Each thread (sharer) took exactly the same share of time to complete its task, and thus the total time was that of any one sharer since they all operated simultaneously.
If the sharers are unbalanced (as they usually are), the sharer that takes the longest to complete its activity is the one limiting the performance of the system.
And the less balanced the various sharers, the worse the performance.
This is extremely important as the application scales across different workloads.
An unbalanced workload means that one resource is used far more intensively than others.
It also means that all other parallel resources are being underutilized, and that the overused resource is highly likely to be a performance bottleneck in the system.
If you have a large amount of data that needs to reside on disk, a typical strategy for improving access and searches of the data is to split up the data among many different files (preferably on separate disks)
Partitioning the data provides support for parallel access to the data, which takes advantage of I/O and CPU parallelism.
Separates the data into logically distinct datasets and allocates each dataset to a separate file/disk.
Places data in multiple files/disks with location based on a hash function.
Uses a logical expression to determine the mapping of data to file/disk.
Although your design does not need to support a specific partitioning scheme, it should support partitioning in general if it is relevant to your application.
The performance characteristics of an application vary with the number of different factors the application can deal with.
These variable factors can include the number of users, the amount of data dealt with, the number of objects used by the application, etc.
During the design phase, whenever considering performance, you should consider how the performance scales as the load on the application varies.
It is usually not possible to predict (or measure) the performance for all possible variations of these factors.
But you should select several representative sets of values for the factors, and predict (and measure) performance for these sets.
Has a varying load predicted to represent normal operating conditions.
Has spiked loads (where the load is mostly "normal" but occasionally spikes to the maximum supported)
Consistently has the maximum load the application was designed to support.
You need to ensure that your scaling conditions include variations in threads, objects, and users, and variations in network conditions if appropriate.
Measure response times and throughput for the various different scenarios and decide whether any particular situation needs optimizing for throughput of the system.
It is clear that many extra factors need to be taken into account during scaling.
The tools you have for profiling scaling behavior are fairly basic: essentially, only graphs of response times or throughput against scaled parameters.
It is typical to have a point at which the application starts to have bad scaling behavior: the knee or elbow in the response-time curve.
At that point, the application has probably reached some serious resource conflict that requires tuning so that "nice" scaling behavior can be extended further.
Clearly, tuning for scaling behavior is likely to be a long process, but you cannot shortcut this process if you want to be certain your application scales.[12]
By including timer-based delays in the application code, at least one multiuser application has deliberately slowed response times for low-scaled situations.
The artificial delay is reduced or cut out at higher scaling values.
The users perceive a system with a similar response time under most loads.
The essential design points for ensuring good performance of distributed applications are:
Decoupling process activities from each other in such a way that no process is forced to wait for others (using queues achieves this)
Determining the bottleneck in a distributed application requires looking at the throughput of every component:
Tuning any component other than the current bottleneck gives no improvement.
You need to assume average rates of performance from the underlying resource and expect performance based on those average rates.
So when performance is bad, the application tends to slow significantly more than in nondistributed applications.
The distributed design aspects should emphasize asynchronous and concurrent operations.
Activities that can be configured at runtime to run in different locations.
The key to good performance in a distributed application is to minimize the amount of communication necessary.
Performance problems tend to be caused by too many messages flying back and forth between distributed components.
Bell's rule of networking applies: "Money can buy bandwidth, but latency is forever."[13]
Unfortunately, communication overhead can be incurred by many different parts of a distributed application.
Allow the application to be partitioned according to the data and processing power.
Any particular task should be able to run in several locations, and the location that provides the best performance should be chosen at runtime.
Usually the best location for the task is where the data required for the task is stored, as transferring data tends to be a significant overhead.
Distributed garbage collection can be a severe overhead on any distributed application.
Reduce the costs of keeping data synchronized by minimizing the duplication of data.
This conflicts directly with the last point, so the two techniques must be balanced to find the optimal data duplication points.
Use compression to reduce the time taken to transfer large amounts of data.
My advice for object design is to use interfaces and interface-like patterns throughout the code.
Although there are slightly higher runtime costs from using interfaces, that cost is well outweighed by the benefits of being able to easily replace one object implementation with another.
Using interfaces means you can design with the option to replace any class or component with a faster one.
Consider also where the design requires comparison by identity or by equality and where these choices can be made at implementation time.
Those JDK classes and other third-party classes that do not have interface definitions should be wrapped by your own classes so that their use can be made more generic.
Applications that need to minimize download time, such as applets, may need to avoid the extra overhead that wrapping causes.
Object creation is one significant place where interfaces fall down, since interfaces do not support constructor declarations, and constructors cannot return an object of a different class.
To handle object creation in a way similar to interfaces, you should use the factory pattern.
The factory design pattern recommends that object creation be centralized in a particular factory method.
So rather than calling new Something( ) when you want to create an instance of the Something class, you call a method such as.
Again, this pattern has performance costs, as there is the overhead of an extra method call for every object creation, but the pattern provides more flexibility when it comes to tuning.
Design for reusable objects: do not unnecessarily throw away objects.
The factory design pattern can help, as it supports the recycling of objects.
Keep in mind that stateless objects can usually be safely shared, so try to move to stateless objects where appropriate.
Using stateless objects is a good way to support changing algorithms easily by implementing different algorithms in particular types of objects.
For example, see Section 9.2, where different sorting algorithms are implemented in various sorting classes.
The resulting objects can be interchanged whenever the sorting algorithm needs to be varied.
Or, the object could simply store added values and calculate the average and standard deviation each time those statistics are accessed, making the update as fast as possible, but increasing the time for statistics access.
Predicting performance is the mainstay of performance tuning at the analysis and design stages.
Often it is the experience of the designers that steers design one way or another.
Knowing why a particular design element has caused bad performance in another project allows the experienced designer to alter the design in just the right way to get good performance.
Some general guidelines can guide the application designer and avoid bad performance.
In the following sections we consider some of these guidelines.
The design allows the intercomponent communication to be based on a local or remote call, which allows components to be placed very flexibly.
However, the performance of different types of calls varies hugely and helps define whether some designs can perform fast enough.
Specifically, if local procedure calls have an average time overhead of one unit, a local interprocess call incurs an overhead of about 100 units.
Applying these variations to the design and factoring the number of messages that components need to send to each other may rule out some distributed architectures.
Alternatively, the overhead predictions may indicate that a redesign is necessary to reduce the number of intercomponent messages.
Note also that process startup overhead may need to be considered.
For example, Common Gateway Interface (CGI) scripts for HTTP servers typically need to be started for every message sent to the server.
For this type of design, the time taken to start up a script is significant, and when many scripts are started together, this can slow down the server considerably.
Accesses and updates to system memory are always going to be significantly faster than accesses and updates to other memory media.
For example, reads from a local disk can be a thousand times slower than memory access, and disk writes are typically half as fast as disk reads.
Random access of disks is significantly slower than sequential access.
Recognizing these variations may steer your design to alternatives you might otherwise not have considered.
For example, one application server that supports a shared persistent cache redesigned the persistent cache update mechanism to take account of these different update times (the GemStone application server, http://www.gemstone.com)
The original architecture performed transactional updates to objects by writing the changes to the objects on the disk, which required random disk access and updates.
The modified architecture wrote all changes to shared memory as well as to a sequential journaling log file (for crash recovery)
Another asynchronous process handled flushing the changes from shared memory to the objects stored on disk.
Because disk navigation to the various objects was significant, this change in architecture improved performance by completely removing that bottleneck from the transaction.
Ideally, you have a detailed simulation of your application that allows you to predict the performance under any set of conditions.
More usually, you have a vague simulation that has some characteristics similar to your intended application.
It is important to keep striving for the full detailed simulation to be able to predict the performance of the application.
But since your resources are limited, you need to project measurements as close as possible to your target application.
You should try to include loads and delays in your simulation that approximate to the expected load of the application.
Try to acquire the resources your finished application will use, even if those resources are not used in the simulation.
For example, spawn as many threads as you expect the application to use, even if the threads do little more than sleep restlessly.[14]
Sleeping restlessly is calling Thread.sleep( ) in a loop, with the sleep time set to some value that requires many loop iterations before the loop terminates.
Other activities can be run intermittently in the loop to simulate work.
It is worth checking vendor or standard benchmarks if you need some really basic statistics, but bear in mind that those benchmarks seldom have much relevance to a particular application.
Try stripping your design to the bare essentials or going back to the specification.
Consider how to create a special-purpose implementation that handles the specification for a specific set of inputs.
This can give you an estimate of the actual work your application will do.
Now consider your design and look at the overhead added by the design for each piece of functionality.
This provides a good way to focus on the overhead and determine if it is excessive.
Shared resources almost always cause performance problems if they have not been designed to optimize performance.
Ensure that any simulation correctly simulates the sharing of resources, and use prediction analyses such as those in Section 13.4.1.3 earlier in this chapter to predict the behavior of multiple objects using shared resources.
Consider what happens when your design is spread over multiple threads, processes, CPUs, machines, etc.
This analysis can be quite difficult without a simulation and test bed, but it can help to identify whether the design limits the use of parallelism.
Many applications convert data between different types (e.g., between strings and numbers)
From your design, you should be able to determine the frequency and types of data conversions, and it is fairly simple to create small tests that determine the costs of the particular conversions you are using.
Don't forget to include any concurrency or use of shared resources in the tests.
Remember that external transfer of objects or data normally includes some data conversions.
The cost of data conversion may be significant enough to direct you to alter your design.
Some repeated tasks can be processed as a batch instead of one at a time.
Batch processing can take advantage of a number of efficiencies, such as accessing and creating some objects just once, eliminating some tests for shared resources, processing tasks in optimal order, avoiding repeated searches, etc.
If any particular set of tasks could be processed in batch mode, consider the effect this would have on your application and how much faster the processing could be.
The simplest conceptual example is that of adding characters one by one to a StringBuffer, as opposed to using a char array to add all the characters together.
Adding the characters using a char array is much faster for any significant number of characters.
For many applications such as agent applications, services, servlets and servers, multiuser applications, enterprise systems, etc., there needs to be constant monitoring of the application performance after deployment to ensure that no degradation takes place.
This is mainly relevant to enterprise systems that are being administered.
Shrinkwrapped or similar software is normally tuned the same way as before deployment, using standard profiling tools.
Monitoring the application is the primary tuning activity after deployment.
The application should be built with hooks that enable tools to connect to it and gather statistics and response times.
The application should be constantly monitored, and all performance logs retained.
Monitoring should record as many parameters as possible throughout the system, though clearly you want to avoid monitoring so much that the performance of the running application is compromised by a significant amount.
Of course, almost any act of measuring a system affects performance.
But the advantage of having performance logs normally pays off enormously, and a few percent decrease in performance should be acceptable.
Individual records in the performance logs should include at least the following six categories:
A standard set of performance logs should be used to give a background system measurement and kept as a reference.
Periodically, the standard should be regenerated, as most enterprise applications change their performance characteristics over time.
Ideally, the standard logs can be automatically compared against the current logs, and any significant change in behavior is automatically identified and causes an alert to be sent to the administrators.
Trends away from the standard should also trigger a notification; sometimes performance degrades slowly but consistently because of a gradually depleting resource.
Administrators should note every single change to the system: every patch, every upgrade, every configuration change, etc.
These changes are the source of most performance problems in production.
Patches are cheaper short-term fixes than upgrades, but they usually add to the complexity of the application and increase maintenance costs.
Upgrades and rereleases are more expensive in the short term, but cheaper overall.
A user may be wrong, or might have hit a known system problem or temporary administrative shutdown.
Repeat the measurements several times and take averages and variations.
Ensure that caching effects do not skew measurements of a reported problem.
When looking for reasons why performance may have changed, consider any recent changes such as an increase in the number of users, other applications added to the system, code changes on the client or server, hardware changes, etc.
In addition to user response time measurements, look at where the distributed code is executing, what volumes of data are being used, and where the code is spending most of its time.
Many factors can easily give misleading or temporarily different measurements to the application.
Distributed garbage collection may have cut in, system clocks may become unsynchronized, background processes may be triggered, and relative processor power may change, causing obscure effects.
Consider if anyone else is using the processors, and if so, what they are doing and why.
General slowness, perhaps reflecting that the application was not tuned for the current load, or that the systems or networks are saturated.
A sudden slowdown that continues, often the result of a change to the system.
Each of these characteristic changes in performance indicates a different set of problems.
The following sections discuss some aspects of the application that may not immediately strike you as part of the performance of the application.
But they do affect the user's perception of the application performance, and so are relevant.
The application's user interface has a significant effect on the user's perception of performance.
The time required to navigate through the user interface to execute some functionality is seen by the user as part of the application's response time.
If window and menu navigation is difficult, performance is seen to be bad (and, actually, it is bad)
The user interface should support the natural flow of the user's activity; otherwise, you are forcing the user to perform less efficiently.
Improving only the navigability of the user interface, even with no other changes to the application, improves the perceived performance of an application.
Training users to use the application is also a performance issue.
Without proper training, users may not use the application efficiently and may compare the application unfavorably with another application they are comfortable with.
Since they are comparing similar functionality, the user immediately focuses on the differences.
He simply feels that executing some function in your application takes forever, as he stumbles through menu options trying to find what he wants, fills in forms incorrectly, etc.
Note that making help desks available is an essential part of the training program.
Training is seldom so thorough that all parts of the application are covered in enough detail, and it is also common for people to forget some of their training.
A help desk keeps the users from getting lost and giving up on the most efficient route to solve their tasks.
If people can't start a piece of software, they can get frustrated, but they don't normally view this as bad performance.
Most people would instead get annoyed at the quality of the software.
But when servers are not running, this can be perceived differently.
Sometimes a server that isn't running is perceived as bad-quality software, but sometimes it is seen as poor performance.
If the server stops responding in the middle of processing, this is invariably seen as slow performance.
Consider your own response to a stalled download from an HTTP server.
Avoiding server downtime is a robustness issue as well as a performance issue.
Servers should be designed to avoid unnecessary downtime and minimize necessary downtime.
One issue when running servers is altering their configuration and patching them.
If you need to stop a server from running while you make changes, this affects its perceived performance.
You must signal it to stop accepting requests and either wait for current requests to terminate or forcibly terminate all requests; either way, this causes interruptions in service to the clients.[15]
Load-balancing products often provide features that allow server maintenance with minimum downtime.
It is possible to design servers that can be reconfigured and patched with no downtime.
One common solution is for the server to periodically test the timestamp of the configuration file and reread it if the timestamp changes.
This solution also provides good security (as presumably, the configuration file can be changed only by authorized persons)
Another solution is for the server to recognize and accept a particular signal as a sign to reset its configuration.
In most servers using this solution, the signal is an operating-system signal that is trapped.
However, Java does not support operating-system signal handling, so if you steer down this path, you need either to install operating-system handlers yourself (using the Java native interface) or use another communication mechanism, such as sockets.
If you do use sockets, you need to consider security aspects; you don't want unauthorized persons triggering a server reconfiguration.
You need to provide some level of indirection on how the requestprocessing classes (and all the classes they depend on) are loaded.
The most basic solution is to use the configuration file to list names of all classes.
Then the server must be built using Class.forName( ) to access and create any classes and instances.
This way, providing a new version requires only changing the class names in the configuration (in an atomic way to avoid corruption)
This chapter has described how to factor in performance at various stages of development.
Integrating this advice allows you to create a performance plan, as outlined in this section.
During the specification stage, the performance requirements of the application need to be defined.
Your customers or business experts need to establish what response time is acceptable for most functions the user will execute.
It may be more useful to start by specifying what response times will be unacceptable.
This task can be undertaken at a later stage of development.
In fact, it can be simpler, if a prototype has already been created, to use the prototype and other business information in specifying acceptable responses.
If code tuning starts without performance requirements, then goals are inadequately defined, and tuning effort will be wasted on parts of the application that do not require tuning.
If your development environment is layered (e.g., application layer, component layer, technical architecture layer), try to define performance specifications that map to each layer, so that each team has its own set of performance targets to work on.
If this is not possible, the performance experts will need to be able to tune across all layers and interact with all teams.
During the analysis stage, the main performance focus is to analyze the requirements for shared and limited resources in the application (e.g., a network connection is both a shared and a limited resource; a database table is a shared resource; threads are a limited resource)
These are the resources that will cost the most to fix later in development if they are not identified and designed correctly at the outset.
Analysis of data volume and load-carrying capacities of the components of the system should also be carried out to determine the limitations of the system.
This task should fit in comfortably as part of the normal analysis stage.
To be on the safe side, or to highlight the requirement for performance analysis, you may wish to allocate 10% of planned analysis time for performance analysis in this phase.
The analysis team must be aware of the performance impact of different design choices so that they do not miss aspects of the system that need analysis (see the earlier Section 13.3)
The analysis should be made in association with the technical architecture analysis so that you end up with an architectural blueprint that clearly identifies performance aspects.
Progressing from the analysis stage, the performance focus in the design phase should be on how shared resources will be used by the application and on the performance consequences of the expected physical architecture of the deployed application.
Ensure that the designers are aware of the performance consequences of different decisions by asking for performance-impact predictions to be included with the normal design aspects.
The external design review should either include design experts familiar with the performance aspects of design choices, or a secondary performance expert familiar with design should review the application design.
A 10% budget allocation for performance planning and testing highlights the emphasis on performance.
The design should include reference to scalability both for users and for data/object volumes, the amount of distribution possible for the application depending on the required level of messaging between distributed components, and the transaction mechanisms and modes (pessimistic, optimistic, required locks, durations of transactions and locks held)
The theoretical limitation to the performance of many multiuser applications is the amount and duration of locks held on shared resources.
The designers should also include a section on handling queries against large datasets, if that will be significant for your application.
Specify benchmark functions and required response times based on the specification.
Ensure that a reasonably accurate test environment for the system is available.
Buy or build various performance tools for your performance experts to evaluate, including profiling tools, monitoring tools, benchmark harnesses, web loading, GUI capture/playback, or other client emulation tools.
Note that this is not QA: the tests should not be testing failure modes of the system, only normal, expected activity.
This is normally system-specific and usually evolves as the testing proceeds.
Plan for code versioning and release from your development environment to your performance environment, according to your performance test plan.
Note that this often requires a round of bugfixing to properly run the tests, and time restrictions usually mean that it is not possible to wait for the full QA release, so plan for some developer support.
Create a simulation of the system that faithfully represents the main components of the application.
The simulation should be implemented so that you can test the scalability of the system and determine how shared resources respond to increased loads and at what stage limited resources start to become exhausted or bottlenecked.
The simulation should allow finished components to be integrated as they become available.
If budget resources are unavailable, skip the initial simulation, but start testing as soon as sufficient components become available to implement a skeleton version of the system.
The targets are to determine response times and scalability of the system for design validation feedback as.
If you have a "Proof of Concept" stage planned, it could provide the simulation or a good basis for the simulation.
Ideally, the validation would take place as part of the "Proof of Concept."
This logging should be deployed with the released application (see Step 8), so performance logging should be designed to be low-impact.
Performance logging should be added to all the layer boundaries: servlet I/O and marshalling; JVM server I/O and marshalling; database access/update; transaction boundaries; and so on.
Performance logging should not produce more than one line of output to a log file per 20 seconds.
It should be designed so that it adds less than 1% of time to all application activity.
Logging should be configurable to aggregate variable amounts of statistics so that it can be deployed to produce one summary log line per configurable time unit (e.g., one summary line every minute)
Ideally, logging should be designed so that the output can be analyzed in a spreadsheet, allowing for effective and easy-to-read aggregation results.
During code implementation, unit performance testing should be scheduled along with QA.
No unit performance tuning is required until the unit is ready for QA.
Unit performance tuning proceeds by integrating the unit into the system simulation and running scaling tests with profiling.
It is important to test the full system or a simulation of it as soon as is feasible, even if many of the units are incomplete.
Simulated units are perfectly okay at an early stage of system performance testing.
Initially, the purpose of this system performance test is to validate the design and architecture and identify any parts of the design or implementation that will not scale.
Later, the tests should provide detailed logs and profiles that will allow developers to target bottlenecks in the system and produce faster versions of the application.
To support the later-stage performance testing, the test bed should be configured to provide performance profiles of any JVM processes, including system and network statistics, in addition to performance logging.
Your performance experts should be able to produce JVM profiles and obtain and analyze statistics from your target system.
The performance tests should scale to higher loads of users and data.
Twice the peak expected throughput, together with the peak expected data volume and the peak expected users.
Twice the peak expected data volume, together with the peak expected throughput and the peak expected users.
Twice the peak expected users, together with the peak expected data volume and the peak expected throughput.
User activity should be simulated as accurately as possible, but it is most important that data is simulated to produce the expected real data variety; otherwise, cache activity can produce completely misleading results.
The numbers of objects should be scaled to reasonable amounts: this is especially important for query testing and batch updates.
Do not underestimate the complexity of creating large amounts of realistic data for scalability testing.
Such logging provides remote analysis and constant monitoring capabilities for the deployed application.
Ideally, you should develop tools that automatically analyze the performance logs.
At minimum, the performance-log analysis tools should generate summaries of the logs, compare performance against a set of reference logs, and highlight anomalies.
Two other useful tools identify long-term trends in the performance logs and generate alerts when particular performance measurements exceed defined ranges.
Create a performance plan that anticipates all the various performance issues that regularly crop up.
Leave code tuning until after the code is functional and debugged.
Consider how a particular performance change will affect other parts of the application.
Consider how the performance scales as the application load varies.
Determine the general characteristics of the application in the analysis and design phases.
Consider the numbers, sizes, and sources of objects, data, and other parameters of the application.
Create an abstract model of the application to identify any performance problems.
Design applets to engage the user as soon as possible.
Identify and focus on the performance costs of shared resources.
Target decoupling, indirection, abstraction, and extra layers in the design.
Predict the performance of design elements that block, copy, queue, or distribute.
Unbalanced parallel activities may limit the performance of the system.
Split up the data among many different files (preferably on separate disks)
Decouple activities so that no activity is unnecessarily blocked by another activity.
Design more flexible method entry points to your classes to provide greater performance flexibility when developing reusable code.
Partition distributed applications according to the data and processing power of components.
Design objects so that they can be easily replaced by a faster implementation.
Consider whether to optimize objects for update or for access.
Minimize the number and size of developed classes for applications that need to minimize download time.
Listen to the application users, but double-check any reported problems.
Ensure that caching effects do not skew the measurements of a reported problem.
Implement a performance plan as an integral part of application design and development.
If you control the operating system and hardware where the application will be deployed, there are a number of changes you can make to improve performance.
This chapter applies to most server systems running Java applications, including servlets, where you usually specify (or have specified to you) the underlying system, and where you have some control over tuning the system.
Client and standalone Java programs are likely to benefit from this chapter only if you have some degree of control over the target system, but some tips in the chapter apply to all Java programs.
I don't cover operating-system and hardware tuning in any great detail, though I give basic tips on monitoring the system.
More detailed information on Unix systems can be obtained from the excellent System Performance Tuning by Mike Loukides (O'Reilly)
Note that Macintoshes running OS X should include the Unix tools I mention in this chapter.
It is usually best to target the operating system and hardware as a last tuning choice.
Tuning the application itself generally provides far more significant speedups than tuning the systems on which the application is running.
Application tuning also tends to be easier (though buying more powerful hardware components is easier still and a valid choice for tuning)
However, application and system tuning are actually complementary activities, so you can get speedups from tuning both the system and the application if you have the skills and resources.
Constantly monitor the entire system with any monitoring tools available and keep records.
This allows you to get a background usage pattern and also lets you compare the current situation with situations previously considered stable.
This ensures that there is no extra load on the system when the users are executing online tasks, and enhances performance of both online and offline activities.
If you need to run extra tasks during the day, try to slot them into times with low user activity.
You should be able to determine the user-activity cycles appropriate to your system by examining the results of normal monitoring.
The reduced conflict for system resources during periods of low activity improves performance.
You should specify timeouts for all processes under the control of your application (and others on the system, if possible) and terminate processes that have passed their timeout value.
Apply any partitioning available from the system to allocate determinate resources to your application.
For example, you can specify disk partitions, memory segments, and even CPUs to be allocated to particular processes.
In most cases, applications can be tuned so that disk I/O does not cause any serious performance problems.
But if, after application tuning, you find that disk I/O is still causing a performance problem, your best bet may be to upgrade the system disks.
Identifying whether the system has a problem with disk utilization is the first step.
At minimum, you need to identify whether paging is an issue (look at disk-scan rates) and assess the overall utilization of your disks (e.g., performance monitor on Windows, output from iostat -D on Unix)
It may be that the system has a problem independent of your application (e.g., unbalanced disks), and correcting this problem may resolve the performance issue.
If the disk analysis does not identify an obvious system problem that is causing the I/O overhead, you could try making a disk upgrade or a reconfiguration.
This type of tuning can consist of any of the following:
Changing the disks to be striped (where files are striped across several disks, thus providing parallel I/O, e.g., with a RAID system)
Running the data on raw partitions when this is shown to be faster.
Distributing simultaneously accessed files across multiple disks to gain parallel I/O.
Using memory-mapped disks or files (see Section 14.1.3 later in this chapter)
If you have applications that run on many systems and you do not know the specification of the target system, bear in mind that you can never be sure that any particular disk is local to the user.
There is a significant possibility that the disk being used by the application is a network-mounted disk.
The weakest link, whether it is the network or the disk, is the limiting factor in this case.
And this weakest link will probably not even be constant.
A network disk is a shared resource, as is the network itself, so performance is hugely and unpredictably affected by other users and network load.
Do not underestimate the impact of disk writes on the system as a whole.
For example, all database vendors strongly recommend that the system swap files[1] be placed on a separate disk from their databases.
The impact of not doing so can decrease database throughput (and system activity) by an order of magnitude.
This performance decrease comes from not splitting the I/O of two disk-intensive applications (in this case, OS paging and database I/O)
The disk files for the virtual memory of the operating system; see the later section Section 14.3
Identifying that there is an I/O problem is usually fairly easy.
The most basic symptom is that things take longer than expected, while at the same time the CPU is not at all heavily worked.
The disk-monitoring utilities will also tell you that there is a lot of work being done to the disks.
At the system level, you should determine the average and peak requirements on the disks.
Your disks will have some statistics that are supplied by the vendor, including:
The average and peak transfer rates, normally in megabytes (MB) per second, e.g., 5MB/sec.
This is the time required for the disk head to move radially to the correct location on the disk.
Rotational speed, normally in revolutions per minute (rpm), e.g., 7200 rpm.
From this, you can calculate the average rotational delay in moving the disk under the disk-head reader, i.e., the time taken for half a revolution.
So half a revolution takes just over 4 ms, which is consequently the average rotational delay.
This list allows you to calculate the actual time it takes to load a random 8K page from the disk, this being seek time + rotational delay + transfer time.
This calculation gives you a worst-case scenario for the disk-transfer rates for your application, allowing you to determine if the system is up to the required performance.
Note that if you are reading data stored sequentially on disk (as when reading a large file), the seek time and rotational delay are incurred less than once per 8K page loaded.
Basically, these two times are incurred only at the beginning of opening the file and whenever the file is fragmented.
But this calculation is confounded by other processes also executing I/O to the disk at the same time.
This overhead is part of the reason why swap and other intensive I/O files should not be put on the same disk.
One mechanism for speeding up disk I/O is to stripe disks.
Disk striping allows data from a particular file to be spread over several disks.
Striping allows reads and writes to be performed in parallel across the disks without requiring any application changes.
However, be aware that the seek and rotational overhead previously listed still applies, and if you are making many small random reads, there may be no performance gain from striping disks.
Finally, note again that using remote disks adversely affects I/O performance.
You should not be using remote disks mounted from the network with any I/O-intensive operations if you need good performance.
Reading many files sequentially is faster if the files are clustered together on the disk, allowing the disk-head reader to flow from one file to the next.
This clustering is best done in conjunction with defragmenting the disks.
The overhead in finding the location of a file on the disk (detailed in the previous section) is also minimized for sequential reads if the files are clustered.
If you cannot specify clustering files at the disk level, you can still provide similar functionality by putting all the files together into one large file (as is done with the ZIP filesystem)
This is fine if all the files are readonly files or if there is just one file that is writeable (you place that at the end)
However, when there is more than one writeable file, you need to manage the location of the internal files in your system as one or more grow.
This becomes a problem and is not usually worth the effort.
If the files have a known bounded size, you can pad the files internally, thus regaining the single file efficiency.
Most operating systems provide the ability to map a filesystem into the system memory.
This ability can speed up reads and writes to certain files in which you control your target environment.
Typically, this technique has been used to speed up the reading and writing of temporary files.
If these files are created and written directly to the system memory, the speed of compilation is greatly increased.
Similarly, if you have a set of external files that are needed by your application, it is possible to map these directly into the system memory, thus allowing their reads and writes to be speeded up greatly.
But note that these types of filesystems are not persistent.
In the same way the system memory of the machine gets cleared when it is rebooted, so these filesystems are removed on reboot.
If the system crashes, anything in a memory-mapped filesystem is lost.
For this reason, these types of filesystems are usually suitable only for temporary files or read-only versions of disk-based files (such as mapping a CD-ROM into a memory-resident filesystem)
Remember that you do not have the same degree of fine control over these filesystems that you have over your application.
A memory-mapped filesystem does not use memory resources as efficiently as working directly from your application.
If you have direct control over the files you are reading and writing, it is usually better to optimize this within your application rather than outside it.
You should consider whether it would be better to let your application grow in memory instead of letting the filesystem take up that system memory.
For multiuser applications, it is usually more efficient for the system to map shared files directly into memory, as a particular file then takes up just one memory location rather than being duplicated in each process.
A memory-mapped file uses system resources to read the file into system memory, and that data can then be accessed from Java through the appropriate java.nio buffer.
A memory-mapped filesystem does not require the java.nio package and, as far as Java is concerned, files in that filesystem are simply files like any others.
The creation of memory-mapped filesystems is completely system-dependent, and there is no guarantee that it is available on any particular system (though most modern operating systems do support this feature)
On Unix systems, the administrator needs to look at the documentation of the mount command and its subsections on cachefs and tmpfs.
Under Windows, you should find details by looking at the documentation on how to set up a RAM disk, a portion of memory mapped to a logical disk drive.
In a similar way, there are products available that precache shared libraries (DLLs) and even executables in memory.
This usually means only that an application starts quicker or loads the shared library quicker, and so may not be much help in speeding up a running system (for example, Norton SpeedStart caches DLLs and device drivers in memory on Windows systems)
But you can apply the technique of memory-mapping filesystems directly and quite usefully for applications in which processes are frequently started.
Copy the Java distribution and all class files (all JDK, application, and third-party class files) onto a memory-mapped filesystem and ensure that all executions and classload s take place from that filesystem.
Because only the startup (and classloading) time is affected, this technique gives only a small boost to applications that are not frequently starting processes, but can be usefully applied if startup time is a problem.
When files are stored on disk, the bytes in the files are not necessarily stored contiguously: their storage depends on file size and contiguous space available on the disk.
Any particular file may have some chunks in one place, and a pointer to the next chunk that may be quite a distance away on the disk.
This delay occurs because the disk header must wind on to the next chunk with each fragmentation, and this takes time.
For optimum performance on any system, it is a good idea to periodically defragment the disks.
This reunites files that have been split up so that the disk heads do not spend so much time searching for data once the file-header locations have been identified, thus speeding up data access.
Most disks have a location from which data is transferred faster than from other locations.
Usually, the closer the data is to the outside edge of the disk, the faster it can be read from the disk.
This means that the linear speed of the disk under a point is faster the farther away the point is from the center of the disk.
Thus, data at the edge of the disk can be read from (and written to) at the fastest possible rate commensurate with the maximum density of data storable on disk.
This location with faster transfer rates is usually termed the disk sweet spot.
Some (commercial) utilities provide mapped access to the underlying disk and allow you to reorganize files to optimize access.
On most server systems, the administrator has control over how logical partitions of the disk apply to the physical layout, and how to position files to the disk sweet spots.
Experts for high-performance database systems sometimes try to position the index tables of the database as close as possible to the disk sweet spot.
These tables consist of relatively small amounts of data that affect the performance of the system in a disproportionately large way, so that any speed improvement in manipulating these tables is significant.
Note that some of the latest operating systems are beginning to include "awareness" of disk sweet spots, and attempt to move executables to sweet spots when defragmenting the disk.
You may need to ensure that the defragmentation procedure does not disrupt your own use of the disk sweet spot.
Java provides a virtual machine runtime system that is just that: an abstraction of a CPU that runs in software.
These virtual machines run on a real CPU, and in this section I discuss the performance characteristics of those real CPUs.
The CPU and many other parts of the system can be monitored using system-level utilities.
On Windows, the task manager and performance monitor can be used for monitoring.
On Unix, a performance monitor (such as perfmeter) is usually available, as well as utilities such as vmstat.
Two aspects of the CPU are worth watching as primary performance points.
These are the CPU utilization (usually expressed in percentage terms) and the runnable queue of processes and threads (often called the load or the task queue)
The first indicator is simply the percentage of the CPU (or CPUs) being used by all the various threads.
If this is up to 100% for significant periods of time, you may have a problem.
On the other hand, if it isn't, the CPU is underutilized, but that is usually preferable.
Low CPU usage can indicate that your application may be blocked for significant periods on disk or network I/O.
High CPU usage can indicate thrashing (lack of RAM) or CPU contention (indicating that you need to tune the code and reduce the number of instructions being processed to reduce the impact on the CPU)
This means that the system is being worked toward its optimum, but that you have left some slack for spikes due to other system or application requirements.
However, note that if more than 50% of the CPU is used by system processes (i.e., administrative and operating-system processes), your CPU is probably underpowered.
This can be identified by looking at the load of the system over some period when you are not running any applications.
The second performance indicator, the runnable queue, indicates the average number of processes or threads waiting to be scheduled for the CPU by the operating system.
They are runnable processes, but the CPU has no time to run them and is keeping them waiting for some significant amount of time.
As soon as the run queue goes above zero, the system may display contention for resources, but there is usually some value above zero that still gives acceptable performance for any particular system.
You need to determine what that value is in order to use this statistic as a useful warning indicator.
A simplistic way to do this is to create a short program that repeatedly does some simple activity.
You can run copies of this process one after the other so that more and more copies are simultaneously running.
Keep increasing the number of copies being run until the run queue starts increasing.
By watching the times recorded for the activity, you can graph that time against the run queue.
This should give you some indication of when the runnable queue becomes too large for useful responses on your system, and you can then set system threshold monitors to watch for that level and alert the administrator if the threshold is exceeded.
One guideline from Adrian Cockcroft is that performance starts to degrade if the run queue grows bigger than four times the number of CPUs.
If you can upgrade the CPU of the target environment, doubling the CPU speed is usually better than doubling the number of CPUs.
And remember that parallelism in an application doesn't necessarily need multiple CPUs.
If I/O is significant, the CPU will have plenty of time for many threads.
The operating system also has the ability to prioritize the processes in terms of providing CPU time by.
If there are other processes that need to run on the same machine but it doesn't matter if they were run more slowly, you can give your application processes a (much) higher priority than those other processes, thus allowing your application the lion's share of CPU time on a congested system.
If your application consists of multiple processes, you should also consider the possibility of giving your various processes different levels of priority.
Being tempted to adjust the priority levels of processes, however, is often a sign that the CPU is underpowered for the tasks you have given it.
Maintaining watch directly on the system memory (RAM) is not usually that helpful in identifying performance problems.
A better indication that memory might be affecting performance can be gained by watching for paging of data from memory to the swap files.
Most current operating systems have a virtual memory that is made up of the actual (real) system memory using RAM chips, and one or more swap files on the system disks.
Processes that are currently running are operating in real memory.
The operating system can take pages from any of the processes currently in real memory and swap them out to disk.
Paging leaves free space in real memory to allocate to other processes that need to bring in a page from disk.[2]
The term swapping refers to moving entire processes between main memory and the swap file.
Most modern operating systems no longer swap processes; instead, they swap pages from processes.
Obviously, if all the processes currently running can fit into real memory, there is no need for the system to swap out any pages.
However, if there are too many processes to fit into real memory, paging allows the system to free up system memory to run more processes.
One obvious way is that if a process has had some pages moved to disk and the process becomes runnable, the operating system has to pull back the pages from the disk before that process can be run.
In addition, both the CPU and the disk I/O subsystem spend time doing the paging, reducing available processing power and increasing the load on the disks.
This cascading effect involving both the CPU and I/O can degrade the performance of the whole system in such a way that it may be difficult to even recognize that paging is the problem.
The extreme version of too much paging is thrashing, in which the system is spending so much time moving pages around that it fails to perform any other significant work.
The next step is likely to be a system crash.
As with runnable queues (see Section 14.2), a little paging of the system does not affect performance enough to cause concern.
It indicates that the system's memory resources are being fully used.
But at the point where paging becomes a significant overhead, the system is overloaded.
On Unix, the utilities vmstat and iostat provide details as to the level of paging, disk activity, and memory levels.
On Windows, the performance monitor has categories to show these details, as well as being able to monitor the system swap files.
If there is more paging than is optimal, the system's RAM is insufficient or processes are too big.
To improve this situation, you need to reduce the memory being used by reducing the number of processes or the memory utilization of some processes.
Assuming that it is your application that is causing the paging (otherwise, either the system needs an upgrade, or someone else's processes may also have to be tuned), you need to reduce the memory resources you are using.
When the problem is caused by a combination of your application and others, you can partially address the situation by using process priorities (see Section 14.2)
The equivalent to priority levels for memory usage is an all-or-nothing option, where you can lock a process in memory.
This option is not available on all systems and is more often applied to shared memory than to processes, but nevertheless it is useful to know.
If this option is applied, the process is locked into real memory and is not paged out at all.
You need to be aware that using this option reduces the amount of RAM available to all other processes, which can make overall system performance worse.
Any deterioration in system performance is likely to occur at heavy system loads, so make sure you extrapolate the effect of reducing the system memory in this way.
The bandwidth (the amount of data that can be carried by the network) tends to be the first culprit checked.
Assuming you have determined that bad performance is attributable to the network component of an application, there are more likely causes for the poor performance than network bandwidth.
The most likely cause of bad network performance is the application itself and how it is handling distributed data and functionality.
The overall speed of a particular network connection is limited by the slowest link in the connection chain and the length of the chain.
Identifying the slowest link is difficult and may not even be consistent: it can vary at different times of the day or for different communication paths.
A network communication path can lead from an application through a TCP/IP stack (which adds various layers of headers, possibly encrypting and compressing data as well), then through the hardware interface, through a modem, over a phone line, through another modem, over to a service provider's router, through many heavily congested data lines of various carrying capacities and multiple routers with differing maximum throughputs and configurations, to a machine at the other end with its own hardware interface, TCP/IP stack, and application.
In addition, there are dropped packets, acknowledgments, retries, bus contention, and so on.
Because so many possible causes of bad network performance are external to an application, one option you can consider including in an application is a network speed-testing facility that reports to the user.
This should test the speed of data transfer from the machine to various destinations: to itself, to another machine on the local network, to the Internet service provider, to the target server across the network, and to any other destinations appropriate.
This type of diagnostic report can tell your users that they are obtaining bad performance from something other than your application.
If you feel that the performance of your application is limited by the actual network communication speed, and not by other (application) factors, this facility will report the maximum possible speeds to your users (and put the blame for poor network performance outside your application, where it belongs)
Latency is different from the load-carrying capacity (bandwidth) of a network.
Bandwidth refers to how much data can be sent down the communication channel for a given period of time (e.g., 64 kilobits per second) and is limited by the link in the communication chain that has the lowest bandwidth.
The latency is the amount of time a particular data packet takes to get from one end of the communication channel to the other.
Bandwidth tells you the limits within which your application can operate before the performance becomes affected by the volume of data being transmitted.
Latency often affects the user's view of the performance even when bandwidth isn't a problem.
For example, on a LAN, latency might be 10 milliseconds.
In this case, you can ignore latency considerations unless your application is making a large number of transmissions.
If your application is making a large number of transmissions, you need to tune the application to reduce the number of transmissions being made.
That 10 ms overhead added to every transmission can add up if you just ignore it and treat the application as if it were not distributed.
In most cases, especially Internet traffic, latency is an important concern.
You can determine the basic roundtrip time for data packets from any two machines using the ping utility.[3] This utility provides a measure of the time it takes a packet of data to reach another machine and be returned.
If the communication channel is congested and the overlying protocol requires retransmissions (often the case for Internet traffic), one transmission at the application level can actually be equivalent to many round trips.
If, for instance, the round-trip time is 400 ms (not unusual for an Internet link), this is the basic overhead time for any request sent to a server and the reply to return, without even adding any processing time for the request.
If you are using TCP/IP and retransmissions are needed because some packets are dropped (TCP automatically handles this as needed), each retransmission adds another 400 ms to the request response time.
If the application is conversational, requiring many data transmissions to be sent back and forth before the request is satisfied, each intermediate transmission adds a minimum of 400 ms of network delay, again without considering TCP retransmissions.
The time can easily add up if you are not careful.
At the network level, you need to monitor the transmission statistics (using the ping and netstat utilities and packet sniffers) and consider tuning any network parameters that you have access to in order to reduce retransmissions.
Because the stacks are usually delivered with the operating system and performance-tested before delivery (since a slow network connection on an otherwise fast machine and fast network is pretty obvious), it is unlikely that the TCP/IP stack itself is a performance problem.
Some older versions of Windows TCP/IP stacks, both those delivered with the OS and others, had performance problems, as did some versions of TCP/IP stacks on the Macintosh OS (up to and including System 7.1)
Because the TCP/IP stack is causing a performance problem, it affects all network applications running on that machine.
In the past I have seen isolated machines on a lightly loaded network with an unexpectedly low transfer speed for FTP transfers compared to other machines on the same network.
Once you suspect the TCP/IP stack, you need to probe the speed of the stack.
Testing the loopback address (127.0.0.0) may be a good starting point, though this address may be optimized by the stack.
The easiest way to avoid the problem is to ensure you are using recent versions of TCP/IP stacks.
In addition to the stack itself, stacks include several tuneable parameters.
Most of these parameters deal with transmission details beyond the scope of this book.
When your application sends data, the underlying protocol breaks the data into packets that are transmitted.
There is an optimal size for packets transmitted over a particular communication channel, and the packet size actually used by the stack is a compromise.
Smaller packets are less likely to be dropped, but they introduce more overhead, as data probably has to be broken up into more packets with more header overhead.
If your communication takes place over a particular set of endpoints, you may want to alter the packet sizes.
For a LAN segment with no router involved, the packets can be big (e.g., 8KB)
For a LAN with routers, you probably want to set the maximum packet size to the size the routers allow to pass unbroken.
Routers can break up the packets into smaller ones; 1500 bytes is the typical maximum packet size and the standard for Ethernet.
The maximum packet size is configurable by the router's network administrator.
Other causes of slow network I/O can be attributed directly to the load or configuration of the network.
For example, a LAN may become congested when many machines are simultaneously trying to communicate over the network.
The potential throughput of the network could handle the load, but the algorithms to provide communication channels slow the network, resulting in a lower maximum throughput.
A congested Ethernet network has an average throughput approximately one-third the potential maximum throughput.
Congested networks have other problems, such as dropped network packets.
If you are using TCP, the communication rate on a congested network is much slower as the protocol automatically resends the dropped packets.
If you are using UDP, your application must resend multiple copies for each transfer.
Dropping packets in this way is common for the Internet.
For LANs, you need to coordinate closely with network administrators to alert them to the problems.
For single machines connected by a service provider, there are several things you can do.
First, there are some commercial utilities available that probe your configuration and the connection to the service provider, suggesting improvements.
The phone line to the service provider may be noisier than expected: if so, you also need to speak to the phone line provider.
It is also worth checking with the service provider, who should have optimal configurations they can demonstrate.
Dropped packets and retransmissions are a good indication of network congestion problems, and you should be on constant lookout for them.
Dropped packets often occur when routers are overloaded and find it necessary to drop some of the packets being transmitted as the router's buffers overflow.
This means that the overlying protocol will request the packets to be resent.
The netstat utility lists retransmission and other statistics that can identify these sorts of problems.
Retransmissions may indicate that the maximum packet size is too large.
Looking up network addresses is an often-overlooked cause of bad network performance.
This is the actual address that the network understands and uses for routing network packets.
The way this translation works is that your system is configured with some seldom-used files that can specify this translation, and a more frequently used Domain Name System (DNS) server that can dynamically provide you with the address from the given string.
The DNS server checks its cache to find an IP address corresponding to that hostname.
If the server does not find an entry in the cache, it asks its own DNS server (usually further up the Internet domain-name hierarchy) until ultimately the name is resolved.
This may be by components of the name being resolved, e.g., first .org, then something.org, etc., each time asking another machine as the search request is successively resolved.
This resolved IP address is added to the DNS server's cache.
The IP address is returned to the original machine running the application.3
The application uses the IP address to connect to the desired destination.4
The address lookup does not need to be repeated once a connection is established, but any other connections.
A session can cache the IP address explicitly after the first lookup, but this needs to be done at the application level by holding on to the InetAddress object.
You can improve this situation by running a DNS server locally on the machine, or on a local server if the application uses a LAN.
A DNS server can be run as a "caching only" server that resets its cache each time the machine is rebooted.
There would be little point in doing this if the machine used only one or two connections per hostname between successive reboots.
For more frequent connections, a local DNS server can provide a noticeable speedup to connections.
Some of these suggestions apply only after a bottleneck has been identified:
Limit application bandwidth requirements to the network segment with the smallest bandwidth.
Aim to minimize the number of network round trips necessary to satisfy an application request.
Constantly monitor the entire system with any monitoring tools available.
Use normal monitoring records to get an early warning of changes in the system usage patterns.
Watch for low CPU activity coupled with high disk activity and delayed responses.
Ensure the CPU runnable queue does not get too large.
Aim for average CPU utilization of not more than 75%
Time all processes and terminate any that exceed timeout thresholds.
Doubling the CPU speed is usually better than doubling the number of CPUs.
Add more swap space when there is no other way to increase the memory available to the application (or to reduce the application's memory usage requirements)
Test to see if running on raw partitions will be faster.
Look at mapping filesystems into memory for speedier startups and accesses.
For multiuser applications, this is an efficient way of sharing in-memory data.
Ensure that system swap files are on different disks from any intensively used files.
Cluster files together at the disk level, if possible, or within one big container file.
Consider altering priority levels of processes to tune the amount of CPU time they get.
Consider locking processes into memory so they do not get paged out.
Partition the system to allocate determinate resources to your application.
Consider tuning the maximum packet size specified by the TCP/IP stack.
Ensure that your TCP/IP stacks have no performance problems associated with them.
Consider running a local caching DNS server to improve the speed of hostname lookups.
J2EE performance tuning builds on lower-level performance-tuning techniques and general architectural considerations described earlier in this book.
Continual load testing, valid test data, appropriate testing environments, good monitoring tools, and well-specified performance targets are all crucial to achieving a high-performing J2EE deployment.
In addition, monitoring after deployment is strongly recommended to maintain good performance.
Don't let the brevity of this section mislead you—making a performance plan, emphasizing the aspects I've just listed, is the single most important indicator of success for a J2EE project's performance.
Contention bottlenecks occur when multiple objects try to use the same resource at the same time and extend across multiple VMs or to outside resources like databases.
J2SE profilers essentially monitor and log various aspects of a single VM.
Potential performance bottlenecks come mainly from three generic locations: processing within components, interfaces between components, and communication between components.
Intercomponent communication overhead (for example, network transfers) is distinct from interface overhead (such as marshalling) or conversions (such as SQL request generation)
Low-overhead performance monitoring lets you monitor constantly without worrying about how profiling overhead affects server behavior.
This means that you can leave monitoring on at all times—in development systems, in test systems, and in production systems—without serious performance degradation.
This situation does not occur with J2SE profilers, which have such a large overhead that running with a profiler on at all times would kill a project.
J2SE profilers tend to have high overhead because it is considered acceptable, given their usage pattern.
J2EE monitors are targeted at production systems as well as development, so they are generally designed to have lower overhead.
Monitoring should correlate incoming requests with subsequently monitored methods, components, and communications.
Without this capability, you can end up targeting many more bottlenecks than necessary or spending significant time trying to determine which requests map to which bottlenecks.
Monitoring should store all data persistently so you can decouple analysis from running the server.
Having things happen during a test run with no way to analyze the data later is annoying because one graph or another is displayed only in real time, with no logged data.
Logging is more important than saving performance by not logging.
Identify spikes and trends that cause performance problems, and then alter the application to handle those problems.
However, too fine a granularity of logging causes too much overhead.
The monitoring tool should scale with the application so you can deploy the monitoring with the application in the production environment.
A separate class of monitoring tools has emerged in the last couple of years, dedicated to monitoring J2EE applications efficiently.
These tools improve J2EE performance-tuning productivity significantly, and obtaining one for your project is worthwhile.
You can obtain a list of such tools from http://www.JavaPerformanceTuning.com/resources.shtml.
Should you wish to implement your own tool, you would need to add logging to all the main communication interfaces of the application, the transaction and session boundaries, the life-cycle boundaries (e.g., creation and destruction of EJBs), and request initiation and completion.
A freely available logging tool designed to work with J2EE applications, such as Steve Souza's JAMon (see http://www.JavaPerformanceTuning.com/tools/jamon/index.shtml), can assist with this task.
You need reliable metrics to gauge performance and effectively compare results before you begin tuning and after you make changes.
Before getting into the specifics of measurements, let's look at a study that shows where bottlenecks tend to be.
A Mercury Interactive Corporation analysis of thousands of load tests on company web sites[1] found that enterprise performance problems come from four main areas: databases, web servers, application servers, and the network.
Each area typically causes about a quarter of the performance problems.
The most common database problems were insufficient indexing, fragmented databases, out-of-date statistics, and faulty application design.
Solutions included tuning the index, compacting the database, updating the database, and rewriting the application so the database server controlled the query process.
The most common web-server problems were poor design algorithms, incorrect configurations, poorly written code, memory problems, and overloaded CPUs.
The most common application-server problems were poor cache management, nonoptimized database queries, incorrect software configuration, and poor concurrent handling of client requests.
The most common network problems included inadequate bandwidth somewhere along the communication route, and undersized, misconfigured, or incompatible routers, switches, firewalls, and load balancers.
The results from this useful study may help you focus on the most likely problems.
However, not all bottlenecks are listed here, and even if yours are, pinpointing their precise location can be difficult.
Taking these suggested measurements may help you isolate the main bottlenecks.
Note that different tools take different measurements, and it is not always possible to match one tool's measurements with another or with this list.
For example, some tools cannot measure the time from the (simulated) user click, but might start measuring once they send the HTTP request.
Eliminate memory leaks before undertaking other tuning to avoid wasted tuning effort.
Eliminating memory leaks is absolutely necessary for J2EE applications, and bottlenecks can be changed (eliminated or added) when eliminating these leaks.
Measure the total time it takes to service the request on the server.
Try not to include transfer time to and from the client.
You can obtain this measurement by wrapping the doGet( ) and doPost( ) servlet methods or by using a ServletFilter that logs execution times.
Naturally, this filter should be the first in the filter chain so the time taken for any other filter is included in the total time recorded.
The time measured will include some network transfer time, since the server-to-client socket write does not complete on the server until the last portion of data is written to the server's network buffer.
This measurement is fully covered in Section 16.1, which explains how to create and use wrappers to measure JDBC performance.
A second technique uses smart proxies to monitor the performance of RMI calls.
The number of available file descriptors is limited in each process and in the overall system.
Each open file and open socket requires a file descriptor.
Use ulimit (Unix only) to monitor the number of file descriptors available to the processes, and make sure this number is high enough to service all connections.
In Windows, you can monitor open files and sockets from the performance monitor.
Look for too many calls to these methods, which can occur with excessive cycling of objects (too many creates) or excessive passivation.
Wrapping can be difficult because the container can be responsible for such calls.
Relying on the JDBC wrapper to catch transaction boundaries might be easiest.
First, you would need to verify that all transaction boundaries correspond to database transaction boundaries.
Cache sizes—the number of objects held and the physical size used—should be monitored.
Use operating-system utilities to measure CPU utilization (no Java API measures CPU utilization of the JVM)
Windows has a performance monitor, and Unix has the sar, vmstat, and iostat utilities.
On Windows, press Ctrl-Break in the window where the Java program is running or (prior to SDK 1.3) click the Close button on the command window.
The stack dump lists the state and Java stack of every currently running thread.
In the case of a deadlock, two or more threads will be in the "W" (wait) state, indicating that they are waiting for locks to be released.
The method at the top of the stack listing is the "current" method—i.e., the method that requested a lock and caused the thread to move into a wait state as it waits for the lock to be granted.
Thus, identifying which methods are causing the deadlock is easy.
When garbage collection kicks in, current VMs stop other processing activity.
These perceptible pauses in activity can result in unacceptable performance.
Use the latest available VMs, and try to tune the garbage collection to minimize "stop the world" pauses.
Concurrent garbage collection (-Xconcgc in Version 1.4.1 of the Sun VM) allows pause times to be minimized.
Use netperf (Unix) or the performance monitor (Windows) to measure network bandwidth.
Any of the following symptoms can indicate a performance problem:
After taking the measurements described here, you may want to analyze several statistics, including the number of users, the number of components, throughput (queries per minute), transaction rates, average and maximum response times, and CPU utilization.
You should look for trends and anomalies, and try to identify whether any resource is limited in the current system.
For example, increasing the number of concurrent users over time may show that throughput flattens out, indicating that the current maximum throughput was reached.
Performance requirements should include the required response times for end users, the perceived steady state and peak user loads, the average and peak amount of data transferred per request, and the expected growth in user load over the first or next 12 months.
Create a testing environment that mirrors the expected real-world environment as closely as possible.
Generally, there will be differences, but the most critical aspects to simulate closely are the expected client activity, the application data, and the peak scaling requirements (amount of data and number of users)
The only reliable way to determine a system's scalability is to perform load tests in which the volume and characteristics of the anticipated traffic are simulated as realistically as possible.
Characterize the anticipated load as objectively and systematically as possible, use existing log files when possible, and characterize user sessions (such as the number and types of pages viewed or the duration of sessions)
Continuously retest and measure against established benchmarks to ensure that application performance hasn't degraded as changes are made.
The server must be designed to handle peak loads, so tests including expected peak loads should be scrutinized.
Peak user loads are the number of concurrent sessions managed by the application server, not the number of possible users.
Determine the maximum acceptable response time for getting a page.1
Simulate user requests, gradually adding simulated users until the application response delay becomes greater than the acceptable response time.
When testing performance, run tests overnight and on weekends to generate longer-term trends.
Testing without a real network connection can give false measures.
Low user simulation can be markedly different from high user simulation.
Network throughput may be larger than in the deployed environment.
Performance testing should continue even after the application is deployed.
For applications expected to perform 24/7, inconsequential issues like database logging can degrade performance.
Continuous monitoring is the key to spotting even the slightest abnormality.
Set performance capacity thresholds, monitor them, and look for trends.
When application transaction volumes reach 40% of maximum expected volumes, you should execute plans to expand the capacity of system.
The point is that you should watch for signs that your application is outgrowing the system and make plans for an upgrade well before the upgrade is needed.
Some connections may fail due to a congested network or overloaded server.
Users perceive the need to reenter data or return to the last screen as bad performance.
Ideally, when a connection is reestablished, the user should find himself back at the same state as before the connection failure.
If the session ID is still valid, then you should hold all the session state so the display can be re-created at any point.
If the session ID is invalidated by the connection failure, then maintaining state in the client should enable display re-creation.
As discussed in Chapter 1, popular browsers try to display screens in a way that seems faster to the user.
Nevertheless, certain ways of laying out pages make the display take longer.
For example, HTML tables are often not displayed until the contents are available to enable table-cell size calculation.
Use size tags to help the browser calculate the display.
Pages constructed from multiple disparate sources (e.g., embedded images) require multiple connections, all of which add to the overall perceived page display time.
A poorly designed page could be seen as slow even if the components of the page individually download quickly.
You should be able to find multiple sites displaying structures similar to those you wish to display.
Compare their performance and choose the best combination for your application.
On the server side, don't rely on the default server buffers to flush the pages.
Different buffer sizes and forced flushing of the output at certain points can improve perceived performance by sending displayable parts of a page more quickly.
Different users have different requirements and, more importantly, different value to your business.
You should balance the performance provided to your users according to their value to your business.
However, doing so is not always a clear-cut matter of giving higher-priority users a faster response at the expense of lower-value users.
For example, many web sites provide premium service to paying users, with restricted access to nonpaying users.
But if nonpaying users find that the web site is too slow, they may be unlikely to convert to paying users, and converting nonpaying users to paying users may be a business priority.
However you may decide to assign priorities, sort incoming requests into different priority queues and service the higher-priority requests first.
Priority queuing can be achieved by initially allocating the incoming request a priority level based on your application requirements or according to the priority associated with the session.
To support priorities throughout the J2EE application, requests probably need to be transferred between components at each stage through multiple queues so that queued requests can be accepted in order of priority level.
On the Internet, there are inevitably some very long response times and communication failures.
This results from the nature of Internet communications, which is liable to variable congestion, spikes, blocked routes, and outages.
Even if your server were up 100% of the time and serviced every request with a subsecond response, there would still be some problems due to Internet communication channels.
You need to construct your application to handle communication failures gracefully, bearing in mind the issue of user perception.
A few long response times from communication failures may not necessarily make a bad impression, especially if handled correctly.
Experienced Internet users expect communication failures and don't necessarily blame the server.
In any case, if a connection or transaction needs to be reestablished, explain to the user why the outage occurred.
For example, the Internet regularly becomes more congested at certain times.
By monitoring your server, you should be able to establish whether these congested times result in an increased number of connection retries.
If so, you can present a warning to the user explaining that current Internet congestion may result in some connection failures (and perhaps suggest that the user try again later after a certain time if performance is unsatisfactory)
Setting the expectation of your users in this way can help reduce the inevitable dissatisfaction that communication failures cause.
Including an automated mechanism for congestion reporting could be difficult.
The Java API doesn't provide a mechanism to measure connection retries.
You could measure download times regularly from some set of reference locations, and use those measurements to identify when congestion causes long download times.
Evaluate performance targets as early as possible (preferably at project specification), and then keep your targets in mind.
Peak traffic can be an order of magnitude higher than the average request rate.
For a highly scaled popular server, ideal peak performance targets would probably consist of subsecond response times and hundreds of (e-commerce) transactions per second.
You can use these basic guidelines to calculate target response times.
The quickest way to lose user interest is to keep the user waiting for screens to display.
Some experts suggest that perceived delays accumulate across multiple screens.
It is not sufficient for individual screens to display within the limit of the user's patience (the subject of the earlier "Page Display" section)
If the user finds himself waiting for several screens to display slowly, one after the other, the cumulative wait time can exceed a limit (perhaps as low as eight seconds) that induces the user to abandon the transaction (and the site)
One of the better ways to keep the cumulative delay low is to avoid making the user go through too many screens to get to his goal.
Your users have a characteristic range of bandwidths, from slow modem dialup speeds to broadband.
Determine what the range of user bandwidths are and test throughout the range.
Different page designs display at different speeds for different bandwidths, and users have different expectations.
Users with broadband connections expect pages to appear instantly, and slow pages stand a good chance of never being.
There is a limit to how fast you can make your application run.
The application has work to do, and that work takes a minimum number of processor instructions to execute.
The faster the machine that runs the application, the faster those instructions execute, but there is a limit to processor speeds.
As the number of concurrent requests received by your server increases, your application exceeds the target response times when the workload is too much for the given machine.
At this point, you have two options: use a faster machine, or use several machines.
The first option is not always available or cost-effective, and ultimately you may use the fastest machine available and still need more power.
Any efficient J2EE design should include the possibility that the application will need to be deployed across multiple servers.
Two technologies that enhance the ability to achieve horizontal scalability are clustering and load balancing.
In this context, clustering means having a group of machines running the application.
Currently, scalable web-application architectures consist of many small servers accessed through a load balancer.
Generally, the load-balancing mechanism should route requests to the least-busy resource.
However, such routing can be difficult to achieve with some load-balancing mechanisms and may be inappropriate for some applications, especially session-based applications.
You should determine the load-balancing mechanism that is most appropriate to your application.
Load balancing can be achieved in a number of ways, as described in the following sections.
The mechanism for obtaining the route to a machine from the machine's name is the Domain Name System (DNS), discussed in Section 14.4.4
Many Internet sites use DNS load balancing; it is a common and simple load-balancing technique.
The result of a DNS lookup is typically cached at various locations, with caches lasting days (though this is configurable and can be any value down to seconds)
Consequently, it is slow to propagate changes when using DNS load balancing, and any one client typically uses the same IP address over multiple connections rather than being directed to alternate servers.
These issues can be problematic, but can also be advantageous if transactional or session-based communications are normal for your application.
Also note that DNS load balancing can be used in conjunction with other load-balancing techniques.
For example, if you use a load-balancing dispatcher, then you can use DNS to balance multiple load-balancing dispatchers to achieve the optimal case in which the load-balancing mechanism has no single point of failure.
However, some clients could still see failures due to lookup caching.
A hardware load balancer is a machine at a single IP address that reroutes communications to other machines—for example, it reroutes IP packets by rewriting the IP address in the header and passing the packet to the appropriate machine.
The technique is more complex and more expensive than DNS load balancing, but much more flexible and capable.
Hardware load balancers may come with extra features such as the ability to automatically detect unavailable servers and eliminate them from their routing tables; to intelligently reroute packets based on server load, IP address, port number, or other criteria; and to decrypt encrypted communications before rerouting packets.
A cluster can be implemented with a frontend dispatcher that accepts requests and passes them on to other clustered servers.
All requests are directed to the dispatcher, either explicitly (the client doesn't know about any machines "behind" the dispatcher) or redirected (as is done when correctly configured browsers have their requests automatically sent to a proxy server)
The dispatcher (or proxy server) services the request in one of three ways:
The request is satisfied by returning a result (document) cached in the dispatcher.
This scenario is common for proxy servers, but unusual for dispatchers.
The request is redirected to another server that services the request and returns the results to the client, either directly or, more commonly, through the dispatcher.
The dispatcher redirects the client to send the request to another server.
The HTTP protocol supports this option with the Location directive.
For example, if a browser connects to a server requesting a particular URL and receives a response like this: Location: http://somewhere.else.com/ then the browser automatically tries to request the new URL.
A dispatcher could also decrypt encrypted requests before handling or forwarding them, thus centralizing security and offloading some processing from the server cluster.
Decide where any particular document or service is best served and specify the appropriate host machine in the URL.
For example, you could retrieve images from the image server and documents from a separate document server.
In addition, the URL could be retained by the client and used later or reused when the specified host is no longer the optimal server for the request.
Where possible, convert dynamic requests into static ones by replacing URLs served dynamically with ones served statically.
Load balancing is possible by varying how pooled objects are handed out.
This type of balancing tends to apply at the application level, where you can create and hand out objects from a pool according to your own algorithm.
The connection mechanism in the client can serve as a load-balancing mechanism.
The client can even check for an available rerouting server to combine client load balancing with server load balancing.
One such load-balancing connection mechanism simply selects from a list of available RMI connections sequentially.
The application itself should be configured for deployments with different usage patterns.
Each type of readonly, read-write, and batch-update bean components should be in different application servers so that each application server can be appropriately configured.
Transactional and nontransactional sessions and messages should be served from separate servers.
Nontransactional server components can be replicated easily for horizontal scaling.
Multiple VMs can be used even within individual server machines to help load-balance the application.
Using one thread per user can create a bottleneck with large numbers of concurrent users.
Stateless sessions are easily load-balanced; replicating transactional sessions is more involved and carries higher overhead.
Pseudo-sessions that encode a session identifier into the URLs and are stored globally are probably the best compromise for load-balancing transactional sessions.
Separate web servers should be used for all static content, which can be served much faster and with much lower overhead than dynamic content.
Priority queues can provide higher-priority users with a higher quality of service.
The section Section 10.8 discusses queue-based load balancing and network queuing in more detail.
The frontend queue can use the network queue as its bottleneck and accept requests only when there are sufficient resources to process the request.
Try to balance the workload of the various components so they all work.
All components should work at any given time; there should be no idle components.
Section 13.4.1.4 discusses database partitioning schemes to assist in load-balancing the application.
In addition to selecting one or more load-balancing mechanisms, you may need to consider optimal loadbalancing algorithms for your application.
Allocate requests to the server with the currently minimum load.
Successively select the next server in a list, starting again from the first server when the list is exhausted.
Allocate requests based on the performance capability of the server.
Allocate requests to the IP address (physically) nearest the client.
Allocate requests according to a value within the HTTP header, such as the URL or a cookie.
Messaging is an important feature for many large J2EE applications.
I cover it here rather than dedicating an entire chapter to JMS.
Close resources (e.g., connections, session objects, producers, and consumers) when you finish with them.
Start the consumer before the producer so the initial messages do not need to queue when waiting for the consumer.
If you have transactional sessions, try to separate nontransactional messages and use nontransactional sessions for them.
You could compress message bodies or eliminate nonessential content to keep the size down.
The redelivery count should be specified to avoid indefinitely redelivered messages.
A higher redelivery delay and lower redelivery limit reduces overhead.
Set the Delivery TimeToLive value as low as is feasible (the default is for messages to never expire)
Since fewer messages can sit in the Delivery queue, they have to be moved along more quickly.
However, if the capacity is too small, efficiency is reduced because producers have to delay sending messages until the Delivery queue has the spare capacity to accept them.
Some more advanced architectural considerations are also worthy of note.
As with most architectures, asynchronous processing is more scalable than synchronous processing.
When messages are sent in high volumes, delivery can become unpredictable and bursty.
Messages can be produced far faster than they can be consumed, causing congestion.
When this condition occurs, message sends need to be throttled with flow control.
A load-balancing message queue may be needed for a high rate of messages (for example, more than 500 messages per second)
In duplicate delivery mode, messages are sent and, if the acknowledgment is delayed long enough, a duplicate message is sent rather than conversing with the server to determine whether the message was received.
However, with duplicate delivery mode, you need to identify whether the message has already been processed because it may be sent more than once.
For this latter scenario, a unicast-based model of message queuing, organized into a hub-and-spoke model, is more efficient than multicast.
Planning for performance is the single most important indicator of success for a J2EE project's performance.
Enterprise performance problems tend to come about equally from four main areas: databases, web servers, application servers, and the network.
Common database problems are insufficient indexing, fragmented databases, out-of-date statistics, and faulty application design.
Solutions include tuning the index, compacting the database, updating the database, and rewriting the application so the database server controls the query process.
Common web-server problems are poor design algorithms, incorrect configurations, poorly written code, memory problems, and overloaded CPUs.
Common application-server problems are poor cache management, unoptimized database queries, incorrect software configuration, and poor concurrent handling of client requests.
Common network problems are inadequate bandwidth somewhere along the communication route, and undersized, misconfigured, or incompatible routers, switches, firewalls, and load balancers.
Monitor JVM heap sizes, request response times, request service times, JDBC requests, RMI communications, file descriptors, bean life cycles, transaction boundaries, cache sizes, CPU utilization, stack traces, GC pauses, and network bandwidth.
Watch out for slow response times, excessive database table scans, database deadlocks, unavailable pages, memory leaks, and high CPU usage (consistently over 85%)
Tests should be as close to the expected deployed system as possible and should be able to run for a long period of time.
One testing methodology is to determine the maximum acceptable response time page download, estimate the maximum number of simultaneous users, increase simulated users until the application response delay becomes unacceptable, and tune until you reach a good response time for the desired number of users.
Tune the redelivery count, the Delivery TimeToLive, and the Delivery capacity.
Usually, this data repository is a relational database (since that is what JDBC was designed for), but the JDBC protocol does not restrict connections to relational databases.
Connecting to a database with JDBC is essential for many applications.
This chapter shows you how to identify the bottlenecks in your communications with the database and how to improve the performance of those communications.
The first section, Section 16.1, provides extensive details on building a tool that allows you to determine whether your JDBC communications cause bottlenecks and, if so, which JDBC statements are to blame.
If you have already identified JDBC bottlenecks in your application, you can skip this first section.
The second section, Section 16.2, provides details on how to tune the JDBC portion of your application.
In addition to JDBC-specific issues, this section addresses tuning SQL (Structured Query Language) usage, since optimizing SQL can result in a huge gain in JDBC performance.
As with all other types of tuning, the first step to tuning JDBC usage is to identify bottlenecks.
The ease or difficulty of measuring JDBC communications can depend on how you use JDBC and how widespread JDBC calls are in your application.
In this section, we'll run through a framework that makes measuring JDBC performance straightforward.
At the outset, you must identify exactly what you should measure.
Effectively profiling distributed applications, such as those that use JDBC, can be difficult.
I/O can be significant in profiling simply because of the nature of a distributed application, which normally has threads waiting on I/O.
Whether threads blocking on reads and writes are part of a significant bottleneck or simply a side issue can be unclear.
If you look in the java.sql package, three interfaces form the core of JDBC: Connection , Statement , and ResultSet.
Obtaining a Connection object from the database driver Obtaining from that Connection object some type of Statement object capable of executing a particular SQL statement.
If that SQL statement reads from the database, using the Statement object to obtain a ResultSet object that provides access to the data in question.
The following method illustrates standard database interaction by accessing all the columns from every row of a specified database table and storing the data from each row in a String[ ] array, putting all the rows in a vector:
Each implementation for these and other JDBC interfaces is created by the producer of the database driver and delivered as part of the database driver package.
If you printed out the class name of the Connection object or other objects that you are using, you would probably see something like XXX Connection , XXX Statement , XXX ConnectionImpl , or XXX StatementImpl , where XXX is the name of the database you are using (Oracle, for example)
That technique works when database interaction is isolated, as it is in this method.
However, a Java application usually spreads its database interaction among many methods in many classes, and it is often difficult to isolate the database interaction.
Ideally, the JDBC classes would have built-in measurement capabilities and you could simply turn them on to analyze the performance of database interactions.
Can we replace the JDBC classes with our own implementations that provide measurements? The replacement classes would have to provide all the current functionality, be able to measure database interactions, require very little change to the application, and be transparent to the rest of the application.
Fortunately, when a framework is defined almost entirely in terms of interfaces, as JDBC is, it is very simple to replace any class with another implementation.
In particular, you can always replace any implementation of an interface with a wrapper class that simply wraps the original class and forwards (or delegates, in object-oriented terms) all the method calls to that original class.
Here, you can replace JDBC classes with wrappers around the original classes.
You can embed your measuring capabilities in the wrapper classes and execute those measurements throughout the application.
Wrapping objects of a particular interface by using dedicated wrapper objects is a useful and established technique.
The synchronized wrappers of the Collection classes are probably best known, but there are many other examples.
I could have used generated proxy objects here, but that would have made the explanation of wrapping JDBC objects more complicated than I wanted, so I stuck with explicitly coded wrapper classes.
Proxy classes are also slightly slower than precompiled classes; for measuring JDBC, though, it probably wouldn't make any difference.
The class has one instance variable of Connection type and a constructor that initializes that instance variable with the constructor parameter.
Most Connection methods are simply defined to delegate the call to the instance variable:
The three statement classes, Statement , PreparedStatement , and CallableStatement , have similar simple wrappers that forward all calls:
I haven't explicitly shown the Array wrapper or the DatabaseMetaData wrapper, but they are straightforward, needing only delegation and ResultSetWrapper s and ConnectionWrapper s returned instead of ResultSet s and Connection s.
However, all the wrapper classes for the different versions can be created the same.
The class I haven't yet shown you is the JDBCLogger class.
A simple implementation of that class would have null calls for the logging methods, providing no logging:
You can also sum the queries in the JDBCLogger class, retain the maximum half-dozen or so measurements, and then print out a summary statement.
The summary measurements I usually like to have are minimum, maximum, average, standard deviation, and the 90th percentile value.
Summary values tend to be more useful for a production environment, whereas printing individual lines for each query is more useful when profiling the system during development.
Wrappers are very useful for determining the performance of JDBC calls in both development and deployed applications.
Because they are simple and robust and require very little alteration to the application, wrappers can be retained in a deployed application.
Creating a configurable JDBCLogger class lets you turn logging on and off at will.
During development, these classes enable you to identify individually expensive database interactions and repeated database interactions that are expensive because of their cumulative cost.
Identifying these expensive database interactions is the first step toward improving your application's performance.
In production, these wrapper classes identify discrepancies between expected performance and reality.
After you use these classes and techniques to pinpoint where JDBC is causing a performance problem, you need to tune the database interactions.
Although JDBC tuning follows general tuning guidelines, specifying that you find the bottlenecks before tuning, it is worth knowing what tuning techniques are available from the outset.
You can structure your JDBC usage so it's easier to tune without violating good design and coding practices.
Always close or release resources when you are finished with them: Connections, Statements, and ResultSets.
For the foreseeable future, upgrade to new versions as soon as they prove stable.
The JDBC package provides interfaces to standardize the API that allows database connections.
However, the JDBC package does not provide concrete implementations of those APIs (except an ODBC driver)
Third-party implementations are necessary to make the actual connection to a particular database, and the database vendor usually provides these implementations.
These implementations are called JDBC drivers, and they fall into four categories or types, which differ in implementation and performance:
Type 1 drivers provide a link to a bridging driver, most commonly to ODBC, that can connect to any ODBCcompatible database (i.e., most databases)
Because this type of driver uses a second bridging product, it is usually the slowest type and should be avoided if possible.
Type 2 drivers connect to a local native binary client.
This connection is equivalent to using a native database client library, connecting to it via JNI, and wrapping that connection by using classes implementing the JDBC APIs.
This configuration should provide reasonable performance, assuming the driver itself has been through a round of performance tuning (most have by now)
If the database is on the local machine and the driver has optimizations to take advantage of that configuration, then a Type 2 driver could provide the fastest.
The JNI connection adds overhead, and that overhead becomes relatively larger as JVMs get faster.
But this type of driver is certainly worth evaluating if you have a choice of available drivers.
Type 3 drivers provide a connection to a middleware product, which in turn connects to a database.
In some ways, they are similar to Type 1 drivers.
For example, ODBC drivers are limited to using ODBC interfaces to databases, and that type of database interface does not usually provide optimal performance.
On the other hand, Type 3 driver middleware can use any interface to the database, including the fastest one.
Type 4 drivers connect from Java directly to a database by using network communications (sockets) and the database's network application protocol.
As it bypasses every type of intermediate bridging, this type of driver generally provides the fastest communication.
However, if the database and the Java code run on the same machine and the database's native client library provides optimizations for that machine's configuration, Type 2 drivers could be faster.
Type 3 drivers are rarer and tend to be third-party drivers, but this often means that they were built by a company dedicated to building high-performance database drivers.
A list of current drivers is available at http://industry.java.sun.com/products/jdbc/drivers, but that page does not provide much information to help select the fastest driver.
Try to use the latest version available, preferably with JDBC 3.0 or higher support.
Opening a connection to a database is costly, potentially requiring several network round trips, resources in both the client and database, and setup of several layers of transport and authentication.
Once a connection is created, keeping it open is not expensive, even if it is idle.
If you make many calls to a database, then repeatedly opening and closing connections can kill the application's performance.
Keeping connections to the database open is an important JDBC performance-tuning technique.
However, if you simply open more connections and keep them all open, you quickly reach the database's limit for open connections and probably exceed the number of connections for optimal efficiency.
To maintain an efficient number of open database connections, use a connection pool: an object pool consisting of Connection objects.
Connection pools are directly supported starting in JDBC 2.0, although earlier drivers may have their own connection pools.
Many third-party and open source connection-pool implementations are available, and creating one yourself is also fairly simple.
Remember that Connection objects are not reentrant: only one client of the pool can use the Connection object at a time.
You can even use a Connection wrapper to intercept close( ) requests and deposit the Connection back into the pool, if you want completely.
The reference page at http://www.JavaPerformanceTuning.com/tips/jdbcconnpool.shtml provides helpful links for finding or building a connection-pool implementation appropriate to your application.
The suggested order of preference for selecting a connection-pool implementation is:
The driver implementer's connection pool (the driver implementer has opportunities to optimize its connection pool with database-specific features)
The ideal pool size is just large enough to service requests without forcing the requests for connections to wait a long time.
If the pool size is flexible (i.e., connections are created when the pool has no more available connections and destroyed when they are idle for too long), then the goal is to tune the connection-pool size to minimize the creation and destruction of database connections.
Timeouts and initial, minimum, and maximum pool sizes all provide parameters to optimize your application's use of the connection pool.
The overall strategy for using pooled connections is to obtain and release pooled connections within each method that requires the connection, if the request is short.
However, do not release the connection only to use it again almost immediately in another method.
Instead, hold the connection until it is not immediately needed.
Once a connection is obtained from the pool, it is essentially an exclusively locked shared resource.
Keeping the connection locked exclusively for the minimum amount of time ensures maximum scalability for the application.
You are generally better off obtaining and releasing connections too frequently, and accepting the performance overhead of that strategy, than holding onto connections for too long.
For optimal use of connection pools, you might want to consider using several pools, with each pool supporting a particular connection feature.
Since some proportion of JDBC operations are inevitably read-only, you may want to have a read-only connection pool as well as a general connection pool.
Developers use SQL to interact with databases when using JDBC.
Drivers can support other syntaxes, such as the database's stored-procedure syntax, but ANSI SQL must be supported.
The SQL used by your application via JDBC can be tuned to create significant gains in overall performance.
They overlap slightly, and sometimes produce conflicting optimizations that need to be balanced.
When you create a Java application that uses JDBC, you don't tend to think about which procedures are operating on the same tables.
Instead, you probably think about which data each method and object needs and which data needs to be updated in the database on a method-by-method and object-by-object basis.
The resulting JDBC code tends to use row-by-row operations, with many JDBC-SQL calls inefficiently handling only one row of a table in the database.
Don't query for one row at a time from a table; try to get all the rows you will need.
Even more usefully, SQL consists of more than SELECT and INSERT.
Instead of using the database purely as a data store, you can construct sophisticated SQL statements that combine queries, processing, and updates without bringing lots of temporary data back to the Java application for local processing.
Combining multiple operations is a good way to take advantage of the efficiencies available in relational databases, and we will discuss batching operations and stored procedures in later sections.
On the database side of the JDBC communication, the database needs to process the data and received SQL instructions, execute the accesses and updates required to satisfy the SQL, and return any requested data.
The more work the server has to do, the longer the operation takes.
The performance-tuning target is to minimize the server's work without disproportionately burdening the Java application.
Some techniques for reducing database work are relatively simple: avoid doing work that doesn't need to be done.
This rule seems obvious, but I've seen unnecessary calls to uppercase data (using upper( )) too many times.
Examine your SQL and the table structure to decide if the SQL adds unnecessary database-side work.
Do you really need to access the same table multiple times? Can you change existing rows of data rather than adding or deleting rows? Each time you avoid changing the table by moving, deleting, or adding rows, you've reduced the database workload slightly.
If you can construct the application to use existing dummy rows instead of inserting rows, the SQL runs faster.
If the database knows a connection is readonly, it does not need to cache new pages, maintain journal entries, or acquire write locks on the data.
Multiway joins invariably degrade performance, and performance gets worse as data volume increases.
Try working with as few tables as possible within each SQL clause.
Queries of large tables that do not use an index require far more processing than tables with indexes.
Further, two checks can help you assess the efficiency of SQL statements.
First, the Connection class includes a nativeSQL( ) method that converts the SQL you provide into the system's native SQL.
This gives the driver an opportunity to convert standard SQL into SQL optimized for the particular database.
This method rarely returns anything other than the SQL string you pass to it, but it's always worth checking.
For databases that support EXPLAIN, you can preface a statement with the keyword EXPLAIN and the database produces an explanation of how it will execute the SQL, including whether an index is used.
Querying the table again with an added index and using an equijoin condition:
Transferring and converting data represents a significant proportion of the cost in many JDBC operations.
However, rather than minimize the costs, try to completely avoid transferring the data.
Efficient SQL lets you minimize the data that is transferred.
Using the * character in the SELECT clause asks the database to return all fields in the rows.
I recently sampled some applications and found that out of thousands of requests to various databases, fewer than 10 requests needed all the fields in the rows queried.
I don't really find the extensive use of SELECT * surprising, though.
During development, it is much easier to use the SELECT * query, especially since the required fields can change.
Then the fields are accessed from the ResultSet row by field index.
Later, converting queries to request only the required fields means changing both the SQL and the indexes of the accessed fields in the ResultSet rows, a bug-prone change in all but the simplest JDBC applications.
Several factors come into play; there is no conversion overhead from the unused fields (in most drivers), and the transfer overhead depends on the size of the unused fields, the number of transferred rows, and the transfer batch size, too! There is no doubt that the latter, more precise SELECT is more efficient, but whether the effort to change the queries to the latter SELECT is worthwhile for your application and query is a judgment call.
In highly scaled systems, with many queries going to the database, every field counts.
A second type of inappropriate transfer, requesting too many rows, is less frequently a problem.
It is easier in development to specify the required rows precisely by using the appropriate clauses.
One common technique that results in transferring too many rows is sorting rows with a SORT BY clause, and then using only the top.
I saw a more realistic example of transferring too many rows in an application that had been taken over by different developers.
Rather than create new queries in the JDBC section of the application, the new developers simply checked data items and discarded those that didn't apply to the new set of criteria.
Effectively, they made a SELECT * query, and then executed a WHERE clause in Java by iterating through the ResultSet and collecting the rows that satisfied the new criteria.
The driver needs to map the name to the column number, which at best requires one extra round trip for the column mapping, and at worst can result in multiple round trips and significant amounts of extra data transferred from the database.
For example, MySQL and mSQL retrieve all rows from the database before accessing a value by name.
Use the column number to access fields, even though doing so may require extra information about table structures to be maintained in the application.
Caching data in the client is a highly effective technique for speeding up applications using JDBC.
The questions you need to ask are which data, and how to cache it.
Cache small read-only tables and tables that are updated infrequently.
Even medium-sized tables are worth considering, though you'll need to test how much space they take up in their final cached form.
Large tables and rapidly updated or frequently changing tables are clearly not good candidates, though select rows from large tables may be if they are rarely changed and are used more often than other rows.
Bearing in mind that any processing through JDBC produces overhead, the best place to cache JDBC-obtained data is after you have processed it (i.e., after it is extracted from the ResultSet and converted into objects or object data)
This is an especially good option if you are in the design or early implementation stage of your project, when you can most effectively build in object caching.
If you are in a late stage of development, you may be able to modify your application so it caches objects, but that depends on the existing design, considering how the data is used and where it ends up.
The second option is to cache at the ResultSet layer.
You cannot use a ResultSet object as the cache object itself because it uses JDBC resources that need to be closed.
However, the techniques discussed and used in the first section of the chapter show how to implement wrappers for most kinds of JDBC objects, including ResultSets.
You can even create a mostly transparent cache with wrappers.
Just add a Hashtable to the ConnectionWrapper that maps SQL query strings to StatementWrappers and ResultSetWrappers.
The ResultSetWrapper itself should wrap the original ResultSet accessed from the database and simply iterate through all the rows, obtaining the data to hold locally.
For example, you might read all the data looping through the ResultSet to collect all rows as an array of String fields:
It is possible to support parameterized queries using similar but considerably more sophisticated wrappers, but the effort is beyond what I can present here.
Finally, an in-memory database product is another reasonable option that can provide relatively transparent database caching, though with a higher overhead.
Several commercial versions and an open source product called hsqldb (available from http://hsqldb.sourceforge.net/) are available.
The memory overhead for such products is small, and they work just like a normal relational database, with their own JDBC driver, so you can switch between databases by switching drivers for queries or by redirecting queries with Connection wrappers.
Use such products to replicate the tables you want to cache and keep in memory.
An in-memory relational database opens up another interesting performance-tuning option.
You can use the database to update cached rows in exactly the same way as you would the proper database.
Because the update mechanism is SQL, you can log the SQL statements locally to disk as you execute them on the inmemory database, a process called journaling.
Then the SQL statements can be applied to the actual database separately and asynchronously, to synchronize it with the in-memory database.
Since SQL is standardized, you can use the same SQL easily for both databases.
When a database executes an SQL statement, it performs two separate tasks.
First, it searches through the data and indexes to determine which data items satisfy the SQL statement.
However, behind the scenes, the database has a second task: to work out exactly how to execute the SQL.
Parse the SQL to see if it is properly constructed.
Identify whether there are indexes for this search (indexes are not specified in the SQL, nor should they be)
These and other tasks combine to form the query plan.
When the database "prepares" a statement, it creates a query plan.
Now the difference between Statement and PreparedStatement may be clearer: Statement prepares and executes the query plan each time, while PreparedStatement prepares the query plan once and then reuses the query plan.
Preparing a statement is also referred to as precompiling a statement.
If that were the whole story, then PreparedStatement would always be the statement of choice, and you would avoid Statement objects completely.
Statement has optimizations that the database can apply; mainly, the database knows that the Statement plan is executed immediately and thrown away.
Statement queries can be prepared and executed in one swoop, using the state of the database at the time, without allocating resources to keeping the plan around.
For example, the query plan would need to be updated or re-created in the case of some types of changes to the database, depending on how detailed the query plan is.
Creating and executing a Statement once is faster than creating and executing a PreparedStatement once.
But PreparedStatement has a one-off hit in its preparation stage, so after the preparation stage is completed, any single PreparedStatement execution should be faster than a single Statement execution.
Note that I say to create and execute the PreparedStatement in the initialization phase.
Although logically, the PreparedStatement should be prepared when the object is created, the API allows the driver to choose whether to prepare the statement at object-creation time or when the first execution takes place.
However, if you cannot prepare your statements during an initialization phase, the choice is more difficult.
You can choose a Statement or PreparedStatement object, depending on which provides the fastest overall cumulative time for repeated queries.
For example, in Java Programming with Oracle JDBC (O'Reilly), Donald Bales looks at the cost of table inserts using Oracle drivers and finds that the statement needs to be repeated Statements.
Inserts have an overhead different from that of access queries, and different drivers and databases have different overhead, so test your configuration to see where the crossover happens if you need to be precise.
If you have spare time in which a PreparedStatement can be initialized with one execution, use PreparedStatement with this separate initialization.
If you cannot separately initialize the statement in spare time and you execute the statement only once, use Statement.
If you cannot separately initialize the statement in spare time and you execute the statement only a few times (say, less than10), use Statement.
If you cannot separately initialize the statement in spare time and you execute the statement many times, use PreparedStatement.
If you cannot separately initialize the statement in spare time but it is acceptable for the first execution of the statement to be slower, use PreparedStatement.
First, the SQL needs to be identical for the query plan to be reusable.
Fortunately, PreparedStatements support parameterized queries so you can repeat the same query with different parameters.
This code executes the previous two queries efficiently by using parameterized statements:
The pooling happens completely under the covers, controlled by the driver.
Indeed, a simplistic approach, which assumes that the JDBC/database automatically understands that the same SQL should use the same query plan without worrying about which Connection object and which PreparedStatement are used, can actually work with JDBC 3.0
Moving to JDBC 3.0 might solve your particular PreparedStatement reuse issues.
First, the JDBC API does not specify how PreparedStatements or Statements should be implemented.
The differences in performance between Statements and PreparedStatements are recommendations based on what I've seen, read about, and expect from documented features of various databases.
But they are not guaranteed: always check the driver and database documentation and test for expected performance improvements.
This overhead can be costly, especially for repeated or looped executions of SQL statements.
Parameterized SQL statements that are supported by PreparedStatements can help you avoid creating extra strings for repeated SQL statements.
Relational databases are designed to be operated optimally with SQL, which is a set-processing language.
Furthermore, operating on one table at a time is normally faster than interleaving operations between multiple tables.
You can take advantage of batching in JDBC in two ways: on access and on update.
These two types of operations are batched very differently, so we'll look at them separately.
You may need to check how any particular driver supports batching.
Some have restrictions, such as only supporting update batching with one type of Statement (Oracle batches updates only with PreparedStatement) or one mode of ResultSet (DB2 doesn't batch when used with scrollable ResultSets)
Essentially, the amount of data (or, more specifically, the number of rows) read by one call to the database is configurable.
Transferring data efficiently is important because network communication time affects performance more than any other factor.
If you access the default number of rows, access is batched automatically.
In practice, rather than being critical for performance, for most applications, adjusting access batch size is more like fine-tuning performance.
If you retrieve only a few rows, the default fetch size may be too large.
But there is not usually a large cost in having too large a fetch size as long as the data volume of the default fetch size is not so large that the.
Typical table rows are not large enough to cause fragmenting.
Nevertheless, reducing the fetch size to be in line with the number of rows you actually read should improve performance by reducing overhead slightly at both ends of the communication (in the JDBC layer and at the database-access layer)
If you regularly access more than the default fetch size, then you can reduce the number of network trips by increasing the fetch size.
This can make a big difference, depending on how much data you retrieve.
Depending on how the data is processed in the application, this change could alleviate a significant bottleneck.
The tradeoff to increasing the fetch size is increased memory use.
All fetched data has to be held in the JDBC layer on the client, and this memory can add up excessively if you use a large default batch size that applies to every request.
The fetched data is held in the ResultSet object generated by executing a query.
Batching updates simply means sending multiple update statements in one transaction and one call to the database.
A more transparent mechanism would use wrappers, constructed in a similar way to the wrappers covered in the first section of this chapter.
For example, Oracle provides this mechanism in its driver as an alternative batching model.
Databases are designed to hold certain types of data more efficiently than others.
The basic data types, such as numeric and string, normally have fixed sizes and often a fixed range of formats.
These restrictions are mainly for performance; they enable the database to be efficient in access and update.
Dealing with fixed-size data having a known format means that you don't need to search for terminators within the data, and knowing the format reduces the number of branch points necessary to handle many formats.
Databases are optimized for these basic data types and formats.
The closer you can come to holding data in your application in the same format required by the database, the more easily you can achieve good JDBC performance.
The data you currently use is frequently updated and accessed in the database.
Other types of data, such as historic or archived data, are not accessed frequently by the database.
However, if you use the same set of tables for both types of data, then the database has no choice but to pull both sets of data into its cache and search through both types of data.
So separating tables of frequently and infrequently used data allows the database to optimize its handling of frequently used data.
It also reduces the possibility that you will need to search through result sets for data items you need.
The data in the database is stored in a different format than the data held in your Java application.
Use the type-correct get( ) method rather than getObject( )
The ResultSet object has a large number of different get( ) methods.
Each method typically accesses the underlying data item and converts the data into the required Java data type or object type.
The closer the underlying data is to the resulting Java data type, the more efficient the conversion.
Strings are usually held in relational databases as single-byte character data (ASCII) and get converted to Unicode two-byte Strings in Java.
You can reduce the conversion cost by storing String and char data as Unicode two-byte characters in the database.
But bear in mind that doing so will cause your database size to grow—in the worst case, doubling the size of the database.
Finally, if you use data types that are not primitive data types, use the most appropriate data type available from the database.
For example, almost every database has a date type, and you should store your dates in that date type and not in the more generic varchar data type.
But note that relying on database-specific data types, although good for performance, can compromise portability.
Metadata is information about how data is structured in the database, such as the size of a column or its numeric precision.
There are a number of considerations for dealing with metadata.
In fact, unless your application is specifically designed to deal with the possibility that the metadata will change, you can assume that the metadata for a particular table and metadata about database features are constant for the lifetime of a particular JVM.
Metadata can be expensive to obtain from the database compared to most other types of queries.
Thus, if you use metadata, obtain it once and cache the data.
Many DatabaseMetaData methods are quite slow, and executing them repeatedly causes a bottleneck.
A null argument has one fewer criterion to restrict a search, which makes the search more intensive.
ResultSetMetaData is more efficient than DatabaseMetaData, so try to use the former.
For example, Don Bales points out that an Oracle SELECT statement (not a prepared statement) makes two round trips to the database: the first for metadata and the second for data.
The query can then avoid the first database trip to query for metadata.
Apart from optimizing metadata queries themselves, using metadata information also allows you to optimize standard queries.
Using this data allows you to optimize your query to use indexes in the query when possible.
The columns returned by this query can include pseudocolumns not available from the getColumns( ) method.
Pseudocolumns can provide the database with a pointer to an internal database location, which allows the database to optimize the operations that include pseudocolumns.
At the very least, record and analyze them for handling in future versions of your application if you cannot handle the exceptions directly.
In addition, not correctly handling JDBC exceptions can leave resources in use but idle, creating resource leakage that inevitably decreases performance over time.
Note that JDBC exceptions often enclose other underlying exceptions, so don't forget to chain through them.
Stored procedures are defined in the database and can be executed from a remote call similar to SQL.
Stored procedures have some overhead associated with execution, but are precompiled and executed entirely on the database server.
Stored procedures are more efficient than PreparedStatements and Statements when the stored procedure is fairly complex, equivalent to many complex SQL statements.
Stored procedures are also relatively efficient if they circumvent intermediate round trips between the client and the database—for example, when a procedure would consist of several separate calls to the database while a stored procedure requires only one call.
On the other hand, the procedure-call overhead for stored procedures indicates that they are inefficient compared with any single simple SQL call.
Replacing each SQL call in your JDBC application with an equivalent call to a stored procedure with the same functionality, a one-for-one replacement of calls, would probably be inefficient, making performance worse.
Another consideration is the relative processing availability of the database server and the database client.
In some cases, the database server may be underutilized compared to the database client, especially when the database client is middleware such as an application server.
In this case, stored procedures can move some of the processing to the database server, making better use of the available CPU power on both machines.
Generally, a stored procedure can improve performance if it replaces any of the following:
Multiple calls to the database with one call to the stored procedure.
Java-side processing when there is spare CPU capacity on the server.
Don't use a stored procedure to replace simple SQL calls.
Stored procedures are best used to avoid moving data back and forth across the network.
And stored procedures are not an option if full database portability needs to be maintained, as they are different for each database.
Stored procedures are not standardized, so use the syntax specific to your database.
Here's a simple example of a stored procedure that takes a parameter:
The very best performance-tuning advice is to avoid doing what doesn't need to be done.
A database needs to maintain all sorts of different resources to ensure that the ACID properties (Atomicity, Consistency, Isolation, and Durability) apply to a transaction, irrespective of any simultaneous operations and of whether the transaction ends with a commit or rollback.
If you can avoid forcing the database to do some of that work, performance improves.
The first way to minimize transaction costs is to combine multiple operations into one transaction.
Turning off auto-commit and making your transactions explicit requires more work, but pays off if you can combine transactions (see "Batching" earlier in this chapter), especially when you scale the system.
On heavily loaded systems, the overhead from transactions can be significant, and the lower that overhead is, the more the system can scale.
Taking manual control over transactions doesn't mean that you should extend the transactions for long periods to catch lots of operations in one transaction.
Leaving transactions open keeps locks on rows, which affects other transactions and reduces scalability.
If you have several operations that could be executed sequentially, then combine them into one transaction, but otherwise keep transactions as short as possible.
Combining operations into one transaction may require extra conditional logic in the SQL statements and possibly even temporary tables.
Even so, this is more efficient than not combining those operations because the database can obtain all the required locks and release them in one step.
Multiple transactions result in more communication overhead, more lock-and-release phases, and a greater likelihood of conflict with other sessions.
Everything occurs in a transaction as if no other operations are occurring in the database during the transaction; that's roughly what the ACID properties mean.
As already noted, this gives the database a large amount of work.
Nevertheless, these transaction levels are useful for many types of operations because they provide better performance with acceptable transactional functionality.
Not all levels defined in the JDBC Connection interface are supported by all databases.
But, of course, no changes can be committed to the database, so it's adequate for read access and temporary row or table creation: any changes are rolled back automatically when the connection is closed.
The API states that dirty, nonrepeatable, and phantom reads can all occur at this transaction level (see the sidebar Some Non-ACID Transaction Problems)
Supported by DB2, not by Oracle, this transaction level represents the atomicity of ACID: your changes are treated as if they all happen at the same time if committed, or as if none happen if rolled back.
The API states that dirty reads are prevented with this transaction level, but nonrepeatable and phantom reads can both occur.
This level is supported by DB2 and by Oracle, where it is the default level.
The API states that dirty and nonrepeatable reads are prevented at this transaction level, but phantom reads can occur.
It is technically implemented by placing locks on rows that are read or written and holding the locks until the end of the transaction.
This is the slowest transaction level, but is fully ACID-compliant.
At this level, transactions are serialized: your transaction is deemed to have taken place in its entirety as if all other committed transactions have either taken place in entirety before or after this transaction.
The API states that dirty, nonrepeatable, and phantom reads are prevented with this transaction.
Look for read queries in the application, and consider for each whether any of the problems outlined in the sidebar adversely affect the query for the given data, data update pattern, and query.
Reads of static tables or tables that are updated only by the same transaction that reads them can safely use the lowest transaction level.
Transactions controlled by the user are another significant JDBC bottleneck.
Any transaction that requires a user to execute an action (such as clicking OK or Cancel) to close the transaction (i.e., to commit or abort it) is a problem waiting to happen.
Inevitably, one or more users forget to terminate the activity or are forced to leave the activity unfinished.
Resources are left open in the application and database, and any concurrent activity can easily conflict with the locked resources, resulting in decreased performance throughout the system.
Only single-user applications, or applications that are effectively single-user because they don't share any of the same resources between users, are immune to this problem.
The main alternative to leaving a user in control of a JDBC transaction is to use optimistic transactions.
Optimistic transactions collect information for update outside of a JDBC transaction and then use a mechanism to check that the update doesn't conflict with any other update that may have been processed in between.
Mechanisms for checking for optimistic conflicts include timestamps, change counters, and checking for differences from an expected state.
For example, when the application gathers user-input data for the update, the data can be sent to the database as a batch set of SQL statements that includes timestamped safeguards that make sure the original data in the database is the same as the data originally used by the client.
A successful transaction updates the rows (including the timestamps) to indicate newly changed data.
If another update that invalidates the user's changes is made by another user, the timestamps will have changed, and the current transaction needs to be rolled back instead of committed.
Usually, no intermediate conflicting transactions have occurred, so most transactions succeed.
When a transaction fails, the application displays the entered data along with the changes that caused the conflict.
Optimistic transactions are unsuitable for applications that consist of multiple transactions that frequently write data concurrently.
This pattern of activity would result in frequently failing optimistic transactions and an increase in overall resource usage.
This feature looks very nice, and I would be surprised if designers using JDBC aren't champing at the bit to use it.
The javax.sql package includes XADataSource and XAConnection interfaces to support distributed transactions.
However, distributed transactions are significantly slower than normal transactions because of all the extra communication required to coordinate the connections.
The following examples use a table called TABLE_SIZES that has two columns, TABLENAME and TABLESIZE.
A dirty read occurs when a transaction can see uncommitted changes to a row.
A nonrepeatable read occurs when a row that is not updated during the transaction is read twice within a transaction with different results.
If your transaction reads a value and another transaction commits a change to that value, your transaction can read that changed value even though your transaction has not committed or rolled back.
A phantom read occurs when a transaction reads a row inserted by another transaction that has been committed.
If another transaction inserts a row to a table, when your transaction queries that table, it can read the new row even if a previous query in the transaction did not read that row.
It provides you with a tuning option of where to run the database.
Running the database on the same machine as the JDBC client means that the JDBC call has a reduced overhead from avoiding a network call.
On the other hand, it also means that all the processes run on the same machine, increasing the load on that machine.
If the client process needs to access the same disks or access through the same disk controller as the database, I/O can be compromised drastically.
Applications that are not too large or complex, that have simple database access and update requirements, and that do not use up most of the machine's processing capacity and memory, are probably better off running the database on the same machine.
But make sure that the database uses a dedicated disk controller with dedicated disks, so that database disk I/O is not reduced.
Applications that are large or complex, such as application servers and other multi-user server systems, are usually better off running the database on a separate machine.
Beware: testing both configurations at lower scales may show that collocating the database on the same machine as the application provides better performance.
However, you can be sure that when the system is scaled up, the system resource requirements of the application and the database will conflict severely, causing reduced system scalability.
I/O can show up as significant in profiling even if I/O is not itself the bottleneck.
It is worthwhile to have separate measurements available for the JDBC subsystems.
Use JDBC wrappers to measure the performance of database calls.
ResultSet.next( ) can spend a significant amount of time fetching rows from the database.
Use JDBC 3.0 or the latest JDBC version if possible.
Optimize the SQL to apply set-based (multi-row) processing rather than one row at a time.
Avoid moving, deleting, and adding rows where possible: use preinserted and null value rows.
Cache row and field data rather than re-query to access the same data.
Using a wrapper, you can transparently cache rows and tables.
Use parameterized PreparedStatements except where a statement will be executed only a few times and there is no spare time to initialize a PreparedStatement.
Reuse the connection associated with the PreparedStatement unless the connection pool supports PreparedStatement pooling (as JDBC 3.0 does)
Create SQL query strings statically, or as efficiently as possible if created dynamically.
Batch updates with executeBatch( ), explicitly managing the transaction by turning off auto-commit.
Try to closely match Java data types and database data types.
Use metadata queries to reduce the amount of transfers needed in subsequent database communications.
Consider using stored procedures to move some execution to the database server.
Don't use a stored procedure to replace any simple SQL calls.
Stored procedures are best used to avoid moving data back and forth across the network.
Take manual control of transactions with explicit begin and commit calls, turning off auto-commit mode, and combining close transactions to minimize the overall transaction costs.
Use the lowest transaction isolation level that won't corrupt the application.
Small, lightly used databases can be efficiently located on the same machine as the application server; otherwise, another machine is probably better.
Since JSPs are compiled into servlets, there is usually little difference in performance between them.
However, explicitly coding servlets can result in faster applications than JSPs because you have much more control over the fine details of the servlet.
This is one of the standard performance tradeoffs: using JSPs results in a better design (separating presentation and logic), but servlets can be tweaked to go faster.
Essentially, if your servlet implements SingleThreadModel, the servlet engine creates a separate servlet instance for each concurrent request using the servlet.
SingleThreadModel does not even guarantee thread safety, since the resulting servlet instances can still access classes and data at the same time.
However, SingleThreadModel does guarantee that more resources will be used than are needed, as maintaining the multiple instances has some cost to the servlet engine.
Instead of using SingleThreadModel, concentrate on writing a thread-safe multi-threaded servlet.
See Section 10.4.2 for details on writing efficient thread-safe code.
Writing your servlet with big synchronized blocks may be highly thread-safe but won't scale.
For example, the following rather extreme implementation synchronizes the entire servlet activity:
However, if your servlet received an average of one request per second and the processing took an average of two seconds, then your servlet can process an average of only one request every two seconds—i.e., half the requests.
This is independent of the CPU capability of the server.
You could have plenty of spare CPU power; indeed, if you had eight CPUs, this implementation would leave seven of them mostly idle.
The result is that the server listen queue would fill up quickly, and your servlet (actually, the TCP stack) would simply reject half the connections.
Try to minimize the amount of time spent in synchronized code while still maintaining a thread-safe servlet.
Use as many servlet threads as are needed to handle the request throughput.
The larger the request service time, the greater the number of threads you need to maintain adequate response times for a given rate of requests.
The following principles will help you optimize page creation and output and thus improve the performance of your servlet.
If you are logging from your servlet, log only essential data and buffer the output.
Don't turn off logging completely; monitoring servlet performance is essential to maintaining good performance, and eliminating logging is counterproductive.
For both logging and page output, use the print( ) methods in preference to the println( ) methods, where appropriate.
System.out, println( ) can cause output to be flushed, prematurely ending the effectiveness of buffer optimization.
For HTML output, println( ) adds only nonsignificant whitespace to the output, adding overhead with no benefit.
Time spent constructing HTML page output is significant for many servlets.
When you need to build strings internally, use StringBuffers or other efficient String or byte arraybuilding mechanisms.
Simply using chars does not cause any overhead; the conversion between chars and bytes creates overhead (i.e., bytes are read and written on the socket)
You will output the results, and the output goes to a network buffer.
Although the network buffer flushing is not under your control, it will be consistent for any one platform, so try to find its operational capabilities.
Use the network buffer by using print( ) on partial strings rather than building the strings in memory and then writing them.
However, the network stack can be suboptimal in flush timing.
Tests by Acme Laboratories identified that the amount of data sent in the first network packet was crucial to optimal performance.
Sending the response headers and the first load of data as a single packet instead of as two separate packets improved performance significantly (see http://www.acme.com/software/thttpd/benchmarks.html)
You can gain improved performance by taking advantage of this fact, using static pages and page elements when possible.
Browsers take time to calculate how much space each element should take.
Precalculate all formatting that is constant for all generated HTML pages.
Use cached in-memory Strings or byte arrays to hold the portions of pages that are common to multiple pages.
This should be faster than repeatedly generating the same elements.
The headers are usually mostly the same, and most web sites have a look and feel that involves the same elements in many pages.
Formatting precalculation is done automatically for JSP pages in the compilation phase.
High-volume web applications pre-render pages that are the same for all users.
Those pages can be served directly from a separate web server optimized for serving static pages, taking away a significant load from the servlet.
Cache these pages or sections, and regenerate them only when they need to change.
Even more efficient than returning a cached page is to tell the browser to use its own cached page.
As part of its request, the browser can send a header telling the server that it has a cached copy of the requested page, including when the copy was cached.
The server can reply that the cached copy is valid without resending the page, in which case the browser simply displays the page from its cache.
This capability is supported by servlets through the getLastModified( ) method.
Implement the getLastModified( ) method in your servlet to return the page's last modified timestamp and allow the browser to use its cached page when possible.
Optimize data conversions that you need to make when generating your HTML output.
For example, use timestamps instead of formatted Dates, or if you need to format a Date, don't do so from scratch each time.
Instead, use a partially cached formatted Date and fill in the changed values.
The date changes only once a day, hours only change once an hour, etc.
There is seldom any requirement to display the current time in a page, and even when it is required, it cannot be accurate to the second because of download time and time differences between machines.
JSPs cannot do this; they always use the PrintWriter, which is one of the reasons why a JSP may be slightly slower than the equivalent servlet.
A JSP could forward to a plain servlet when binary data needs to be sent.
If you will use a PrintWriter, initialize its buffer in the constructor with the optimal size for the pages you write.
Flushing the HTML output in sections lets the browser display partial pages more quickly.
But bear in mind that the browser can display partial pages if it has enough information, so try to send the page sections that help the browser display partial pages quickly.
Explicitly flush those sections, rather than waiting for the network buffer to fill and flush the data, to give the user the impression of a faster display.
Various uses of body tags can make a difference in performance.
The useBean action has a scope associated with the bean created by the action.
You will need to use a wider scope (request, session, or application) to use beans with pages that are included.
Custom tags have a performance cost, but they are useful.
BodyTags are more costly to performance than simpler custom tags.
Using BodyTags to iterate on the page section contents makes the page significantly slower.
On the other hand, Jim Elliott wanted me to point out that custom tags are so much better than the alternative of mixing presentation and logic that eliminating custom tags may not be worth the performance gain.
The forward is a simple call that is internal to the servlet, and a redirect tells the browser to make a new request to the redirected target page.
Caching can improve the responsiveness of a web site significantly.
You should cache pages or page sections for a set length of time rather than update the page or section with each request.
Cache tags allow pages and sections of pages to be cached.
When the page is executed, the cache tag either retrieves the data from its cache or creates the data and caches it.
Cache tags can work on a per-user basis, so they are fairly flexible.
An open source cache-tag library, Open Symphony's OSCache, is available from http://www.opensymphony.com/oscache/
Serge Knystautas describes how cache tags can improve performance in a JavaWorld article.[1] You can also use the application server's caching facility, and the session and application objects' caching facilities with ServletContext and HttpSession's getAttribute( )/setAttribute( ) methods.
Note that "context" has a much wider scope than "session," so use the HttpSession methods for session-related resources.
Using the context can reduce scalability by having resources open over multiple sessions unnecessarily.
Optimal caching needs tuning of timeout settings and other parameters.
Cache elements reused in many pages need to be monitored to ensure that they do not become bottlenecks.
On highly personalized web sites, page-level caching can result in low cache-hit rates, as each page can be mostly unique to each user.
In this case, cache tags are of limited use (perhaps only for small page fragments)
Under the covers, this association is typically done by using a cookie.
If you don't need to maintain session state, there is no need to use HttpSession.
If you are maintaining session state, HttpSession seems to provide adequate performance as long as you are aware of the points covered in the following sections.
However, do bear in mind that too low a setting will really annoy your users if it means that they have to reestablish the session state.
On the other hand, leaving session objects around too long can be a heavy drain, especially if each session uses anything significant in the way of server resources.
Therefore, this method is a classic performance-tuning parameter, requiring optimization on the basis of testing the application to see what value is best.
You can also have pages automatically refresh themselves with embedded page commands, and these pages can keep a session alive indefinitely, even when the page is no longer in use.
Each time the page reloads, the session timeout counter is reset.
A number of sources recommend that you use HttpSession objects to manage session state rather than using stateful beans.
However, in their book J2EE Performance Testing with BEA WebLogic Server (Expert Press), Peter Zadrozny, Philip Aston, and Ted Osborne state that there is no real difference in performance between these two options, as long as the beans are explicitly removed from the container when the session expires.[2] The beans are removed automatically by binding the session to a session listener that calls ejbRemove( ) when the session terminates.
Beans not removed are passivated, which imposes a large overhead on the system and causes enormous performance degradation.
The cited test situation was idealized; the sessions were always removed before the test terminated, and the beans were removed when the session terminated.
In a production system, lingering sessions can be a problem.
Consequently, for optimal performance across the board, use HttpSession rather than stateful session beans to maintain state.
If you prefer stateful session beans for design reasons, ensure timely session termination and bean removal.
HttpSession objects can be serialized by the servlet engine under certain situations: different conditions in different servlet engines cause this to happen.
Memory conditions and session longevity are the two primary reasons for serialization of HttpSession objects.
Longevity can be minimized by timing out sessions; memory usage is best minimized by reducing the number and size of objects stored in the HttpSession.
If your HttpSession is serialized, the smaller the graph of objects reachable from the HttpSession, the faster the serialization will be.
Try to avoid storing large object graphs in the HttpSession, use transient variables wherever possible to avoid serializing objects unnecessarily, and bear in mind the costs of serialization when considering what is stored in the HttpSession.
Spreading your requests across multiple application servers helps make the application more scalable.
If you are maintaining state, you may need to replicate your sessions across the application servers to handle requests that may be distributed across the servers.
If you use a frontend load balancer for your application distribution, then you should ensure that the load balancer can support "sticky" sessions—i.e., that it automatically routes any particular session to the application server handling that session.
A software load balancer may need to be programmed to handle sticky sessions.
If replication of sessions is a definite requirement, then building your own session mechanism is probably better than using HttpSessions.
HttpSession identifiers are not unique or consistent across multiple servers.
You can build your own session mechanism without too much difficulty to replicate the HttpSession functionality while ensuring that the session mechanism is optimized for distribution.
A file-based distributed session mechanism, implemented by altering URLs to encode the session identifier, is described in Budi Kurniawan's article "Pseudo Sessions for JSP, Servlets and HTTP."[3] Many web sites use this method of session management.
Optimize your use of HttpSession objects by following these guidelines, some of which summarize earlier tips:
The servlet init( ) and destroy( ) methods are ideal for creating and destroying limited and expensive resources, such as cached objects and database connections.
The servlet init( ) method is a good place to perform once-only operations.
You can use the jspInit( ) and jspDestroy( ) methods in the same way as init( )and destroy( )
Most popular browsers accept GZIP-compressed pages, decompressing them before displaying them to the user without the user knowing that the page was compressed.
If the page size is large—or more precisely, the compressed page compared to the uncompressed page is so much smaller that the download time is consistently measurably reduced—then compressing pages is a worthwhile option.
To fully determine the potential benefit of compressing pages, you also need to factor in the extra CPU load and time on the server to compress the file (and the extra time to decompress the file on the client, though you can usually ignore this if the download time is significantly improved)
In practice, there is a heavier load on the server, but a significantly faster download for limited bandwidth clients.
The mechanics of the HTTP support for compressed pages follows.
First, the browser tells the server that it can handle GZIP-compressed pages by including the header Accept-Encoding: gzip in the page request.
The server can sense that the browser accepts GZIP compression quite easily by looking for the header with code like this:
In practice, you shouldn't waste time compressing small pages, since there is no gain in network transfer time, so you should test to see if the page is big enough to warrant compression.
You could also use a servlet filter instead of building compression support directly into the servlet.
The effect is the same as the code shown earlier.
Sun has contributed a servlet compression filter to the examples supplied with the Tomcat application server, and a JavaWorld article [4] by Jason Hunter describes the filter in more detail.
If you can cache the compressed version of the page the first time you write it, or statically compress the page prior to starting the servlet, then you gain the benefits of compression with none of the overhead.
Servlet filters add overhead to servlet processing, so the nonfilter solution is slightly more efficient.
However, the filter solution is probably much easier to add to existing deployments.
You will get better performance if you can turn security checking off.
Try to optimize the servlet-loading mechanism (for example, by listing the servlet first in loading configurations)
Instead, log the raw IP addresses and use a separate process to execute reverse DNS lookups to supplement the logs.
Precompile your JSPs to avoid giving the first user a slow experience.
Either run the page once before making it public (which compiles it), or use the application server's features to precompile the servlet.
Event-driven applications can often be scaled more easily than process-driven applications.
Servlet filters provide a standardized technique for wrapping servlet calls.
However, they have some overhead, which translates to a reduction in performance.
Separate the UI controller logic from the servlet business logic, and let the controllers be mobile so they can execute on the client, if possible.
Validate data as close to the data-entry point as possible, preferably on the client.
Business workflow rules should be on the server (or further back than the frontend)
You could use invisible applets in a browser to validate data on the client, but the extra time required to download the applet may make this unusable.
The listen queue is a TCP/IP-level queue for incoming socket accepts and is set with the second argument to the ServerSocket constructor (if you are explicitly creating a server socket)
The application server must expose the parameter as a configuration option for you to adjust this.
Auto-reloading is a development feature of many application servers that repeatedly reloads servlets and JSPs.
Turn this feature off for performance tests and deployed systems.
Tune pools in the application server (see Chapter 18 for details)
Transform your data to minimize the costs of searching it.
If your dataset is small enough, read it all into memory or use an in-memory database (keeping the primary copy on disk for recovery)
An in-memory database avoids overhead in several ways: it eliminates passing data in from a separate process, reduces memory allocation by avoiding data copies as it passes between processes and layers, eliminates the need for data conversion, and enables fine-tuned sorting and filtering.
Precalculation expedites some results by making the database data more efficient to access (by ordering it in advance, for example), or by setting up extra data in advance, generated from the main data, to simplify result calculations.
The procedure they followed to achieve very fast response time is instructive.
The SPECmine tool itself is a JSP that allows a user to query the SPEC database of benchmarks (http://www.spec.org/)
The query page (at http://www.aceshardware.com/SPECmine/index.jsp) allows the user to specify all the parameters for the query, including how to sort the results.
The query is so efficient that most of the transaction time is taken by network communication and browser page display.
The SPEC database is accessible in a number of different ways, but none provides the full set of data required by the SPECmine tool.
Querying the SPEC database each time the SPECmine tool was used would have required multiple connections, data transformations, and parsed and cleaned data.
Holding the data locally was an obvious solution, but more than that, holding the data locally in a format optimal for the SPECmine tool query was the best solution.
This required the SPEC database to be checked periodically for new entries.
New entries must be cleaned and transformed for the SPECmine database.
To clean and transform the data, parses and regular expression conversions were changed to table maps, which are easier to maintain, cleaner, and faster.
Data was now held locally, so the SPECmine query was local rather than remote (across the Internet)
Data was held in an optimal format for the SPECmine query so that only one query was required to obtain the query result, rather than multiple queries together with data processing.
New SPEC entries could be added to the SPECmine database asynchronously, at off-peak time, with no performance degradation to the SPECmine query engine.
The only disadvantage was that the SPECmine tool would occasionally be out of synch with the SPEC database; i.e., the SPEC database would occasionally hold data that was not available from a SPECmine query.
This is perfectly acceptable for the application, and the user was warned of this pitfall.
The delay between SPEC data entry and SPECmine update could be minimized by increasing the frequency of checking for new data, if this option was ever desired.
The amount of data in the SPECmine database (and the projected future amount of data) was quite small: megabytes rather than hundreds of megabytes or gigabytes.
Furthermore, rather than simply map in the data directly, Ace's Hardware decided to convert the data into a Java object format when mapping it in instead of converting data for each query.
The result was a very fast in-memory query for the SPECmine tool, requiring minimal extra processing when a query was executed.
The main disadvantage was that the application was now more complex than it was with a traditional JDBC query: custom querying and sorting capabilities were required.
Locking, data integrity, and scaling would have become issues had the database been larger (or had it required concurrent updates)
In that case, the in-memory custom solution would have been less practical, and in-memory caching would be used instead (and used for other sections of the web site)
There were two main types of optimizations: optimizing query requests with precalculation and reducing String manipulation costs.
You can do this in a small amount of memory by holding an array of sorted elements, with each element pointing to its main entry holding the full data corresponding to that element.
Filtering the presorted array for the elements matching the search criteria gives you a sorted result set.
Another optimization used the fact that a list of strings presented to the user in a list selection box can return indexes to the servlet instead of returning the selected strings.
This means that you can use the indexes in an array, rather than the strings, as keys to a Map.
For SPECmine, the indexes were used with a boolean array to determine which strings were "on" in the search filter.
The remaining String manipulation optimizations eliminated duplicate String objects, avoided unnecessary String concatenations, and precalculated HTML String elements that do not need to be dynamically generated.
The final optimization applied was the GZIP-compression support outlined earlier in this chapter.
The application's speed was such that the search itself was the fastest part of the service, HTML generation took significantly more time, and compression, network transfer, and browser display took most of the total time.
The original report also discussed other parts of the web site, including optimizing parts of the site that need disk-based databases.
Ace's Hardware goes into the overall architecture of their JSP-based web site in more detail at http://www.aceshardware.com/read.jsp?id=45000240
The site serves about 1 million users per month and displays ten times as many pages, illustrating that high performance can be achieved using servlets and JSPs, without excessive resources or tuning.
Make the servlet thread-safe, but try to minimize the amount of time spent in synchronized code while still maintaining a thread-safe servlet.
Use as many servlet threads as necessary to handle the request throughput.
Use resource pools to distribute resources among the servlet threads.
The amount of data sent in the first network packet is crucial to optimal performance.
Send the response headers and the first load of data as a single packet instead of two separate packets.
Use StringBuffers or other efficient String or byte array-building mechanisms.
Use the browser's caching mechanism to have pages reread by correctly implementing the getLastModified( ) method.
High-volume web applications prerender pages that are the same for all users.
Avoid creating HttpSession objects if not needed, and time out HttpSessions when they are needed.
The servlet init( ) and destroy( ) or jspInit( ) and jspDestroy( ) methods are ideal for creating and destroying limited and expensive resources, such as cached objects and database connections.
Use Servlet 2.3 application and session events to make the application event-driven.
Remember that servlet filters have overhead associated with the filter mechanism.
Validate data at the client if it can be done efficiently.
Transform your data to minimize the costs of searching it.
The performance of EJB-based J2EE systems overwhelmingly depends on their design.
If you get the design right, tuning the server is similar to tuning a J2SE system: profile and tune the server, targeting object creation as a priority (since the consequences in a multiuser system are an order of magnitude greater)
If you get the design wrong, you are unlikely to simply tweak your way to adequate performance.
In contrast, a J2SE application can often achieve adequate performance with a nonoptimal design after sufficient performance tuning.
This design sensitivity is one of the reasons why J2EE design patterns have become so popular: design patterns assist everyone from novices to experienced designers in achieving adequate performance.
This design sensitivity is also the reason for the many stories about badly performing EJB projects.
In exchange for the ability to have a standard for components that developers, managers, tool vendors, and other third-party producers all work together to use, there are some overheads and design issues.
It is also usually more difficult to maintain and support.
EJBs have third-party support products for development, testing, tuning, deploying, scaling, persisting, clustering, and load balancing.
If you don't need standardized components, EJBs may not be the best option for your project.
A plain J2SE + JDBC solution has its own advantages.
In Section 18.2 later in this chapter, I describe several design patterns that help EJB systems attain adequate performance.
But first, I will discuss some primary design guidelines to consider before you can apply patterns.
EJBs should be designed to have large granularity—one remote invocation to an EJB should perform a large amount of work instead of requiring many remote invocations.
This criterion is extremely important for a successful EJB design.
Coarse-grained EJBs tend to provide a more efficient application because they minimize the number of remote communications needed to complete the work.
A more refined guideline is that any remotely accessed EJBs should be coarse-grained.
Any EJBs that are always accessed locally can be fine-grained, if the local access is not treated as a remote access.
Bear in mind that prior to the EJB 2.0 specification, all EJB access was (theoretically) treated remotely, even with EJBs in the same container.
This means that the parameters could always be marshaled and passed through a socket, incurring a significant portion of remote-calling overhead.
Some application servers detect local EJB communication automatically and optimize that communication to avoid remote-calling overhead.
Since EJB 2.0, local entity beans can be defined, allowing optimized communications for local EJBs.
But that is not a runtime decision, so it needs to be factored into the design.
Local EJBs were added to the EJB specification to address this issue of improving performance among locally collocated EJBs.
The following are some detailed guidelines for achieving this combination design target of coarse-grained remote EJBs and fine-grained local EJBs.
In the following list, I consider EJBs either local or remote, but an EJB can implement both interfaces, if appropriate to your application.
Design the application to access entity beans from session beans.
This optimizes the likelihood that an EJB call is local and supports several other design optimizations (listed in the subsequent section covering design patterns)
Determine which EJBs will be collocated within the same VM.
These EJBs can communicate with one another by using optimized local communications.
Defined normally as remote EJBs and collocated within an application server that is capable of optimizing local EJB communications; or.
Built as normal JavaBeans, and then wrapped in an EJB to provide one coarse-grained EJB (see the CompositeEntity pattern)
EJBs that communicate remotely should combine methods to reduce possible remote invocations.
Multiple calls frequently specify various parameters, and these parameters can be combined as a parameter object to be passed for one remote call.
Section 12.2 gives a concrete example of how to combine methods to reduce the number of remote calls required to perform an action.
Don't design EJBs with one access method per data attribute unless they are definitely local EJBs.
More accurately, don't define data attribute accessors and updators as remote, as they have relatively high overheads.
Bear in mind that any EJB service could be called remotely if you define a remote interface for it, and try to anticipate the resulting costs to the application.
EJBs should not be simple wrappers on database table rows.
An EJB should be a fully fledged business object that represents and can manipulate underlying database data, applying business logic to provide appropriate refined information to callers of the EJB.
If you need to access database data, but not for business-object purposes, use JDBC directly (probably from session beans) without intermediate EJB objects.
While this inefficiency can be justified when the EJB adds information value to the data, it is pure overhead in the absence of such business logic, and plain JDBC could be optimized much better.
Read-only data should be identified and separated from read-write data.
When treating read-only data and read-only attributes of objects, a whole host of optimizations are possible.
Some optimizations use design patterns, and others are available from the application server.
Transactions that consist purely of read-only data are much more efficient than read-write data.
Trying to decouple read-only data from read-write data after the application has been designed is difficult.
That means that all the services it provides do not depend on what it just did.
So a single stateless session bean can serve one client, then another, and then come back to the first, while each client can be in a different or the same state.
The stateless session bean doesn't need to worry about which client does what.
The result is that one stateless bean instance can serve multiple clients, thereby decreasing the average number of resources required per client.
The stateless bean pool doesn't need to grow and shrink according to the number of clients; instead, it can be optimized for the overall rate of requests.
As each bean services multiple clients, the bean pool can be kept smaller, which is more optimal.
To optimize the session-bean pool for your application, choose a (maximum) size that minimizes activations and passivations of beans.
The container dynamically adjusts the size to optimally handle the current request rate, which may conflict with trying to choose a single size for the pool.
Stateful beans, in contrast, require one instance for each client accessing the bean.
The stateful-bean pool grows and shrinks depending on the current number of clients, increasing pool overhead.
If you have stateful beans, try to remove any that are finished so that fewer beans are serialized if the container needs to passivate them (see Section 17.5.2, which details how explicitly removing beans can improve performance)
If you have stateful beans in your design, the best technique to reduce their overhead is to convert them to stateless session beans.
Primarily, this involves adding parameters that hold the extra state to the bean methods to pass the bean the current client state whenever it needs to execute.
An extended example of converting a stateful bean to a stateless bean is available in Brett McLaughlin's Building Java Enterprise Applications, Volume I: Architecture (O'Reilly), and online at.
The example even shows that you can retain the stateful-bean interface while using stateless beans by using the Proxy design pattern.
If state needs to be accessible on the server, you can hold it outside session beans, for example, in an HttpSession object, or in a global cache that provides access to the state through a unique session identifier.
Converting stateful session beans to stateless session beans adds extra data to the client-server transfers, but the extra data can be minimized by using identifiers and a server data store.
For highperformance J2EE systems, the advantages tend to outweigh the disadvantages.
There is even a dedicated pattern for caching EJBHome objects (the EJBHomeFactory pattern) because it is such a frequently suggested optimization.
Should you use container-managed persistence (CMP) or bean-managed persistence (BMP)? This is one of the most frequently discussed questions about EJBs.
You could build a very fast, simple persistence layer, mainly raw JDBC calls, but it would not be flexible enough for the kinds of development changes constantly made in most J2EE systems.
However, if speed is the top priority, this option is viable.
You can build in the persistency that is required by the bean, avoiding any generic overhead.
That's fine if you have five EJB types in your application.
But more realistically, with tens or hundreds of EJB types, writing optimal BMP code for each EJB and keeping that code optimal across versions of the application is unachievable (though again, if you can impose the required discipline in your development changes, then it is achievable)
With multiple beans and bean types, CMP can apply many optimizations:
Efficient combinations of multiple queries to the same table (i.e., multiple beans of the same type that can be handled together)
Optimized multi-row deletion to handle deletion of beans and their dependents.
However, CMP is not yet mature, which makes the judgment more complex.
It may come down to which technique your development team is more comfortable with.
If you do use CMP, profile the application to determine which beans cause bottlenecks from their persistency.
Use the Data Access Object design pattern (described later) to abstract your BMP implementations so you can take advantage of optimizations for multiple beans or database-specific features.
You may also need to use BMP where CMP cannot support the required logic—e.g., if fields use stored procedures, or one bean maps to multiple tables.
Tuning EJB transactions is much like tuning JDBC transactions; you will find Section 16.2.15 very relevant for EJB transactions.
Commit the data after the transaction completes rather than after each method call.
That is, if multiple methods are executed close together, each needing to execute a transaction, then combine their transactions into one transaction.
The target is to minimize the overall transaction time rather than simplistically targeting each currently defined transaction.
Use read-only in the deployment descriptor to avoid unnecessary calls to ejbStore( ) by the application server (not all application servers support this feature)
Choose the lowest-cost transaction isolation level that avoids corrupting the data.
Don't use client-initiated transactions in the EJB environment because long-running transactions increase the likelihood of conflict, making rows inaccessible to other sessions.
If the client controls the duration of the transaction, you may have no way to force the transaction to close from the server, thus allowing long or indefinite transactions.
The longer a transaction lasts, the more likely it is to conflict with another transaction.
If you need client-initiated transactions, set an appropriate transaction timeout in the ejb-jar.xml deployment descriptor file.
Setting a timeout ensures that the application doesn't start leaking resources from transactions that are opened at the client but not completed.
Declare nontransactional methods of session beans with NotSupported or Never transaction attributes (in the ejb-jar.xml deployment descriptor file)
Use a dirty flag where supported by the EJB server or in a BMP or DAO implementation to avoid writing unchanged EJBs to the database.
Dirty flags are a standard way to avoid writing unchanged data.
The write is guarded with the dirty flag and performed only if the flag is dirty.
Initially the flag is clean, and any change to the EJB sets the flag to dirty.
However, because of their importance to EJB design, this section lists design patterns that are particularly relevant.
Use a Value Object to encapsulate all of a business object's data attributes and access the Value Object remotely rather than accessing individual data attributes one at a time.
The Value Object sends all data in one network transfer.
Section 12.2 shows how to use the Value Object pattern to reduce the number of network transfers required to access multiple data attributes.
One variation, the Value Object Assembler pattern, uses a Session EJB to aggregate all required data from different EJBs as various types of Value Objects.
Using a Value Object may result in very large objects being transferred if too many data attributes are combined into one Value Object.
A large Value Object may still be more efficient than separate multiple remote requests, but typically, only a subset of the data held by a large Value Object is needed, in which case the large Value Object should be broken down into multiple smaller Value Objects, each holding the data subset required to satisfy its remote request.
This last approach minimizes both the number of network transfers and the amount of transferred data.
Once transferred, the Value Object's data is no longer necessarily up to date.
So if you use the Value Object to hold the data locally for a period of time (as a locally cached object), the data could be stale and you might need to refresh it according to your application's requirements.
Use Data Access Objects to decouple business logic from data-access logic, allowing decoupling of data-access optimizations from other types of optimizations.
Data Access Objects usually perform complex JDBC operations behind a simplified interface, providing a platform for optimizing those operations.
Data Access Objects allow optimizations in bulk access and update for multiple EJBs, and also allow specialized optimizations by using database-specific optimized access features while keeping complexity low.
For read-only access to a set of data that does not change rapidly, use the Fast Lane Reader pattern, which bypasses the EJBs and uses a (possibly nontransactional) Data Access Object that encapsulates access to the data.
The Data Access Object in the Fast Lane Reader pattern accesses the database to get all the required read-only data efficiently, avoiding the overhead of multiple EJB accesses to the database.
The resulting data is transferred to the client using a Value Object.
The Value Object can also be cached on the server for repeated use, improving performance even further.
This means that the Fast Lane Reader pattern efficiently reads unchanging (or slowly changing) data from the server and displays all of the data in one transfer.
If long lists of data are returned by queries, use the Page-by-Page Iterator pattern.
This pattern is used when the result set is large and the client may not need all of the results.
It consists of a server-side object that holds data on the server and supplies batches of results to the client.
When the client makes a request, the results of the request are held in a stream-like object on the server, and only the first "pageful" of results is returned.
The client can control the page size, and when data from the next page needs to be viewed, the whole page is sent.
Section 12.7 shows how to use a Page-by-Page Iterator pattern to reduce the amount of transferred data and improve client display time.
Note that the Page-by-Page Iterator pattern actually increases the number of transfers made.
However, it is an essential pattern for any server handling multiple requests that may return large amounts of data to clients.
When implementing the Page-by-Page Iterator pattern, you should try to avoid making copies of the data on the server.
If the underlying collection data is concurrently altered, care should be taken to ensure the client gets consistent pages.
There is no upper limit to the size of a result set that this pattern can handle.
The ValueListHandler pattern avoids using multiple Entity beans to access the database.
Instead, it uses Data Access Objects that explicitly query the database and return the data to the client in batches rather than in one big chunk, as in the Page-by-Page Iterator pattern.
The Service Locator pattern improves performance by caching service objects with a high lookup cost.
For example, EJBHome objects and other JNDI lookups are often costly, but need to be performed regularly.
However, many such objects are infrequently changed and thus ideal for caching.
The Service Locator pattern simply interposes a Service Locator between the object initiating the lookup and the actual lookup.
The Service Locator caches any looked-up object and returns the cached object where possible.
The Verified Service Locator pattern anticipates that objects in the Service Locator cache occasionally become stale and need to be refreshed.
The Verified Service Locator periodically and asynchronously tests the cache elements to identify and refresh stale objects.
An asynchronous periodic test minimizes the impact of stale objects to callers of the service, which would otherwise require a time-consuming synchronous call to obtain a refreshed service object.
The Verified Service Locator pattern element management is appropriate for JNDI lookups, when cache elements need to be refreshed only when the JNDI server is restarted, which should be infrequently.
The EJBHomeFactory pattern is simply a ServiceLocator dedicated to EJBHome objects.
It is such a frequently mentioned optimization that it was given its own name.
The CompositeEntity pattern reduces the number of actual entity beans by wrapping multiple Java objects.
The Factory pattern allows optimizations to occur at the object-creation stage by redirecting object-creation calls to a factory object.
Use the Builder pattern to break the construction of complex objects into a series of simpler Builder objects.
A Director object combines the Builders to form a complex object.
You can then use Recycler (a type of Director) to replace only the broken parts of the complex object, reducing the number of objects that need to be re-created.
The Optimistic Locking pattern checks for data integrity only at update time and uses no locks.
This feature increases the scalability of an application compared to pessimistic locking, since lock contention is avoided.
The Optimistic Locking pattern is appropriate when concurrent access predominates over concurrent update (i.e., most sessions spend most of their time reading data, and very little time writing data)
The updated row contains a timestamp field that should not be newer than when the row was accessed or the transaction started.
A simple version counter is maintained and checked to ensure that it matches the version at transaction beginning.
At update time, all relevant database data is checked to ensure that it matches the "old" data.
Optimistic locking has high rollback costs when conflicts are detected, so it should not be used when conflicts are frequent.
The Reactor pattern demultiplexes events and dispatches them to registered object handlers.
It is similar to the Observer pattern (not described here), but the Observer handles only a single source of events, whereas the Reactor pattern handles multiple event sources.
The Reactor pattern enables efficient load-balancing servers with multiplexing communications.
The multiplexing of network I/O using NIO Selectors is an excellent example of the Reactor pattern.
The Front Controller pattern centralizes incoming client requests, channeling all client requests through a single decision point that lets you balance the application at runtime (see also Section 15.6)
This pattern also allows optimizations in aggregating the resulting view.
Proxy and Decorator objects let you redirect, batch, multiplex, and delay method invocations.
They enable application partitioning by intelligently caching data or forwarding method invocations.
The Proxy pattern differentiates by Proxies often instantiating their real objects, while the Decorator pattern rarely does.
A Proxy object is usually created as a wrapper on the "real" object, and other objects only ever get to handle the Proxy.
The Decorator is more typically given the "real" object to wrap, allowing access to both the Decorator and the "real" object.
Synchronized wrappers are an example of the Decorator pattern: you can pass the original collection object to the wrapper factory and access both the original collection and the wrapped collection.
Considerations other than performance frequently drive the choice of application server.
That might not be as serious as it sounds, since all the most popular application servers target good performance as an important feature.
I often read about projects in which the application server was exchanged for an alternative, with performance cited as a reason.
However, these exchanges seem to be balanced: for each project that moved from application server A to application server B, there seems to be another that moved in the reverse direction.
Nevertheless, I would still recommend that application servers be evaluated with performance and scalability as primary criteria.
The ECperf benchmark may help differentiate EJB server performance within your short list of application servers.
Application servers should offer multiple caches for session beans, EJBs, JNDI, web pages, and data access.
Caching provides the biggest improvement in performance for most enterprise applications.
Load balancing is absolutely necessary to support clustered systems efficiently.
If one part of the system goes down, a fault-tolerant system suffers performance degradation.
However, a system without fault tolerance has no service until the system is restarted.
You can roll your own connection pool, but one should come standard with any application server.
It is necessary to efficiently manage system resources if your application uses hundreds or thousands of threads or serves hundreds or thousands of users.
All subsystems, including RMI, JMS, JDBC drivers, JSP tags, and cacheable page fragments, should be optimized, and the more optimized, the better.
The latest VMs with threaded garbage collection may not benefit from this option.
Supported directly by the application server, distributed caching with synchronization lets clustered servers handle sessions without requiring that a particular session always be handled by one particular server, enhancing load balancing.
Optimistic transactions reduce contention for most types of applications, enabling the application to handle more users.
If you need distributed transactions, they are usually handled more efficiently if the application server supports them.
Holding session state information in memory allows clustered servers to handle sessions without requiring that a particular session be handled by one particular server, enhancing load balancing.
Of course, your application may have its own single points of failure.
Hot-deployment lets you do so with almost no downtime, enhancing 24/7 availability.
Try to dedicate separate application servers to handle secure transactions.
Most types of security (SSL, password authentication, security contexts and access lists, and encryption) degrade performance significantly.
Many systems use the frontend load balancer to decrypt communications before passing on requests.
In any case, try to consider security issues as early as possible in the design.
The gross configuration of the system might involve several different servers: application servers, web servers, and database servers.
An optimal configuration runs these servers on different machines so each has its own set of specifically tuned resources.
When this separation is not possible, you need to be very careful about how the servers are configured.
Allocate separate disks, not just separate partitions, to the various servers.
Make sure that the operating-system page cache is on yet another disk.
Limit memory requirements so it is not possible for any one server to take an excessive amount of memory.
Set process priority levels to appropriately allocate CPU availability (see Chapter 14 for more details)
When request rates increase, you should be able to maintain performance by simply adding more resources (for instance, an extra server)
This target requires both a well-designed application and correctly configured application servers.
Try load-testing the system at higher scales with an extra application server to see how the configuration requirements change.
Application servers have multiple configuration parameters, and many affect performance: cache sizes, pool.
Some configurations are optimal for read-write beans, and others are for readonly beans, etc.
The popular application-server vendors now show how to performance-tune their products (see http://www.JavaPerformanceTuning.com/tips/appservers.shtml)
The single most important tuneable parameter for an application server is the VM heap size.
For long-lived server VMs, memory leaks (or, more accurately, object retention) are particularly important to eliminate.
Another strategy is to distribute the application over several server VMs.
This distribution spreads the garbage-collection impact, since the various VMs will most likely collect garbage at different times.
Optimal cache and pool sizing are the next set of parameters to target.
Caches are optimized by trying to get a good ratio of hits to misses (i.e., when an attempt is made to access an object or data from the cache, the object or data is probably in the cache)
Too small a cache can result in useful objects/data being thrown away to make way for new objects/data.
Too large a cache uses up more memory than is required, taking that memory away from other parts of the system.
Look at the increase in cache-hit rates as memory is increased, and when the rate of increase starts flattening out, the cache is probably at about the right size.
Each pool has its own criteria that identify when it is correctly sized.
Well-sized bean pools minimize activation and passivation costs, as well as bean creation and destruction.
A well-sized connection pool minimizes the amount of time requests have to wait for an available connection.
If the connection pool can vary in size at runtime, the maximum and minimum sizes should minimize the creation and destruction of database connections.
For thread pools, too many threads causes too much context switching; too few threads leaves the CPU underutilized and decreases response times because requests get queued.
Other parameters depend on what the application server makes available for tuning.
For example, as connections come into the server, they are queued in the network stack "listen" queue.
If many client connections are dropped or refused, the TCP listen queue may be too short.
However, not all application servers allow you to alter the listen queue size.
A few additional tuning suggestions for EJBs are listed here:
You can use the Factory pattern with new to avoid the filesystem check.
Tune the message-driven beans' pool size to optimize the concurrent processing of messages.
The .NET optimizations appear mostly to have been SQL optimizations together with moving much of the application server logic to database-stored procedures.
You may also want to check out the discussion of the Oracle improvements in the Server Side (http://www.theserverside.com/home/thread.jsp? thread_id=12753)
The original Pet Store didn't try to optimize data handling.
All information that might be needed is automatically loaded on the client.
This is unrealistic in real-world applications that should minimize data transfers.
The Pet Store made no attempt to optimize the SQL queries.
This lack of optimization is appropriate for a tutorial, where the most simple SQL is easier to understand.
Oracle changed the EJBs so they use isModified( ) to avoid unnecessary database updates.
It is always good practice to avoid doing what doesn't need to be done.
This is a good example of using a dirty flag, discussed earlier in Section 18.1.7
These methods were rewritten to use only one connection at a time, reducing contention and increasing scalability.
The Pet Store application default settings produced too much unnecessary data.
Oracle used the Pageby-Page Iterator pattern with limited page size to improve performance and scalability.
Session data was moved from the ServletContext to the HttpSession, and the JSP was modified to use the session rather than the context.
Without this change, multiuser access to the Pet Store application was very limited, as all catalog access to the DB was forced through a single connection.
Connection code was rewritten to keep the DB connections very short, as is optimal with connection pooling.
String-handling code was rewritten to use StringBuffer instead of String, removing unnecessary concatenations.
The combined effect of these optimizations from Oracle produced a greater than 400-fold improvement in performance.
Elite.com is a successful Internet startup subsidiary of Elite Information Group, and provides an online time and billing solution.
The report covers only a few performance enhancements, but they show some of the most common high-level J2EE performance issues:
Elite.com includes a queueing subsystem that asynchronously accepts external communications, such as email entries.
Entries can be batched and run with minimal impact on the online system.
Like most enterprise applications, Elite.com experienced conflicting concurrent access to some resources.
When this caused severe decreases in performance in one subsystem, Elite.com solved the problem by using a resource pool (an EJB connection pool shared among servlets), improving the subsystem performance.
Moving components so they are local to each other can significantly improve performance by eliminating marshalling and remote-transfer overhead.
Collocating the EJBs and servlets and converting the communication to local calls can speed performance dramatically.
Explicitly remove beans from the container when a session is expired.
Leaving beans too long will get them serialized by the container, which can dramatically decrease performance.
Remote EJB calls should be combined to reduce the required remote invocations.
Design the application to access entity beans from session beans.
Collocated EJBs should be defined as Local EJBs (from EJB 2.0), collocated within an application server that can optimize local EJB communications, or built as normal JavaBeans and then wrapped in an EJB to provide one coarse-grained EJB (CompositeEntity design pattern)
EJBs should not be simple wrappers on database data rows; they should have business logic.
If you have stateful beans in your design, convert them to stateless session beans by adding parameters that hold the extra state to the bean methods.
Optimize read-only EJBs to use their own design, their own application server, read-only transactions, and their own optimal configuration.
Profile the application to determine which beans cause bottlenecks from their persistency, and implement bean-managed persistence (BMP) for those beans.
Use the Data Access Object design pattern to abstract your BMP implementations so you can take advantage of optimizations possible when dealing with multiple beans or database-specific features.
Minimize the time spent in any transaction, but don't shorten transactions so much that you are unnecessarily increasing the total number of transactions.
Combine transactions that are close in time to minimize overall transaction time.
This may require controlling the transaction manually (i.e., turning off auto-commit for JDBC transactions or using TX_REQUIRED for EJBs)
Choose the lowest-cost transaction isolation level that avoids corrupting the data.
Don't leave transactions open, relying on the user to close them.
There will inevitably be times when the user does not close the transaction, and the consequent long transaction will decrease the performance.
Bulk or batch updates are usually more efficiently performed in larger transactions.
Use the Factory pattern with new to avoid the filesystem check.
Tune the message-driven beans' pool size to optimize the concurrent processing of messages.
Tune the application server's JVM heap, pool sizes, and cache sizes.
Many of these profilers have been reviewed in the various magazines listed previously.
You can usually search the magazine web sites to identify which issue of the magazine provides a review.
The profiler vendors should also be happy to provide pointers to reviews.
The annual "best of Java" awards includes a section for profilers (see the Java Developer's Journal)
TowerJ environment (compiler & runtime) from Tower Technology Corporation (http://www.towerj.com)
Our look is the result of reader comments, our own experimentation, and feedback from distribution channels.
Distinctive covers complement our distinctive approach to technical topics, breathing personality and life into potentially dry subjects.
Our look is the result of reader comments, our own experimentation, and feedback from distribution channels.
Distinctive covers complement our distinctive approach to technical topics, breathing personality and life into potentially dry subjects.
Emma Colby designed the cover of this book, based on a series design by Edie Freedman.
The cover image is a 19th-century engraving from the Couvier Pictorial Archive.
The tip and warning icons were drawn by Christopher Bing.
The online edition of this book was created by the Safari production group (John Chodacki, Becki Maisch, and Madeleine Newell) using a set of Frame-to-XML conversion and cleanup tools written and maintained by Erik Ray, Benn Salter, John Chodacki, and Jeff Liggett.
Object class changing to record object creation clone( ) hashCode( ) toString( ) wait( ) and notify( ) methods object creation garbage collection, avoiding flattening objects general guidelines to limit object generation mapping objects with arrays primitive data types, using guidelines for efficient memory usage initialization early and late lazy initialization performance checklists preallocating objects ternary search tree optimization profiling Reference objects SoftReference flushing types reusing objects canonicalizing objects pool management reusable parameters ThreadLocal objects statistics on StreamTokenizer vs.
PreparedStatement, when to use statements combining multiple operations Stack class  2nd stack traces exception overhead and exceptions, using without sampling with -Xrunhprof stacks recursion and size per thread, setting TCP/IP, performance of startup of applets of caches from disk sweet spots overhead for processes responsiveness of timings and of VMs with threaded class loading stateful beans converting to stateless pooling session beans HttpSession vs.
StreamTokenizer avoiding in long to string conversion reducing number of temporary variables of primitive data types vs.
VectorPoolManager class (example) vectors handles to, memory reuse caused by pool management and for threads verbose option Verified Service Locator pattern in EJB design version management, hot-deploy and hot-undeploy applications for virtual memory VMs (virtual machines)  [See also -verbosegc option] -verbosegc option Sun JDK 1.2 output with -Xloggc:<file> option application distribution over client/server modes, calculation of free space in heap EJBs located within, local communications among eliminating disabled assertion statements faster other optimizations startup time variations in speed generating helpful information for.
