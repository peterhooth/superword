Many of the designations by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Purpose/Goals This new Java edition describes data structures, methods of organizing large amounts of data, and algorithm analysis, the estimation of the running time of algorithms.
As computers become faster and faster, the need for programs that can handle large amounts of input becomes more acute.
Paradoxically, this requires more careful attention to efﬁciency, since inefﬁciencies in programs become most obvious when input sizes are large.
By analyzing an algorithm before it is actually coded, students can decide if a particular solution will be feasible.
For example, in this text students look at speciﬁc problems and see how careful implementations can reduce the time constraint for large amounts of data from centuries to less than a second.
Therefore, no algorithm or data structure is presented without an explanation of its running time.
In some cases, minute details that affect the running time of the implementation are explored.
Once a solution method is determined, a program must still be written.
As computers have become more powerful, the problems they must solve have become larger and more complex, requiring development of more intricate programs.
The goal of this text is to teach students good programming and algorithm analysis skills simultaneously so that they can develop such programs with the maximum amount of efﬁciency.
This book is suitable for either an advanced data structures (CS7) course or a ﬁrst-year graduate course in algorithm analysis.
Students should have some knowledge of intermediate programming, including such topics as object-based programming and recursion, and some background in discrete math.
Summary of the Most Signiﬁcant Changes in the Third Edition The third edition incorporates numerous bug ﬁxes, and many parts of the book have undergone revision to increase the clarity of presentation.
Chapter 4 includes implementation of the AVL tree deletion algorithm—a topic often requested by readers.
Chapter 5 has been extensively revised and enlarged and now contains material on two newer algorithms: cuckoo hashing and hopscotch hashing.
Additionally, a new section on universal hashing has been added.
Chapter 7 now contains material on radix sort, and a new section on lower bound proofs has been added.
Chapter 12 adds material on sufﬁx trees and sufﬁx arrays, including the linear-time sufﬁx array construction algorithm by Karkkainen and Sanders (with implementation)
The sections covering deterministic skip lists and AA-trees have been removed.
Approach Although the material in this text is largely language independent, programming requires the use of a speciﬁc language.
As the title implies, we have chosen Java for this book.
Java offers many beneﬁts, and programmers often view Java as a safer, more portable, and easier-to-use language than C++
As such, it makes a ﬁne core language for discussing and implementing fundamental data structures.
Other important parts of Java, such as threads and its GUI, although important, are not needed in this text and thus are not discussed.
Complete versions of the data structures, in both Java and C++, are available on the Internet.
We use similar coding conventions to make the parallels between the two languages more evident.
Overview Chapter 1 contains review material on discrete math and recursion.
I believe the only way to be comfortable with recursion is to see good uses over and over.
Chapter 1 also presents material that serves as a review of inheritance in Java.
Many examples are provided, including an in-depth explanation of logarithmic running time.
Simple recursive programs are analyzed by intuitively converting them into iterative programs.
More complicated divide-and-conquer programs are introduced, but some of the analysis (solving recurrence relations) is implicitly delayed until Chapter 7, where it is performed in detail.
It now includes a discussion of the Collections API ArrayList and LinkedList classes, and it provides implementations of a signiﬁcant subset of the collections API ArrayList and LinkedList classes.
Chapter 4 covers trees, with an emphasis on search trees, including external search trees (B-trees)
The UNIX ﬁle system and expression trees are used as examples.
Data structures for an external medium are considered as the ﬁnal topic in several chapters.
New to this edition is a discussion of the Collections API TreeSet and TreeMap classes, including a signiﬁcant example that illustrates the use of three separate maps to efﬁciently solve a problem.
Chapter 5 discusses hash tables, including the classic algorithms such as separate chaining and linear and quadratic probing, as well as several newer algorithms, namely cuckoo hashing and hopscotch hashing.
Universal hashing is also discussed, and extendible hashing is covered at the end of the chapter.
Binary heaps are covered, and there is additional material on some of the theoretically interesting implementations of priority queues.
It is very speciﬁc with respect to coding details and analysis.
All the important general-purpose sorting algorithms are covered and compared.
Four algorithms are analyzed in detail: insertion sort, Shellsort, heapsort, and quicksort.
New to this edition is radix sort and lower bound proofs for selection-related problems.
External sorting is covered at the end of the chapter.
Chapter 8 discusses the disjoint set algorithm with proof of the running time.
This is a short and speciﬁc chapter that can be skipped if Kruskal’s algorithm is not discussed.
Algorithms on graphs are interesting, not only because they frequently occur in practice, but also because their running time is so heavily dependent on the proper use of data structures.
Virtually all the standard algorithms are presented along with appropriate data structures, pseudocode, and analysis of running time.
To place these problems in a proper context, a short discussion on complexity theory (including NP-completeness and undecidability) is provided.
Chapter 10 covers algorithm design by examining common problem-solving techniques.
Pseudocode is used in these later chapters so that the student’s appreciation of an example algorithm is not obscured by implementation details.
Chapter 12 covers search tree algorithms, the sufﬁx tree and array, the k-d tree, and the pairing heap.
This chapter departs from the rest of the text by providing complete and careful implementations for the search trees and pairing heap.
The material is structured so that the instructor can integrate sections into discussions from other chapters.
Chapters 1–9 provide enough material for most one-semester data structures courses.
A graduate course on algorithm analysis could cover Chapters 7–11
The advanced data structures analyzed in Chapter 11 can easily be referred to in the earlier chapters.
The discussion of NP-completeness in Chapter 9 is far too brief to be used in such a course.
You might ﬁnd it useful to use an additional work on NP-completeness to augment this text.
Exercises Exercises, provided at the end of each chapter, match the order in which material is presented.
The last exercises may address the chapter as a whole rather than a speciﬁc section.
Difﬁcult exercises are marked with an asterisk, and more challenging exercises have two asterisks.
References References are placed at the end of each chapter.
Generally the references either are historical, representing the original source of the material, or they represent extensions and improvements to the results given in the text.
Visit the IRC or contact your campus Pearson representative for access.
Acknowledgments Many, many people have helped me in the preparation of books in this series.
Some are listed in other versions of the book; thanks to all.
As usual, the writing process was made easier by the professionals at Pearson.
I’d like to thank my editor, Michael Hirsch, and production editor, Pat Brown.
I’d also like to thank Abinaya Rajendran and her team in Integra Software Services for their ﬁne work putting the ﬁnal pieces together.
My wonderful wife Jill deserves extra special thanks for everything she does.
Finally, I’d like to thank the numerous readers who have sent e-mail messages and pointed out errors or inconsistencies in earlier versions.
In this chapter, we discuss the aims and goals of this text and brieﬂy review programming concepts and discrete mathematics.
See that how a program performs for reasonably large input is just as important as its performance on moderate amounts of input.
Most students who have had a programming course or two would have no difﬁculty writing a program to solve this problem.
One way to solve this problem would be to read the N numbers into an array, sort the array in decreasing order by some simple algorithm such as bubblesort, and then return the element in position k.
A somewhat better algorithm might be to read the ﬁrst k elements into an array and sort them (in decreasing order)
As a new element arrives, it is ignored if it is smaller than the kth element in the array.
Otherwise, it is placed in its correct spot in the array, bumping one element out of the array.
When the algorithm ends, the element in the kth position is returned as the answer.
Both algorithms are simple to code, and you are encouraged to do so.
The natural questions, then, are which algorithm is better and, more important, is either algorithm good enough? A simulation using a random ﬁle of 30 million elements and k = 15,000,000 will show that neither algorithm ﬁnishes in a reasonable amount of time; each requires several days of computer processing to terminate (albeit eventually with a correct answer)
An alternative method, discussed in Chapter 7, gives a solution in about a second.
Thus, although our proposed algorithms work, they cannot be considered good algorithms, because they are entirely impractical for input sizes that a third algorithm can handle in a reasonable amount of time.
Again, there are at least two straightforward algorithms that solve the problem.
For each word in the word list, we check each ordered triple (row, column, orientation) for the presence of the word.
This amounts to lots of nested for loops but is basically straightforward.
Alternatively, for each ordered quadruple (row, column, orientation, number of characters) that doesn’t run off an end of the puzzle, we can test whether the word indicated is in the word list.
It is possible to save some time if the maximum number of characters in any word is known.
It is relatively easy to code up either method of solution and solve many of the real-life puzzles commonly published in magazines.
Suppose, however, we consider the variation where only the puzzle board is given and the word list is essentially an English dictionary.
Both of the solutions proposed require considerable time to solve this problem and therefore are not acceptable.
However, it is possible, even with a large word list, to solve the problem in a matter of seconds.
An important concept is that, in many problems, writing a working program is not good enough.
If the program is to be run on a large data set, then the running time becomes an issue.
Throughout this book we will see how to estimate the running time of a program for large inputs and, more important, how to compare the running times of two programs without actually coding them.
We will see techniques for drastically improving the speed of a program and for determining program bottlenecks.
These techniques will enable us to ﬁnd the section of the code on which to concentrate our optimization efforts.
This section lists some of the basic formulas you need to memorize or be able to derive and reviews basic proof techniques.
In computer science, all logarithms are to the base 2 unless speciﬁed otherwise.
Therefore, X + Y = Z, which proves the theorem.
Some other useful formulas, which can all be derived in a similar manner, follow.
Another type of common series in analysis is the arithmetic series.
Any such series can be evaluated from the basic formula.
The next two formulas pop up now and then but are fairly uncommon.
There are many theorems that apply to modular arithmetic, and some of them require extraordinary proofs in number theory.
We will use modular arithmetic sparingly, and the preceding theorems will sufﬁce.
The two most common ways of proving statements in data structure analysis are proof by induction and proof by contradiction (and occasionally proof by intimidation, used by professors only)
The best way of proving that a theorem is false is by exhibiting a counterexample.
Most mathematical functions that we are familiar with are described by a simple formula.
For instance, we can convert temperatures from Fahrenheit to Celsius by applying the formula.
There are several important and possibly confusing points about recursion.
A common question is: Isn’t this just circular logic? The answer is that although we are deﬁning a method in terms of itself, we are not deﬁning a particular instance of the method in terms of itself.
The two most important issues are probably the how and why questions.
Using recursion for numerical calculations is usually a bad idea.
In Chapter 3, the how and why issues are formally resolved.
With recursive programs, there is no such thing as a “special case.”
These considerations lead to the ﬁrst two fundamental rules of recursion:
You must always have some base cases, which can be solved without recursion.
For the cases that are to be solved recursively, the recursive call must always be to a case that makes progress toward a base case.
Throughout this book, we will use recursion to solve problems.
As an example of a nonmathematical use, consider a large dictionary.
Words in dictionaries are deﬁned in terms of other words.
When we look up a word, we might not always understand the deﬁnition, so we might have to look up words in the deﬁnition.
Likewise, we might not understand some of those, so we might have to continue this search for a while.
Because the dictionary is ﬁnite, eventually either (1) we will come to a point where we understand all of the words in some deﬁnition (and thus understand that deﬁnition and retrace our path through the other deﬁnitions) or (2) we will ﬁnd that the deﬁnitions are circular and we are stuck, or that some word we need to understand for a deﬁnition is not in the dictionary.
Our recursive strategy to understand words is as follows: If we know the meaning of a word, then we are done; otherwise, we look the word up in the dictionary.
If we understand all the words in the deﬁnition, we are done; otherwise, we ﬁgure out what the deﬁnition means by recursively looking up the words we don’t know.
This procedure will terminate if the dictionary is well deﬁned but can loop indeﬁnitely if a word is either not deﬁned or circularly deﬁned.
Printing Out Numbers Suppose we have a positive integer, n, that we wish to print out.
Assume that the only I/O routines available will take a single-digit number and output it to the terminal.
The second step is easily accomplished with the statement printDigit(n%10), but the ﬁrst doesn’t seem any simpler than the original problem.
Indeed it is virtually the same problem, so we can solve it recursively with the statement printOut(n/10)
Recursion and Induction Let us prove (somewhat) rigorously that the recursive number-printing program works.
This proof probably seems a little strange in that it is virtually identical to the algorithm description.
It illustrates that in designing a recursive program, all smaller instances of the same problem (which are on the path to a base case) may be assumed to work correctly.
The recursive program needs only to combine solutions to smaller problems, which are “magically” obtained by recursion, into a solution for the current problem.
This rule is important because it means that when designing recursive programs, you generally don’t need to know the details of the bookkeeping arrangements, and you don’t have to try to trace through the myriad of recursive calls.
Frequently, it is extremely difﬁcult to track down the actual sequence of recursive calls.
Of course, in many cases this is an indication of a good use of recursion, since the computer is being allowed to work out the complicated details.
The main problem with recursion is the hidden bookkeeping costs.
Although these costs are almost always justiﬁable, because recursive programs not only simplify the algorithm design but also tend to give cleaner code, recursion should never be used as a substitute for a simple for loop.
We’ll discuss the overhead involved in recursion in more detail in Section 3.6
When writing recursive routines, it is crucial to keep in mind the four basic rules of recursion:
You must always have some base cases, which can be solved without recursion.
For the cases that are to be solved recursively, the recursive call must always be to a case that makes progress toward a base case.
Never duplicate work by solving the same instance of a problem in separate recursive calls.
The fourth rule, which will be justiﬁed (along with its nickname) in later sections, is the reason that it is generally a bad idea to use recursion to evaluate simple mathematical functions, such as the Fibonacci numbers.
As long as you keep these rules in mind, recursive programming should be straightforward.
An important goal of object-oriented programming is the support of code reuse.
An important mechanism that supports this goal is the generic mechanism: If the implementation is identical except for the basic type of the object, a generic implementation can be used to describe the basic functionality.
For instance, a method can be written to sort an array of items; the logic is independent of the types of objects being sorted, so a generic method could be used.
Unlike many of the newer languages (such as C++, which uses templates to implement generic programming), before version 1.5, Java did not support generic implementations directly.
Instead, generic programming was implemented using the basic concepts of inheritance.
This section describes how generic methods and classes can be implemented in Java using the basic principles of inheritance.
Direct support for generic methods and classes was announced by Sun in June 2001 as a future language addition.
However, using generic classes requires an understanding of the pre-Java 5 idioms for generic programming.
The basic idea in Java is that we can implement a generic class by using an appropriate superclass, such as Object.
An example is the MemoryCell class shown in Figure 1.5
There are two details that must be considered when we use this strategy.
To access a speciﬁc method of the object, we must downcast to the correct type.
Of course, in this example, we do not need the downcast, since we are simply invoking the toString method at line 9, and this can be done for any object.
A second important detail is that primitive types cannot be used.
When we implement algorithms, often we run into a language typing problem: We have an object of one type, but the language syntax requires an object of a different type.
This technique illustrates the basic theme of a wrapper class.
One typical use is to store a primitive type, and add operations that the primitive type either does not support or does not support correctly.
In Java, we have already seen that although every reference type is compatible with Object, the eight primitive types are not.
As a result, Java provides a wrapper class for each of the eight primitive types.
For instance, the wrapper for the int type is Integer.
Each wrapper object is immutable (meaning its state can never change), stores one primitive value that is set when the object is constructed, and provides a method to retrieve the value.
The wrapper classes also contain a host of static utility methods.
As an example, Figure 1.7 shows how we can use the MemoryCell to store integers.
Using Object as a generic type works only if the operations that are being performed can be expressed using only methods available in the Object class.
Consider, for example, the problem of ﬁnding the maximum item in an array of items.
The basic code is type-independent, but it does require the ability to compare any two objects and decide which is larger and which is smaller.
Thus we cannot simply ﬁnd the maximum of an array of Object—we need more information.
The simplest idea would be to ﬁnd the maximum of an array of Comparable.
To determine order, we can use the compareTo method that we know must be available for all Comparables.
The code to do this is shown in Figure 1.8, which provides a main that ﬁnds the maximum in an array of String or Shape.
First, only objects that implement the Comparable interface can be passed as elements of the Comparable array.
Objects that have a compareTo method but do not declare that they implement Comparable are not Comparable, and do not have the requisite IS-A relationship.
It is also implicit in the test program that Circle, Square, and Rectangle are subclasses of Shape.
Second, if the Comparable array were to have two objects that are incompatible (e.g., a String and a Shape), the compareTo method would throw a ClassCastException.
Third, as before, primitives cannot be passed as Comparables, but the wrappers work because they implement the Comparable interface.
Fourth, it is not required that the interface be a standard library interface.
Finally, this solution does not always work, because it might be impossible to declare.
And if the class is ﬁnal, we can’t extend it to create a new class.
Section 1.6 offers another solution for this problem, which is the function object.
The function object uses interfaces also and is perhaps one of the central themes encountered in the Java library.
One of the difﬁculties in language design is how to handle inheritance for aggregate types.
Does this imply that Employee[] IS-A Person[]? In other words, if a routine is written to accept Person[] as a parameter, can we pass an Employee[] as an argument?
At ﬁrst glance, this seems like a no-brainer, and Employee[] should be type-compatible with Person[]
Both assignments compile, yet arr[0] is actually referencing an Employee, and Student IS-NOT-A Employee.
The runtime system cannot throw a ClassCastException since there is no cast.
The easiest way to avoid this problem is to specify that the arrays are not typecompatible.
Each array keeps track of the type of object it is allowed to store.
Java 5 supports generic classes that are very easy to use.
In this section, we illustrate the basics of how generic classes and methods are written.
We do not attempt to cover all the constructs of the language, which are quite complex and sometimes tricky.
Instead, we show the syntax and idioms that are used throughout this book.
Here, we have changed the name to GenericMemoryCell because neither class is in a package and thus the names cannot be the same.
When a generic class is speciﬁed, the class declaration includes one or more type parameters enclosed in angle brackets <> after the class name.
Line 1 shows that the GenericMemoryCell takes one type parameter.
Inside the GenericMemoryCell class declaration, we can declare ﬁelds of the generic type and methods that use the generic type as a parameter or return type.
For example, prior to Java 5 the Comparable interface was not generic, and its compareTo method took an Object as the parameter.
As a result, any reference variable passed to the compareTo method would compile, even if the variable was not a sensible type, and only at runtime would the error be reported as a ClassCastException.
The String class, for instance, now implements Comparable<String> and has a compareTo method that takes a String as a parameter.
By making the class generic, many of the errors that were previously only reported at runtime become compile-time errors.
The code in Figure 1.7 is annoying to write because using the wrapper class requires creation of an Integer object prior to the call to write, and then the extraction of the int value from the Integer, using the intValue method.
Prior to Java 5, this is required because if an int is passed in a place where an Integer object is required, the compiler will generate an error message, and if the result of an Integer object is assigned to an int, the compiler will generate an error message.
This resulting code in Figure 1.7 accurately reﬂects the distinction between primitive types and reference types, yet it does not cleanly express the programmer’s intent of storing ints in the collection.
If an int is passed in a place where an Integer is required, the compiler will insert a call to the Integer constructor behind the scenes.
And if an Integer is passed in a place where an int is required, the compiler will insert a call to the intValue method behind the scenes.
Note that the entities referenced in the GenericMemoryCell are still Integer objects; int cannot be substituted for Integer in the GenericMemoryCell instantiations.
The diamond operator simpliﬁes the code, with no cost to the developer, and we use it throughout the text.
Figure 1.12 shows a static method that computes the total area in an array of Shapes (we assume Shape is a class with an area method; Circle and Square extend Shape)
Suppose we want to rewrite the method so that it works with a parameter that is Collection<Shape>
Collection is described in Chapter 3; for now, the only important thing about it is that it stores a collection of items that can be accessed with an enhanced for loop.
Because of the enhanced for loop, the code should be identical, and the resulting code is shown in Figure 1.13
Recall from Section 1.4.4 that the technical term for this is whether we have covariance.
In Java, as we mentioned in Section 1.4.4, arrays are covariant.
On the one hand, consistency would suggest that if arrays are covariant, then collections should be covariant too.
Because the entire reason to have generics is to generate compiler.
What we are left with is that generics (and the generic collections) are not covariant (which makes sense), but arrays are.
Without additional syntax, users would tend to avoid collections because the lack of covariance makes the code less ﬂexible.
In some sense, the totalArea method in Figure 1.14 is generic, since it works for different types.
But there is no speciﬁc type parameter list, as was done in the GenericMemoryCell.
Sometimes the speciﬁc type is important perhaps because one of the following reasons apply:
The type is used in more than one parameter type.
If so, then an explicit generic method with type parameters must be declared.
For instance, Figure 1.15 illustrates a generic static method that performs a sequential.
By using a generic method instead of a nongeneric method that uses Object as the parameter types, we can get compile-time errors if searching for an Apple in an array of Shapes.
The generic method looks much like the generic class in that the type parameter list uses the same syntax.
The type parameters in a generic method precede the return type.
This code cannot work because the compiler cannot prove that the call to compareTo at line 6 is valid; compareTo is guaranteed to exist only if AnyType is Comparable.
Figure 1.16 Generic static method to ﬁnd largest element in an array that does not work.
Figure 1.17 Generic static method to ﬁnd largest element in an array.
As a result, what we need to say is that AnyType IS-A Comparable<T> where T is a superclass of AnyType.
Since we do not need to know the exact type T, we can use a wildcard.
The compiler will accept arrays of types T only such that T implements the Comparable<S> interface, where T IS-A S.
Fortunately, we won’t see anything more complicated than this idiom.
Generic types, for the most part, are constructs in the Java language but not in the Virtual Machine.
Generic classes are converted by the compiler to nongeneric classes by a process known as type erasure.
The simpliﬁed version of what happens is that the compiler generates a raw class with the same name as the generic class with the type parameters removed.
The type variables are replaced with their bounds, and when calls are made.
If a generic class is used without a type parameter, the raw class is used.
One important consequence of type erasure is that the generated code is not much different than the code that programmers have been writing before generics and in fact is not any faster.
The signiﬁcant beneﬁt is that the programmer does not have to place casts in the code, and the compiler will do signiﬁcant type checking.
Every one of the restrictions listed here is required because of type erasure.
Primitive Types Primitive types cannot be used for a type parameter.
Eventually, a runtime error results at the last line because the call to read tries to return a String but cannot.
As a result, the typecast will generate a warning, and a corresponding instanceof test is illegal.
Static Contexts In a generic class, static methods and ﬁelds cannot refer to the class’s type variables since, after erasure, there are no type variables.
Further, since there is really only one raw class, static ﬁelds are shared among the class’s generic instantiations.
Instantiation of Generic Types It is illegal to create an instance of a generic type.
Generic Array Objects It is illegal to create an array of a generic type.
Because we cannot create arrays of generic objects, generally we must create an array of the erased type and then use a typecast.
This typecast will generate a compiler warning about an unchecked type conversion.
Arrays of Parameterized Types Instantiation of arrays of parameterized types is illegal.
Thus, this code has no casts, yet it will eventually generate a ClassCastException at line 5, which is exactly the situation that generics are supposed to avoid.
In Section 1.5, we showed how to write generic algorithms.
As an example, the generic method in Figure 1.16 can be used to ﬁnd the maximum item in an array.
However, that generic method has an important limitation: It works only for objects that implement the Comparable interface, using compareTo as the basis for all comparison decisions.
For instance, it is a stretch to presume that a Rectangle class will implement Comparable, and even if it does, the compareTo method that it has might not be the one we want.
Or perhaps if we are trying to ﬁt the rectangle through an opening, the larger rectangle is the rectangle with the larger minimum dimension.
As a second example, if we wanted to ﬁnd the maximum string (alphabetically last) in an array of strings, the default compareTo does not ignore case distinctions, so “ZEBRA” would be considered to precede “alligator” alphabetically, which is probably not what we want.
The solution in these situations is to rewrite findMax to accept two parameters: an array of objects and a comparison function that explains how to decide which of two objects is the larger and which is the smaller.
In effect, the objects no longer know how to compare themselves; instead, this information is completely decoupled from the objects in the array.
An ingenious way to pass functions as parameters is to notice that an object contains both data and methods, so we can deﬁne a class with no data and one method and pass.
Figure 1.18 Using a function object as a second parameter to findMax; output is ZEBRA.
In effect, a function is being passed by placing it inside an object.
Figure 1.18 shows the simplest implementation of the function object idea.
The Comparator interface is speciﬁed in java.util and contains a compare method.
The bounded wildcard at line 4 is used to signal that if we are ﬁnding the maximum in an array of items, the comparator must know how to compare items, or objects of the items’ supertype.
To use this version of findMax, at line 26, we can see that findMax is called by passing an array of String and an object that.
In Chapter 4, we will give an example of a class that needs to order the items it stores.
We will write most of the code using Comparable and show the adjustments needed to use the function objects.
Elsewhere in the book, we will avoid the detail of function objects to keep the code as simple as possible, knowing that it is not difﬁcult to add function objects later.
This chapter sets the stage for the rest of the book.
The time taken by an algorithm confronted with large amounts of input will be an important criterion for deciding if it is a good algorithm.
What is fast for one problem on one machine might be slow for another problem or a different machine.
We will begin to address these issues in the next chapter and will use the mathematics discussed here to establish a formal model.
Draw a table showing the running time of your program for various values of N.
Include statements may be nested; in other words, the ﬁle ﬁlename may itself contain an include statement, but, obviously, a ﬁle can’t include itself in any chain.
Write a program that reads in a ﬁle and outputs the ﬁle as modiﬁed by the include statements.
The ﬁrst routine is a driver that calls the second and prints all the permutations of the characters in String str.
If str is "abc", then the strings that are output are abc, acb, bac, bca, cab, and cba.
Provide public methods isEmpty, makeEmpty, insert, remove, findMin, and findMax.
Using the findMax routines in Figure 1.18, write a main that creates an array of Rectangle and ﬁnds the largest Rectangle ﬁrst on the basis of area, and then on the basis of perimeter.
There are many good textbooks covering the mathematics reviewed in this chapter.
Reference [11] is speciﬁcally geared toward the analysis of algorithms.
It is the ﬁrst volume of a three-volume series that will be cited throughout this text.
The material in this chapter is meant to serve as an overview of the features that we will use in this text.
We also assume familiarity with recursion (the recursion summary in this chapter is meant to be a quick review)
We will attempt to provide hints on its use where appropriate throughout the textbook.
Readers not familiar with recursion should consult [14] or any good intermediate programming textbook.
An algorithm is a clearly speciﬁed set of simple instructions to be followed to solve a problem.
Once an algorithm is given for a problem and decided (somehow) to be correct, an important step is to determine how much in the way of resources, such as time or space, the algorithm will require.
An algorithm that solves a problem but requires a year is hardly of any use.
Likewise, an algorithm that requires hundreds of gigabytes of main memory is not (currently) useful on most machines.
The analysis required to estimate the resource use of an algorithm is generally a theoretical issue, and therefore a formal framework is required.
Throughout the book we will use the following four deﬁnitions:
When we apply this to the analysis of algorithms, we shall see why this is the important measure.
To prove that some function T(N) = O(f(N)), we usually do not apply these deﬁnitions formally but instead use a repertoire of known results.
In general, this means that a proof (or determination that the assumption is incorrect) is a very simple calculation and should not involve calculus, except in extraordinary circumstances (not likely to occur in an algorithm analysis)
First, it is very bad style to include constants or low-order.
In both cases, the correct form is T(N) = O(N2)
This means that in any analysis that will require a Big-Oh answer, all sorts of shortcuts are possible.
Lower-order terms can generally be ignored, and constants can be thrown away.
Usually the relation between f(N) and g(N) can be derived by simple algebra.
This is like determining which of log2 N or N grows faster.
This is a simple problem, because it is already known that N grows faster than any power of a log.
As an example of the typical kinds of analysis that are performed, consider the problem of downloading a ﬁle over the Internet.
This is the typical characteristic of linear-time algorithms, and it is why we write T(N) = O(N), ignoring constant factors.
Although using Big-Theta would be more precise, Big-Oh answers are typically given.
Observe, too, that this behavior is not true of all algorithms.
For the ﬁrst selection algorithm described in Section 1.1, the running time is controlled by the time it takes to perform a sort.
For a simple sorting algorithm, such as the suggested bubble sort, when the amount of input doubles, the running time increases by a factor of four for large amounts of input.
Instead, as we will see when we discuss sorting, trivial sorting algorithms are O(N2), or quadratic.
In order to analyze algorithms in a formal framework, we need a model of computation.
Our model is basically a normal computer, in which instructions are executed sequentially.
Our model has the standard repertoire of simple instructions, such as addition, multiplication, comparison, and assignment, but, unlike the case with real computers, it takes exactly one time unit to do anything (simple)
To be reasonable, we will assume that, like a modern computer, our model has ﬁxed-size (say, 32-bit) integers and that there are no fancy operations, such as matrix inversion or sorting, that clearly cannot be done in one time unit.
Obviously, in real life, not all operations take exactly the same time.
In particular, in our model one disk read counts the same as an addition, even though the addition is typically several orders of magnitude faster.
Also, by assuming inﬁnite memory, we ignore the fact that the cost of a memory access can increase when slower memory is used due to larger memory requirements.
The most important resource to analyze is generally the running time.
Some, such as the compiler and computer used, are obviously beyond the scope of any theoretical model, so, although they are important, we cannot deal with them here.
The other main factors are the algorithm used and the input to the algorithm.
However, this is often of little interest, because it does not represent typical behavior.
Average-case performance often reﬂects typical behavior, while worst-case performance represents a guarantee for performance on any possible input.
Notice, also, that, although in this chapter we analyze Java code, these bounds are really bounds for the algorithms rather than programs.
Programs are an implementation of the algorithm in a particular programming language, and almost always the details of the programming language do not affect a Big-Oh answer.
If a program is running much more slowly than the algorithm analysis suggests, there may be an implementation inefﬁciency.
This is more common in languages (like C++) where arrays can be inadvertently copied in their entirety, instead of passed with references.
Thus in future chapters we will analyze the algorithms rather than the programs.
Generally, the quantity required is the worst-case time, unless otherwise speciﬁed.
One reason for this is that it provides a bound for all input, including particularly bad input, which an average-case analysis does not provide.
The other reason is that average-case bounds are usually much more difﬁcult to compute.
In some instances, the deﬁnition of “average” can affect the result.
As an example, in the next section, we shall consider the following problem:
For convenience, the maximum subsequence sum is 0 if all the integers are negative.
The running time on some computer (the exact computer is unimportant) for these algorithms is given in Figure 2.2
There are several important things worth noting in this table.
For a small amount of input, the algorithms all run in a blink of the eye, so if only a small amount of input is expected, it might be silly to expend a great deal of effort to design a clever algorithm.
On the other hand, there is a large market these days for rewriting programs that were written ﬁve years ago based on a no-longer-valid assumption of small input size.
Figure 2.2 Running times of several algorithms for maximum subsequence sum (in seconds)
Second, the times given do not include the time required to read the input.
For algorithm 4, the time merely to read in the input from a disk is likely to be an order of magnitude larger than the time required to solve the problem.
Reading the data is generally the bottleneck; once the data are read, the problem can be solved quickly.
For inefﬁcient algorithms this is not true, and signiﬁcant computer resources must be used.
Thus it is important that, whenever possible, algorithms be efﬁcient enough not to be the bottleneck of a problem.
Notice that algorithm 4, which is linear, exhibits the nice behavior that as the problem size increases by a factor of ten, the running time also increases by a factor of ten.
Figure 2.3 shows the growth rates of the running times of the four algorithms.
Although the graph for the O(N logN) algorithm seems linear, it is easy to verify that it is not by using a straight-edge (or piece of paper)
Although the graph for the O(N) algorithm seems constant, this is only because for small values of N, the constant term is larger than the linear term.
It dramatically illustrates how useless inefﬁcient algorithms are for even moderately large amounts of input.
There are several ways to estimate the running time of a program.
If two programs are expected to take similar times, probably the best way to decide which is faster is to code them both up and run them!
Generally, there are several algorithmic ideas, and we would like to eliminate the bad ones early, so an analysis is usually required.
Furthermore, the ability to do an analysis usually provides insight into designing efﬁcient algorithms.
The analysis also generally pinpoints the bottlenecks, which are worth coding carefully.
To simplify the analysis, we will adopt the convention that there are no particular units of time.
We will also throw away low-order terms, so what we are essentially doing is computing a Big-Oh running time.
Since Big-Oh is an upper bound, we must be careful never to underestimate the running time of the program.
In effect, the answer provided is a guarantee that the program will terminate within a certain time period.
The program may stop earlier than this, but never later.
If we had to perform all this work every time we needed to analyze a program, the task would quickly become infeasible.
Fortunately, since we are giving the answer in terms of Big-Oh, there are lots of shortcuts that can be taken without affecting the ﬁnal answer.
Line 1 is obviously insigniﬁcant compared with the for loop, so it is silly to waste time here.
The running time of a for loop is at most the running time of the statements inside the for loop (including tests) times the number of iterations.
The total running time of a statement inside a group of nested loops is the running time of the statement multiplied by the product of the sizes of all the loops.
Clearly, this can be an overestimate in some cases, but it is never an underestimate.
If there are method calls, these must be analyzed ﬁrst.
If the recursion is really just a thinly veiled for loop, the analysis is usually trivial.
For instance, the following method is really just a simple loop and is O(N):
When recursion is properly used, it is difﬁcult to convert the recursion into a simple loop structure.
In this case, the analysis will involve a recurrence relation that needs to be solved.
To see what might happen, consider the following program, which turns out to be a horrible use of recursion:
This program is slow because there is a huge amount of redundant work being performed, violating the fourth major rule of recursion (the compound interest rule), which was presented in Section 1.3
The amount of information thrown away compounds recursively and results in the huge running time.
This is perhaps the ﬁnest example of the maxim “Don’t compute anything more than once” and should not scare you away from using recursion.
Throughout this book, we shall see outstanding uses of recursion.
We will now present four algorithms to solve the maximum subsequence sum problem posed earlier.
The ﬁrst algorithm, which merely exhaustively tries all possibilities, is depicted in Figure 2.5
Also, the algorithm does not compute the actual subsequences; additional code is required to do this.
Convince yourself that this algorithm works (this should not take much convincing)
The sum can be evaluated inside out, using formulas from Section 1.2.3
In particular, we will use the formulas for the sum of the ﬁrst N integers and ﬁrst N squares.
We can avoid the cubic running time by removing a for loop.
This is not always possible, but in this case there are an awful lot of unnecessary computations present in the algorithm.
The inefﬁciency that the improved algorithm corrects can be seen by noticing.
There is a recursive and relatively complicated O(N logN) solution to this problem, which we now describe.
If there didn’t happen to be an O(N) (linear) solution, this would be an excellent example of the power of recursion.
The idea is to split the problem into two roughly equal subproblems, which are then solved recursively.
The “conquer” stage consists of patching together the two solutions of the subproblems, and possibly doing a small amount of additional work, to arrive at a solution for the whole problem.
In our case, the maximum subsequence sum can be in one of three places.
Either it occurs entirely in the left half of the input, or entirely in the right half, or it crosses the middle and is in both halves.
The last case can be obtained by ﬁnding the largest sum in the ﬁrst half that includes the last element.
We see, then, that among the three ways to form a large maximum subsequence, for our example, the best way is to include elements from both halves.
The general form of the call for the recursive method is to pass the input array along with the left and right borders, which.
If left == right, there is one element, and it is the maximum subsequence if the element is nonnegative.
The case left > right is not possible unless N is negative (although minor perturbations in the code could mess this up)
We can see that the recursive calls are always on a smaller problem than the original, although minor perturbations in the code could destroy this property.
The sum of these two values is the maximum sum that spans both halves.
The routine max3 (not shown) returns the largest of the three possibilities.
Algorithm 3 clearly requires more effort to code than either of the two previous algorithms.
As we have seen in the earlier table showing the running times of the algorithms, this algorithm is considerably faster than the other two for all but the smallest of input sizes.
The running time is analyzed in much the same way as for the program that computes the Fibonacci numbers.
Let T(N) be the time it takes to solve a maximum subsequence sum problem of size N.
These lines solve two subsequence problems of size N/2 (assuming N is even)
The total time for the algorithm then is 2T(N/2) + O(N)
This analysis assumes N is even, since otherwise N/2 is not deﬁned.
By the recursive nature of the analysis, it is really valid only when N is a power of 2, since otherwise we eventually get a subproblem that is not an even size, and the equation is invalid.
When N is not a power of 2, a somewhat more complicated analysis is required, but the Big-Oh result remains unchanged.
In future chapters, we will see several clever applications of recursion.
Here, we present a fourth algorithm to ﬁnd the maximum subsequence sum.
This algorithm is simpler to implement than the recursive algorithm and also is more efﬁcient.
It should be clear why the time bound is correct, but it takes a little thought to see why the algorithm actually works.
One observation is that if a[i] is negative, then it cannot possibly represent the start of the optimal sequence, since any subsequence that begins by including a[i] would be improved by beginning with a[i+1]
Similarly, any negative subsequence cannot possibly be a preﬁx of the optimal subsequence (same logic)
If, in the inner loop, we detect that the subsequence from a[i] to a[j] is negative, then we can advance i.
To see this, let p be any index between i+1 and j.
Any subsequence that starts at index p is not larger than the corresponding subsequence that starts at index i and includes the subsequence from a[i] to a[p-1], since the latter subsequence is not negative (j is the ﬁrst index that causes the subsequence starting at index i to become negative)
Thus advancing i to j+1 is risk free: we cannot miss an optimal solution.
This algorithm is typical of many clever algorithms: The running time is obvious, but the correctness is not.
For these algorithms, formal correctness proofs (more formal than the sketch above) are almost always required; even then, however, many people still are not convinced.
In addition, many of these algorithms require trickier programming, leading to longer development.
But when these algorithms work, they run quickly, and we can.
An extra advantage of this algorithm is that it makes only one pass through the data, and once a[i] is read and processed, it does not need to be remembered.
Thus, if the array is on a disk or is being transmitted over the Internet, it can be read sequentially, and there is no need to store any part of it in main memory.
Furthermore, at any point in time, the algorithm can correctly give an answer to the subsequence problem for the data it has already read (the other algorithms do not share this property)
An online algorithm that requires only constant space and runs in linear time is just about as good as possible.
The most confusing aspect of analyzing algorithms probably centers around the logarithm.
We have already seen that some divide-and-conquer algorithms will run in O(N logN) time.
On the other hand, if constant time is required to merely reduce the problem by a constant amount (such as to make the problem smaller by 1), then the algorithm is O(N)
Binary Search The ﬁrst example is usually referred to as binary search.
However, this algorithm does not take advantage of the fact that the list is sorted and is thus not likely to be best.
A better strategy is to check if X is the middle element.
If X is smaller than the middle element, we can apply the same strategy to the sorted subarray to the left of the middle element; likewise, if X is larger than the middle element, we look to the right half.
Figure 2.9 shows the code for binary search (the answer is mid)
Binary search can be viewed as our ﬁrst data structure implementation.
It supports the contains operation in O(logN) time, but all other operations (in particular insert) require O(N) time.
In applications where the data are static (that is, insertions and deletions are not allowed), this could be very useful.
The input would then need to be sorted once, but afterward accesses would be fast.
An example is a program that needs to maintain information about the periodic table of elements (which arises in chemistry and physics)
This table is relatively stable, as new elements are added infrequently.
Since there are only about 118 elements, at most eight accesses would be required to ﬁnd an element.
The algorithm works by continually computing remainders until 0 is reached.
As before, estimating the entire running time of the algorithm depends on determining how long the sequence of remainders is.
Indeed, the remainder does not decrease by a constant factor in one iteration.
However, we can prove that after two iterations, the remainder is at most half of its original value.
This would show that the number of iterations is at most 2 logN = O(logN) and establish the running time.
Exponentiation Our last example in this section deals with raising an integer to a power (which is also an integer)
Numbers that result from exponentiation are generally quite large, so an analysis works only if we can assume that we have a machine that can store such large integers.
We will count the number of multiplications as the measurement of running time.
For instance, to compute X62, the algorithm does the following calculations, which involve only nine multiplications:
The number of multiplications required is clearly at most 2 logN, because at most two multiplications (if N is odd) are required to halve the problem.
Indeed, the program will still run in O(logN), because the sequence of multiplications is the same as before.
However, all of the following alternatives for line 8 are bad, even though they look correct:
Figure 2.11 to use BigInteger instead of long is straightforward.
Thus no progress is made, and an inﬁnite loop results (in an eventual abnormal termination)
An analysis will show that the running time is no longer O(logN)
We leave it as an exercise to the reader to determine the new running time.
Sometimes the analysis is shown empirically to be an overestimate.
If this is the case, then either the analysis needs to be tightened (usually by a clever observation), or it may be that the average running time is signiﬁcantly less than the worst-case running time and no improvement in the bound is possible.
For many complicated algorithms the worst-case bound is achievable by some bad input but is usually an overestimate in practice.
Unfortunately, for most of these problems, an average-case analysis is extremely complex (in many cases still unsolved), and a worst-case bound, even though overly pessimistic, is the best analytical result known.
This chapter gives some hints on how to analyze the complexity of programs.
Simple programs usually have simple analyses, but this is not always the case.
The analysis of Shellsort is still not complete, and the disjoint set algorithm has an analysis that is extremely difﬁcult and requires pages and pages of intricate calculations.
Most of the analyses that we will encounter here will be simple and involve counting through loops.
We close by mentioning that some of the algorithms described here have real-life application.
The gcd algorithm and the exponentiation algorithm are both used in cryptography.
Since the calculations require dealing with 600-digit numbers, efﬁciency is obviously important.
Give an analysis of the running time (Big-Oh will do)
Implement the code in Java, and give the running time for several values of N.
Same as algorithm (1), but keep an extra array called the used array.
When a random number, ran, is ﬁrst put in the array a, set used[ran] = true.
This means that when ﬁlling a[i] with a random number, you can test in one step to see whether the random number has been used, instead of the (possibly) i steps in the ﬁrst algorithm.
Prove that all three algorithms generate only legal permutations and that all permutations are equally likely.
Give as accurate (Big-Oh) an analysis as you can of the expected running time of each algorithm.
Write (separate) programs to execute each algorithm 10 times, to get a good average.
Interpolate the running times for these algorithms and estimate the time required to compute the maximum subsequence sum of 1 million numbers.
If the function is continuous and has two points low and high such that f(low) and f(high) have opposite signs, then a root must exist between low and high and can be found by a binary search.
Write a function that takes as parameters f , low, and high and solves for a zero.
To implement a generic function as a parameter, pass a function object that implements the Function interface, which you can deﬁne to contain a single method f.
Modify them so that they return in a single object the value of the maximum subsequence and the indices of the actual sequence.
Write a program to determine if a positive integer, N, is prime.
Let B equal the number of bits in the binary representation of N.
In terms of B, what is the worst-case running time of your program? e.
Is it more reasonable to give the running time in terms of N or B? Why?
We begin by making a table of integers 2 to N.
Which program has the better guarantee on the running time, for large values of.
Which program has the better guarantee on the running time, for small values.
Which program will run faster on average for N = 1,000? d.
Is it possible that program B will run faster than program A on all possible.
If there is no majority element, your program should indicate this.
Here is a sketch of an algorithm to solve the problem:
First, a candidate majority element is found (this is the harder part)
This candidate is the only element that could possibly be the majority element.
The second step determines if this candidate is actually the majority.
To ﬁnd a candidate in the array, A, form a second array, B.
If they are equal, add one of these to B; otherwise do nothing.
Again if they are equal, add one of these to B; otherwise do nothing.
Continue in this fashion until the entire array is read.
Then recursively ﬁnd a candidate for B; this is the candidate for A (why?)
Give an O(N) worst-case algorithm that decides if a number X is in the matrix.
Suppose we ﬁx the size of the longest word to be 10 characters.
In terms of R and C, which are the number of rows and columns in the puzzle, and W, which is the number of words, what are the running times of the algorithms described in Chapter 1?
Show how to use binary search to obtain an algorithm with signiﬁcantly better running time.
What pattern is evident? Can you give a combinatoric explanation of this phenomenon?
This chapter discusses three of the most simple and basic data structures.
Virtually every signiﬁcant program will use at least one of these structures explicitly, and a stack is always implicitly used in a program, whether or not you declare one.
In this chapter, we provide code that implements a signiﬁcant subset of two library classes: ArrayList and LinkedList.
Abstract data types are mathematical abstractions; nowhere in an ADT’s deﬁnition is there any mention of how the set of operations is implemented.
Objects such as lists, sets, and graphs, along with their operations, can be viewed as abstract data types, just as integers, reals, and booleans are data types.
Integers, reals, and booleans have operations associated with them, and so do abstract data types.
For the set ADT, we might have such operations as add, remove, and contains.
Alternatively, we might only want the two operations union and ﬁnd, which would deﬁne a different ADT on the set.
The Java class allows for the implementation of ADTs, with appropriate hiding of implementation details.
Thus any other part of the program that needs to perform an operation on the ADT can do so by calling the appropriate method.
If for some reason implementation details need to be changed, it should be easy to do so by merely changing the routines that perform the ADT operations.
This change, in a perfect world, would be completely transparent to the rest of the program.
There is no rule telling us which operations must be supported for each ADT; this is a design decision.
Error handling and tie breaking (where appropriate) are also generally up to the program designer.
The three data structures that we will study in this chapter are.
We will see how each can be implemented in several ways, but if they are done correctly, the programs that use them will not necessarily need to know which implementation was used.
Of course, the interpretation of what is appropriate for a method is entirely up to the programmer, as is the handling of special cases (for example, what does find(1) return above?)
We could also add operations such as next and previous, which would take a position as argument and return the position of the successor and predecessor, respectively.
All these instructions can be implemented just by using an array.
Although arrays are created with a ﬁxed capacity, we can create a different array with double the capacity when needed.
This solves the most serious problem with using an array, namely that historically, to use an array, an estimate of the maximum size of the list was required.
This estimate is not needed in Java, or any modern programming language.
The following code fragment illustrates how an array, arr, which initially has length 10, can be expanded as needed:
An array implementation allows printList to be carried out in linear time, and the findKth operation takes constant time, which is as good as can be expected.
However, insertion and deletion are potentially expensive, depending on where the insertions and deletions occur.
In the worst case, inserting into position 0 (in other words, at the front of the list) requires pushing the entire array down one spot to make room, and deleting the ﬁrst element requires shifting all the elements in the list up one spot, so the worst case for these operations is O(N)
On average, half of the list needs to be moved for either operation, so linear time is still required.
On the other hand, if all the operations occur at the high end of the list, then no elements need to be shifted, and then adding and deleting take O(1) time.
There are many situations where the list is built up by insertions at the high end, and then only array accesses (i.e., findKth operations) occur.
In such a case, the array is a suitable implementation.
However, if insertions and deletions occur throughout the list, and in particular, at the front of the list, then the array is not a good option.
The next subsection deals with the alternative: the linked list.
In order to avoid the linear cost of insertion and deletion, we need to ensure that the list is not stored contiguously, since otherwise entire parts of the list will need to be moved.
Figure 3.1 shows the general idea of a linked list.
The linked list consists of a series of nodes, which are not necessarily adjacent in memory.
Each node contains the element and a link to a node containing its successor.
To execute printList or find(x) we merely start at the ﬁrst node in the list and then traverse the list by following the next links.
This operation is clearly linear-time, as in the array implementation, although the constant is likely to be larger than if an array implementation were used.
The findKth operation is no longer quite as efﬁcient as an array implementation; findKth(i) takes O(i) time and works by traversing down the list in the obvious manner.
In practice, this bound is pessimistic, because frequently the calls to findKth are in sorted order (by i)
The remove method can be executed in one next reference change.
Figure 3.2 shows the result of deleting the third element in the original list.
The insert method requires obtaining a new node from the system by using a new call and then executing two reference maneuvers.
As we can see, in principle, if we know where a change is to be made, inserting or removing an item from a linked list does not require moving lots of items and instead involves only a constant number of changes to node links.
The special case of adding to the front or removing the ﬁrst item is thus a constanttime operation, presuming of course that a link to the front of the linked list is maintained.
The special case of adding at the end (i.e., making the new item as the last item) can be constant-time, as long as we maintain a link to the last node.
Thus, a typical linked list keeps links to both ends of the list.
Removing the last item is trickier, because we have to ﬁnd the next-to-last item, change its next link to null, and then update the link that maintains the last node.
In the classic linked list, where each node stores a link to its next node, having a link to the last node provides no information about the next-to-last node.
The obvious idea of maintaining a third link to the next-to-last node doesn’t work, because it too would need to be updated during a remove.
Instead, we have every node maintain a link to its previous node in the list.
This is shown in Figure 3.4 and is known as a doubly linked list.
The Java language includes, in its library, an implementation of common data structures.
This part of the language is popularly known as the Collections API.
The List ADT is one of the data structures implemented in the Collections API.
The notion of a collection, which stores a collection of identically typed objects, is abstracted in the Collection interface.
Figure 3.5 shows the most important parts of this interface (some methods are not shown)
Many of the methods in the Collection interface do the obvious things that their names suggest.
So size returns the number of items in the collection; isEmpty returns true if and only if the size of the collection is zero.
Note that the interface doesn’t specify how the collection decides if x is in the collection—this is determined by the actual classes that implement the Collection interface.
For instance, a remove can fail if the item is not present in the collection, and if the particular collection does not allow duplicates, then add can fail when an attempt is made to insert a duplicate.
Classes that implement the Iterable interface can have the enhanced for loop used on them to view all their items.
For instance, the routine in Figure 3.6 can be used to print all the items in any collection.
Collections that implement the Iterable interface must provide a method named iterator that returns an object of type Iterator.
The Iterator is an interface deﬁned in package java.util and is shown in Figure 3.7
Figure 3.5 Subset of the Collection interface in package java.util.
Figure 3.6 Using the enhanced for loop on an Iterable type.
The idea of the Iterator is that via the iterator method, each collection can create, and return to the client, an object that implements the Iterator interface and stores internally its notion of a current position.
Each call to next gives the next item in the collection (that has not yet been seen)
Thus the ﬁrst call to next gives the ﬁrst item, the second call gives the second item, and so forth.
When the compiler sees an enhanced for loop being used on an object that is Iterable, it mechanically replaces the enhanced for loop with calls to the iterator method to obtain an Iterator and then calls to next and hasNext.
Thus the previously seen print routine is rewritten by the compiler as shown in Figure 3.8
Because of the limited set of methods available in the Iterator interface, it is hard to use the Iterator for anything more than a simple traversal through the Collection.
Figure 3.8 The enhanced for loop on an Iterable type rewritten by the compiler to use an iterator.
Although the Collection interface also contains a remove method, there are presumably advantages to using the Iterator’s remove method instead.
The main advantage of the Iterator’s remove method is that the Collection’s remove method must ﬁrst ﬁnd the item to remove.
Presumably it is much less expensive to remove an item if you know exactly where it is.
An example that we will see in the next section removes every other item in the collection.
This code is easy to write with an iterator, and potentially more efﬁcient than using the Collection’s remove method.
This is necessary to avoid ugly situations in which the iterator is prepared to give a certain item as the next item, and then that item is either removed, or perhaps a new item is inserted just prior to the next item.
This means that you shouldn’t obtain an iterator until immediately prior to the need to use it.
However, if the iterator invokes its remove method, then the iterator is still valid.
This is a second reason to prefer the iterator’s remove method sometimes.
The collection that concerns us the most in this section is the list, which is speciﬁed by the List interface in package java.util.
The List interface extends Collection, so it contains all the methods in the Collection interface, plus a few others.
Thus, an add at position 0 is adding at the front, whereas an add at position size() is adding an item as the new last item.
In addition to the standard remove that takes AnyType as a parameter, remove is overloaded to remove an item at a speciﬁed position.
Finally, the List interface speciﬁes the listIterator method that produces a more.
Figure 3.9 Subset of the List interface in package java.util.
The ArrayList provides a growable array implementation of the List ADT.
The advantage of using the ArrayList is that calls to get and set take constant time.
The disadvantage is that insertion of new items and removal of existing items is expensive, unless the changes are made at the end of the ArrayList.
The LinkedList provides a doubly linked list implementation of the List ADT.
The advantage of using the LinkedList is that insertion of new items and removal of existing items is cheap, provided that the position of the changes is known.
This means that adds and removes from the front of the list are constant-time operations, so much so that the LinkedList provides methods addFirst and removeFirst, addLast and removeLast, and getFirst and getLast to efﬁciently add, remove, and access the items at both ends of the list.
The disadvantage is that the LinkedList is not easily indexable, so calls to get are expensive unless they are very close to one of the ends of the list (if the call to get is for an item near the back of the list, the search can proceed from the back of the list)
To see the differences, we look at some methods that operate on a List.
First, suppose we construct a List by adding items at the end.
Regardless of whether an ArrayList or LinkedList is passed as a parameter, the running time of makeList1 is O(N) because each call to add, being at the end of the list, takes constant time (the occasional expansion of the ArrayList is safe to ignore)
The next routine attempts to compute the sum of the numbers in a List:
Here, the running time is O(N) for an ArrayList, but O(N2) for a LinkedList, because in a LinkedList, calls to get are O(N) operations.
Instead, use an enhanced for loop, which will make the running time O(N) for any List, because the iterator will efﬁciently advance from one item to the next.
Both ArrayList and LinkedList are inefﬁcient for searches, so calls to the Collection contains and remove methods (that take an AnyType as parameter) take linear time.
In an ArrayList, there is a notion of a capacity, which represents the size of the underlying array.
The ArrayList automatically increases the capacity as needed to ensure that it is at least as large as the size of the list.
If an early estimate of the size is available, ensureCapacity can set the capacity to a sufﬁciently large amount to avoid a later expansion of the array capacity.
Also, trimToSize can be used after all ArrayList adds are completed to avoid wasted space.
As an example, we provide a routine that removes all even-valued items in a list.
There are several possible ideas for an algorithm that deletes items from the list as they are encountered.
Of course, one idea is to construct a new list containing all the odd numbers, and then clear the original list and copy the odd numbers back into it.
But we are more interested in writing a clean version that avoids making a copy and instead removes items from the list as they are encountered.
This is almost certainly a losing strategy for an ArrayList, since removing from almost anywhere in an ArrayList is expensive.
In a LinkedList, there is some hope, as we know that removing from a known position can be done efﬁciently by rearranging some links.
On an ArrayList, as expected, the remove is not efﬁcient, so the routine takes quadratic time.
First, the call to get is not efﬁcient, so the routine takes quadratic time.
Additionally, the call to remove is equally inefﬁcient, because it is expensive to get to position i.
Instead of using get, we use an iterator to step through the list.
Figure 3.10 Removes the even numbers in a list; quadratic on all types of lists.
Figure 3.12 Removes the even numbers in a list; quadratic on ArrayList, but linear time for LinkedList.
This is not an efﬁcient operation because the remove method has to search for the item again, which takes linear time.
But if we run the code, we ﬁnd out that the situation is even worse: The program generates an exception because when an item is removed, the underlying iterator used by the enhanced for loop is invalidated.
The code in Figure 3.10 explains why: we cannot expect the enhanced for loop to understand that it must advance only if an item is not removed.
Figure 3.12 shows an idea that works: After the iterator ﬁnds an even-valued item, we can use the iterator to remove the value it has just seen.
For a LinkedList, the call to the iterator’s remove method is only constant time, because the iterator is at (or near) the node that needs to be removed.
Thus, for a LinkedList, the entire routine takes linear time, rather than quadratic time.
For an ArrayList, even though the iterator is at the point that needs to be removed, the remove is still expensive, because array items must be shifted, so as expected, the entire routine still takes quadratic time for an ArrayList.
Figure 3.13 Subset of the ListIterator interface in package java.util.
Figure 3.13 shows that a ListIterator extends the functionality of an Iterator for Lists.
The notion of the current position is abstracted by viewing the iterator as being between the item that would be given by a call to next and the item that would be given by a call to previous, an abstraction that is illustrated in Figure 3.14
As an example, it can be used to subtract 1 from all the even numbers in a List, which would be hard to do on a LinkedList without using the ListIterator’s set method.
In this section, we provide the implementation of a usable ArrayList generic class.
To avoid ambiguities with the library class, we will name our class MyArrayList.
We do not provide a MyCollection or MyList interface; rather, MyArrayList is standalone.
Before examining the (nearly one hundred lines of) MyArrayList code, we outline the main details.
The MyArrayList will maintain the underlying array, the array capacity, and the current number of items stored in the MyArrayList.
The MyArrayList will provide a mechanism to change the capacity of the underlying array.
The capacity is changed by obtaining a new array, copying the old array into the new array, and allowing the Virtual Machine to reclaim the old array.
The MyArrayList will provide an implementation of get and set.
The MyArrayList will provide basic routines, such as size, isEmpty, and clear, which are typically one-liners; a version of remove; and also two versions of add.
The add routines will increase capacity if the size and capacity are the same.
The MyArrayList will provide a class that implements the Iterator interface.
This class will store the index of the next item in the iteration sequence and provide implementations of next, hasNext, and remove.
The MyArrayList’s iterator method simply returns a newly constructed instance of the class that implements the Iterator interface.
Like its Collections API counterpart, there is some error checking to ensure valid bounds; however, in order to concentrate on the basics of writing the iterator class, we do not check for a structural modiﬁcation that could invalidate an iterator, nor do we check for an illegal iterator remove.
These checks are shown in the subsequent implementation of MyLinkedList in Section 3.5 and are exactly the same for both list implementations.
As shown on lines 5–6, the MyArrayList stores the size and array as its data members.
At line 46, we see an idiom that is required because generic array creation is illegal.
Instead, we create an array of the generic type’s bound and then use an array cast.
This will generate a compiler warning but is unavoidable in the implementation of generic collections.
The ﬁrst adds at the end of the list and is trivially implemented by calling the more general version that adds at the speciﬁed position.
That version is computationally expensive because it requires shifting elements that are at or after the speciﬁed position an additional position higher.
The remove method is similar to add, in that elements that are at or after the speciﬁed position must be shifted to one position lower.
The remaining routine deals with the iterator method and the implementation of the associated iterator class.
The ArrayListIterator stores the notion of a current position, and provides implementations of hasNext, next, and remove.
The ArrayListIterator class uses a tricky Java construct known as the inner class.
Clearly the class is declared inside of the MyArrayList class, a feature that is supported by many languages.
However, an inner class in Java has a more subtle property.
To see how an inner class works, Figure 3.17 sketches the iterator idea (however, the code is ﬂawed), making ArrayListIterator a top-level class.
We focus only on the data ﬁelds of MyArrayList, the iterator method in MyArrayList, and the ArrayListIterator (but not its remove method)
In Figure 3.17, ArrayListIterator is generic, it stores a current position, and the code attempts to use the current position in next to index the array and then advance.
Note that if arr is an array, arr[idx++] uses idx to the array, and then advances idx.
The form we used is called the postﬁx ++ operator, in which the ++ is after idx.
But in the preﬁx ++ operator, arr[++idx] advances idx and then uses the new idx to index the array.
It doesn’t work because theItems and size() are not part of the ArrayListIterator class.
It doesn’t work because theItems is private in the MyArrayList class.
The simplest solution is shown in Figure 3.18, which is unfortunately also ﬂawed, but in a more minor way.
In Figure 3.18, we solve the problem of not having the array in the iterator by having the iterator store a reference to the MyArrayList that it is iterating over.
This reference is a second data ﬁeld and is initialized by a new one-parameter constructor for ArrayListIterator.
Now that we have a reference to MyArrayList, we can access the array ﬁeld that is contained in MyArrayList (and also get the size of the MyArrayList, which is needed in hasNext)
The ﬂaw in Figure 3.18 is that theItems is a private ﬁeld in MyArrayList, and since ArrayListIterator is a different class, it is illegal to access theItems in the next method.
The simplest ﬁx would be to change the visibility of theItems in MyArrayList from private to something less restrictive (such as public, or the default which is known as package visibility)
But this violates basic principles of good object-oriented programming, which requires data to be as hidden as possible.
Instead, Figure 3.19 shows a solution that works: Make the ArrayListIterator class a nested class.
When we make ArrayListIterator a nested class, it is placed inside of another class (in this case MyArrayList) which is the outer class.
We must use the word static to signify that it is nested; without static we will get an inner class, which is sometimes good and sometimes bad.
The nested class is the type of class that is typical of many programming languages.
Observe that the nested class can be made private, which is nice.
It works because the nested class is considered part of the MyArrayList class.
More importantly, because the nested class is considered to be part of the outer class, there are no visibility issues that arise: theItems is a visible member of class MyArrayList, because next is part of MyArrayList.
Now that we have a nested class, we can discuss the inner class.
The problem with the nested class is that in our original design, when we wrote theItems without referring to MyArrayList that it was contained in, the code looked nice, and kind of made sense, but was illegal because it was impossible for the compiler to deduce which MyArrayList was being referred to.
It would be nice not to have to keep track of this ourselves.
This is exactly what an inner class does for you.
When you declare an inner class, the compiler adds an implicit reference to the outer class object that caused the inner class object’s construction.
If the name of the outer class is Outer, then the implicit reference is Outer.this.
Thus if ArrayListIterator is declared as an inner class, without the static, then MyArrayList.this and theList would both be referencing the same MyArrayList.
The inner class is useful in a situation in which each inner class object is associated with exactly one instance of an outer class object.
In such a case, the inner class object can never exist without having an outer class object with which to be associated.
In the case of the MyArrayList and its iterator, Figure 3.20 shows the relationship between the iterator class and MyArrayList class, when inner classes are used to implement the iterator.
This is hardly an improvement, but a further simpliﬁcation is possible.
First, the ArrayListIterator is implicitly generic, since it is now tied to MyArrayList, which is generic; we don’t have to say so.
The removal of theList as a data member also removes the associated constructor, so the code reverts back to the style in Version #1
We can implement the iterator’s remove by calling MyArrayList’s remove.
Note that after the item is removed, elements shift, so for current to be viewing the same element, it must also shift.
They are not needed to write any Java code, but their presence in the language allows the Java programmer to write code in the style that was natural (like Version #1), with the compiler writing the extra code required to associate the inner class object with the outer class object.
In this section, we provide the implementation of a usable LinkedList generic class.
As in the case of the ArrayList class, our list class will be named MyLinkedList to avoid ambiguities with the library class.
Recall that the LinkedList class will be implemented as a doubly linked list, and that we will need to maintain references to both ends of the list.
Doing so allows us to maintain constant time cost per operation, so long as the operation occurs at a known position.
The known position can be either end, or at a position speciﬁed by an iterator (however, we do not implement a ListIterator, thus leaving some code for the reader)
In considering the design, we will need to provide three classes:
The MyLinkedList class itself, which contains links to both ends, the size of the list, and a host of methods.
The Node class, which is likely to be a private nested class.
A node contains the data and links to the previous and next nodes, along with appropriate constructors.
The LinkedListIterator class, which abstracts the notion of a position and is a private inner class, implementing the Iterator interface.
Because the iterator classes store a reference to the “current node,” and the end marker is a valid position, it makes sense to create an extra node at the end of the list to represent the end marker.
Further, we can create an extra node at the front of the list, logically representing the beginning marker.
These extra nodes are sometimes known as sentinel nodes; speciﬁcally, the node at the front is sometimes known as a header node, and the node at the end is sometimes known as a tail node.
Figure 3.22 A doubly linked list with header and tail nodes.
Figure 3.23 An empty doubly linked list with header and tail nodes.
The advantage of using these extra nodes is that they greatly simplify the coding by removing a host of special cases.
For instance, if we do not use a header node, then removing the ﬁrst node becomes a special case, because we must reset the list’s link to the ﬁrst node during the remove, and also because the remove algorithm in general needs to access the node prior to the node being removed (and without a header node, the ﬁrst node does not have a node prior to it)
Figure 3.22 shows a doubly linked list with header and tail nodes.
Figure 3.24 shows the outline and partial implementation of the MyLinkedList class.
We can see at line 3 the beginning of the declaration of the private nested Node class.
Figure 3.25 shows the Node class, consisting of the stored item, links to the previous and next Node, and a constructor.
Recall that in a class, the data members are normally private.
However, members in a nested class are visible even in the outer class.
Since the Node class is private, the visibility of the data members in the Node class is irrelevant; the MyLinkedList methods can see all Node data members, and classes outside of MyLinkedList cannot see the Node class at all.
We also keep track of the size in a data member, so that the size method can be implemented in constant time.
At line 47, we have one additional data ﬁeld that is used to help the iterator detect changes in the collection.
The idea is that when an iterator is created, it will store the modCount of the collection.
The rest of the MyLinkedList class consists of the constructor, the implementation of the iterator, and a host of methods.
Figure 3.26 clear routine for MyLinkedList class, which invokes private doClear.
The doClear method in Figure 3.26 is invoked by the constructor.
We’ll discuss those details when we see the actual implementations later.
Figure 3.27 illustrates how a new node containing x is spliced in between a node referenced by p and p.prev.
The assignment to the node links can be described as follows:
But then these two lines can also be combined, yielding:
Figure 3.27 Insertion in a doubly linked list by getting new node and then changing pointers in the order indicated.
Figure 3.29 Removing node speciﬁed by p from a doubly linked list.
Figure 3.30 shows the basic private remove routine that contains the two lines of code shown above.
Observe that when current is positioned at the endMarker, a call to next is illegal.
In order to detect a situation in which the collection has been modiﬁed during the iteration, at line 4 the iterator stores in the data ﬁeld expectedModCount the modCount of the linked list at the time the iterator is constructed.
At line 5, the Boolean data ﬁeld okToRemove is true if a next has been performed, without a subsequent remove.
Thus okToRemove is initially false, set to true in next, and set to false in remove.
It is mostly error checking (which is why we avoided the error checks in the ArrayListIterator)
The actual remove at line 30 mimics the logic in the ArrayListIterator.
But here, current remains unchanged, because the node that current is viewing is unaffected by the removal of the prior node (in the ArrayListIterator, items shifted, requiring an update of current)
A stack is a list with the restriction that insertions and deletions can be performed in only one position, namely, the end of the list, called the top.
The fundamental operations on a stack are push, which is equivalent to an insert, and pop, which deletes the most recently.
Figure 3.33 Stack model: input to a stack is by push, output is by pop and top.
Figure 3.34 Stack model: Only the top element is accessible.
The most recently inserted element can be examined prior to performing a pop by use of the top routine.
A pop or top on an empty stack is generally considered an error in the stack ADT.
On the other hand, running out of space when performing a push is an implementation limit but not an ADT error.
Stacks are sometimes known as LIFO (last in, ﬁrst out) lists.
The model depicted in Figure 3.33 signiﬁes only that pushes are input operations and pops and tops are output.
The usual operations to make empty stacks and test for emptiness are part of the repertoire, but essentially all that you can do to a stack is push and pop.
The general model is that there is some element that is at the top of the stack, and it is the only element that is visible.
Since a stack is a list, any list implementation will do.
Clearly ArrayList and LinkedList support stack operations; 99% of the time they are the most reasonable choice.
Occasionally it can be faster to design a special-purpose implementation (for instance, if the items being placed on the stack are a primitive type)
Because stack operations are constant-time operations, this is unlikely to yield any discernable improvement except under very unique circumstances.
For these special times, we will give two popular implementations.
Linked List Implementation of Stacks The ﬁrst implementation of a stack uses a singly linked list.
We perform a push by inserting at the front of the list.
We perform a pop by deleting the element at the front of the list.
A top operation merely examines the element at the front of the list, returning its value.
Sometimes the pop and top operations are combined into one.
Notice that these operations are performed in not only constant time, but very fast constant time.
On some machines, pushes and pops (of integers) can be written in one machine instruction, operating on a register with auto-increment and auto-decrement addressing.
The fact that most modern machines have stack operations as part of the instruction set enforces the idea that the stack is probably the most fundamental data structure in computer science, after the array.
It should come as no surprise that if we restrict the operations allowed on a list, those operations can be performed very quickly.
The big surprise, however, is that the small number of operations left are so powerful and important.
The third application gives a deep insight into how programs are organized.
Balancing Symbols Compilers check your programs for syntax errors, but frequently a lack of one symbol (such as a missing brace or comment starter) will cause the compiler to spill out a hundred lines of diagnostics without identifying the real error.
A useful tool in this situation is a program that checks whether everything is balanced.
Thus, every right brace, bracket, and parenthesis must correspond to its left counterpart.
Obviously, it is not worthwhile writing a huge program for this, but it turns out that it is easy to check these things.
For simplicity, we will just check for balancing of parentheses, brackets, and braces and ignore any other character that appears.
The simple algorithm uses a stack and is as follows:
If the character is an opening symbol, push it onto the stack.
If it is a closing symbol, then if the stack is empty report.
If the symbol popped is not the corresponding opening symbol, then report an error.
At end of ﬁle, if the stack is not empty report an error.
You should be able to convince yourself that this algorithm works.
It is clearly linear and actually makes only one pass through the input.
Extra work can be done to attempt to decide what to do when an error is reported—such as identifying the likely cause.
Postﬁx Expressions Suppose we have a pocket calculator and would like to compute the cost of a shopping trip.
To do so, we add a list of numbers and multiply the result by 1.06; this computes the purchase price of some items with local sales tax added.
On the other hand, some items are taxable and some are not, so if only the ﬁrst and last items were actually taxable, then the sequence.
The time to evaluate a postﬁx expression is O(N), because processing each element in the input consists of stack operations and thus takes constant time.
Notice that when an expression is given in postﬁx notation, there is no need to know any precedence rules; this is an obvious advantage.
Inﬁx to Postﬁx Conversion Not only can a stack be used to evaluate a postﬁx expression, but we can also use a stack to convert an expression in standard form (otherwise known as inﬁx) into postﬁx.
We will concentrate on a small version of the general problem by allowing only the operators +, *, (, ), and insisting on the usual precedence rules.
When an operand is read, it is immediately placed onto the output.
The correct thing to do is to place operators that have been seen, but not placed on the output, onto the stack.
We will also stack left parentheses when they are encountered.
If we see a right parenthesis, then we pop the stack, writing symbols until we encounter a (corresponding) left parenthesis, which is popped but not output.
One exception is that we never remove a ( from the stack except when processing a )
When the popping is done, we push the operator onto the stack.
Finally, if we read the end of input, we pop the stack until it is empty, writing symbols onto the output.
The idea of this algorithm is that when an operator is seen, it is placed on the stack.
However, some of the operators on the stack that have high precedence are now known to be completed and should be popped, as they will no longer be pending.
Thus prior to placing the operator on the stack, operators that are on the stack and are to be completed prior to the current operator, are popped.
We can view a left parenthesis as a high-precedence operator when it is an input symbol (so that pending operators remain pending), and a low-precedence operator when it is on the stack (so that it is not accidentally removed by an operator)
To see how this algorithm performs, we will convert the long inﬁx expression above into its postﬁx form.
First, the symbol a is read, so it is passed through to the output.
Next b is read and passed through to the output.
The state of affairs at this juncture is as follows:
Checking the stack, we ﬁnd that we will pop a * and place it on the output; pop the other +, which is not of lower but equal priority, on the stack; and then push the +
Since open parentheses do not get removed except when a closed parenthesis is being processed, there is no output.
Now we read a ), so the stack is emptied back to the (
We read a * next; it is pushed onto the stack.
The input is now empty, so we pop and output symbols from the stack until it is empty.
As before, this conversion requires only O(N) time and works in one pass through the input.
We can add subtraction and division to this repertoire by assigning subtraction and addition equal priority and multiplication and division equal priority.
A subtle point is that the expression a-b-c will be converted to ab-c- and not abc--
Our algorithm does the right thing, because these operators associate from left to right.
Method Calls The algorithm to check balanced symbols suggests a way to implement method calls in compiled procedural and object-oriented languages.1 The problem here is that when a call is made to a new method, all the variables local to the calling routine need to be saved by the system, since otherwise the new method will overwrite the memory used by the calling routine’s variables.
Furthermore, the current location in the routine must be saved so that the new method knows where to go after it is done.
The variables have generally been assigned by the compiler to machine registers, and there are certain to be conﬂicts (usually all methods get some variables assigned to register #1), especially if recursion is involved.
The reason that this problem is similar to balancing symbols is that a method call and method return are essentially the same as an open parenthesis and closed parenthesis, so the same ideas should work.
When there is a method call, all the important information that needs to be saved, such as register values (corresponding to variable names) and the return address (which can be obtained from the program counter, which is typically in a register), is saved “on a piece of paper” in an abstract way and put at the top of a pile.
Then the control is transferred to the new method, which is free to replace the registers with its values.
If it makes other method calls, it follows the same procedure.
When the method wants to return, it looks at the “paper” at the top of the pile and restores all the registers.
Since Java is interpreted, rather than compiled, some details in this section may not apply to Java, but the general concepts still do in Java and many other languages.
Figure 3.35 A bad use of recursion: printing a linked list.
Clearly, all of this work can be done using a stack, and that is exactly what happens in virtually every programming language that implements recursion.
The information saved is called either an activation record or stack frame.
Typically, a slight adjustment is made: The current environment is represented at the top of the stack.
The stack in a real computer frequently grows from the high end of your memory partition downward, and on many non-Java systems there is no checking for overﬂow.
There is always the possibility that you will run out of stack space by having too many simultaneously active methods.
Needless to say, running out of stack space is always a fatal error.
In languages and systems that do not check for stack overﬂow, programs crash without an explicit explanation.
In normal events, you should not run out of stack space; doing so is usually an indication of runaway recursion (forgetting a base case)
On the other hand, some perfectly legal and seemingly innocuous programs can cause you to run out of stack space.
The routine in Figure 3.35, which prints out a collection, is perfectly legal and actually correct.
It properly handles the base case of an empty collection, and the recursion is ﬁne.
Activation records are typically large because of all the information they contain, so this program is likely to run out of stack space.
If 20,000 elements are not enough to make the program crash, replace the number with a larger one.
This program is an example of an extremely bad use of recursion known as tail recursion.
Tail recursion refers to a recursive call at the last line.
Tail recursion can be mechanically eliminated by enclosing the body in a while loop and replacing the recursive call with one assignment per method argument.
This simulates the recursive call because nothing needs to be saved; after the recursive call ﬁnishes, there is really no need to know the saved values.
Because of this, we can just go to the top of the method with the values that would have been used in a recursive call.
The method in Figure 3.36 shows the mechanically improved version.
Removal of tail recursion is so simple that some compilers do it automatically.
Even so, it is best not to ﬁnd out that yours does not.
Figure 3.36 Printing a list without recursion; a compiler might do this.
Recursion can always be completely removed (compilers do so in converting to assembly language), but doing so can be quite tedious.
The general strategy requires using a stack and is worthwhile only if you can manage to put the bare minimum on the stack.
We will not dwell on this further, except to point out that although nonrecursive programs are certainly generally faster than equivalent recursive programs, the speed advantage rarely justiﬁes the lack of clarity that results from removing the recursion.
With a queue, however, insertion is done at one end, whereas deletion is performed at the other end.
The basic operations on a queue are enqueue, which inserts an element at the end of the list (called the rear), and dequeue, which deletes (and returns) the element at the start of the list (known as the front)
As with stacks, any list implementation is legal for queues.
Like stacks, both the linked list and array implementations give fast O(1) running times for every operation.
The linked list implementation is straightforward and left as an exercise.
For each queue data structure, we keep an array, theArray, and the positions front and back, which represent the ends of the queue.
The following ﬁgure shows a queue in some intermediate state.
To enqueue an element x, we increment currentSize and back, then set theArray[back]=x.
To dequeue an element, we set the return value to theArray[front], decrement currentSize, and then increment front.
After 10 enqueues, the queue appears to be full, since back is now at the last array index, and the next enqueue would be in a nonexistent position.
However, there might only be a few elements in the queue, because several elements may have already been dequeued.
Queues, like stacks, frequently stay small even in the presence of a lot of operations.
The simple solution is that whenever front or back gets to the end of the array, it is wrapped around to the beginning.
The extra code required to implement the wraparound is minimal (although it probably doubles the running time)
If incrementing either back or front causes it to go past the array, the value is reset to the ﬁrst position in the array.
Some programmers use different ways of representing the front and back of a queue.
For instance, some do not use an entry to keep track of the size, because they rely on the base case that when the queue is empty, back = front-1
The size is computed implicitly by comparing back and front.
This is a very tricky way to go, because there are some special cases, so be very careful if you need to modify code written this way.
Pick any style you like and make sure that all your routines.
Since there are a few options for implementation, it is probably worth a comment or two in the code, if you don’t use the currentSize ﬁeld.
In applications where you are sure that the number of enqueues is not larger than the capacity of the queue, the wraparound is not necessary.
As with stacks, dequeues are rarely performed unless the calling routines are certain that the queue is not empty.
Thus error checks are frequently skipped for this operation, except in critical code.
This is generally not justiﬁable, because the time savings that you are likely to achieve are minimal.
There are many algorithms that use queues to give efﬁcient running times.
For now, we will give some simple examples of queue usage.
When jobs are submitted to a printer, they are arranged in order of arrival.
Thus, essentially, jobs sent to a line printer are placed on a queue.2
Virtually every real-life line is (supposed to be) a queue.
For instance, lines at ticket counters are queues, because service is ﬁrst-come ﬁrst-served.
There are many network setups of personal computers in which the disk is attached to one machine, known as the ﬁle server.
Users on other machines are given access to ﬁles on a ﬁrst-come ﬁrst-served basis, so the data structure is a queue.
A whole branch of mathematics, known as queuing theory, deals with computing, probabilistically, how long users expect to wait on a line, how long the line gets, and other such questions.
The answer depends on how frequently users arrive to the line and how long it takes to process a user once the user is served.
Both of these parameters are given as probability distribution functions.
An example of an easy case would be a phone line with one operator.
If the operator is busy, callers are placed on a waiting line (up to some maximum limit)
This problem is important for businesses, because studies have shown that people are quick to hang up the phone.
If there are k operators, then this problem is much more difﬁcult to solve.
Problems that are difﬁcult to solve analytically are often solved by a simulation.
In our case, we would need to use a queue to perform the simulation.
If k is large, we also need other data structures to do this efﬁciently.
This amounts to a deletion from the middle of the queue, which is a violation of the strict deﬁnition.
Additional uses for queues abound, and as with stacks, it is staggering that such a simple data structure can be so important.
This chapter describes the concept of ADTs and illustrates the concept with three of the most common abstract data types.
The primary objective is to separate the implementation of the abstract data types from their function.
The program must know what the operations do, but it is actually better off not knowing how it is done.
Lists, stacks, and queues are perhaps the three fundamental data structures in all of computer science, and their use is documented through a host of examples.
In particular, we saw how stacks are used to keep track of method calls and how recursion is actually implemented.
This is important to understand, not just because it makes procedural languages possible, but because knowing how recursion is implemented removes a good deal of the mystery that surrounds its use.
Although recursion is very powerful, it is not an entirely free operation; misuse and abuse of recursion can result in programs crashing.
The operation printLots(L,P) will print the elements in L that are in positions speciﬁed by P.
You may use only the public Collections API container operations.
After M passes, the person holding the hot potato is eliminated, the circle closes ranks, and the game continues with the person who was sitting after the eliminated person picking up the hot potato.
Write a program to solve the Josephus problem for general values of M and N.
Why is theSize saved prior to entering the for loop? b.
What is the running time of removeFirstHalf if lst is an ArrayList? c.
What is the running time of removeFirstHalf if lst is a LinkedList? d.
Does using an iterator make removeHalf faster for either type of List?
Method addAll adds all items in the speciﬁed collection given by items to the end of the MyArrayList.
The method signature for you to use is slightly different than the one in the Java Collections API, and is as follows:
Method removeAll removes all items in the speciﬁed collection given by items from the MyLinkedList.
The method signature for you to use is slightly different than the one in the Java Collections API, and is as follows:
The ListIterator interface in java.util has more methods than are shown in Section 3.3.5
Notice that you will write a listIterator method to return a newly constructed ListIterator, and further, that the existing iterator method can return a newly constructed ListIterator.
Thus you will change ArrayListIterator so that it implements ListIterator instead of Iterator.
Then you could print a MyArrayList L in reverse by using the code.
To delete an element, we merely mark it deleted (using an extra bit ﬁeld)
The number of deleted and nondeleted elements in the list is kept as part of the data structure.
If there are as many deleted elements as nondeleted elements, we traverse the entire list, performing the standard deletion algorithm on all marked nodes.
Write routines to implement the standard linked list operations using lazy.
Explain how to print out an error message that is likely to reﬂect the probable cause.
Write a program to convert an inﬁx expression that includes (, ), +, -, *, and / to postﬁx.
Write a program to convert a postﬁx expression to inﬁx.
Your stack routines should not declare an overﬂow unless every slot in the array is used.
Write routines to support the deque that take O(1) time per operation.
This instruction implies that you cannot use recursion, but you may assume that your algorithm is a list member function.
In a self-adjusting list, all insertions are performed at the front.
A self-adjusting list adds a find operation, and when an element is accessed by a find, it is moved to the front of the list without changing the relative order of the other items.
Assume that you are given a linked list that contains N nodes.
Design an O(N) algorithm to determine if the list contains a cycle.
In a circular linked list, the last node’s next link links to the ﬁrst node.
Assume the list does not contain a header and that we can maintain, at most, one iterator corresponding to a node in the list.
For which of the following representations can all basic queue operations be performed in constant worst-case time? Justify your answers.
Maintain an iterator that corresponds to the ﬁrst item in the list.
Maintain an iterator that corresponds to the last item in the list.
We do not have references to any other nodes (except by following links)
Describe an O(1) algorithm that logically removes the value stored in such a node from the linked list, maintaining the integrity of the linked list.
Insert item x before position p (given by an iterator)
Remove the item stored at position p (given by an iterator)
For large amounts of input, the linear access time of linked lists is prohibitive.
In this chapter we look at a simple data structure for which the running time of most operations is O(logN) on average.
We also sketch a conceptually simple modiﬁcation to this data structure that guarantees the above time bound in the worst case and discuss a second modiﬁcation that essentially gives an O(logN) running time per operation for a long sequence of instructions.
The data structure that we are referring to is known as a binary search tree.
The binary search tree is the basis for the implementation of two library collections classes, TreeSet and TreeMap, which are used in many applications.
Trees in general are very useful abstractions in computer science, so we will discuss their use in other, more general applications.
See how trees are used to implement the ﬁle system of several popular operating systems.
Tk, each of whose roots are connected by a directed edge from r.
The root of each subtree is said to be a child of r, and r is the parent of each subtree root.
Figure 4.1 shows a typical tree using the recursive deﬁnition.
In the tree of Figure 4.2, the root is A.
Each node may have an arbitrary number of children, possibly zero.
Nodes with no children are known as leaves; the leaves in the tree above are B,C,H, I, P,Q,K, L,M, and N.
Nodes with the same parent are siblings; thus K, L, and M are all siblings.
Grandparent and grandchild relations can be deﬁned in a similar manner.
For any node ni, the depth of ni is the length of the unique path from the root to ni.
The height of ni is the length of the longest path from ni to a leaf.
The height of a tree is equal to the height of the root.
The depth of a tree is equal to the depth of the deepest leaf; this is always equal to the height of the tree.
One way to implement a tree would be to have in each node, besides its data, a link to each child of the node.
However, since the number of children per node can vary so greatly and is not known in advance, it might be infeasible to make the children direct links in the data.
The solution is simple: Keep the children of each node in a linked list of tree nodes.
Figure 4.4 shows how a tree might be represented in this implementation.
Null links are not drawn, because there are too many.
In the tree of Figure 4.4, node E has both a link to a sibling (F) and a link to a child (I), while some nodes have neither.
One of the popular uses is the directory structure in many common operating systems, including UNIX and DOS.
Figure 4.5 is a typical directory in the UNIX ﬁle system.
The asterisk next to the name indicates that /usr is itself a directory.
Each / after the ﬁrst indicates an edge; the result is the full pathname.
This hierarchical ﬁle system is very popular, because it allows users to organize their data logically.
Furthermore, two ﬁles in different directories can share the same name, because they must have different paths from the root and thus have different pathnames.
A directory in the UNIX ﬁle system is just a ﬁle with a list of all its children, so the directories are structured almost exactly in accordance.
Figure 4.6 Pseudocode to list a directory in a hierarchical ﬁle system.
Suppose we would like to list the names of all of the ﬁles in the directory.
Our output format will be that ﬁles that are depth di will have their names indented by di tabs.
The heart of the algorithm is the recursive method listAll.
This routine needs to be started with a depth of 0, to signify no indenting for the root.
This depth is an internal bookkeeping variable and is hardly a parameter that a calling routine should be expected to know about.
Thus the driver routine is used to interface the recursive routine to the outside world.
Each directory in the UNIX ﬁle system also has one entry that points to itself and another entry that points to the parent of the directory.
Thus, technically, the UNIX ﬁle system is not a tree, but is treelike.
The Java code to implement this is provided in the ﬁle FileSystem.java online.
It uses Java features that have not been discussed in the text.
The name of the ﬁle object is printed out with the appropriate number of tabs.
If the entry is a directory, then we process all children recursively, one by one.
These children are one level deeper and thus need to be indented an extra space.
In a preorder traversal, work at a node is performed before (pre) its children are processed.
When this program is run, it is clear that line 1 is executed exactly once per node, since each name is output once.
Furthermore, line 4 can be executed at most once for each child of each node.
But the number of children is exactly one less than the number of nodes.
Finally, the for loop iterates once per execution of line 4, plus once each time the loop ends.
Thus, the total amount of work is constant per node.
If there are N ﬁle names to be output, then the running time is O(N)
Figure 4.8 UNIX directory with ﬁle sizes obtained via postorder traversal.
Figure 4.9 Pseudocode to calculate the size of a directory.
Another common method of traversing a tree is the postorder traversal.
In a postorder traversal, the work at a node is performed after (post) its children are evaluated.
As an example, Figure 4.8 represents the same directory structure as before, with the numbers in parentheses representing the number of disk blocks taken up by each ﬁle.
Since the directories are themselves ﬁles, they have sizes too.
Suppose we would like to calculate the total number of blocks used by all the ﬁles in the tree.
The pseudocode method size in Figure 4.9 implements this strategy.
If the current object is not a directory, then size merely returns the number of blocks it uses.
Otherwise, the number of blocks used by the directory is added to the number of blocks (recursively) found in all the children.
To see the difference between the postorder traversal strategy and the preorder traversal strategy, Figure 4.10 shows how the size of each directory or ﬁle is produced by the algorithm.
A binary tree is a tree in which no node can have more than two children.
Figure 4.11 shows that a binary tree consists of a root and two subtrees, TL and TR, both of which could possibly be empty.
A property of a binary tree that is sometimes important is that the depth of an average binary tree is considerably smaller than N.
Because a binary tree node has at most two children, we can keep direct links to them.
The declaration of tree nodes is similar in structure to that for doubly linked lists in that a node is a structure consisting of the element information plus two references (left and right) to other nodes (see Figure 4.13)
We could draw the binary trees using the rectangular boxes that are customary for linked lists, but trees are generally drawn as circles connected by lines, because they are.
Friendly data; accessible by other package routines Object element; // The data in the node BinaryNode left; // Left child BinaryNode right; // Right child.
We also do not explicitly draw null links when referring to trees, because every binary tree with N nodes would require N + 1 null links.
Binary trees have many important uses not associated with searching.
One of the principal uses of binary trees is in the area of compiler design, which we will now explore.
The leaves of an expression tree are operands, such as constants or variable names, and the other nodes contain operators.
This particular tree happens to be binary, because all the operators are binary, and although this is the simplest case, it is possible for nodes to have more than two children.
It is also possible for a node to have only one child, as is the case with the unary minus operator.
We can evaluate an expression tree, T, by applying the operator at the root to the values obtained by recursively evaluating the left and right subtrees.
We can produce an (overly parenthesized) inﬁx expression by recursively producing a parenthesized left expression, then printing out the operator at the root, and ﬁnally recursively producing a parenthesized right expression.
This general strategy (left, node, right) is known as an inorder traversal; it is easy to remember because of the type of expression it produces.
An alternate traversal strategy is to recursively print out the left subtree, the right subtree, and then the operator.
This traversal strategy is generally known as a postorder traversal.
We have seen this traversal strategy earlier in Section 4.1
A third traversal strategy is to print out the operator ﬁrst and then recursively print out the left and right subtrees.
We will return to these traversal strategies later in the chapter.
Constructing an Expression Tree We now give an algorithm to convert a postﬁx expression into an expression tree.
Since we already have an algorithm to convert inﬁx to postﬁx, we can generate expression trees from the two common types of input.
The method we describe strongly resembles the postﬁx evaluation algorithm of Section 3.6.3
If the symbol is an operand, we create a one-node tree and push it onto a stack.
The ﬁrst two symbols are operands, so we create one-node trees and push them onto a stack.3
Next, a + is read, so two trees are popped, a new tree is formed, and it is pushed onto the stack.
Next, c, d, and e are read, and for each a one-node tree is created and the corresponding tree is pushed onto the stack.
For convenience, we will have the stack grow from left to right in the diagrams.
Now a + is read, so two trees are merged.
Finally, the last symbol is read, two trees are merged, and the ﬁnal tree is left on the stack.
An important application of binary trees is their use in searching.
Let us assume that each node in the tree stores an item.
In our examples, we will assume for simplicity that these are integers, although arbitrarily complex items are easily handled in Java.
We will also assume that all the items are distinct and deal with duplicates later.
The property that makes a binary tree into a binary search tree is that for every node, X, in the tree, the values of all the items in its left subtree are smaller than the item in X, and the values of all the items in its right subtree are larger than the item in X.
Notice that this implies that all the elements in the tree can be ordered in some consistent manner.
In Figure 4.15, the tree on the left is a binary search tree, but the tree on the right is not.
We now give brief descriptions of the operations that are usually performed on binary search trees.
Note that because of the recursive deﬁnition of trees, it is common to write these routines recursively.
Because the average depth of a binary search tree turns out to be O(logN), we generally do not need to worry about running out of stack space.
The binary search tree requires that all the items can be ordered.
To write a generic class, we need to provide an interface type that represents this property.
The interface tells us that two items in the tree can always be compared using a compareTo method.
An alternative, described in Section 4.3.1, is to allow a function object.
Figure 4.16 also shows the BinaryNode class that, like the node class in the linked list class, is a nested class.
Figure 4.15 Two binary trees (only the left tree is a search tree)
The single data ﬁeld is a reference to the root node; this reference is null for empty trees.
The public methods use the general technique of calling private recursive methods.
This operation requires returning true if there is a node in tree T that has item X, or false if there is no such node.
If T is empty, then we can just return false.
Otherwise, if the item stored at T is X, we can return true.
Otherwise, we make a recursive call on a subtree of T, either left or right, depending on the relationship of X to the item stored in T.
The code in Figure 4.18 is an implementation of this strategy.
The remaining tests are arranged with the least likely case.
Also note that both recursive calls are actually tail recursions and can be easily removed with a while loop.
The use of tail recursion is justiﬁable here because the simplicity of algorithmic expression compensates for the decrease in speed, and the amount of stack space used is expected to be only O(log N)
Figure 4.19 shows the trivial changes required to use a function object rather than requiring that the items be Comparable.
These private routines return a reference to the node containing the smallest and largest elements in the tree, respectively.
To perform a findMin, start at the root and go left as long as there is a left child.
The findMax routine is the same, except that branching is to the right child.
This is so easy that many programmers do not bother using recursion.
We will code the routines both ways by doing findMin recursively and findMax nonrecursively (see Figure 4.20)
Notice how we carefully handle the degenerate case of an empty tree.
Although this is always important to do, it is especially crucial in recursive programs.
Also notice that it is safe to change t in findMax, since we are only working with a copy of a reference.
Always be extremely careful, however, because a statement such as t.right = t.right.right will make changes.
Figure 4.19 Illustrates use of a function object to implement binary search tree.
To insert X into tree T, proceed down the tree as you would with a contains.
Otherwise, insert X at the last spot on the path traversed.
Figure 4.20 Recursive implementation of findMin and nonrecursive implementation of findMax for binary search trees.
To insert 5, we traverse the tree as though a contains were occurring.
Duplicates can be handled by keeping an extra ﬁeld in the node record indicating the frequency of occurrence.
This adds some extra space to the entire tree but is better than putting duplicates in the tree (which tends to make the tree very deep)
Of course, this strategy does not work if the key that guides the compareTo method is only part of a larger structure.
If that is the case, then we can keep all of the structures that have the same key in an auxiliary data structure, such as a list or another search tree.
Since t references the root of the tree, and the root changes on the ﬁrst insertion, insert is written as a method that returns a reference to the root of the new tree.
As is common with many data structures, the hardest operation is deletion.
Once we have found the node to be deleted, we need to consider several possibilities.
If the node is a leaf, it can be deleted immediately.
If the node has one child, the node can be deleted after its parent adjusts a link to bypass the node (we will draw the link directions explicitly for clarity)
The complicated case deals with a node with two children.
The general strategy is to replace the data of this node with the smallest data of the right subtree (which is easily found) and recursively delete that node (which is now empty)
Because the smallest node in the right subtree cannot have a left child, the second remove is an easy one.
Figure 4.24 shows an initial tree and the result of a deletion.
It is replaced with the smallest data in its right subtree (3), and then that node is deleted as before.
It is inefﬁcient, because it makes two passes down the tree to ﬁnd and delete the smallest node in the right subtree when this is appropriate.
It is easy to remove this inefﬁciency by writing a special removeMin method, and we have left it in only for simplicity.
If the number of deletions is expected to be small, then a popular strategy to use is lazy deletion: When an element is to be deleted, it is left in the tree and merely marked.
This is especially popular if duplicate items are present, because then the ﬁeld that keeps count of the frequency of appearance can be decremented.
If the number of real nodes in the tree is the same as the number of “deleted” nodes, then the depth of the tree is only expected to go up by a small constant (why?), so there is a very small time penalty associated with lazy deletion.
Also, if a deleted item is reinserted, the overhead of allocating a new cell is avoided.
Intuitively, we expect that all of the operations of the previous section should take O(logN) time, because in constant time we descend a level in the tree, thus operating on a tree that is now roughly half as large.
Indeed, the running time of all the operations is O(d), where d is the depth of the node containing the accessed item (in the case of remove this may be the replacement node in the two-child case)
We prove in this section that the average depth over all nodes in a tree is O(logN) on the assumption that all insertion sequences are equally likely.
The sum of the depths of all nodes in a tree is known as the internal path length.
We will now calculate the average internal path length of a binary search tree, where the average is taken over all possible insertion sequences into binary search trees.
This recurrence will be encountered and solved in Chapter 7, obtaining an average value of D(N) = O(N logN)
It is tempting to say immediately that this result implies that the average running time of all the operations discussed in the previous section is O(logN), but this is not entirely.
We could try to eliminate the problem by randomly choosing between the smallest element in the right subtree and the largest in the left when replacing the deleted element.
This apparently eliminates the bias and should keep the trees balanced, but nobody has actually proved this.
In any event, this phenomenon appears to be mostly a theoretical novelty, because the effect does not show up at all for small trees, and stranger still, if o(N2) insert/remove pairs are used, then the tree seems to gain balance!
The main point of this discussion is that deciding what “average” means is generally extremely difﬁcult and can require assumptions that may or may not be valid.
In the absence of deletions, or when lazy deletion is used, we can conclude that the average running times of the operations above are O(logN)
Except for strange cases like the one discussed above, this result is very consistent with observed behavior.
If the input comes into a tree presorted, then a series of inserts will take quadratic time and give a very expensive implementation of a linked list, since the tree will consist only of nodes with no left children.
One solution to the problem is to insist on an extra structural condition called balance: No node is allowed to get too deep.
There are quite a few general algorithms to implement balanced trees.
Most are quite a bit more complicated than a standard binary search tree, and all take longer on average for updates.
They do, however, provide protection against the embarrassingly simple cases.
Below, we will sketch one of the oldest forms of balanced search trees, the AVL tree.
A second, newer method is to forgo the balance condition and allow the tree to be arbitrarily deep, but after every operation, a restructuring rule is applied that tends to make future operations efﬁcient.
These types of data structures are generally classiﬁed as self-adjusting.
In the case of a binary search tree, we can no longer guarantee an O(logN) bound on any single operation but can show that any sequence of M operations takes total time O(M logN) in the worst case.
This is generally sufﬁcient protection against a bad worst case.
An AVL (Adelson-Velskii and Landis) tree is a binary search tree with a balance condition.
The balance condition must be easy to maintain, and it ensures that the depth of the tree is O(logN)
The simplest idea is to require that the left and right subtrees have the same height.
As Figure 4.28 shows, this idea does not force the tree to be shallow.
S(h) is closely related to the Fibonacci numbers, from which the bound claimed above on the height of an AVL tree follows.
Thus, all the tree operations can be performed in O(logN) time, except possibly insertion (we will assume lazy deletion)
When we do an insertion, we need to update all the balancing information for the nodes on the path back to the root, but the reason that insertion is potentially difﬁcult is that inserting a node could violate the AVL tree property.
If this is the case, then the property has to be restored before the insertion step is considered over.
It turns out that this can always be done with a simple modiﬁcation to the tree, known as a rotation.
After an insertion, only nodes that are on the path from the insertion point to the root might have their balance altered because only those nodes have their subtrees altered.
As we follow the path up to the root and update the balancing information, we may ﬁnd a node whose new balance violates the AVL condition.
We will show how to rebalance the tree at the ﬁrst (i.e., deepest) such node, and we will prove that this rebalancing guarantees that the entire tree satisﬁes the AVL property.
The ﬁrst case, in which the insertion occurs on the “outside” (i.e., left–left or rightright), is ﬁxed by a single rotation of the tree.
The second case, in which the insertion occurs on the “inside” (i.e., left–right or right–left) is handled by the slightly more complex double rotation.
These are fundamental operations on the tree that we’ll see used several times in balanced-tree algorithms.
The remainder of this section describes these rotations, proves that they sufﬁce to maintain balance, and gives a casual implementation of the AVL tree.
Chapter 12 describes other balanced-tree methods with an eye toward a more careful implementation.
The before picture is on the left, and the after is on the right.
Node k2 violates the AVL balance property because its left subtree is two levels deeper than its right subtree (the dashed lines in the middle of the diagram mark the levels)
Subtree X has grown to an extra level, causing it to be exactly two levels deeper than Z.
To ideally rebalance the tree, we would like to move X up a level and Z down a level.
Note that this is actually more than the AVL property would require.
Here is an abstract scenario: Visualize the tree as being ﬂexible, grab the child node k1, close your eyes, and shake it, letting gravity take hold.
The result is that k1 will be the new root.
As a result of this work, which requires only a few link changes, we have another binary search tree that is an AVL tree.
This happens because X moves up one level, Y stays at the same level, and Z moves down one level.
Furthermore, the new height of the entire subtree is exactly the same as the height of the original subtree prior to the insertion that caused X to grow.
Thus no further updating of heights on the path to the root is needed, and consequently no further rotations are needed.
As we mentioned earlier, case 4 represents a symmetric case.
The ﬁrst problem occurs when it is time to insert 1 because the AVL property is violated at the root.
We perform a single rotation between the root and its left child to ﬁx the problem.
A dashed line joins the two nodes that are the subject of the rotation.
Besides the local change caused by the rotation, the programmer must remember that the rest of the tree has to be informed of this change.
Forgetting to do so is easy and would destroy the tree (4 would be inaccessible)
The next item we insert is 7, which causes another rotation:
The problem is that subtree Y is too deep, and a single rotation does not make it any less deep.
The double rotation that solves the problem is shown in Figure 4.35
The fact that subtree Y in Figure 4.34 has had an item inserted into it guarantees that it is nonempty.
Thus, we may assume that it has a root and two subtrees.
Consequently, the tree may be viewed as four subtrees connected by three nodes.
This is case 3, which is solved by a right–left double rotation.
Next we insert 14, which also requires a double rotation.
If 13 is now inserted, there is an imbalance at the root.
We insert 8 without a rotation creating an almost perfectly balanced tree:
Finally, we will insert 9 to show the symmetric case of the double rotation.
The programming details are fairly straightforward except that there are several cases.
To insert a new node with item X into an AVL tree T, we recursively insert X into the appropriate subtree of T (let us call this TLR)
If the height of TLR does not change, then we are done.
Otherwise, if a height imbalance appears in T, we do the appropriate single or double rotation depending on X and the items in T and TLR, update the heights (making the connection from the rest of the tree above), and are done.
Since one rotation always sufﬁces, a carefully coded nonrecursive version generally turns out to be faster than the recursive version, but on modern compilers the difference is not as signiﬁcant as in the past.
However, nonrecursive versions are quite difﬁcult to code correctly, whereas a casual recursive implementation is easily readable.
With all this, we are ready to write the AVL routines.
We show some of the code here; the rest is online.
Figure 4.38 Method to compute height of an AVL node.
This method is necessary to handle the annoying case of a null reference.
The basic insertion routine is easy to write (see Figure 4.39): It adds only a single line at the end that invokes a balancing method.
The balancing method applies a single or double rotation if needed, updates the height, and returns the resulting tree.
Since deletion in a binary search tree is somewhat more complicated than insertion, one can assume that deletion in an AVL tree is also more complicated.
In a perfect world, one would hope that the deletion routine in Figure 4.25 could easily be modiﬁed by changing the last line to return after calling the balance method, as was done for insertion.
This change works! A deletion could cause one side of the tree to become two levels shallower than the other side.
The case-by-case analysis is similar to the imbalances that are caused by insertion, but not exactly the same.
We leave veriﬁcation of the remaining cases as an exercise.
The basic idea of the splay tree is that after a node is accessed, it is pushed to the root by a series of AVL tree rotations.
Notice that if a node is deep, there are many nodes on the path that are also relatively deep, and by restructuring we can make future accesses cheaper on all these nodes.
Thus, if the node is unduly deep, then we want this restructuring to have the side effect of balancing the tree (to some extent)
Besides giving a good time bound in theory, this method is likely to have practical utility, because in many applications, when a node is accessed, it is likely to be accessed again in the near future.
Studies have shown that this happens much more often than one would expect.
Splay trees also do not require the maintenance of height or balance information, thus saving space and simplifying the code to some extent (especially when careful implementations are written)
This means that we rotate every node on the access path with its parent.
As an example, consider what happens after an access (a find) on k1 in the following tree.
First, we would perform a single rotation between k1 and its parent, obtaining the following tree.
Then two more rotations are performed until we reach the root.
The splaying strategy is similar to the rotation idea above, except that we are a little more selective about how rotations are performed.
Let X be a (nonroot) node on the access path at which we are rotating.
If the parent of X is the root of the tree, we merely rotate X and the root.
Otherwise, X has both a parent (P) and a grandparent (G), and there are two cases, plus symmetries, to consider.
The ﬁrst case is the zig-zag case (see Figure 4.45)
Here, X is a right child and P is a left child (or vice versa)
If this is the case, we perform a double rotation, exactly like an AVL double rotation.
Otherwise, we have a zig-zig case: X and P are both left children (or, in the symmetric case, both right children)
In that case, we transform the tree on the left of Figure 4.46 to the tree on the right.
As an example, consider the tree from the last example, with a contains on k1:
Although it is hard to see from small examples, splaying not only moves the accessed node to the root but also has the effect of roughly halving the depth of most nodes on the access path (some shallow nodes are pushed down at most two levels)
This takes a total of O(N), as before, and yields the same tree as simple rotations.
Thus we do not get the same bad behavior from splay trees that is prevalent in the simple rotation strategy.
Actually, this turns out to be a very good case.
A rather complicated proof shows that for this example, the N accesses take a total of O(N) time.
These ﬁgures highlight the fundamental and crucial property of splay trees.
When access paths are long, thus leading to a longer-than-normal search time, the rotations tend to be good for future operations.
When accesses are cheap, the rotations are not as good and can be bad.
The extreme case is the initial tree formed by the insertions.
All the insertions were constant-time operations leading to a bad initial tree.
At that point in time, we had a very bad tree, but we were running ahead of schedule and had the compensation of less total running time.
The main theorem, which we will prove in Chapter 11, is that we never fall behind a pace of O(logN) per operation: We are always on schedule, even though there are occasionally bad operations.
We can perform deletion by accessing the node to be deleted.
If it is deleted, we get two subtrees TL and TR (left and right)
If we ﬁnd the largest element in TL (which is easy), then this element is rotated to the root of TL, and TL will now have a root with no right child.
We can ﬁnish the deletion by making TR the right child.
The analysis of splay trees is difﬁcult, because it must take into account the ever-changing structure of the tree.
On the other hand, splay trees are much simpler to program than AVL trees, since there are fewer cases to consider and no balance information to maintain.
Some empirical evidence suggests that this translates into faster code in practice, although the case for this is far from complete.
Finally, we point out that there are several variations of splay trees that can perform even better in practice.
The recursive method in Figure 4.57 does the real work.
As we have seen before, this kind of routine when applied to trees is known as an inorder traversal (which makes sense, since it lists the items in order)
The general strategy of an inorder traversal is to process the left subtree ﬁrst, then perform processing at the current node, and ﬁnally process the right subtree.
The interesting part about this algorithm, aside from its simplicity, is that the total running time is O(N)
This is because there is constant work being performed at every node in the tree.
Each node is visited once, and the work performed at each node is testing against null, setting up two method calls, and doing a println.
Since there is constant work per node and N nodes, the running time is O(N)
Sometimes we need to process both subtrees ﬁrst before we can process a node.
For instance, to compute the height of a node, we need to know the height of the subtrees ﬁrst.
Since it is always a good idea to check the special cases—and crucial when recursion is involved—notice that the routine will declare the height of a leaf to be zero, which is correct.
This general order of traversal, which we have also seen before, is known as a postorder traversal.
Again, the total running time is O(N), because constant work is performed at each node.
Figure 4.57 Routine to print a binary search tree in order.
Figure 4.58 Routine to compute the height of a tree using a postorder traversal.
The third popular traversal scheme that we have seen is preorder traversal.
This could be useful, for example, if you wanted to label each node with its depth.
The common idea in all these routines is that you handle the null case ﬁrst, and then the rest.
These routines pass only the reference to the node that roots the subtree and do not declare or pass any extra variables.
The more compact the code, the less likely that a silly bug will turn up.
A fourth, less often used, traversal (which we have not seen yet) is level-order traversal.
Level-order traversal differs from the other traversals in that it is not done recursively; a queue is used, instead of the implied stack of recursion.
Thus far, we have assumed that we can store an entire data structure in the main memory of a computer.
Suppose, however, that we have more data than can ﬁt in main memory, meaning that we must have the data structure reside on disk.
When this happens, the rules of the game change because the Big-Oh model is no longer meaningful.
The problem is that a Big-Oh analysis assumes that all operations are equal.
However, this is not true, especially when disk I/O is involved.
That is pretty fast, mainly because the speed depends largely on electrical properties.
Its speed depends largely on the time it takes to spin the disk and to move a disk head.
On average, we might expect that we have to spin a disk halfway to ﬁnd what we are looking for, but this is compensated by the time to move the disk head, so we get an access time of 8.3 ms.
This is a very charitable estimate; 9–11 ms access times are more common.
Consequently, we can do approximately 120 disk accesses per second.
This sounds pretty good, until we compare it with the processor speed.
What we have is billions of instructions equal to 120 disk accesses.
Of course, everything here is a rough calculation, but the relative speeds are pretty clear: Disk accesses are incredibly expensive.
Furthermore, processor speeds are increasing at a much faster rate than disk speeds (it is disk sizes that are increasing quite quickly)
So we are willing to do lots of calculations just to save a disk access.
In almost all cases, it is the number of disk accesses that will dominate the running time.
Thus, if we halve the number of disk accesses, the running time will also halve.
Here is how the typical search tree performs on disk.
Suppose we want to access the driving records for citizens in the State of Florida.
Thus, in 1 sec, we can execute billions of instructions or perform six disk accesses.
The worst case of 1.44 logN is unlikely to occur, and the typical case is very close to logN.
We want to reduce the number of disk accesses to a very small constant, such as three or four; and we are willing to write complicated code to do this because machine instructions are essentially free, as long as we are not ridiculously unreasonable.
It should probably be clear that a binary search tree will not work, since the typical AVL tree is close to optimal height.
We cannot go below logN using a binary search tree.
The solution is intuitively simple: If we have more branching, we have less height.
Whereas a complete binary tree has height that is roughly log2 N, a complete M-ary tree has height that is roughly logM N.
One way to implement this is to use a B-tree.
Many variations and improvements are possible, and an implementation is somewhat complex because there are quite a few cases.
However, it is easy to see that, in principle, a B-tree guarantees only a few disk accesses.
A B-tree of order M is an M-ary tree with the following properties:5
The root is either a leaf or has between two and M children.
What is described is popularly known as a B+ tree.
The remaining issue is how to add and remove items from the B-tree; the ideas involved are sketched next.
A search down the tree reveals that it is not already in the tree.
We can then add it to the leaf as a ﬁfth item.
Note that we may have to reorganize all the data in the leaf.
However, the cost of doing this is negligible when compared to that of the disk access, which in this case also includes a disk write.
Of course, that was relatively painless because the leaf was not already full.
The solution is simple, however: Since we now have L+1 items, we split them into two leaves, both guaranteed to have the minimum number of data records needed.
Two disk accesses are required to write these leaves, and a third disk access is required to update the parent.
Note that in the parent, both keys and branches change, but they do so in a controlled way that is easily calculated.
Although splitting nodes is time-consuming because it requires at least two additional disk writes, it is a relatively rare occurrence.
Put another way, for every split, there are roughly L/2 nonsplits.
The node splitting in the previous example worked because the parent did not have its full complement of children.
When the parent is split, we must update the values of the keys and also the parent’s parent, thus incurring an additional two disk writes (so this insertion costs ﬁve disk writes)
However, once again, the keys change in a very controlled manner, although the code is certainly not simple because of a host of cases.
When a nonleaf node is split, as is the case here, its parent gains a child.
What if the parent already has reached its limit of children? In that case, we continue splitting nodes up the tree until either we ﬁnd a parent that does not need to be split or we reach the root.
If we split the root, then we have two roots.
Obviously, this is unacceptable, but we can create a new root that has the split roots as its two children.
This is why the root is granted the special two-child minimum exemption.
It also is the only way that a B-tree gains height.
Needless to say, splitting all the way up to the root is an exceptionally rare event, because a tree with four levels indicates that the root has been split three times throughout the entire sequence of insertions (assuming no deletions have occurred)
In fact, splitting of any nonleaf node is also quite rare.
There are other ways to handle the overﬂowing of children.
One technique is to put a child up for adoption should a neighbor have room.
This technique requires a modiﬁcation of the parent because the keys are affected.
However, it tends to keep nodes fuller and thus saves space in the long run.
We can perform deletion by ﬁnding the item that needs to be removed and then removing it.
The problem is that if the leaf it was in had the minimum number of data items, then it is now below the minimum.
We can rectify this situation by adopting a neighboring item, if the neighbor is not itself at its minimum.
If it is, then we can combine with the neighbor to form a full leaf.
Unfortunately, this means that the parent has lost a child.
If this loss causes the parent to fall below its minimum, then it follows the same strategy.
This process could percolate all the way up to the root.
The root cannot have just one child (and even if this were allowed, it would be silly)
If a root is left with one child as a result of the adoption process, then we remove the root and make its child the new root of the tree.
This is the only way for a B-tree to lose height.
Since the leaf has only two items, and its neighbor is already at its minimum of three, we combine the items into a new leaf of ﬁve items.
However, it can adopt from a neighbor because the neighbor has four children.
The List containers discussed in Chapter 3, namely ArrayList and LinkedList, are inefﬁcient for searching.
Consequently, the Collections API provides two additional containers, Set and Map, that provide efﬁcient implementations for basic operations such as insertion, deletion, and searching.
The Set interface represents a Collection that does not allow duplicates.
A special kind of Set, given by the SortedSet interface, guarantees that the items are maintained in sorted order.
Because a Set IS-A Collection, the idioms used to access items in a List, which are inherited from Collection, also work for a Set.
The print method described in Figure 3.6 will work if passed a Set.
The unique operations required by the Set are the abilities to insert, remove, and perform a basic search (efﬁciently)
For a Set, the add method returns true if the add succeeds and false if it fails because the item being added is already present.
The implementation of Set that maintains items in sorted order is a TreeSet.
By default, ordering assumes that the items in the TreeSet implement the Comparable interface.
An alternative ordering can be speciﬁed by instantiating the TreeSet with a Comparator.
A Map is an interface that represents a collection of entries that consists of keys and their values.
Keys must be unique, but several keys can map to the same values.
In a SortedMap, the keys in the map are maintained in logically sorted order.
The basic operations for a Map include methods such as isEmpty, clear, size, and most importantly, the following:
If there are no null values in the Map, the value returned by get can be used to determine if key is in the Map.
However, if there are null values, you have to use containsKey.
Method put places a key/value pair into the Map, returning either null or the old value associated with key.
Iterating through a Map is trickier than a Collection because the Map does not provide an iterator.
Instead, three methods are provided that return the view of a Map as a Collection.
Since the views are themselves Collections, the views can be iterated.
Methods keySet and values return simple collections (the keys contain no duplicates, thus the keys are returned in a Set)
The entrySet is returned as a Set of entries (there are no duplicate entries, since the keys are unique)
For an object of type Map.Entry, the available methods include accessing the key, the value, and changing the value:
KeyType getKey( ) ValueType getValue( ) ValueType setValue( ValueType newValue )
Java requires that TreeSet and TreeMap support the basic add, remove, and contains operations in logarithmic worst-case time.
Consequently, the underlying implementation is a balanced binary search tree.
Typically, an AVL tree is not used; instead, top-down red-black trees, which are discussed in Section 12.2, are often used.
An important issue in implementing TreeSet and TreeMap is providing support for the iterator classes.
Of course, internally, the iterator maintains a link to the “current” node.
The hard part is efﬁciently advancing to the next node.
There are several possible solutions, some of which are listed here:
When the iterator is constructed, have each iterator store as its data an array containing the TreeSet items.
This is lame, because we might as well use toArray and have no need for an iterator.
Have the iterator maintain a stack storing nodes on the path to the current node.
With this information, one can deduce the next node in the iteration, which is either the node in the current node’s right subtree that contains the minimum item, or the nearest ancestor that contains the current node in its left subtree.
This makes the iterator somewhat large, and makes the iterator code clumsy.
Have each node in the search tree store its parent in addition to the children.
The iterator is not as large, but there is now extra memory required in each node, and the code to iterate is still clumsy.
Have each node maintain extra links: one to the next smaller, and one to the next larger node.
This takes space, but the iteration is very simple to do, and it is easy to maintain these links.
Boolean variables to allow the routines to tell if a left link is being used as a standard binary search tree left link or a link to the next smaller node, and similarly for the right link (Exercise 4.50)
This idea is called a threaded tree, and is used in many balanced binary search tree implementations.
For instance, by changing the ﬁrst letter, the word wine can become dine, fine, line, mine, nine, pine, or vine.
By changing the third letter, wine can become wide, wife, wipe, or wire, among others.
By changing the fourth letter, wine can become wind, wing, wink, or wins, among others.
This gives 15 different words that can be obtained by changing only one letter in wine.
In fact, there are over 20 different words, some more obscure.
We would like to write a program to ﬁnd all words that can be changed into at least 15 other words by a single one-character substitution.
We assume that we have a dictionary consisting of approximately 89,000 different words of varying lengths.
In reality, the most changeable words are three-, four- and ﬁve-letter words, but the longer words are the time-consuming ones to check.
The most straightforward strategy is to use a Map in which the keys are words and the values are lists containing the words that can be changed from the key with a one-character substitution.
The routine in Figure 4.65 shows how the Map that is eventually produced (we have yet to write code for that part) can be used to print the required answers.
The code obtains the entry set and uses the enhanced for loop to step through the entry set and view entries that are pairs consisting of a word and a list of words.
Figure 4.65 Given a map containing words as keys and a list of words that differ in only one character as values, output words that have minWords or more words obtainable by a one-character substitution.
Figure 4.66 Routine to check if two words differ in only one character.
The main issue is how to construct the Map from an array that contains the 89,000 words.
The routine in Figure 4.66 is a straightforward function to test if two words are identical except for a one-character substitution.
We can use the routine to provide the simplest algorithm for the Map construction, which is a brute-force test of all pairs of words.
Among other things, this avoids repeated calls to cast from Object to String, which occur behind the scenes if generics are used.
Figure 4.67 Function to compute a map containing words as keys and a list of words that differ in only one character as values.
In the private update method, at line 26 we see if there is already a list of words associated with the key.
All in all, this is a standard idiom for maintaining a Map, in which the value is a collection.
The problem with this algorithm is that it is slow, and takes 75 seconds on our computer.
An obvious improvement is to avoid comparing words of different lengths.
We can do this by grouping words by their length, and then running the previous algorithm on each of the separate groups.
To do this, we can use a second map! Here the key is an integer representing a word length, and the value is a collection of all the words of that length.
We can use a List to store each collection, and the same idiom applies.
Compared to the ﬁrst algorithm, the second algorithm is only marginally more difﬁcult to code and runs in 16 seconds, or about ﬁve times as fast.
Our third algorithm is more complex, and uses additional maps! As before, we group the words by word length, and then work on each group separately.
Then ﬁrst we want to ﬁnd word pairs such as wine and nine that are identical except for the ﬁrst letter.
One way to do this, for each word of length 4, is to remove the ﬁrst character, leaving a three-character word representative.
Form a Map in which the key is the representative, and the value is a List of all words that have that representative.
For instance, in considering the ﬁrst character of the four-letter word group, representative "ine" corresponds to "dine", "fine", "wine", "nine", "mine", "vine", "pine", "line"
Representative "oot" corresponds to "boot", "foot", "hoot", "loot", "soot", "zoot"
Each individual List that is a value in this latest Map forms a clique of words in which any word can be changed to any other word by a onecharacter substitution, so after this latest Map is constructed, it is easy to traverse it and add entries to the original Map that is being computed.
We would then proceed to the second character of the four-letter word group, with a new Map.
And then the third character, and ﬁnally the fourth character.
It is interesting to note that although the use of the additional Maps makes the algorithm faster, and the syntax is relatively clean, the code makes no use of the fact that the keys of the Map are maintained in sorted order.
As such, it is possible that a data structure that supports the Map operations but does not guarantee sorted order can perform better, since it is being asked to do less.
Chapter 5 explores this possibility and discusses the ideas behind the alternative Map implementation, known as a HashMap.
A HashMap reduces the running time of the implementation from one second to roughly 0.8 seconds.
Figure 4.68 Function to compute a map containing words as keys and a list of words that differ in only one character as values.
Figure 4.69 Function to compute a map containing words as keys and a list of words that differ in only one character as values.
We have seen uses of trees in operating systems, compiler design, and searching.
Expression trees are a small example of a more general structure known as a parse tree, which is a central data structure in compiler design.
Parse trees are not binary but are relatively simple extensions of expression trees (although the algorithms to build them are not quite so simple)
They support almost all the useful operations, and the logarithmic average cost is very small.
Nonrecursive implementations of search trees are somewhat faster, but the recursive versions are sleeker, more elegant, and easier to understand and debug.
The problem with search trees is that their performance depends heavily on the input being random.
If this is not the case, the running time increases signiﬁcantly, to the point where search trees become expensive linked lists.
The operations that do not change the tree, as insertion does, can all use the standard binary search tree code.
This can be somewhat complicated, especially in the case of deletion.
We showed how to restore the tree after insertions in O(logN) time.
Nodes in splay trees can get arbitrarily deep, but after every access the tree is adjusted in a somewhat mysterious manner.
The net effect is that any sequence of M operations takes O(M logN) time, which is the same as a balanced tree would take.
In practice, the running time of all the balanced tree schemes, while slightly faster for searching, is worse (by a constant factor) for insertions and deletions than the simple binary search tree, but this is generally acceptable in view of the protection being given against easily obtained worst-case input.
Chapter 12 discusses some additional search tree data structures and provides detailed implementations.
A ﬁnal note: By inserting elements into a search tree and then performing an inorder traversal, we obtain the elements in sorted order.
This gives an O(N logN) algorithm to sort, which is a worst-case bound if any sophisticated search tree is used.
We shall see better ways in Chapter 7, but none that have a lower time bound.
Add to each node a link to the parent node.
Add to each node a link to the next smallest and next largest node.
To make your code simpler, add a header and tail node which are not part of the binary search tree, but help make the linked list part of the code simpler.
Explain how to generate a random integer between 1 and M that is already in the tree (so a random deletion can be performed)
Replace with the largest node, X, in TL and recursively remove X.
Alternately replace with the largest node in TL and the smallest node in TR, and.
Which strategy seems to give the most balance? Which takes the least CPU time to process the entire sequence?
Especially challenging are findMin and findMax, which must now be done recursively.
What is the minimum number of nodes in an AVL tree of height 15?
What is the smallest AVL tree that overﬂows an 8-bit height counter?
Show that if all nodes in a splay tree are accessed in sequential order, the resulting tree consists of a chain of left children.
Count the total number of rotations performed over the sequence.
How does the running time compare to AVL trees and unbalanced binary search trees?
This was done by assigning an (x, y) coordinate to each tree node, drawing a circle around each coordinate (this is hard to see in some pictures), and connecting each node to its parent.
Assume you have a binary search tree stored in memory (perhaps generated by one of the routines above) and that each node has two extra ﬁelds to store the coordinates.
The x coordinate can be computed by assigning the inorder traversal number.
Write a routine to do this for each node in the tree.
The y coordinate can be computed by using the negative of the depth of the.
Write a routine to do this for each node in the tree.
In terms of some imaginary unit, what will the dimensions of the picture be?
How can you adjust the units so that the tree is always roughly two-thirds as high as it is wide?
Prove that using this system no lines cross, and that for any node, X, all elements in X’s left subtree appear to the left of X and all elements in X’s right subtree appear to the right of X.
DrawLine(i, j) The ﬁrst instruction draws a circle at (X, Y), and the second instruction connects the ith circle to the jth circle (circles are numbered in the order drawn)
You should either make this a program and deﬁne some sort of input language or make this a method that can be called from any program.
Write a program that reads graph-assembler instructions and generates Java code that draws into a canvas.
Note that you have to scale the stored coordinates into pixels.
Write a method to decide whether two binary trees are similar.
For instance, the two trees in Figure 4.74 are isomorphic because they are the same if the children of A, B, and G, but not the other nodes, are swapped.
Give a polynomial time algorithm to decide if two trees are isomorphic.
What is the running time of your program (there is a linear solution)?
Give an algorithm to perform this transformation using O(N logN) rotations on average.
The operation findKth(k) returns the kth smallest item in the tree.
Explain how to modify the binary search tree to support this operation in O(logN) average time, without sacriﬁcing the time bounds of any other operation.
Suppose that if a node has a null left child, we make its left child link to its inorder predecessor, and if a node has a null right child, we make its right child link to its inorder successor.
This is known as a threaded tree, and the extra links are called threads.
How can we distinguish threads from real children links? b.
Write routines to perform insertion and deletion into a tree threaded in the.
Use the results of Exercise 4.6 to determine the average number of leaves in an N node binary search tree.
Each identiﬁer should be output with a list of line numbers on which it occurs.
The input ﬁle consists of a set of index entries.
Each line consists of the string IX:, followed by an index entry name enclosed in braces, followed by a page number that is enclosed in braces.
Each ! in an index entry name represents a sub-level.
Otherwise, do not collapse or expand ranges on your own.
Several papers deal with the lack of balance caused by biased deletion algorithms in binary search trees.
Simulation results for AVL trees, and variants in which the height imbalance is allowed to be at most k for various values of k, are presented in [21]
Analysis of the average search cost in AVL trees is incomplete, but some results are contained in [24]
The implementation described in the original paper allows data to be stored in internal nodes as well as leaves.
The data structure we have described is sometimes known as a B+-tree.
A survey of the different types of B-trees is presented in [9]
Empirical results of the various schemes are reported in [17]
In Chapter 4, we discussed the search tree ADT, which allowed various operations on a set of elements.
In this chapter, we discuss the hash table ADT, which supports only a subset of the operations allowed by binary search trees.
Hashing is a technique used for performing insertions, deletions, and searches in constant average time.
Tree operations that require any ordering information among the elements are not supported efﬁciently.
Thus, operations such as findMin, findMax, and the printing of the entire table in sorted order in linear time are not supported.
The central data structure in this chapter is the hash table.
The only remaining problems deal with choosing a function, deciding what to do when two keys hash to the same value (this is known as a collision), and deciding on the table size.
If the input keys are integers, then simply returning Key mod TableSize is generally a reasonable strategy, unless Key happens to have some undesirable properties.
In this case, the choice of hash function needs to be carefully considered.
For instance, if the table size is 10 and the keys all end in zero, then the standard hash function is a bad choice.
For reasons we shall see later, and to avoid situations like the one above, it is often a good idea to ensure that the table size is prime.
When the input keys are random integers, then this function is not only very simple to compute but also distributes the keys evenly.
Usually, the keys are strings; in this case, the hash function needs to be chosen carefully.
One option is to add up the ASCII (or Unicode) values of the characters in the string.
The hash function depicted in Figure 5.2 is simple to implement and computes an.
This hash function assumes that Key has at least three characters.
Even if none of these combinations collide, only 28 percent of the table can actually be hashed to.
Thus this function, although easily computable, is also not appropriate if the hash table is reasonably large.
The hash function takes advantage of the fact that overﬂow is allowed.
This may introduce a negative number; thus the extra test at the end.
The hash function described in Figure 5.4 is not necessarily the best with respect to table distribution but does have the merit of extreme simplicity and is reasonably fast.
If the keys are very long, the hash function will take too long to compute.
A common practice in this case is not to use all the characters.
The length and properties of the keys would then inﬂuence the choice.
For instance, the keys could be a complete street address.
The hash function might include a couple of characters from the street address and perhaps a couple of characters from the city name and ZIP code.
Some programmers implement their hash function by using only the characters in the odd spaces, with the idea that the time saved computing the hash function will make up for a slightly less evenly distributed function.
If, when an element is inserted, it hashes to the same value as an already inserted element, then we have a collision and need to resolve it.
We will discuss two of the simplest: separate chaining and open addressing; then we will look at some more recently discovered alternatives.
The ﬁrst strategy, commonly known as separate chaining, is to keep a list of all elements that hash to the same value.
If space is tight, it might be preferable to avoid their use (since those lists are doubly linked and waste space)
The table size is not prime but is used here for simplicity.
To perform a search, we use the hash function to determine which list to traverse.
To perform an insert, we check the appropriate list to see whether the element is already in place (if duplicates are expected, an extra ﬁeld is usually kept, and this ﬁeld would be incremented in the event of a match)
If the element turns out to be new, it is inserted at the front of the list, since it is convenient and also because frequently it happens that recently inserted elements are the most likely to be accessed in the near future.
The class skeleton required to implement separate chaining is shown in Figure 5.6
The hash table stores an array of linked lists, which are allocated in the constructor.
Figure 5.8 Example of Employee class that can be in a hash table.
Just as the binary search tree works only for objects that are Comparable, the hash tables in this chapter work only for objects that follow a certain protocol.
In Java such objects must provide an appropriate equals method and a hashCode method that returns an int.
The hash table can then scale this int into a suitable array index via myHash, as shown in Figure 5.7
Figure 5.8 illustrates an Employee class that can be stored in a hash table.
The Employee class provides an equals method and a hashCode method based on the Employee’s name.
The hashCode for the Employee class works by using the hashCode deﬁned in the Standard String class.
The code to implement contains, insert, and remove is shown in Figure 5.10
Figure 5.9 Constructors and makeEmpty for separate chaining hash table.
In the insertion routine, if the item to be inserted is already present, then we do nothing; otherwise, we place it in the list.
The element can be placed anywhere in the list; using add is most convenient in our case.
Any scheme could be used besides linked lists to resolve the collisions; a binary search tree or even another hash table would work, but we expect that if the table is large and the hash function is good, all the lists should be short, so basic separate chaining makes no attempt to try anything complicated.
Figure 5.10 contains, insert, and remove routines for separate chaining hash table.
The collision for 69 is handled in a similar manner.
As long as the table is big enough, a free cell can always be found, but the time to do so can get quite large.
Worse, even if the table is relatively empty, blocks of occupied cells start forming.
This effect, known as primary clustering, means that any key that hashes into the cluster will require several attempts to resolve the collision, and then it will add to the cluster.
The corresponding formulas, if clustering is not a problem, are fairly easy to derive.
We will assume a very large table and that each probe is independent of the previous probes.
Figure 5.11 Hash table with linear probing, after each insertion.
These formulas are clearly better than the corresponding formulas for linear probing.
Clustering is not only a theoretical problem but actually occurs in real implementations.
Figure 5.12 compares the performance of linear probing (dashed curves) with what would be expected from more random collision resolution.
Successful searches are indicated by an S, and unsuccessful searches and insertions are marked with U and I, respectively.
Figure 5.12 Number of probes plotted against load factor for linear probing (dashed) and random strategy (S is successful search, U is unsuccessful search, and I is insertion)
Figure 5.13 Hash table with quadratic probing, after each insertion.
Quadratic probing is a collision resolution method that eliminates the primary clustering problem of linear probing.
Quadratic probing is what you would expect—the collision function is quadratic.
Figure 5.13 shows the resulting hash table with this collision function on the same input used in the linear probing example.
For linear probing it is a bad idea to let the hash table get nearly full, because performance degrades.
For quadratic probing, the situation is even more drastic: There is no guarantee of ﬁnding an empty cell once the table gets more than half full, or even before the table gets half full if the table size is not prime.
This is because at most half of the table can be used as alternative locations to resolve collisions.
Indeed, we prove now that if the table is half empty and the table size is prime, then we are always guaranteed to be able to insert a new element.
If quadratic probing is used, and the table size is prime, then a new element can always be inserted if the table is at least half empty.
If the table is even one more than half full, the insertion could fail (although this is extremely unlikely)
It is also crucial that the table size be prime.1 If the table size is not prime, the number of alternative locations can be severely reduced.
Standard deletion cannot be performed in a probing hash table, because the cell might have caused a collision to go past it.
For instance, if we remove 89, then virtually all the remaining contains operations will fail.
Thus, probing hash tables require lazy deletion, although in this case there really is no laziness implied.
The class skeleton required to implement probing hash tables is shown in Figure 5.14
Instead of an array of lists, we have an array of hash table entry cells, which are also shown in Figure 5.14
Each entry in the array of HashEntry references is either.
Not null, and the entry is active (isActive is true)
Not null, and the entry is marked deleted (isActive is false)
Constructing the table (Figure 5.15) consists of allocating space and then setting each HashEntry reference to null.
We ensure in the insert routine that the hash table is at least twice as large as the number of elements in the table, so quadratic resolution will always work.
In the implementation in Figure 5.16, elements that are marked as deleted count as being in the table.
This can cause problems, because the table can get too full prematurely.
As with separate chaining hashing, we do nothing if x is already present.
Otherwise, we place it at the spot suggested by the findPos routine.
If the load factor exceeds 0.5, the table is full and we enlarge the hash table.
This is called rehashing, and is discussed in Section 5.5
Although quadratic probing eliminates primary clustering, elements that hash to the same position will probe the same alternative cells.
Simulation results suggest that it generally causes less than an extra half probe per search.
The following technique eliminates this, but does so at the cost of computing an extra hash function.
Figure 5.14 Class skeleton for hash tables using probing strategies, including the nested HashEntry class.
Figure 5.16 contains routine (and private helpers) for hashing with quadratic probing.
Figure 5.17 insert routine for hash tables with quadratic probing.
Figure 5.18 Hash table with double hashing, after each insertion.
If the table gets too full, the running time for the operations will start taking too long and insertions might fail for open addressing hashing with quadratic resolution.
This can happen if there are too many removals intermixed with insertions.
A solution, then, is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table.
Because the table is so full, a new table is created.
The size of this table is 17, because this is the ﬁrst prime that is twice as large as the old table size.
This is obviously a very expensive operation; the running time is O(N), since there are N elements to rehash and the table size is roughly 2N, but it is actually not all that bad, because it happens very infrequently.
On the other hand, if the hashing is performed as part of an interactive system, then the unfortunate user whose insertion caused a rehash could see a slowdown.
Rehashing can be implemented in several ways with quadratic probing.
One alternative is to rehash as soon as the table is half full.
This is why the new table is made twice as large as the old table.
A third, middle-of-the-road strategy is to rehash when the table reaches a certain load factor.
Since performance does degrade as the load factor increases, the third strategy, implemented with a good cutoff, could be best.
Figure 5.22 shows that rehashing is simple to implement, and provides an implementation for separate chaining rehashing also.
The items in the HashSet (or the keys in the HashMap) must provide an equals and hashCode method, as described earlier in Section 5.3
The HashSet and HashMap are currently implemented using separate chaining hashing.
These classes can be used if it is not important for the entries to be viewable in sorted order.
For instance, in the word-changing example in Section 4.8, there were three maps:
A map in which the key is a word length, and the value is a collection of all words of that word length.
A map in which the key is a representative, and the value is a collection of all words with that representative.
A map in which the key is a word, and the value is a collection of all words that differ in only one character from that word.
Because the order in which word lengths are processed does not matter, the ﬁrst map can be a HashMap.
Because the representatives are not even needed after the second map is built, the second map can be a HashMap.
The performance of a HashMap can often be superior to a TreeMap, but it is hard to know for sure without writing the code both ways.
Thus, in cases where either a HashMap or TreeMap is acceptable, it is preferable to declare variables using the interface type Map and then change the instantiation from a TreeMap to a HashMap, and perform timing tests.
Figure 5.22 Rehashing for both separate chaining and probing hash tables.
In Java, library types that can be reasonably inserted into a HashSet or as keys into a HashMap already have equals and hashCode deﬁned.
Because the expensive part of the hash table operations is computing the hashCode, the hashCode method in the String class contains an important optimization: Each String object stores internally the value of its hashCode.
Initially it is 0, but if hashCode is invoked, the value is remembered.
This technique is called caching the hash code, and represents another classic time-space tradeoff.
Figure 5.23 shows an implementation of the String class that caches the hash code.
Although two String objects with the same state must have their hash codes computed independently, there are many situations in which the same String object keeps having its hash code queried.
One situation where caching the hash code helps occurs during rehashing, because all the Strings involved in the rehashing have already had their hash codes cached.
On the other hand, caching the hash code does not help in the representative map for the word changing example.
Each of the representatives is a different String computed by removing a character from a larger String, and thus each individual String has to have its hash code computed separately.
However, in the third map, caching the hash code does help, because the keys are only Strings that were stored in the original array of Strings.
The hash tables that we have examined so far all have the property that with reasonable load factors, and appropriate hash functions, we can expect O(1) cost on average for insertions, removes, and searching.
But what is the expected worst case for a search assuming a reasonably well-behaved hash function?
Similar types of bounds are observed (or provable) for the length of the longest expected probe sequence in a probing hash table.
In some applications, such as hardware implementations of lookup tables for routers and memory caches, it is especially important that the search have a deﬁnite (i.e., constant) amount of completion time.
Let us assume that N is known in advance, so no rehashing is needed.
If we are allowed to rearrange items as they are inserted, then O(1) worst-case cost is achievable for searches.
In the remainder of this section we describe the earliest solution to this problem, namely perfect hashing, and then two more recent approaches that appear to offer promising alternatives to the classic hashing schemes that have been prevalent for many years.
Suppose, for purposes of simpliﬁcation, that all N items are known in advance.
If a separate chaining implementation could guarantee that each list had at most a constant number of items, we would be done.
We know that as we make more lists, the lists will on average be shorter, so theoretically if we have enough lists, then with a reasonably high probability we might expect to have no collisions at all!
But there are two fundamental problems with this approach: First, the number of lists might be unreasonably large; second, even with lots of lists, we might still get unlucky.
The second problem is relatively easy to address in principle.
Suppose we choose the number of lists to be M (i.e., TableSize is M), which is sufﬁciently large to guarantee that with probability at least 12 , there will be no collisions.
Then if a collision is detected, we simply clear out the table and try again using a different hash function that is independent of the ﬁrst.
If we still get a collision, we try a third hash function, and so on.
Section 5.8 discusses the crucial issue of how to produce additional hash functions.
If a pair (i, j) of balls are placed in the same bin, we call that a collision.
Let Ci,j be the expected number of collisions produced by any two balls (i, j)
Thus the expected number of collisions in the entire table is.
However, the preceding analysis suggests the following alternative: Use only N bins, but resolve the collisions in each bin by using hash tables instead of linked lists.
The idea is that because the bins are expected to have only a few items each, the hash table that is used for each bin can be quadratic in the bin size.
As with the original idea, each secondary hash table will be constructed using a different hash function until it is collision free.
The primary hash table can also be constructed several times if the number of collisions that are produced is higher than required.
All that remains to be shown is that the total size of the secondary hash tables is indeed expected to be linear.
If N items are placed into a primary hash table containing N bins, then the total size of the secondary hash tables has expected value at most 2N.
Once that is done, each secondary hash table will itself require only an average of two trials to be collision free.
After the tables are built, any lookup can be done in two probes.
Perfect hashing works if the items are all known in advance.
There are dynamic schemes that allow insertions and deletions (dynamic perfect hashing), but instead we will investigate two newer alternatives that are relatively easy to code and appear to be competitive in practice with the classic hashing algorithms.
We maintain two tables each more than half empty, and we have two independent has functions that can assign each item to a position in each table.
Cuckoo hashing maintains the invariant that an item is always stored in one of these two locations.
Immediately, this implies that a search in a cuckoo hash table requires at most two.
But there is an important detail: How is the table built? For instance, in Figure 5.25, there are only three available locations in the ﬁrst table for the six items, and there are only three available locations in the second table for the six items.
So there are only six available locations for these six items, and thus we must ﬁnd an ideal matching of slots for our six items.
One could argue that this means that the table would simply be too loaded (G would yield a 0.70 load factor), but at the same time, if the table had thousands of items, and were lightly loaded, but we had A, B, C, D, E, F, G with these hash positions, it would still be impossible to insert all seven of those items.
So it is not at all obvious that this scheme can be made to work.
The answer in this situation would be to pick another hash function, and this can be ﬁne as long as it is unlikely that this situation occurs.
The cuckoo hashing algorithm itself is simple: To insert a new item x, ﬁrst make sure it is not already there.
We can then use the ﬁrst hash function and if the (ﬁrst) table location is empty, the item can be placed.
So Figure 5.26 shows the result of inserting A into an empty hash table.
It happens that in this case it is not, but the algorithm that the standard cuckoo hash table uses does not bother to look.
It is easy to insert C, and this is shown in Figure 5.28
Note also, that the Table 2 location is not already taken, but we don’t look there.
However, while that could be successful in general, in this case there is a cycle and the insertion will not terminate.
However, if the table’s load factor is at 0.5 or higher, then the probability of a cycle becomes drastically higher, and this scheme is unlikely to work well at all.
After the publication of cuckoo hashing, numerous extensions were proposed.
While this increases the cost of a lookup, it also drastically increases the theoretical space utilization.
In some applications the lookups through separate hash functions can be done in parallel and thus cost little to no additional time.
Another extension is to allow each table to store multiple keys; again this can increase space utilization and also make it easier to do insertions and can be more cache-friendly.
And ﬁnally, often cuckoo hash tables are implemented as one giant table with two (or more) hash functions that probe the entire table, and some variations attempt to place an item in the second hash table immediately if there is an available spot, rather than starting a sequence of displacements.
Cuckoo Hash Table Implementation Implementing cuckoo hashing requires a collection of hash functions; simply using hashCode to generate the collection of hash functions makes no sense, since any hashCode collisions will result in collisions in all the hash functions.
Figure 5.36 shows a simple interface that can be used to send families of hash functions to the cuckoo hash table.
We will code a variant that will allow an arbitrary number of hash functions (speciﬁed by the HashFamily object that constructs the hash table) which uses a single array that is addressed by all the.
Thus our implementation differs from the classic notion of two separately addressable hash tables.
We can implement the classic version by making relatively minor changes to the code; however, this version provided in this section seems to perform better in tests using simple hash functions.
We also deﬁne ALLOWED_REHASHES, which speciﬁes how many rehashes we will perform if evictions take too long.
In theory, ALLOWED_REHASHES can be inﬁnite, since we expect only a small constant number of rehashes are needed; in practice, depending on several factors such as the number of hash functions, the quality of the hash functions, and the load factor, the rehashes could signiﬁcantly slow things down, and it might be worthwhile to expand the table, even though this will cost space.
The data representation for the cuckoo hash table is straightforward: We store a simple array, the current size, and the collections of hash functions, represented in a HashFamily instance.
We also maintain the number of hash functions, even though that is always obtainable from the HashFamily instance.
In Figure 5.42, we can see that the basic plan is to check to see if the item is already present, returning if so.
Otherwise, we check to see if the table is fully loaded, and if so, we expand it.
Finally we call a helper routine to do all the dirty work.
The helper routine for insertion is shown in Figure 5.43
We declare a variable rehashes to keep track of how many attempts have been made to rehash in this insertion.
Our insertion routine is mutually recursive: If needed, insert eventually calls rehash, which eventually calls back into insert.
Thus rehashes is declared in an outer scope for code simplicity.
We have already tested that the item to insert is not already present.
At lines 15–25, we check to see if any of the valid.
Evicting the item purely randomly did not perform well in experiments: In particular, with only two hash functions, it tended to create cycles.
To alleviate the last problem, we maintain the last position that was evicted and if our random item was the last evicted item, we select a new random item.
This will loop forever if used with two hash functions, and both hash functions happen to probe to the same.
Figure 5.39 Routines to ﬁnd the location of an item in the cuckoo hash table and to compute the hash code for a given table.
The code for expand and rehash is shown in Figure 5.44
The zero-parameter rehash leaves the array size unchanged but creates a new array that is populated with newly chosen hash functions.
Finally, Figure 5.45 shows the StringHashFamily class that provides a set of simple hash functions for strings.
Figure 5.41 Routine to remove from a cuckoo hash table.
Hopscotch hashing is a new algorithm that tries to improve on the classic linear probing algorithm.
Recall that in linear probing, cells are tried in sequential order, starting from the hash location.
Because of primary and secondary clustering, this sequence can be long on average as the table gets loaded, and thus many improvements such as quadratic probing, double hashing, and so forth, have been proposed to reduce the number of collisions.
However, on some modern architectures, the locality produced by probing adjacent cells is a more signiﬁcant factor than the extra probes, and linear probing can still be practical or even a best choice.
The idea of hopscotch hashing is to bound the maximal length of the probe sequence by a predetermined constant that is optimized to the underlying computer’s architecture.
Doing so would give constant-time lookups in the worst case, and like cuckoo hashing, the lookup could be parallelized to simultaneously check the bounded set of possible locations.
If an insertion would place a new item too far from its hash location, then we efﬁciently go backward toward the hash location, evicting potential items.
If we are careful, the evictions can be done quickly and guarantee that those evicted are not placed too far from their hash locations.
The algorithm is deterministic in that given a hash function, either the items can be evicted or they can’t.
The latter case implies that the table is likely too crowded, and a rehash is in order; but this would happen only at extremely high load factors, exceeding 0.9
Figure 5.43 Insertion routine for cuckoo hashing uses a different algorithm that chooses the item to evict randomly, attempting not to re-evict the last item.
The table will attempt to select new hash functions (rehash) if there are too many evictions and will expand if there are too many rehashes.
Figure 5.44 Rehashing and expanding code for cuckoo hash tables.
Since position 11 is now close enough to the hash value of H, we can now insert H.
These steps, along with the changes to the Hop information, are shown in Figure 5.47
Linear probing suggests position 14, but of course that is too far away.
There we ﬁnd all zeros in the ﬁrst two positions.
The hops tell which of the positions in the block are occupied with cells containing this hash value.
Insertion of I continues: Next B is evicted, and ﬁnally we have a spot that is close enough to the hash value and can insert I.
Hopscotch hashing is a relatively new algorithm, but the initial experimental results are very promising, especially for applications that make use of multiple processors and require signiﬁcant parallelism and concurrency.
It remains to be seen if either cuckoo hashing or hopscotch hashing emerge as a practical alternative to the classic separate chaining and linear/quadratic probing schemes.
Although hash tables are very efﬁcient and have average cost per operation, assuming appropriate load factors, their analysis and performance depend on the hash function having two fundamental properties:
The hash function must be computable in constant time (i.e., independent of the number of items in the hash table)
The hash function must distribute its items uniformly among the array slots.
In particular, if the hash function is poor, then all bets are off, and the cost per operation can be linear.
In this section, we discuss universal hash functions, which allow us to choose the hash function randomly in such a way that condition 2 above is satisﬁed.
As in Section 5.7, we use M to represent Tablesize.
Although a strong motivation for the use of universal hash functions is to provide theoretical justiﬁcation for the assumptions used in the classic hash table analyses, these functions can also be used in applications that require a high level of robustness, in which worst-case (or even substantially degraded) performance, perhaps based on inputs generated by a saboteur or hacker, simply cannot be tolerated.
As in Section 5.7, we use M to represent TableSize.
The deﬁnition above means that if we choose a hash function randomly from a universal family H, then the probability of a collision between any two distinct items is at most 1/M, and when adding into a table with N items, the probability of a collision at the initial point is at most N/M, or the load factor.
The use of a universal hash function for separate chaining or hopscotch hashing would be sufﬁcient to meet the assumptions used in the analysis of those data structures.
However, it is not sufﬁcient for cuckoo hashing, which requires a stronger notion of independence.
In cuckoo hashing, we ﬁrst see if there is a vacant location; if there is not, and we do an eviction, a different item is now involved in looking for a vacant location.
This repeats until we ﬁnd the vacant location, or decide to rehash [generally within O(logN) steps]
In order for the analysis to work, each step must have a collision probability of N/M independently, with a different item x being subject to the hash function.
We can formalize this independence requirement in the following deﬁnition.
Our universal family H will consist of the following set of functions, where a and b are chosen randomly:
Our last topic in this chapter deals with the case where the amount of data is too large to ﬁt in main memory.
As we saw in Chapter 4, the main consideration then is the number of disk accesses required to retrieve data.
As before, we assume that at any point we have N records to store; the value of N changes over time.
Furthermore, at most M records ﬁt in one disk block.
If either probing hashing or separate chaining hashing is used, the major problem is that collisions could cause several blocks to be examined during a search, even for a well-distributed hash table.
Furthermore, when the table gets too full, an extremely expensive rehashing step must be performed, which requires O(N) disk accesses.
A clever alternative, known as extendible hashing, allows a search to be performed in two disk accesses.
Then any search after the ﬁrst would take one disk access, since, presumably, the root node could be stored in main memory.
The problem with this strategy is that the branching factor is so high that it would take considerable processing to determine which leaf the data were in.
If the time to perform this step could be reduced, then we would have a practical scheme.
This would go into the third leaf, but as the third leaf is already full, there is no room.
We thus split this leaf into two leaves, which are now determined by the ﬁrst three bits.
Notice that all the leaves not involved in the split are now pointed to by two adjacent directory entries.
Thus, although an entire directory is rewritten, none of the other leaves is actually accessed.
This very simple strategy provides quick access times for insert and search operations on large databases.
There are a few important details we have not considered.
First, it is possible that several directory splits will be required if the elements in a leaf agree in more than D + 1 leading bits.
This is an easy detail to take care of, but must not be forgotten.
Second, there is the possibility of duplicate keys; if there are more than M duplicates, then this algorithm does not work at all.
In this case, some other arrangements need to be made.
These possibilities suggest that it is important for the bits to be fairly random.
This can be accomplished by hashing the keys into a reasonably long integer—hence the name.
We close by mentioning some of the performance properties of extendible hashing, which are derived after a very difﬁcult analysis.
These results are based on the reasonable assumption that the bit patterns are uniformly distributed.
This is the same as for B-trees, which is not entirely surprising, since for both data structures new nodes are created when the (M + 1)th entry is added.
If M is very small, then the directory can get unduly large.
In this case, we can have the leaves contain links to the records instead of the actual records, thus increasing the value of M.
This adds a second disk access to each search operation in order to maintain a smaller directory.
If the directory is too large to ﬁt in main memory, the second disk access would be needed anyway.
Hash tables can be used to implement the insert and search operations in constant average time.
It is especially important to pay attention to details such as load factor when using hash tables, since otherwise the time bounds are not valid.
It is also important to choose the hash function carefully when the key is not a short string or integer.
For separate chaining hashing, the load factor should be close to 1, although performance does not signiﬁcantly degrade unless the load factor becomes very large.
For probing hashing, the load factor should not exceed 0.5, unless this is completely unavoidable.
Rehashing can be implemented to allow the table to grow (and shrink), thus maintaining a reasonable load factor.
This is important if space is tight and it is not possible just to declare a huge hash table.
Other alternatives such as cuckoo hashing and hopscotch hashing can also yield good results.
Because all these algorithms are constant time, it is difﬁcult to make strong statements about which hash table implementation is the “best”; recent simulation results provide conﬂicting guidance and suggest that the performance can depend strongly on the types of items being manipulated, the underlying computer hardware, and the programming language.
Binary search trees can also be used to implement insert and contains operations.
Although the resulting average time bounds are O(logN), binary search trees also support routines that require order and are thus more powerful.
Using a hash table, it is not possible to ﬁnd the minimum element.
It is not possible to search efﬁciently for a string unless the exact string is known.
A binary search tree could quickly ﬁnd all items in a certain range; this is not supported by hash tables.
Furthermore, the O(logN) bound is not necessarily that much more than O(1), especially since no multiplications or divisions are required by search trees.
On the other hand, the worst case for hashing generally results from an implementation error, whereas sorted input can make binary trees perform poorly.
Balanced search trees are quite expensive to implement, so if no ordering information is required and there is any suspicion that the input might be sorted, then hashing is the data structure of choice.
Compilers use hash tables to keep track of declared variables in source code.
Identiﬁers are typically short, so the hash function can be computed quickly, and alphabetizing the variables is often unnecessary.
A hash table is useful for any graph theory problem where the nodes have real names instead of numbers.
Here, as the input is read, vertices are assigned integers from 1 onward by order of appearance.
Again, the input is likely to have large groups of alphabetized entries.
A third common use of hash tables is in programs that play games.
As the program searches through different lines of play, it keeps track of positions it has seen by computing a hash function based on the position (and storing its move for that position)
If the same position reoccurs, usually by a simple transposition of moves, the program can avoid.
This general feature of all game-playing programs is known as the transposition table.
Yet another use of hashing is in online spelling checkers.
If misspelling detection (as opposed to correction) is important, an entire dictionary can be prehashed and words can be checked in constant time.
Hash tables are well suited for this, because it is not important to alphabetize words; printing out misspellings in the order they occurred in the document is certainly acceptable.
Hash tables are often used to implement caches, both in software (for instance, the cache in your Internet browser) and in hardware (for instance, the memory caches in modern computers)
In this case, we can rehash to a table half as large.
Assume that we rehash to a larger table when there are twice as many elements as the table size.
How empty should the table be before we rehash to a smaller table?
Do this by having findPos maintain, with an additional variable, the location of the ﬁrst inactive cell it encounters.
Explain the circumstances under which the revised algorithm is faster than the original algorithm.
Is it worth computing this once prior to entering the loop?
Since computing the hash function is expensive, suppose objects provide a hash member function of their own, and each object stores the result in an additional data member the ﬁrst time the hash function is computed for it.
Show how such a scheme would apply for the Employee class in Figure 5.8, and explain under what circumstances the remembered hash value remains valid in each Employee.
Each polynomial is represented as a linked list of objects consisting of a coefﬁcient and an exponent (Exercise 3.12)
One method is to sort these terms and combine like terms, but this requires sorting MN records, which could be expensive, especially in small-memory environments.
Alternatively, we could merge terms as they are computed and then sort the result.
If the output polynomial has about O(M + N) terms, what is the running time.
Show that the running time is O(k + N) plus the time spent refuting false matches.
Show that the expected number of false matches is negligible.
Explain how hash tables can be used by the compiler to implement this language addition.
Control is passed by use of a goto or gosub and a statement number.
Write a program that reads in a legal BASIC program and renumbers the statements so that the ﬁrst starts at number F and each statement has a number D higher than the previous statement.
You may assume an upper limit of N statements, but the statement numbers in the input might be as large as a 32-bit integer.
Implement the word puzzle program using the algorithm described at the end of the chapter.
We can get a big speed increase by storing, in addition to each word W, all of W’s preﬁxes.
If one of W’s preﬁxes is another word in the dictionary, it is stored as a real word.
Although this may seem to increase the size of the hash table drastically, it does not, because many words have the same preﬁxes.
When a scan is performed in a particular direction, if the word that is looked up is not even in the hash table as a preﬁx, then the scan in that direction can be terminated early.
Use this idea to write an improved program to solve the word puzzle.
If we are willing to sacriﬁce the sanctity of the hash table ADT, we can speed up the program in part (b) by noting that if, for example, we have just computed the hash function for “excel,” we do not need to compute the hash function for “excels” from scratch.
Adjust your hash function so that it can take advantage of its previous calculation.
Incorporate the idea of using preﬁxes into your binary search algorithm.
The implementation will store a hash table of pairs (key, deﬁnition)
Assume that the dictionary comes from two sources: an existing large dictionary and a second ﬁle containing a personal dictionary.
Output all misspelled words and the line numbers in which they occur.
Also, for each misspelled word, list any words in the dictionary that are obtainable by applying any of the following rules: a.
The simplest way to do this is to use a single array and modify the hash function to access either the top half or the bottom half.
If the table is small enough to ﬁt in main memory, how does its performance compare with separate chaining and open addressing hashing?
Despite the apparent simplicity of hashing, much of the analysis is quite difﬁcult, and there are still many unresolved questions.
Luhn wrote an internal IBM memorandum that used separate chaining hashing.
A wealth of information on the subject, including an analysis of hashing with linear probing under the assumption of totally random and independent hashing, can be found in [25]
Precise analytic and simulation results for separate chaining, linear probing, quadratic probing, and double hashing can be found in [19]
However, due to changes (improvements) in computer architecture and compilers, simulation results tend to quickly become dated.
Yet another collision resolution scheme is coalesced hashing, described in [33]
Yao [37] has shown the uniform hashing, in which no clustering exists, is optimal with respect to cost of a successful search, assuming that items cannot move once placed.
Although jobs sent to a printer are generally placed on a queue, this might not always be the best thing to do.
For instance, one job might be particularly important, so it might be desirable to allow that job to be run as soon as the printer is available.
Unfortunately, most systems do not do this, which can be particularly annoying at times.
Similarly, in a multiuser environment, the operating system scheduler must decide which of several processes to run.
Generally a process is allowed to run only for a ﬁxed period of time.
Jobs are initially placed at the end of the queue.
The scheduler will repeatedly take the ﬁrst job on the queue, run it until either it ﬁnishes or its time limit is up, and place it at the end of the queue if it does not ﬁnish.
This strategy is generally not appropriate, because very short jobs will seem to take a long time because of the wait involved to run.
Generally, it is important that short jobs ﬁnish as fast as possible, so these jobs should have precedence over jobs that have already been running.
Furthermore, some jobs that are not short are still very important and should also have precedence.
This particular application seems to require a special kind of queue, known as a priority queue.
The data structures we will see are among the most elegant in computer science.
A priority queue is a data structure that allows at least the following two operations: insert, which does the obvious thing; and deleteMin, which ﬁnds, returns, and removes the minimum element in the priority queue.
The insert operation is the equivalent of enqueue, and deleteMin is the priority queue equivalent of the queue’s dequeue operation.
As with most data structures, it is sometimes possible to add other operations, but these are extensions and not part of the basic model depicted in Figure 6.1
In Chapter 7, we will see how priority queues are used for external sorting.
There are several obvious ways to implement a priority queue.
We could use a simple linked list, performing insertions at the front in O(1) and traversing the list, which requires O(N) time, to delete the minimum.
Alternatively, we could insist that the list be kept always sorted; this makes insertions expensive (O(N)) and deleteMins cheap (O(1))
The former is probably the better idea of the two, based on the fact that there are never more deleteMins than insertions.
Another way of implementing priority queues would be to use a binary search tree.
This gives an O(logN) average running time for both operations.
This is true in spite of the fact that although the insertions are random, the deletions are not.
Recall that the only element we ever delete is the minimum.
Repeatedly removing a node that is in the left subtree would seem to hurt the balance of the tree by making the right subtree heavy.
In the worst case, where the deleteMins have depleted the left subtree, the right subtree would have at most twice as many elements as it should.
This adds only a small constant to its expected depth.
Notice that the bound can be made into a worst-case bound by using a balanced tree; this protects one against bad insertion sequences.
Using a search tree could be overkill because it supports a host of operations that are not required.
The basic data structure we will use will not require links and will support both operations in O(logN) worst-case time.
Insertion will actually take constant time on average, and our implementation will allow building a priority queue of N items in linear time, if no deletions intervene.
We will then discuss how to implement priority queues to support efﬁcient merging.
This additional operation seems to complicate matters a bit and apparently requires the use of a linked structure.
The implementation we will use is known as a binary heap.
Its use is so common for priority queue implementations that, in the context of priority queues, when the word heap is used without a qualiﬁer, it is generally assumed to be referring to this implementation.
In this section, we will refer to binary heaps merely as heaps.
Like binary search trees, heaps have two properties, namely, a structure property and a heaporder property.
As with AVL trees, an operation on a heap can destroy one of the properties, so a heap operation must not terminate until all heap properties are in order.
A heap is a binary tree that is completely ﬁlled, with the possible exception of the bottom level, which is ﬁlled from left to right.
Such a tree is known as a complete binary tree.
An important observation is that because a complete binary tree is so regular, it can be represented in an array and no links are necessary.
The only problem with this implementation is that an estimate of the maximum heap size is required in advance, but typically this is not a problem (and we can resize if necessary)
The array has a position 0; more on this later.
A heap data structure will, then, consist of an array (of Comparable objects) and an integer representing the current heap size.
Throughout this chapter, we shall draw the heaps as trees, with the implication that an actual implementation will use simple arrays.
Figure 6.5 Two complete trees (only the left tree is a heap)
The property that allows operations to be performed quickly is the heap-order property.
Since we want to be able to ﬁnd the minimum quickly, it makes sense that the smallest element should be at the root.
If we consider that any subtree should also be a heap, then any node should be smaller than all of its descendants.
In a heap, for every node X, the key in the parent of X is smaller than (or equal to) the key in X, with the exception of the root (which has no parent).1 In Figure 6.5 the tree on the left is a heap, but the tree on the right is not (the dashed line shows the violation of heap order)
By the heap-order property, the minimum element can always be found at the root.
Thus, we get the extra operation, findMin, in constant time.
It is easy (both conceptually and practically) to perform the two required operations.
All the work involves ensuring that the heap-order property is maintained.
If X can be placed in the hole without violating heap order, then we do so and are done.
Otherwise we slide the element that is in the hole’s parent node into the hole, thus bubbling the hole up toward the root.
We continue this process until X can be placed in the hole.
Analogously, we can declare a (max) heap, which enables us to efﬁciently ﬁnd and remove the maximum element, by changing the heap-order property.
Thus, a priority queue can be used to ﬁnd either a minimum or a maximum, but this needs to be decided ahead of time.
This general strategy is known as a percolate up; the new element is percolated up the heap until the correct location is found.
Insertion is easily implemented with the code shown in Figure 6.8
We could have implemented the percolation in the insert routine by performing repeated swaps until the correct order was established, but a swap requires three assignment statements.
If an element is percolated up d levels, the number of assignments performed by the swaps would be 3d.
If the element to be inserted is the new minimum, it will be pushed all the way to the top.
At some point, hole will be 1 and we will want to break out of the loop.
We could do this with an explicit test, or we can put a reference to the inserted item in position 0 in order to make the loop terminate.
We elect to place x into position 0 in our implementation.
The time to do the insertion could be as much as O(logN), if the element to be inserted is the new minimum and is percolated all the way to the root.
Finding the minimum is easy; the hard part is removing it.
When the minimum is removed, a hole is created at the root.
Since the heap now becomes one smaller, it follows that the last element X in the heap must move somewhere in the heap.
If X can be placed in the hole, then we are done.
This is unlikely, so we slide the smaller of the hole’s children into the hole, thus pushing the hole down one level.
We repeat this step until X can be placed in the hole.
Thus, our action is to place X in its correct spot along a path from the root containing minimum children.
In Figure 6.9 the left ﬁgure shows a heap prior to the deleteMin.
The value 31 cannot be placed in the hole, because this would violate heap order.
We use the same technique as in the insert routine to avoid the use of swaps in this routine.
A frequent implementation error in heaps occurs when there are an even number of elements in the heap, and the one node that has only one child is encountered.
You must make sure not to assume that there are always two children, so this usually involves an extra test.
One extremely tricky solution is always to ensure that your algorithm thinks every node has two children.
Do this by placing a sentinel, of value higher than any in the heap, at the spot after the heap ends, at the start of each percolate down when the heap size is even.
You should think very carefully before attempting this, and you must put in a prominent comment if you do use this technique.
Although this eliminates the need to test for the presence of a right child, you cannot eliminate the requirement that you test when you reach the bottom, because this would require a sentinel for every leaf.
On average, the element that is placed at the root is percolated almost to the bottom of the heap (which is the level it came from), so the average running time is O(logN)
Figure 6.12 Method to perform deleteMin in a binary heap.
Notice that although ﬁnding the minimum can be performed in constant time, a heap designed to ﬁnd the minimum element (also known as a (min)heap) is of no help whatsoever in ﬁnding the maximum element.
In fact, a heap has very little ordering information, so there is no way to ﬁnd any particular element without a linear scan through the entire heap.
To see this, consider the large heap structure (the elements are not shown) in Figure 6.13, where we see that the only information known about the maximum element is that it is at one of the leaves.
Half the elements, though, are contained in leaves, so this is practically useless information.
For this reason, if it is important to know where elements are, some other data structure, such as a hash table, must be used in addition to the heap.
Recall that the model does not allow looking inside the heap.
If we assume that the position of every element is known by some other method, then several other operations become cheap.
The ﬁrst three operations below all run in logarithmic worst-case time.
Since this might violate the heap order, it must be ﬁxed by a percolate up.
This operation could be useful to system administrators: They can make their programs run with highest priority.
Many schedulers automatically drop the priority of a process that is consuming excessive CPU time.
This constructor takes as input N items and places them into a heap.
Since each insert will take O(1) average and O(logN) worstcase time, the total running time of this algorithm would be O(N) average but O(N logN) worst-case.
Since this is a special instruction and there are no other operations intervening, and we already know that the instruction can be performed in linear average time, it is reasonable to expect that with reasonable care a linear time bound can be guaranteed.
The general algorithm is to place the N items into the tree in any order, maintaining the structure property.
Then, if percolateDown(i) percolates down from node i, the buildHeap routine in Figure 6.14 can be used by the constructor to create a heap-ordered tree.
The ﬁrst tree in Figure 6.15 is the unordered tree.
Each dashed line corresponds to two comparisons: one to ﬁnd the smaller child and one to compare the smaller child with the node.
To bound the running time of buildHeap, we must bound the number of dashed lines.
This can be done by computing the sum of the heights of all the nodes in the heap, which is the maximum number of dashed lines.
What we would like to show is that this sum is O(N)
A complete tree is not a perfect binary tree, but the result we have obtained is an upper bound on the sum of the heights of the nodes in a complete tree.
We have already mentioned how priority queues are used in operating systems design.
In Chapter 9, we will see how priority queues are used to implement several graph algorithms efﬁciently.
Here we will show how to use priority queues to obtain solutions to two problems.
Recall that the input is a list of N elements, which can be totally ordered, and an integer k.
The selection problem is to ﬁnd the kth largest element.
Algorithm 6A For simplicity, we assume that we are interested in ﬁnding the kth smallest element.
The last element extracted from the heap is our answer.
It should be clear that by changing the heap-order property, we could solve the original problem of ﬁnding the kth largest element.
Notice that if we run this program for k = N and record the values as they leave the heap, we will have essentially sorted the input ﬁle in O(N logN) time.
In Chapter 7, we will reﬁne this idea to obtain a fast sorting algorithm known as heapsort.
Algorithm 6B For the second algorithm, we return to the original problem and ﬁnd the kth largest element.
At any point in time we will maintain a set S of the k largest elements.
After the ﬁrst k elements are read, when a new element is read it is compared with the kth largest element, which we denote by Sk.
If the new element is larger, then it replaces Sk in S.
At the end of the input, we ﬁnd the smallest element in S and return it as the answer.
In Chapter 7, we will see how to solve this problem in O(N) average time.
In Chapter 10, we will see an elegant, albeit impractical, algorithm to solve this problem in O(N) worst-case time.
Recall that we have a system, such as a bank, where customers arrive and wait in a line until one of k tellers is available.
Customer arrival is governed by a probability distribution function, as is the service time (the amount of time to be served once a teller is available)
We are interested in statistics such as how long on average a customer has to wait or how long the line might be.
With certain probability distributions and values of k, these answers can be computed exactly.
However, as k gets larger, the analysis becomes considerably more difﬁcult, so it is appealing to use a computer to simulate the operation of the bank.
In this way, the bank ofﬁcers can determine how many tellers are needed to ensure reasonably smooth service.
The two events here are (a) a customer arriving and (b) a customer departing, thus freeing up a teller.
We can use the probability functions to generate an input stream consisting of ordered pairs of arrival time and service time for each customer, sorted by arrival time.
We do not need to use the exact time of day.
Rather, we can use a quantum unit, which we will refer to as a tick.
One way to do this simulation is to start a simulation clock at zero ticks.
We then advance the clock one tick at a time, checking to see if there is an event.
If there is, then we process the event(s) and compile statistics.
When there are no customers left in the input stream and all the tellers are free, then the simulation is over.
The problem with this simulation strategy is that its running time does not depend on the number of customers or events (there are two events per customer), but instead depends on the number of ticks, which is not really part of the input.
To see why this is important, suppose we changed the clock units to milliticks and multiplied all the times in the input by 1,000
The result would be that the simulation would take 1,000 times longer!
The key to avoiding this problem is to advance the clock to the next event time at each stage.
At any point, the next event that can occur is either (a) the next customer in the input ﬁle arrives or (b) one of the customers at a teller leaves.
Since all the times when the events will happen are available, we just need to ﬁnd the event that happens nearest in the future and process that event.
If the event is a departure, processing includes gathering statistics for the departing customer and checking the line (queue) to see whether there is another customer waiting.
If so, we add that customer, process whatever statistics are required, compute the time when that customer will leave, and add that departure to the set of events waiting to happen.
It is then straightforward, although possibly time-consuming, to write the simulation routines.
Binary heaps are so simple that they are almost always used when priority queues are needed.
A simple generalization is a d-heap, which is exactly like a binary heap except that all nodes have d children (thus, a binary heap is a 2-heap)
The most glaring weakness of the heap implementation, aside from the inability to perform finds, is that combining two heaps into one is a hard operation.
There are quite a few ways of implementing heaps so that the running time of a merge is O(logN)
We will now discuss three data structures, of various complexity, that support the merge operation efﬁciently.
Like a binary heap, a leftist heap has both a structural property and an ordering property.
Indeed, a leftist heap, like virtually all heaps used, has the same heap-order property we have already seen.
The only difference between a leftist heap and a binary heap is that leftist heaps are not perfectly balanced but actually attempt to be very unbalanced.
The leftist heap property is that for every node X in the heap, the null path length of the left child is at least as large as that of the right child.
This property is satisﬁed by only one of the trees in Figure 6.20, namely, the tree on the left.
Figure 6.20 Null path lengths for two trees; only the left tree is leftist.
Indeed, a tree consisting of a long path of left nodes is possible (and actually preferable to facilitate merging)—hence the name leftist heap.
Because leftist heaps tend to have deep left paths, it follows that the right path ought to be short.
Indeed, the right path down a leftist heap is as short as any in the heap.
Otherwise, there would be a path that goes through some node X and takes the left child.
Notice that insertion is merely a special case of merging, since we may view an insertion as a merge of a one-node heap with a larger heap.
We will ﬁrst give a simple recursive solution and then show how this might.
In addition to space for the data and left and right references, each node will have an entry that indicates the null path length.
If either of the two heaps is empty, then we can return the other heap.
Otherwise, to merge the two heaps, we compare their roots.
First, we recursively merge the heap with the larger root with the right subheap of the heap with the smaller root.
Since this tree is formed recursively, and we have not yet ﬁnished the description of the algorithm, we cannot at this point show how this heap was obtained.
However, it is reasonable to assume that the resulting tree is a leftist heap, because it was obtained via a recursive step.
This is much like the inductive hypothesis in a proof by induction.
However, it is easy to see that the remainder of the tree must be leftist.
The right subtree of the root is leftist, because of the recursive step.
The left subtree of the root has not been changed, so it too must still be leftist.
Notice that if the null path length is not updated, then all null path lengths will be 0, and the heap will not be leftist but merely random.
In this case, the algorithm will work, but the time bound we will claim will no longer be valid.
The node class (Figure 6.25) is the same as the binary tree, except that it is augmented with the npl (null path length) ﬁeld.
The leftist heap stores a reference to the root as its data member.
We have seen in Chapter 4 that when an element is inserted into an empty binary tree, the node referenced by the root will need to change.
We use the usual technique of implementing private recursive methods to do the merging.
The public merge method merges rhs into the controlling heap.
The time to perform the merge is proportional to the sum of the length of the right paths, because constant work is performed at each node visited during the recursive calls.
Thus we obtain an O(logN) time bound to merge two leftist heaps.
We can also perform this operation nonrecursively by essentially performing two passes.
In the ﬁrst pass, we create a new tree by merging the right paths of both heaps.
A second pass is made up the heap, and child swaps are performed at nodes that violate the leftist heap property.
The nonrecursive version is simpler to visualize but harder to code.
We leave it to the reader to show that the recursive and nonrecursive procedures do the same thing.
As mentioned above, we can carry out insertions by making the item to be inserted a one-node heap and performing a merge.
To perform a deleteMin, we merely destroy the root, creating two heaps, which can then be merged.
Finally, we can build a leftist heap in O(N) time by building a binary heap (obviously using a linked implementation)
Although a binary heap is clearly leftist, this is not necessarily the best solution, because the heap we obtain is the worst possible leftist heap.
Furthermore, traversing the tree in reverse-level order is not as easy with links.
The buildHeap effect can be obtained by recursively building the left and right subtrees and then percolating the root down.
A skew heap is a self-adjusting version of a leftist heap that is incredibly simple to implement.
The relationship of skew heaps to leftist heaps is analogous to the relation between splay trees and AVL trees.
Skew heaps are binary trees with heap order, but there is no structural constraint on these trees.
Unlike leftist heaps, no information is maintained about the null path length of any node.
The right path of a skew heap can be arbitrarily long at any time, so the worst-case running time of all operations is O(N)
However, as with splay trees, it can be shown (see Chapter 11) that for any M consecutive operations, the total worst-case running time is O(M logN)
As with leftist heaps, the fundamental operation on skew heaps is merging.
The merge routine is once again recursive, and we perform the exact same operations as before, with.
The difference is that for leftist heaps, we check to see whether the left and right children satisfy the leftist heap structure property and swap them if they do not.
For skew heaps, the swap is unconditional; we always do it, with the one exception that the largest of all the nodes on the right paths does not have its children swapped.
This one exception is what happens in the natural recursive implementation, so it is not really a special case at all.
Furthermore, it is not necessary to prove the bounds, but since this node is guaranteed not to have a right child, it would be silly to perform the swap and give it one.
In our example, there are no children of this node, so we do not worry about it.
Again, suppose our input is the same two heaps as before, Figure 6.31
Again, this is done recursively, so by the third rule of recursion (Section 1.3) we need not worry about how it was obtained.
This heap happens to be leftist, but there is no.
The entire tree is leftist, but it is easy to see that that is not always true: Inserting 15 into this new heap would destroy the leftist property.
We can perform all operations nonrecursively, as with leftist heaps, by merging the right paths and swapping left and right children for every node on the right path, with the exception of the last.
After a few examples, it becomes clear that since all but the last node on the right path have their children swapped, the net effect is that this becomes the new left path (see the preceding example to convince yourself)
This makes it very easy to merge two skew heaps visually.3
The implementation of skew heaps is left as a (trivial) exercise.
Note that because a right path could be long, a recursive implementation could fail because of lack of stack space, even though performance would otherwise be acceptable.
Skew heaps have the advantage that no extra space is required to maintain path lengths and no tests are required to determine when to swap children.
It is an open problem to determine precisely the expected right path length of both leftist and skew heaps (the latter is undoubtedly more difﬁcult)
Such a comparison would make it easier to determine whether the slight loss of balance information is compensated by the lack of testing.
This is not exactly the same as the recursive implementation (but yields the same time bounds)
If we only swap children for nodes on the right path that are above the point where the merging of right paths terminated due to exhaustion of one heap’s right path, we get the same result as the recursive version.
Although both leftist and skew heaps support merging, insertion, and deleteMin all effectively in O(logN) time per operation, there is room for improvement because we know that binary heaps support insertion in constant average time per operation.
Binomial queues support all three operations in O(logN) worst-case time per operation, but insertions take constant time on average.
As an example, a priority queue of six elements could be represented as in Figure 6.35
The minimum element can then be found by scanning the roots of all the trees.
Since there are at most logN different trees, the minimum can be found in O(logN) time.
Alternatively, we can maintain knowledge of the minimum and perform the operation in O(1) time, if we remember to update the minimum when it changes during other operations.
Merging two binomial queues is a conceptually easy operation, which we will describe by example.
The merge is performed by essentially adding the two queues together.
Since merging two binomial trees takes constant time with almost any reasonable implementation, and there are O(logN) binomial trees, the merge takes O(logN) time in the worst case.
To make this operation efﬁcient, we need to keep the trees in the binomial queue sorted by height, which is certainly a simple thing to do.
We count this as three steps (two tree merges plus the stopping case)
The next insertion after 7 is inserted is another bad case and would require three tree merges.
The deleteMin operation requires the ability to ﬁnd all the subtrees of the root quickly, so the standard representation of general trees is required: The children of each node are kept.
This operation also requires that the children be ordered by the size of their subtrees.
We also need to make sure that it is easy to merge two trees.
When two trees are merged, one of the trees is added as a child to the other.
Since this new tree will be the largest subtree, it makes sense to maintain the subtrees in decreasing sizes.
Only then will we be able to merge two binomial trees, and thus two binomial queues, efﬁciently.
The binomial queue will be an array of binomial trees.
To summarize, then, each node in a binomial tree will contain the data, ﬁrst child, and right sibling.
The children in a binomial tree are arranged in decreasing rank.
Figure 6.52 shows the type declarations for a node in the binomial tree, and the binomial queue class skeleton.
In order to merge two binomial queues, we need a routine to merge two binomial trees of the same size.
Figure 6.53 shows how the links change when two binomial trees are merged.
The code to do this is simple and is shown in Figure 6.54
At any point we are dealing with trees of rank i.
Depending on each of the eight possible cases, the tree that results for rank i and the carry tree of rank i + 1 is formed.
This process proceeds from rank 0 to the last rank in the resulting binomial queue.
The deleteMin routine for binomial queues is given in Figure 6.56
We can extend binomial queues to support some of the nonstandard operations that.
A decreaseKey is a percolateUp, which can be performed in O(logN) time if we add a ﬁeld to each node that stores a parent link.
An arbitrary delete can be performed by a combination of decreaseKey and deleteMin in O(logN) time.
Prior to Java 1.5, there was no support in the Java library for priority queues.
However, in Java 1.5, there is a generic PriorityQueue class.
In this class, insert, findMin, and deleteMin are expressed via calls to add, element, and remove.
The PriorityQueue can be constructed either with no parameters, a comparator, or another compatible collection.
Because there are many efﬁcient implementations of priority queues, it is unfortunate that the library designers did not choose to make PriorityQueue an interface.
Nonetheless, the PriorityQueue implementation in Java 1.5 is sufﬁcient for most priority queue applications.
In this chapter we have seen various implementations and uses of the priority queue ADT.
The standard binary heap implementation is elegant because of its simplicity and speed.
It requires no links and only a constant amount of extra space, yet supports the priority queue operations efﬁciently.
We considered the additional merge operation and developed three implementations, each of which is unique in its own way.
The leftist heap is a wonderful example of the power of recursion.
The skew heap represents a remarkable data structure because of the lack of balance criteria.
Its analysis, which we will perform in Chapter 11, is interesting in its own right.
The binomial queue shows how a simple idea can be used to achieve a good time bound.
We have also seen several uses of priority queues, ranging from operating systems scheduling to simulation.
Show the result of using the linear-time algorithm to build a binary heap using the same input.
Suppose we try to use an array representation of a binary tree that is not complete.
Determine how large the array must be for the following: a.
Show that a heap of eight elements can be constructed in eight comparisons between heap elements.
Does your algorithm extend to any of the other heap structures discussed in this chapter?
Give an algorithm that ﬁnds an arbitrary item X in a binary heap using at most roughly 3N/4 comparisons.
Compare the running time of both algorithms for sorted, reverse-ordered, and random inputs.
Do the savings in comparisons compensate for the increased complexity of your algorithm?
Give a simple algorithm to ﬁnd the tree node that is at implicit position i.
The structure is identical to a binary heap, but the heaporder property is that for any node, X, at even depth, the element stored at X is smaller than the parent but larger than the grandparent (where this makes sense), and for any node X at odd depth, the element stored at X is larger than the parent but smaller than the grandparent.
What changes, if any (if possible), are required to do this?
When a findMin or deleteMin is performed, there is a potential problem if the root is marked deleted, since then the node has to be actually deleted and the real minimum needs to be found, which may involve deleting other marked nodes.
In this strategy, deletes cost one unit, but the cost of a deleteMin or findMin depends on the number of nodes that are marked deleted.
Suppose that after a deleteMin or findMin there are k fewer marked nodes than before the operation.
Prove that this algorithm is O(N) in the worst case.
Why might this algorithm be preferable to the algorithm described in the text?
Can we use the same merging strategy described in Exercise 6.25 for skew heaps to get an O(N) running time?
Show that N inserts into an initially empty binomial queue takes O(N) time in the worst case.
Propose an algorithm to insert M nodes into a binomial queue of N elements in O(M + logN) worst-case time.
Modify the merge routine to terminate merging if there are no trees left in H2 and.
Modify the merge so that the smaller tree is always merged into the larger.
The object is to pack all the items without placing more weight in any box than its capacity and using as few boxes as possible.
In general, this problem is very hard, and no efﬁcient solution is known.
Place the weight in the ﬁrst box for which it ﬁts (creating a new box if there is no box with enough room)
This strategy and all that follow would give three boxes, which is suboptimal.
The result of this operation is that all keys in the heap have their value decreased by an amount.
For the heap implementation of your choice, explain the necessary modiﬁcations so that all other operations retain their running times and decreaseAllKeys runs in O(1)
The Fibonacci heap allows all operations to be performed in O(1) amortized time, except for deletions, which are O(logN)
Relaxed heaps [13] achieve identical bounds in the worst case (with the exception of merge)
The procedure of [3] achieves optimal worst-case bounds for all operations.
In this chapter we discuss the problem of sorting an array of elements.
To simplify matters, we will assume in our examples that the array contains only integers, although our code will once again allow more general objects.
For most of this chapter, we will also assume that the entire sort can be done in main memory, so that the number of elements is relatively small (less than a few million)
Sorts that cannot be performed in main memory and must be done on disk or tape are also quite important.
This type of sorting, known as external sorting, will be discussed at the end of the chapter.
The rest of this chapter will describe and analyze the various sorting algorithms.
These algorithms contain interesting and important ideas for code optimization as well as algorithm design.
Sorting is also an example where the analysis can be precisely performed.
Be forewarned that where appropriate, we will do as much analysis as possible.
Each will be passed an array containing the elements; we assume all array positions contain data to be sorted.
We will assume that N is the number of elements passed to our sorting routines.
The objects being sorted are of type Comparable, as described in Section 1.4
We thus use the compareTo method to place a consistent ordering on the input.
Besides (reference) assignments, this is the only operation allowed on the input data.
The sorting algorithms are easily rewritten to use Comparators, in the event that the default ordering is unavailable or unacceptable.
In pass p, we move the element in position p left until its correct place is found among the ﬁrst p + 1 elements.
The element in position p is saved in tmp, and all larger elements (prior to position p) are moved one spot to the right.
This is the same technique that was used in the implementation of binary heaps.
Because of the nested loops, each of which can take N iterations, insertion sort is O(N2)
Furthermore, this bound is tight, because input in reverse order can achieve this bound.
Notice that this is exactly the number of swaps that needed to be (implicitly) performed by insertion sort.
This is always the case, because swapping two adjacent elements that are out of place removes exactly one inversion, and a sorted array has no inversions.
Since there is O(N) other work involved in the algorithm, the running time of insertion sort is O(I + N), where I is the number of inversions in the original array.
Thus, insertion sort runs in linear time if the number of inversions is O(N)
We can compute precise bounds on the average running time of insertion sort by computing the average number of inversions in a permutation.
We will assume that there are no duplicate elements (if we allow duplicates, it is not even clear what the average number of duplicates is)
Using this assumption, we can assume that the input is some permutation of the ﬁrst N integers (since only relative ordering is important) and that all are equally likely.
It is valid not only for insertion sort, which performs adjacent exchanges implicitly, but also for other simple algorithms such as bubble sort and selection sort, which we will not describe here.
In fact, it is valid over an entire class of sorting algorithms, including those undiscovered, that perform only adjacent exchanges.
Although this lower-bound proof is rather simple, in general proving lower bounds is much more complicated than proving upper bounds and in some cases resembles magic.
This lower bound shows us that in order for a sorting algorithm to run in subquadratic, or o(N2), time, it must do comparisons and, in particular, exchanges between elements that are far apart.
A sorting algorithm makes progress by eliminating inversions, and to run efﬁciently, it must eliminate more than just one inversion per exchange.
Shellsort, named after its inventor, Donald Shell, was one of the ﬁrst algorithms to break the quadratic time barrier, although it was not until several years after its initial discovery that a subquadratic time bound was proven.
As suggested in the previous section, it works by comparing elements that are distant; the distance between comparisons decreases as the algorithm runs until the last phase, in which adjacent elements are compared.
For this reason, Shellsort is sometimes referred to as diminishing increment sort.
Figure 7.4 Shellsort routine using Shell’s increments (better increments are possible)
The program in Figure 7.4 avoids the explicit use of swaps in the same manner as our implementation of insertion sort.
Although Shellsort is simple to code, the analysis of its running time is quite another story.
The running time of Shellsort depends on the choice of increment sequence, and the proofs can be rather involved.
The average-case analysis of Shellsort is a long-standing open problem, except for the most trivial increment sequences.
We will prove tight worst-case bounds for two particular increment sequences.
To ﬁnish the proof, we show the upper bound of O(N2)
As we have observed before, a pass with increment hk consists of hk insertion sorts of about N/hk elements.
We will prove only the upper bound and leave the proof of the lower bound as an exercise.
The proof requires some well-known results from additive number theory.
References to these results are provided at the end of the chapter.
For the upper bound, as before, we bound the running time of each pass and sum over all passes.
Although this bound holds for the other increments, it is too large to be useful.
Intuitively, we must take advantage of the fact that this increment sequence is special.
What we need to show is that for any element a[p] in position p, when it is time to perform an hk-sort, there are only a few elements to the left of position p that are larger than a[p]
N, and assuming that t is even, the total running time is then.
There are several other results on Shellsort that (generally) require difﬁcult theorems from number theory and combinatorics and are mainly of theoretical interest.
Shellsort is a ﬁne example of a very simple algorithm with an extremely complex analysis.
The performance of Shellsort is quite acceptable in practice, even for N in the tens of thousands.
The simplicity of the code makes it the algorithm of choice for sorting up to moderately large input.
As mentioned in Chapter 6, priority queues can be used to sort in O(N logN) time.
The algorithm based on this idea is known as heapsort and gives the best Big-Oh running time we have seen so far.
Recall, from Chapter 6, that the basic strategy is to build a binary heap of N elements.
The elements leave the heap smallest ﬁrst, in sorted order.
By recording these elements in a second array and then copying the array back, we sort N elements.
Since each deleteMin takes O(logN) time, the total running time is O(N logN)
The main problem with this algorithm is that it uses an extra array.
Notice that the extra time spent copying the second array back to the ﬁrst is only O(N), so that this is not likely to affect the running time signiﬁcantly.
Thus the cell that was last in the heap can be used to store the element that was just deleted.
As an example, suppose we have a heap with six elements.
Using this strategy, after the last deleteMin the array will contain the elements in decreasing sorted order.
If we want the elements in the more typical increasing sorted order, we can change the ordering property so that the parent has a larger key than the child.
Figure 7.7 shows the heap that results after the ﬁrst deleteMax.
After 5 more deleteMax operations, the heap will actually have only one element, but the elements left in the heap array will be in sorted order.
The code to perform heapsort is given in Figure 7.8
Thus the code is a little different from the binary heap code.
Experiments have shown that the performance of heapsort is extremely consistent: On average it uses only slightly fewer comparisons than the worst-case bound suggests.
For many years, nobody had been able to show nontrivial bounds on heapsort’s average running time.
The problem, it seems, is that successive deleteMax operations destroy the heap’s randomness, making the probability arguments very complex.
Consequently, for any sequence D, the number of distinct corresponding deleteMax sequences is at most.
It follows that the number of distinct deleteMax sequences that require cost exactly equal to M is at most the number of cost sequences of total cost M times the number of deleteMax sequences for each of these cost sequences.
The total number of heaps with cost sequence less than M is at most.
Mergesort runs in O(N logN) worst-case running time, and the number of comparisons used is nearly optimal.
The fundamental operation in this algorithm is merging two sorted lists.
Because the lists are sorted, this can be done in one pass through the input, if the output is put in a third list.
The basic merging algorithm takes two input arrays A and B, an output array C, and three counters, Actr, Bctr, and Cctr, which are initially set to the beginning of their respective arrays.
The smaller of A[Actr] and B[Bctr] is copied to the next entry in C, and the appropriate counters are advanced.
When either input list is exhausted, the remainder of the other list is copied to C.
An example of how the merge routine works is provided for the following input.
The remainder of the B array is then copied to C.
If N = 1, there is only one element to sort, and the answer is at hand.
Otherwise, recursively mergesort the ﬁrst half and the second half.
This gives two sorted halves, which can then be merged together using the merging algorithm described above.
The problem is divided into smaller problems and solved recursively.
Divide-and-conquer is a very powerful use of recursion that we will see many times.
The public mergeSort is just a driver for the private recursive method mergeSort.
If a temporary array is declared locally for each recursive call of merge, then there could be logN temporary arrays active at any point.
A close examination shows that since merge is the last line of mergeSort, there only needs to be one temporary array active at any point, and that the temporary array can be created in the public mergeSort driver.
Further, we can use any part of the temporary array; we will use the same portion as the input array a.
This allows the improvement described at the end of this section.
Mergesort is a classic example of the techniques used to analyze recursive routines: we have to write a recurrence relation for the running time.
We will assume that N is a power of 2, so that we always split into even halves.
Otherwise, the time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge, which is linear.
This is a standard recurrence relation, which can be solved several ways.
The ﬁrst idea is to divide the recurrence relation through by N.
This equation is valid for any N that is a power of 2, so we may also write.
Internal method that merges two sorted halves of a subarray.
This means that we add all of the terms on the left-hand side and set the result equal to the sum of all of the terms on the right-hand side.
Observe that the term T(N/2)/(N/2) appears on both sides and thus cancels.
In fact, virtually all the terms appear on both sides and cancel.
T(N) = N logN + N = O(N logN) Notice that if we did not divide through by N at the start of the solutions, the sum.
This is why it was necessary to divide through by N.
An alternative method is to substitute the recurrence relation continually on the righthand side.
T(N) = NT(1) + N logN = N logN + N The choice of which method to use is a matter of taste.
The answer turns out to be almost identical (this is usually the case)
Although mergesort’s running time is O(N logN), it has the signiﬁcant problem that merging two sorted lists uses linear extra memory.1 The additional work involved in copying to the temporary array and back, throughout the algorithm, slows the sort considerably.
This copying can be avoided by judiciously switching the roles of a and tmpArray at alternate levels of the recursion.
A variant of mergesort can also be implemented nonrecursively (Exercise 7.16)
It is theoretically possible to use less extra memory, but the resulting algorithm is complex and impractical.
The running time of mergesort, when compared with other O(N logN) alternatives, depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array)
For instance, in Java, when performing a generic sort (using a Comparator), an element comparison can be expensive (because comparisons might not be easily inlined, and thus the overhead of dynamic dispatch could slow things down), but moving elements is cheap (because they are reference assignments, rather than copies of large objects)
Mergesort uses the lowest number of comparisons of all the popular sorting algorithms, and thus is a good candidate for general-purpose sorting in Java.
In fact, it is the algorithm used in the standard Java library for generic sorting.
On the other hand, in C++, in a generic sort, copying objects can be expensive if the objects are large, while comparing objects often is relatively cheap because of the ability of the compiler to aggressively perform inline optimization.
In this scenario, it might be reasonable to have an algorithm use a few more comparisons, if we can also use signiﬁcantly fewer data movements.
Quicksort, which we discuss in the next section, achieves this tradeoff, and is the sorting routine commonly used in C++ libraries.
In Java, quicksort is also used as the standard library sort for primitive types.
Here, the costs of comparisons and data moves are similar, so using signiﬁcantly fewer data movements more than compensates for a few extra comparisons.
As its name implies, quicksort is a fast sorting algorithm in practice and is especially useful in C++, or for sorting primitive types in Java.
It is very fast, mainly due to a very tight and highly optimized inner loop.
It has O(N2) worst-case performance, but this can be made exponentially unlikely with a little effort.
By combining quicksort with heapsort, we can achieve quicksort’s fast running time on almost all inputs, with heapsort’s O(N logN) worst-case running time.
The quicksort algorithm is simple to understand and prove correct, although for many years it had the reputation of being an algorithm that could in theory be highly optimized but in practice was impossible to code correctly.
Let us begin with the following simple sorting algorithm to sort a list.
Arbitrarily choose any item, and then form three groups: those smaller than the chosen item, those equal to the chosen item, and those larger than the chosen item.
Recursively sort the ﬁrst and third groups, and then concatenate the three groups.
The result is guaranteed by the basic principles of recursion to be a sorted arrangement of the original list.
A direct implementation of this algorithm is shown in Figure 7.11, and its performance, is generally speaking, quite respectable on most inputs.
In fact, if the list contains large numbers of duplicates with relatively few distinct items, as is sometimes the case, then the performance is extremely good.
The algorithm we have described forms the basis of the quicksort.
However, by making the extra lists, and doing so recursively, it is hard to see how we have improved upon.
In order to do better, we must avoid using signiﬁcant extra memory and have inner loops that are clean.
Thus quicksort is commonly written in a manner that avoids creating the second group (the equal items), and the algorithm has numerous subtle details that affect the performance; therein lies the complications.
We now describe the most common implementation of quicksort—“classic quicksort,” in which the input is an array, and in which no extra arrays are created by the algorithm.
The classic quicksort algorithm to sort an array S consists of the following four easy steps:
Since the partition step ambiguously describes what to do with elements equal to the pivot, this becomes a design decision.
Part of a good implementation is handling this case as efﬁciently as possible.
Figure 7.12 shows the action of quicksort on a set of numbers.
The remaining elements in the set are partitioned into two smaller sets.
The sorted arrangement of the entire set is then trivially obtained.
It should be clear that this algorithm works, but it is not clear why it is any faster than mergesort.
Like mergesort, it recursively solves two subproblems and requires linear additional work (step 3), but, unlike mergesort, the subproblems are not guaranteed to be of equal size, which is potentially bad.
The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efﬁciently.
This efﬁciency can more than make up for the lack of equal-sized recursive calls.
The algorithm as described so far lacks quite a few details, which we now ﬁll in.
Even the slightest deviations from this method can cause surprisingly bad results.
Although the algorithm as described works no matter which element is chosen as the pivot, some choices are obviously better than others.
A Wrong Way The popular, uninformed choice is to use the ﬁrst element as the pivot.
The practical effect is that if the ﬁrst element is used as the pivot and the input is presorted, then quicksort will take quadratic time to do essentially nothing at all, which is quite embarrassing.
Moreover, presorted input (or input with a large presorted section) is quite frequent, so using the ﬁrst element as the pivot is an absolutely horrible idea and should be discarded immediately.
An alternative is choosing the larger of the ﬁrst two distinct elements as the pivot, but this has the same bad properties as merely choosing the ﬁrst element.
A Safe Maneuver A safe course is merely to choose the pivot randomly.
This strategy is generally perfectly safe, unless the random number generator has a ﬂaw (which is not as uncommon as you might think), since it is very unlikely that a random pivot would consistently provide a poor partition.
On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all.
There are several partitioning strategies used in practice, but the one described here is known to give good results.
It is very easy, as we shall see, to do this wrong or inefﬁciently, but it is safe to use a known method.
The ﬁrst step is to get the pivot element out of the way by swapping it with the last element.
If the original input was the same as before, the following ﬁgure shows the current situation.
For now we will assume that all the elements are distinct.
Later on we will worry about what to do in the presence of duplicates.
As a limiting case, our algorithm must do the proper thing if all of the elements are identical.
It is surprising how easy it is to do the wrong thing.
What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part.
While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot.
We move j left, skipping over elements that are larger than the pivot.
When i and j have stopped, i is pointing at a large element and j is pointing at a small element.
If i is to the left of j, those elements are swapped.
The effect is to push a large element to the right and a small element to the left.
In the example above, i would not move and j would slide over one place.
We then swap the elements pointed to by i and j and repeat the process until i and j cross.
At this stage, i and j have crossed, so no swap is performed.
The ﬁnal part of the partitioning is to swap the pivot element with the element pointed to by i.
When the pivot is swapped with i in the last step, we know that every element in a position p < i must be small.
This is because either position p contained a small element to start with, or the large element originally in position p was replaced during a swap.
A similar argument shows that elements in positions p > i must be large.
One important detail we must consider is how to handle elements that are equal to the pivot.
The questions are whether or not i should stop when it sees an element equal to the pivot and whether or not j should stop when it sees an element equal to the pivot.
Intuitively, i and j ought to do the same thing, since otherwise the partitioning step is biased.
For instance, if i stops and j does not, then all elements that are equal to the pivot will wind up in S2
To get an idea of what might be good, we consider the case where all the elements in the array are identical.
If both i and j stop, there will be many swaps between identical elements.
Although this seems useless, the positive effect is that i and j will cross in the middle, so when the pivot is replaced, the partition creates two nearly equal subarrays.
The mergesort analysis tells us that the total running time would then be O(N logN)
If neither i nor j stops, and code is present to prevent them from running off the end of the array, no swaps will be performed.
Although this seems good, a correct implementation would then swap the pivot into the last spot that i touched, which would be the next-tolast position (or last, depending on the exact implementation)
If all the elements are identical, the running time is O(N2)
The effect is the same as using the ﬁrst element as a pivot for presorted input.
Thus, we ﬁnd that it is better to do the unnecessary swaps and create even subarrays than to risk wildly uneven subarrays.
Therefore, we will have both i and j stop if they encounter an element equal to the pivot.
This turns out to be the only one of the four possibilities that does not take quadratic time for this input.
At ﬁrst glance it may seem that worrying about an array of identical elements is silly.
After all, why would anyone want to sort 50,000 identical elements? However, recall that quicksort is recursive.
Eventually, quicksort will make the recursive call on only these 50,000 elements.
Then it really will be important to make sure that 50,000 identical elements can be sorted efﬁciently.
The general form of the routines will be to pass the array and the range of the array (left and right) to be sorted.
The easiest way to do this is to sort a[left], a[right], and a[center] in place.
This has the extra advantage that the smallest of the three winds up in a[left], which is where the partitioning step would put it anyway.
The largest winds up in a[right], which is also the correct place, since it is larger than the pivot.
Yet another beneﬁt is that because a[left] is smaller than the pivot, it will act as a sentinel for j.
Since i will stop on elements equal to the pivot, storing the pivot in a[right-1] provides a sentinel for i.
The code in Figure 7.14 does the median-of-three partitioning with all the side effects described.
It may seem that it is only slightly inefﬁcient to compute the pivot by a method that does not actually sort a[left], a[center], and a[right], but, surprisingly, this produces bad results (see Exercise 7.51)
The real heart of the quicksort routine is in Figure 7.15
This initialization depends on the fact that median-of-three partitioning has some side.
The swapping action at line 22 is sometimes written explicitly, for speed purposes.
For the algorithm to be fast, it may be necessary to force the compiler to compile this code inline.
Many compilers will do this automatically if swapReferences is a final method, but for those that do not, the difference can be signiﬁcant.
Figure 7.16 A small change to quicksort, which breaks the algorithm.
There is no extra juggling as there is in mergesort.
This does not work, because there would be an inﬁnite loop if a[i] = a[j] = pivot.
Like mergesort, quicksort is recursive, and hence, its analysis requires solving a recurrence formula.
We will do the analysis for a quicksort, assuming a random pivot (no medianof-three partitioning) and no cutoff for small arrays.
The running time of quicksort is equal to the running time of the two recursive calls plus the linear time spent in the partition (the pivot selection takes only constant time)
Worst-Case Analysis The pivot is the smallest element, all the time.
To see that this is the worst possible case, note that the total cost of all the partitions in recursive calls at depth d must be at most N.
Since the recursion depth is at most N, this gives as O(N2) worst-case bound for quicksort.
Best-Case Analysis In the best case, the pivot is in the middle.
To simplify the math, we assume that the two subarrays are each exactly half the size of the original, and although this gives a slight overestimate, this is acceptable because we are only interested in a Big-Oh answer.
This assumption is actually valid for our pivoting and partitioning strategy, but it is not valid for some others.
Partitioning strategies that do not preserve the randomness of the subarrays cannot use this analysis.
Interestingly, these strategies seem to result in programs that take longer to run in practice.
We need to remove the summation sign to simplify matters.
We note that we can telescope with one more equation.
T(N) = O(N logN) (7.25) Although this analysis seems complicated, it really is not—the steps are natural once.
The highly optimized version that was described above has also been analyzed, and this result gets extremely difﬁcult, involving complicated recurrences and advanced mathematics.
The effect of equal elements has also been analyzed in detail, and it turns out that the code presented does the right thing.
Recall that by using a priority queue, we can ﬁnd the kth largest (or smallest) element in O(N + k logN)
For the special case of ﬁnding the median, this gives an O(N logN) algorithm.
Since we can sort the array in O(N logN) time, one might expect to obtain a better time bound for selection.
The algorithm we present to ﬁnd the kth smallest element in a set S is almost identical to quicksort.
In contrast to quicksort, quickselect makes only one recursive call instead of two.
The worst case of quickselect is identical to that of quicksort and is O(N2)
The analysis is similar to quicksort’s and is left as an exercise.
Using a median-of-three pivoting strategy makes the chance of the worst case occurring almost negligible.
By carefully choosing the pivot, however, we can eliminate the quadratic worst case and ensure an O(N) algorithm.
The overhead involved in doing this is considerable, so the resulting algorithm is mostly of theoretical interest.
In Chapter 10, we will examine the linear-time worst-case algorithm for selection, and we shall also see an interesting technique of choosing the pivot that results in a somewhat faster selection algorithm in practice.
A decision tree is an abstraction used to prove lower bounds.
In our context, a decision tree is a binary tree.
Each node represents a set of possible orderings, consistent with comparisons that have been made, among the elements.
The decision tree in Figure 7.18 represents an algorithm that sorts the three elements a, b, and c.
The initial state of the algorithm is at the root.
No comparisons have been done, so all orderings are legal.
The ﬁrst comparison that this particular algorithm performs compares a and b.
If the algorithm reaches node 2, then it will compare a and c.
Other algorithms might do different things; a different algorithm would have a different decision tree.
Since there is only one ordering that is consistent, the algorithm can terminate and report that it has completed the sort.
If a < c, the algorithm cannot do this, because there are two possible orderings and it cannot possibly be sure which is correct.
In this case, the algorithm will require one more comparison.
Every algorithm that sorts by using only comparisons can be represented by a decision tree.
Of course, it is only feasible to draw the tree for extremely small input sizes.
The number of comparisons used by the sorting algorithm is equal to the depth of the deepest leaf.
In our case, this algorithm uses three comparisons in the worst case.
Since a decision tree is large, it follows that there must be some long paths.
To prove the lower bounds, all that needs to be shown are some basic tree properties.
Section 7.8 employed a decision tree argument to show the fundamental lower bound that any comparison-based sorting algorithm must use roughly N logN comparisons.
In this section we show additional lower bounds for selection in an N-element collection, speciﬁcally.
The lower bounds for all these problems, with the exception of ﬁnding the median, are tight: Algorithms exist that use exactly the speciﬁed number of comparisons.
In all our proofs, we assume all items are unique.
If all the leaves in a decision tree are at depth d or higher, the decision tree must have at least 2d leaves.
Note that all nonleaf nodes in a decision tree have two children.
The ﬁrst lower bound, for ﬁnding the smallest item, is the easiest and most trivial to show.
Every element, x, except the smallest element, must be involved in a comparison with some other element y, in which x is declared larger than y.
Otherwise, if there were two different elements that had not been declared larger than any other elements, then either could be the smallest.
The bound for selection is a little more complicated and requires looking at the structure of the decision tree.
Observe that any algorithm that correctly identiﬁes the kth smallest element t must be able to prove that all other elements x are either larger than or smaller than t.
Otherwise, it would be giving the same answer regardless of whether x was larger or smaller than t, and the answer cannot be the same in both circumstances.
A direct application of Lemma 7.5 allows us to prove the lower bounds for ﬁnding the second smallest element, and also the median.
Any comparison-based algorithm to ﬁnd the kth smallest element must use at least.
Although decision-tree arguments allowed us to show lower bounds for sorting and some selection problems, generally the bounds that result are not that tight, and sometimes are trivial.
Every element, x, except the smallest element, must be involved in a comparison with some other element y, in which x is declared larger than y.
Otherwise, if there were two different elements that had not been declared larger than any other elements, then either could be the smallest.
This is the underlying idea of an adversary argument which has some basic steps:
Establish that some basic amount of information must be obtained by any algorithm that solves a problem.
In each step of the algorithm, the adversary will maintain an input that is consistent with all the answers that have been provided by the algorithm thus far.
Argue that with insufﬁcient steps, there are multiple consistent inputs that would provide different answers to the algorithm; hence the algorithm has not done enough steps, because if the algorithm were to provide an answer at that point, the adversary would be able to show an input for which the answer is wrong.
To see how this works, we will reprove the lower bound for ﬁnding the smallest element using this proof template.
When an item is declared larger than another item, we will change its marking to E (for eliminated)
Initially each unknown item has a value of 0, but there have been no comparisons, so this ordering is consistent with prior answers.
A comparison between two items is either between two unknowns, or it involves at least one item eliminated from being the minimum.
Figure 7.20 shows how our adversary will construct the input values, based on the questioning.
If the comparison is between two unknowns, the ﬁrst is declared the smaller, and the second is automatically eliminated, providing one unit of information.
We then assign it (irrevocably) a number larger than 0; the most convenient is the number of eliminated items.
If a comparison is between an eliminated number and an unknown, the eliminated number (which is larger than 0 by the prior sentence) will be declared larger, and there will be no changes, no eliminations, and no information obtained.
If two eliminated numbers are compared, then they will be different, and a consistent answer can be provided, again with no changes, and no information provided.
Figure 7.20 Adversary constructs input for ﬁnding the minimum as algorithm runs.
It remains to show that the adversary can maintain values that are consistent with its answers.
If both items are unmarked, then obviously they can be safely assigned values consistent with the comparison answer; this case yields two units of information.
Otherwise, if one of the items involved in a comparison is unmarked, it can be assigned a value the ﬁrst time, consistent with the other item in the comparison.
If both are WL, then we can answer consistently with the current assignment, yielding no information.2
Otherwise at least one of the items has only an L or only a W.
We will allow that item to compare redundantly (if it is an L then it loses again, if it is a W then it wins again), and its value can be easily adjusted if needed, based on the other item in the comparison (an L can be lowered as needed; a W can be raised as needed)
This yields at most one unit of information for the other item in the comparison, possibly zero.
Figure 7.21 summarizes the action of the adversary, making y the primary element whose value changes in all cases.
Figure 7.21 Adversary constructs input for ﬁnding the maximum and minimum as algorithm runs.
It is easy to see that this bound is achievable.
Pair up the elements, and perform a comparison between each pair.
Then ﬁnd the maximum among the winners, and the minimum amoung the losers.
For bucket sort to work, extra information must be available.
If this is the case, then the algorithm is simple: Keep an array called count, of size M, which is initialized to all 0’s.
Thus, count has M cells, or buckets, which are initially empty.
After all the input is read, scan the count array, printing out a representation of the.
Although this algorithm seems to violate the lower bound, it turns out that it does not because it uses a more powerful operation than simple comparisons.
By incrementing the appropriate bucket, the algorithm essentially performs an M-way comparison in unit time.
This is similar to the strategy used in extendible hashing (Section 5.9)
This is clearly not in the model for which the lower bound was proven.
This algorithm does, however, question the validity of the model used in proving the lower bound.
The model actually is a strong model, because a general-purpose sorting algorithm cannot make assumptions about the type of input it can expect to see, but must make decisions based on ordering information only.
Naturally, if there is extra information available, we should expect to ﬁnd a more efﬁcient algorithm, since otherwise the extra information would be wasted.
Although bucket sort seems like much too trivial an algorithm to be useful, it turns out that there are many cases where the input is only small integers, so that using a method like quicksort is really overkill.
If all the strings have the same length L, then by using buckets for each character, we can implement a radix sort in O (NL)
Figure 7.23 Simple implementation of radix sort for strings, using an ArrayList of buckets.
The most straightforward way of doing this is shown in Figure 7.23
In our code, we assume that all characters are ASCII, residing in the ﬁrst 256 positions of the Unicode character set.
In each pass, we add an item to the appropriate bucket, and then after all the buckets are populated, we step through the buckets dumping everything back to the array.
Notice that when a bucket is populated and emptied in the next pass, the order from the current pass is preserved.
Counting radix sort is an alternative implementation of radix sort that avoids using ArrayLists.
Instead, we maintain a count of how many items would go in each bucket; this information can go into an array count, so that count[k] is the number of items that are in bucket k.
Then we can use another array offset, so that offset[k] represents the number of items whose value is strictly smaller than k.
Then when we see a value k for the ﬁrst time in the ﬁnal scan, offset[k] tells us a valid array spot where it can be written to (but we have to use a temporary array for the write), and after that is done, offset[k]
Counting radix sort thus avoids the need to keep lists.
As a further optimization, we can avoid using offset, by reusing the count array.
The modiﬁcation is that we initially have count[k+1] represent the number of items that are in bucket k.
Then after that information is computed, we can scan the count array from the smallest to largest index, and increment count[k] by count[k-1]
It is easy to verify that after this scan, the count array stores exactly the same information that would have been stored in offset.
Initially, in represents arr and out represents the temporary array, buffer.
After each pass, we switch the roles of in and out.
If there are an even number of passes, then at the end, out is referencing arr, so the sort is complete.
Otherwise, we have to copy from the buffer back into arr.
Generally, counting radix sort is prefereable to using ArrayLists, but it can suffer from poor locality (out is ﬁlled in non-sequentially) and thus surprisingly, it is not always faster than using an array of ArrayLists.
We can extend either version of radix sort to work with variable-length strings.
The basic algorithm is to ﬁrst sort the strings by their length.
Instead of looking at all the strings, we can then look only at strings that we know are long enough.
Since the string lengths are small numbers, the initial sort by length can be done by—bucket sort! Figure 7.25 shows this implementation of radix sort, with ArrayLists.
Radix sort for strings will perform especially well when the characters in the string are drawn from a reasonably small alphabet, and when the strings either are relatively short or are very similar.
Because the O(N logN) comparison-based sorting algorithms will generally look only at a small number of characters in each string comparison, once the average string length starts getting large, radix sort’s advantage is minimized or evaporates completely.
So far, all the algorithms we have examined require that the input ﬁt into main memory.
There are, however, applications where the input is much too large to ﬁt into memory.
This section will discuss external sorting algorithms, which are designed to handle very large inputs.
Most of the internal sorting algorithms take advantage of the fact that memory is directly addressable.
Shellsort compares elements a[i] and a[i-hk] in one time unit.
Quicksort, with median-of-three partitioning, requires comparing a[left], a[center], and a[right] in a constant number of time units.
If the input is on a tape, then all these operations lose their efﬁciency, since elements on a tape can only be accessed sequentially.
Even if the data is on a disk, there is still a practical loss of efﬁciency because of the delay required to spin the disk and move the disk head.
To see how slow external accesses really are, create a random ﬁle that is large, but not too big to ﬁt in main memory.
Read the ﬁle in and sort it using an efﬁcient algorithm.
The time it takes to read the input is certain to be signiﬁcant compared to the time to sort the input, even though sorting is an O(N logN) operation and reading the input is only O(N)
The wide variety of mass storage devices makes external sorting much more device dependent than internal sorting.
The algorithms that we will consider work on tapes, which are probably the most restrictive storage medium.
Since access to an element on tape is done by winding the tape to the correct location, tapes can be efﬁciently accessed only in sequential order (in either direction)
The basic external sorting algorithm uses the merging algorithm from mergesort.
Depending on the point in the algorithm, the a and b tapes are either input tapes or output tapes.
Suppose further that the internal memory can hold (and sort) M records at a time.
We will call each set of sorted records a run.
Suppose we have the same input as our example for Shellsort.
If M = 3, then after the runs are constructed, the tapes will contain the data indicated in the following ﬁgure.
We take the ﬁrst run from each tape and merge them, writing the result, which is a run twice as long, onto Ta1
Then we take the next run from each tape, merge these, and write the result to Ta2
At this point either both are empty or there is one run left.
In the latter case, we copy this run to the appropriate tape.
We rewind all four tapes and repeat the same steps, this time using the a tapes as input and the b tapes as output.
We continue the process until we get one run of length N.
If we have extra tapes, then we can expect to reduce the number of passes required to sort our input.
We do this by extending the basic (two-way) merge to a k-way merge.
Merging two runs is done by winding each input tape to the beginning of each run.
Then the smaller element is found, placed on an output tape, and the appropriate input.
If there are k input tapes, this strategy works the same way, the only difference being that it is slightly more complicated to ﬁnd the smallest of the k elements.
We can ﬁnd the smallest of these elements by using a priority queue.
To obtain the next element to write on the output tape, we perform a deleteMin operation.
The appropriate input tape is advanced, and if the run on the input tape is not yet completed, we insert the new element into the priority queue.
Using the same example as before, we distribute the input onto the three tapes.
We then need two more passes of three-way merging to complete the sort.
The k-way merging strategy developed in the last section requires the use of 2k tapes.
It is possible to get by with only k + 1 tapes.
As an example, we will show how to perform two-way merging using only three tapes.
The problem is that since all the runs are on one tape, we must now put some of these runs on T2 to perform another merge.
This has the effect of adding an extra half pass for every pass we do.
An alternative method is to split the original 34 runs unevenly.
The following table shows the number of runs on each tape after each pass.
The original distribution of runs makes a great deal of difference.
At this point the going gets slow, because we can only merge two sets of runs before T3 is exhausted.
After three more passes, T2 has two runs and the other tapes are empty.
We must copy one run to another tape, and then we can ﬁnish the merge.
The last item we will consider is construction of the runs.
The strategy we have used so far is the simplest possible: We read as many records as possible and sort them, writing the result to some tape.
This seems like the best approach possible, until one realizes that as soon as the ﬁrst record is written to an output tape, the memory it used becomes available.
If the next record on the input tape is larger than the record we have just output, then it can be included in the run.
Using this observation, we can give an algorithm for producing runs.
Initially, M records are read into memory and placed in a priority queue.
We perform a deleteMin, writing the smallest (valued) record to the output tape.
If it is larger than the record we have just written, we can add it to the priority queue.
Since the priority queue is smaller by one element, we can store this new element in the dead space of the priority queue until the run is completed and use the element for the next run.
Storing an element in the dead space is similar to what is done in heapsort.
We continue doing this until the size of the priority queue is zero, at which point the run is over.
We start a new run by building a new priority queue, using all the elements in the dead space.
In this example, replacement selection produces only three runs, compared with the ﬁve runs obtained by sorting.
Because of this, a three-way merge ﬁnishes in one pass instead of two.
If the input is randomly distributed, replacement selection can be shown to produce runs of average length 2M.
In this case, we have not saved a pass, although we might if we get lucky and have 125 runs or less.
Since external sorts take so long, every pass saved can make a signiﬁcant difference in the running time.
As we have seen, it is possible for replacement selection to do no better than the standard algorithm.
However, the input is frequently sorted or nearly sorted to start with, in which case replacement selection produces only a few very long runs.
This kind of input is common for external sorts and makes replacement selection extremely valuable.
Show that for any N, there exists a six-increment sequence such that Shellsort.
Before line 11, subtract one from gap if it is even.
Before line 11, add one to gap if it is even.
For the quicksort implementation in this chapter, what is the running time when all keys are equal?
Suppose we change the partitioning strategy so that neither i nor j stops when an element with the same key as the pivot is found.
What ﬁxes need to be made in the code to guarantee that quicksort works, and what is the running time, when all keys are equal?
Suppose we change the partitioning strategy so that i stops at an element with the same key as the pivot, but j does not stop in a similar case.
What ﬁxes need to be made in the code to guarantee that quicksort works, and when all keys are equal, what is the running time of quicksort?
Does this make it unlikely that quicksort will require quadratic time?
Rewrite the code so that the second recursive call is unconditionally the last.
Do this by reversing the if/else and returning after the call to insertionSort.
Remove the tail recursion by writing a while loop and altering left.
Remove the tail recursion by writing a while loop and altering left or right, as.
Prove that the number of recursive calls is logarithmic in the worst case.
Modify the recursive quicksort to call heapsort on its current subarray if the level.
Hint: Decrement depth as you make recursive calls; when it is 0, switch to heapsort.
Prove that the worst-case running time of this algorithm is O(N logN)
Explain why the technique in Exercise 7.26 would no longer be needed.
Give an algorithm that performs a three-way in-place partition of an N-element.
Prove that using the algorithm above, sorting an N-element array that contains only d different values, takes O(dN) time.
Which of the sorting algorithms in this chapter are stable and which are not? Why?
How large can f(N) be for the entire list still to be sortable in O(N) time?
Give a nontrivial lower bound on the number of comparisons required to merge two sorted lists of N elements, by taking the logarithm of your answer in part (a)
Show that this algorithm is suboptimal, regardless of the choices for Algorithms A, B, and C.
However, there is a better algorithm that makes use of sorting and runs in O(N2 logN) time.
Otherwise, split the input into two halves, divided as evenly as possibly (if N is odd, one of the two halves will have one more element than the other)
Recursively ﬁnd the maximum and minimum of each half, and then in two additional comparisons produce the maximum and minimum for the entire problem.
Modify the algorithm as follows: When N is even, but not divisible by four, split.
Give an O(N) algorithm to rearrange the list so that all false elements precede the true elements.
Give an O(N) algorithm to rearrange the list so that all false elements precede maybe elements, which in turn precede true elements.
Pivot: ﬁrst element, middle element, random element, median of three, median.
We want to determine if there are two numbers whose sum equals a given number K.
After that is done, you can solve the problem in linear time.
Code both solutions and compare the running times of your algorithms.
The normal routine is to compare X’s children and then move the child up to X if it is larger (in the case of a (max)heap) than the element we are trying to place, thereby pushing the hole down; we stop when it is safe to place the new element in the hole.
The alternative strategy is to move elements up and the hole down as far as possible, without testing whether the new cell can be inserted.
Write a routine to include this idea, and compare the running time with a standard implementation of heapsort.
Gonnet and Baeza-Yates [5] has some more results, as well as a huge bibliography.
An exact average-case analysis of mergesort has been described in [7]
An algorithm to perform merging in linear time without extra space is described in [12]
This paper analyzes the basic algorithm, describes most of the improvements, and includes the selection algorithm.
A detailed analysis and empirical study was the subject of Sedgewick’s dissertation [27]
Decision trees and sorting optimality are discussed in Ford and Johnson [5]
This paper also provides an algorithm that almost meets the lower bound in terms of number of comparisons (but not other operations)
This algorithm was eventually shown to be slightly suboptimal by Manacher [17]
The lower bound for ﬁnding the maximum and minimum simultaneously is from Pohl [21]
In this chapter, we describe an efﬁcient data structure to solve the equivalence problem.
Each routine requires only a few lines of code, and a simple array can be used.
The implementation is also extremely fast, requiring constant average time per operation.
This data structure is also very interesting from a theoretical point of view, because its analysis is extremely difﬁcult; the functional form of the worst case is unlike any we have yet seen.
An equivalence relation is a relation R that satisﬁes three properties:
Electrical connectivity, where all connections are by metal wires, is an equivalence relation.
The relation is clearly reﬂexive, as any component is connected to itself.
If a is electrically connected to b, then b must be electrically connected to a, so the relation is symmetric.
Finally, if a is connected to b and b is connected to c, then a is connected to c.
Two cities are related if they are in the same country.
It is easily veriﬁed that this is an equivalence relation.
Suppose town a is related to b if it is possible to travel from a to b by taking roads.
This relation is an equivalence relation if all the roads are two-way.
This algorithm is dynamic because, during the course of the algorithm, the sets can change via the union operation.
The algorithm must also operate online: When a find is performed, it must give an answer before continuing.
Such an algorithm would be allowed to see the entire sequence of unions and finds.
The answer it provides for each find must still be consistent with all the unions that were performed up until the find, but the algorithm can give all its answers after it has seen all the questions.
The difference is similar to taking a written exam (which is generally off-line—you only have to give the answers before time expires), and an oral exam (which is online, because you must answer the current question before proceeding to the next question)
Our second observation is that the name of the set returned by find is actually fairly arbitrary.
All that really matters is that find(a)==find(b) is true if and only if a and b are in the same set.
These operations are important in many graph theory problems and also in compilers that process equivalence (or type) declarations.
One ensures that the find instruction can be executed in constant worst-case time, and the other ensures that the union instruction can be executed in constant worst-case time.
It has been shown that both cannot be done simultaneously in constant worst-case time.
Recall that the problem does not require that a find operation return any speciﬁc name, just that finds on two elements return the same answer if and only if they are in the same set.
One idea might be to use a tree to represent each set, since each element in a tree has the same root.
Thus, the root can be used to name the set.
Recall that a collection of trees is known as a forest.
The trees we will use are not necessarily binary trees, but their representation is easy, because the only information we will need is a parent link.
The name of a set is given by the node at the root.
Since only the name of the parent is required, we can assume that this tree is stored implicitly in an array: Each entry s[i] in the array represents the parent.
To perform a union of two sets, we merge the two trees by making the parent link of one tree’s root link to the root node of the other tree.
It should be clear that this operation takes constant time.
The implicit representation of the last forest is shown in Figure 8.5
In our routine, unions are performed on the roots of the trees.
Sometimes the operation is performed by passing any two elements, and having the union perform two finds to determine the roots.
Quadratic running time for a sequence of operations is generally unacceptable.
Fortunately, there are several ways of easily ensuring that this running time does not occur.
The unions above were performed rather arbitrarily, by making the second tree a subtree of the ﬁrst.
A simple improvement is always to make the smaller tree a subtree of the larger, breaking ties by any method; we call this approach union-by-size.
The three unions in the preceding example were all ties, and so we can consider that they were performed by size.
Had the size heuristic not been used, a deeper tree would have been formed (Figure 8.11)
We can prove that if unions are done by size, the depth of any node is never more than logN.
When its depth increases as a result of a union, it is placed in a tree that is at least twice as large as before.
Thus, its depth can be increased at most logN times.
We used this argument in the quick-ﬁnd algorithm at the end of Section 8.2
This implies that the running time for a find operation is O(logN), and a sequence of M operations takes O(M logN)
To implement this strategy, we need to keep track of the size of each tree.
Since we are really just using an array, we can have the array entry of each root contain the negative of.
Figure 8.13 show a forest and its implicit representation for both union-by-size and union-by-height.
Figure 8.13 Forest with implicit representation for union-by-size and union-by-height.
The union/ﬁnd algorithm, as described so far, is quite acceptable for most cases.
It is very simple and linear on average for a sequence of M instructions (under all models)
However, the worst case of O(M logN) can occur fairly easily and naturally.
For instance, if we put all the sets on a queue and repeatedly dequeue the ﬁrst two sets and enqueue the union, the worst case occurs.
If there are many more finds than unions, this running time is worse than that of the quick-ﬁnd algorithm.
Moreover, it should be clear that there are probably no more improvements possible for the union algorithm.
This is based on the observation that any method to perform the unions will yield the same worst-case trees, since it must break ties arbitrarily.
Therefore, the only way to speed the algorithm up, without reworking the data structure entirely, is to do something clever on the find operation.
Path compression is performed during a find operation and is independent of the strategy used to perform unions.
Then the effect of path compression is that every node on the path from x to the root has its parent changed to the root.
Thus, the fast future accesses on these nodes will pay (we hope) for the extra work to do the path compression.
As the code in Figure 8.16 shows, path compression is a trivial change to the basic find algorithm.
The only change to the find routine is that s[x] is made equal to the value returned by find; thus after the root of the set is found recursively, x’s parent link references it.
This occurs recursively to every node on the path to the root, so this implements path compression.
When unions are done arbitrarily, path compression is a good idea, because there is an abundance of deep nodes and these are brought near the root by path compression.
It has been proven that when path compression is done in this case, a sequence of M.
Figure 8.16 Code for the disjoint set find with path compression.
It is still an open problem to determine what the average-case behavior is in this situation.
Path compression is perfectly compatible with union-by-size, and thus both routines can be implemented at the same time.
Since doing union-by-size by itself is expected to execute a sequence of M operations in linear time, it is not clear that the extra pass involved in path compression is worthwhile on average.
However, as we shall see later, the combination of path compression and a smart union rule guarantees a very efﬁcient algorithm in all cases.
Path compression is not entirely compatible with union-by-height, because path compression can change the heights of the trees.
It is not at all clear how to recompute them efﬁciently.
The answer is do not!! Then the heights stored for each tree become estimated heights (sometimes known as ranks), but it turns out that union-by-rank (which is what this has now become) is just as efﬁcient in theory as union-by-size.
As with union-by-size, it is not clear whether path compression is worthwhile on average.
What we will show in the next section is that with either union heuristic, path compression signiﬁcantly reduces the worst-case running time.
We begin by establishing two lemmas concerning the properties of the ranks.
At any point in the union/ﬁnd algorithm, the ranks of the nodes on a path from the leaf to a root increase monotonically.
The lemma is obvious if there is no path compression.
If, after path compression, some node v is a descendant of w, then clearly v must have been a descendant of w when only unions were considered.
Hence the rank of v is less than the rank of w.
Algorithm A works and computes all the answers correctly, but algorithm B does not compute correctly, or even produce useful answers.
Suppose, however, that every step in algorithm A can be mapped to an equivalent step in algorithm B.
Then it is easy to see that the running time for algorithm B describes the running time for algorithm A, exactly.
Figure 8.18 A large disjoint set tree (numbers below nodes are ranks)
We can use this idea to analyze the running time of the disjoint sets data structure.
We will describe an algorithm B whose running time is exactly the same as the disjoint sets structure, and then algorithm C, whose running time is exactly the same as algorithm B.
Thus any bound for algorithm C will be a bound for the disjoint sets data structure.
Partial Path Compression Algorithm A is our standard sequence of union-by-rank and ﬁnd with path compression operations.
We design an algorithm B that will perform the exact same sequence of path compression operations as algorithm A.
In algorithm B, we perform all the unions prior to any ﬁnd.
Then each ﬁnd operation in algorithm A is replaced by a partial ﬁnd operation in algorithm B.
A partial ﬁnd operation speciﬁes the search item and the node up to which the path compression is performed.
The node that will be used is the node that would have been the root at the time the matching ﬁnd was performed in algorithm A.
Figure 8.19 shows that algorithm A and algorithm B will get equivalent trees (forests) at the end, and it is easy to see that the exact same amount of parent changes are performed by algorithm A’s ﬁnds, compared to algorithm B’s partial ﬁnds.
But algorithm B should be simpler to analyze, since we have removed the mixing of unions and ﬁnds from the equation.
The basic quantity to analyze is the number of parent changes that can occur in any sequence of partial ﬁnds, since all but the top two nodes in any ﬁnd with path compression will obtain new parents.
A Recursive Decomposition What we would like to do next is to divide each tree into two halves: a top half and a bottom half.
We would then like to ensure that the number of partial ﬁnd operations in the top half plus the number of partial ﬁnd operations in the bottom half is exactly the same as the total number of partial ﬁnd operations.
We would then like to write a formula for the total path compression cost in the tree in terms of the path compression cost in the top half plus the path compression cost in the bottom half.
Figure 8.19 Sequences of union and ﬁnd operations replaced with equivalent cost of union and partial ﬁnd operations.
In Figure 8.20, the partial ﬁnd resides entirely in the bottom half.
Thus one partial ﬁnd in the bottom half corresponds to one original partial ﬁnd, and the charges can be recursively assigned to the bottom half.
In Figure 8.21, the partial ﬁnd resides entirely in the top half.
Thus one partial ﬁnd in the top half corresponds to one original partial ﬁnd, and the charges can be recursively assigned to the top half.
However, we run into lots of trouble when we reach Figure 8.22
Here x is in the bottom half, and y is in the top half.
The path compression would require that all nodes from x to y’s child acquire y as its parent.
For nodes in the top half, that is no problem, but for nodes in the bottom half this is a deal breaker: Any recursive charges to the bottom.
So as Figure 8.23 shows, we can perform the path compression on the top, but while some nodes in the bottom will need new parents, it is not clear what to do, because the new parents for those bottom nodes cannot be top nodes, and the new parents cannot be other bottom nodes.
The only option is to make a loop where these nodes’ parents are themselves and make sure these parent changes are correctly charged in our accounting.
Although this is a new algorithm because it can no longer be used to generate an identical tree, we don’t need identical trees; we only need to be sure that each original partial ﬁnd can be mapped into a new partial ﬁnd operation, and that the charges are identical.
Figure 8.24 shows what the new tree will look like, and so the big remaining issue is the accounting.
Looking at Figure 8.24, we see that the path compression charges from x to y can be split into three parts.
First there is the path compression from z (the ﬁrst top node on the upward path) to y.
Then there is the charge from the topmost-bottom node w to z.
But that is only one unit, and there can be at most one of those per partial ﬁnd operation.
In fact, we can do a little better: There can be at most one of those per partial ﬁnd operation on the top half.
But how do we account for the parent changes on the path from x to w? One idea would be to argue that those changes would be exactly the same cost as if there were a partial ﬁnd from x to w.
But there is a big problem with that argument: It converts an original partial ﬁnd into a partial ﬁnd on the top plus a partial ﬁnd on the bottom, which means the number of operations, M, would no longer be the same.
Can we get in trouble on a subsequent partial ﬁnd given that our reformulation detaches the nodes between x and w from the path to y? The answer is no.
In the original partial ﬁnd, suppose any of the nodes between x and w are involved in a subsequent original partial ﬁnd.
In that case, it will be with one of y’s ancestors, and when that happens, any of those nodes will be the topmost “bottom node” in our reformulation.
Thus on the subsequent partial ﬁnd, the original partial ﬁnd’s parent change will have a corresponding one unit charge in our reformulation.
Let M be the total number of original partial ﬁnd operations.
Let Mt be the total number of partial ﬁnd operations performed exclusively on the top half, and let Mb be the total number of partial ﬁnd operations performed exclusively on the bottom half.
Let Nt be the total number of tophalf nodes, and let Nb be the total number of bottom half nodes, and let Nnrb be the total number of non-root bottom nodes (i.e the number of bottom nodes whose parents are also bottom nodes prior to any partial ﬁnds)
Thus each partial ﬁnd is replaced by exactly one partial ﬁnd operation on one of the halves.
Our basic idea is that we are going to partition the nodes so that all nodes with rank s or lower are in the bottom, and the remaining nodes are in the top.
The choice of s will be made later in the proof.
The next lemma shows that we can provide a recursive formula for the number of parent changes, by splitting the charges into the top and bottom groups.
One of the key ideas is that a recursive formula is written not only in terms of M and N, which would be obvious, but also in terms of the maximum rank in the group.
Let C(M,N, r) be the number of parent changes for a sequence of M ﬁnds with path compression on N items, whose maximum rank is r.
Suppose we partition so that all nodes with rank at s or lower are in the bottom, and the remaining nodes are in the top.
The path compression that is performed in each of the three cases is covered by C(Mt,Nt, r) + C(Mb,Nb, s)
Node w in case 3 is accounted for by Mt.
Finally, all the other bottom nodes on the path are non-root nodes that can have their parent set to themselves at most once in the entire sequence of compressions.
Each of those children are deﬁnitely root nodes in the bottom (their parent is a top node)
Let C(M,N, r) be the number of parent changes for a sequence of M ﬁnds with path compression on N items, whose maximum rank is r.
Suppose we partition so that all nodes with rank at s or lower are in the bottom, and the remaining nodes are in the top.
If we look at Lemma 8.5, we see that C(M,N, r) is recursively deﬁned in terms of two smaller instances.
Our basic goal at this point is to remove one of these instances, by providing a bound for it.
What we would like to do is to remove C(Mt,Nt, r)
Why? Because, if we do so, what is left is C(Mb,Nb, s)
In that case, we have a recursive formula in which r is reduced to s.
If s is small enough, we can make use of a variation of Equation 8.1, namely that the solution to.
The bound in Theorem 8.2 is pretty good, but with a little work, we can do even better.
Recall, that a central idea of the recursive decomposition is choosing s to be as small as possible.
But to do this, the other terms must also be small, and as s gets smaller, we would expect C(Mt,Nt, r) to get larger.
But the bound for C(Mt,Nt, r) used a primitive estimate, and Theorem 8.1 itself can now be used to give a better estimate for this term.
Since the C(Mt,Nt, r) estimate will now be lower, we will be able to use a lower s.
An example of the use of the union/ﬁnd data structure is the generation of mazes, such as the one shown in Figure 8.25
In Figure 8.25, the starting point is the top-left corner, and the ending point is the bottom-right corner.
We can view the maze as a 50-by-88 rectangle of cells in which the top-left cell is connected to the bottom-right cell, and cells are separated from their neighboring cells via walls.
A simple algorithm to generate the maze is to start with walls everywhere (except for the entrance and exit)
We then continually choose a wall randomly, and knock it down if the cells that the wall separates are not already connected to each other.
If we repeat this process until the starting and ending cells are connected, then we have a maze.
Figure 8.26 Initial state: all walls up, all cells in their own set.
We use the union/ﬁnd data structure to represent sets of cells that are connected to each other.
Initially, walls are everywhere, and each cell is in its own equivalence class.
Figure 8.27 shows a later stage of the algorithm, after a few walls have been knocked down.
By performing two find operations, we see that these are in different sets;
Therefore, we knock down the wall that separates them, as shown in Figure 8.28
At the end of the algorithm, depicted in Figure 8.29, everything is connected, and we are done.
We have seen a very simple data structure to maintain disjoint sets.
When the union operation is performed, it does not matter, as far as correctness is concerned, which set retains its name.
A valuable lesson that should be learned here is that it can be very important to consider the alternatives when a particular step is not totally speciﬁed.
The union step is ﬂexible; by taking advantage of this, we are able to get a much more efﬁcient algorithm.
Path compression is one of the earliest forms of self-adjustment, which we have seen elsewhere (splay trees, skew heaps)
Its use is extremely interesting, especially from a theoretical point of view, because it was one of the ﬁrst examples of a simple algorithm with a not-so-simple worst-case analysis.
Your program should process a long sequence of equivalence operations using all six of the possible strategies.
What is the solution to the recurrence T(N) = Nf(N)T(f(N)) + N with appropriate initial conditions?
Show that if M = N2, then the running time of M union/ﬁnd operations is O(M)
Show that if M = N logN, then the running time of M union/ﬁnd operations is.
Show that if we do union-by-height and finds without path compression, then.
Prove that if path halving is performed on the finds and either union-by-height.
A general union/ﬁnd structure, supporting more operations, is given in [20]
In this chapter we discuss several common problems in graph theory.
Not only are these algorithms useful in practice, they are also interesting because in many real-life applications they are too slow unless careful attention is paid to the choice of data structures.
See an important technique, known as depth-ﬁrst search, and show how it can be used to solve several seemingly nontrivial problems in linear time.
For undirected graphs, we require that the edges be distinct.
The logic of these requirements is that the path u, v, u in an undirected graph should not be considered a cycle, because (u, v) and (v, u) are the same edge.
In a directed graph, these are different edges, so it makes sense to call this a cycle.
A directed graph is acyclic if it has no cycles.
A directed acyclic graph is sometimes referred to by its abbreviation, DAG.
An undirected graph is connected if there is a path from every vertex to every other vertex.
A directed graph with this property is called strongly connected.
If a directed graph is not strongly connected, but the underlying graph (without direction to the arcs) is connected, then the graph is said to be weakly connected.
A complete graph is a graph in which there is an edge between every pair of vertices.
An example of a real-life situation that can be modeled by a graph is the airport system.
Each airport is a vertex, and two vertices are connected by an edge if there is a nonstop ﬂight from the airports that are represented by the vertices.
The edge could have a weight, representing the time, distance, or cost of the ﬂight.
It is reasonable to assume that such a graph is directed, since it might take longer or cost more (depending on local taxes, for example) to ﬂy in different directions.
We would probably like to make sure that the airport system is strongly connected, so that it is always possible to ﬂy from any airport to any other airport.
We might also like to quickly determine the best ﬂight between any two airports.
Each street intersection represents a vertex, and each street is an edge.
The edge costs could represent, among other things, a speed limit or a capacity (number of lanes)
We could then ask for the shortest route or use this information to ﬁnd the most likely location for bottlenecks.
In the remainder of this chapter, we will see several more applications of graphs.
Many of these graphs can be quite large, so it is important that the algorithms we use be efﬁcient.
We will consider directed graphs (undirected graphs are similarly represented)
If the graph is not dense, in other words, if the graph is sparse, a better solution is an adjacency list representation.
For each vertex, we keep a list of all adjacent vertices.
If the edges have weights, then this additional information is also stored in the adjacency lists.
Undirected graphs can be similarly represented; each edge (u, v) appears in two lists, so the space usage essentially doubles.
A common requirement in graph algorithms is to ﬁnd all vertices adjacent to some given vertex v, and this can be done, in time proportional to the number of such vertices found, by a simple scan down the appropriate adjacency list.
First, observe that the lists themselves can be maintained in any kind of List, namely ArrayLists or LinkedLists.
However, for very sparse graphs, when using ArrayLists, the programmer may need to start the ArrayLists with a smaller capacity than the default; otherwise there could be signiﬁcant wasted space.
Because it is important to be able to quickly obtain the list of adjacent vertices for any vertex, the two basic options are to use a map in which the keys are vertices and the values are adjacency lists, or to maintain each adjacency list as a data member of a Vertex class.
The ﬁrst option is arguably simpler, but the second option can be faster, because it avoids repeated lookups in the map.
In the second scenario, if the vertex is a String (for instance, an airport name, or the name of a street intersection), then a map can be used in which the key is the vertex name and the value is a Vertex and each Vertex object keeps a list of adjacent vertices, and perhaps also the original String name.
In most of the chapter, we present the graph algorithms using pseudocode.
We will do this to save space and, of course, to make the presentation of the algorithms much clearer.
At the end of Section 9.3, we provide a working Java implementation of a routine that makes underlying use of a shortest-path algorithm to obtain its answers.
A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from vi to vj, then vj appears after vi in the ordering.
The graph in Figure 9.3 represents the course prerequisite structure at a state university in Miami.
A directed edge (v,w) indicates that course v must be completed before course w may be attempted.
A topological ordering of these courses is any course sequence that does not violate the prerequisite requirement.
It is clear that a topological ordering is not possible if the graph has a cycle, since for two vertices v and w on the cycle, v precedes w and w precedes v.
Furthermore, the ordering is not necessarily unique; any legal ordering will do.
A simple algorithm to ﬁnd a topological ordering is ﬁrst to ﬁnd any vertex with no incoming edges.
We can then print this vertex, and remove it, along with its edges, from the graph.
Then we apply this same strategy to the rest of the graph.
To formalize this, we deﬁne the indegree of a vertex v as the number of edges (u, v)
We compute the indegrees of all vertices in the graph.
It returns null if no such vertex exists; this indicates that the graph has a cycle.
By paying more careful attention to the data structures, it is possible to do better.
The cause of the poor running time is the sequential scan through the array of vertices.
However, in the search for a vertex of indegree 0, we look at (potentially) all the vertices, even though only a few have changed.
We can remove this inefﬁciency by keeping all the (unassigned) vertices of indegree 0 in a special box.
To implement the box, we can use either a stack or a queue; we will use a queue.
Then all vertices of indegree 0 are placed on an initially empty queue.
While the queue is not empty, a vertex v is removed, and all vertices adjacent to v have their indegrees decremented.
The topological ordering then is the order in which the vertices dequeue.
A pseudocode implementation of this algorithm is given in Figure 9.7
As before, we will assume that the graph is already read into an adjacency list and that the indegrees are computed and stored with the vertices.
We also assume each vertex has a ﬁeld named topNum, in which to place its topological numbering.
This is apparent when one realizes that the body of the for loop is executed at most once per edge.
The queue operations are done at most once per vertex, and the other initialization steps, including the computation of indegrees, also take time proportional to the size of the graph.
The input is a weighted graph: associated with each edge (vi, vj) is a cost ci,j to traverse the edge.
Given as input a weighted graph, G = (V, E), and a distinguished vertex, s, ﬁnd the shortest weighted path from s to every other vertex in G.
Generally, when it is not speciﬁed whether we are referring to a weighted or an unweighted path, the path is weighted if the graph is.
There are many examples where we might want to solve the shortest-path problem.
If the vertices represent computers; the edges represent a link between computers; and the costs represent communication costs (phone bill per megabyte of data), delay costs (number of seconds required to transmit a megabyte), or a combination of these and other.
We can model airplane or other mass transit routes by graphs and use a shortestpath algorithm to compute the best route between two points.
In this and many practical applications, we might want to ﬁnd the shortest path from one vertex, s, to only one other vertex, t.
Currently there are no algorithms in which ﬁnding the path from s to one vertex is any faster (by more than a constant factor) than ﬁnding the path from s to all vertices.
We will examine algorithms to solve four versions of this problem.
First, we will consider the unweighted shortest-path problem and show how to solve it in O(|E|+|V|)
Next, we will show how to solve the weighted shortest-path problem if we assume that there are no negative edges.
Using some vertex, s, which is an input parameter, we would like to ﬁnd the shortest path from s to all other vertices.
We are only interested in the number of edges contained on the path, so there are no weights on the edges.
For now, suppose we are interested only in the length of the shortest paths, not in the actual paths themselves.
Keeping track of the actual paths will turn out to be a matter of simple bookkeeping.
We can mark this information, obtaining the graph in Figure 9.11
Figure 9.11 Graph after marking the start node as reachable in zero edges.
Now we can start looking for all vertices that are a distance 1 away from s.
These can be found by looking at the vertices that are adjacent to s.
Figure 9.13 shows the progress that has been made so far.
All vertices have now been calculated, and so Figure 9.14 shows the ﬁnal result of the algorithm.
This strategy for searching a graph is known as breadth-ﬁrst search.
It operates by processing vertices in layers: The vertices closest to the start are evaluated ﬁrst, and the most distant vertices are evaluated last.
This is much the same as a level-order traversal for trees.
Figure 9.15 shows the initial conﬁguration of the table that our algorithm will use to keep track of its progress.
Figure 9.15 Initial conﬁguration of table used in unweighted shortest-path computation.
For each vertex, we will keep track of three pieces of information.
First, we will keep its distance from s in the entry dv.
The entry in pv is the bookkeeping variable, which will allow us to print the actual paths.
The entry known is set to true after a vertex is processed.
Initially, all entries are not known, including the start vertex.
When a vertex is marked known, we have a guarantee that no cheaper path will ever be found, and so processing for that vertex is essentially complete.
By tracing back through the pv variable, the actual path can be printed.
We will see how when we discuss the weighted case.
A very simple but abstract solution is to keep two boxes.
The test to ﬁnd an appropriate vertex v can be replaced by ﬁnding any vertex in box #1
After updating w (inside the innermost if block), we can add w to box #2
We can reﬁne this idea even further by using just one queue.
At the start of the pass, the queue contains only vertices of distance currDist.
When we add adjacent vertices of distance currDist + 1, since they enqueue at the rear, we are guaranteed that they will not be processed until after all the vertices of distance currDist have been processed.
We merely need to begin the process by placing the start node on the queue by itself.
In the pseudocode, we have assumed that the start vertex, s, is passed as a parameter.
Also, it is possible that the queue might empty prematurely, if some vertices are unreachable from the start node.
In this case, a distance of INFINITY will be reported for these nodes, which is perfectly reasonable.
Finally, the known ﬁeld is not used; once a vertex is processed it can never enter the queue again, so the fact that it need not be reprocessed is implicitly marked.
Figure 9.19 shows how the values on the graph we have been using are changed during the algorithm.
If the graph is weighted, the problem (apparently) becomes harder, but we can still use the ideas from the unweighted case.
Thus, each vertex is marked as either known or unknown.
A tentative distance dv is kept for each vertex, as before.
This distance turns out to be the shortest path length from s to v using only known vertices as intermediates.
As before, we record pv, which is the last vertex to cause a change to dv.
The general method to solve the single-source shortest-path problem is known as Dijkstra’s algorithm.
This thirty-year-old solution is a prime example of a greedy algorithm.
Greedy algorithms generally solve a problem in stages by doing what appears to be the best thing at each stage.
This greedy algorithm gives change using the minimum number of coins.
The main problem with greedy algorithms is that they do not always work.
Dijkstra’s algorithm proceeds in stages, just like the unweighted shortest-path algorithm.
At each stage, Dijkstra’s algorithm selects a vertex v, which has the smallest dv.
Figure 9.19 How the data change during the unweighted shortest-path algorithm.
The remainder of a stage consists of updating the values of dw.
Now that v1 is known, some entries need to be adjusted.
Both these vertices get their entries adjusted, as indicated in Figure 9.22
Figure 9.21 Initial conﬁguration of table used in Dijkstra’s algorithm.
Figure 9.24 shows the table after these vertices are selected.
Figure 9.28 graphically shows how edges are marked known and vertices updated during Dijkstra’s algorithm.
To print out the actual path from a start vertex to some vertex v, we can write a recursive routine to follow the trail left in the p variables.
Each Vertex stores various data ﬁelds that are used in the algorithm.
The path can be printed out using the recursive routine in Figure 9.30
The routine recursively prints the path all the way up to the vertex before v on the path and then just prints v.
Figure 9.31 shows the main algorithm, which is just a for loop to ﬁll up the table using the greedy selection rule.
Selection of the vertex v is a deleteMin operation, since once the unknown minimum vertex is found, it is no longer unknown and must be removed from future consideration.
The update of w’s distance can be implemented two ways.
The time to ﬁnd the minimum is then O(log |V|), as is the time to perform updates, which amount to decreaseKey operations.
Since priority queues do not efﬁciently support the find operation, the location in the priority queue of each value of di will need to be maintained and updated whenever di changes in the priority queue.
If the priority queue is implemented by a binary heap, this will be messy.
If a pairing heap (Chapter 12) is used, the code is not too bad.
DistType cvw = cost of edge from v to w;
Notice that for the typical problems, such as computer mail and mass transit commutes, the graphs are typically very sparse because most vertices have only a couple of edges, so it is important in many applications to use a priority queue to solve this problem.
There are better time bounds possible using Dijkstra’s algorithm if different data structures are used.
In Chapter 11, we will see another priority queue data structure called the Fibonacci heap.
Fibonacci heaps have good theoretical time bounds but a fair amount of overhead, so it is not clear whether using Fibonacci heaps is actually better in practice than Dijkstra’s algorithm with binary heaps.
To date, there are no meaningful average-case results for this problem.
If the graph has negative edge costs, then Dijkstra’s algorithm does not work.
The problem is that once a vertex u is declared known, it is possible that from some other, unknown vertex v there is a path back to u that is very negative.
In such a case, taking a path from s to v back to u is better than going from s to u without using v.
A tempting solution is to add a constant to each edge cost, thus removing negative edges, calculate a shortest path on the new graph, and then use that result on the original.
The naive implementation of this strategy does not work because paths with many edges become more weighty than paths with few edges.
A combination of the weighted and unweighted algorithms will solve the problem, but at the cost of a drastic increase in running time.
We forget about the concept of known vertices, since our algorithm needs to be able to change its mind.
We ﬁnd all vertices w adjacent to v such that dw > dv + cv,w.
We update dw and pw, and place w on a queue if it is not already there.
A bit can be set for each vertex to indicate presence in the queue.
If the graph is known to be acyclic, we can improve Dijkstra’s algorithm by changing the order in which vertices are declared known, otherwise known as the vertex selection rule.
The new rule is to select vertices in topological order.
The algorithm can be done in one pass, since the selections and updates can take place as the topological sort is being performed.
This selection rule works because when a vertex v is selected, its distance, dv, can no longer be lowered, since by the topological ordering rule it has no incoming edges emanating from unknown nodes.
An acyclic graph could model some downhill skiing problem—we want to get from point a to b but can only go downhill, so clearly there are no cycles.
Another possible application might be the modeling of (nonreversible) chemical reactions.
We could have each vertex represent a particular state of an experiment.
Edges would represent a transition from one state to another, and the edge weights might represent the energy released.
If only transitions from a higher energy state to a lower are allowed, the graph is acyclic.
A more important use of acyclic graphs is critical path analysis.
The graph in Figure 9.33 will serve as our example.
Update w w.dist = v.dist + cvw; w.path = v; if( w is not already in q )
Figure 9.32 Pseudocode for weighted shortest-path algorithm with negative edge costs.
The edges represent precedence relationships: An edge (v,w) means that activity v must be completed before activity w may begin.
Of course, this implies that the graph must be acyclic.
We assume that any activities that do not depend (either directly or indirectly) on each other can be performed in parallel by different servers.
This type of a graph could be (and frequently is) used to model construction projects.
In this case, there are several important questions which would be of interest to answer.
First, what is the earliest completion time for the project? We can see from the graph that 10 time units are required along the path A, C, F, H.
Another important question is to determine which activities can be delayed, and by how long, without affecting the minimum completion time.
For instance, delaying any of A, C, F, or H would push the completion time past 10 units.
On the other hand, activity B is less critical and can be delayed up to two time units without affecting the ﬁnal completion time.
To perform these calculations, we convert the activity-node graph to an event-node graph.
Each event corresponds to the completion of an activity and all its dependent activities.
Events reachable from a node v in the event-node graph may not commence until after the event v is completed.
Dummy edges and nodes may need to be inserted in the case where an activity depends on.
This is necessary in order to avoid introducing false dependencies (or false lack of dependencies)
To ﬁnd the earliest completion time of the project, we merely need to ﬁnd the length of the longest path from the ﬁrst event to the last event.
For general graphs, the longest-path problem generally does not make sense, because of the possibility of positive-cost cycles.
These are the equivalent of negative-cost cycles in shortest-path problems.
If positive-cost cycles are present, we could ask for the longest simple path, but no satisfactory solution is known for this problem.
Since the event-node graph is acyclic, we need not worry about cycles.
In this case, it is easy to adapt the shortest-path algorithm to compute the earliest completion time for all nodes in the graph.
If ECi is the earliest completion time for node i, then the applicable rules are.
Figure 9.35 shows the earliest completion time for each event in our example event-node graph.
We can also compute the latest time, LCi, that each event can ﬁnish without affecting the ﬁnal completion time.
These values can be computed in linear time by maintaining, for each vertex, a list of all adjacent and preceding vertices.
The earliest completion times are computed for vertices by their topological order, and the latest completion times are computed by reverse topological order.
The slack time for each edge in the event-node graph represents the amount of time that the completion of the corresponding activity can be delayed without delaying the overall completion.
Figure 9.37 Earliest completion time, latest completion time, and slack.
There is at least one path consisting entirely of zero-slack edges; such a path is a critical path.
Sometimes it is important to ﬁnd the shortest paths between all pairs of vertices in the graph.
Although we could just run the appropriate single-source algorithm |V| times, we might expect a somewhat faster solution, especially on a dense graph, if we compute all the information at once.
Although, for dense graphs, this is the same bound as running a simple (nonpriority queue) Dijkstra’s algorithm |V| times, the loops are so tight that the specialized all-pairs algorithm is likely to be faster in practice.
On sparse graphs, of course, it is faster to run |V| Dijkstra’s algorithms coded with priority queues.
In this section we write some Java routines to compute word ladders.
In a word ladder each word is formed by changing one character in the ladder’s previous word.
For instance, we can convert zero to five by a sequence of one-character substitutions as follows: zero hero here hire fire five.
This is an unweighted shortest problem in which each word is a vertex, and two vertices have edges (in both directions) between them if they can be converted to each other with a one-character substitution.
In Section 4.8, we described and wrote a Java routine that would create a Map in which the keys are words, and the values are Lists containing the words that can result from a one-character transformation.
As such, this Map represents the graph, in adjacency list format, and we only need to write one routine to run the single-source unweighted shortest-path algorithm and a second routine to output the sequence of words, after the single-source shortest-path algorithm has completed.
The ﬁrst routine is findChain, which takes the Map representing the adjacency lists and the two words to be connected and returns a Map in which the keys are words, and the corresponding value is the word prior to the key on the shortest ladder starting at first.
In other words, in the example above, if the starting word is zero, the value for key five is fire, the value for key fire is hire, the value for key hire is here, and so on.
It assumes that first is a valid word, which is an easily testable condition prior to the call.
The basic loop incorrectly assigns a previous entry for first (when the initial word adjacent to first is processed), so at line 25 that entry is repaired.
By using a LinkedList and inserting at the front, we obtain the word ladder in the correct order.
Suppose we are given a directed graph G = (V, E) with edge capacities cv,w.
These capacities could represent the amount of water that could ﬂow through a pipe or the amount of trafﬁc that could ﬂow on a street between two intersections.
We have two vertices: s, which we call the source, and t, which is the sink.
Through any edge, (v,w), at most cv,w units of “ﬂow” may pass.
At any vertex, v, that is not either s or t, the total ﬂow coming in must equal the total ﬂow going out.
The maximum ﬂow problem is to determine the maximum amount of ﬂow that can pass from s to t.
Although this example graph is acyclic, this is not a requirement; our (eventual) algorithm will work even if the graph has a cycle.
As required by the problem statement, no edge carries more ﬂow than its capacity.
Vertex a has three units of ﬂow coming in, which it distributes to c and d.
Vertex d takes three units of ﬂow from a and b and combines this, sending the result to t.
A vertex can combine and distribute ﬂow in any manner that it likes, as long as edge capacities are not violated and as long as ﬂow conservation is maintained (what goes in must come out)
Figure 9.40 A cut in graph G partitions the vertices with s and t in different groups.
We cut the graph into two parts; one part contains s and some other vertices; the other part contains t.
Since ﬂow must cross through the cut, the total capacity of all edges (u, v) where u is in s’s partition and v is in t’s partition is a bound on the maximum ﬂow.
Any graph has a large number of cuts; the cut with minimum total capacity provides a bound on the maximum ﬂow, and as it turns out (but it is not immediately obvious), the minimum cut capacity is exactly equal to the maximum ﬂow.
A ﬁrst attempt to solve the problem proceeds in stages.
We start with our graph, G, and construct a ﬂow graph Gf.
Gf tells the ﬂow that has been attained at any stage in the algorithm.
Initially all edges in Gf have no ﬂow, and we hope that when the algorithm terminates, Gf contains a maximum ﬂow.
We also construct a graph, Gr, called the residual graph.
Gr tells, for each edge, how much more ﬂow can be added.
We can calculate this by subtracting the current ﬂow from the capacity for each edge.
An edge in Gr is known as a residual edge.
At each stage, we ﬁnd a path in Gr from s to t.
The minimum edge on this path is the amount of ﬂow that can be added to every edge on the path.
When we ﬁnd no path from s to t in Gr, we terminate.
This algorithm is nondeterministic, in that we are free to choose any path from s to t; obviously some choices are better than others, and we will address this issue later.
Keep in mind that there is a slight ﬂaw in this algorithm.
There are many paths from s to t in the residual graph.
Then we can send two units of ﬂow through every edge on this path.
We will adopt the convention that once we have ﬁlled (saturated) an edge, it is removed from the residual graph.
Next, we might select the path s, a, c, t, which also allows two units of ﬂow.
Making the required adjustments gives the graphs in Figure 9.43
The only path left to select is s, a, d, t, which allows one unit of ﬂow.
The algorithm terminates at this point, because t is unreachable from s.
The resulting ﬂow of 5 happens to be the maximum.
To see what the problem is, suppose that with our initial graph, we chose the path s, a, d, t.
This path allows three units of ﬂow and thus seems to be a good choice.
The result of this choice, however, leaves only one path from s to t in the residual graph; it allows one more unit of ﬂow, and thus, our algorithm has.
Figure 9.41 Initial stages of the graph, ﬂow graph, and residual graph.
Figure 9.42 G, Gf , Gr after two units of ﬂow added along s, b, d, t.
Figure 9.43 G, Gf , Gr after two units of ﬂow added along s, a, c, t.
Figure 9.44 G, Gf , Gr after one unit of ﬂow added along s, a, d, t—algorithm terminates.
Figure 9.45 G, Gf , Gr if initial action is to add three units of ﬂow along s, a, d, t—algorithm terminates after one more step with suboptimal solution.
Figure 9.46 Graphs after three units of ﬂow added along s, a, d, t using correct algorithm.
This is an example of a greedy algorithm that does not work.
In order to make this algorithm work, we need to allow the algorithm to change its mind.
To do this, for every edge (v,w) with ﬂow fv,w in the ﬂow graph, we will add an edge in the residual graph (w, v) of capacity fv,w.
In effect, we are allowing the algorithm to undo its decisions by sending ﬂow back in the opposite direction.
Starting from our original graph and selecting the augmenting path s, a, d, t, we obtain the graphs in Figure 9.46
Notice that in the residual graph, there are edges in both directions between a and d.
Either one more unit of ﬂow can be pushed from a to d, or up to three units can be pushed back—we can undo ﬂow.
By pushing two units of ﬂow from d to a, the algorithm takes two units of ﬂow away from the edge (a, d) and is essentially changing its mind.
Figure 9.47 Graphs after two units of ﬂow added along s, b, d, a, c, t using correct algorithm.
Figure 9.48 The vertices reachable from s in the residual graph form one side of a cut; the unreachables form the other side of the cut.
There is no augmenting path in this graph, so the algorithm terminates.
Note that the same result would occur if at Figure 9.46, the augmenting path s, a, c, t was chosen which allows one unit of ﬂow, because then a subsequent augmenting path could be found.
It is easy to see that if the algorithm terminates, then it must terminate with a maximum ﬂow.
Termination implies that there is no path from s to t in the residual graph.
So cut the residual graph, putting the vertices reachable from s on one side, and the unreachables (which include t) on the other side.
Clearly any edges in the original graph G that cross the cut must be saturated; otherwise, there would be residual ﬂow remaining on one of the edges, which would then imply an edge that crosses the cut (in the wrong disallowed direction) in Gr.
But that means that the ﬂow in G is exactly equal to the capacity of a cut in G; hence we have a maximum ﬂow.
If the edge costs in the graph are integers, then the algorithm must terminate; each augmentation adds a unit of ﬂow, so we eventually reach the maximum ﬂow, though there.
Random augmentations could continually augment along a path that includes the edge connected by a and b.
A simple method to get around this problem is always to choose the augmenting path that allows the largest increase in ﬂow.
Finding such a path is similar to solving a weighted shortest-path problem and a single-line modiﬁcation to Dijkstra’s algorithm will do the trick.
If capmax is the maximum edge capacity, then one can show that O(|E| log capmax) augmentations will sufﬁce to ﬁnd the maximum ﬂow.
Another way to choose augmenting paths is always to take the path with the least number of edges, with the plausible expectation that by choosing a path in this manner, it is less likely that a small, ﬂow-restricting edge will turn up on the path.
With this rule, each augmenting step computes the shortest unweighted path from s to t in the residual graph, so assume that each vertex in the graph maintains dv, representing the shortest-path distance from s to v in the residual graph.
Each augmenting step can add new edges into the residual graph, but it is clear that no dv can decrease, because an edge is added in the opposite direction of an existing shortest path.
Suppose edge (u, v) is saturated; at that point, u had distance du and v had distance dv = du +1; then (u, v) was removed from.
But if it does, then the distance to u at that point must be dv + 1, which would be two higher than at the time (u, v) was previously removed.
This means that any edge can reappear at most |V|/2 times.
Each augmentation causes some edge to reappear so the number of augmentations is O(|E||V|)
The analyses required to produce these bounds are rather intricate, and it is not clear how the worst-case results relate to the running times encountered in practice.
A related, even more difﬁcult problem is the min-cost ﬂow problem.
Each edge has not only a capacity, but also a cost per unit of ﬂow.
The problem is to ﬁnd, among all maximum ﬂows, the one ﬂow of minimum cost.
The next problem we will consider is that of ﬁnding a minimum spanning tree in an undirected graph.
The problem makes sense for directed graphs but appears to be more difﬁcult.
Informally, a minimum spanning tree of an undirected graph G is a tree formed from graph edges that connects all the vertices of G at lowest total cost.
A minimum spanning tree exists if and only if G is connected.
Although a robust algorithm should report the case that G is unconnected, we will assume that G is connected and leave the issue of robustness as an exercise to the reader.
For any spanning tree T, if an edge e that is not in T is added, a cycle is created.
The removal of any edge on the cycle reinstates the spanning tree property.
The cost of the spanning tree is lowered if e has lower cost than the edge that was removed.
If, as a spanning tree is created, the edge that is added is the one of minimum cost that avoids creation of a cycle, then the cost of the resulting spanning tree cannot be improved, because any replacement edge would have cost at least as much as an edge already in the spanning tree.
Figure 9.50 A graph G and its minimum spanning tree.
This shows that greed works for the minimum spanning tree problem.
The two algorithms we present differ in how a minimum edge is selected.
One way to compute a minimum spanning tree is to grow the tree in successive stages.
In each stage, one node is picked as the root, and we add an edge, and thus an associated vertex, to the tree.
At any point in the algorithm, we can see that we have a set of vertices that have already been included in the tree; the rest of the vertices have not.
The algorithm then ﬁnds, at each stage, a new vertex to add to the tree by choosing the edge (u, v) such that the cost of (u, v) is the smallest among all edges where u is in the tree and v is not.
Initially, v1 is in the tree as a root with no edges.
Each step adds one edge and one vertex to the tree.
We can see that Prim’s algorithm is essentially identical to Dijkstra’s algorithm for shortest paths.
As before, for each vertex we keep values dv and pv and an indication of whether it is known or unknown.
The rest of the algorithm is exactly the same, with the exception that since the deﬁnition of dv is different, so is the.
Figure 9.52 Initial conﬁguration of table used in Prim’s algorithm.
For this problem, the update rule is even simpler than before: After a vertex v is selected, for each unknown w adjacent to v, dw = min(dw, cw,v)
The initial conﬁguration of the table is shown in Figure 9.52
The table resulting from this is shown in Figure 9.53
The next vertex chosen is v2 (arbitrarily breaking a tie)
The entire implementation of this algorithm is virtually identical to that of Dijkstra’s algorithm, and everything that was said about the analysis of Dijkstra’s algorithm applies here.
A second greedy strategy is continually to select the edges in order of smallest weight and accept an edge if it does not cause a cycle.
The action of the algorithm on the graph in the preceding example is shown in Figure 9.58
When the algorithm terminates, there is only one tree, and this is the minimum spanning tree.
Figure 9.59 shows the order in which edges are added to the forest.
It turns out to be simple to decide whether edge (u, v) should be accepted or rejected.
The invariant we will use is that at any point in the process, two vertices belong to the same set if and only if they are connected in the current spanning forest.
If u and v are in the same set, the edge is rejected, because since they are already connected, adding (u, v) would form a cycle.
Otherwise, the edge is accepted, and a union is performed on the two sets containing u and v.
It is easy to see that this maintains the set invariant, because once the edge (u, v) is added to the spanning forest, if w was connected to u and x was connected to v, then x and w must now be connected, and thus belong in the same set.
The edges could be sorted to facilitate the selection, but building a heap in linear time is a much better idea.
Then deleteMins give the edges to be tested in order.
Typically, only a small fraction of the edges need to be tested before the algorithm can terminate, although.
Method kruskal in Figure 9.60 ﬁnds a minimum spanning tree.
In practice, the algorithm is much faster than this time bound would indicate.
For each vertex, the ﬁeld visited is initialized to false.
By recursively calling the procedures only on nodes that have not been visited, we guarantee that we do not loop indeﬁnitely.
If the graph is undirected and not connected, or directed and not strongly connected, this strategy might fail to visit some nodes.
An undirected graph is connected if and only if a depth-ﬁrst search starting from any node visits every node.
Because this test is so easy to apply, we will assume that the graphs we deal with are connected.
If they are not, then we can ﬁnd all the connected components and apply our algorithm on each of these in turn.
As an example of depth-ﬁrst search, suppose in the graph of Figure 9.62 we start at vertex A.
Then we mark A as visited and call dfs(B) recursively.
We have actually touched every edge twice, once as (v,w) and again as (w, v), but this is really once per adjacency list entry.
We graphically illustrate these steps with a depth-ﬁrst spanning tree.
The root of the tree is A, the ﬁrst vertex visited.
Each edge (v,w) in the graph is present in the tree.
If, when we process (v,w), we ﬁnd that w is unmarked, or if, when we process (w, v), we ﬁnd that v is unmarked, we indicate this with a tree edge.
If, when we process (v,w), we ﬁnd that w is already marked, and when processing (w, v), we ﬁnd that v is already marked, we draw.
A preorder numbering of the tree, using only tree edges, tells us the order in which the vertices were marked.
If the graph is not connected, then processing all nodes (and edges) requires several calls to dfs, and each generates a tree.
A connected undirected graph is biconnected if there are no vertices whose removal disconnects the rest of the graph.
If the nodes are computers and the edges are links, then if any computer goes down, network mail is unaffected, except, of course, at the down computer.
Similarly, if a mass transit system is biconnected, users always have an alternate route should some terminal be disrupted.
If a graph is not biconnected, the vertices whose removal would disconnect the graph are known as articulation points.
The graph in Figure 9.64 is not biconnected: C and D are articulation points.
The removal of C would disconnect G, and the removal of D would disconnect E and F, from the rest of the graph.
Depth-ﬁrst search provides a linear-time algorithm to ﬁnd all articulation points in a connected graph.
First, starting at any vertex, we perform a depth-ﬁrst search and number the nodes as they are visited.
For each vertex v, we call this preorder number Num(v)
Then, for every vertex v in the depth-ﬁrst search spanning tree, we compute the lowest-numbered vertex, which we call Low(v), that is reachable from v by taking zero or more tree edges and then possibly one back edge (in that order)
The depth-ﬁrst search tree in Figure 9.65 shows the preorder number ﬁrst, and then the lowest-numbered vertex reachable under the rule described above.
The lowest-numbered vertex reachable by A, B, and C is vertex 1 (A), because they can all take tree edges to D and then one back edge back to A.
Figure 9.64 A graph with articulation points C and D.
Figure 9.65 Depth-ﬁrst tree for previous graph, with Num and Low.
By the deﬁnition of Low, Low(v) is the minimum of.
The ﬁrst condition is the option of taking no edges, the second way is to choose no tree edges and a back edge, and the third way is to choose some tree edges and possibly a back edge.
This third method is succinctly described with a recursive call.
Since we need to evaluate Low for all the children of v before we can evaluate Low(v), this is a postorder traversal.
For any edge (v,w), we can tell whether it is a tree edge or a back edge merely by checking Num(v) and Num(w)
Thus, it is easy to compute Low(v): We merely scan down v’s adjacency list, apply the proper rule, and keep track of the minimum.
All that is left to do is to use this information to ﬁnd articulation points.
Figure 9.66 Depth-ﬁrst tree that results if depth-ﬁrst search starts at C.
We will assume that Vertex contains the data ﬁelds visited (initialized to false), num, low, and parent.
We will also keep a (Graph) class variable called counter, which is initialized to 1, to assign the preorder traversal numbers, num.
We also leave out the easily implemented test for the root.
As we have already stated, this algorithm can be implemented by performing a preorder traversal to compute Num and then a postorder traversal to compute Low.
A third traversal can be used to check which vertices satisfy the articulation point criteria.
The second and third passes, which are postorder traversals, can be implemented by the code in Figure 9.68
If w is adjacent to v, then the recursive call to w will ﬁnd v adjacent to w.
This is not a back edge, only an edge that has already been considered and needs to be ignored.
Otherwise, the procedure computes the minimum of the various low and num entries, as speciﬁed by the algorithm.
There is no rule that a traversal must be either preorder or postorder.
It is possible to do processing both before and after the recursive calls.
The procedure in Figure 9.69 combines the two routines assignNum and assignLow in a straightforward manner to produce the procedure findArt.
Figure 9.68 Pseudocode to compute Low and to test for articulation points (test for the root is omitted)
A popular puzzle is to reconstruct these ﬁgures using a pen, drawing each line exactly once.
The pen may not be lifted from the paper while the drawing is being performed.
As an extra challenge, make the pen ﬁnish at the same point at which it started.
Stop reading if you would like to try to solve it.
Figure 9.69 Testing for articulation points in one depth-ﬁrst search (test for the root is omitted) (pseudocode)
The ﬁrst ﬁgure can be drawn only if the starting point is the lower left- or right-hand corner, and it is not possible to ﬁnish at the starting point.
The second ﬁgure is easily drawn with the ﬁnishing point the same as the starting point, but the third ﬁgure cannot be drawn at all within the parameters of the puzzle.
We can convert this problem to a graph theory problem by assigning a vertex to each intersection.
Then the edges can be assigned in the natural manner, as in Figure 9.71
After this conversion is performed, we must ﬁnd a path in the graph that visits every edge exactly once.
If we are to solve the “extra challenge,” then we must ﬁnd a cycle that visits every edge exactly once.
This graph problem was solved in 1736 by Euler and marked the beginning of graph theory.
The problem is thus commonly referred to as an Euler path (sometimes Euler tour) or Euler circuit problem, depending on the speciﬁc problem.
The Euler tour and Euler circuit problems, though slightly different, have the same basic solution.
Thus, we will consider the Euler circuit problem in this section.
The ﬁrst observation that can be made is that an Euler circuit, which must end on its starting vertex, is possible only if the graph is connected and each vertex has an even degree (number of edges)
This is because, on the Euler circuit, a vertex is entered and then left.
If any vertex v has odd degree, then eventually we will reach the point where only one edge into v is unvisited, and taking it will strand us at v.
If exactly two vertices have odd degree, an Euler tour, which must visit every edge but need not return to its starting vertex, is still possible if we start at one of the odd-degree vertices and ﬁnish at the other.
If more than two vertices have odd degree, then an Euler tour is not possible.
The observations of the preceding paragraph provide us with a necessary condition for the existence of an Euler circuit.
It does not, however, tell us that all connected graphs that satisfy this property must have an Euler circuit, nor does it give us guidance on how to ﬁnd one.
It turns out that the necessary condition is also sufﬁcient.
That is, any connected graph, all of whose vertices have even degree, must have an Euler circuit.
We can assume that we know that an Euler circuit exists, since we can test the necessary and sufﬁcient condition in linear time.
Then the basic algorithm is to perform a depth-ﬁrst search.
There are a surprisingly large number of “obvious” solutions that do not work.
The main problem is that we might visit a portion of the graph and return to the starting point prematurely.
If all the edges coming out of the start vertex have been used up, then part of the graph is untraversed.
The easiest way to ﬁx this is to ﬁnd the ﬁrst vertex on this path that has an untraversed edge, and perform another depth-ﬁrst search.
This will give another circuit, which can be spliced into the original.
It is easily seen that this graph has an Euler circuit.
Then we are stuck, and most of the graph is still untraversed.
We then continue from vertex 4, which still has unexplored edges.
The graph that remains after this is shown in Figure 9.74
Notice that in this graph all the vertices must have even degree, so we are guaranteed to ﬁnd a cycle to add.
The remaining graph might not be connected, but this is not important.
As all the edges are traversed, the algorithm terminates with an Euler circuit.
To make this algorithm efﬁcient, we must use appropriate data structures.
We will sketch some of the ideas, leaving the implementation as an exercise.
To make splicing simple, the path should be maintained as a linked list.
To avoid repetitious scanning of adjacency lists, we must maintain, for each adjacency list, the last edge scanned.
When a path is spliced in, the search for a new vertex from which to perform the next depthﬁrst search must begin at the start of the splice point.
This guarantees that the total work performed on the vertex search phase is O(|E|) during the entire life of the algorithm.
A very similar problem is to ﬁnd a simple cycle, in an undirected graph, that visits every vertex.
Although it seems almost identical to the Euler circuit problem, no efﬁcient algorithm for it is known.
Using the same strategy as with undirected graphs, directed graphs can be traversed in linear time, using depth-ﬁrst search.
If the graph is not strongly connected, a depth-ﬁrst search starting at some node might not visit all nodes.
In this case we repeatedly perform depth-ﬁrst searches, starting at some unmarked node, until all vertices have been visited.
As an example, consider the directed graph in Figure 9.76
Arbitrarily, we start at H, which visits J and I.
Finally, we start at G, which is the last vertex that needs to be visited.
The corresponding depth-ﬁrst search tree is shown in Figure 9.77
The dashed arrows in the depth-ﬁrst spanning forest are edges (v,w) for which w was already marked at the time of consideration.
In undirected graphs, these are always back edges, but, as we can see, there are three types of edges that do not lead to new vertices.
First, there are back edges, such as (A, B) and (I,H)
There are also forward edges, such as (C,D) and (C, E), that lead from a tree node to a descendant.
Finally, there are cross edges, such as (F,C) and (G, F), which connect two tree nodes that are not directly related.
Depthﬁrst search forests are generally drawn with children and new trees added to the forest from.
In a depth-ﬁrst search of a directed graph drawn in this manner, cross edges always go from right to left.
Some algorithms that use depth-ﬁrst search need to distinguish between the three types of nontree edges.
This is easy to check as the depth-ﬁrst search is being performed, and it is left as an exercise.
One use of depth-ﬁrst search is to test whether or not a directed graph is acyclic.
The rule is that a directed graph is acyclic if and only if it has no back edges.
The graph above has back edges, and thus is not acyclic.
By performing two depth-ﬁrst searches, we can test whether a directed graph is strongly connected, and if it is not, we can actually produce the subsets of vertices that are strongly connected to themselves.
This can also be done in only one depth-ﬁrst search, but the method used here is much simpler to understand.
First, a depth-ﬁrst search is performed on the input graph G.
The vertices of G are numbered by a postorder traversal of the depth-ﬁrst spanning forest, and then all edges in G are reversed, forming Gr.
The algorithm is completed by performing a depth-ﬁrst search on Gr, always starting a new depth-ﬁrst search at the highest-numbered vertex.
This leads nowhere, so the next search is started at H.
The next calls after this are dfs(D) and ﬁnally dfs(E)
The resulting depth-ﬁrst spanning forest is shown in Figure 9.79
To see why this algorithm works, ﬁrst note that if two vertices v and w are in the same strongly connected component, then there are paths from v to w and from w to v in the original graph G, and hence also in Gr.
Now, if two vertices v and w are not in the same depth-ﬁrst spanning tree of Gr, clearly they cannot be in the same strongly connected component.
To prove that this algorithm works, we must show that if two vertices v and w are in the same depth-ﬁrst spanning tree of Gr, there must be paths from v to w and from w to v.
Equivalently, we can show that if x is the root of the depth-ﬁrst spanning tree of Gr containing v, then there is a path from x to v and from v to x.
Applying the same logic to w would then give a path from x to w and from w to x.
These paths would imply paths from v to w and w to v (going through x)
Since v is a descendant of x in Gr ’s depth-ﬁrst spanning tree, there is a path from x to v in Gr and thus a path from v to x in G.
Furthermore, since x is the root, x has the higher postorder number from the ﬁrst depth-ﬁrst search.
Therefore, during the ﬁrst depth-ﬁrst search, all the work processing v was completed before the work at x was completed.
Since there is a path from v to x, it follows that v must be a descendant of x in the spanning tree for G—otherwise v would ﬁnish after x.
This implies a path from x to v in G and completes the proof.
In this chapter, we have seen solutions to a wide variety of graph theory problems.
We have also mentioned, in passing, that for some problems certain variations seem harder than the original.
Recall that the Euler circuit problem, which ﬁnds a path that touches every edge exactly once, is solvable in linear time.
The Hamiltonian cycle problem asks for a simple cycle that contains every vertex.
The single-source unweighted shortest-path problem for directed graphs is also solvable in linear time.
No linear-time algorithm is known for the corresponding longestsimple-path problem.
The situation for these problem variations is actually much worse than we have described.
Not only are no linear algorithms known for these variations, but there are no known algorithms that are guaranteed to run in polynomial time.
The best known algorithms for these problems could take exponential time on some inputs.
In this section we will take a brief look at this problem.
This topic is rather complex, so we will only take a quick and informal look at it.
Because of this, the discussion may be (necessarily) somewhat imprecise in places.
We will see that there are a host of important problems that are roughly equivalent in complexity.
The exact complexity of these NP-complete problems has yet to be determined and remains the foremost open problem in theoretical computer science.
Either all these problems have polynomial-time solutions or none of them do.
When classifying problems, the ﬁrst step is to examine the boundaries.
We have already seen that many problems can be solved in linear time.
We have also seen some O(logN) running times, but these either assume some preprocessing (such as input already being read or a data structure already being built) or occur on arithmetic examples.
For instance, the gcd algorithm, when applied on two numbers M and N, takes O(logN) time.
Since the numbers consist of logM and logN bits respectively, the gcd algorithm is really taking time that is linear in the amount or size of input.
Thus, when we measure running time, we will be concerned with the running time as a function of the amount of input.
At the other end of the spectrum lie some truly hard problems.
This does not mean the typical exasperated moan, which means that it would take a genius to solve the problem.
Is it possible to have your Java compiler have an extra feature that not only detects syntax errors, but also all inﬁnite loops? This seems like a hard problem, but one might expect that if some very clever programmers spent enough time on it, they could produce this enhancement.
The intuitive reason that this problem is undecidable is that such a program might have a hard time checking itself.
For this reason, these problems are sometimes called recursively undecidable.
If an inﬁnite loop–checking program could be written, surely it could be used to check itself.
It prints out the phrase YES if P loops when run on itself.
If P terminates when run on itself, a natural thing to do would be to print out NO.
Instead of doing that, we will have LOOP go into an inﬁnite loop.
What happens when LOOP is given itself as input? Either LOOP halts, or it does not halt.
The problem is that both these possibilities lead to contradictions, in much the same way as does the phrase “This sentence is a lie.”
By our deﬁnition, LOOP(P) goes into an inﬁnite loop if P(P) terminates.
Then, according to the LOOP program, LOOP(P) is obligated to go into an inﬁnite loop.
Thus, we must have LOOP(LOOP) terminating and entering an inﬁnite loop, which is clearly not possible.
On the other hand, suppose that when P = LOOP, P(P) enters an inﬁnite loop.
Then LOOP(P) must terminate, and we arrive at the same set of contradictions.
Thus, we see that the program LOOP cannot possibly exist.
A few steps down from the horrors of undecidable problems lies the class NP.
A deterministic machine, at each point in time, is executing an instruction.
Depending on the instruction, it then goes to some next instruction, which is unique.
It is free to choose any that it wishes, and if one of these steps leads to a solution, it will always choose the correct one.
A nondeterministic machine thus has the power of extremely good (optimal) guessing.
This probably seems like a ridiculous model, since nobody could possibly build a nondeterministic computer, and because it would seem to be an incredible upgrade to your standard computer (every problem might now seem trivial)
We will see that nondeterminism is a very useful theoretical construct.
Furthermore, nondeterminism is not as powerful as one might think.
For instance, undecidable problems are still undecidable, even if nondeterminism is allowed.
A simple way to check if a problem is in NP is to phrase the problem as a yes/no question.
The problem is in NP if, in polynomial time, we can prove that any “yes” instance is correct.
We do not have to worry about “no” instances, since the program always makes the right choice.
Thus, for the Hamiltonian cycle problem, a “yes” instance would be any simple circuit in the graph that includes all the vertices.
This is in NP, since, given the path, it is a simple matter to check that it is really a Hamiltonian cycle.
Appropriately phrased questions, such as “Is there a simple path of length > K?” can also easily be checked and are in NP.
Any path that satisﬁes this property can be checked trivially.
Notice also that not all decidable problems are in NP.
Consider the problem of determining whether a graph does not have a Hamiltonian cycle.
To prove that a graph has a Hamiltonian cycle is a relatively simple matter—we just need to exhibit one.
Nobody knows how to show, in polynomial time, that a graph does not have a Hamiltonian cycle.
It seems that one must enumerate all the cycles and check them one by one.
Thus the Non–Hamiltonian cycle problem is not known to be in NP.
Among all the problems known to be in NP, there is a subset, known as the NP-complete problems, which contains the hardest.
An NP-complete problem has the property that any problem in NP can be polynomially reduced to it.
Solve P2, and then map the answer back to the original.
As an example, numbers are entered into a pocket calculator in decimal.
The decimal numbers are converted to binary, and all calculations are performed in binary.
Then the ﬁnal answer is converted back to decimal for display.
The reason that NP-complete problems are the hardest NP problems is that a problem that is NP-complete can essentially be used as a subroutine for any problem in NP, with only a polynomial amount of overhead.
Thus, if any NP-complete problem has a polynomial-time solution, then every problem in NP must have a polynomial-time solution.
This makes the NP-complete problems the hardest of all NP problems.
As an example, suppose that we already know that the Hamiltonian cycle problem is NP-complete.
For instance, printed circuit boards need to have holes punched so that chips, resistors, and other electronic components can be placed.
Punching the hole is a quick operation; the time-consuming step is positioning the hole puncher.
The time required for positioning depends on the distance traveled from hole to hole.
Figure 9.80 Hamiltonian cycle problem transformed to traveling salesman problem.
There is now a long list of problems known to be NP-complete.
To prove that some new problem is NP-complete, it must be shown to be in NP, and then an appropriate NP-complete problem must be transformed into it.
Although the transformation to a traveling salesman problem was rather straightforward, most transformations are actually quite involved and require some tricky constructions.
Generally, several different NP-complete problems are considered before the problem that actually provides the reduction.
As we are only interested in the general ideas, we will not show any more transformations; the interested reader can consult the references.
The alert reader may be wondering how the ﬁrst NP-complete problem was actually proven to be NP-complete.
Since proving that a problem is NP-complete requires transforming it from another NP-complete problem, there must be some NP-complete problem for which this strategy will not work.
The ﬁrst problem that was proven to be NP-complete was the satisﬁability problem.
The satisﬁability problem takes as input a Boolean expression and asks whether the expression has an assignment to the variables that gives a value of true.
Satisﬁability is certainly in NP, since it is easy to evaluate a Boolean expression and check whether the result is true.
In 1971, Cook showed that satisﬁability was NP-complete by directly proving that all problems that are in NP could be transformed to satisﬁability.
To do this, he used the one known fact about every problem in NP: Every problem in NP.
The formal model for a computer is known as a Turing machine.
Cook showed how the actions of this machine could be simulated by an extremely complicated and long, but still polynomial, Boolean formula.
This Boolean formula would be true if and only if the program which was being run by the Turing machine produced a “yes” answer for its input.
Once satisﬁability was shown to be NP-complete, a host of new NP-complete problems, including some of the most classic problems, were also shown to be NP-complete.
In addition to the satisﬁability, Hamiltonian circuit, traveling salesman, and longestpath problems, which we have already examined, some of the more well-known NPcomplete problems which we have not discussed are bin packing, knapsack, graph coloring, and clique.
The list is quite extensive and includes problems from operating systems (scheduling and security), database systems, operations research, logic, and especially graph theory.
In this chapter we have seen how graphs can be used to model many real-life problems.
Many of the graphs that occur are typically very sparse, so it is important to pay attention to the data structures that are used to implement them.
We have also seen a class of problems that do not seem to have efﬁcient solutions.
In Chapter 10, some techniques for dealing with these problems will be discussed.
Propose a method that stores a graph in an adjacency matrix (so that testing for the existence of an edge is O(1)) but avoids the quadratic running time.
Find the shortest path from A to all other vertices for the graph in Figure 9.82
Find the shortest unweighted path from B to all other vertices for the graph in.
Give an example where Dijkstra’s algorithm gives the wrong answer in the presence of a negative edge but no negative-cost cycle.
Explain how to modify Dijkstra’s algorithm to produce a count of the number of different minimum paths from v to w.
Explain how to modify Dijkstra’s algorithm so that if there is more than one minimum path from v to w, a path with the fewest number of edges is chosen.
Give a linear-time algorithm to ﬁnd a maximum ﬂow from s to t.
A matching of four edges (indicated by dashed edges) is shown in Figure 9.83
There is a matching of ﬁve edges, which is maximum.
Show how the bipartite matching problem can be used to solve the following problem: We have a set of instructors, a set of courses, and a list of courses that each instructor is qualiﬁed to teach.
If no instructor is required to teach more than one course, and only one instructor may teach a given course, what is the maximum number of courses that can be offered? c.
Show that the network ﬂow problem can be used to solve the bipartite matching.
What is the time complexity of your solution to part (b)?
Give an algorithm to ﬁnd an augmenting path that permits the maximum ﬂow.
Let f be the amount of ﬂow remaining in the residual graph.
Show that |E| ln f iterations sufﬁce to produce the maximum ﬂow.
Show the depth-ﬁrst spanning tree and the values of Num and Low for each vertex.
Give an algorithm to ﬁnd the minimum number of edges that need to be removed from an undirected graph so that the resulting graph is acyclic.
Modify the algorithm in Figure 9.69 to ﬁnd the biconnected components instead of the articulation points.
Show that all edges in the tree are either tree edges or cross edges.
Write a program to ﬁnd an Euler circuit in a graph if one exists.
Write a program to ﬁnd an Euler tour in a graph if one exists.
Prove that a directed graph has an Euler circuit if and only if it is strongly connected and every vertex has equal indegree and outdegree.
Give a linear-time algorithm to ﬁnd an Euler circuit in a directed graph where one exists.
Consider the following solution to the Euler circuit problem: Assume that the graph is biconnected.
Perform a depth-ﬁrst search, taking back edges only as a last resort.
If the graph is not biconnected, apply the algorithm recursively on the biconnected components.
Suppose that when taking back edges, we take the back edge to the nearest ancestor.
Show that neither of the graphs in Figure 9.87 is planar.
Show that in a planar graph, there must exist some vertex which is connected to.
Which of the algorithms in this chapter work without modiﬁcation for multigraphs? What modiﬁcations need to be done for the others?
Use depth-ﬁrst search to design a linear algorithm to convert each edge in G to a directed edge such that the resulting graph is strongly connected, or determine that this is not possible.
Each stick is speciﬁed by its two endpoints; each endpoint is an ordered triple giving its x, y, and z coordinates; no stick is vertical.
A stick may be picked up only if there is no stick on top of it.
Explain how to write a routine that takes two sticks a and b and reports whether.
Give a linear-time algorithm to test a graph for two-colorability.
Assume graphs are stored in adjacency list format; you must specify any additional data structures that are needed.
Give a linear-time algorithm that ﬁnds a vertex whose removal from an N vertex tree leaves no subtree with more than N/2 vertices.
Two squares belong to the same group if they share a common edge.
In Figure 9.88, there is one group of four occupied squares, three groups of two occupied squares, and two individual occupied squares.
Assume that the grid is represented by a two-dimensional array.
Computes the size of a group when a square in the group is given.
Suppose we want to output the path in the maze.
Assume that the maze is represented as a matrix; each cell in the matrix stores information about what walls are present (or absent)
Write a program that computes enough information to output a path in the.
Write a program that draws the maze and, at the press of a button, draws the path.
If the penalty is 0, then the problem is trivial.
Describe an algorithm to solve this version of the problem.
Describe a linear-time algorithm that determines the minimum number of walls.
Describe an algorithm (not necessarily linear-time) that ﬁnds a shortest path after knocking down the minimum number of walls.
Note that the solution to part (a) would give no information about which walls would be the best to knock down.
As mentioned at the end of Section 9.3.6, this is essentially a weighted shortest-path problem.
Explain how each of the following problems (Exercises 9.50–9.53) can be solved by applying a shortest-path algorithm.
Then design a mechanism for representing an input, and write a program that solves the problem.
If all teams have at least one win and a loss, we can generally prove, by a silly transitivity argument, that any team is better than any other.
For instance, in the six-team league where everyone plays three games, suppose we have the following results: A beat B and C; B beat C and F; C beat D; D beat E; E beat A; F beat D and E.
Then we can prove that A is better than F, because A beat B, who in turn beat F.
Similarly, we can prove that F is better than A because F beat E and E beat A.
Given a list of game scores and two teams X and Y, either ﬁnd a proof (if one exists) that X is better than Y, or indicate that no proof of this form can be found.
Assume that all courses are offered every semester and that the student can take an unlimited number of courses.
Given a list of courses and their prerequisites, compute a schedule that requires the minimum number of semesters.
The minimum number of links is an actor’s Bacon number.
Sally Field has a Bacon number of 2, because she was in Forrest Gump with.
Assume that you have a comprehensive list of actors, with roles,3 and do the following: a.
Explain how to ﬁnd the actor with the highest Bacon number.
Explain how to ﬁnd the minimum number of links between two arbitrary actors.
Prove that the Hamiltonian cycle problem is NP-complete for directed graphs.
Prove that the unweighted simple longest-path problem is NP-complete for.
Ford and Fulkerson’s seminal work on network ﬂow is [15]
The idea of augmenting along shortest paths or on paths admitting the largest ﬂow increase is from [13]
An algorithm for the min-cost ﬂow problem can be found in [20]
An early minimum spanning tree algorithm can be found in [4]
An empirical study of these algorithms suggests that Prim’s algorithm, implemented with decreaseKey, is best in practice on most graphs [42]
The ﬁrst linear-time strong components algorithm (Exercise 9.28) appears in the same paper.
The algorithm presented in the text is due to Kosaraju (unpublished) and Sharir [45]
The classic reference work for the theory of NP-complete problems is [21]
An approximation algorithm for the traveling salesman problem, which generally gives nearly optimal results, can be found in [40]
The problem can be generalized by adding weights to the edges and removing the restriction that the graph is bipartite.
Efﬁcient solutions for the unweighted matching problem for general graphs are quite complex.
Exercise 9.35 deals with planar graphs, which commonly arise in practice.
Planar graphs are very sparse, and many difﬁcult problems are easier on planar graphs.
An example is the graph isomorphism problem, which is solvable in linear time for planar graphs [29]
So far, we have been concerned with the efﬁcient implementation of algorithms.
We have seen that when an algorithm is given, the actual data structures need not be speciﬁed.
It is up to the programmer to choose the appropriate data structure in order to make the running time as small as possible.
In this chapter, we switch our attention from the implementation of algorithms to the design of algorithms.
Most of the algorithms that we have seen so far are straightforward and simple.
Chapter 9 contains some algorithms that are much more subtle, and some require an argument (in some cases lengthy) to show that they are indeed correct.
In this chapter, we will focus on ﬁve of the common types of algorithms used to solve problems.
For many problems, it is quite likely that at least one of these methods will work.
Discuss, in general terms, the time and space complexity, where appropriate.
The ﬁrst type of algorithm we will examine is the greedy algorithm.
We have already seen three greedy algorithms in Chapter 9: Dijkstra’s, Prim’s, and Kruskal’s algorithms.
In each phase, a decision is made that appears to be good, without regard for future consequences.
This “take what you can get now” strategy is the source of the name for this class of algorithms.
When the algorithm terminates, we hope that the local optimum is equal to the global optimum.
If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution.
If the absolute best answer is not required, then simple greedy algorithms are sometimes used to generate approximate answers, rather than using the more complicated algorithms generally required to generate an exact answer.
Thus, to give out seventeen dollars and sixty-one cents in change, we give out a ten-dollar bill, a ﬁve-dollar bill, two one-dollar bills, two quarters, one dime, and one penny.
By doing this, we are guaranteed to minimize the number of bills and coins.
This algorithm does not work in all monetary systems, but fortunately, we can prove that it does work in the American monetary system.
Indeed, it works even if two-dollar bills and ﬁfty-cent pieces are allowed.
Trafﬁc problems provide an example where making locally optimal choices does not always work.
For example, during certain rush hour times in Miami, it is best to stay off the prime streets even if they look empty, because trafﬁc will come to a standstill a mile down the road, and you will be stuck.
Even more shocking, it is better in some cases to make a temporary detour in the direction opposite your destination in order to avoid all trafﬁc bottlenecks.
In the remainder of this section, we will look at several applications that use greedy algorithms.
Virtually all scheduling problems are either NP-complete (or of similar difﬁcult complexity) or are solvable by a greedy algorithm.
The second application deals with ﬁle compression and is one of the earliest results in computer science.
Finally, we will look at an example of a greedy approximation algorithm.
What is the best way to schedule these jobs in order to minimize the average completion time? In this entire section, we will assume nonpreemptive scheduling: Once a job is started, it must run to completion.
As an example, suppose we have the four jobs and associated running times shown in Figure 10.1
The schedule given in Figure 10.3 is arranged by shortest job ﬁrst.
We can show that this will always yield an optimal schedule.
From this, we see that the total cost, C, of the schedule is.
Notice that in Equation (10.2), the ﬁrst sum is independent of the job ordering, so only the second sum affects the total cost.
Then a calculation shows that by swapping jix and jiy , the second sum increases, decreasing the total cost.
Thus, any schedule of jobs in which the times are.
The only schedules left are those in which the jobs are arranged by smallest running time ﬁrst, breaking ties arbitrarily.
This result indicates the reason the operating system scheduler generally gives precedence to shorter jobs.
The Multiprocessor Case We can extend this problem to the case of several processors.
We will assume without loss of generality that the jobs are ordered, shortest running time ﬁrst.
Figure 10.5 shows an optimal arrangement to minimize mean completion time.
Even if P does not divide N exactly, there can still be many optimal solutions, even if all the job times are distinct.
Minimizing the Final Completion Time We close this section by considering a very similar problem.
Suppose we are only concerned with when the last job ﬁnishes.
Although this schedule does not have minimum mean completion time, it has merit in that the completion time of the entire sequence is earlier.
If the same user owns all these jobs, then this is the preferable method of scheduling.
Although these problems are very similar, this new problem turns out to be NP-complete; it is just another way of phrasing the knapsack or bin-packing problems, which we will encounter later in this section.
Figure 10.6 A second optimal solution for the multiprocessor case.
In this section, we consider a second application of greedy algorithms, known as ﬁle compression.
Suppose we have a ﬁle that contains only the characters a, e, i, s, t, plus blank spaces and newlines.
Figure 10.9 Representation of the original code in a tree.
Many of the very large ﬁles are output of some program and there is usually a big disparity between the most frequent and least frequent characters.
For instance, many large data ﬁles have an inordinately large amount of digits, blanks, and newlines, but few q’s and x’s.
We might be interested in reducing the ﬁle size in the case where we are transmitting it over a slow phone line.
Also, since on virtually every machine, disk space is precious, one might wonder if it would be possible to provide a better code and reduce the total number of bits required.
The general strategy is to allow the code length to vary from character to character and to ensure that the frequently occurring characters have short codes.
Notice that if all the characters occur with the same frequency, then there are not likely to be any savings.
The binary code that represents the alphabet can be represented by the binary tree shown in Figure 10.9
The tree in Figure 10.9 has data only at the leaves.
A better code than the one given in Figure 10.9 can be obtained by noticing that the.
By placing the newline symbol one level higher at its parent, we obtain the new tree in Figure 10.10
This new tree has cost of 173, but is still far from optimal.
Notice that the tree in Figure 10.10 is a full tree: All nodes either are leaves or have two children.
An optimal code will always have this property, since otherwise, as we have already seen, nodes with only one child could move up a level.
If the characters are placed only at the leaves, any sequence of bits can always be decoded unambiguously.
The remainder of the code is a, space, t, i, e, and newline.
Thus, it does not matter if the character codes are different lengths, as long as no character code is a preﬁx of another character code.
Conversely, if a character is contained in a nonleaf node, it is no longer possible to guarantee that the decoding will be unambiguous.
Putting these facts together, we see that our basic problem is to ﬁnd the full binary tree of minimum total cost (as deﬁned above), where all characters are contained in the leaves.
The tree in Figure 10.11 shows the optimal tree for our sample alphabet.
These can be obtained by swapping children in the encoding tree.
The main unresolved question, then, is how the coding tree is constructed.
Thus, this coding system is commonly referred to as a Huffman code.
At the beginning of the algorithm, there are C single-node trees—one for each character.
At the end of the algorithm there is one tree, and this is the optimal Huffman coding tree.
A worked example will make the operation of the algorithm clear.
Figure 10.13 shows the initial forest; the weight of each tree is shown in small type at the root.
The two trees of lowest weight are merged together, creating the forest shown in Figure 10.14
We will name the new root T1, so that future merges can be stated unambiguously.
We have made s the left child arbitrarily; any tiebreaking procedure can be used.
The total weight of the new tree is just the sum of the weights of the old trees, and can thus be easily computed.
It is also a simple matter to create the new tree, since we merely need to get a new node, set the left and right links, and record the weight.
Now there are six trees, and we again select the two trees of smallest weight.
After the third merge is completed, the two trees of lowest weight are the single-node trees representing i and the blank space.
The ﬁfth step is to merge the trees with roots e and T3, since these trees have the two smallest weights.
The result of this step is shown in Figure 10.18
Finally, the optimal tree, which was shown in Figure 10.11, is obtained by merging the two remaining trees.
We will sketch the ideas involved in proving that Huffman’s algorithm yields an optimal code; we will leave the details as an exercise.
First, it is not hard to show by contradiction that the tree must be full, since we have already seen how a tree that is not full is improved.
We can then argue that the characters in any two nodes at the same depth can be swapped without affecting optimality.
This shows that an optimal tree can always be found that contains the two least frequent symbols as siblings; thus the ﬁrst step is not a mistake.
The proof can be completed by using an induction argument.
As trees are merged, we consider the new character set to be the characters in the roots.
This is probably the trickiest part of the proof; you are urged to ﬁll in all of the details.
The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations.
First, the encoding information must be transmitted at the start of the compressed ﬁle, since otherwise it will be impossible to decode.
There are several ways of doing this; see Exercise 10.4
For small ﬁles, the cost of transmitting this table will override any possible savings in compression, and the result will probably be ﬁle expansion.
Of course, this can be detected and the original left intact.
For large ﬁles, the size of the table is not signiﬁcant.
The second problem is that as described, this is a two-pass algorithm.
The ﬁrst pass collects the frequency data and the second pass does the encoding.
This is obviously not a desirable property for a program dealing with large ﬁles.
In this section, we will consider some algorithms to solve the bin-packing problem.
These algorithms will run quickly but will not necessarily produce optimal solutions.
We will prove, however, that the solutions that are produced are not too far from optimal.
In this version, each item must be placed in a bin before the next item can be processed.
In an off-line algorithm, we do not need to do anything until all the input has been read.
The distinction between online and off-line algorithms was discussed in Section 8.2
Online Algorithms The ﬁrst issue to consider is whether or not an online algorithm can actually always give an optimal answer, even if it is allowed unlimited computation.
What the argument above shows is that an online algorithm never knows when the input might end, so any performance guarantees it provides must hold at every instant throughout the algorithm.
If we follow the foregoing strategy, we can prove the following.
There are inputs that force any online bin-packing algorithm to use at least 43 the optimal number of bins.
Suppose otherwise, and suppose for simplicity that M is even.
Consider any online algorithm A running on the input sequence I1, above.
Recall that this sequence consists of M small items followed by M large items.
Let us consider what the algorithm A has done after processing the Mth item.
At this point in the algorithm, the optimal number of bins is M/2, because we can place two elements in each bin.
Now consider the performance of algorithm A after all items have been packed.
All bins created after the bth bin must contain exactly one item, since all small items are placed in the ﬁrst b bins, and two large items will not ﬁt in a bin.
There are three simple algorithms that guarantee that the number of bins used is no more than twice optimal.
There are also quite a few more complicated algorithms with better guarantees.
When processing any item, we check to see whether it ﬁts in the same bin as the last item.
If it does, it is placed there; otherwise, a new bin is created.
This algorithm is incredibly simple to implement and runs in linear time.
Not only is next ﬁt simple to program, its worst-case behavior is also easy to analyze.
If we apply this result to all pairs of adjacent bins, we see that at most half of the space is wasted.
Thus next ﬁt uses at most twice the optimal number of bins.
Thus, next ﬁt can be forced to use almost twice as many bins as optimal.
First Fit Although next ﬁt has a reasonable performance guarantee, it performs poorly in practice, because it creates new bins when it does not need to.
The ﬁrst ﬁt strategy is to scan the bins in order and place the new item in the ﬁrst bin that is large enough to hold it.
Thus, a new bin is created only when the results of previous placements have left no other alternative.
Figure 10.24 shows the packing that results from ﬁrst ﬁt on our standard input.
A moment’s thought will convince you that at any point, at most one bin can be more than half empty, since if a second bin were also half empty, its contents would ﬁt into the ﬁrst bin.
Thus, we can immediately conclude that ﬁrst ﬁt guarantees a solution with at most twice the optimal number of bins.
On the other hand, the bad case that we used in the proof of next ﬁt’s performance bound does not apply for ﬁrst ﬁt.
Thus, one might wonder if a better bound can be proven.
Best Fit The third online strategy we will examine is best ﬁt.
Instead of placing a new item in the ﬁrst spot that is found, it is placed in the tightest spot among all bins.
One might expect that since we are now making a more educated choice of bins, the performance guarantee would improve.
This is not the case, because the generic bad cases are the same.
Best ﬁt is never more than roughly 1.7 times as bad as optimal, and there are inputs for which it (nearly) achieves this bound.
Nevertheless, best ﬁt is also simple to code, especially if an O(N logN) algorithm is required, and it does perform better for random inputs.
Off-line Algorithms If we are allowed to view the entire item list before producing an answer, then we should expect to do better.
Indeed, since we can eventually ﬁnd the optimal packing by exhaustive search, we already have a theoretical improvement over the online case.
The major problem with all the online algorithms is that it is hard to pack the large items, especially when they occur late in the input.
The natural way around this is to sort the items, placing the largest items ﬁrst.
We can then apply ﬁrst ﬁt or best ﬁt, yielding the algorithms ﬁrst ﬁt decreasing and best ﬁt decreasing, respectively.
Figure 10.27 shows that in our case this yields an optimal solution (although, of course, this is not true in general)
In this section, we will deal with ﬁrst ﬁt decreasing.
Since it is possible that the item sizes are not distinct, some authors.
We will also assume, without loss of generality, that input sizes are already sorted.
To prove the lemma we will show that there is no way to place all the items in M bins, which contradicts the premise of the lemma.
It is possible to prove a much tighter bound for both ﬁrst ﬁt decreasing and next ﬁt decreasing.
Another common technique used to design algorithms is divide and conquer.
Divide: Smaller problems are solved recursively (except, of course, base cases)
Conquer: The solution to the original problem is then formed from the solutions to the subproblems.
Traditionally, routines in which the text contains at least two recursive calls are called divide-and-conquer algorithms, while routines whose text contains only one recursive call are not.
We generally insist that the subproblems be disjoint (that is, essentially nonoverlapping)
Let us review some of the recursive algorithms that have been covered in this text.
In Section 2.4.3, we saw an O(N logN) solution to the maximum subsequence sum problem.
In Chapter 7, we saw the classic examples of divide and conquer, namely mergesort and quicksort, which have O(N logN) worst-case and averagecase bounds, respectively.
We have also seen several examples of recursive algorithms that probably do not classify as divide and conquer, but merely reduce to a single simpler case.
In Section 1.3, we saw a simple routine to print a number.
In Chapter 2, we used recursion to perform efﬁcient exponentiation.
In Chapter 4, we examined simple search routines for binary search trees.
In Section 6.6, we saw simple recursion used to merge leftist heaps.
In Section 7.7, an algorithm was given for selection that takes linear average time.
Chapter 9 showed routines to recover the shortest path in Dijkstra’s algorithm and other procedures to perform depth-ﬁrst search in graphs.
None of these algorithms are really divide-and-conquer algorithms, because only one recursive call is performed.
We have also seen, in Section 2.4, a very bad recursive routine to compute the Fibonacci numbers.
This could be called a divide-and-conquer algorithm, but it is terribly inefﬁcient, because the problem really is not divided at all.
All the efﬁcient divide-and-conquer algorithms we will see divide the problems into subproblems, each of which is some fraction of the original problem, and then perform some additional work to compute the ﬁnal answer.
As an example, we have seen that mergesort operates on two problems, each of which is half the size of the original, and then uses O(N) additional work.
This yields the running time equation (with appropriate initial conditions)
The following theorem can be used to determine the running time of most divide-and-conquer algorithms.
If we divide through by am, we obtain the equation.
We can apply this equation for other values of m, obtaining.
Virtually all the terms on the left cancel the leading terms on the right, yielding.
Since the sum of inﬁnite series would converge to a constant, this ﬁnite sum is also bounded by a constant, and thus Equation (10.10) applies:
There are two important cases that are not covered by Theorem 10.6
We state two more theorems, leaving the proofs as exercises.
Let us assume that the points have been sorted by x coordinate.
At worst, this adds O(N logN) to the ﬁnal time bound.
Since we will show an O(N logN) bound for the entire algorithm, this sort is essentially free, from a complexity standpoint.
Since the points are sorted by x coordinate, we can draw an imaginary vertical line that partitions the point set into two halves, PL and PR.
Now we have almost exactly the same situation as we saw in the maximum subsequence sum problem in Section 2.4.3
Either the closest points are both in PL, or they are both in PR, or one is in PL and the other is in PR.
Figure 10.30 shows the partition of the point set and these three distances.
Figure 10.30 P partitioned into PL and PR; shortest distances are shown.
Since we would like an O(N logN) solution, we must be able to compute dC with only O(N) additional work.
We have already seen that if a procedure consists of two half-sized recursive calls and O(N) additional work, then the total time will be O(N logN)
Figure 10.31 Two-lane strip, containing all points considered for dC strip.
There are two strategies that can be tried to compute dC.
For large point sets that are uniformly distributed, the number of points that are expected to be in the strip is very small.
Points are all in the strip and sorted by y-coordinate.
Figure 10.35 At most eight points ﬁt in the rectangle; there are two coordinates shared by two points each.
The problem is that we have assumed that a list of points sorted by y coordinate is available.
If we perform this sort for each recursive call, then we have O(N logN) extra work:
This is not all that bad, especially when compared to the brute force O(N2)
However, it is not hard to reduce the work for each recursive call to O(N), thus ensuring an O(N logN) algorithm.
One is the point list sorted by x coordinate, and the other is the point list sorted by y coordinate.
These can be obtained by a preprocessing sorting step at cost O(N logN) and thus does not affect the time bound.
We have already seen that P is easily split in the middle.
Once the dividing line is known, we step through Q sequentially, placing each element in QL or QR as appropriate.
It is easy to see that QL and QR will be automatically sorted by y coordinate.
When the recursive calls return, we scan through the Q list and discard all the points whose x coordinates are not within the strip.
Then Q contains only points in the strip, and these points are guaranteed to be sorted by their y coordinates.
This strategy ensures that the entire algorithm is O(N logN), because only O(N) extra work is performed.
The solution in Chapter 7 uses a variation of quicksort and runs in O(N) average time.
Indeed, it is described in Hoare’s original paper on quicksort.
Although this algorithm runs in linear average time, it has a worst case of O(N2)
Selection can easily be solved in O(N logN) worst-case time by sorting the elements, but for a long time it was unknown whether or not selection could be accomplished in O(N) worst-case time.
The quickselect algorithm outlined in Section 7.7.6 is quite efﬁcient in practice, so this was mostly a question of theoretical interest.
In order to obtain a linear algorithm, we must ensure that the subproblem is only a fraction of the original and not merely only a few elements smaller than the original.
Of course, we can always ﬁnd such an element if we are willing to spend some time to do so.
The difﬁcult problem is that we cannot spend too much time ﬁnding the pivot.
For quicksort, we saw that a good choice for pivot was to pick three elements and use their median.
This gives some expectation that the pivot is not too bad but does not.
Indeed, we will see that we can use it to improve the expected number of comparisons that quickselect makes.
To get a good worst case, however, the key idea is to use one more level of indirection.
Instead of ﬁnding the median from a sample of random elements, we will ﬁnd the median from a sample of medians.
We will also show that the pivot can be computed quickly enough to guarantee an O(N) running time for the entire selection algorithm.
Let us assume for the moment that N is divisible by 5, so there are no extra elements.
Suppose also that N/5 is odd, so that the set M contains an odd number of elements.
We will also assume that all the elements are distinct.
The actual algorithm must make sure to handle the case where this is not true.
In Figure 10.36, v represents the element which is selected by the algorithm as pivot.
Since v is the median of nine elements, and we are assuming that all elements are distinct, there must be four medians that are larger than v and four that are smaller.
Consider a group of ﬁve elements with a large median (type L)
The median of the group is smaller than two elements in the group and larger than two elements in the group.
These are elements that are known to be larger than a large median.
Similarly, T represents the tiny elements, which are smaller than a small median.
There are 10 elements of type H: Two are in each of the groups with an L type median, and two elements are in the same group as v.
In this case, there are k elements of type L and k elements of type S.
There are still some details that need to be ﬁlled in if an actual implementation is desired.
For instance, duplicates must be handled correctly, and the algorithm needs a cutoff large enough to ensure that the recursive calls make progress.
There is quite a large amount of overhead involved, and this algorithm is not practical at all, so we will not describe any more of the details that need to be considered.
Even so, from a theoretical standpoint, the algorithm is a major breakthrough, because, as the following theorem shows, the running time is linear in the worst case.
This analysis shows that ﬁnding the median requires about 1.5N comparisons on average.
Of course, this algorithm requires some ﬂoating-point arithmetic to compute s, which can slow down the algorithm on some machines.
In this section we describe a divide-and-conquer algorithm that multiplies two N-digit numbers.
Our previous model of computation assumed that multiplication was done in constant time, because the numbers were small.
If we measure multiplication in terms of the size of numbers being multiplied, then the natural multiplication algorithm takes quadratic time.
We also present the classic divide-and-conquer algorithm that multiplies two N by N matrices in subcubic time.
Let us break X and Y into two halves, consisting of the most signiﬁcant and least signiﬁcant digits, respectively.
This and the subsequent additions add only O(N) additional work.
If we perform these four multiplications recursively using this algorithm, stopping at an appropriate base case, then we obtain the recurrence.
To achieve a subquadratic algorithm, we must use less than four recursive calls.
It is easy to see that now the recurrence equation satisﬁes.
To complete the algorithm, we must have a base case, which can be solved without recursion.
When both numbers are one-digit, we can do the multiplication by table lookup.
If one number has zero digits, then we return zero.
In practice, if we were to use this algorithm, we would choose the base case to be that which is most convenient for the machine.
Although this algorithm has better asymptotic performance than the standard quadratic algorithm, it is rarely used, because for small N the overhead is signiﬁcant, and for larger N there are even better algorithms.
These algorithms also make extensive use of divide and conquer.
If the matrix multiplications are done recursively, then the running time satisﬁes.
Strassen used a strategy similar to the integer multiplication divide-and-conquer algorithm and showed how to use only seven recursive calls by carefully arranging the computations.
Once the multiplications are performed, the ﬁnal answer can be obtained with eight more additions.
It is straightforward to verify that this tricky ordering produces the desired values.
As usual, there are details to consider, such as the case when N is not a power of 2, but these are basically minor nuisances.
Strassen’s algorithm is worse than the straightforward algorithm until N is fairly large.
It does not generalize for the case where the matrices are sparse (contain many zero entries), and it does not easily parallelize.
When run with ﬂoating-point entries, it is less stable numerically than the classic algorithm.
Nevertheless, it represents an important theoretical milestone and certainly shows that in computer science, as in many other ﬁelds, even though a problem seems to have an intrinsic complexity, nothing is certain until proven.
Any recursive mathematical formula could be directly translated to a recursive algorithm, but the underlying reality is that often the compiler will not do justice to the recursive algorithm, and an inefﬁcient program results.
When we suspect that this is likely to be the case, we must provide a little more help to the compiler, by rewriting the recursive algorithm as a nonrecursive algorithm that systematically records the answers to the subproblems in a table.
One technique that makes use of this approach is known as dynamic programming.
Figure 10.42 Trace of the recursive calculation of Fibonacci numbers.
We could then write the simple program in Figure 10.43 to evaluate the recursion.
In the case of four matrices, it is simple to solve the problem by exhaustive search, since there are only ﬁve ways to order the multiplications.
The calculations show that the best ordering uses roughly one-ninth the number of multiplications as the worst ordering.
Thus, it might be worthwhile to perform a few calculations to determine the optimal ordering.
Unfortunately, none of the obvious greedy strategies seems to work.
Whenever we make a change to MLeft,Right, we record the value of i that is responsible.
Although the emphasis of this chapter is not coding, it is worth noting that many programmers tend to shorten variable names to a single letter.
However, it is generally best to avoid l as a variable name, because l looks too much like 1 and can make for very difﬁcult debugging if you make a transcription error.
The minimum number of multiplications is left in m[ 1 ][ n ]
Figure 10.46 Program to ﬁnd optimal ordering of matrix multiplications.
Returning to the algorithmic issues, this program contains a triply nested loop and is easily seen to run in O(N3) time.
The references describe a faster algorithm, but since the time to perform the actual matrix multiplication is still likely to be much larger than the time to compute the optimal ordering, this algorithm is still quite practical.
The problem is to arrange these words in a binary search tree in a way that minimizes the expected total access time.
In a binary search tree, the number of comparisons needed to access an element at depth d is d + 1, so if wi is placed at depth di, then we want to minimize.
As an example, Figure 10.47 shows seven words along with their probability of occurrence in some context.
The word with the highest probability of being accessed was placed at the root.
Neither of these trees is optimal, as demonstrated by the existence of the third tree.
From this we can see that neither of the obvious solutions works.
This is initially surprising, since the problem appears to be very similar to the construction of a Huffman encoding tree, which, as we have already seen, can be solved by a greedy algorithm.
Construction of an optimal binary search tree is harder, because the data are not constrained to appear only at the leaves, and also because the tree must satisfy the binary search tree property.
Figure 10.47 Sample input for optimal binary search tree problem.
Figure 10.48 Three possible binary search trees for data in previous table.
From this equation, it is straightforward to write a program to compute the cost of the optimal binary search tree.
As usual, the actual search tree can be maintained by saving the value of i that minimizes CLeft,Right.
The standard recursive routine can be used to print the actual tree.
Figure 10.51 shows the table that will be produced by the algorithm.
For each subrange of words, the cost and root of the optimal binary search tree are maintained.
The bottommost entry computes the optimal binary search tree for the entire set of words in the input.
The optimal tree is the third tree shown in Figure 10.48
The precise computation for the optimal binary search tree for a particular subrange, namely, am..if, is shown in Figure 10.52
It is obtained by computing the minimum-cost tree obtained by placing am, and, egg, and if at the root.
Figure 10.51 Computation of the optimal binary search tree for sample input.
The running time of this algorithm is O(N3), because when it is implemented, we obtain a triple loop.
An O(N2) algorithm for the problem is sketched in the exercises.
Our third and ﬁnal dynamic programming application is an algorithm to compute shortest weighted paths between every pair of points in a directed graph G = (V, E)
In Chapter 9, we saw an algorithm for the single-source shortest-path problem, which ﬁnds the shortest path from some arbitrary vertex s to all others.
That algorithm (Dijkstra’s) runs in O(|V|2) time on dense graphs, but substantially faster on sparse graphs.
We will give a short algorithm to solve the all-pairs problem for dense graphs.
The algorithm also performs correctly if there are negative edge costs, but no negative-cost cycles; Dijkstra’s algorithm fails in this case.
Dijkstra’s algorithm provides the idea for the dynamic programming algorithm: we select the vertices in sequential order.
We will deﬁne Dk,i,j to be the weight of the shortest.
On a complete graph, where every pair of vertices is connected (in both directions), this algorithm is almost certain to be faster than |V| iterations of Dijkstra’s algorithm, because the loops are so tight.
Thus, this algorithm seems to be well suited for parallel computation.
Dynamic programming is a powerful algorithm design technique, which provides a starting point for a solution.
It is essentially the divide-and-conquer paradigm of solving simpler problems ﬁrst, with the important difference being that the simpler problems are not a clear division of the original.
Because subproblems are repeatedly solved, it is important to record their solutions in a table rather than recompute them.
In some cases, the solution can be improved (although it is certainly not always obvious and frequently difﬁcult), and in other cases, the dynamic programming technique is the best approach known.
In some sense, if you have seen one dynamic programming problem, you have seen them all.
More examples of dynamic programming can be found in the exercises and references.
Suppose you are a professor who is giving weekly programming assignments.
You want to make sure that the students are doing their own programs or, at the very least, understand the code they are submitting.
One solution is to give a quiz on the day that each program is due.
On the other hand, these quizzes take time out of class, so it might only be practical to do this for roughly half of the programs.
Your problem is to decide when to give the quizzes.
Of course, if the quizzes are announced in advance, that could be interpreted as an implicit license to cheat for the 50 percent of the programs that will not get a quiz.
One could adopt the unannounced strategy of giving quizzes on alternate programs, but students would ﬁgure out the strategy before too long.
Another possibility is to give quizzes on what seem like the important programs, but this would likely lead to similar quiz patterns from semester to semester.
Student grapevines being what they are, this strategy would probably be worthless after a semester.
One method that seems to eliminate these problems is to use a coin.
A quiz is made for every program (making quizzes is not nearly as time-consuming as grading them), and at the start of class, the professor will ﬂip a coin to decide whether the quiz is to be given.
This way, it is impossible to know before class whether or not the quiz will occur, and these patterns do not repeat from semester to semester.
Thus, the students will have to expect that a quiz will occur with 50 percent probability, regardless of previous quiz patterns.
The disadvantage is that it is possible that there is no quiz for an entire semester.
This is not a likely occurrence, unless the coin is suspect.
Each semester, the expected number of quizzes is half the number of programs, and with high probability, the number of quizzes will not deviate much from this.
At least once during the algorithm, a random number is used to make a decision.
The running time of the algorithm depends not only on the particular input, but also on the random numbers that occur.
The worst-case running time of a randomized algorithm is often the same as the worstcase running time of the nonrandomized algorithm.
The important difference is that a good randomized algorithm has no bad inputs, but only bad random numbers (relative to the particular input)
This may seem like only a philosophical difference, but actually it is quite important, as the following example shows.
Throughout the text, in our calculations of running times, we have assumed that all inputs are equally likely.
This is not true, because nearly sorted input, for instance, occurs much more often than is statistically expected, and this causes problems, particularly for quicksort and binary search trees.
By using a randomized algorithm, the particular input is no longer important.
The random numbers are important, and we can get an expected running time, where we now average over all possible random numbers instead of over all possible inputs.
Using quicksort with a random pivot gives an O(N logN)-expectedtime algorithm.
This means that for any input, including already-sorted input, the running time is expected to be O(N logN), based on the statistics of random numbers.
An expected running-time bound is somewhat stronger than an average-case bound but, of course, is weaker than the corresponding worst-case bound.
On the other hand, as we saw in the selection problem, solutions that obtain the worst-case bound are frequently not as practical as their average-case counterparts.
In this section we will examine two additional uses of randomization.
First, we will see a novel scheme for supporting the binary search tree operations in O(logN) expected time.
Once again, this means that there are no bad inputs, just bad random numbers.
From a theoretical point of view, this is not terribly exciting, since balanced.
Nevertheless, the use of randomization leads to relatively simple algorithms for searching, inserting, and especially deleting.
Our second application is a randomized algorithm to test the primality of large numbers.
The algorithm we present runs quickly but occasionally makes an error.
The probability of error can, however, be made negligibly small.
Since our algorithms require random numbers, we must have a method to generate them.
Actually, true randomness is virtually impossible to do on a computer, since these numbers will depend on the algorithm and thus cannot possibly be random.
Generally, it sufﬁces to produce pseudorandom numbers, which are numbers that appear to be random.
Random numbers have many known statistical properties; pseudorandom numbers satisfy most of these properties.
One way to do this is to examine the system clock.
The clock might record time as an integer that counts the number of seconds since some starting time.
The problem is that this does not work well if a sequence of random numbers is needed.
One second is a long time, and the clock might not change at all while the program is running.
Even if the time was recorded in units of microseconds, if the program was running by itself the sequence of numbers that would be generated would be far from random, since the time between calls to the generator would be essentially identical on every program invocation.
We see, then, that what is really needed is a sequence of random numbers.2 These numbers should appear independent.
If a coin is ﬂipped and heads appears, the next coin ﬂip should still be equally likely to come up heads or tails.
We will use random in place of pseudorandom in the rest of this section.
Generally, a class variable is used to hold the current value in the sequence of x’s.
When the program seems to work, either the system clock can be used or the user can be asked to input a value for the seed.
The problem with this class is that the multiplication could overﬂow; although this is not an error, it affects the result and thus the pseudorandomness.
Even though we could use 64-bit longs, this would slow down the computation.
Schrage gave a procedure in which all the calculations can be done on a 32-bit machine without overﬂow.
We compute the quotient and remainder of M/A and deﬁne these as Q and R, respectively.
One might be tempted to assume that all machines have a random number generator at least as good as the one in Figure 10.55 in their standard library.
Unfortunately, these generators always produce values of xi that alternate between even and odd—hardly a desirable property.
Indeed, the lower k bits cycle with period 2k (at best)
Many other random number generators have much smaller cycles than the one provided in Figure 10.55
These are not suitable for the case where long sequences of random numbers are needed.
The Java library and the UNIX drand48 function use a generator of this form.
However, they use a 48-bit linear congruential generator and return only the.
It is somewhat slower than the 31-bit random number generator, but not much so, and yields a signiﬁcantly longer period.
Figure 10.56 shows a respectable implementation of this random number generator.
Lines 7–10 show the basic constants of the random number generator.
Because M is a power of 2, we can use bitwise operators.
The next routine returns a speciﬁed number (at most 32) of random bits from the computed state, using the high-order bits which are more random than the lower bits.
However, linear congruential generators are unsuitable for some applications, such as cryptography or in simulations that require large numbers of highly independent and uncorrelated random numbers.
Our ﬁrst use of randomization is a data structure that supports both searching and insertion in O(logN) expected time.
As mentioned in the introduction to this section, this means that the running time for each operation on any input sequence has expected value O(logN), where the expectation is based on the random number generator.
It is possible to add deletion and all the operations that involve ordering and obtain expected time bounds that match the average time bounds of binary search trees.
The simplest possible data structure to support searching is the linked list.
The time to perform a search is proportional to the number of nodes that have to be examined, which is at most N.
Figure 10.58 Linked list with links to two cells ahead.
Figure 10.59 Linked list with links to four cells ahead.
Each of these steps consumes at most O(logN) total time during a search.
Notice that the search in this data structure is essentially a binary search.
When it comes time to insert a new element, we allocate a new node for it.
We must at this point decide what level the node should be.
We choose the level of the node randomly, in accordance with this probability distribution.
The easiest way to do this is to ﬂip a coin until a head occurs and use the total number of ﬂips as the node level.
Given this, the skip list algorithms are simple to describe.
To perform a search, we start at the highest link at the header.
We traverse along this level until we ﬁnd that the next node is larger than the one we are looking for (or null)
When this occurs, we go to the next lower level and continue the strategy.
When progress is stopped at level 1, either we are in front of the node we are looking for, or it is not in the list.
To perform an insert, we proceed as in a search, and keep track of each point where we switch to a lower level.
The new node, whose level is determined randomly, is then spliced into the list.
A cursory analysis shows that since the expected number of nodes at each level is unchanged from the original (nonrandomized) algorithm, the total amount of work that is expected to be performed traversing to nodes on the same level is unchanged.
This tells us that these operations have O(logN) expected costs.
Of course, a more formal proof is required, but it is not much different from this.
Skip lists are similar to hash tables, in that they require an estimate of the number of elements that will be in the list (so that the number of levels can be determined)
If an estimate is not available, we can assume a large number or use a technique similar to rehashing.
Experiments have shown that skip lists are as efﬁcient as many balanced search tree implementations and are certainly much simpler to implement in many languages.
Skip lists also have efﬁcient concurrent implementations, unlike balanced binary search trees.
In this section we examine the problem of determining whether or not a large number is prime.
In order to implement this scheme, we need a method of generating these two primes.
If d is the number of digits in N, the obvious method of testing for the divisibility by odd numbers from 3 to.
In this section, we will give a polynomial-time algorithm that can test for primality.
If the algorithm declares that the number is not prime, we can be certain that the number is not prime.
If the algorithm declares that the number is prime, then, with high probability but not 100 percent certainty, the number is prime.
The error probability does not depend on the particular number that is being tested but instead depends on random choices made by the algorithm.
Thus, this algorithm occasionally makes a mistake, but we will see that the error ratio can be made arbitrarily negligible.
The key to the algorithm is a well-known theorem due to Fermat.
In Chapter 7, we proved a theorem related to quadratic probing.
Randomized algorithms for primality testing are important because they have long been signiﬁcantly faster than the best nonrandomized algorithms, and although the randomized algorithm can occasionally produce a false positive, the chances of this happening can be made small enough to be negligible.
For many years, it was suspected that it was possible to test deﬁnitively the primality of a d-digit number in time polynomial in d, but no such algorithm was known.
Recently, however, deterministic polynomial time algorithms for primality testing have been discovered.
While these algorithms are tremendously exciting theoretical results, they are not yet competitive with the randomized algorithms.
The last algorithm design technique we will examine is backtracking.
In many cases, a backtracking algorithm amounts to a clever implementation of exhaustive search, with generally unfavorable performance.
This is not always the case, however, and even so, in some cases, the savings over a brute-force exhaustive search can be signiﬁcant.
A practical example of a backtracking algorithm is the problem of arranging furniture in a new house.
There are many possibilities to try, but typically only a few are actually considered.
Starting with no arrangement, each piece of furniture is placed in some part of the room.
If all the furniture is placed and the owner is happy, then the algorithm terminates.
If we reach a point where all subsequent placement of furniture is undesirable, we have to undo the last step and try an alternative.
Of course, this might force another undo, and so forth.
If we ﬁnd that we undo all possible ﬁrst steps, then there is no placement of furniture that is satisfactory.
Notice that although this algorithm is essentially brute force, it does not try all possibilities directly.
For instance, arrangements that consider placing the sofa in the kitchen are never tried.
Many other bad arrangements are discarded early, because an undesirable subset of the arrangement is detected.
The elimination of a large group of possibilities in one step is known as pruning.
Our second example shows how computers select moves in games, such as chess and checkers.
Of course, given one solution to the problem, an inﬁnite number of others can be constructed by adding an offset to all the points.
This is why we insist that the ﬁrst point is anchored at 0 and that the point set that constitutes a solution is output in nondecreasing order.
If this also fails, we give up and report no solution.
Figure 10.64 shows a decision tree representing the actions taken to arrive at the solution.
Instead of labeling the branches, we have placed the labels in the branches’ destination nodes.
A node with an asterisk indicates that the points chosen are inconsistent with the given distances; nodes with two asterisks have only impossible nodes as children, and thus represent an incorrect path.
Figure 10.64 Decision tree for the worked turnpike reconstruction example.
We have used one-letter variable names, which is generally poor style, for consistency with the worked example.
We also, for simplicity, do not give the type of variables.
We can maintain D as a balanced binary search (or splay) tree (this would require a code modiﬁcation, of course)
This claim is obvious for deletions, since D has O(N2) elements and no element is ever reinserted.
Thus, if there is no backtracking, the running time is O(N2 logN)
As our last application, we will consider the strategy that a computer might use to play a strategic game, such as checkers or chess.
We will use, as an example, the much simpler game of tic-tac-toe, because it makes the points easier to illustrate.
By performing a careful case-by-case analysis, it is not a difﬁcult matter to construct an algorithm that never loses and always wins when presented the opportunity.
This can be done, because certain positions are known traps and can be handled by a lookup table.
Other strategies, such as taking the center square when it is available, make the analysis simpler.
If this is done, then by using a table we can always choose a move based only on the current position.
Of course, this strategy requires the programmer, and not the computer, to do most of the thinking.
If a position is not terminal, the value of the position is determined by recursively assuming optimal play by both sides.
This is known as a minimax strategy, because one player (the human) is trying to minimize the value of the position, while the other player (the computer) is trying to maximize it.
A successor position of P is any position Ps that is reachable from P by playing one move.
If the computer is to move when in some position P, it recursively evaluates the value of all the successor positions.
The computer chooses the move with the largest value; this is the value of P.
To evaluate any successor position Ps, all of Ps’s successors are recursively evaluated, and the smallest value is chosen.
This smallest value represents the most favorable reply for the human player.
The code in Figure 10.67 makes the computer’s strategy more clear.
If neither of these cases apply, then the position is nonterminal.
This is recursive, because, as we will see, findHumanMove calls findCompMove.
If the human’s response to a move leaves the computer with a more favorable position than that obtained with the previously best computer move, then the value and bestMove are updated.
Figure 10.68 shows the method for the human’s move selection.
The logic is virtually identical, except that the human player chooses the move that leads to the lowest-valued position.
Indeed, it is not difﬁcult to combine these two procedures into one by passing an extra variable, which indicates whose turn it is to move.
This does make the code somewhat less readable, so we have stayed with separate routines.
Since these routines must pass back both the value of the position and the best move, we pass these two variables in a MoveInfo object.
The most costly computation is the case where the computer is asked to pick the opening move.
For more complex games, such as checkers and chess, it is obviously infeasible to search all the way to the terminal nodes.6 In this case, we have to stop the search after a certain depth of recursion is reached.
The nodes where the recursion is stopped become terminal nodes.
These terminal nodes are evaluated with a function that estimates the.
We numbered the squares starting from the top left and moving right.
It is estimated that if this search were conducted for chess, at least 10100 positions would be examined for the ﬁrst move.
Even if the improvements described later in this section were incorporated, this number could not be reduced to a practical level.
For instance, in a chess program, the evaluation function measures such variables as the relative amount and strength of pieces and positional factors.
The evaluation function is crucial for success, because the computer’s move selection is based on maximizing this function.
The best computer chess programs have surprisingly sophisticated evaluation functions.
Nevertheless, for computer chess, the single most important factor seems to be number of moves of look-ahead the program is capable of.
To implement this, an extra parameter is given to the search routines.
The basic method to increase the look-ahead factor in game programs is to come up with methods that evaluate fewer nodes without losing any information.
One method which we have already seen is to use a table to keep track of all positions that have been evaluated.
For instance, in the course of searching for the ﬁrst move, the program will examine the positions in Figure 10.69
If the values of the positions are saved, the second occurrence of a position need not be recomputed; it essentially becomes a terminal position.
The data structure that records this is known as a transposition table; it is almost always implemented by hashing.
For instance, in a chess endgame, where there are relatively few pieces, the time savings can allow a search to go several levels deeper.
Figure 10.71 shows the evaluation of the same game tree, with several (but not all possible) unevaluated nodes.
Almost half of the terminal nodes have not been checked.
We show that evaluating them would not change the value at the root.
Figure 10.72 shows the information that has been gathered when it is time to evaluate D.
At this point, we are still in findHumanMove and are contemplating a call to findCompMove on D.
However, we already know that findHumanMove will return at most 40, since it is a min node.
In many games, computers are among the best players in the world.
The techniques used are very interesting and can be applied to more serious problems.
This chapter illustrates ﬁve of the most common techniques found in algorithm design.
When confronted with a problem, it is worthwhile to see if any of these methods apply.
A proper choice of algorithm, combined with judicious use of data structures, can often lead quickly to efﬁcient solutions.
Each job ji earns di dollars if it is completed by the time limit ti, but no money if completed after the time limit.
Give a method for constructing the header of size at most O(N) (in addition to the symbols), where N is the number of symbols.
Show how the number of comparisons can be reduced by more careful use of the information.
Compute the multiplication between the two matrices Mi and Mi+1, such that.
Write a method and give an analysis of the running time to compute the binomial coefﬁcients as follows: a.
What is the expected performance of the skip list algorithms if the random number generator uses a modulus of the form M = 2B (which is unfortunately prevalent on many systems)?
Give an O(N logN) algorithm to ﬁnd one such arrangement.
Your algorithm may serve as a proof for part (a)
A Voronoi diagram is a partition of the plane into N regions Ri such that all points in Ri are closer to pi than any other point in P.
Figure 10.78 shows a sample Voronoi diagram for seven (nicely arranged) points.
Give an O(N logN) algorithm to construct the Voronoi diagram.
Give an O(N logN) algorithm to ﬁnd the convex hull.
Give the time and space complexities for your algorithm (as a function of the number of words, N)
Consider the special case where we are using a ﬁxed-width font, and assume the optimal value of b is 1 (space)
Give a linear-time algorithm to generate the least ugly setting for this case.
Give an algorithm to solve the longest common subsequence problem.
A character can be in S that is not in P.
A character can be in P that is not in S.
As an example, if we are searching for the pattern “textbook” with at most three mismatches in the string “data structures txtborpk”, we ﬁnd a match (insert an e, change an r to an o, delete a p)
Is there a subset of A whose sum is exactly K? a.
Give an algorithm that solves the knapsack problem in O(NK) time.
Give an algorithm that computes the minimum number of coins required to.
Give an algorithm that computes the number of different ways to give K cents.
Two queens are said to attack each other if they are on the same row, column, or (not necessarily main) diagonal.
Give a randomized algorithm to place eight nonattacking queens on the board.
Why does this algorithm not work for general graphs? b.
A submatrix S of A is any group of contiguous entries that forms a square.
Design an O(N2) algorithm that determines the size of the largest submatrix of.
For instance, in the matrix that follows, the largest submatrix is a 4-by-4 square.
Some early chess programs were problematic in that they would get into a repetition of position when a forced win was detected, thereby allowing the opponent to claim a draw.
In tic-tac-toe, this is not a problem, because the program eventually will win.
Modify the tic-tactoe algorithm so that when a winning position is found, the move that leads to the shortest win is always taken.
The object is to ﬁnd words in the grid that are subject to the constraint that two adjacent letters must be adjacent in the grid, and each item in the grid can be used, at most, once per word.
The board is represented as an N-by-N grid of numbers randomly placed at the start of the game.
At each turn, a player must select a grid element in the current row or column.
The value of the selected position is added to the player’s score, and that position becomes the current position and cannot be selected again.
Players alternate until all grid elements in the current row and column are already selected, at which point the game ends and the player with the higher score wins.
What is the ﬁnal score if play on both sides is optimal?
Here the codes have a ﬁxed length but represent strings instead of characters.
The analysis of bin-packing heuristics ﬁrst appeared in Johnson’s Ph.D.
It was shown in [24] that the problem can be solved in O(N logN) time.
In particular, we will consider the worst-case running time for any sequence of M operations.
This contrasts with the more typical analysis, in which a worst-case bound is given for any single operation.
Amortized bounds are weaker than the corresponding worst-case bounds, because there is no guarantee for any single operation.
Since this is generally not important, we are willing to sacriﬁce the bound on a single operation, if we can retain the same bound for the sequence of operations and at the same time simplify the data structure.
For instance, binary search trees have O(logN) average time per operation, but it is still possible for a sequence of M operations to take O(MN) time.
Because deriving an amortized bound requires us to look at an entire sequence of operations instead of just one, we expect that the analysis will be more tricky.
Consider the following puzzle: Two kittens are placed on opposite ends of a football ﬁeld, 100 yards apart.
They walk toward each other at the speed of 10 yards per minute.
At the same time, their mother is at one end of the ﬁeld.
The mother runs from one kitten to the other, making turns with no loss of speed, until the kittens (and thus the mother) meet at midﬁeld.
It is not hard to solve this puzzle with a brute-force calculation.
We leave the details to you, but one expects that this calculation will involve computing the sum of an inﬁnite geometric series.
Although this straightforward calculation will lead to an answer, it turns out that a much simpler solution can be arrived at by introducing an extra variable, namely, time.
This puzzle illustrates the point that sometimes it is easier to solve a problem indirectly than directly.
The amortized analyses that we will perform will use this idea.
We will introduce an extra variable, known as the potential, to allow us to prove results that seem very difﬁcult to establish otherwise.
The rank of a node in a binomial tree is equal to the number of children; in particular, the rank of the root of Bk is k.
A binomial queue is a collection of heap-ordered binomial trees, in which there can be at most one binomial tree Bk for any k.
To merge two binomial queues, an operation similar to addition of binary integers is performed: At any stage we may have zero, one, two, or possibly three Bk trees, depending on whether or not the two priority queues contain a Bk tree and whether or not a Bk tree is carried over from the previous step.
If there is zero or one Bk tree, it is placed as a tree in the resultant binomial queue.
If there are two Bk trees, they are melded into a Bk+1 tree and carried over; if there are three Bk trees, one is placed as a tree in the binomial queue and the other two are melded and carried over.
Insertion is performed by creating a one-node binomial queue and performing a merge.
The time to do this is M+ 1, where M represents the smallest type of binomial tree BM not present in the binomial queue.
Suppose we want to build a binomial queue of N elements.
We know that building a binary heap of N elements can be done in O(N), so we expect a similar bound for binomial queues.
A binomial queue of N elements can be built by N successive insertions in O(N) time.
The claim, if true, would give an extremely simple algorithm.
Since the worst-case time for each insertion is O(logN), it is not obvious that the claim is true.
Recall that if this algorithm were applied to binary heaps, the running time would be O(N logN)
To prove the claim, we could do a direct calculation.
To measure the running time, we deﬁne the cost of each insertion to be one time unit plus an extra unit for each linking step.
Summing this cost over all insertions gives the total running time.
This total is N units plus the total number of linking steps.
We could add this all up and bound the number of linking steps by N, proving the claim.
This brute-force calculation will not help when we try to analyze a sequence of operations that include more than just insertions, so we will use another approach to prove this result.
Let Ti be the number of trees after the ith insertion.
If we add all these equations, most of the Ti terms cancel, leaving.
During the buildBinomialQueue routine, each insertion had a worst-case time of.
O(logN), but since the entire routine used at most 2N units of time, the insertions behaved as though each used no more than two units each.
The state of the data structure at any time is given by a function known as the potential.
The potential function is not maintained by the program but rather is an accounting device that will help with the analysis.
When operations take less time than we have allocated for them, the unused time is “saved” in the form of a higher potential.
In our example, the potential of the data structure is simply the number of trees.
In the analysis above, when we have insertions that use only one unit instead of the two units that are allocated, the extra unit is saved for later by an increase in potential.
When operations occur that exceed the allotted time, then the excess time is accounted for by a decrease in potential.
One may view the potential as representing a savings account.
If an operation uses less than its allotted time, the difference is saved for use later on by more expensive operations.
Figure 11.4 shows the cumulative running time used by buildBinomialQueue over a sequence of insertions.
Observe that the running time never exceeds 2N and that the potential in the binomial queue after any insertion measures the amount of savings.
Once a potential function is chosen, we write the main equation:
Tactual + Potential = Tamortized (11.2) Tactual, the actual time of an operation, represents the exact (observed) amount of time required to execute a particular operation.
In a binary search tree, for example, the actual time to perform a contains(x) is 1 plus the depth of the node containing x.
If we sum the basic equation over the entire sequence, and if the ﬁnal potential is at least as large as the initial potential, then the amortized time is an upper bound on the actual time used during the execution of the sequence.
Notice that while Tactual varies from operation to operation, Tamortized is stable.
Picking a potential function that proves a meaningful bound is a very tricky task; there is no one method that is used.
Nevertheless, the discussion above suggests a few rules, which tell us the properties that good potential functions have.
Always assume its minimum at the start of the sequence.
A popular method of choosing potential functions is to ensure that the potential function is initially 0, and always nonnegative.
All the examples that we will encounter use this strategy.
We can now perform a complete analysis of binomial queue operations.
The amortized running times of insert, deleteMin, and merge are O(1), O(logN), and O(logN), respectively, for binomial queues.
The initial potential is 0, and the potential is always nonnegative, so the amortized time is an upper bound on the actual time.
After the merge, there can be at most logN trees, so the potential can increase by at most O(logN)
Figure 11.5 The insertion cost and potential change for each operation in a sequence.
The analysis of binomial queues is a fairly easy example of an amortized analysis.
As is common with many of our examples, once the right potential function is found, the analysis is easy.
Recall that for skew heaps, the key operation is merging.
To merge two skew heaps, we merge their right paths and make this the new left path.
For each node on the new path, except the last, the old left subtree is attached as the right subtree.
The last node on the new left path is known to not have a right subtree, so it is silly to give it one.
The bound does not depend on this exception, and if the routine is coded recursively, this is what will happen naturally.
Figure 11.6 shows the result of merging two skew heaps.
What is needed is some sort of a potential function that captures the effect of skew heap operations.
Recall that the effect of a merge is that every node on the right path is moved to the left path, and its old left child becomes the new right child.
One idea might be to classify each node as a right node or left node, depending on whether or not it is a right child, and use the number of right nodes as a potential function.
The result is that this potential function cannot be used to prove the desired bound.
A similar idea is to classify nodes as either heavy or light, depending on whether or not the right subtree of any node has more nodes than the left subtree.
A node p is heavy if the number of descendants of p’s right subtree is at least half of the number of descendants of p, and light otherwise.
Note that the number of descendants of a node includes the node itself.
The potential function we will use is the number of heavy nodes in the (collection of) heaps.
This seems like a good choice, because a long right path will contain an inordinate.
Because nodes on this path have their children swapped, these nodes will be converted to light nodes as a result of the merge.
The amortized time to merge two skew heaps is O(logN)
Now the only nodes whose heavy/light status can change are nodes that are initially on the right path (and wind up on the left path), since no other nodes have their subtrees altered.
The proof is completed by noting that the initial potential is 0 and that the potential is always nonnegative.
It is important to verify this, since otherwise the amortized time does not bound the actual time and is meaningless.
Since the insert and deleteMin operations are basically just merges, they also have O(logN) amortized bounds.
In order to lower this time bound, the time required to perform the decreaseKey operation must be improved.
Fibonacci heaps1 generalize binomial queues by adding two new concepts:
A different implementation of decreaseKey: The method we have seen before is to percolate the element up toward the root.
It does not seem reasonable to expect an O(1) amortized bound for this strategy, so a new method is needed.
Lazy merging: Two heaps are merged only when it is required to do so.
For lazy merging, merges are cheap, but because lazy merging does not actually combine trees, the deleteMin operation could encounter lots of trees, making that operation expensive.
Any one deleteMin could take linear time, but it is always possible to charge the time to previous merge operations.
In particular, an expensive deleteMin must have been preceded by a large number of unduly cheap merges, which were able to store up extra potential.
In binary heaps, the decreaseKey operation is implemented by lowering the value at a node and then percolating it up toward the root until heap order is established.
The name comes from a property of this data structure, which we will prove later in the section.
We see that for leftist heaps, another strategy is needed for the decreaseKey operation.
Our example will be the leftist heap in Figure 11.10
If we make the change, we ﬁnd that we have created a violation of heap order, which is indicated by a dashed line in Figure 11.11
We do not want to percolate the 0 to the root, because, as we have seen, there are cases where this could be expensive.
The solution is to cut the heap along the dashed line, thus creating two trees, and then merge the two trees back into one.
Let X be the node to which the decreaseKey operation is being applied, and let P be its parent.
If these two trees were both leftist heaps, then they could be merged in O(logN) time, and we would be done.
It is easy to see that H1 is a leftist heap, since none of its nodes have had any changes in their descendants.
Thus, since all of its nodes originally satisﬁed the leftist property, they still must.
Nevertheless, it seems that this scheme will not work, because T2 is not necessarily leftist.
However, it is easy to reinstate the leftist heap property by using two observations:
Only nodes on the path from P to the root of T2 can be in violation of the leftist heap property; these can be ﬁxed by swapping children.
The heap that results in our example is shown in Figure 11.14
The second idea that is used by Fibonacci heaps is lazy merging.
We will apply this idea to binomial queues and show that the amortized time to perform a merge operation (as well as insertion, which is a special case) is O(1)
The idea is as follows: To merge two binomial queues, merely concatenate the two lists of binomial trees, creating a new binomial queue.
This new queue may have several trees of the same size, so it violates the binomial queue property.
We will call this a lazy binomial queue in order to maintain consistency.
This is a fast operation that always takes constant (worst-case) time.
As before, an insertion is done by creating a one-node binomial queue and merging.
The deleteMin operation is much more painful, because it is where we ﬁnally convert the lazy binomial queue back into a standard binomial queue, but, as we will show, it is still O(logN) amortized time—but not O(logN) worst-case time, as before.
To perform a deleteMin, we ﬁnd (and eventually return) the minimum element.
As before, we delete it from the queue, making each of its children new trees.
We then merge all the trees into a binomial queue by merging two equal-sized trees until it is no longer possible.
As an example, Figure 11.15 shows a lazy binomial queue.
In a lazy binomial queue, there can be more than one tree of the same size.
To perform the deleteMin, we remove the smallest element, as before, and obtain the tree in Figure 11.16
We now have to merge all the trees and obtain a standard binomial queue.
A standard binomial queue has at most one tree of each rank.
In order to do this efﬁciently, we must be able to perform the merge in time proportional to the number of trees present (T) (or logN, whichever is larger)
LRmax+1, where Rmax is the rank of the largest tree.
Each list LR contains all of the trees of rank R.
Figure 11.18 Combining the binomial trees into a binomial queue.
Amortized Analysis of Lazy Binomial Queues To carry out the amortized analysis of lazy binomial queues, we will use the same potential function that was used for standard binomial queues.
Thus, the potential of a lazy binomial queue is the number of trees.
The amortized running times of merge and insert are both O(1) for lazy binomial queues.
The potential function is the number of trees in the collection of binomial queues.
The initial potential is 0, and the potential is always nonnegative.
Thus, over a sequence of operations, the total amortized time is an upper bound on the total actual time.
Figure 11.19 shows one tree in a Fibonacci heap prior to a decreaseKey operation.
Therefore, the node is cut from its parent, becoming the root of a new tree.
We can do this because we can place the constant implied by the Big-Oh notation in the potential function and still get the cancellation of terms, which is needed in the proof.
Figure 11.20 The resulting segment of the Fibonacci heap after the decreaseKey operation.
This will be a crucial observation in our proof of the time bound.
Recall that the reason for marking nodes is that we needed to bound the rank (number of children) R of any node.
We will now show that any node with N descendants has rank O(logN)
Because it is well known that the Fibonacci numbers grow exponentially, it immediately follows that any node with s descendants has rank at most O(log s)
The rank of any node in a Fibonacci heap is O(logN)
If all we were concerned about were the time bounds for the merge, insert, and deleteMin operations, then we could stop here and prove the desired amortized time bounds.
Of course, the whole point of Fibonacci heaps is to obtain an O(1) time bound for decreaseKey as well.
The actual time required for a decreaseKey operation is 1 plus the number of cascading cuts that are performed during the operation.
Since the number of cascading cuts could be much more than O(1), we will need to pay for this with a loss in potential.
If we look at Figure 11.20, we see that the number of trees actually increases with each cascading cut, so we will have to enhance the potential function to include something that decreases during cascading cuts.
Notice that we cannot just throw out the number of trees from the potential function, since then we will not be able to prove the time bound for the merge operation.
Looking at Figure 11.20 again, we see that a cascading cut causes a decrease in the number of marked nodes, because each node that is the victim of a cascading cut becomes an unmarked root.
This way, we have a chance of canceling out the number of cascading cuts.
The amortized time bounds for Fibonacci heaps are O(1) for insert, merge, and decreaseKey and O(logN) for deleteMin.
The potential is the number of trees in the collection of Fibonacci heaps plus twice the number of marked nodes.
As usual, the initial potential is 0 and is always nonnegative.
Thus, over a sequence of operations, the total amortized time is an upper bound on the total actual time.
For the insert operation, the actual time is constant, the number of trees increases by 1, and the number of marked nodes is unchanged.
As a ﬁnal example, we analyze the running time of splay trees.
Recall, from Chapter 4, that after an access of some item X is performed, a splaying step moves X to the root by a series of three operations: zig, zig-zag, and zig-zig.
We adopt the convention that if a tree rotation is being performed at node X, then prior to the rotation P is its parent and (if X is not a child of the root) G is its grandparent.
Recall that the time required for any tree operation on node X is proportional to the number of nodes on the path from the root to X.
If we count each zig operation as one rotation and each zig-zig or zig-zag as two rotations, then the cost of any access is equal to 1 plus the number of rotations.
In order to show an O(logN) amortized bound for the splaying step, we need a potential function that can increase by at most O(logN) over the entire splaying step but that will also cancel out the number of rotations performed during the step.
It is not at all easy to ﬁnd a potential function that satisﬁes these criteria.
Figure 11.21 zig, zig-zag, and zig-zig operations; each has a symmetric case (not shown)
The potential function is the sum, over all nodes i in the tree T, of the logarithm of S(i)
The terminology is similar to what we used in the analysis of the disjoint set algorithm, binomial queues, and Fibonacci heaps.
In all these data structures, the meaning of rank is somewhat different, but the rank is generally meant to be on the order (magnitude) of the logarithm of the size of the tree.
Using the sum of ranks as a potential function is similar to using the sum of heights as a potential function.
The important difference is that while a rotation can change the heights of many nodes in the tree, only X, P, and G can have their ranks changed.
Before proving the main theorem, we need the following lemma.
With the preliminaries taken care of, we are ready to prove the main theorem.
If X is the root of T, then there are no rotations, so there is no potential change.
Thus, we may assume that there is at least one rotation.
From Figure 11.21 we see that Sf (X) = Si(G), so their ranks must be equal.
Because every operation on a splay tree requires a splay, the amortized cost of any operation is within a constant factor of the amortized cost of a splay.
Thus, all splay tree access operations take O(logN) amortized time.
To show that insertions and deletions take O(logN), amortized time, potential changes that occur either prior to or after the splaying step should be accounted for.
A deletion consists of a nonsplaying step that attaches one tree to another.
This does increase the rank of one node, but that is limited by log N (and is compensated by the removal of a node, which at the time was a root)
Thus the splaying costs accurately bound the cost of a deletion.
By using a more general potential function, it is possible to show that splay trees have several remarkable properties.
In this chapter, we have seen how an amortized analysis can be used to apportion charges among operations.
To perform the analysis, we invent a ﬁctitious potential function.
A high-potential data structure is volatile, having been built on relatively cheap operations.
When the expensive bill comes for an operation, it is paid for by the savings of previous operations.
One can view potential as standing for potential for disaster, in that very expensive operations can occur only when the data structure has a high potential and has used considerably less time than has been allocated.
Low potential in a data structure means that the cost of each operation has been roughly equal to the amount allocated for it.
Negative potential means debt; more time has been spent than has been allocated, so the allocated (or amortized) time is not a meaningful bound.
As expressed by Equation (11.2), the amortized time for an operation is equal to the sum of the actual time and potential change.
Taken over an entire sequence of operations, the amortized time for the sequence is equal to the total sequence time plus the net change in potential.
As long as this net change is positive, then the amortized bound provides an upper bound for the actual time spent and is meaningful.
The keys to choosing a potential function are to guarantee that the minimum potential occurs at the beginning of the algorithm, and to have the potential increase for cheap operations and decrease for expensive operations.
It is important that the excess or saved time be measured by an opposite change in potential.
Why does this not contradict the amortized bound of O(1) for insertion?
Show how to reduce the number of links, at the cost of at most a constant factor in the running time.
Let the weight function W(i) be some function assigned to each node in the tree, and let S(i) be the sum of the weights of all the nodes in the subtree rooted at i, including i itself.
The special case W(i) = 1 for all nodes corresponds to the function used in the proof of the splaying bound.
Let N be the number of nodes in the tree, and let M be the number of accesses.
The total access time is O(M + (M + N) logN)
Give a formal amortized analysis, with potential function, to show that the amortized cost of an insertion is still O(1)
Describe how to support these operations in constant amortized time per operation.
Deﬁne the potential of a binomial queue to be the number of trees plus the rank of the largest tree.
This suggests using the sum over all nodes of the logarithm of each node’s depth as a potential function.
What is the maximum value of the potential function? b.
What is the minimum value of the potential function? c.
The difference in the answers to parts (a) and (b) gives some indication that.
An excellent survey of amortized analysis is provided in [10]
Most of the references below duplicate citations in earlier chapters.
Exercise 11.9(a) shows that splay trees are optimal, to within a constant factor of the best static search trees.
Exercise 11.9(b) shows that splay trees are optimal, to within a constant factor of the best optimal search trees.
These, as well as two other strong results, are proved in the original splay tree paper [7]
Amortization is used in [2] to merge a balanced seach tree efﬁciently.
The merge operation for splay trees is described in [6]
Amortized analysis is used in [8] to design an online algorithm that processes a series of queries in time only a constant factor larger than any off-line algorithm in its class.
In this chapter, we discuss six data structures with an emphasis on practicality.
These include an optimized version of the splay tree, the red-black tree, and the treap.
We also examine the sufﬁx tree, which allows searching for a pattern in a large text.
We then examine a data structure that can be used for multidimensional data.
Finally, we examine the pairing heap, which seems to be the most practical alternative to the Fibonacci heap.
Nonrecursive, top-down (instead of bottom-up) search tree implementations when appropriate.
Detailed, optimized implementations that make use of, among other things, sentinel nodes.
In Chapter 4, we discussed the basic splay tree operation.
When an item X is inserted as a leaf, a series of tree rotations, known as a splay, makes X the new root of the tree.
A splay is also performed during searches, and if an item is not found, a splay is performed on the last node on the access path.
In Chapter 11, we showed that the amortized cost of a splay tree operation is O(logN)
A direct implementation of this strategy requires a traversal from the root down the tree, and then a bottom-up traversal to implement the splaying step.
This can be done either by maintaining parent links, or by storing the access path on a stack.
Unfortunately, both methods require a substantial amount of overhead, and both must handle many special cases.
In this section, we show how to perform rotations on the initial access path.
The result is a procedure that is faster in practice, uses only O(1) extra space, but retains the O(logN) amortized time bound.
Figure 12.1 shows the rotations for the zig, zig-zig, and zig-zag cases.
Tree L stores nodes in the tree T that are less than X, but not in X’s subtree; similarly tree R stores nodes in the tree T that are larger than X, but not in X’s subtree.
Initially, X is the root of T, and L and R are empty.
If the rotation should be a zig, then the tree rooted at Y becomes the new root of the middle tree.
Note carefully that Y does not have to be a leaf for the zig case to apply.
If we are searching for an item that is smaller than Y, and Y has no left child (but does have a right child), then the zig case will apply.
The crucial point is that a rotation between X and Y is performed.
The zig-zag case brings the bottom node Z to the top in the middle tree and attaches subtrees X and Y to R and L, respectively.
Note that Y is attached to, and then becomes, the largest item in L.
The zig-zag step can be simpliﬁed somewhat because no rotations are performed.
Instead of making Z the root of the middle tree, we make Y the root.
This simpliﬁes the coding because the action for the zig-zag case becomes.
For simplicity we don’t distinguish between a “node” and the item in the node.
In the code, the smallest node in R does not have a null left link because there is no need for it.
This means that printTree(r) will include some items that logically are not in R.
This would seem advantageous because testing for a host of cases is time-consuming.
The disadvantage is that by descending only one level, we have more iterations in the splaying procedure.
Once we have performed the ﬁnal splaying step, Figure 12.3 shows how L, R, and the middle tree are arranged to form a single tree.
Note carefully that the result is different from bottom-up splaying.
The crucial fact is that the O(logN) amortized bound is preserved (Exercise 12.1)
An example of the top-down splaying algorithm is shown in Figure 12.4
The search for 19 then results in a terminal zig.
The reassembly, in accordance with Figure 12.3, terminates the splay step.
We will use a header with left and right links to eventually reference the roots of the left and right trees.
Since these trees are initially empty, a header is used to correspond to the min or max node of the right or left tree, respectively, in this initial state.
This way the code can avoid checking for empty trees.
The ﬁrst time the left tree becomes nonempty, the right link will get initialized and will not change in the future; thus it will contain the root of the left tree at the end of the top-down search.
Similarly, the left link will eventually contain the root of the right tree.
The SplayTree class, whose skeleton is shown in Figure 12.5, includes a constructor that is used to allocate the nullNode sentinel.
We use the sentinel nullNode to represent logically a null reference.
We will repeatedly use this technique to simplify the code (and consequently make the code somewhat faster)
The header node allows us to be certain that we can attach X to the largest node in R without having to worry that R might be empty (and similarly for the symmetric case dealing with L)
As we mentioned above, before the reassembly at the end of the splay, header.left and header.right reference the roots of R and L, respectively (this is not a typo—follow the links)
Figure 12.7 shows the method to insert an item into a tree.
A new node is allocated (if necessary), and if the tree is empty, a one-node tree is created.
If the data in the new root is equal to x, we have a duplicate; instead of reinserting x, we preserve newNode for a future insertion and return immediately.
If the new root contains a value larger than x, then the new root and its right subtree become a right subtree of newNode, and root’s left subtree becomes the left subtree of newNode.
Similar logic applies if root’s new root contains a value smaller than x.
In Chapter 4, we showed that deletion in splay trees is easy, because a splay will place the target of the deletion at the root.
We close by showing the deletion routine in Figure 12.8
It is indeed rare that a deletion procedure is shorter than the corresponding insertion procedure.
A historically popular alternative to the AVL tree is the red-black tree.
Operations on redblack trees take O(logN) time in the worst case, and, as we will see, a careful nonrecursive implementation (for insertion) can be done relatively effortlessly (compared with AVL trees)
A red-black tree is a binary search tree with the following coloring properties:
If a node is red, its children must be black.
Every path from a node to a null reference must contain the same number of black nodes.
The difﬁculty, as usual, is inserting a new item into the tree.
The new item, as usual, is placed as a leaf in the tree.
If we color this item black, then we are certain to violate condition 4, because we will create a longer path of black nodes.
If the parent is already red, then we will violate condition 3 by having consecutive red nodes.
The basic operations that are used to do this are color changes and tree rotations.
As we have already mentioned, if the parent of the newly inserted item is black, we are done.
There are several cases (each with a mirror image symmetry) to consider if the parent is red.
First, suppose that the sibling of the parent is black (we adopt the convention that null nodes are black)
Let X be the newly added leaf, P be its parent, S be the sibling of the parent (if it exists), and G be the grandparent.
Only X and P are red in this case; G is black, because otherwise there would be two consecutive red nodes prior to the insertion, in violation of red-black rules.
Adopting the splay tree terminology, X, P, and G can form either a zig-zig chain or a zig-zag chain (in either of two directions)
Figure 12.10 shows how we can rotate the tree for the case where P is a left child (note there is a symmetric case)
Even though X is a leaf, we have drawn a more general case that allows X to be in the middle of the tree.
The ﬁrst case corresponds to a single rotation between P and G, and the second case corresponds to a double rotation, ﬁrst between X and P and then between X and G.
When we write the code, we have to keep track of the parent, the grandparent, and, for reattachment purposes, the great-grandparent.
In both cases, the subtree’s new root is colored black, and so even if the original great-grandparent was red, we removed the possibility of two consecutive red nodes.
Equally important, the number of black nodes on the paths into A, B, and C has remained unchanged as a result of the rotations.
After the rotation, there must still be only one black node.
But in both cases, there are three nodes (the new root, G, and S) on the path to C.
Since only one may be black, and since we cannot have consecutive red nodes, it follows that we’d have to color both S and the subtree’s new root red, and G (and our fourth node) black.
That’s great, but what happens if the great-grandparent is also red? In that case, we.
Figure 12.10 Zig rotation and zig-zag rotation work if S is black.
Implementing the percolation would require maintaining the path using a stack or parent links.
We saw that splay trees are more efﬁcient if we use a top-down procedure, and it turns out that we can apply a top-down procedure to red-black trees that guarantees that S won’t be red.
On the way down, when we see a node X that has two red children, we make X red and the two children black.
This will induce a red-black violation only if X’s parent P is also red.
But in that case, we can apply the appropriate rotations in Figure 12.10
What if X’s parent’s sibling is red? This possibility has been removed by our actions on the way down, and so X’s parent’s sibling can’t be red! Speciﬁcally, if on the way down the tree we see a node Y that has two red children, we know that Y ’s grandchildren must be black, and that since Y ’s children are made black too, even after the rotation that may occur, we won’t see another red node for two levels.
Thus when we see X, if X’s parent is red, it is not possible for X’s parent’s sibling to be red also.
On the way down the tree, we see node 50, which has two red children.
We then continue, performing an identical action if we see other nodes on the path that contain two red children.
When we get to the leaf, we insert 45 as a red node, and since the parent is black, we are done.
As Figure 12.12 shows, the red-black tree that results is frequently very well balanced.
Experiments suggest that the average red-black tree is about as deep as an average AVL tree and that, consequently, the searching times are typically near optimal.
The advantage of red-black trees is the relatively low overhead required to perform insertion, and the fact that in practice rotations occur relatively infrequently.
An actual implementation is complicated not only by the host of possible rotations, but also by the possibility that some subtrees (such as 10’s right subtree) might be empty, and the special case of dealing with the root (which among other things, has no parent)
Figure 12.11 Color ﬂip: Only if X’s parent is red do we continue with a rotation.
Figure 12.14 shows the RedBlackTree skeleton (omitting the methods), along with the constructors.
Next, Figure 12.15 shows the routine to perform a single rotation.
Rather than keeping track of the type of rotation as we descend the tree, we pass item as a parameter.
Since we expect very few rotations during the insertion procedure, it turns out that it is not only simpler, but actually faster, to do it this way.
The routine handleReorient is called when we encounter a node with two red children, and also when we insert a leaf.
Because the result is attached to the parent, there are four cases.
The most tricky part is the observation that a double rotation is really two single rotations, and is done only when branching to X (represented in the insert method by current) takes opposite directions.
As we mentioned in the earlier discussion, insert must keep track of the parent, grandparent, and great-grandparent as the tree is descended.
Since these are shared with handleReorient, we make these class members.
Note that after a rotation, the values stored in the grandparent and great-grandparent are no longer correct.
However, we are assured that they will be restored by the time they are next needed.
Everything boils down to being able to delete a leaf.
This is because to delete a node that has two children, we replace it with the smallest node in the right subtree; that node, which must have at most one child, is then deleted.
Nodes with only a right child can be deleted in the same manner, while nodes with only a left child can be deleted by replacement with the largest node in the left subtree, and subsequent deletion of that node.
Note that for red-black trees, we don’t want to use the strategy of bypassing for the case of a node with one child because that may connect two red nodes in the middle of the tree, making enforcement of the red-black condition difﬁcult.
The solution is to ensure during the top-down pass that the leaf is red.
Throughout this discussion, let X be the current node, T be its sibling, and P be their parent.
As we traverse down the tree, we attempt to ensure that X is red.
When we arrive at a new node, we are certain that P is red (inductively, by the invariant we are trying to maintain), and that X and T are black (because we can’t have two consecutive red nodes)
Then there are three subcases, which are shown in Figure 12.17
If T also has two black children, we can ﬂip the colors of X, T, and P to maintain the invariant.
Figure 12.17 Three cases when X is a left child and has two black children.
Note carefully that this case will apply for the leaf, because nullNode is considered to be black.
In this case, we fall through to the next level, obtaining new X, T, and P.
If we’re lucky, X will land on the red child, and we can continue onward.
If not, we know that T will be red, and X and P will be black.
We can rotate T and P, making X’s new parent red; X and its grandparent will, of course, be black.
At this point we can go back to the ﬁrst main case.
If both children are red, we can apply either rotation.
As usual, there are symmetric rotations for the case when X is a right child that are not shown.
Our last type of binary search tree, known as the treap, is probably the simplest of all.
Like the skip list, it uses random numbers and gives O(logN) expected time behavior for any input.
Searching time is identical to an unbalanced binary search tree (and thus slower than balanced search trees), while insertion time is only slightly slower than a recursive unbalanced binary search tree implementation.
Although deletion is much slower, it is still O(logN) expected time.
The treap is so simple that we can describe it without a picture.
Each node in the tree stores an item, a left and right link, and a priority that is randomly assigned when the node is created.
A treap is a binary search tree with the property that the node priorities satisfy heap order: Any node’s priority must be at least as large as its parent’s.
After the rotation, t is nullNode, and the left child stores the item that is to be deleted.
Note also that our implementation assumes that there are no duplicates; if this is not true, then the remove could fail (why?)
The treap is particularly easy to implement because we never have to worry about adjusting the priority ﬁeld.
One of the difﬁculties of the balanced tree approaches is that it is difﬁcult to track down errors that result from failing to update balance information in the course of an operation.
In terms of total lines for a reasonable insertion and deletion package, the treap, especially a nonrecursive implementation, seems like the hands-down winner.
One of the most fundamental problems in data processing is to ﬁnd the location of a pattern P in a text T.
For instance, we may be interested in answering questions such as.
Assuming that the size of P is less than T (and usually it is signiﬁcantly less), then we would reasonably expect that the time to solve this problem for a given P and T would be at least linear in the length of T, and in fact there are several O( | T | ) algorithms.
However, we are interested in a more common problem, in which T is ﬁxed, and queries with different P occur frequently.
For instance, T could be a huge archive of email messages, and we are interested in repeatedly searching the email messages for different patterns.
In this case, we are willing to preprocess T into a nice form that would make each individual search much more efﬁcient, taking time signiﬁcantly less than linear in the size of T—either logarithmic in the size of T, or even better, independent of T and dependent only on the length of P.
One such data structure is the sufﬁx array and sufﬁx tree (that sounds like two data structures, but as we will see, they are basically equivalent, and trade time for space)
A sufﬁx array for a text T is simply an array of all sufﬁxes of T arranged in sorted order.
Then the sufﬁx array for banana is shown in Figure 12.21:
A sufﬁx array that stores the sufﬁxes explicitly would seem to require quadratic space, since it stores one string of each length 1 to N (where N is the length of T)
In Java, this is not exactly true, since Java strings are implemented by maintaining an array of characters and a starting and ending index.
This means that when a String is created via a call to substring, the same array of characters is shared, and the additional memory requirement is only the starting and ending index for the new substring.
Nonetheless, even this could be considered to be too much space: The sufﬁx array would be constructed for the text, not the pattern, and the text could be huge.
Thus it is common for a practical implementation to store only the starting indices of the sufﬁxes in the sufﬁx array, instead of the entire substring.
For instance, if a pattern P occurs in the text, then it must be a preﬁx of some sufﬁx.
A binary search of the sufﬁx array would be enough to determine if the pattern P is in the text: The binary search either lands on P, or P would be between two values, one smaller than P and one larger than P.
If P is a preﬁx of some substring, it is a preﬁx of the larger value found at the end of the binary search.
Figure 12.22 Sufﬁx array that stores only Indices (full substrings shown for reference)
We can also use the sufﬁx array to ﬁnd the number of occurrences of P: They will be stored sequentially in the sufﬁx array, thus two binary searches sufﬁx to ﬁnd a range of sufﬁxes that will be guaranteed to begin with P.
Figure 12.23 shows the LCP computed for each substring, relative to the preceding substring.
The longest common preﬁx also provides information about the longest pattern that occurs twice in the text: Look for the largest LCP value, and take that many characters of the corresponding substring.
Figure 12.24 shows simple code to compute the sufﬁx array and longest common preﬁx information for any string.
The running time of the sufﬁx array computation is dominated by the sorting step, which uses O(N logN ) comparisons.
Figure 12.23 Sufﬁx array for “banana”; includes longest common preﬁx (LCP)
Figure 12.24 Simple algorithm to create sufﬁx array and LCP array.
English-language novel can be built in just a few seconds.
One such example occurs in pattern searching of DNA, whose alphabet consists of four characters (A, C, G, T) and whose strings can be huge.
In the degenerate case of a String that contains only one character, repeated N times, it is easy to see that each comparison takes O(N) time, and the total cost is O( N2 log N )
In Section 12.4.3, we will show a linear-time algorithm to construct the sufﬁx array.
Sufﬁx arrays are easily searchable by binary search, but the binary search itself automatically implies log T cost.
What we would like to do is ﬁnd a matching sufﬁx even more efﬁciently.
One idea is to store the sufﬁxes in a trie.
A binary trie was seen in our discussion of Huffman codes in Section 10.1.2
The basic idea of the trie is to store the sufﬁxes in a tree.
At the root, instead of having two branches, we would have one branch for each possible ﬁrst character.
Then at the next level, we would have one branch for the next character, and so on.
At each level we are doing multiway branching, much like radix sort, and thus we can ﬁnd a match in time that would depend only on the length of the match.
In Figure 12.25, we see on the left a basic trie to store the sufﬁxes of the string deed.
In this trie, internal branching nodes are drawn in circles, and the sufﬁxes that are reached are drawn in rectangles.
Each branch is labeled with the character that is chosen, but the branch prior to a completed sufﬁx has no label.
This representation could waste signiﬁcant space if there are many nodes that have only one child.
Thus in Figure 12.25, we see an equivalent representation on the right, known as a compressed trie.
Notice that although the branches now have multicharacter labels, all the labels for the branches of any given node must have unique ﬁrst characters.
Thus it is still just as easy as before to choose which branch to take.
Thus we can see that a search for a pattern P depends only on the length of the pattern P, as desired.
Then each node stores an array representing each possible branch and we can locate the appropriate branch in constant time.
If the original string has length N, the total number of branches is less than 2N.
However, this by itself does not mean that the compressed trie uses linear space: The labels on the edges take up space.
And of course writing all the sufﬁxes in the leaves could.
So if the original used quadratic space, so does the compressed trie.
Fortunately, we can get by with linear space as follows:
In the leaves, we use the index where the sufﬁx begins (as in the sufﬁx array)
In the internal nodes, we store the number of common characters matched from the root until the internal node; this number represents the letter depth.
Figure 12.26 shows how the compressed trie is stored for the sufﬁxes of banana.
The leaves are simply the indices of the starting points for each sufﬁx.
The internal node with a letter depth of 1 is representing the common string “a” in all nodes that are below it.
The internal node with a letter depth of 3 is representing the common string “ana” in all nodes that are below it.
And the internal node with a letter depth of 2 is representing the common string “na” in all nodes that are below it.
In fact, this analysis makes clear that a sufﬁx tree is equivalent to a sufﬁx array plus an LCP array.
At that time we can compute the LCP as follows: If the sufﬁx node value PLUS the letter depth of the parent is equal to N, then use the letter depth of the grandparent as the LCP; otherwise use the parent’s letter depth as the LCP.
In Figure 12.26, if we proceed inorder, we obtain for our sufﬁxes and LCP values.
The sufﬁx array and LCP array also uniquely deﬁne the sufﬁx tree.
The sufﬁx tree solves many problems efﬁciently, especially if we augment each internal node to also maintain the number of sufﬁxes stored below it.
A small sampling of sufﬁx tree applications includes the following:
Find the longest repeated substring in T: Traverse the tree, ﬁnding the internal node with the largest number letter depth; this represents the maximum LCP.
This generalizes to the longest substring repeated at least k times.
This can be done in time proportional to the total size of the strings and generalizes to an O( kN ) algorithm for k strings of total length N.
Find the number of occurrences of the pattern P: Assuming that the sufﬁx tree is augmented so that each leaf keeps track of the number of sufﬁxes below it, simply follow the path down the internal node; the ﬁrst internal node that is a preﬁx of P provides the answer; if there is no such node, the answer is either zero or one and is found by checking the sufﬁx at which the search terminates.
This takes time proportional to the length of the pattern P and is independent of the size of |T|
In this section we describe an O( N ) worst-case time algorithm to compute the sufﬁx array.
Either way, we can thus also build a sufﬁx tree in linear time.
Sort the remaining sufﬁxes, B, by using the now-sorted sample of sufﬁxes A.
To get an intuition of how step 3 might work, suppose the sample A of sufﬁxes are all sufﬁxes that start at an odd index.
Then the remaining sufﬁxes B, are those sufﬁxes that start at an even index.
So suppose we have computed the sorted set of sufﬁxes A.
To compute the sorted set of sufﬁxes B, we would in effect need to sort all the sufﬁxes that start at even indices.
But these sufﬁxes each consist of a single ﬁrst character in an even position, followed by a string that starts with the second character, which must be in an odd position.
Thus the string that starts in the second character is exactly a string that is in A.
So to sort all the sufﬁxes B, we can do something similar to a radix sort: First sort the strings in B starting from the second character.
This should take linear time, since the sorted order of A is already known.
Then stably sort on the ﬁrst character of the strings in B.
Thus B could be sorted in linear time, after A is sorted recursively.
If A and B could then be merged in linear time, we would have a linear-time algorithm.
The algorithm we present uses a different sampling step, that admits a simple linear-time merging step.
As we describe the algorithm, we will also show how it computes the sufﬁx array for the string ABRACADABRA.
Then use those numbers for the remainder of the algorithm.
Note that the numbers that are assigned depend on the text.
So, if the text contains DNA characters A, C, G, and T only, then there will be only four numbers.
Then pad the array with three 0’s to avoid boundary cases.
If we assume that the alphabet is a ﬁxed size, then the sort takes some constant amount of time.
Figure 12.27 Mapping of character in string to an array of integers.
However, since this would be three recursive calls on problems 1/3 the original size, that would result in an O( N log N ) algorithm.
So the idea is going to be to avoid one of the three recursive calls, by computing two of the sufﬁx groups recursively and using that information to compute the third sufﬁx group.
Example: In our example, if we look at the original character set and use $ to represent the padded character, we get.
In order to compute this sufﬁx array, we will need to sort the new alphabet of tri-characters.
If in fact all the tri-characters in the new alphabet are unique, then we do not even need to bother with a recursive call.
If T(N) is the running time of the sufﬁx array construction algorithm, then the recursive call takes T(2N/3) time.
Notice that these are not exactly the same as the corresponding sufﬁxes in S; however, if we strip out characters starting at the ﬁrst $, we do have a match of sufﬁxes.
Also note that the indices returned by the recursive call do not correspond directly to the indices in S, though it is a simple matter to map them back.
Entries in the next to last row are easily obtained from the prior two rows.
Then we do a second pass on the single characters from S, using the prior radix sort to break ties.
Since this is a two-pass radix sort, this step takes O( N )
Step 5: Merge the two sufﬁx arrays using the standard algorithm to merge two sorted lists.
The only issue is that we must be able to compare each sufﬁx pair in constant time.
We advance in the second group and now we have.
So that means that now 7 goes into the ﬁnal sufﬁx array, and we advance the second group, obtaining.
Thus 0 goes into the ﬁnal sufﬁx array and we advance the ﬁrst group.
At this point, there are no ties for a while, so we quickly advance to the last characters of each group:
Thus the R in index 9 advances, and then we can ﬁnish the merge.
Since this is a standard merge, with at most two comparisons per sufﬁx pair, this step takes linear time.
The entire algorithm thus satisﬁes T(N) = T(2N/3) + O( N ) and takes linear time.
Although we have only computed the sufﬁx array, the LCP information can also be.
In Figure 12.32, we allocate the arrays that have three extra slots for padding and call makeSuffixArray, which is the basic linear-time algorithm.
Figure 12.33 The main routine for linear-time sufﬁx array construction.
Figure 12.34 Routine to compute and assign the tri-character names.
We can use the basic counting radix sort from Chapter 7 to obtain a linear-time sort.
Figure 12.35 A counting radix sort for the sufﬁx array.
The array in represents the indexes into s; the result of the radix sort is that the indices are sorted so that the characters in s are sorted at those indices (where the indices are offset as speciﬁed)
Finally, the merge routine is shown in Figure 12.37, with some supporting routines in.
The merge routine has the same basic look and feel as the standard merging algorithm seen in Figure 7.10
Suppose that an advertising company maintains a database and needs to generate mailing labels for certain constituencies.
In one dimension, the problem can be solved by a simple recursive algorithm in O(M + logN) average time, by traversing a preconstructed binary search tree.
Here M is the number of matches reported by the query.
We would like to obtain a similar bound for two or more dimensions.
The two-dimensional search tree has the simple property that branching on odd levels is done with respect to the ﬁrst key, and branching on even levels is done with respect to the second key.
The root is arbitrarily chosen to be an odd level.
Insertion into a 2-d tree is a trivial extension of insertion into a binary search.
To keep our code simple, we assume that a basic item is an array of two elements.
We use recursion in this section; a nonrecursive implementation that would be used in practice is straightforward and left as Exercise 12.17
One difﬁculty is duplicates, particularly since several items can agree in one key.
Our code allows duplicates and always places them in right branches; clearly this can be a problem if there are too many duplicates.
A moment’s thought will convince you that a randomly constructed 2-d tree has the same structural properties as a random binary search tree: The height is O(logN) on average, but O(N) in the worst case.
Unlike binary search trees, for which clever O(logN) worst-case variants exist, there are no schemes that are known to guarantee a balanced 2-d tree.
The problem is that such a scheme would likely be based on tree rotations, and tree rotations don’t work in 2-d trees.
The best one can do is to periodically rebalance the tree by reconstructing a subtree, as described in the exercises.
Similarly, there are no deletion algorithms beyond the obvious lazy deletion strategy.
Several kinds of queries are possible on a 2-d tree.
We can ask for an exact match, or a match based on one of the two keys; the latter type of request is a partial match query.
Both of these are special cases of an (orthogonal) range query.
An orthogonal range query gives all items whose ﬁrst key is between a speciﬁed set of values and whose second key is between another speciﬁed set of values.
This is exactly the problem that was described in the introduction to this section.
A range query is easily solved by a recursive tree traversal, as shown in Figure 12.41
By testing before making a recursive call, we can avoid unnecessarily visiting all nodes.
An insertion or exact match search in a 2-d tree takes time that is proportional to the depth of the tree, namely, O(logN) on average and O(N) in the worst case.
The running time of a range search depends on how balanced the tree is, whether or not a partial match is requested, and how many items are actually found.
Although there are several exotic structures that support range searching, the k-d tree is probably the simplest such structure that achieves respectable running times.
The last data structure we examine is the pairing heap.
The analysis of the pairing heap is still open, but when decreaseKey operations are needed, it seems to outperform other heap structures.
The most likely reason for its efﬁciency is its simplicity.
The decreaseKey operation, as we will see, requires that each node contain an additional link.
A node that is a leftmost child contains a link to its parent; otherwise the node is a right sibling and contains a link to its left sibling.
The procedure is generalized to allow the second subheap to have siblings.
As we mentioned earlier, the subheap with the larger root is made a leftmost child of the other subheap.
Notice that we have several instances in which a node reference is tested against null before assigning its prev ﬁeld; this suggests that perhaps it would be useful to have a nullNode sentinel, which was customary in this chapter’s search tree implementations.
Figure 12.46 shows the PairNode class and Position interface, which are both nested in the PairingHeap class.
The insert and decreaseKey operations are, then, simple implementations of the abstract description.
Since the position of an item is determined (irrevocably) when an item is ﬁrst inserted, insert returns the PairNode it creates back to the caller.
Our routine for decreaseKey throws an exception if the new value is not smaller than the old; otherwise, the resulting structure might not obey heap order.
The basic deleteMin procedure follows directly from the abstract description and is shown in Figure 12.48
The element ﬁeld is set to null, so if the Position is used in a decreaseKey, it will be possible for decreaseKey to detect that the Position is no longer valid.
The devil, of course, is in the details: How is combineSiblings implemented? Several variants have been proposed, but none has been shown to provide the same amortized.
Internal method that is the basic operation to maintain order.
It has recently been shown that almost all the proposed methods are in fact theoretically less efﬁcient than the Fibonacci heap.
Even so, the method coded in Figure 12.49 always seems to perform as well as or better than other heap structures, including the binary heap, for the typical graph theory uses that involve a host of decreaseKey operations.
This method, known as two-pass merging, is the simplest and most practical of the many variants that have been suggested.
We ﬁrst scan left to right, merging pairs of children.4 After the ﬁrst scan, we have half as many trees to merge.
At each step we merge the rightmost tree remaining from the.
We must be careful if there is an odd number of children.
When that happens, we merge the last child with the result of the rightmost merge to complete the ﬁrst scan.
The only simple merging strategy that is easily seen to be poor is a left-to-right single-pass merge (Exercise 12.29)
The pairing heap is a good example of “simple is better” and seems to be the method of choice for serious applications requiring the decreaseKey or merge operation.
In this chapter, we’ve seen several efﬁcient variations of the binary search tree.
The top-down splay tree provides O(logN) amortized performance, the treap gives O(logN) randomized performance, and the red-black tree, gives O(logN) worst-case performance for the basic operations.
The trade-offs between the various structures involve code complexity, ease of deletion, and differing searching and insertion costs.
Recurring themes include tree rotations and the use of sentinel nodes to eliminate many of the annoying tests for null references that would otherwise be necessary.
The sufﬁx tree and array are a powerful data structure that allows quick repeated searching for a ﬁxed text.
The k-d tree provides a practical method for performing range searches, even though the theoretical bounds are not optimal.
Finally, we described and coded the pairing heap, which seems to be the most practical mergeable priority queue, especially when decreaseKey operations are required, even though it is theoretically less efﬁcient than the Fibonacci heap.
Show that with a recursive call to S3S5S6, we have enough information to sort.
Alternatively, generate a random number each time an item X is accessed.
If this number is smaller than X’s current priority, use it as X’s new priority (performing the appropriate rotation)
How much coding effort is saved by using the sentinel?
Adopt the following strategy: If the left and right subtrees have weights that are not within a factor of 2 of each other, then completely rebuild the subtree rooted at the node.
We can rebuild a node in O(S), where S is the weight of the node.
We can rebuild a node in a k-d tree in O(S log S) time, where S is the weight of.
We can apply the algorithm to k-d trees, at a cost of O(log2 N) per insertion.
Explain in detail all the reasons that the result is no longer a usable 2-d tree.
How do we ﬁnd the item with minimum key #1?
How do we ﬁnd the item with minimum key #2? d.
Give an algorithm to insert a new item into the 2-d heap.
Give an algorithm to perform deleteMin with respect to either key.
You should be able to obtain the following bounds: insert in O(logN), deleteMin in O(2k logN), and buildHeap in O(kN)
Clearly, the basic algorithms still work; what are the new time bounds?
What would you expect the average running time to be for a random tree?
The ﬁrst insertion, of p1, splits the plane into a left part and a right part.
The second insertion, of p2, splits the left part into a top part and a bottom part, and so on.
For a given set of N items, does the order of insertion affect the ﬁnal partition? b.
If two different insertion sequences result in the same tree, is the same partition.
Give a formula for the number of regions that result from the partition after N.
Figure 12.52 shows how a plane is partitioned by a quad tree.
Initially we have a region (which is often a square, but need not be)
If a second point is inserted into a region, then the region is split into four equal-sized quadrants (northeast, southeast, southwest, and northwest)
For a given set of N items, does the order of insertion affect the ﬁnal partition? b.
Show the ﬁnal partition if the same elements that were in the 2-d tree in.
Each node is either a leaf that stores an inserted item, or has exactly four children, representing four quadrants.
To perform a search, we begin at the root and repeatedly branch to an appropriate quadrant until a leaf (or null entry) is reached.
What factors inﬂuence how deep the (quad) tree will be? c.
Describe an algorithm that performs an orthogonal range query in a quad tree.
Top-down splay trees were described in the original splay tree paper [36]
A similar strategy, but without the crucial rotation, was described in [38]
An implementation of top-down red-black trees without sentinel nodes is given in [15]; this provides.
A related data structure is the priority search tree [27]
Farach [13] provided an alternate algorithm that is the basis for many of the linear-time sufﬁx array construction algorithms.
Numerous applications of sufﬁx trees can be found in the text by Gusﬁeld [19]
Because the input sizes for practical applications are so large, space is important, and thus much recent work has centered on sufﬁx array and LCP array construction.
In particular, for many algorithms, a cache-friendly slightly nonlinear algorithm can be preferable in practice to a noncache friendly linear algorithm [33]
For truly huge input sizes, in-memory construction is not always feasible.
The solutions to most of the exercises can be found in the primary references.
Exercise 12.15 represents a “lazy” balancing strategy that has become somewhat popular.
A tree that satisﬁes the property in Exercise 12.15 is weight-balanced.
