Learn how to test web applications using Apache JMeter with practical, hands-on examples.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Bayo Erinle is a senior software engineer with over nine years' experience in designing, developing, testing, and architecting software.
He has worked in various spectrums of the IT field, including government, finance, and health care.
As a result, he has been involved in the planning, development, implementation, integration, and testing of numerous applications, including multi-tiered, standalone, distributed, and cloud-based applications.
He is always intrigued by new technology and enjoys learning new things.
He currently resides in Maryland, US, and when he is not hacking away at some new technology, he enjoys spending time with his wife Nimota and their three children, Mayowa, Durotimi, and Fisayo.
Dmitri Nevedrov has been working in software research and development for many years, primarily focusing on Java, J2EE technology, and performance optimization techniques.
Shantonu Sarker is a proactive software test engineer with seven years of experience in test automation, development (C# and Java), and project management with Agile (Scrum and Kanban)
He also owns a startup software company named QualitySofts, which specializes in software development and testing services.
He also gives training on software development (C# and Java) and software test tools contractually.
Before starting his career in the software industry, he was a computer teacher.
I would like to thank Guru Mahajatok, because without his guidance I would not be what I am today.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Preface Performance Testing with JMeter 2.9 is about a type of testing intended to determine the responsiveness, reliability, throughput, interoperability, and scalability of a system and/or application under a given workload.
It is critical and essential to the success of any software product launch and its maintenance.
It also plays an integral part in scaling an application out to support a wider user base.
Apache JMeter is a free open source, cross-platform performance testing tool that has been around since the late 90s.
It has a large user base and offers lots of plugins to aid testing.
This is a practical hands-on book that focuses on how to leverage Apache JMeter to meet your testing needs.
It starts with a quick introduction on performance testing, but quickly moves into engaging topics such as recording test scripts, monitoring system resources, an extensive look at several JMeter components, leveraging the cloud for testing, and extending Apache JMeter capabilities via plugins.
Along the way, you will do some scripting, learn and use tools such as Vagrant, Puppet, Apache Tomcat, and be armed with all the knowledge you need to take on your next testing engagement.
Whether you are a developer or tester, this book is sure to give you some valuable knowledge to aid you in attaining success in your future testing endeavors.
What this book covers Chapter 1, Performance Testing Fundamentals, covers the fundamentals of performance testing and the installation and configuration of JMeter.
Chapter 2, Recording Your First Test, dives into recording your first JMeter test script and covers the anatomy of a JMeter test script.
It includes handling various HTML form elements, (checkboxes, radio buttons, file uploads, downloads, and so on), JSON data, and XML.
Chapter 4, Managing Sessions, explains session management, including cookies and URL rewriting.
Chapter 5, Resource Monitoring, dives into active monitoring of system resources while executing tests.
You get to start up a server and extend JMeter via plugins.
Chapter 6, Distributed Testing, takes an in-depth look at leveraging the cloud for performance testing.
We dive into tools such as Vagrant, Puppet, and AWS.
Chapter 7, Helpful Tips, provides you with helpful techniques and tips for getting the most out of JMeter.
What you need for this book To follow along with the examples in this book, you will need the following:
In addition, for Chapter 4, Resource Monitoring, you need the following:
And for Chapter 6, Distributed Testing, you need the following:
The book contains pointers and additional helpful links in setting all these up.
Who this book is for The book is targeted primarily at developers and testers.
Developers who have always been intrigued by performance testing and wanted to dive in on the action will find it extremely useful and gain insightful skills as they walk through the practical examples in the book.
Testers will also benefit from this book since it will guide them through solving practical, real-world challenges when testing modern web applications, giving them ample knowledge to aid them in becoming better testers.
Additionally, they will be exposed to certain helpful testing tools that will come in handy at some point in their testing careers.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "Append %JAVA_HOME%/bin to the end of the existing path value (if any)."
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "clicking the Next button moves you to the next screen"
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you.
The color images will help you better understand the changes in the output.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
The company achieves this goal by providing a suite of products, including online courses, onsite training, and offsite training.
As such, one of their flagship products, TrainBot—a web-based application—is focused solely on registering individuals for courses of interest that will aid them in attaining career goals.
Once registered, the client can then go on to take a series of interactive online courses.
The incident Up until recently, traffic on TrainBot had been light as it had only been opened to a handful of clients, since it was still in closed beta.
Everything was fully operational and the application as a whole was very responsive.
Just a few weeks ago, TrainBot was open to the public and all was still good and dandy.
To celebrate the launch and promote its online training courses, Baysoft Training Inc.
However, that promotional offer caused a sudden influx on TrainBot, far beyond what the company had anticipated.
Web traffic shot up by 300 percent and suddenly things took a turn for the worse.
Network resources weren't holding up well, server CPUs and memory were at 90-95 percent and database servers weren't far behind due to high I/O and contention.
As a result, most web requests began to get slower response times, making TrainBot totally unresponsive for most of its first-time clients.
It didn't take too long after that for the servers to crash and for the support lines to get flooded.
How did this happen? Could this have been avoided? Why was the application and system not able to handle the load? Why weren't adequate performance and stress tests conducted on the system and application? Was it an application problem, a system resource issue or a combination of both? All of these were questions management demanded answers to from the group of engineers, which comprised software developers, network and system engineers, quality assurance (QA) testers, and database administrators gathered in the WAR room.
There sure was a lot of finger pointing and blame to go around the room.
After a little brainstorming, it wasn't too long for the group to decide what needed to be done.
The application and its system resources will need to undergo extensive and rigorous testing.
This will include all facets of the application and all supporting system resources, including, but not limited to, infrastructure, network, database, servers, and load balancers.
Such a test will help all the involved parties to discover exactly where the bottlenecks are and address them accordingly.
Performance testing Performance testing is a type of testing intended to determine the responsiveness, reliability, throughput, interoperability, and scalability of a system and/or application under a given workload.
It could also be defined as a process of determining the speed or effectiveness of a computer, network, software application, or device.
Testing can be conducted on software applications, system resources, targeted application components, databases, and a whole lot more.
It normally involves an automated test suite as this allows for easy, repeatable simulations of a variety of normal, peak, and exceptional load conditions.
Such forms of testing help verify whether a system or application meets the specifications claimed by its vendor.
The process can compare applications in terms of parameters such as speed, data transfer rate, throughput, bandwidth, efficiency, or reliability.
Performance testing can also aid as a diagnostic tool in determining bottlenecks and single points of failure.
It is often conducted in a controlled environment and in conjunction with stress testing; a process of determining the ability of a system or application to maintain a certain level of effectiveness under unfavorable conditions.
Why bother? Using Baysoft's case study mentioned earlier, it should be obvious why companies bother and go through great lengths to conduct performance testing.
Disaster could have been minimized, if not totally eradicated, if effective performance testing had been conducted on TrainBot prior to opening it up to the masses.
As we go ahead in this chapter, we will continue to explore the many benefits of effective performance testing.
At a very high level, performance testing is always almost conducted to address one or more risks related to expense, opportunity costs, continuity, and/or corporate reputation.
Conducting such tests help give insights to software application release readiness, adequacy of network and system resources, infrastructure stability, and application scalability, just to name a few.
Gathering estimated performance characteristics of application and system resources prior to the launch helps to address issues early and provides valuable feedback to stakeholders, helping them make key and strategic decisions.
Performance testing covers a whole lot of ground including areas such as:
Most of these areas are intertwined with each other, each aspect contributing to attaining the overall objectives of stakeholders.
However, before jumping right in, let's take a moment to understand the core activities in conducting performance tests:
Identify the test environment: Becoming familiar with the physical test and production environments is crucial to a successful test run.
Knowing things, such as the hardware, software, and network configurations of the environment help derive an effective test plan and identify testing challenges from the outset.
In most cases, these will be revisited and/or revised during the testing cycle.
Identify acceptance criteria: What is the acceptable performance of the various modules of the application under load? Specifically, identify the response time, throughput, and resource utilization goals and constraints.
How long should the end user wait before rendering a particular page? How long should the user wait to perform an operation? Response time is usually a user concern, throughput a business concern, and resource utilization a system concern.
As such, response time, throughput, and resource utilization are key aspects of performance testing.
Acceptance criteria is usually driven by stakeholders and it is important to continuously involve them as testing progresses as the criteria may need to be revised.
Plan and design tests: Know the usage pattern of the application (if any), and come up with realistic usage scenarios including variability among the various scenarios.
For example, if the application in question has a user registration module, how many users typically register for an account in a day? Do those registrations happen all at once, or are they spaced out? How many people frequent the landing page of the application within an hour? Questions such as these help to put things in perspective and design variations in the test plan.
Having said that, there may be times where the application under test is new and so no usage pattern has been formed yet.
At such times, stakeholders should be consulted to understand their business process and come up with as close to a realistic test plan as possible.
Prepare the test environment: Configure the test environment, tools, and resources necessary to conduct the planned test scenarios.
It is important to ensure that the test environment is instrumented for resource monitoring to help analyze results more efficiently.
Depending on the company, a separate team might be responsible for setting up the test tools, while another may be responsible for configuring other aspects such as resource monitoring.
In other organizations, a single team is responsible for setting up all aspects.
Record the test plan: Using a testing tool, record the planned test scenarios.
There are numerous testing tools available, both free and commercial that do the job quite well, each having their pros and cons.
Some of these are commercial while others are not as mature or as portable or extendable as JMeter is.
It does offer a much nicer graphical interface and monitoring capability though.
Gatling is the new kid on the block, is free and looks rather promising.
It is still in its infancy and aims to address some of the shortcomings of JMeter, including easier testing DSL (domain specific language) versus JMeter's verbose XML, nicer and more meaningful HTML reports, among others.
Having said that, it still has only a tiny user base when compared with JMeter, and not everyone may be comfortable with building test plans in Scala, its language of choice.
In this book, our tool of choice will be Apache JMeter to perform this step.
That shouldn't be a surprise considering the title of the book.
Run the tests: Once recorded, execute the test plans under light load and verify the correctness of the test scripts and output results.
In cases where test or input data is fed into the scripts to simulate more realistic data (more on that in the later chapters), also validate the test data.
Another aspect to pay careful attention to during test plan execution is the server logs.
This can be achieved through the resource monitoring agents set up to monitor the servers.
A high rate of errors, for example, could be indicative that something is wrong with the test scripts, application under test, system resource, or a combination of these.
Analyze results, report, and retest: Examine the results of each successive run and identify areas of bottleneck that need addressing.
System-related bottlenecks may lead to infrastructure changes such as increasing the memory available to the application, reducing CPU consumption, increasing or decreasing thread pool sizes, revising database pool sizes, and reconfiguring network settings.
Database-related bottlenecks may lead to analyzing database I/O operations, top queries from the application under test, profiling SQL queries, introducing additional indexes, running statistics gathering, changing table page sizes and locks, and a lot more.
Once the identified bottlenecks are addressed, the test(s) should then be rerun and compared with previous runs.
To help better track what change or group of changes resolved a particular bottleneck, it is vital that changes are applied in an orderly fashion, preferably one at a time.
In other words, once a change is applied, the same test plan is executed and the results compared with a previous run to see if the change made had any improved or worsened effect on results.
This process repeats until the performance goals of the project have been met.
Performance testing is usually a collaborative effort between all parties involved.
Parties include business stakeholders, enterprise architects, developers, testers, DBAs, system admins, and network admins.
Such collaboration is necessary to effectively gather accurate and valuable results when conducting testing.
Monitoring network utilization, database I/O and waits, top queries, and invocation counts, for example, helps the team find bottlenecks and areas that need further attention in ongoing tuning efforts.
Performance testing and tuning There is a strong relationship between performance testing and tuning, in the sense that one often leads to the other.
Often, end-to-end testing unveils system or application bottlenecks that are regarded as incompatible with project target goals.
Once those bottlenecks are discovered, the next step for most teams is a series of tuning efforts to make the application perform adequately.
Reducing round trips in application calls; sometimes leading to re-designing.
Tuning efforts may also commence if the application has reached acceptable performance but the team wants to reduce the amount of system resources being used, decrease volume of hardware needed, or further increase system performance.
After each change (or series of changes), the test is re-executed to see whether performance has increased or declined as a result of the changes.
The process will be continued until the performance results reach acceptable goals.
The outcome of these test-tuning circles normally produces a baseline.
Baselines Baseline is a process of capturing performance metric data for the sole purpose of evaluating the efficacy of successive changes to the system or application.
It is important that all characteristics and configurations except those specifically being varied for comparison remain the same, in order to make effective comparisons as to which change (or series of changes) is the driving result towards the targeted goal.
Armed with such baseline results, subsequent changes can be made to system configuration or application and testing results compared to see whether such changes were relevant or not.
They evolve and may need to be redefined from time to time.
Load and stress testing Load testing is the process of putting demand on a system and measuring its response; that is, determining how much volume the system can handle.
Stress testing is the process of subjecting the system to unusually high loads far beyond its normal usage pattern to determine its responsiveness.
These are different from performance testing whose sole purpose is to determine the response and effectiveness of a system; that is, how fast is the system.
Since load ultimately affects how a system responds, performance testing is almost always done in conjunction with stress testing.
JMeter to the rescue In the previous section, we covered the fundamentals of conducting a performance test.
One of the areas performance testing covers is testing tools.
Which testing tool do you use to put the system and application under load? There are numerous testing tools available to perform this operation, from free to commercial solutions.
However, our focus in this book will be on Apache JMeter, a free open source, cross platform desktop application from The Apache Software Foundation.
JMeter has been around since 1998 according to historic change logs on its official site, making it a mature, robust, and reliable testing tool.
Cost may also have played a role in its wide adoption.
Small companies usually may not want to foot the bill for commercial testing tools, which often still place restrictions on how many concurrent users one can spin off, for example.
My first encounter with JMeter was exactly as a result of this.
I worked in a small shop that had paid for a commercial testing tool, but during the course of testing, we had overrun the licensing limits of how many concurrent users we needed to simulate for realistic test plans.
Since JMeter was free, we explored it and were quite delighted with the offerings and the sheer number of features we got for free.
Performance test of different server types including web (HTTP and HTTPS), SOAP, database, LDAP, JMS, mail, and native commands or shell scripts.
JMeter allows multiple concurrent users to be simulated on the application allowing you to work towards most of the target goals mentioned earlier in the chapter, such as attaining baseline, identifying bottlenecks, and so on.
Will the application still be responsive if 50 users are accessing it concurrently?
How reliable will it be under a load of 200 users?
How much system resources will be consumed under a load of 250 users?
What is throughput going to look like when 1000 users are active in the system?
What is the response time for the various components in the application under load?
It doesn't perform all the operations supported by browsers; in particular, JMeter does not execute JavaScript found in HTML pages, nor does it render HTML pages the way a browser does.
It does give you the ability to view request responses as HTML through one of its many listeners, but the timings are not included in any samples.
Furthermore, there are limitations as to how many users can be spun on a single machine.
These vary depending on the machine specifications (for example, memory and processor speed) and the test scenarios being executed.
Up and running with JMeter Now let's get up and go running with JMeter, beginning with its installation.
Installation JMeter comes as a bundled archive so it is super easy to get started with it.
Those working in corporate environments behind a firewall or machines with non-admin privileges appreciate this more.
To get started, grab the latest binary release by pointing your browser to http://jmeter.apache.org/download_jmeter.cgi.
At the time of writing, the current release version is 2.9
The download site offers the bundle as both zip and tar.
In this book, we will use the ZIP option, but feel free to download the TGZ if that's your preferred way of grabbing archives.
Once downloaded, extract the archive to a location of your choice.
Provided you have a JDK/JRE correctly installed and a JAVA_HOME environment variable set, you are all set and ready to run!
Download Java JDK (not JRE) compatible with the system you will be using to test.
On Windows systems, the default location for the JDK is under Program Files.
While there is nothing wrong with that, the issue is that the folder name contains a space, which can sometimes be problematic when attempting to set PATH and run programs such as JMeter from the command line.
With that in mind, it is advisable to change the default location to something such as C:\tools\jdk.
Locate Path (under System variables; bottom half of the screen)
Append %JAVA_HOME%/bin to the end of the existing path value (if any)
On Unix For illustrative purposes, we assume you have installed Java JDK at /opt/ tools/jdk:
It is advisable to set this in your shell profile settings such as .bash_profile (for Bash users) or .zshrc (for zsh users) so you won't have to set it for each new terminal window you open.
Running JMeter Once installed, the bin folder under JMETER_HOME folder contains all the executable scripts that can be run.
Based on which operating system you installed JMeter on, you either execute the shell scripts (.sh) for Unix/Linux flavored operating systems or their batch (.bat) counterparts on Windows operating systems.
JMeter files are saved as XML files with a .jmx extension.
We refer to them as test scripts or JMX files in this book.
To start JMeter, open a terminal shell, change to the JMETER_HOME\bin folder and run the following:
After a short moment, you should see the JMeter GUI (as shown in the following screenshot)
Hover over each icon to see a short description of what it does.
The Apache JMeter team has done an excellent job with the GUI.
Most icons are very similar to what you are used to, which helps ease the learning curve for new adapters.
Some of the icons, for example, stop, and shutdown, are disabled until a scenario/test is being conducted.
In the next chapter, we will explore the GUI in more detail as we record our first test script.
Command-line options Running JMeter with incorrect option provides you with usage info.
The previous code snippet (non-exhaustive list) is what you might see if you did the same.
We will explore some, but not all of these options as we go through the book.
JMeter's Classpath Since JMeter is 100 percent pure Java, it comes packed with functionality to get most test cases scripted.
However, there might come a time when you need to pull in a functionality provided by a third-party library or one developed by yourself, which is not present by default.
As such, JMeter provides two directories where such third-party libraries can be placed to be autodiscovered in its classpath.
All custom developed JMeter components should be placed in the lib\ext folder, while third-party libraries (JAR files), should reside in the lib folder.
Configuring the proxy server If you are working from behind a corporate firewall, you may need to configure JMeter to work with it by providing the proxy server host and port number.
To do so, supply additional command-line parameters to JMeter when starting it up.
Do not confuse the proxy server mentioned here with JMeter's built-in HTTP Proxy Server, which is used for recording HTTP or HTTPS browser sessions.
We will be exploring that in the next chapter when we record our first test scenario.
Running in non-GUI mode As described earlier, JMeter can run in non-GUI mode.
This is needed for times when you are running remotely, or want to optimize your testing system by not taking the extra overhead cost of running the GUI.
Normally, you will run the default (GUI), when recording your test scripts and running a light load but run in non-GUI mode for higher loads.
R: This command-line option runs the test on the specified remote servers (for example, -Rserver1,server2)
In addition, you can also use the -H and -P options to specify proxy server host and port, as we saw earlier:
Running in server mode This is used when performing distributed testing; that is, using more testing servers to generate additional load on your system.
JMeter will be kicked off in server mode on each remote server (slaves) and then a GUI on the master server is used to control the slave nodes.
We will discuss this in detail when we dive into distributed testing in Chapter 6, Distributed Testing.
Overriding properties JMeter provides two ways to override Java, JMeter, and logging properties.
We'll suggest you take a peek into this file and see the vast number of properties you can override.
This is one of the things that make JMeter so powerful and flexible.
On most occasions, you will not need to override the defaults, as they have sensible default values.
The other way to override these values is directly from the command line when starting JMeter.
Overriding a logging setting by setting a category to a given priority level:
Tracking errors during test execution JMeter keeps track of all errors that occur during a test in a logfile named jmeter.log by default.
The file resides in the folder from which JMeter was launched.
The name of this log file, like most things, can be configured in jmeter.
When running the GUI, the error count is indicated in the top-right corner, to the left of the number of threads running for the test.
Clicking on it reveals the log file contents directly at the bottom of the GUI.
The log file provides an insight into what exactly is going on in JMeter when your tests are being executed and helps determine the cause of error(s) when they occur.
If you find the provided SAX parser buggy for some of your use cases, this provides you the option to override it with another implementation.
When running JMeter in a distributed environment, list the machines where you have JMeter remote servers running.
This will allow you to control those servers from this machine's GUI.
This applies only while doing distributed testing and is not mandatory.
Since JMeter has quite a number of components, you may wish to restrict it to show only components you are interested in or those you use regularly.
You may list their classname or their class label (the string that appears in JMeter's UI) here, and they will no longer appear in the menus.
The defaults are fine, and in our experience we have never had to customize this, but we list it here so that you are aware of its existence.
These are added after the initial property file, but before the -q and -J options are processed.
In addition, these properties file can be used to fine-tune JMeter components' log verbosity.
This is in addition to any JARs found in the lib\ext folder.
You could use this to specify an alternate location on the machine to pick up the plugins.
These are added before the -S and -D options are processed.
This typically provides you with the ability to fine-tune various SSL settings, key stores, and certificates.
If for some reason, the default built-in Java implementation of SSL, which is quite robust, doesn't meet your particular usage scenario, this allows you to provide a custom one.
The command-line options are processed in the following order of precedence:
Summary In this chapter, we have covered the fundamentals of performance testing.
We also learned key concepts and activities surrounding performance testing in general.
In addition, we installed JMeter, learned how to get it fully running on a machine and explored some of the configurations available with it.
We explored some of the options that make JMeter a great tool of choice for your next performance testing engagement.
These include the fact that it is free and mature, open-sourced, easily extensible and customizable, completely portable across various operating systems, has a great-plugin ecosystem, large user community, built-in GUI, and recording and validating test scenarios among others.
In comparison with the other tools for performance testing, JMeter holds its own.
In the next chapter, we will record our first test scenario and dive deeper into JMeter.
Recording Your First Test JMeter comes with a built-in proxy server (http://en.wikipedia.org/wiki/ Proxy_server) to aid you record test plans.
The proxy server, once configured, watches your actions as you perform operations on a website, creates test sample objects for them and eventually stores them in your test plan; that is, a JMX file.
JMeter gives you the option of creating test plans manually, but this is mostly impractical for recording most testing scenarios.
You will save a whole lot of time using the proxy recorder, as you will see in a bit.
So without further ado, let's record our first test! For this, we will record the browsing of JMeter's own official website as a user would normally do.
For the proxy server to be able to watch your actions, it will need to be configured.
Configuring the JMeter HTTP proxy server The first step is to configure the proxy server in JMeter.
What is important is to choose a port that is not currently used by an existing process on the machine.
This allows you to group a series of requests as constituting a page load.
This instructs the proxy server to bypass recording requests of a series of elements which are not relevant to test execution.
Thankfully, JMeter provides a handy button that excludes the often-excluded elements.
With these settings, the proxy server will start on port 7000, monitor all requests going through that port, and record them to a test plan using the default recording controller.
Setting up your browser to use the proxy server There are several ways to set up the browser of your choice to use the proxy server.
We'll go over two of the most common ways, starting with our personal favorite, which is using a browser extension.
Using a browser extension Google Chrome and Firefox have vibrant browser plugin ecosystems that allow you to extend the capabilities of your browser with each plugin you choose.
For setting up a proxy, we really like FoxyProxy (http://getfoxyproxy.org/)
It is a neat add-on to the browser that allows you to set up various proxy settings and toggle between them on the fly, without having to mess around with system settings on the machine.
If you are using any of those, you are in luck; go ahead and grab it!
Changing the system settings For those who would rather configure the proxy natively on their operating system, we have provided the following steps for Windows and Mac OS.
On a Windows OS, perform the following steps to configure a proxy:
In the Internet Options dialog box, click on the Connections tab.
To enable the use of a proxy server, check the box for Use a proxy server for your LAN (These settings will not apply to dial-up or VPN connections) as shown in the following screenshot:
In the proxy's Address box, enter localhost in the IP address.
On a Mac OS, perform the following steps to configure a proxy:
For all other systems, please consult the related operating system's documentation.
Now that all of that is out of the way and the connections have been made, let's get to recording.
Admittedly, we have just scraped the surface of recording test plans, but we are off to a good start.
We will record many more plans, even complex ones, as we proceed through the book.
Running your first recorded scenario We can go right ahead and replay or run our recorded scenario now, but before that let's add a listener or two to give us feedback on the results of the execution.
We will cover listeners in depth in Chapter 5, Resource Monitoring, when we discuss resource monitoring, but for now it is enough to know that they are components that show the results of the test run.
There is no limit to the amount of listeners we can attach to a test plan, but we will often use only one or two.
For our test plan, let's add three listeners for illustrative purposes.
Each gathers a different kind of metric that can help analyze performance test results.
Now that we can see more interesting data, let's change some settings at the thread group level.
Before we proceed with test execution, save the test plan by clicking on the Save icon.
Once saved, click on the Start icon (the green play icon on the menu) and watch the test run.
As the test runs, you can click on Graph Results (or either of the other two) and watch the results gathering in real time.
From the Aggregate Report listener, we can see that there were 600 requests made to both the changes and usermanual links.
In addition, we see what the throughput is per second for the various links and that there was a 0.33 percent error rate on the changes link, meaning some requests to that link failed.
Looking at the View Results Tree listener, we see exactly which changes link requests failed and the reasons for their failure.
This can be valuable information to developers or system engineers in diagnosing the root cause of the errors.
The Graph Results listener also gave a pictorial representation of what is seen in the View Results Tree listener in the preceding screenshot.
If you clicked on it as the test was going on, you would have seen the graph get drawn in real time as the requests were coming in.
The graph is self-explanatory, with lines representing the average, median, deviation, and throughput.
Average, Median, and Deviation show average, median, and deviation of the number of samplers per minute respectively, while Throughput shows the average rate of network packets delivered over the network for our test run in bits per minute.
Please consult the Web (for example, Wikipedia) for detailed explanation of these terms.
For example, we mostly care about the average and throughput.
Let's uncheck Data, Median, and Deviation and you will see that only the data plots for Average and Throughput remain.
With our little recorded scenario, you have seen some of the major components that constitute a JMeter test plan.
Let's record another scenario, this time using another application that will allow us to enter form values.
We will explore this in depth in the next chapter, but for now let's take a sneak peek.
We'll borrow a website created by the wonderful folks at Excilys, a company focused on delivering skills and services in IT (http://www.excilys.com/)
It's a light banking web application created for illustrative purposes.
Set up the proxy like we did previously, and start recording.
At this point, we could add listeners to gather results of our execution and then replay the recorded scenario as we did before.
If we did, we would be in for a surprise.
We would have several failed requests after login, since we did not include the component to manage sessions and cookies needed to successfully replay this scenario.
Thankfully, JMeter has such a component and it is called HTTP Cookie Manager.
This seemingly simple, yet powerful, component helps maintain an active session via HTTP cookies, once our client has established a connection with the server, after login.
It ensures that a cookie is stored upon successful authentication and passed around for subsequent requests, hence allowing those to go through.
Each JMeter thread (that is, user) has its own cookie storage area.
This is vital since you wouldn't want a user gaining access to the site under another user's identity.
This becomes more apparent when we test for websites requiring authentication and authorization (like the one we just recorded) for multiple users.
At this point, we can simulate more load by increasing the number of threads at the Thread Group level.
If executed, the test plan will pass, but this is not realistic.
We have just emulated one user and essentially repeated the process five times.
To make the test realistic, what we want is each thread authenticating as a different user of the application.
In reality, your bank creates a unique user for you, and only you and your spouse will be privileged to see your account details.
So with that in mind, let's tweak the test to accommodate such a scenario.
Since it is expensive to generate unique random values at runtime due to high CPU and memory consumption, it is advisable to define those values upfront.
The CSV Data Set Config component is used to read lines from a file and split them into variables that can then be used to feed input into the test plan.
JMeter gives you a choice for the placement of this component within the test plan.
You would normally add the component at the HTTP request level of the request that needs values fed from it.
In our case, this will be the login HTTP request, where the username and password are entered.
Another is to add it at the Thread Group level; that is, as a direct child of the Thread Group.
If a particular data set is applied to only a Thread Group, it makes sense to add it at that level.
The third place where this component can be placed is at the Test Plan root level.
If a data set applies to all running threads, it makes sense to add it at the root level.
In our opinion, this also makes your test plans more readable and maintainable as it is easier to see what is going on when inspecting or troubleshooting a test plan, since this component can easily be seen at the root level rather than being deeply nested at other levels.
So for our scenario, let's add this at the Test Plan root level.
You can always move the components around using drag-and-drop, even after adding them to the Test Plan.
Once added, the Filename entry is all that is needed if you have included headers in the input file.
For example, if the input file is defined as such:
If the Variable Names field is left blank, JMeter will use the first line of the input file as the variable names for the parameters.
In cases where headers are not included, the variable names can be entered here.
This defaults to All threads, meaning that all running threads will use the same set of data.
If the number of running threads exceeds the input data, entries will be reused from the top of the file, provided that Recycle on EOF is set to true (the default)
The other options for sharing modes include Current thread group and Current thread.
Use the former for cases where the data set is specific for a certain Thread Group and the latter for cases where the data set is specific to each thread.
The other properties of the component are self-explanatory, and additional information about them can be found in JMeter's online user guide.
We can now run our test plan and it should work as before, only this time the values are dynamically bound through the configuration we have set up.
So far, we have run it only for a single user.
This is because we are trying to access an account that does not belong to the user that is logged in.
You can trace this by adding a View Results Tree listener to the test plan and returning the test.
If you closely examine some of the HTTP requests in the Request tab of the View Results Tree listener, you'll notice requests such as the following:
Observant readers would have noticed that our input datafile also contains an account_id column.
We can leverage this column to parameterize all requests containing account numbers to pick the right accounts for each user that is logged in.
To do that, we change the following line of code:
Once completed, we can re-run our test plan, and this time things are logically correct and will work fine.
You can also verify all work as expected after test execution, by examining the View Results Tree listener, clicking on some account requests URL, changing the response display from text to HTML, and you should see an account other than ACCT1
Sometimes, it is useful to parse the response to get the required information, rather than have it sent as a column of the input data.
This could further help make your test plans more robust.
In our preceding test plan, we could have leveraged this feature and parsed the response to get the required account number for users rather than sending it along as an input parameter.
Once parsed and obtained, we can save and use the account number for other requests down the chain.
Let's go ahead and record a new test plan as we did before.
To aid us extract a variable from the response data, we will use one of JMeter's post processor components, Regular Expression Extractor.
This component runs after each sample request in its scope, applying the regular expression and extracting the requested values.
A template string is then generated and the result of this is stored into a variable name.
This variable name is then used to parameterize, as in the case of the CSV Data Set Config component we saw earlier.
Unlike the CSV Data Set Config component we saw earlier, this component has to be placed directly as a child element of the request it will be acting on, since it's a post processor component.
Its configuration should be as shown in the following screenshot:
Using the View Results Tree to verify the response data.
When configuring the Regular Expression Extractor component, use the following values for each of the indicated fields:
The following screenshot shows what the component will look like with all of the entries filled out:
Here is a brief summary of the various configuration variables for the Regular Expression Extractor component:
Apply to: The default, Main sample only, is almost always okay, but there are times when the sample contains child samples that request embedded resources.
The options allow you to target the main sample, subsamples, or both.
The last option, JMeter Variable, allows assertions to be applied to the contents of the named variable.
Response field to check: This parameter specifies which field the regular expression should apply to.
Body (unescaped) – The body of the response with all HTML escape.
Headers – These may not be present for non-HTTP samples.
Reference name: The variable name under which the parsed results will be saved.
As a side note, JMeter regular expressions differ from their Perl counterparts.
While all regular expressions in Perl must be enclosed within //, the same is invalid in JMeter.
Regular expressions are a broad topic and you will see more of them throughout the course of the book, but we encourage you to read more at http://en.wikipedia.org/wiki/ Regular_expression.
Template: The template used to create a string from the matches found.
Match No.: This parameter indicates which match to use since the regular expression may match multiple times.
This indicates that JMeter should use a match at random.
Negative numbers can be used in conjunction with a ForEach controller.
Default value: If the regular expression doesn't match, the variable will be set to the default value set.
This is an optional parameter, but we recommend you always set it as it helps debug and diagnose issues while creating your test plans.
Anatomy of a JMeter test With the samples we have explored so far, we have seen a similar pattern emerging.
We have seen what mostly constitutes a JMeter test plan.
We'll use the remainder of this chapter to explore the anatomy and composition of JMeter tests.
Firstly, it allows you to define user variables (name-value pairs) that can be used later in your scripts.
It also allows us to configure how the Thread Groups it contains should run; that is, should Thread Groups run one at a time? As test plans evolve over time, you'll often have several Thread Groups contained within a test plan.
By default, all Thread Groups are set to run concurrently.
A useful option when getting started is Functional Test Mode.
When checked, all server responses returned from each sample are captured.
This can prove useful for small simulation runs, ensuring JMeter is configured correctly and the server is returning the expected results, but the downside is that JMeter will see performance degradation and file sizes could be huge.
It is set to off by default and shouldn't be checked when conducting real test simulations.
One more useful configuration is the ability to add third-party libraries that can be used to provide additional functionality for test cases.
A time may come when your simulation needs additional libraries, those that are not bundled with JMeter by default.
At such times, you can add those JARs via this configuration.
Thread Groups Thread Groups, as we have seen, are the entry points for any test plan.
They represent the number of threads/users JMeter will use to execute the test plan.
All controllers and samplers for a test must reside under a Thread Group.
Other elements, such as listeners, may be placed directly under a test plan in cases where you want them to apply to all Thread Groups or under a single Thread Group if they only pertain to that group.
Thread Group configurations provide options to specify the number of threads that will be used for the test plan, how long it will take for all threads to become active (ramp up), and the number of times to execute the test.
Each thread will execute the test plan completely independently of other threads.
JMeter spins off multiple threads to simulate concurrent connections to the server.
It is important that the ramp up be long enough to avoid too large a workload at the start of a test, as this can often lead to network saturation and invalidate test results.
If the intention is to have X number of users active in the system, it is better to ramp up slowly and increase the number of iterations.
This allows setting the start and end time of a test execution.
For example, you can kick off a test to run during off-peak hours for exactly 1 hour.
Controllers Controllers drive the processing of a test and come in two flavors: sampler controllers and logical controllers.
JMeter has a comprehensive list of samplers, but we will mostly focus on HTTP request samplers in this book since we are focusing on testing web applications.
Logical controllers, on the other hand, allow the customization of the logic used to send the requests.
For example, a loop controller can be used to repeat an operation a certain number of times, the if controller is for selectively executing a request, and the while controller for continuing to execute a request until some condition becomes false.
As of the time of this writing, JMeter 2.9 came bundled with sixteen different controllers, each serving a different purpose.
Samplers Samplers are components that help send requests to the server and wait for a response.
Requests are processed in the order they appear in the tree.
Each of these has properties that can be tweaked further to suit your needs.
In most cases, the default configurations are fine and can be used as is.
You should consider adding assertions to samplers to perform basic validation on server responses.
Often, during testing, the server may return a status code of 200, indicative of a successful request, but fail to display the page correctly.
At such times, assertions can help to make sure that the request was indeed successful.
Logic controllers Logic controllers help customize the logic used to decide how requests are sent to a server.
They can modify requests, repeat requests, interleave requests, control the duration of requests' execution, switch requests, measure the overall time taken to perform requests, and so on.
At the time of writing, JMeter comes bundled with a total of fifteen logic controllers.
Test fragments Test fragments are a special type of controller purely for code re-use within a test plan.
They exist on the test plan tree at the same level as the Thread Group element and are not executed unless referenced either by an Include or Module Controller.
Listeners Listeners are components that gather the results of a test run, allowing it to be further analyzed.
In addition, listeners provide the ability to direct the data to a file for later use.
Furthermore, they allow allows us to choose which fields to save and whether to use the CSV or XML format.
All listeners save the same data, with the only difference being the way the data is presented on the screen.
Listeners can be added anywhere in the test, including directly under the test plan.
They will collect data only from the elements at or below their level.
JMeter comes bundled with about eighteen different listeners, all serving different purposes.
Though you will often use only a handful of them, it is advisable to become familiar with what each offers to know when to use them.
They are okay to use for debugging and functional testing.
Timers By default, JMeter threads send requests without pausing between each request.
It is recommended that you specify a delay by adding one of the available timers to the Thread Group(s)
This also helps make your test plans more realistic as real users couldn't possibly send requests at that speed.
The timer causes JMeter to pause a certain amount of time before each sampler in its scope.
Assertions Assertions are components that allow you to verify responses received from the server.
In essence, they allow you to verify that the application is functioning correctly and that the server is returning the expected results.
Assertions can be run on XML, JSON, HTTP, and other forms of responses returned from the server.
Configuration elements Configuration elements work closely with a sampler, enabling requests to be modified or added to.
They are only accessible from inside the tree branch where you place the element.
Pre-processor and post-processor elements A pre-processor element, as the name implies, executes some actions prior to a request being made.
Pre-processor elements are often used to modify the settings of a request just before it runs or to update variables that aren't extracted from the response text.
Post-processor elements execute some actions after a request has been made.
They are often used to process response data and extract values from it.
Summary We have covered quite a lot in this chapter.
We have learned how to configure JMeter and our browsers to help record test plans.
In addition, we have learned about some built-in components that can help us feed data into our test plan and/or extract data from server responses.
In addition, we have learned what composes a JMeter test plan and got a good grasp on those components.
In the next chapter, we will dive deeper into form submission and explore more JMeter components.
Submitting Forms In this chapter, we'll expand on the foundations we started building in Chapter 2, Recording Your First Test, and dive deeper into submitting forms in greater detail.
While most of the forms you encounter while recording test plans might be simple in nature, some are a whole different beast and require you to pay them more careful attention.
For example, more and more websites are embracing RESTful web services, and as such, you would mainly interact with JSON objects when recording or executing test plans for such applications.
Another area of interest will be recording applications that make use of AJAX heavily to accomplish business functionality.
Google, for one, is known to be a mastermind at this.
Most of their products, including Search, Gmail, Maps, YouTube, and so on, use AJAX extensively.
Occasionally, you might have to deal with XML response data; for example, extracting parts of it to use for samples further down the chain in your test plan.
You might also come across cases when you need to upload a file to the server or download one from it.
For all these and more, we will explore some practical examples in this chapter and gain some helpful insights as to how to deal with these scenarios when you come across them as you prepare your test plans.
Capturing simple forms We have already encountered a variation of form submission in Chapter 2, Recording Your First Test, when we submitted a login form to authenticate with the server.
The form had two text fields for username and password respectively.
Most websites requiring authentication will have a similar feel to them.
These include checkboxes, radio buttons, select and multiselect drop-down lists, text areas, file uploads, and so on.
In this section, we take a look at handling other HTML input types.
We have created a sample application we will be using throughout most of this chapter to illustrate some of the concepts we will be discussing.
Take a minute to browse around and take it for a manual spin so as to have an idea what the test scripts we record will be doing.
Handling checkboxes Capturing checkbox submission is similar to that of capturing textbox submissions, which we encountered earlier in Chapter 2, Recording Your First Test.
Depending on the use case, there might be one or more related/unrelated checkboxes on a form.
With your JMeter proxy server running and capturing your actions, perform the following steps:
At this point, if you examine the recorded test plan, the /form1/submit post request has parameters for the following:
Finally, we can expand the test plan further by parsing the response from the /form1/create sample to determine what hobbies are available on the form using a post processor element (for example, the regular expression extractor) and then randomly choosing one or more of them to submit.
Handling radio buttons Radio buttons are normally used as option fields on a web page; that is, they are normally grouped together to present a series of choices to the user, allowing them to select one per group.
Things such as marital status, favorite food, and polls are practical uses of them.
Capturing their submission is quite similar to dealing with checkboxes, except that we will have just one entry per submission for each radio group.
Our sample at http://jmeterbook.aws.af.cm/radioForm/index has only one radio group, allowing users to identify their marital status.
Hence, after recording this, we will only have one entry submission for a user.
Viewing the HTML source of the page (right-click anywhere on the page and select View Source) will normally get you the "IDs" the server is expecting back for each option presented on the page.
Armed with that information, we can expand our input test data, allowing us to run this same scenario for more users with varying data.
As always, you can use a post-processor component to further eliminate the need to send the radio button IDs in your input feed.
Handling a drop-down list is no different to this scenario.
Handling all other forms of HTML input types; for example, text and text area fall under the categories we have explored thus far.
Handling file uploads You may encounter situations where uploading a file to the server is part of the functionality of the system under testing.
In addition to checking the option to make a post request multipart, you will need to specify the absolute path of the file, in cases where the file you are uploading is not within JMeter's bin directory, or the relative path in cases where the file resides within JMeter's bin directory.
Choose a file to upload by clicking on the Choose File button.
Note that files to be uploaded can't be larger than 1 MB.
Depending on the location of the file you choose, you might encounter an error similar to the following:
Do not be alarmed! This is because JMeter is expecting to find the file in its bin directory.
You will have to either tweak the file location in the recorded script to point to the absolute path of the file or place it in the bin directory or a subdirectory.
Handling file downloads Another common situation you may encounter will be testing a system that has file download capabilities exposed as a function to its users.
Users, for example, might download reports, user manuals, and documentation from a website.
Knowing how much strain this can put on the server could be an area of interest to stakeholders.
JMeter provides the ability to record and test such scenarios.
As an example, let's record a user retrieving a PDF tutorial from JMeter's website.
You could add a View Results Tree listener and examine the response output after playing back the recording.
You could also add a Save Responses to file listener and have JMeter save the contents of the response to a file you can later inspect.
This is the route we have opted for in the sample recorded with the book.
Files will be created in the bin directory of JMeter's installation directory.
Also, using a Save Responses to file listener is useful for cases when you would like to capture the response, in this case a file, and feed it to other actions further on in the test scenario.
For example, we could have saved the response and used it to upload the file to another section of the same server or a different server entirely.
Posting JSON data REST (REpresentational State Transfer) is a simple stateless architecture that generally runs over HTTP/HTTPS.
Requests and responses are built around the transfer of representations of resources.
It emphasizes interactions between clients and services by providing a limited number of operations (GET, POST, PUT, and DELETE)
Flexibility is provided by assigning resources their own unique universal resource indicators (URIs)
Since each operation has a specific meaning, REST avoids ambiguity.
In modern times, the typical object structure passed between client and server is JSON.
More information about REST can be found at http://en.wikipedia.org/ wiki/REST.
When dealing with websites that expose RESTful services in one form or another, you will most likely have to interact with JSON data in some way.
Such websites may provide means to create, update, and delete data on the server via posting JSON data.
URLs could also be designed to return existing data in JSON format.
This happens even more in most modern websites, which use AJAX to an extent, as we use JSON mostly when interacting with AJAX.
In all such scenarios, you will need to be able to capture and post data to the server using JMeter.
JSON, also known as JavaScript Object Notation, is a text-based open standard designed for human readable data interchange.
You can find out more information about it at http://en.wikipedia.org/wiki/JSON and http://www.json.org/
For this book, it will suffice to know what the structure of a JSON object looks like.
Some basic rules of thumb when dealing with JSON are as follows:
The second sample shows a person named Barry White, born on 9/1/1965, who is both a doctor and fireman.
Now that we have covered a sample JSON structure, let's examine how JMeter can help with posting JSON data.
The example website provides a URL to save the Person object.
A person has a first name, last name, and date of birth attributes.
So a valid JSON structure to store a person might look like the following code:
Instead of recording, we will manually construct the test scenario for this case; we have intentionally not provided a form to save a person's entry so as to give you hands-on practice in writing test plans for such scenarios.
Fill in the properties of the HTTP Request Sampler as:
Under Send Parameters with Request, click on Add and fill in the attributes as follows:
Add an attribute to HTTP Header Manager by clicking on it, and clicking on the Add button.
If you have done everything correctly, your HTTP Request Sampler should look like the following screenshot:
Now you should be able to run the test, and if all was correctly set, Bob Jones should now be saved on the server.
You can verify that by examining the View Results Tree listener.
The request should be green and in the Response data tab, you should see Bob Jones listed as one of the entries returned.
Even better yet, you could view the last ten stored entries in the browser directly at http://jmeterbook.
Of course all other tricks we have learned thus far apply here as well.
We can use a CSV Data Config element to parameterize the test and have variation in our data input.
Regarding input data variation, since jobs are optional for this input set, it might make sense to parameterize the whole JSON string read from input feed to give you more variation.
Although simplistic in nature, what we have covered here should give you all of the information you need to post JSON data when recording your test plans.
When dealing with RESTful requests in general, it helps to have some tools handy to examine requests, inspect responses, and view network latency, among many others.
The following is a list of handy tools that could help:
Firebug (an add-on that is available with Firefox, Chrome, and IE): http:// getfirebug.com/
Reading JSON data Now that we know how to post JSON data, let's take a brief look at how to consume it in JMeter.
Depending on the use case, you might find yourself dealing more with reading JSON than posting it.
JMeter provides a number of ways to digest this information, store it if needed, and use it further down the chain in your test plans.
The example website has a link that provides details of the last ten person entries stored on the server.
If we were to process the JSON response and use the first and last name further down the chain, we could use a Regular Expression Extractor post processor to extract those.
Fill in the properties of the HTTP Request Sampler as follows:
The interesting bit here is the cryptic regular expression we are using here.
It basically says to match words and store them in the variable defined as name.
The \w+? regular expression instructs the pattern engine not to be greedy when matching and to stop on the first occurrence.
The full capabilities of regular expressions are beyond the scope of this book, but we encourage you to master some as they will help you while scripting your scenarios.
For now, just believe that it does what it says.
Once you execute the test plan, you will be able to see the matches in the debug sampler of the View Results Tree.
Here's a snippet of what you should expect to see:
Using the BSF PostProcessor When dealing with much more complicated JSON structures, you might find that the Regular Expression Extractor post processor just doesn't cut it.
You might struggle to come up with the right regular expression to extract all the info you need.
Examples of that might be deeply nested object graphs that have an embedded list of objects in them.
At such times, a BSF PostProcessor will fit the bill.
This opens a whole realm of possibilities, allowing you to leverage the knowledge and power of scripting languages within your test plan while still retaining access to Java class libraries.
Scripting languages supported within JMeter at the time of writing include AppleScript, JavaScript, BeanShell, ECMAScript, and Java to name a few.
Let's jump right in with an example of querying Google's search service.
Fill in the properties of the HTTP Request Sampler as follows:
Once saved, you can execute the test plan and see the full JSON returned by the request and the extracted values that have now been stored as JMeter variables.
If all is correct, you should see values similar to the following:
The BSF PostProcessor exposes a few variables that can be used in your scripts by default.
In our preceding example, we have used two of them (prev and var)
A quick run down of the code is as follows:
Retrieves the response data of the previous sampler as a string and uses the JavaScript eval() function to turn it into a JSON structure.
Once a JSON structure has been extracted, we can call methods like we normally would in JavaScript.
This gets the size of the results that were returned and stores the result in a JMeter variable called url_cnt.
Handling the XML response Yet another structure you may encounter as you build test plans is XML.
Some websites may hand off XML as their response to certain calls.
For example, we could get our test application to return an XML representation of the person list we were working with earlier in this chapter by making a call to http://jmeterbook.aws.af.cm/person/list?format=xml.
Describing XML in detail goes beyond the scope of this book, but you can find much more about it online.
For our exercise, it will suffice just to know what it looks like.
Have a look at the XML returned by the previous link.
Now that you know what XML looks like, let's get going with a sample test plan that deals with retrieving an XML response and extracting variables from it.
Our goal is to extract all the artifactId elements (deeply nested within the structure) into variables that we can then use later in our test plan, if we choose.
Fill in the properties of the HTTP Request Sampler as follows:
Once saved, you will be able to execute the test plan and see the artifact_id variables in the View Results Tree listener.
The only new element we have used here is the XPath Extractor post processor.
This nifty JMeter component allows you to use the XPath query language to extract values from a structured XML or (X)HTML response.
This will look for the tail element (artifactId) of the query string within the following structure:
Now you know just how to get at the information you need when dealing with XML responses.
Summary In this chapter, we have gone through the details of how to capture form submission in JMeter.
The same concepts covered in those sections can be applied to other input form elements such as text areas and comboboxes.
We then explored how to deal with file uploads and downloads when recording test plans.
Along the way, we addressed working with JSON data, both posting and consuming it.
Finally, we took a look at how to deal with XML data when we encounter it.
For that, we covered yet another post processor JMeter offers, XPath Extractor PostProcessor.
You should now be able to use what we have learned so far to accomplish most tasks you need to accomplish with forms while planning and scripting your test plans.
In the next chapter, we will dive into managing sessions with JMeter.
Managing Sessions In this chapter, we'll cover session management in JMeter in detail.
Web applications, by their very nature, use client and server sessions.
Both work in harmony to give each user a distinct enclosure to maintain a series of communication with the server without affecting other users.
For example, in Chapter 2, Recording Your First Test, the server session was created the moment a user logged in to the application, and maintained for all requests sent to the server by that user until he/she logged off or timed out.
This is what protects other users from seeing each other's information.
Depending on the application's architecture, the session may be maintained through cookies (most commonly used) or URL rewriting (less commonly used)
The former maintains the session by sending a cookie in the HTTP headers of each request while the latter rewrites the URLs to append the session ID.
The main differences are that the former relies on a client's browser choosing to accept cookies and is transparent to the application developer, while the latter isn't transparent and works regardless of if cookies are enabled or not.
That said, diving into the details of the two modes goes beyond the scope of this book, but we would encourage you to spend some time reading some online resources to gain better understanding if you are the curious type.
For this book, it will suffice to know that there are two modes and that JMeter handles both.
Let's dig right in and explore these scenarios and see how JMeter deals with each.
Managing sessions with cookies A majority of web applications rely on cookies to maintain the session state.
In the very early stages of the Internet, cookies were only used to keep the session ID.
Things have since evolved and cookies now store a lot more information, such as user IDs and location preferences.
The banking application we used as a case study in Chapter 2, Recording Your First Test, for example, relies on cookies to help each user maintain a valid session with the server, enabling the user to make a series of requests to the server.
An example will help clear things up, so let's get right to one.
For our example, some resources are protected based on the role of the user that is logged in.
The steps to manage sessions with cookies are as follows:
Attempting to execute the recorded scenario upon saving it will not yield the expected results.
Once the simulation is run, examine the responses from the server through the View Results Tree listener.
Even though all responses are green, indicating successful requests (since we got a response code of 200 from the server), we are actually still just getting back the login page after successfully logging in (see the Response tab of View Results Tree for subsequent requests after successful authentication)
If you examine the Request tab, you will see the reason for that.
Following is a snippet of the Request data of the login process.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.
First, there is a [no cookies] line present,  indicating JMeter didn't find any stored cookie to use for this request.
Second is the jsessionid cookie in the first line of the request.
The server uses this to group all requests from a user under the same session ID, once authentication is established.
If you compare this with the subsequent calls in View Results Tree, you will notice different jsessionid values, further indicating that the server is treating those subsequent calls as new requests and not associating it with a previous request.
Thirdly, the URL for subsequent calls also mimics what we saw earlier in http://jmeterbook.
All this is evidence that JMeter is not currently managing the session appropriately.
But how can it? We have not instructed it to.
JMeter comes with a couple of components to help maintain sessions.
Since our sample here relies on cookies to maintain sessions, we will use the HTTP Cookie Manager component.
This component stores and sends cookies just as web browsers do.
If an HTTP request and response contains a cookie, the Cookie Manager automatically stores that cookie and will use it for all future requests to the application.
Since a thread is synonymous to a user in JMeter, each thread has its own cookie storage area, giving us the ability to run multiple users for a simulation with each maintaining a separate session.
Let's go ahead and add a Cookie Manager to our test plan.
This component allows you to define additional cookies, but the default will usually suffice except in cases where your application might be doing something tricky.
Once that is added, if we rerun our test plan and examine the Request tab, we will see a different outcome.
This time, the jsessionid cookie is stored and maintained across requests and the [no cookie] line is gone.
Here is a snippet of the two subsequent requests in View Results Tree:
Notice that the same session ID is maintained across the requests.
If you examine the Response data, you will see that we are now able to access the intended protected resources.
Refer to the following screenshot, which shows how to use the HTTP Cookie Manager component to define additional cookies:
This completes our exploration of the HTTP Cookie Manager element.
It is possible to have more than one Cookie Manager in a test plan depending on the application needs.
For example, if you have multiple thread groups within a test plan, it is possible to have a Cookie Manager per thread group.
If there is more than one Cookie Manager in the scope of a sampler, there is no way to specify which will be used.
Also, a cookie stored in one Cookie Manager is not available to any other manager, so exercise caution when using multiple Cookie Managers.
Managing sessions with URL rewriting In the absence of cookie support, the alternative method web applications use to manage session information is a technique known as URL rewriting.
With this approach, the session ID is attached to all URLs that are within the HTML page that is sent as a response to the client.
This ensures that the session ID is automatically sent back to the server as part of the request, without the need to put it in the header.
The advantage of this technique is that it works even if a client browser has cookies disabled.
Let's examine a sample and see how JMeter comes to the rescue.
Click on Another Link (at the bottom of the page)
Click on the jmeter-book link on the banner on the navigation bar at the top.
If you re-execute the test plan after saving it, you'll notice that all the links have a jsessionid cookie appended to them.
This ensures that the same session ID is sent along to the server, thereby treating our series of request as one whole conversation with the server; in short, our session is maintained.
Since we recorded this, the session ID sent with all the requested links is the one the server generated at the time we recorded.
Obviously, we will need to turn this into a variable that can then be used for multiple threads, as each new thread will be treated as a new user with each getting their own unique session ID.
This component is similar to the HTML Link Parser modifier except that its specific purpose is to extract session IDs from the response; that is, a page or link.
See the following screenshot to see what the configuration elements are.
This allows you to specify the session ID parameter name to grab from the response.
Java web applications, for example, usually have this as jsessionid (as in our case) or JSESSIONID.
Web applications that are not written in Java might have a variation of this; for example, SESSION_ID.
Inspect the application under test and see what key the session ID is getting stored in.
Refer to the following screenshot to see the configuration elements of the HTTP URL Re-writing Modifier:
Path Extension: If checked, a semicolon will be used to separate the session ID and the argument URL.
Java web applications fall into this category, so go ahead and check it for our sample.
Do not use equals in path extension: If checked, omits the use of = when capturing the rewrite URL.
Java web applications use =, so we leave this unchecked.
Do not use questionmark in path extension: This prevents the query string from ending up in the path extension.
Cache Session Id: Saves the value of the session ID for later use, when it is not present, for example, in subsequent page requests.
We want the same session ID sent for all page requests by a thread/user.
The last thing to clean up before we rerun our test plan is the already existing session IDs that were captured during our recording.
Go through each sampler and delete that from the URL request paths.
This will be captured by the HTTP URL Re-writing Modifier component and appended to subsequent calls automatically.
At this point, we are ready to rerun our sample and see the outcome.
Remember to add a View Results Tree listener to the plan if you haven't already done so.
Once run, we should be able to verify that the outcome is what we expected.
The same session ID should be maintained for subsequent requests from a user.
Although we have placed the element at the Thread Group level, it can also be placed at the sampler level.
In such a case, it will modify only that request and not affect subsequent calls.
This wraps up the different ways in which we can manage sessions with JMeter.
The web applications you test will normally fall under one of these two major categories, cookie management or URL rewriting.
Based on your needs, JMeter provides components to help manage sessions for both.
Summary In this chapter, we have covered how JMeter helps manage web sessions for your test plans.
First we examined the most common way web applications manage sessions, using a cookie.
For these cases, JMeter provides a component called HTTP Cookie Manager, whose primary job is to help capture the cookie generated by the server and store it for future use during test execution.
We then explored web applications that use URL rewriting to maintain sessions as opposed to cookies.
This led us to the HTTP URL Re-writing Modifier, another component JMeter provides for handling these cases.
In conclusion, what we have covered here should suffice in helping you effectively manage sessions as you build test plans for your own applications.
Resource Monitoring So far, we have seen how JMeter can help with conducting performance testing.
In this chapter, we will explore what it offers in terms of resource monitoring.
Resource monitoring is a broad subject that covers analyzing system hardware usage, which includes CPU, memory, disk, and network.
As you conduct testing, it is important to know how each of these resources are behaving under load to better understand if there are bottlenecks and address them accordingly.
Most organizations have dedicated teams (for example, network and system engineers) for configuring and monitoring these resources.
In addition, there are dedicated tools for monitoring and analyzing them.
We have said all that to say that what JMeter offers pales in comparison to what you will get using such dedicated tools.
Moreover, not all companies can afford such tools or have personnel in charge of setting up adequate monitoring.
You just might be a one-man shop doing testing and monitoring all by yourself!
Since this is a book on JMeter, let's see how we can go about doing some resource monitoring with it.
Basic server monitoring JMeter comes with an out-of-the-box monitoring controller.
This allows you to monitor the general health of the application or web server.
Metrics such as active threads, memory, health, and load are gathered and reported in a graphical form.
Having such metrics makes it easier to see the relationship between server performance and response time on the clients.
Multiple servers can be monitored using a single monitor controller.
Although originally designed to work with the Apache Tomcat server (http://tomcat.apache.org/), any servlet container (http://en.wikipedia.org/wiki/Servlet_container) supporting JMX (Java Management Extension) can port the Tomcat status servlet to provide the same information.
Providing such ports for other servers goes beyond the scope of this book, so we will stick to using Apache Tomcat for our use case.
Monitoring servers during test executions helps identify potential bottlenecks in the application or system resources.
It can draw focus to long-running queries, insufficient thread and data source pools, insufficient heap size, high I/O activity, server capacity inadequacies, slow-performing application components, CPU usage, and so on.
All these are important to troubleshooting performance issues and attaining the targeted goals.
To get started, we first need a server to monitor.
Let's download Apache Tomcat and get it up and running.
At the time of writing, version 7.0.37 was the latest.
That is what we will use for our purposes, though an older version should work just as well.
If all goes OK, the server should start up and you should see something similar to the following on the console:
Please refer to the Apache Tomcat documentation for more details, at http://tomcat.apache.org/tomcat-7.0-doc/setup.html.
Go to http://localhost:8080 and verify that you are greeted with the Apache Tomcat home screen.
Congratulations, your server is now up and running! To monitor it, we need to perform one more step on the server.
We need to set up at least one user account with the proper role on the server to get us the information we need.
The account we set up will later be used when we configure the monitoring controller in JMeter.
Configuring Tomcat users The following are the steps to configure Tomcat users:
This creates a user named admin with password admin for authenticating with the Tomcat manager application.
Finally, with the server configuration behind us, we can now proceed with setting up JMeter to monitor the server.
Setting up a monitor controller in JMeter The following are the steps to set up a monitor controller in JMeter:
Assuming you haven't changed the default server ports of Tomcat, the provided test plan should work right off the bat.
Before starting the provided test plan, let's change the monitor test plan to loop forever, so we can watch the server metrics as activity continues on the server.
For the monitor test plan, click on Thread Group and check the forever box for the loop count.
For the constant timer in the monitor test plan, intervals shorter than 5 seconds add stress to the server.
You should consult with infrastructure engineers in your company (if any) to see what an acceptable interval might be before configuring monitoring for a production environment.
As a rule of thumb, 5 seconds is a decent number.
To run the provided test plan alongside the monitoring test plan, you need to launch another instance of JMeter and open the provided test plan in it.
So, without further ado, let's kick off the monitor test plan and then execute the provided test plan to put stress on the server and see the monitoring results.
If it has all been set up properly, you should see some results starting to show up under the monitor results listener.
The Health tab might look similar to the following screenshot, and the Performance tab like the screenshot after that.
As you can see from the following screenshot, our run gradually progressed from healthy and stopped at active at the end of the simulation run.
We didn't get to warning or dead levels, which is a good sign our server stayed healthy overall.
In the following screenshot, you can see the memory (represented by the yellow line) and load (represented by the blue line) gradually spike up during our simulation.
The thread percentage (represented by the red line) and health (represented by the green line) also stayed at healthy levels, consistent with what we observed.
That wraps up our look into basic monitoring with JMeter.
In the next section, we will see how we can leverage JMeter's plugin architecture and use a plugin to provide even more granular monitoring metrics for our needs.
Monitoring the server with a JMeter plugin So far we have examined how we can use the inbuilt server monitoring capabilities of JMeter to monitor server health.
While this might be OK for basic needs, it falls short for advanced needs.
For instance, the graphs generated don't provide CPU and disk I/O metrics that could be deemed critical for your analysis.
To get such metrics, you could extend JMeter with a suite of plugins that give better results.
The project provides additional samplers, graphs, listeners, and so on, all of which make it easier to work with JMeter.
In this section, we will install this suite of plugins and use the monitoring capability it provides to get better metrics.
Installing the plugins The plugin comes with three archives, all of which must be extracted to different destinations.
At the time of writing, the project was at version 1.0.0, which is what we will be working with.
This archive contains additional third party JARs used by some of the custom plugins provided.
This archive contains server resource monitoring agents to use with the PerfMon Metrics Collector plugin standalone utility.
With those steps, we have installed a whole suite of plugins, adding new features to JMeter.
If you were to relaunch JMeter now, you will notice additional samplers, listeners, timers, and so on, all beginning with jp@gc to distinguish them from the bundled ones.
Let's start the server agent, which will feed the JMeter listener probe we will add to our test plan later.
You should get logs similar to the following ones if the agent has started successfully:
As you can see, the agent has started on port 4444, the default.
We will use this port later when configuring the monitor listener for JMeter.
If this port is not satisfactory for you, the plugin provides configuration files that can be edited to choose a desired port.
With the server agent running, let's add a few monitor listeners to our test plan.
For our purposes, we have chosen the sample test plan we recorded using the samples provided by Apache Tomcat.
Please note that this same concept can be applied to other applications deployed on the same server where the monitor agent has been installed.
Add one row each to gather these metrics (CPU, Memory, Network I/O, and Disks I/O)
With the server agent running, and our additional monitor listeners set up, we are ready to kick off the simulation execution.
While it's executing, you can see the graphical representation of the metrics you have chosen to analyze if you click on jp@gc - PerfMon Metrics Collector.
As you can see from the following screenshot, CPU is spiking up and down, showing quite a decent load on the server.
Memory stays almost constant while network, which is relatively stable, spiked quite high two minutes into our simulation run.
It immediately dropped down, back to the low ranges after the spike, so something might have transpired on the network at the time of the execution run, causing such a spike.
Since this test plan doesn't involve any disk I/O, it stays on 0 for the duration of our simulation.
Threads listener shows a true picture of how much time the server takes to service each request in relation to the number of executing threads.
The graph can be a bit messy to read, so in the Rows tab, you can check only the requests you are interested in analyzing.
We have done just that in the following screenshot, and chose only a handful of requests.
Though the graph isn't shown here, the last listener we added was the Transactions per Second listener.
It shows just how many requests (transactions) the server was able to handle during the course of our simulation run on a second-by-second basis.
Like the Response Times vs Threads listener, the chart can be messy and you will need to selectively choose which requests you were interested in to make some sense of the graph.
As you can see, these new listeners, along with the server agent, allow you to monitor resources in far greater detail than with those shipped with JMeter.
In addition to the metrics we gathered, you can choose to gather additional ones including swap, TCP, and JMX if those were areas of concern.
By and large, we can use this to effectively monitor resources on the server.
Although we have only set this up for one server, the monitor can be set up to monitor multiple servers; for example, in cases where you have a cluster of servers.
Summary In this chapter, we walked through how JMeter can help with monitoring server resources.
To do that, we set up an Apache Tomcat server.
Once done, we examined the built-in capabilities of JMeter with regards to monitoring.
We further examined how we could get more granular monitoring metrics by extending JMeter with custom-developed plugins.
This allowed us to monitor server resources such as CPU, disk I/O, memory, and network I/O, among other things.
Through the plugin, we also got additional samplers, timers, processors, and listeners that allowed us to monitor transactions per second and response time versus thread metrics.
Though not an extensive monitoring tool, JMeter proved itself a capable tool to do basic monitoring of server resources.
In the next chapter, we will go into depth on distributed testing and see how to leverage the capabilities of JMeter to accomplish this.
Distributed Testing There will come a time when running your test plans on a single machine won't cut it any longer performance-wise, since resources on the single box are limited.
For example, this could be the case when you want to spin-off a thousand users for a test plan.
Depending on the power and resources of the machine you are testing on, and the nature of your test plans, a single machine can probably spin-off with 300-600 threads before starting to error out or causing inaccurate test results.
One is because there is a limit to the amount of threads you can spin-off on a single machine.
Most operating systems guard against complete system failure by placing such limits on hosted applications.
Also, your use case may require you to simulate requests from various IP addresses.
Distributed testing allows you to replicate tests across many low-end machines, enabling you to start more threads and thereby simulating more load on the server.
In this chapter, we will learn how to leverage JMeter for distributed testing and put more load on the server under test in the process.
Remote testing with JMeter JMeter has inbuilt support for distributed testing.
This enables a single JMeter GUI instance, known as the master, to control a number of remote JMeter instances, known as slaves, and collect all the test results from them.
Replicating the test plan from the master node to each controlled server.
Each server will execute the same test plan in its entirety.
Though the test plan is replicated across to each server, the data needed by the test plan, if any, is not.
In cases where input data such as CSV data is needed to run the tests, such data needs to be made available on each server where the test plan will be executed.
The remote mode is more resource intensive than running the same number of non-GUI tests independently.
If many server instances are used, the client's JMeter can become overloaded, as can the client's network connection.
It is important for all slave nodes and the master node to run the same version of JMeter, and if possible the same version of the Java Runtime Environment.
Mostly, minor JRE variations are fine, but not major ones.
Configuring JMeter slave nodes There are a number of ways to get the slave nodes going.
In this section, we will go over two options that will often fit the bill for accomplishing your goals.
The most obvious is to go out and buy new machines just for this purpose.
Another option is to get hold of extra computers lying around in the office, configure them appropriately, and use them for this purpose.
While that will work perfectly, it might be time-consuming to get all of the boxes set up without the appropriate tools, knowledge, and expertise.
Yet another option is to use virtual machines to accomplish the same outcome.
This is the option we will be focusing on in this section.
We don't necessarily need another physical machine to try out distributed testing.
We can leverage Vagrant and Puppet, excellent infrastructure automation tools, to set up virtual boxes with the required software with little interaction from us.
We can be up and running with a few virtual machines in less time than it takes to run to your local coffee shop and grab a cup of coffee.
The same concepts can be applied to leverage machines in the cloud.
In case you haven't heard of Vagrant before, don't be alarmed.
It's an excellent tool that makes building development environments easy.
It allows you to create and configure lightweight, reproducible, and portable development environments.
Elaborating on the uses of Vagrant and Puppet go beyond the scope of this book, but I would encourage you to read more about them at http://www.vagrantup.
At the time of writing, version 1.1.4 was the latest and that is what we will be using in this chapter.
For the book, We have prepared the necessary scripts needed to provision boxes.
The only requirement to use the scripts is to have Oracle's VirtualBox installed on your machine.
At the time of writing, VirtualBox was at Version 4.2.10 and that is what we have installed.
With both Vagrant and VirtualBox installed, we are ready to configure our distributed testing environment.
Configuring one slave per machine In this configuration, we are going to set up three slave machines and control them with one master client.
This will mimic having four separate physical machines with one of them acting as master (where the JMeter GUI client runs) and the other three acting as slave nodes (where the JMeter server scripts are kicked-off)
In a few moments, a fully functional VirtualBox will be created with JMeter installed and ready to run! You should see logs similar to the following:
Don't take our word for it though; verify that the box is properly configured by performing the following on the command line (from the node_one folder):
This should show you the version of JMeter that you are running on the guest machine.
This is because the server is returning an IP address of 127.0.1.1, which is considered a loop back address.
To get the assigned IP address from the newly created virtual machine, run this on the command line:
The line of interest here is the line containing 192.168.x.x.
Look for the line beginning with RMI_HOST_DEF and add the following just below it:
Be sure to replace 192.168.1.27 with the assigned IP address of your own virtual box.
Save the file (by pressing Esc and then typing :wq) and this machine is ready to act as a server.
Before we configure a second node, it would be wise to take the first for a spin.
This time it should succeed and you should see something similar to the following on the console:
Let's go right ahead and configure the master to control it.
Configuring the master node to be tested against one slave per machine Now that we have one slave node configured, we can test it out by configuring the master node to connect to it and control it.
To do that, we will have to add the slave node's IP address to the master's node configuration file.
On the host machine (where the JMeter GUI client is running), perform the following steps:
By clicking on the Slave IP address, the master node will make a connection with the remote server running on the VirtualBox.
You should see a similar log on the client and the server.
The following will appear on the JMeter GUI client console:
Congratulations! We are now able to control this slave node from the master.
We can proceed with testing at this point, but since we are focusing on distributed testing in this chapter, it will help to have two or more nodes to control.
At the end of it all, we should have three slave nodes we can control from the master node.
Now your JMeter GUI client should have three server IP addresses which you will find by navigating to Run | Remote Start and you can either kick-off an individual server node by targeting the server IP address of choice or start all the configured slave nodes at once by navigating to Run | Remote Start All (Command + Shift + R on Mac or Ctrl + Shift + R on Windows)
When starting all the configured node servers, if everything has been properly configured, you should see logs similar to the following ones on the master console with each server node responding and acknowledging the kicking-off of the intended test plan:
With the connections verified, we can now pick a test plan to run and gather the results on the master node.
For our first test, we are going to execute a test that doesn't require input data.
It does no data entry and therefore doesn't need any input data.
Load that into the master node's JMeter GUI and kick it off on all the slave nodes.
Your mileage may vary depending on the computing power of your machine.
It should be noted that in our case we are still running all these virtual slave nodes on a single box, so the resources are still limited.
That is, all the slaves are still sharing the resources of the host machine.
Therefore, attempting to distribute more load than could originally be handled by the host machine can lead to degraded performance with high response times.
However, nothing prevents you from running the provided Vagrant scripts on additional physical machines to simulate more load without worrying about constrained resources.
The second test is one we have seen before, in Chapter 2, Recording Your First Test.
It's the Excilys banking application that requires an input data file.
As JMeter only sends the test plans to slave nodes, we need to get the input files across to all the slave nodes in order to successfully execute the test.
To do that, perform the following steps on the command line:
This puts the users2.txt file, which is needed by the test plan, in a location that can be seen by the JMeter server on the slave nodes.
Feel free to increase the number of threads, ramp-ups, and iterations, but please be careful not to crash the server.
Configuring multiple slave nodes on a single box JMeter allows you to configure multiple slave nodes on a single box as long as they are configured to broadcast on different RMI ports.
This could come handy in cases where the machine you are using is powerful enough to handle it or you don't have access to additional physical machines.
Just as in the previous section, we will be using Vagrant to configure a single virtual machine and spin off multiple JMeter slave nodes on it.
For this illustration, I have prepared a Vagrant script with Puppet provisioning, similar to what we used in the last section.
These are the different ports that will be used by the different slave nodes when starting the server.
At this point, you should see the three slave nodes present on the machine.
From the VirtualBox, run the following on the command line:
This will provide you the assigned IP address of the box.
Edit the jmeter-server script to add the box's IP address using the following steps:
Save and close the file (press Esc and type :wq)
To start the jmeter-1 slave node in a new shell/console, perform the following steps:
Start the JMeter server on the default port, 1099, by using the following:
To start the jmeter-2 slave node in a new shell/console, perform the following steps:
To start the jmeter-3 slave node in a new shell/console, perform the following steps:
Configuring the master node to be tested against multiple slave nodes on a single box With the slave nodes configured, we need to configure the master node to communicate with them before we can proceed with executing our tests remotely.
To do that, we have to add the slave nodes' IP addresses and ports to the master node's configuration file.
On the host machine (where the JMeter GUI client is running), perform the following steps:
With that done, we are ready to kick off our tests as we did in the previous section.
The only difference now is that all our slave nodes are configured on one virtual host.
Now kick off the test remotely on all slave nodes.
If you compare the results of this run with the previous run that had slaves configured on separate virtual boxes, you will see quite an increase in the response times.
The following screenshot shows the results we got from our run:
One conclusion that can be drawn from these results is that spinning off multiple slave nodes on a machine is not always optimal and should not be your first choice.
Your mileage may vary based on the capacity of the machine you use.
Leveraging the cloud for distributed testing So far, we have seen how we can distribute load to various physical or virtual machines and by so doing achieve more load than could ever be possible with a single machine.
Our setup thus far, though, has been internal to our network using a master/slave configuration.
Sometimes, it helps to isolate any artificial bottlenecks occurring on the LAN and run your tests from more realistic locations external to your network.
This has the added benefit of leveraging substantially larger hardware at minimal cost thanks to the various cloud offerings now at our disposal.
Another area worth considering is the master/slave setup we have employed thus far.
While this will work perfectly fine when few slaves are configured, as more slaves get added to the mix, the master node becomes a huge bottleneck.
This shouldn't come as a surprise since I/O and network operations increase as more and more slave nodes try to feed ongoing testing results to the master.
What would be most efficient and ideal is to have each slave node run its test in isolation in the non-GUI mode, save the results, and its cumulative results from all the slave nodes gathered at the end of the test.
The challenge of course is kicking off all the test executions on all the nodes in harmony and gathering the results from each.
That could be a little bit daunting, not to mention time-consuming.
Thankfully, we can use Vagrant, our Swiss-Army-knife environmental setup tool, to get partly there.
We will employ it to start server instances on AWS (Amazon Web Service), set up the Java Runtime Environment (JRE), JMeter, and upload our test scripts to the cloud virtual machines we bring up.
Amazon has an excellent variety of cloud services that make it easy to run your whole company's infrastructure in the cloud, if you so choose.
Provided the application under test is accessible from outside your corporate network, the methods described here should suit your needs just fine.
The first step is to register for an AWS account, if you don't already have one.
You can do that by going to http://aws.amazon.com/ and clicking on the Sign up button.
Once registered, you'll need to obtain your access key, secret key, and a key pair to use for authenticating with the machines you create on AWS.
Obtaining your access key, secret key, and key pair To obtain your access key, secret key, and key pair, perform the following steps:
In the upper right-hand corner, click on the My Account/Console drop-down list.
Generate a key pair by following the instructions at http://docs.aws.
If you use the browser, it will create a key pair and automatically download the private key for you.
Copy or move it to a location of your choice.
We will use the name and location created here later.
With all that done, we are ready to start launching some instances in the cloud! See the following screenshot for details:
At the time of writing, for a small instance that we used during the course of this section, it costs $0.10/hr for each instance, which is not bad considering all the effort it saves getting a box, setting it up, and doing it multiple times.
We have prepared a Vagrant script with Puppet provisioning, just as we did in the previous sections.
The only difference this time is that is it configured to work with AWS as opposed to virtual boxes in our intranet.
To use it, you need to install the Vagrant AWS plugin.
Do that by running the following from the command line:
This simple one liner makes Vagrant AWS aware, and now it understands how to interact with machines on AWS.
We can now transparently spin off virtual machines on Amazon's infrastructure just as we did with VirtualBox locally.
By running the vagrant plugin install command, we assume you have already installed Vagrant on the machine where this operation is performed.
Launching an AWS instance With the Vagrant AWS plugin installed, the next step is to follow these listed instructions:
These are values as generated in the previous section, Obtain your access key, secret key, and key pair.
You will see a bunch of entries (similar to what follows) written to the console and the whole process could take up to a minute or two depending on network latency, Internet speed, communication with AWS, among others.
Verify that you are able to connect to the box and that JMeter was successfully installed on the machine.
Now our first VirtualBox is up and running, ready to execute our test plans.
Start up three additional console/shell windows, one for each additional virtual machine we will bring up.
In each of the new shell windows, bring up an additional box running the following commands:
Verify that each of them is properly set up, just as we did for the first virtual machine.
With all four machines running, we are ready to proceed with executing our test plan.
Executing the test plan Since we are not using a master/slave node configuration in this section for reasons described earlier, we'll need to execute the following command on all four virtual machines simultaneously as best we can.
To execute our test plan, run the following on the virtual boxes:
On vm1, type (or copy) the following on the console:
On vm2, type (or copy) the following on the console:
On vm3, type (or copy) the following on the console:
On vm4, type (or copy) the following on the console:
Each virtual machine will print simulation results to CSV files.
Now that all the consoles are ready, press Enter on your keyboard in each console to execute the test plan on each virtual machine.
You should see a log similar to the following on each console:
You should be able to verify the results file was generated by listing the contents of the current directory using the ls -l command.
Viewing the results from the virtual machines To view the results, we need to grab the files from each host machine and then concatenate them together to form a composite whole.
We can then view the final merged file using a JMeter GUI client.
To grab the files, we can use any SFTP tool of our choice.
If you are on a Unix-flavored machine, chances are that you already have the scp command-line utility handy.
To proceed, we will need the name of the host machine we are trying to connect to.
To get that, type in the exit command on the console of the first virtual machine.
We can now connect to the box using our keypair file and retrieve the results file.
As an example, on our box, our keypair file named book-test.pem is stored under the .ec2 directory in our home directory and we want to place the results file in /tmp directory.
This will transfer all the .csv files on the AWS instance to our local machine under the /tmp directory.
Remember to use the correct hostname for each of the virtual boxes.
After transferring all the result files from the virtual machines, we can terminate all the instances since we are done with them.
If you are done with a box, remember to shut it down, else you will incur unneeded charges.
With our entire results file from all hosts now available locally, we will need to merge them together to get an aggregate of response time across all hosts.
We can do this with any editor that can deal with CSV file formats.
Alternatively, this can all be done from the command line.
For those on Unix-flavored machines, the cat command can be employed.
Open the command line and go to the directory where you have transferred the result files.
This creates a file named merged-out.csv that can now be opened in our JMeter GUI client.
Since we ran this across four nodes, we have a total of 2,400 samplers generated, as can be seen from the following screenshot.
We also see that the Max response time is not too shabby.
There were no errors reported on any of the nodes and the throughput was good for our run.
These are not bad numbers considering we used small instances of AWS.
We can always put more stress on the application or web servers by spinning off more nodes to run test plans or by using higher capacity machines on AWS.
Although we have only used four virtual boxes for illustrative purposes here, nothing prevents you from scaling out to hundreds of machines to run your test plan.
At the end of the tests, it terminates all AWS instances that were started.
We gave it a spin, but couldn't quite get it working as advertised.
Furthermore, we should mention that there are some services out on the Web helping to bring ease to distributed testing.
With that, we wrap up our look into distributed testing with JMeter.
Though the test plan we used had no input test data, nothing prevents you from using one that does.
Also, all the other techniques we have learned in other chapters can be applied whenever they make sense.
Also, not using a master/node configuration got us past the hurdle of known limitations.
Network saturation due to high number of slave nodes writing to the master node.
The master node server could be easily overwhelmed with very few slave nodes reporting to it, depending on its resources (CPU and memory)
Summary We have covered quite a lot of ground in this chapter.
We have learned how we can distribute load using different techniques when executing test plans.
We learned how to have JMeter work in a master/node configuration.
With the help of tools such as Vagrant and Puppet, we made a daunting task really easy.
We learned how to spin off several node machines on the same physical box (or different boxes) and use a master node to control them all from a JMeter GUI.
While convenient, we saw that this method was limiting in terms of scalability.
As the number of slave nodes grew, the master quickly became the bottleneck due to high I/O generated from several nodes trying to report progress to it.
To overcome such restrictions and ultimately achieve infinite scalability, we learned how to run several test machines in parallel to execute our test plans.
In the process, we leveraged the AWS infrastructure and saw how we can use the cloud to aid testing more efficiently, thus helping us reach our targeted goals.
In the next chapter, we will look at some tips that are helpful to have handy when working with JMeter.
Helpful Tips At this point, you have hopefully become familiar with the inner workings of JMeter and are comfortable with using it to achieve most of your testing needs.
However, before we wrap up the book, there are some helpful tips worth mentioning that will make working with JMeter more pleasant and perhaps save you time in the process.
These are some techniques we have learned over the years and they have proven useful in almost every environment we have found ourselves.
The value of the remote_hosts property encountered in the previous chapter is a good example of this.
Properties can be referenced from within a test plan, but cannot be used for thread-specific values because of their global nature (shared among all threads)
JMeter variables, on the other hand, are local to each thread.
The values may stay the same or vary between threads.
In cases where a variable is updated by a thread, only the thread copy of the variable is changed, thus remaining invisible to other running threads.
A good example of this is the Regular Expression Extractor post processor we encountered in the previous chapters.
The values extracted and acted upon are in the context of the samples of the running thread.
The variables that are extracted are user-defined and available to the whole test plan at startup.
If the same variable is defined by multiple user-defined variable elements, the last one wins.
As simple as they appear, using JMeter variables wisely can save you time by allowing you to use the same recorded scripts from one environment to another environment without having to rescript for every single environment you are targeting, provided the two environments are structured similarly architecturally.
So, for instance, test plans recorded against the User Acceptance Test (UAT) environment can be run in production if those two environments bear a resemblance in structure.
To accomplish that, you can either define User Defined Variables (UDV) at the Test Plan root level, or replace individual URLs for HTTP Request samplers.
For example, we can define the following UDVs at the Test plan root level:
The number of threads will remain 10 (the default) since it wasn't overridden.
This concept saves a lot of time when developing test plans against various environments, allowing you to record once and target various environments with the same set of scripts.
For instance, this is useful when a particular environment isn't ready yet and scripts have already been developed targeting an active environment.
Once the environment becomes available, the same scripts can target the newly available environment without having to rerecord them.
It's from the banking application sample test plan we saw in , Recording Your First Test.
To run it against Cloudbees, you will need to override the hostname variable when you start JMeter like so:
JMeter functions JMeter functions are special values that can populate fields or any sampler or other element in the test plan.
Functions can prove useful in certain situations, allowing the computation of new values at runtime based on previous response data, which thread the function is in, time, and numerous other sources.
Their values are generated afresh for every request throughout the course of the test.
There are also some restrictions regarding where certain functions can be invoked.
Since JMeter thread variables are not fully initialized when functions are processed, variable names passed as parameters will not be set up, causing variable references to fail.
Each occurrence of a function call is handled by a separate function instance.
The Regular Expression tester Throughout the course of the book, we have seen Regular Expression Extractor post processors in action in several of our scenarios.
These components allow you to extract values from a server response using a Perl-type regular expression.
As a post processor, this element executes after each sample request in its scope, applying the regular expression; extracts the requested values, generating the template string; and finally stores the result into a given variable name, which can then be referenced further down the test plan.
To fully maximize the use of the Regular Expression Extractor post processor, it helps to get acquainted with regular expressions in general.
There are numerous online resources that can help, but you can start with this one: http://www.regularexpressions.info/tutorial.html.
The RegExp Tester view is one of the options you can choose from the View Results Tree listener dropdown menu items.
It allows you to test various regular expressions against the server response on a per-sampler basis.
When you are interested in extracting a variable or group of variables that vary dynamically based on which thread is currently executing, this gives you the maximum flexibility to test and tune your regular expression until you find the exact match that suits your needs.
Without such an element, significant time could be spent nailing down the right pattern matcher, as it would involve rerunning your test plan several times with various inaccurate expressions, hoping it eventually matches.
In our browse iTunes store test plan from the previous chapter, say we were interested in extracting the class elements from the HTML response of the / itunes/charts/ sampler.
Once the test has been exercised, we could explore the RegExp Tester view to find the right regular expression for this.
We can then copy that pattern into a Regular Expression Extractor post processor under the /itunes/ charts/ sampler and store the results in a variable to use further down the chain in our test plan.
The Debug sampler The debug sampler generates a sample containing all the values of JMeter variables and/or properties.
A View Results Tree listener must be present in the test plan to view its results.
This nifty component helps you debug your test plans appropriately, providing you with the tools to analyze the runtime-assigned values of various variables during test execution.
In our example above, suppose we added a Regular Expression Extractor post processor to the /itunes/charts sampler and stored it in a variable.
We can view the value assigned to the variable, and more importantly how to get to the different values if there is more than one match.
As you can see from this screenshot, the multiple matches are stored under linkclass_n (where n is a match position), followed by the variable name declared in our Regular Expression Extractor post processor.
Using timers in your test plan By default, JMeter doesn't put timers in your test plans when a scenario is recorded.
Ideally, users will have a think or wait time between page views and requests.
Getting JMeter to simulate such pauses or waits makes your test plans more realistic, bringing it closer to how actual users might behave.
JMeter offers various built-in timer components that help achieve this.
Each varies from the others in how it varies the simulated pauses.
The following is a list of included timers as of the time of writing.
The Constant timer The Constant timer is used if you want each thread to pause for the same amount of time between requests.
The Gaussian random timer The Gaussian random timer pauses each thread request for a random amount of time with most of the time intervals occurring near a particular value.
The total delay is the sum of the Gaussian distributed value times, the value specified, and the offset.
The Uniform random timer The Uniform random timer pauses thread requests for a random amount of time, with each time interval having the same probability of occurring.
The total delay is the sum of the random and offset values.
The Constant throughput timer The Constant throughput timer introduces variable pauses calculated to keep the total throughput that is, samples per minute as close as possible to the targeted figure.
Though called a constant throughput timer, the throughput can be varied by using a counter value, JavaScript value, BeanShell value, or remote BeanShell server.
The Synchronizing timer The Synchronizing timer helps simulate large instantaneous loads on various points in the test plan by blocking threads until a certain number of threads have been blocked, then releasing them all at once.
The Poisson random timer The Poisson random timer, like the Gaussian random timer, pauses thread requests for a random amount of time, with most of the time intervals occurring near a particular value.
The total delay is the sum of the Poisson distributed value and the offset value.
The JDBC Request sampler Sometimes, it's necessary to test durability and I/O operations against the database directly.
How fast are inserts, updates, and selects on the tables in question? For such tests, JMeter provides a JDBC Request sampler to help issue SQL queries against the database.
However, to use it, we need to set up a JDBC Connection Configuration component.
Setting up this component requires us to point to a database.
Normally, this will already be set up for you to test against, but for illustrative purposes, we are going to assume none has been set up.
We will be using H2, an open source pure Java SQL database.
Start the H2 database server by issuing either of these commands.
This will launch your browser and point to the H2 Admin console as seen in the following screenshot.
Create a test database named test by changing your JDBC URL value to either of the following:
Now that we have a database and table to test with, we can go ahead and configure a JDBC Connection Configuration component to point to it.
Since H2 is Java-based, to run it, you need to have a JRE (Java Runtime Environment) set up on the machine of choice.
Please refer to Chapter 1, Performance Testing Fundamentals, for instructions on setting up JRE on your machine if you don't already have it.
Configuring a JDBC Connection Configuration component As the name suggests, this component helps create a connection to the database from the supplied settings.
Each thread could get its own dedicated connection, or connections may be pooled between threads.
Adding a JDBC Request sampler Now that we have a JDBC Connection Configuration component configured, the final step is to add a JDBC Request sampler to our test plan to make use of it.
Adding that is no different from how we have added other samplers throughout the book.
The JDBC Request sampler allows you to issue complex queries with bind parameters, inserts, updates, deletes, and even stored procedures.
A Dummy sampler Though not part of the built-in JMeter samplers, this sampler can be added to your JMeter toolkit via the JMeter extensions project.
We discussed this in detail in Chapter 5, Resource Monitoring, so if you don't already have it configured, please refer to that chapter to get the gist of it.
This sampler generates samples with just the values that are defined for it.
It comes in extremely handy when debugging post processors without having to repeat the entire execution of the test plan or waiting for the exact condition in the application under testing.
This component allows you to determine if the response should be marked a successful sample, what response code to return, the response message, the latency, and response times.
In addition, it allows you to specify a request and a response, which can be anything you choose; for example, HTML, XML, and JSON.
Once the plugins have been properly installed into your JMeter instance, you should see additional samplers available to pick from.
For complex JSON structures, using JMeter's bundled XPath Extractor can sometimes lead to heartache when trying to get at targeted elements.
No matter how nested the structure is, JSON Path Extractor gets the job done elegantly.
Handling RESTful web services An increasing number of applications are shifting to RESTful web services due to their simplicity to build, test, and consume compared to their SOAP counterparts.
All REST communication is done over the HTTP protocol between the parties involved.
The built-in HTTP Request sampler in JMeter is more than up to the task.
The body of the request can be in XML or JSON format.
An HTTP Header Manager component can be used to send additional HTTP header attributes if needed.
In our sample, we are going to create a new person in our sample application using a POST request, and then verify that the person was actually created using a GET request.
Add another HTTP Request sampler (this will retrieve the newly created person using the extracted ID)
By the same token, we can issue DELETE and PUT requests to delete and update resources if our application supports it.
Summary In this chapter, we have learned some helpful tips that are essential to making testing with JMeter more efficient.
We have covered variables, functions, regular expression testers, and timers, to name a few.
Along the way, we covered some additional helpful components provided by the excellent JMeter plugin extensions.
We barely scratched the surface of the additional components it provides.
Finally, we looked at how JMeter can help us work with the database and REST web services.
We hope by now you know enough about JMeter to become proficient and attain your testing goals.
In just a short time, you have grown from novice to pro.
Though we couldn't cover all JMeter has to offer, we hope we have covered enough to make you see it as a valuable tool of choice when embarking on your next performance testing engagement and that you have enjoyed reading the book as much as we have had writing it.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cutting-edge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licenses, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Create better interaction, design, and web development with simple JavaScript techniques.
Combine high volume data movement, complex transformations and real-time data integration with the robust capabilities of ODI in this practical guide.
Discover the comprehensive and sophisticated orchestration of data integration tasks made possible with ODI, including monitoring and error-management.
Get to grips with the product architecture and building data integration processes with technologies including Oracle, Microsoft SQL Server and XML files.
Packed with comprehensive recipes to secure, tune, and extend your Java EE applications.
Secure your Java applications using Java EE built-in features as well as the well-known Spring Security framework.
Explore various ways to extend a Java EE environment with the use of additional dynamic languages as well as frameworks.
Develop your own service-based applications, from simple deployments through to complex legacy integrations.
Learn how services can communicate with each other and the benefits to be gained from loose coupling.
Contains clear, practical instructions for service development, highlighted through the use of numerous working examples.
Chapter 2: Recording Your First Test Configuring the JMeter HTTP proxy server Setting up your browser to use the proxy server Using a browser extension Changing the machine system settings Running your first recorded scenario.
Monitoring the server with a JMeter plugin Installing the plugins Add monitor listeners to the test plan.
Chapter 6: Distributed Testing Remote testing with JMeter Configuring JMeter slave nodes Configuring one slave per machine Configuring the master node to be tested against one slave per machine Configuring multiple slave nodes on a single box Configuring the master node to be tested against multiple slave nodes on a single box.
Leveraging the cloud for distributed testing Obtaining your access key, secret key, and key pair Launching an AWS instance Executing the test plan Viewing the results from the virtual machines.
