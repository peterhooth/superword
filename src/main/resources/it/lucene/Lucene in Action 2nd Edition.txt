Please post comments or corrections to the Author Online forum at http://www.manning-sandbox.com/forum.jspa?forumID=451
For more information on this and other Manning titles go to.
Lucene is a powerful Java search library that lets you easily add search to any application.
In recent years Lucene has become exceptionally popular and is now the most widely used information retrieval library: it powers the search features behind many Web sites and desktop tools.
One of the key factors behind Lucene’s popularity is its simplicity, but don’t let that fool you: under the hood there are sophisticated, state of the art Information Retrieval techniques quietly at work.
The careful exposure of its indexing and searching API is a sign of the well-designed software.
Consequently, you don’t need in-depth knowledge about how Lucene’s information indexing and retrieval work in order to start using it.
Moreover, Lucene’s straightforward API requires using only a handful of classes to get started.
In this chapter we cover the overall architecture of a typical search application, and where Lucene fits.
We show you how to perform basic indexing and searching with Lucene with ready-to-use code examples.
We’ll start next with the very modern problem of information explosion, to understand why we need powerful search functionality in the first place.
By the time you’re reading this, likely Lucene’s APIs and features will have changed.
In order to make sense of the perceived complexity of the world, humans have invented categorizations, classifications, genuses, species, and other types of hierarchical organizational schemes.
The Dewey decimal system for categorizing items in a library collection is a classic example of a hierarchical categorization scheme.
The explosion of the Internet and electronic data repositories has brought large amounts of information within our reach.
With time, however, the amount of data available has become so vast that we needed alternate, more dynamic ways of finding information (see Figure 1.1)
Although we can classify data, trawling through hundreds or thousands of categories and subcategories of data is no longer an efficient method for finding information.
The need to quickly locate certain specific information you need out of the sea of data isn’t limited to the Internet realm—desktop computers store increasingly more data.
Changing directories and expanding and collapsing hierarchies of folders isn’t an effective way to access stored documents.
Furthermore, we no longer use computers just for their raw computing abilities: They also serve as communication devices, multimedia players and media storage devices.
Those uses for computers require the ability to quickly find a specific piece of data; what’s more, we need to make rich media—such as images, video, and audio files in various formats—easy to locate.
With this abundance of information, and with time being one of the most precious commodities for most people, we need to be able to make flexible, free-form, ad-hoc queries that can quickly cut across rigid category boundaries and find exactly what we’re after while requiring the least effort possible.
To illustrate the pervasiveness of searching across the Internet and the desktop, figure 1.1 shows a search for lucene at Google.
The figure includes a context menu that lets us use Google to search for the highlighted text.
Figure 1.2 shows the Apple Mac OS X Finder (the counterpart to Microsoft’s Explorer on Windows) and the search feature embedded at upper right.
The Mac OS X music player, iTunes, also has embedded search capabilities, as shown in figure 1.3
Search is needed everywhere! All major operating systems have embedded searching.
The Spotlight feature in Mac OS X integrates indexing and searching across all file types including rich metadata specific to each type of file, such as emails, contacts, and more.1
Erik and Mike freely admit to fondness of all things Apple.
To understand what role Lucene plays in search, let’s start from the basics and learn about what Lucene is and how it can help you with your search needs.
Some have been working on novel user interfaces, some on intelligent agents, and others on developing sophisticated search tools and libraries like Lucene.
Before we jump into action with code samples later in this chapter, we’ll give you a high-level picture of what Lucene is, what it is not, and how it came to be.
Lucene is a high performance, scalable Information Retrieval (IR) library.
Information retrieval refers to the process of searching for documents, information within documents or metadata about documents.
It is a mature, free, open-source project implemented in Java; it’s a project in the Apache Software Foundation, licensed under the liberal Apache Software License.
As such, Lucene is currently, and has been for quite a few years, the most popular free IR library.
Throughout the book, we’ll use the term Information Retrieval (IR) to describe search tools like Lucene.
People often refer to IR libraries as  search engines, but you shouldn’t confuse IR libraries with web search engines.
As you’ll soon discover, Lucene provides a simple yet powerful core API that requires minimal understanding of full-text indexing and searching.
You need to learn about only a handful of its classes in order to start integrating Lucene into an application.
Because Lucene is a Java library, it doesn’t make assumptions about what it indexes and searches, which gives it an advantage over a number of other search applications.
Its design is compact and simple, allowing Lucene to be easily embedded into desktop applications.
People new to Lucene often mistake it for a ready-to-use application like a file-search program, a web crawler, or a web site search engine.
That isn’t what Lucene is: Lucene is a software library, a toolkit if you will, not a full-featured search application.
It concerns itself with text indexing and searching, and it does those things very well.
Lucene lets your application deal with business rules specific to its problem domain while hiding the.
You can think of Lucene as the core that the application wraps around, as shown figure 1.4
A number of full-featured search applications have been built on top of Lucene.
If you’re looking for something prebuilt or a framework for crawling, document handling, and searching, consult the Lucene Wiki “powered by” page (http://wiki.apache.org/lucene-java/PoweredBy) for many options.
We also describe some of these options in section 1.3.4
Lucene allows you to add search capabilities to your application.
Lucene can index and make searchable any data that you can extract text from.
As you can see in figure 1.4, Lucene doesn’t care about the source of the data, its format, or even its language, as long as you can derive text from it.
This means you can index and search data stored in files: web pages on remote web servers, documents stored in local file systems, simple text files, Microsoft Word documents, XML or HTML or PDF files, or any other format from which you can extract textual information.
Similarly, with Lucene’s help you can index data stored in your databases, giving your users full-text search capabilities that many databases don’t provide.
Once you integrate Lucene, users of your applications can perform searches by entering queries like +George +Rice -eat -pudding, Apple pie +Tiger, animal:monkey AND food:banana, and so on.
With Lucene, you can index and search email messages, mailing-list archives, instant messenger chats, your Wiki pages … the list goes on.
Lucene was originally written by Doug Cutting;2 it was initially available for download from its home at the SourceForge web site.
It now has a number of sub-projects, which you can see at http://lucene.apache.org.
With each release, the project has enjoyed increased visibility, attracting more users and developers.
Lucene is Doug’s wife’s middle name; it’s also her maternal grandmother’s first name.
Doug Cutting remains a strong force behind Lucene, and many more developers have joined the project with time.
At the time of this writing, Lucene’s core team includes about half a dozen active developers, three of whom are authors of this book.
In addition to the official project developers, Lucene has a fairly large and active technical user community that frequently contributes patches, bug fixes, and new features.
Lucene’s popularity can be seen by its diverse usage and numerous ports to other programming languages.
Lucene is in use in a surprisingly diverse and growing number of places.
In addition to those organizations mentioned on the Powered by Lucene page on Lucene’s Wiki, a number of other large, wellknown, multinational organizations are using Lucene.
One way to judge the success of open source software is by the number of times it’s been ported to other programming languages.
Using this metric, Lucene is quite a success! Although Lucene is written in Java, as of this writing there are Lucene ports and bindings in many other programming environments, including Perl, Python, Ruby, C/C++, PHP, and C# (.NET)
This is excellent news for developers who need to access Lucene indices from applications written in diverse programming languages.
In order to understand exactly how Lucene fits into a search application, including what Lucene can and cannot do, we will now review the architecture of a “typical” modern search application.
Figure 1.4 Typical components of search application; the shaded components show which parts Lucene handles.
Some run quietly, as a small component deeply embedded inside an existing tool, searching a very specific set of content (local files, email messages, calendar entries, etc)
Others run on a remote Web site, on a dedicated server infrastructure, interacting with many users via Web browser or mobile device, perhaps searching a product catalog or known and clearly scoped set of documents.
Some run inside a company’s intranet and search a massive collection of varied documents visible inside the company.
Still others index a large subset of the entire web and must deal with unbelievable scale both in content and in simultaneous search traffic.
Yet, despite all this variety, search engines generally share a common overall architecture, as shown in figure 1.4
It’s important to grasp the big picture of a search application, so that you have a clear understanding of which parts Lucene can handle and which parts you must separately handle.
A common misconception is that Lucene is an entire search application, when in fact it’s simply the core indexing and searching component.
When designing your application you clearly have strong opinions on what features are necessary and how they should work.
But be forewarned: modern popular web search engines (notably Google) have pretty much set the baseline requirements that all users will expect once they use your application.
When in doubt, look to Google for the basic features your search application must provide.
Also look to Google for inspiration when you’re struggling with difficult decisions.
Let’s walk through a search application, one component at a time.
As you’re reading along, think through what your application requires from each of these components to understand how you could use Lucene to achieve your search goals.
Starting from the bottom of figure 1.4, and working upwards, is the first part of all search engines, a concept called indexing: processing the original data into a highly efficient cross-reference lookup in order to facilitate rapid searching.
You can think of an index as a data structure that allows fast random access to words stored inside it.
The concept behind it is analogous to an index at the end of a book, which lets you quickly locate pages that discuss certain topics.
In the case of Lucene, an index is a specially designed data structure, typically.
We cover the structure of separate index files in detail in appendix B, but for now just think of a Lucene index as a tool that allows quick word lookup.
When you look closer, indexing in fact consists of a sequence of logically distinct steps.
First you must somehow gain access to the content you need to search.
This process, which is often referred to as a crawler or spider, gathers and scopes the content that needs to be indexed.
That may be trivial, for example if you are indexing a set of XML files that reside in a specific directory in the file system or if all your content resides in a well-organized database.
Entitlements, which means only allowing certain authenticated users to see certain documents, can complicate content acquisition, as it may require “superuser” access when acquiring the content.
Furthermore, the access rights or ACLs must be acquired along with the document’s content, and added to the document as additional fields that are used during searching to properly enforce the entitlements.
For large content sets, it’s important this component is efficiently incremental, so that it can visit only changed documents since it was last run.
It may also be "live", meaning it is a continuously running service, waiting for new or changed content to arrive and loading it the moment it’s available.
Lucene, being a core search library, does not provide any functionality to support acquiring content.
This is entirely up to your application, or a separate piece of software.
If your application has scattered content, it might makes sense to use a pre-existing crawling tool.
Such tools are typically designed to make it easy to load content stored in various systems, and sometimes provide pre-built connectors to common content stores, such as web sites, databases, popular content management systems, filesystems, etc.
If your content source doesn’t have a pre-existing connector for the crawler, it’s likely straightforward to build your own.
The next step is to create bite-sized pieces, called documents, out of your content.
The document typically consists of several separately named fields with values, for example title, body, abstract, author, url, etc.
Often the approach is obvious, for example one email message becomes one document, or one PDF file or web page is one document.
But sometimes it's less so: how should you handle attachments on an email message?  Should you glom together all text extracted from the attachments into a single document, or, make separate documents for them, somehow linked back to the original email message, for each attachment? Once you have worked out this design you’ll need to extract text from the original raw content for each document.
If your content is already textual in nature, this is nearly a no op.
You’ll need to run document filters to extract text from such content, before creating the search engine document.
Often interesting business logic may also apply during this step, to create additional fields.
For example, if you have a large “body text” field you might run semantic analyzers to pull out proper names, places, dates, times, locations, etc, into separate fields in the document.
Or perhaps you tie in content available in a separate store (for example a database) and merge this together for a single document to the search engine.
Another common part of building the document is to inject boosts to individual documents and fields that are deemed more or less important.
Perhaps you’d like your press releases to come out ahead of all other documents, all things being equal?  Perhaps recently modified documents are more important than older documents?  Boosting may be done statically (per document and field) at indexing time, which we cover in detail in section 2.6, or dynamically during searching, which we cover in section 5.7
Nearly all search engines, including Lucene, automatically statically boost fields that are shorter over fields that are longer.
While Lucene provides an API for building fields and documents, it does not provide any logic to build a document because it’s entirely application specific.
It also does not provide any document filters, although Lucene has a sister project at Apache, Tika, which handles document filtering very well.
The textual fields in a document cannot be indexed by the search engine, yet.
In order to do that, the text must first be analyzed.
Each token corresponds roughly to a “word” in the language, and this step determines how the textual fields in the document are divided into a series of tokens.
There are all sorts of interesting questions here: how do you handle compound words?  Should you apply spell correction (if your content itself has typos)?  Should you inject synonyms inlined with your original tokens, so that a search for “laptop” also returns products mentioning “notebook”?  Should you collapse singular and plural forms to the same token?  Often a stemmer, such as Dr Martin Porter’s Snowball stemmer (covered in Section 8.3.1) is used to derive roots.
Should you preserve or destroy differences in case?  For non-Latin languages, how can you even determine what a “word” is?  This component is so important that we have a whole chapter, chapter 4 Analysis, describing it.
Lucene provides a wide array of built-in analyzers that allow you fine control over this process.
It’s also straightforward to build your own analyzer, or create arbitrary analyzer chains combining Lucene’s tokenizers and token filters, to customize how tokens are created.
Finally, you’ll be happy to know, your content is finally in a state where Lucene can index it.
Lucene of course provides everything necessary for this step, and works quite a bit of magic under a surprisingly simple API.
Chapter 2 takes you through all the nitty gritty steps for how to tune how Lucene indexes your documents.
We’re done reviewing the typical indexing steps for a search application.
It’s important to remember that indexing is something of a “necessary evil” that you must undertake in order to provide a good search experience: you should design and customize your indexing process only to the extent that improves your users’ search experience.
Searching is the process of looking up words in an index to find documents where they appear.
The quality of a search is typically described using precision and recall metrics.
Recall measures how well the search system finds relevant documents, whereas precision measures how well the system filters out the irrelevant documents.
Appendix D describes how to use Lucene’s benchmark contrib framework to measure precision and recall of your search application.
However, you must consider a number of other factors when thinking about searching.
We already mentioned speed and the ability to quickly search large quantities of text.
Support for single and multiterm queries, phrase queries, wildcards, fuzzy queries, result ranking, and sorting are also important, as is a friendly syntax for entering those queries.
Let’s work through the typical components of a search engine, this time working top down in figure 1.4, starting with the search user interface.
Believe it or not, this is the most important part of your search application!  You could have the greatest search engine in the world under the hood, tuned with fabulous state-of-the-art functionality, but with one silly mistake on the user interface it will lack consumability, thus confusing your precious and fickle users who will quietly move on to your competitors.
Keep the interface simple -- don't present a bunch of advanced options on the first page.
Provide a ubiquitous, prominent search box, visible everywhere, rather than requiring a 2-step process of first clicking on a search link and then entering the search text, a common mistake.
Simple details, like failing to highlight matches in the titles and excerpts, or using a small font and cramming too much text into the results, can.
Be sure the sort order is clearly called out, and defaults to an appropriate starting point (usually relevance)
Be fully transparent: if your search application is doing something “interesting”, such as expanding the search to include synonyms, using boosts to influence sort order, or automatically correcting spelling, say so clearly at the top of the search results and make it easy for the user to turn off.
The worst thing that can happen, and it happens quite easily, is to erode the user’s trust in the search results.
Once this happens you may never again have the chance to earn that trust back, and your users will quietly move on.
Most of all, eat your own dog food: use your own search application extensively yourself.
Enjoy what’s good about it, but aggressively fix the bad things.
Lucene has a sandbox component, spellchecker, covered in section 8.XXX, that you can use.
Likewise, providing dynamic excerpts (sometimes called summaries) with hit highlighting is also important, and Lucene has a sandbox component, highlighter, covered in section 8.XXX, to handle this.
Lucene does not provide any default search user interface; it’s entirely up to your application to build this.
Once a user interacts with your search interface, she or he submits a search request which first must be translated into an appropriate Query for the search engine.
You must then translate the request into the search engine’s Query object.
Lucene provides a powerful package, called QueryParser, to process the user’s text into a query object using a common search syntax, described at http://lucene.apache.org/java/docs/queryparsersyntax.html.
The query may contain Boolean operations, phrase queries (in double quotes), wildcard terms, etc.
If your application has further controls on the search UI, or further interesting constraints, you must implement logic to translate this into the equivalent query.
For example, if there are entitlement constraints to restrict which set of documents each user is allowed to search, you’ll need to set up filters on the query.
Many applications will at this point also modify the search query so as to boost or filter for important things, if the boosting was not done during indexing (see section 1.3.1)
Often an ecommerce site will boost categories of products that are more profitable, or filter out products presently out of stock (so you don’t see that they are out of stock and then go elsewhere to buy them)
Resist the temptation to heavily boost and filter the search results: users will catch on and lose trust.
Sometimes, you’ll want to use the output of QueryParser but then add your own logic afterwards to further refine the query object.
Still other times you want to customize the QueryParser’s syntax, or customize which Query instances it actually creates, which thanks to Lucene’s open source nature, is straightforward.
Finally, you’re ready to actually execute the search request to retrieve results.
This is the very complex inner workings of the search engine, and Lucene handles all of this magic for you, under a surprisingly simple API.
Lucene is also wonderfully extensible at this point, so if you’d like to customize how results are gathered, filtered, sorted, etc., it’s straightforward.
Pure boolean model -- Documents either match or do not match the provided query, and no scoring is done.
In this model there are no relevance scores associated with matching documents; a query simply identifies a subset of the overall corpus as matching the query.
Vector space model -- Both queries and documents are modeled as vectors in a very high dimensional space, where each unique term is a dimension.
Relevance, or similarity, between a query and a document is computed by a vector distance measure between these vectors.
Probabilistic model -- Computes the probability that a document is a good match to a query using a full probabilistic approach.
Lucene’s approach combines the vector space and pure Boolean models.
Lucene returns documents which you next must render in a very consumable way for your users.
Importantly, the user interface should also offer a clear path for follow-on searches or actions, such as clicking to the next page, refining the search in some way, finding document similar to one of the matches, etc, so that user never hits a “dead end”
Lucene’s core doesn’t offer any components to fully render results, but the sandbox contains the highlighter package, described in section 8.xxx, for producing dynamic summaries and highlighting hits.
We’ve finished reviewing the components of both the indexing and searching paths in a search application, but we are not yet done.
Believe it or not, there is in fact still quite a bit more to a typical fully functional search engine, especially a search engine running on a web site.
This includes administration, in order to keep track of the application’s health, configure the different components, start and stop servers, etc.
It also includes analytics, allowing you to see different views into how your users are using the search feature, thus giving you the necessary guidance on what’s working and what’s not.
Finally, for large search applications, scaleout in both size of the content you need to search and number of simultaneous search requests, is a very important feature.
Spanning the left side of figure 1.4 is the Administration Interface.
If you are using a crawler to discover your content, the administration interface should let you set the starting URLs, create rules to scope which sites the crawler should visit or which document types it should load, set how quickly it’s allowed to read documents, etc.
Starting and stopping servers, managing replication (if it’s a high scale search, or, if high availability failover is required), culling search logs, checking overall system health, creating and restoring from backups, etc., are all examples of what an administration interface might offer.
Many search applications, such as desktop search, don’t require this component, whereas a full enterprise search application may have a very complex Administration Interface.
Often the interface is primarily web-based but may also consist of additional command-line tools.
On the right side of figure 1.4 is the analytics interface.
Analytics is important: you can gain a lot of intelligence about your users and why they do or do not buy your widgets through your web site, by looking for patterns in the search logs.
Some would say this is the most important reason to deploy a good search engine!  If you are an ecommerce web site, seeing how your users run searches, which searches failed to produce satisfactory results, which results users clicked on, how often a purchase followed or did not follow a search, etc, are all incredibly powerful tools to optimize the buying experience of your users.
If your search application is web-based, Google Analytics is a fast way to create an analytics interface.
If that’s not right, you can also build your own charts based on Google’s visualization API.
The vast majority of search applications do not have enough content nor simultaneous search traffic to require scaleout beyond a single computer.
Lucene can handle a sizable amount of content on a single modern computer.
Still, such applications may want to run two identical computers to have no single point of failure (no downtime) plus a means of taking one part of the search out of production to do maintenance, upgrades, etc.
There are two dimensions to scaleout: net amount of content, and net query throughput.
If you have a tremendous amount of content, you must divide it into shards, such that a separate computer searches each shard.
A front end server sends a single incoming query to all shards, and then coalesces the results into a single result set.
If, instead, you have high search throughput during your peaks you’ll have to take the same index and have multiple computers search it.
A front-end load balancer sends each incoming query to the least loaded back-end computer.
If you require both dimensions of scaleout, as a Web scale search engine will, you combine both of these practices.
Both of these require a reliable means of replicating the search index across computers.
However, both Solr and Nutch and others, which build additional search engine functionality on top of Lucene, do.
We’ve finished reviewing the components of a modern search application.
Now it’s time to think about whether Lucene is a fit for your application.
Yet, the needs of a specific application for each of these components vary greatly.
Lucene covers many of these components (the gray shaded ones from figure 1.4) very well, but other components are best covered by complementary open-source software or by your own custom application logic.
Or, it’s possible your application is specialized enough to not require certain components.
You should at this point have a good sense of what we mean when we say Lucene is a search library, not a full application.
If Lucene is not a direct fit, it’s likely one of the open-source projects that complements or builds upon Lucene does fit.
For example, Solr, a sister open-source project under the Lucene Apache umbrella, adds a server that exposes an administration interface, scaleout, indexing content from a database, and adds important end-user functionality like faceted navigation, to Lucene.
Lucene is the search library while Solr provides most components of an entire search application.
Chapter XXX covers Solr, and we also include a case study of how Lucid Imagination uses Solr in chapter XXX.
Nutch takes scaleout even further, and is able to build indexes to handle Web-sized content collections.
Projects like DBSight, Hibernate Search, LuSQL, Compass and Oracle/Lucene make searching database content very simple by handling the “Acquire Content” and “Build Document” steps seamlessly, as long as your content resides in a database.
Many open-source document filters exist, for deriving textual content from binary document types.
Most of the raw ingredients are there for you to pull together a powerful, fully open source search application!
Now let’s dive down and see a concrete example of using Lucene for indexing and searching.
To do that, recall the problem of indexing and searching files, which we described in section 1.3.1
Furthermore, suppose you need to index and search files stored in a directory tree, not just in a single directory.
To show you Lucene’s indexing and searching capabilities, we’ll use a pair of command-line applications: Indexer and Searcher.
First we’ll index a directory tree containing text files; then we’ll search the created index.
These example applications will familiarize you with Lucene’s API, its ease of use, and its power.
If file indexing/searching is the problem you need to solve, then you can copy the code listings and tweak them to suit your needs.
In the chapters that follow, we’ll describe each aspect of Lucene’s use in much greater detail.
Before we can search with Lucene, we need to build an index, so we start with our Indexer application.
In this section you’ll see a simple class called Indexer which indexes all files in a directory ending with the .txt extension.
When Indexer completes execution it leaves behind a Lucene index for its sibling, Searcher (presented next in section 1.4.2)
We don’t expect you to be familiar with the few Lucene classes and methods used in this examplewe’ll explain them shortly.
After the annotated code listing, we show you how to use Indexer; if it helps you to learn how Indexer is used before you see how it’s coded, go directly to the usage discussion that follows the code.
This example intentionally focuses on plain text files with .txt extensions to keep things simple, while demonstrating Lucene’s usage and power.
In chapter 7, we’ll show you how to index other common document types using the Tika framework.
Let’s use Indexer to build our first Lucene search index! RUNNING INDEXER The simplest way to run Indexer is to use ant.
You’ll first have to unpack the zip file containing source code with this book, and then change to the directory “lia2e”
If you don’t see the file build.xml in your working directory, then you’re not in the right directory.
If this is the first time you’ve run any targets, ant will compile all the example sources, build the test index, and finally run Indexer, prompting you for the index and document directory, in case you’d like to change the defaults.
It’s also fine to run Indexer using java from the command-line; just ensure your CLASSPATH includes the jars under the lib subdirectory as well as the build/classes directory.
Go ahead and type “ant Indexer”, and you should see output like this:
Indexer prints out the names of files it indexes, so you can see that it indexes only files with the .txt.
When it completes indexing, Indexer prints out the number of files it indexed and the time it took to do so.
Because the reported time includes both file-directory listing and indexing, you shouldn’t consider it an official performance measure.
In our example, each of the indexed files was small, but roughly 0.8 seconds to index a handful of text files is reasonably impressive.
But generally, searching is far more important since an index is built once but searched many times.
For now, let’s look at Searcher, a command-line program that we’ll use to search the index created by Indexer.
Keep in mind that our Searcher serves the purpose of demonstrating the use of Lucene’s search API.
Your search application could also take a form of a web or desktop application with a GUI, a web application, and so on.
In the previous section, we indexed a directory of text files.
The index, in this example, resides in a directory of its own on the file system.
We instructed Indexer to create a Lucene index in the indexes/MeetLucene directory, relative to the directory from which we invoked Indexer.
As you saw in listing 1.1, this index contains the indexed contents of each file, along with the absolute path.
Now we need to use Lucene to search that index in order to find files that contain a specific piece of text.
For instance, we may want to find all files that contain the keyword patent or redistribute, or we may want to find files that include the phrase “modified version”
Searcher, like its Indexer sibling, is quite simple and has only a few lines of code dealing with Lucene:
We use Lucene’s IndexSearcher class to open our index for searching.
We use QueryParser to parse a human-readable search text into Lucene’s Query class.
Searching returns hits in the form of a TopDocs object.
Print details on the search (how many hits were found and time taken)
Note that the TopDocs object contains only references to the underlying documents.
That call returns a Document object from which we can then retrieve individual field values.
Because Indexer stores files’ absolute paths in the index, Searcher can print them out.
It’s worth noting that storing the file path as a field was our decision and appropriate in this case, but from Lucene’s perspective, it’s arbitrary meta-data attached to indexed documents.
Of course, you can use more sophisticated queries, such as ’patent AND freedom' or 'patent AND NOT apache' or '+copyright +developers', and so on.
Our example indexing and searching applications demonstrate Lucene in a lot of its glory.
The bulk of the code (and this applies to all applications interacting with Lucene) is plumbing relating to the business purpose—in this case, Indexer’s parsing of command line arguments and directory listing to look for text file and Searcher’s code that prints matched filenames.
But don’t let this fact, or the conciseness of the examples, tempt you into complacence: There is a lot going on under the covers of Lucene.
To effectively leverage Lucene, it’s important to understand more about how it works and how to extend it when the need arises.
The remainder of this book is dedicated to giving you these missing pieces.
Next we’ll drill down into the core classes Lucene exposes for indexing and searching.
As you saw in our Indexer class, you need the following classes to perform the simplest indexing procedure:
This class creates a new index or opens an existing one, and then adds, removes or updates documents in the index.
You can think of IndexWriter as an object that gives you write access to the index but doesn’t let you read or search it.
IndexWriter needs somewhere to store its index, and that’s what Directory is for.
The Directory class represents the location of a Lucene index.
It’s an abstract class that allows its subclasses to store the index as they see fit.
In our Indexer example, we created an FSDirectory, which stores real files in a directory in the filesystem, and passed it to IndexWriter’s constructor.
The other commonly used implementation of Directory is a class called RAMDirectory.
Although it exposes an interface identical to that of FSDirectory, RAMDirectory holds all its data in memory.
This implementation is therefore useful for smaller indices that can be fully loaded in memory and can be destroyed upon the termination of an application.
Because all data is held in the fast-access memory and not on a slower hard disk, RAMDirectory is suitable for situations where you need very quick access to the index, whether during indexing or searching.
For instance, Lucene’s developers make extensive use of RAMDirectory in all their unit tests: When a test runs, a fast in-memory index is created or searched; and when a test completes, the index is automatically destroyed, leaving no residuals on the disk.
Of course, the performance difference between RAMDirectory and FSDirectory is less visible when Lucene is used on operating systems that cache files in memory since the index may very well fit entirely into the operating system’s IO cache.
You’ll see both Directory implementations used in code snippets in this book.
IndexWriter can’t index text unless it’s first been broken into separate words, using an Analyzer.
The analysis process requires a Document, containing separate fields to be indexed.
You can think of it as a virtual document—a chunk of data, such as a web page, an email message, or a text file—that you want to make retrievable at a later time.
Fields of a document represent the document or meta-data associated with that document.
The original source (such as a database record, a Word document, a chapter from a book, and so on) of document data is irrelevant to Lucene.
It’s the text that you extract from such binary documents, and add as a Field, that Lucene processes.
The meta-data such as author, title, subject, date modified, and so on, are indexed and stored separately as fields of a document.
When we refer to a document in this book, we mean a Microsoft Word, RTF, PDF, or other type of a document; we aren’t talking about Lucene’s Document class.
Although various types of documents can be indexed and made searchable, processing them isn’t as straightforward as processing purely textual content that can easily be converted to a String or Reader Java type.
So, for each text file we find, we create a new instance of the Document class, populate it with Fields (described next), and add that Document to the index, effectively indexing the file.
Similarly, in your application, you must carefully design how a Lucene document and its fields will be constructed to match specific needs of your content sources and application.
A Document is simply a container for multiple Fields, which is the class that actually holds the textual content to be indexed.
Each Document in an index contains one or more named fields, embodied in a class called Field.
Each field has a name and corresponding value, and a bunch of options, described in Section 2.2.1, that control precisely how Lucene will index the Field’s value.
A document may have more than one field with the same name.
In this case the values of the fields are appended, during indexing, in the order they were added to the document.
When searching, it is exactly as if the text from all the fields were concatenated and treated as a single text field.
You’ll apply this handful of classes most often when using Lucene for indexing.
In order to implement basic search functionality, you need to be familiar with an equally small and simple set of Lucene search classes.
The basic search interface that Lucene provides is as straightforward as the one for indexing.
Only a few classes are needed to perform the basic search operation:
IndexSearcher is to searching what IndexWriter is to indexing: the central link to the index that exposes several search methods.
You can think of IndexSearcher as a class that opens an index in a read-only mode.
It offers a number of search methods, some of which are implemented in its abstract parent class Searcher; the simplest takes a Query object and an int topN count as parameters and returns a TopDocs object.
We cover the details of IndexSearcher in chapter 3, along with more advanced information in.
Similar to the Field object, it consists of a pair of string elements: the name of the field and the word (text value) of that field.
Note that Term objects are also involved in the indexing process.
However, they’re created by Lucene’s internals, so you typically don’t need to think about them while indexing.
During searching, you may construct Term objects and use them together with TermQuery:
This code instructs Lucene to find the top 10 documents that contain the word patent in a field named contents, sorting the documents by descending relevance.
Because the TermQuery object is derived from the abstract parent class Query, you can use the Query type on the left side of the statement.
So far in this chapter we’ve mentioned only the most basic Lucene Query: TermQuery.
It contains several utility methods, the most interesting of which is setBoost(float), which enables you to tell Lucene that certain sub-queries should have a stronger contribution to the final relevance than other sub-queries.
Next we cover TermQuery, which is the building block for most complex queries in Lucene.
TermQuery is the most basic type of query supported by Lucene, and it’s one of the primitive query types.
It’s used for matching documents that contain fields with specific values, as you’ve seen in the last few paragraphs.
Finally, wrapping up our brief tour of the core classes used for searching, we touch on TopDocs which represents the result set returned by searching.
The TopDocs class is a simple container of pointers to the top N ranked search results—documents that match a given query.
For each of the top N results, TopDocs records the int docID (which you can use to retrieve the document) as well as the float score.
In this chapter, you’ve gained some healthy background knowledge on the architecture of search applications, as well as some basic Lucene knowledge.
You now know that Lucene is an Information Retrieval library, not a ready-to-use standalone product, and that it most certainly does not contain a web crawler, document filters or a search user interface, as people new to Lucene sometimes think.
However, as confirmation of Lucene’s popularity, there are numerous projects that integrate with or build on Lucene, that could be a good fit for your application.
In addition, there are numerous ways to access Lucene’s functionality from programming environments other than Java.
You’ve also learned a bit about how Lucene came to be and about the key people and the organization behind it.
In the spirit of Manning’s in Action books, we quickly got to the point by showing you two standalone applications, Indexer and Searcher, which are capable of indexing and searching text files stored in a file system.
We then briefly described each of the Lucene classes used in these two applications.
Search is everywhere, and chances are that if you’re reading this book, you’re interested in search being an integral part of your applications.
Depending on your needs, integrating Lucene may be trivial, or it may involve challenging architectural considerations.
We’ve organized the next couple of chapters as we did this chapter.
So you want to search files stored on your hard disk, or perhaps search your email, web pages, or even data stored in a database.
However, before you can search something, you have to index it, and that’s what you’ll learn to do in this chapter.
This chapter goes further and teaches you about index updates, parameters you can use to tune the indexing process, and more advanced indexing techniques that will help you get the most out of Lucene.
Here you’ll also find information about the structure of a Lucene index, important issues to keep in mind when accessing a Lucene index with multiple threads and processes, sharing an index over the NFS file system, and the locking mechanism that Lucene employs to prevent concurrent index modification.
Despite the great detail we’ll now take you through on how Lucene indexes documents, don’t forget the big picture: indexing is simply a means to an end.
What really matters is the search experience your applications present to its users; indexing is “merely” the necessary evil you must go through in order to enable a strong user search experience.
So while there are great fun details here about indexing, your time is generally better spent working on how to improve the search experience.
That being said, clearly there are many important search metrics that require you to “do the right thing” during indexing in order to enable the search functionality.
Before we dive into the specifics of Lucene’s indexing API, let’s first walk through its conceptual approach to modeling content.
We’ll start with Lucene's fundamental units of indexing and searching, Documents and Fields, and then move on to some important differences between Lucene and the more structured model of modern databases.
A Document is Lucene’s atomic unit of indexing and searching.
It’s actually just a container that holds one or more Fields, which in turn contain the “real” content.
Each Field has a name to identify it, a text or binary value, and a series of detailed options that describes what Lucene should do with the Field’s value when you add the document to the index.
In order to index your raw content sources, you must first translate it into Lucene’s Documents and Fields.
Then, at search time, the field values are searched; for example users could search for “title:lucene” to find all documents whose title field value contains the terms “lucene”
At a high level, there are three things Lucene can do with each field:
A field must be indexed if you intend to search on it.
Only text fields may be indexed (binary valued fields may only be stored)
When a field is indexed, tokens are first derived from its text value, using a process called analysis, and then those tokens are enrolled into the index.
See section XXX for all options on how the field’s value is indexed.
If it is indexed, the field may also optionally store term vectors, which is really a miniature inverted index for that one field, allowing you to retrieve all tokens for that field.
This enables certain advanced use cases like searching for documents similar to an existing one (more uses are covered in Section 5.7)
See section XXX for all options with how term vectors are indexed.
Separately, the field’s value may be stored, meaning a verbatim copy of the un-analyzed value is written away in the index so that it can later be retrieved.
This is useful for fields you'd like to present unchanged to the user, such as the document's title or abstract.
See section XXX for options on how fields are stored.
How you factor your raw content sources into Lucene’s documents and fields is typically an iterative design process and very application dependent.
Lucene couldn’t care less which fields you use, what their names are, etc.
Documents usually have quite a few fields, for example title, author, date, abstract, body text, URL, keywords, etc., might all be different fields.
The common approach is to gather all textual field values into a single “contents” field, and only index that one field, but still separately store all the original fields for presentation at search time.
Once you’ve created your document, you add it to your index.
Then, at search time, you can retrieve the documents that match each query and use their stored fields to present results to the end user.
Lucene is often compared to a database, since both can store content and retrieve it later.
When you retrieve a document from the index, only the stored fields will be present.
For example, fields that were indexed but not stored will not be in the document.
Unlike a database, Lucene has no notion of a fixed global schema.
In other words, each document you add to the index is a blank slate and can be completely different from the document before it: it can have whatever fields, with any indexing and storing and term vector options.
It need not have the same fields as the previous document added.
It can even have the same field, with different options, than in other documents.
This is actually quite powerful: It allows you to take an iterative approach to building your index.
You can jump right in and index documents without having to pre-design the schema.
If you change your mind about your fields, just start adding additional fields later on and then go back and re-index previously added documents, or just rebuild the index.
This also means a single index can hold Documents that represent different entities.
For instance, you could have Documents that represent retail products with Fields such as name and price, and Documents that represent people with Fields such as name, age, and gender.
You could also include un-searchable “meta” documents, which simply hold metadata about the index or your application, such as what time the index was last updated or which product catalog was indexed, but are never included in search results.
The second major difference between Lucene and databases is that Lucene requires you to flatten, or denormalize, your content when you index it.
One common challenge is resolving any “mismatch” between the structure of your documents versus what Lucene can represent.
For example, XML can describe a recursive document structure by nesting tags within one another.
A database can have an arbitrary number of joins, via primary and secondary keys, relating tables to one other.
Such recursion and joins must be denormalized when creating your documents.
Open source projects that build on Lucene, like Hibernate Search, Compass, LuSQL, DBSight, Browse Engine and Oracle/Lucene integration each have different and interesting approaches for handling this denormalization.
Now that you understand how Lucene models documents at a conceptual level, it’s time to visit the steps of the indexing process at a high level.
As you saw in the chapter 1, only a few methods of Lucene’s public API need to be called in order to index a document.
As a result, from the outside, indexing with Lucene looks like a deceptively simple and.
However, behind the simple API lies an interesting and relatively complex set of operations that we can break down into three major and functionally distinct groups, as described in the following sections and depicted in Figure 2.1
Figure 2.1 Indexing with Lucene breaks down into three main operations: extracting text from source documents, analyzing it and saving it to the index.
During indexing, the text is first extracted from the original content and used to create an instance of Document, containing Field instances hold the content.
The text in the fields is then analyzed, to produce a stream of tokens.
Finally, those tokens are added to the index in a segmented architecture.
To index data with Lucene, you must extract plain text from it, the format that Lucene can digest, and then create a Lucene Document.
In chapter 1, we limited our examples to indexing and searching .txt files, which allowed us to easily slurp their content and use it to populate Field instances.
However, things aren’t always that simple: the “Build Document” step from Figure 1.4 has quite a bit of work hidden behind it.
Suppose you need to index a set of manuals in PDF format.
To prepare these manuals for indexing, you must first find a way to extract the textual information from the PDF documents and use that extracted text to create Lucene Documents and their Fields.
No methods would accept a PDF Java type, even if such a type existed.
You face the same situation if you want to index Microsoft Word documents or any document format other than plain text.
Even when you’re dealing with XML or HTML documents, which use plain-text characters, you still need to be smart about preparing the data for indexing, to avoid indexing the XML elements or HTML tags, and index only the real text.
The details of text extraction are in chapter 7 where we describe the Tika framework, which makes it almost too simple to extract text from documents in diverse formats.
One you have the text you’d like to index, and you’ve created a Document with all Fields you’d like to index, all text must then be analyzed.
Once you’ve created Lucene Documents populated with Fields, you can call IndexWriter’s addDocument method and hand your data off to Lucene to index.
When you do that, Lucene first analyzes the text, a process that splits the textual data into a stream of tokens, and performs a number of optional operations on them.
Typically it’s also desirable to remove all stop words, which are frequent but meaningless tokens, from the input (for example a, an, the, in, on, and so on, in English text) using StopFilter.
Similarly, it’s common to process input tokens to reduce them to their roots, for example by using PorterStemFilter for English text (similar classes exist in Lucene’s contrib packages for other languages)
The combination of an original source of tokens followed by the series of filters that modify the tokens produced by that source, together make up the Analyzer.
You are also free to build up your own analyzer by chaining together Lucene’s token sources and filters, or your own, in customized ways.
This very important step, covered under the “Analyze Document” step in Figure 1.4, is called analysis.
The analysis process produces a stream of tokens that are then written into the files in the index.
After the input has been analyzed, it’s ready to be added to the index.
Lucene stores the input in a data structure known as an inverted index.
This data structure makes efficient use of disk space while allowing quick keyword lookups.
What makes this structure inverted is that it uses tokens extracted from input documents as lookup keys instead of treating documents as the central entities.
If you think about your favorite web search engine and the format of your typical query, you’ll see that this is exactly the query that you want to be as quick as possible.
The core of today’s web search engines are inverted indexes.
Lucene’s index directory has a unique segmented architecture, which we describe next.
Lucene has a rich and detailed index file format that has been carefully optimized with time.
While you really don’t need to know the details of this format in order to use Lucene it’s still helpful to have some basic understanding a high level.
If you find yourself curious about all the details, see Appendix B.
Every Lucene index consists of one or more segments, as depicted in Figure 2.2
Each segment is actually a standalone index itself, holding a subset of all indexed documents.
A new segment is created whenever the writer flushes buffered added and deleted documents into the Directory.
At search time, each segment is visited separately and the results are combined together.
This reduces the number of open file descriptors during searching, at a small cost of searching and indexing performance.
There is one special file, often referred to as “the segments file”, and named segments_<N> that references all live segments.
This file is important!  Lucene first opens this file, and then opens each segment referenced by it.
The value <N>, called “the generation”, is an integer that increases by one every time a change is committed to the index.
Naturally, over time the index will accumulate many segments, especially if you open and close your writer frequently.
Periodically, IndexWriter will select segments and coalesce them by merging all of them into a single new segment.
The selection of segments to be merged is governed by a separate MergePolicy.
Once merges are selected, their actual execution is done by the MergeScheduler.
Let’s now walk through the basic operations (add, update, delete) you do when indexing.
We’ve covered Lucene’s conceptual approach to modeling documents, and then we described the logical steps of the indexing process.
Now it’s time to look at some real code, using Lucene’s APIs to add, remove and update documents.
We start with adding documents to an index since that is the most frequent operation.
Let’s look at how to create a new index and add documents to it.
But be careful!  In order for searches to work correctly you need the analyzer used at search time to “match” the tokens produced by the analyzers at indexing time.
The code in listing 2.1 shows all steps necessary to create a new index, and add two tiny documents.
In this simplistic example, the content for the documents is contained entirely in the source code as Strings, but in the real world the content for your documents would typically come from an external source.
The setUp() method is called by the JUnit framework before every test.
A The setUp() method first creates a new RAMDirectory, to hold the index.
B, #D Next, it creates an IndexWriter on this Directory.
We created the getWriter convenience method since we need to get the IndexWriter in many places #C Finally, setUp() iterates over our content, creating Document and Fields and then adds the Document to the index.
The index contains two documents, each representing a country and a city in that country, whose text is analyzed with WhitespaceAnalyzer.
Since setUp() is called before each test is executed, each test runs against a freshly created index.
In the getWriter method we create the IndexWriter with 3 arguments.
The first argument is the Directory where the index is stored.
The second argument is the analyzer to use when indexing tokenized fields (analysis is covered in Chapter 4)
IndexWriter will detect that there’s no prior index in this Directory and create a new one.
If there were an existing index, IndexWriter would simply add to it.
Some accept a String or File argument, in place of Directory, and will create an FSDirectory at that path.
Others explicitly take a create argument, allowing you to force a new index to be created over an existing one.
Once the index is created, we then construct each document using the for loop.
It’s actually quite simple: first we create a new empty Document, then one by one we add each Field we’d like to have on the document.
After the for loop we close the writer, which commits all changes to the directory.
We could also have called commit(), which would commit the changes to the directory but leave the writer open for further changes.
TestUtil is a utility class that includes a small number of common methods that we re-use throughout the book’s code examples.
It’s methods are quite self-explanatory, and as we use each for the first time we’ll show you the source code.
Next let’s look at the opposite of adding documents: deleting them.
Although most applications are more concerned with getting Documents into a Lucene index, some also need to remove them.
For instance, a newspaper publisher may want to keep only the last week’s worth.
Other applications may want to remove all Documents that contain a certain term or replace an old version of a document with a newer one.
IndexWriter provides various methods to remove documents from an index:
If you intend to delete a single document by Term, you must ensure you’ve indexed a Field on every document, and that all Field values are unique so that each document can be singled out for deletion.
This is the same concepts as a primary key column in a database table.
You can of course name this Field anything you want (“ID” is common)
This field should be indexed as an un-analyzed field (see section 2.4.1) to ensure the analyzer does not break it up into separate tokens.
Be careful with these methods!  If you accidentally specify the wrong Term, for example a Term from.
Instead, they are buffered in memory, just like the added documents, and periodically flushed to disk.
As with added documents, you must call commit() or close() on your writer to commit the changes to the index.
When you delete a document, the disk space for that document is not immediately freed.
Let’s look at Listing 2.2 to see deleteDocuments in action.
We created two test cases, to show the deleteDocuments methods and to illustrate the effect of optimizing after deletion.
Shows the difference between two methods that are often mixed up: maxDoc() and numDocs()
The former returns the total number of deleted or un-deleted documents in the index, while the latter returns the number of un-deleted Documents in an index.
The unit test also demonstrates the use of the hasDeletions() method to check if an index contains any Documents marked for deletion.
Users often confuse the maxDoc() and numDocs() methods in IndexWriter and IndexReader.
The first method, maxDoc() returns the total number of deleted or un-deleted documents in the index, whereas numDocs() returns only the number of un-deleted documents.
We’ve finished adding and deleting documents; now we’ll visit updating documents.
In many applications, after initially indexing a document there may still be further changes to it and you need to re-index it.
For example, if your documents are crawled from a web server, then one way to detect that the content has changed is to look for a changed ETag HTTP header.
If it’s different from when you last indexed the document, then there have been some changes to the content and you should update the document in the index.
The updateDocument methods are probably the most common way to do deletion since they are typically used to replace a single document in the index that has changed.
Since updateDocument uses deleteDocuments under the hood, the same caveat applies: be sure.
We have effectively updated one of the Documents in the index.
We’ve covered the basis on how to add, delete and update documents.
Field is perhaps the most important class when indexing documents: it is the actual class that holds each value to be indexed.
When you create a Field, there are numerous options that you specify to control exactly what Lucene should do with that field once you add the document to the index.
We touched on these options at a high level at the start of this chapter, now it’s time to revisit this topic and enumerate each in more detail.
The options break down into multiple independent categories, which we cover in each subsection below: indexing, storing and term vectors.
After describing those, we enumerate some other values (besides String) that you can assign to a Field.
Finally we enumerate the common combinations of field options, in practice.
Let’s start with the options to control how the Field’s value is added to the inverted index.
Index.ANALYZED – use the analyzer to break the Field’s value into a stream of separate tokens and make each token searchable.
Instead, treat the Field’s entire value as a single token and make that token searchable.
This is useful for fields that you would like to search on, but should not be broken up, such as URLs, file system paths, dates, personal names, Social Security numbers, telephone numbers, and so on.
We indexed the file system path in Indexer (listing 1.1) using this option.
Norms record boost information in the index, but can be memory consuming when searching.
When Lucene builds the inverted index, by default it stores all necessary information to implement the Vector Space model.
This model requires the count of every term that occurred in the document, as well as the positions of each occurrence (needed for phrase searches)
However, sometimes you know the field will be used only for pure Boolean searching and need not contribute to the relevance score.
Fields that are used only for filtering, such as entitlements or date filtering, is a common example.
This will save some disk space in the index, and may also speed up searching and filtering, but will silently prevent searches that require positional information, such as PhraseQuery and SpanQuery, from working.
Let’s move on to controlling how Lucene stores a field’s value.
When the value is stored, the original String in its entirety is recorded in the index and may be retrieved by an IndexReader.
This is useful for fields that you’d like to use when displaying the search results (such as a URL, title or database primary key)
Try not to store very large fields, if index size is a concern, as stored fields consume space in the index.
This is often used along with Index.ANALYZED to index a large text field that doesn’t need to be retrieved in its original form, such as bodies of web pages, or any other type of text document.
Lucene includes a helpful utility class, CompressionTools, that can compress and decompress byte arrays.
Under the hood it simply uses Java’s builtin java.util.Zip classes.
You can use this to compress values before storing them in Lucene.
Note that while this will save space in your index, depending on how compressible the content is, it will slow down indexing and searching.
If the field values are small, compression is rarely worthwhile.
Finally, let’s visit options for controlling how term vectors are indexed.
Sometimes, when you index a document you’d like to retrieve all of its unique terms at search time.
One common use is to speed up highlighting the matched tokens in stored fields.
Another use is to enable a link “Find similar documents” that when clicked runs a new search using the salient terms in an original document.
Section 5.7 show concrete examples of using term vectors, once they are in your index.
But what exactly are term vectors?  Term vectors are something a mix of between an indexed field and a stored field.
They are similar to a stored field because you can quickly retrieve all term vector fields for a given document: term vectors are keyed first by document ID.
But then, they are keyed secondarily by term, meaning they store a miniature inverted index for that one document.
Unlike a stored field, where the original String content is stored verbatim, term vectors store the actual separate terms that were produced by the Analyzer.
This allows you to retrieve all terms, and the frequency of their occurrence.
Note that you cannot index term vectors unless you’ve also turned on indexing for the field.
We’re done with the detailed options to control indexing, storing and term vectors.
In this case the value cannot be stored (hardwired to Store.NO) and is always analyzed and indexed (Index.ANALYZED)
Field(String name, TokenStream tokenStream, TermVector TermVector) allows you to pre-analyze the field value into a TokenStream.
Likewise, such fields are not stored and are always analyzed and indexed.
Field(String name, byte[] value, Store store) is used to store a binary field.
Such fields are never indexed (Index.NO), and have no term vectors (TermVector.NO)
It should be clear by now that Field is quite a rich class and exposes a number of options to express to Lucene precisely how its value should be handled.
Let’s see how these options are typically combined in practice.
Table 2.2 provides a summary of different field characteristics, showing you how fields are created, along with common usage examples.
You’ve now seen all the options for the three categories (indexing, storing and term vectors) you can use to control how Lucene handles a field.
These options can nearly be set independently, resulting in a number of possible combinations.
Table 2.2 lists some commonly used options and their example usage, but remember you are free to set the options however you’d like!
Now that we’re done with the exhaustive indexing options for fields, we’ll change gears and talk about how to handle a field that has more than one value.
Suppose your documents have an author field, but sometimes there’s more than one author for a document.
One way to handle this would be to loop through all the authors, appending them into a single String, which you could then use to create a Lucene Field.
Another, perhaps more elegant way is to just keep adding the same Field with different value, like this:
This is perfectly acceptable and encouraged, as it’s a natural way to represent a field that legitimately.
Internally, whenever multiple fields with the same name appear in one document, both the inverted index and term vectors will logically append the tokens of the field to one another, in the order the fields were added.
There are some advanced options during analysis that control certain important details of this appending; see section 4.2.5 for details.
Sometimes, certain document or fields are known to be important; Lucene offers a feature called boosting to express this.
Not all Documents and Fields are created equal—or at least you can make sure that’s the case by selectively boosting Documents or Fields.
Imagine you have to write an application that indexes and searches corporate email.
Perhaps the requirement is to give company employees’ emails more importance than other email messages when sorting search results.
Document boosting is a feature that makes such a requirement simple to implement.
By default, all Documents have no boost—or, rather, they all have the same boost factor of 1.0
By changing a Document’s boost factor, you can instruct Lucene to consider it more or less important with respect to other Documents in the index, when computing relevance.
The API for doing this consists of a single method, setBoost(float), which can be used as follows (note that certain methods below, like getSenderEmail and isImportant, are not defined in this fragment, but are included in the full examples sources available for download on http://manning.com):
In this example, we check the domain name of the email message sender to determine whether the sender is a company employee.
When we encounter messages from a sender associated with a fictional bad domain, as checked by isUnimportant, we label them as nearly insignificant by lowering their boost factor to 0.1
Just as you can boost Documents, you can also boost individual Fields.
When you boost a Document, Lucene internally uses the same boost factor to boost each of its Fields.
Imagine that another requirement for the email-indexing application is to consider the subject Field more important than the Field with a sender’s name.
In other words, search matches made in the subject Field should be more valuable than equivalent matches in the senderName Field in our earlier example.
To achieve this behavior, we use the setBoost(float) method of the Field class:
The boost factor values you should use depend on what you’re trying to achieve; you may need to do a bit of experimentation and tuning to achieve the desired effect, but remember when you want to change the boost on a field or document you will have to fully remove and then re-add the entire document, or use the updateDocument method, which does the same thing.
It’s worth noting that shorter Fields have an implicit boost associated with them, due to the way Lucene’s scoring algorithm works.
To override this logic, you can implement your own Similarity class, and tell IndexWriter to use it by calling its setSimilarity method.
Boosting is, in general, an advanced feature that many applications can work very well without, so tread carefully!
Document and Field boosting come into play at search time, as you’ll learn in section 3.5.9
Lucene’s search results are ranked according to how closely each Document matches the query, and each matching Document is assigned a score.
Lucene’s scoring formula consists of a number of factors, and the boost factor is one of them.
Alternatively, you may want to boost only at search time, as described in Section 6.1 “custom sorting”
The benefit of this approach is it’s far more dynamic (every search could choose to boost or not to boost)
At search time, you can even expose the choice to the user, such as a checkbox that asks “boost recently modified documents?”
Still, you should be careful: too much boosting, especially without corresponding transparency in the search user interface explaining that certain documents were boosted, can quickly and.
But how are these boosts stored in the index?  This is what norms are for.
During indexing, all sources of index-time boosts are combined together into a single floating point number for each indexed field in the document.
The document may have its own boost; each field may have a boost; finally, Lucene computes a boost based on the number of tokens in the field (shorter fields have a higher boost)
These boosts are combined and then compactly encoded (quantized) into a single byte, which is stored per field per document.
During searching, norms for any field being searched are loaded into memory, decoded back into a floating point number and used to compute the relevance score.
Even though norms are initially computed during indexing, it’s also possible to change them after the fact using IndexReader’s setNorm method.
This is a very advanced method, requiring you to recompute your own norm, but it’s a potentially powerful way to factor in highly dynamic boost factors such as document recency or click-through popularity.
One problem often encountered with norms is that their high memory usage at search time.
This is because the full array of norms, which requires one byte per document per separate field searched, is loaded into RAM.
For a large index with many fields per document, this can quickly add up to a lot of RAM.
This will potentially affect scoring, because no boost information will be used during searching, but it’s possible the effect is trivial, especially when the fields tend to be roughly the same length and you’re not doing any boosting on your own.
But beware: if you decide partway through indexing to turn norms off, you must rebuild the entire index because if even a single document has that field indexed with norms enabled, then through segment merging this will “spread” such that all documents consume one byte even if they had disabled norms.
This is because Lucene does not use sparse storage for norms.
We’ll switch gears now and talk about how to index commonly encountered field values, including dates, times, numbers and fields you plan to sort on.
However, in the real world we encounter many different an interesting types like dates, integers, floating point numbers, etc.
Fortunately, there is helpful support to provide type-specific behavior despite the fact that internally Lucene treats all tokens as string.
When converting a type to a string, it’s necessary to choose a format where a string sort in natural string order “matches” the corresponding sort order of the original type.
Let’s first look at Dates, which Lucene conveniently provides internal support for.
Email messages include sent and received dates, files have several timestamps associated with them, and HTTP responses have a Last-Modified header that includes the date of the requested page’s last modification.
Chances are, like many other Lucene users, you’ll need to index dates.
Lucene comes equipped with a DateTools class, to facilitate conversion from Date to String and vice/versa which makes date indexing easy.
For example, to index today’s date, you can do this:
DateTools simply formats the date and time in the format YYYYMMDDhhmmss, stripping off the suffix.
It’s easy to see that this format ensures that sorting by string value will also match sorting by the original date value.
The finer the resolution, the more distinct terms will be indexed, which as you’ll see in section 6.5, can cause performance problems for certain types of queries.
In practice, you rarely need dates that are precise down to the millisecond, at least to query on.
Generally, you can round dates to an hour or even to a day.
If the full timestamp needs to be preserved, but only for retrieval and presentation (not searching), index a second Field using the finer resolution.
This will enable far more efficient date-only searching while not losing the time portion of the date for presentation.
If you choose to format dates or times in some other manner, take great care that the String representation is lexicographically orderable; doing so allows for sensible date-range queries.
A benefit of indexing dates in YYYYMMDD format is the ability to query by year only, by year and month, or by exact year, month, and day.
To query by year only, use a PrefixQuery for YYYY, for example.
Handling dates was wonderfully simple, thanks to Lucene’s builtin DateTools class.
There are two common scenarios in which number indexing is important.
In one scenario, numbers are embedded in the text to be indexed, and you want to make sure those numbers are indexed so that you can use them later in searches.
In the other scenario, you have Fields that contain only numeric values, and you want to be able to index them and use them for searching.
Moreover, you may want to perform range queries using such Fields.
For example, if you’re indexing email messages, one of the possible index Fields could hold the.
Lucene can index numeric values by treating them as strings internally.
If you need to index numbers that appear in free-form text, the first thing you should do is pick an Analyzer that doesn’t discard numbers.
As we discuss in section 4.3, WhitespaceAnalyzer and StandardAnalyzer are two possible candidates.
On the other hand, SimpleAnalyzer and StopAnalyzer discard numbers from the token stream, which means the search for 1099 won’t match any documents.
If in doubt, use Luke, which is a wonderful tool for inspecting all details of a Lucene index, to check whether numbers survived your analyzer and were added to the index.
However, before just adding their raw values to the index, you need to manipulate them a bit, in order for range queries to work as expected.
When performing range queries, Lucene uses lexicographical values of Fields for ordering.
Notice that the natural and the lexicographical order of the numbers is now consistent.
For more details about searching numeric Fields, see section 6.3.3
When you index Fields with numeric values, prefix them with zeros if you want to use them for range queries.
Many applications allow their users to sort on certain fields instead of the default score.
When returning documents that match a search, Lucene orders them by their score by default.
Sometimes, however, you need to order results using some other criteria.
For instance, if you’re searching email messages, you may want to order results by sent or received date, or perhaps by message size.
Fields used for sorting must be convertible to Integers, Floats, or Strings:
Although we’ve indexed numeric values as Strings, you can specify the correct Field type (such as Integer or Long) at sort time, as described in section 5.1.7
Fields used for sorting have to be indexed and must contain one token per document.
Some applications index documents whose sizes aren’t known in advance.
As a safety mechanism to control the amount of RAM and hard-disk space used, they may need to limit the amount of input they index.
It’s also possible that a large binary document is accidentally mis-classified as a text document, or contains binary content embedded in it that your document filter failed to process, which quickly adds many absurd binary terms to your index, much to your horror.
Other applications deal with documents of known size but want to index only a portion of each document.
For example, you may want to index only the first 200 words of each document.
To support these diverse cases, IndexWriter allows you to truncate per-Field indexing such that only the first N terms are indexed for an analyzed field.
When you instantiate IndexWriter, you must pass in a MaxFieldLength instance expressing this limit.
After creating IndexWriter, you may alter the limit at any time by calling setMaxFieldLength, or retrieve the limit with getMaxFieldLength.
However, any documents already indexed will have been truncated at the previous value: changes to maxFieldLength are not retroactive.
If there are multiple Field instances by the same name, the truncation applies separately to each of them, meaning each field has its first N terms indexed.
Think carefully before using any field truncation!  It means that only the first N terms are available for searching, and any text beyond the Nth term is completely ignored.
Eventually users will notice that your search engine fails to find certain document in certain situations.
There have been many times when someone asks the Lucene user’s list “why doesn’t this search find this document”, and the answer is inevitably “you’ll have to increase your maxFieldLength”
Use maxFieldLength sparingly!  Since truncation means some documents’ text will be completely ignored, your users will eventually discover that your search fails to find some documents.
User trust is the most important thing to protect in your application.
We’re done visiting all the interesting things you can do with Fields.
When you index documents, especially many documents or using multiple sessions with IndexWriter, you’ll invariably create an index that has many separate segments.
When you search the index, Lucene must search each segment separately and then combine the results.
While this works flawlessly, applications that handle large indexes should see search performance improvements by optimizing the index, which merges many segments down to one or a few segments.
An optimized index also consumes fewer file descriptors during searching.
After describing the optimization process and the available methods, we’ll talk about disk space consumed during optimization.
Remember that index optimization involves a lot of disk IO, so use it judiciously.
It is a tradeoff of a large one-time cost, for faster searching.
If you only update your index rarely, and do lots of searching between updates, this tradeoff is worthwhile.
Many users are surprised by how much temporary disk space is required by optimize.
Because Lucene must merge segments together, while the merge is running, temporary disk space is used to hold the files for the new segment.
But the old segments cannot be removed until the merge is complete.
This means, roughly, you should expect the size of your index to double, temporarily, during optimize.
Once optimize completes, and you’ve closed all open readers, disk usage will fall back to a lower level than the starting size of the index.
Section 10.3.1 describes overall disk usage of Lucene in more detail.
Recall from chapter 1 that the purpose of Lucene’s abstract Directory class is to present a simple filelike storage API.
Whenever Lucene needs to write to or read from files in the index, it uses the Directory methods to do so.
The most commonly used Directory implementation is FSDirectory, which simply stores files in a real filesystem directory.
Lucene also provides a Directory implementation, called RAMDirectory, that stores all “files” in memory instead of on disk.
This is useful in cases where the index is small enough to fit in available memory and where the index is easily and quickly regenerated from the source documents.
For example, Lucene’s unit tests make extensive use of RAMDirectory to create short-lived indices for testing.
To build a new index in RAMDirectory, simply instantiate your writer like this:
You can then use the writer as you normally would to add, delete or update documents.
Just remember that once the JVM exits, your index is gone! Alternatively, you can load the contents of another Directory otherDir into RAMDirectory like this:
This is typically used to speed up searching of an existing on-disk index when it is small enough.
If you’d like to do the reverse (copying an index from RAMDirectory into another Directory), use this static method:
But beware that this blindly replaces any existing files in otherDir, and you must ensure no IndexWriter is open on the source directory since the copy method does not do any locking.
There are other addIndexes methods in IndexWriter, however, each of them does their own optimize which likely you don’t need or want.
In past versions of Lucene, it was beneficial to control memory buffering yourself by first batch indexing into a RAMDirectory, and then periodically adding the index into an index stored on disk.
However, as of Lucene 2.3, IndexWriter makes very efficient use of memory for buffering changes to the index and this is no longer a win.
See section 10.1.2 for other ways to improve indexing throughput.
There is also MMapDirectory, which is similar to FSDirectory in that it stores files in the file system.
The difference is instead of using normal IO to access the files, it uses memory mapping.
It’s similar to FSDirectory in that all files are stored in a real filesystem directory.
The difference is that it uses java’s native io package (java.nio.*) when reading from the files, which allows it to avoid locking that the normal FSDirectory must do when multiple threads read from the same file.
If your application has many threads sharing a single searcher it’s likely switching to NIOFSDirectory will improve your query throughput.
However, because of known problems with the implementation of java.nio.* under Sun’s JRE on Windows, NIOFSDirectory offers no gains on that platform and is likely slower than FSDirectory.
In this section, we cover three closely related topics: concurrent index access, thread-safety of IndexReader and IndexWriter, and the locking mechanism that Lucene uses to enforce these rules.
These issues are often misunderstood by users new to Lucene.
Understanding these topics is important, because it will eliminate surprises that can result when your indexing application starts serving multiple users simultaneously or when it has to deal with a sudden need to scale by parallelizing some of its operations.
Lucene’s concurrency rules are simple but should be strictly followed:
Any number of IndexReaders may be open at once on a single index.
It doesn’t matter if these readers are in the same JVM or multiple JVMs, or on the same computer or multiple computers.
Remember, though, that within a single JVM it’s best for resource utilization and performance reasons to share a single IndexReader instance for a given index using multiple threads.
For instance, multiple threads or processes may search the same index in parallel.
Only a single writer may be open on an index at once.
Lucene uses a write lock file to enforce this (see section 2.13 below)
As soon as an IndexWriter is created, a write lock is obtained.
Only when that IndexWriter is closed is the write lock released.
IndexReaders may be open even while a single IndexWriter is making changes to the index.
Each IndexReader will always show the index as of the point-in-time that it was opened.
It will not see any changes being done by the IndexWriter, until the writer commits and the reader is re-opened.
Any number of threads can share a single instance of IndexReader or IndexWriter.
These classes are not only thread safe but also thread friendly, meaning they generally scale well as you add threads, assuming your hardware has concurrency, because the amount of synchronized code inside these classes is kept to a minimum.
Figure 2.8 A single IndexWriter can be shared by multiple threads.
In order to enforce a single writer at a time, which means an IndexWriter or an IndexReader doing deletions or changing norms, Lucene uses a file-based lock: If the file write.lock exists in your index directory, a writer currently has the index open.
Be sure to call this before opening an IndexWriter on that Directory instance.
Table 2.2 lists the core locking implementations provided with Lucene.
Beware that if the JVM crashes or IndexWriter is not closed before the JVM exits, this may leave a leftover write.lock file which you must manually remove.
However, this locking implementation may not work correctly over certain shared file systems, notably NFS.
Use this when you know all IndexWriters will be instantiated in a single JVM.
Be careful!  Only use this when you are absolutely certain that Lucene’s normal locking safeguard is not necessary, for example when using a private RAMDirectory with a single IndexWriter instance.
You can also easily implement your own locking implementation, but take care: if you have a bug and accidentally allow two writers access to the same index at once, you will easily corrupt your index.
If you are unsure whether your new lock factory is working properly, use the LockStressTest to find out.
You should be aware of two additional methods related to locking:
This method can be handy when an application needs to check whether the index is locked before attempting to create an IndexWriter.
Although this method gives you power to unlock any Lucene index at any time, using it is dangerous.
Lucene creates locks for a good reason, and unlocking an index while it’s being modified will quickly result in a corrupt and unusable index.
Although you now know about Lucene’s write lock, you should resist touching this file directly.
If you don’t, your code may break if Lucene starts using a different locking mechanism in the future, or even if it changes the name or location of its lock files.
To demonstrate locking, listing 2.6 shows how the write lock prevents more than one writer from accessing an index simultaneously.
In the testWriteLock() method, Lucene blocks the second IndexWriter from opening an index that has already been opened by another IndexWriter.
When we run this code we see an exception stack trace caused by the locked index, which resembles the following stack trace:
As we mentioned earlier, new users of Lucene sometimes don’t have a good understanding of the concurrency issues described in this section and consequently run into locking issues, such as the one show in the previous stack trace.
If you see similar exceptions in your applications, please don’t disregard them if the consistency of your indexes is at all important to you! Lock-related exceptions are typically a sign of a misuse of the Lucene API; if they occur in your application, you should scrutinize your code to resolve them promptly.
Up until now, we’ve covered how Lucene models documents, the steps of the indexing process, and how specifically to use the APIs to create and index Documents and Fields.
We’ll switch gears now to cover some more advanced topics, including using IndexReader to delete documents and the disk space consumed by deleted documents.
Then we’ll discuss how IndexWriter manages buffering, flushing, committing and merging.
While these are technical details of the internals of IndexWriter, and you could happily perform indexing without understanding these concepts, you may someday find yourself wondering exactly when and how changes.
We’ll start by discussing a different way to delete documents from an index.
Why would you want two ways to do the same thing?  Well, there are some interesting differences:
This means you could do a search, step through matching document numbers, perhaps apply some application logic, and then pick and choose which document numbers to delete.
IndexWriter cannot expose such a method because document numbers may change suddenly due to merging (see section XXX)
However, IndexReader returns the number of documents deleted, whereas IndexWriter does not.
This is due to a difference in the implementation: IndexReader determines immediately which documents were deleted, and is therefore able to count up the affected documents, whereas IndexWriter simply buffers the deleted Term and applies it later.
IndexReader’s deletions take effect immediately, if you use that same reader for searching.
This means you can do deletion then immediately run a search, and the deleted documents will no longer appear in the search results.
Whereas with IndexWriter, the deletes must be flushed and committed, and then a new IndexReader must be opened, before the deletions take effect.
IndexWriter is able to delete by Query, but IndexReader is not (though it’s not hard to run your own Query and simply delete every document number that was returned)
If you are tempted to use IndexReader for deletion, remember that Lucene only allows one “writer” to be open at once.
Confusingly, an IndexReader that is performing deletions counts as a “writer”
This means you are forced to close any open IndexWriter before doing deletions with IndexReader and vice/versa.
If you find that you are quickly interleaving added and deleted documents, this will slow down your indexing throughput substantially.
It’s better to batch up your additions and deletions, to get better performance.
Generally, unless one of the differences above is compelling for your application, it’s best to simply use IndexWriter for all deletions.
Let’s look at the disk space consumed by deleted documents.
Lucene uses a black-list approach when recording deleted documents in the index.
This means that the document is simply marked as deleted in a bit array, which is a very quick operation, but the data corresponding to that document still consumes disk space in the index.
This is necessary because in an inverted index, a given document’s terms are scattered all over the place, and it would be impractical to try to reclaim that space when the document is deleted.
It’s not until segments are merged, either by normal merging over time or by an explicit call to optimize, that these bytes are reclaimed.
You can also call expungeDeletes to reclaim all disk space consumed by deleted documents.
This call merges any segments that have pending deletions, which might be a somewhat lower cost operation than optimize.
As shown in figure 2.2, when new Documents are added to a Lucene index, or deletions are pending, they’re initially buffered in memory instead of being immediately written to the disk.
This is done for performance reasons, to minimize disk IO.
Periodically, these changes are flushed to the index Directory as a new segment.
IndexWriter triggers a flush according to three possible criteria which are set by the application.
To flush when the buffer has consumed more than a pre-set amount of RAM, use setRAMBufferSizeMB.
The RAM buffer size should not be taken as an exact maximum of memory usage since there are many other factors to consider when measuring overall JVM memory usage.
It’s also possible to flush after a specific number of documents have been added by calling setMaxBufferedDocs.
Flushing happens whenever one of these triggers is hit, whichever comes first.
By default, IndexWriter flushes only when RAM usage is 16 MB.
Figure 2.3 An in-memory Document buffer helps improve Lucene’s indexing performance.
When a flush occurs, the writer creates new segment and deletion files in the Directory.
However, these files are neither visible nor usable to a newly opened IndexReader until the writer commits the changes.
Flushing is done to free up memory consumed by buffered changes to the index, whereas committing is done to make all flushed changes persistent and visible in the index.
This means IndexReader always sees the starting state of the index (when IndexWriter was opened), until the writer commits.
While an IndexWriter is making changes to the index, an IndexReader will not see any of these changes until commit() or close() is called.
Next we talk through some useful APIs to manage multiple commits.
Finally we describe how to involve Lucene in a two-phased commit with other resources.
There’s no question these are advanced topics and most likely your application won’t need to use this functionality; but if you are one of the few applications that do needs this, you’ll be glad to know Lucene makes it straightforward.
JLucene implements the ACID transactional model, with the restriction that only one transaction (writer) may be open at once.
Here’s what ACID stands for, along with details about how Lucene meets it:
Atomic – all changes done with the writer are either committed to the index, or none are; there is nothing in-between.
Consistency – the index will also be consistent, for example you will never see a delete without the corresponding addDocument from updateDocument; you will always see all or none of the indexes added from an addIndexes call.
Isolation -- While you are making changes with IndexWriter, no changes are visible to a newly opened IndexReader, until you successfully commit.
Durability -- If your application hits an unhandled exception, or the JVM crashes, or the OS crashes, or the computer loses power, the index will remain consistent and will contain all changes included in the last successful commit.
If your application, the JVM, the OS or the machine crashes, then the index will not be corrupt and will automatically rollback to the last successful commit.
However, Lucene relies on the OS and IO system that holds the index to properly implement the “fsync” system call, by flushing any OS or IO write caches to the actual underlying stable storage.
In some cases, it may be necessary to disable write caching on the underlying IO devices.
Note that commit can be a costly operation, and doing so frequently will slow down your indexing throughput.
Sync all newly created files, including newly flushed files and also any files produced by merges that have finished, since commit was last called or since the IndexWriter was opened.
IndexWriter calls Directory.sync to achieve this, which does not return until all pending writes in the specified file have been written to stable storage on the underlying IO system.
This is usually a costly operation as it forces the OS to flush any pending writes.
Once this completes, IndexReaders will suddenly see all changes done since the last commit.
You can create your own implementation of this class to customize which commits are deleted, and when.
Let’s look at how you can keep multiple commits present in a single index.
Most of the time you should simply use this default.
But for some advanced applications, where you’d like to keep an old point-in-time snapshot around even though further changes have been committed to the index, you may implement your own policy.
For example, when sharing an index over NFS, it may be necessary to customize the deletion policy such that a commit is not deleted until all readers using the index have switched to the most recent commit, based on application specific logic (see section 2.13 for indexing and searching over NFS)
Another example is a retail company that would like to keep the last N versions of its catalog available for searching.
Note that whenever your policy chooses to keep a commit around, that commit will necessarily consume additional disk space in the index.
If you keep multiple commits in your index, there are some useful APIs to help you tell them apart.
However, by implementing a custom deletion policy, you can easily accumulate many commits in the index.
Then, you can step through each and gather whatever details you need.
This string may store something meaningful to your application, enabling you to pick out a particular commit of interest.
Once you’ve found a commit, you can open an IndexReader on it: several of the static open methods accept an IndexCommit.
You could use this to explicitly search a previous version of the index.
Using the same logic, you can open an IndexWriter on a prior commit, but the use case is very different: this allows you rollback to a previous commit, and start indexing new documents from that point, effectively undoing all changes to the index that had happened after that commit.
This is similar to IndexWriter’s rollback method, except that method only rolls back changes done within the current.
IndexWriter session, whereas opening on a prior commit lets you rollback changes that were already committed to the index, perhaps long ago.
After prepareCommit() is called, you should then either call rollback(), to abort the commit, or commit()
Commit() is a very fast call if prepareCommit() was already called.
If an error will be hit, for example disk full, most likely prepareCommit() will hit the error, not commit()
The separation of these two steps of committing allows you to build a distributed two-phase commit protocol involving Lucene.
Next we describe how Lucene merges segments, and what you can do to control this process.
When an index has too many segments, IndexWriter selects some of the segments and merges them into a single, large segment.
It reduces the number of segments in the index because once the merge completes, all of the old segments are removed and a single large segment is added in their place.
This makes searching faster since there are fewer segments to search, and also prevents hitting the file descriptor limit enforced by the operating system.
For example, if there were deletes pending on the merged segments, the merging process frees up the bytes consumed by deleted documents.
Even if there are no pending deletions, a single merged segment will generally use fewer bytes to represent exactly the same set of indexed documents.
So when exactly is a merge necessary?  What specifically does “too many segments” mean?  That is decided by the MergePolicy.
But, MergePolicy only decides which merges should be done; it’s up to MergeScheduler to actually carry out these merges.
Whenever new segments are flushed, or a previously selected merge has completed, the MergePolicy is consulted to determine if a merge is now necessary, and if so, precisely which segments will be merged.
Besides picking “normal” segment merges to do, the MergePolicy also selects merges necessary to optimize the index and to run expungeDeletes.
Lucene provides two core merge policies, both subclassing from LogMergePolicy.
This policy measures the size of a segment as the total size in bytes of all files for that segment.
The second one, LogDocMergePolicy, makes the same merging decisions except it measures size of a segment by the document count of the segment.
If the core merge policies don’t suit your application, you can subclass MergePolicy to implement your own.
For example, you could implement a time-dependent policy that defers large merges until offpeak hours, to ensure merging doesn’t conflict with ongoing searches.
Or perhaps you’d like a policy that tries harder to select segments with many pending deletions, so as to reclaim disk space sooner in the index.
Some of these are also exposed as convenience methods in IndexWriter.
Limits the size in bytes of a segment to be merged.
Limits the number of documents for a segment to be merged.
To understand these parameters we first must understand how both of these policies select merges.
For each segment, its level is computed using this formula:
This effectively groups the segments of roughly equal size (in log space) into the same level.
Tiny segments, less than minMergeMB, are always forced into the lowest level to prevent too many tiny segments in the index.
In general, each level contains segments that are up to mergeFactor times larger than the previous level.
Once a given level has mergeFactor or more segments, they are merged.
Thus, mergeFactor controls not only when to trigger a merge but also how many segments are merged at once.
The larger this is, the more segments will exist in your index and the less frequently merges will be done, for a given number of documents in the index.
Larger values generally result in faster indexing throughput, but may result it too many open file descriptors (see Section 10.3.2 for more details on controlling file descriptor usage)
It’s probably best to leave this at its default value (10) unless you see strong gains when testing different values.
When the merge completes, a new segment at the next higher level replaces the merged segments.
To prevent merges of very large segments, set maxMergeMB or maxMergeDocs.
If ever a segment is over maxMergeMB in byte size, or maxMergeDocs in its document count, that segment will never be merged.
By setting maxMergeDocs you can force extremely large segments to remain separate forever in your index.
Besides selecting merges for normal ongoing maintenance of the index, MergePolicy is also responsible for selecting merges when optimize or expungeDeletes is called.
In fact, it’s really up to the MergePolicy to define what these methods actually mean.
For example, maybe during optimize you want to skip segments larger than a certain size.
Or perhaps for expungeDeletes you only want to merge a segment if it has more than 10% of its documents deleted.
The number of segments in your index is proportional to the logarithm of the net size, in bytes or number of documents, of your index.
This generally does a good job keeping segment count low while minimizing the net merge cost.
However, some of these settings can be tuned to improve indexing throughput, as described in section 10.1.2
IndexWriter relies on a subclass of MergeScheduler to achieve this.
You could also implement your own MergeScheduler: perhaps you want to defer very large segment merges until after 2 AM but do smaller merges whenever they are needed.
Generally, customizing MergePolicy settings, or implementing your own MergePolicy or MergeScheduler, are extremely advanced use cases.
If you are curious about when IndexWriter is flushing and merging, you can call its setInfoStream method, as described in Section 2.16
We’ll switch gears now and talk about how to make Lucene work over a common unix remote filesystem.
The NFS file system, or Networked File System, is ubiquitous and very useful on Unix platforms as a means of sharing files across multiple computers, and sharing a Lucene index is no exception.
One common approach is to have one computer create and update the index on a directory which is then shared over NFS to multiple computers that do the searching.
While this is convenient, since it avoids having to replicate copies of the index out to multiple computers, NFS presents certain challenges for Lucene that you must work around.
The core challenge with using Lucene over NFS is how NFS handles deletion of files that are still held open on other computers.
For example, Windows simply disallows deletion of an open file, whereas most native Unix file systems allow the deletion to proceed, but the actual bytes of the file remain allocated on disk until all open file handles are closed (this is called “delete on last close” semantics)
In both approaches, an open file handle can still be used to read all bytes in the file after the file deletion is attempted.
Alternatively, on hitting the “Stale NFS file handle” during searching, you could at that moment reopen your searcher and then redo the search.
This is a viable approach only if reopening a searcher is not too time consuming.
Otherwise, the unlucky query that hit the error will take unacceptably long to get results.
Finally, realize that performance over NFS is not great, because the bytes must cross the wires to get to the computer doing the searching.
It’s possible mounting the NFS directory as read-only may improve the performance, but likely you’ll still be far from the performance of a local native directory.
Our final topic for this already quite long chapter shows you how to gain some insight into the internal operations IndexWriter is doing.
Let’s discuss one final, fairly unknown Lucene feature (if we may so call it)
If you ever need to debug Lucene’s index-writing process, remember that you can get Lucene to output information about its indexing operations by setting IndexWriter’s setInfoStream method, passing in an OutputStream such as System.out:
This reveals very detailed diagnostic information about segment flushes and merges, as shown here, and may help you tune indexing parameters described earlier in the chapter.
Likely if you are experiencing an issue, something you may believe to be a bug in Lucene, and you take your issue to the Lucene user’s list at Apache, the first request you’ll get back is someone asking you to post the output from setting infoStream.
In addition, if you need to peek inside your index once it’s built, you can use Luke, a handy third-party.
We’ve covered a lot of ground in this chapter!  You now have a solid understanding of how to make changes to a Lucene index.
You saw Lucene’s conceptual model for documents and fields, including a flexible but flat schema (when compared to a database)
We saw that the indexing process consists of gathering content, extracting text from it, creating Documents and Fields from it, analyzing the text into a token stream and then handing it off to IndexWriter for addition to an index.
We also briefly discussed the interesting segmented structure of an index.
You now know how to add, delete and update documents.
We delved into a great many interesting options for controlling how a Field is indexed, including how the value is added to the inverted index, stored fields and term vectors, and how a Field can hold certain values other than String.
We described variations like multi-valued fields, field and document boosting, and value truncation.
You now know how to index dates,  times and numbers, as well as fields for sorting.
We discussed segment-level changes, like optimizing and index and using expungeDeletes to reclaim disk space consumed by deleted documents.
You now know of all the Directory implementations you could use to hold an index, such as RAMDirectory and NIOFSDirectory.
We discussed Lucene’s concurrency rules, and the locking it uses to protect an index from more than one writer.
Finally we covered a number of advanced topics: how and why to delete documents using IndexReader instead of IndexWriter; buffering, flushing and committing; IndexWriter’s support for transactions; merging and the classes available for customizing it; using an index over the NFS file system; and turning on IndexWriter’s infoStream to see details on the steps its taking internally.
Much of this advanced functionality will not be needed by the vast majority of search applications; in fact a few of IndexWriter’s APIs are enough to build a solid search application.
By now you should be dying to learn how to search with Lucene, and that’s what you’ll read about in the next chapter.
Even if we have indexed documents, our effort is wasted unless it pays off by providing a reliable and fast way to find those documents.
Give me a list of all books published in the last 12 months on the subject of “Java” where “open source” or “Jakarta” is mentioned in the contents.
Restrict the results to only books that are on special.
Oh, and under the covers, also ensure that books mentioning “Apache” are picked up, because we explicitly specified “Jakarta”
And make it snappy, on the order of milliseconds for response time.
Do you have a repository of hundreds, thousands, or millions of documents that needs similar search capability? Providing search capability using Lucene’s API is straightforward and easy, but lurking under the covers is a sophisticated mechanism that can meet your search requirements, such as returning the most relevant documents first and retrieving the results incredibly quickly.
This chapter covers common ways to search using the Lucene API.
The majority of applications using Lucene search can provide a search feature that performs nicely using the techniques shown in this chapter.
We begin with a simple example showing that the code you write to implement search is generally no more than a few lines long.
Next we illustrate the scoring formula, providing a deep look into one of Lucene’s most special attributes.
With this example and a high-level understanding of how Lucene ranks search results, we’ll then explore the various types of search queries Lucene handles natively.
Finally we show how to create a search query from a text search expression entered by the end user.
You’ve tackled getting the data indexed, but now it’s time to expose the full-text searching to the end users.
It’s hard to imagine that adding search could be any simpler than it is with Lucene.
Obtaining search results requires only a few lines of code, literally.
Lucene provides easy and highly efficient access to those search results, too, freeing you to focus your application logic and user interface around those results.
Of course, as described in Chapter 2, you will first have to build up a search index.
In this chapter, we’ll limit our discussion to the primary classes in Lucene’s API that you’ll typically use for search integration (shown in table 3.1)
All searches come through an IndexSearcher instance using any of the several overloaded search methods.
Query (and subclasses) Concrete subclasses encapsulate logic for a particular query type.
QueryParser Processes a human-entered (and readable) expression into a concrete Query object.
Lucene computes a score (a numeric value of relevance) for each document, given a query.
The ScoreDocs themselves aren’t the actual matching documents, but rather are references, via an integer document ID, to the documents matched.
In most applications that display search results, users access only the first few documents, so it isn’t necessary to retrieve the actual documents for all results; you need to retrieve only the documents that will be presented to the user.
For large indexes, it wouldn’t even be possible to collect all matching documents into available physical computer memory.
In the next section, we put IndexSearcher, Query, TopDocs and ScoreDoc to work with some basic term searches.
After that, we show how to use QueryParser to create Query instances from an end user’s textual search query.
IndexSearcher is the central class used to search for documents in an index.
You can search for a specific term using the most commonly used search method.
A term is a String value that is paired with its containing field name—in this case, subject.
Important: The original text may have been normalized into terms by the analyzer, which may eliminate terms (such as stop words), convert terms to lowercase, convert terms to base word forms (stemming), or insert additional terms (synonym processing)
It’s crucial that the terms passed to IndexSearcher be consistent with the terms produced by analysis of the source documents during indexing.
Using our example book data index, which is stored in the build/index subdirectory with the book’s source code, we’ll query for the words ant and junit, which are words we know were indexed.
Listing 3.1 performs a term query and asserts that the single document expected is found.
Lucene provides several built-in Query types (see section 3.4), TermQuery being the most basic.
We’ll discuss this object in section 3.2, but for now just.
Full documents aren’t immediately returned; instead, you fetch them on demand.
In this example we didn’t concern ourselves with the actual documents associated with the docs returned because we were only interested in checking that the proper number of documents were found.
Note that we close the searcher, and then the directory, after we are done.
In a real application, it’s best to keep these open and share a single searcher for all queries that need to run.
Opening a new searcher can be a costly operation as it must load and populate internal data structures from the index.
This example programmatically constructed a very simple query (a single term)
Next, we discuss how to transform a user-entered query expression into a Query object.
Figure 3.XXX Using QueryParser to search using a textual expression.
Two more features round out what the majority of searching applications require: sophisticated query expression parsing and access to the documents returned.
Parsing a query expression is the act of turning a user-entered textual query such as “mock OR junit” into an appropriate Query object instance1; in this case, the Query  object would be an instance of BooleanQuery  with two non-required clauses, one for each term.
The following code parses two query expressions and asserts that they worked as expected.
After returning the hits, we retrieve the title from the first document found:
It parses rich expressions such as the two shown ("+JUNIT +ANT -MOCK" and "mock OR junit") into one of the Query implementations.
The resulting Query instances can be very rich and complex!  Dealing with human-entered queries is the primary purpose of the QueryParser.
As you can see in Figure 3.1, QueryParser requires an analyzer to break pieces of the query text into terms.
The terms of the contents field, however, were lowercased when indexed.
QueryParser, in this example, uses SimpleAnalyzer, which lowercases the terms before constructing a Query object.
Analysis is covered in great detail in the next chapter, but it’s intimately intertwined with indexing text and searching with QueryParser.
The main point regarding analysis to consider in this chapter is that you need to be sure to query on the actual terms indexed.
QueryParser is the only searching piece that uses an analyzer.
Querying through the API using TermQuery and the others discussed in section 3.4 doesn’t use an analyzer but does rely on matching terms to what was indexed.
Therefore, if you construct queries entirely programmatically you must ensure the Terms included in all of your queries match the Tokens produced by the analyzer used.
In section 4.1.2, we talk more about the interactions of QueryParser and the analysis process.
Equipped with the examples shown thus far, you’re more than ready to begin searching your indexes.
There are, of course, many more details to know about searching.
Next is an overview of how to use QueryParser, which we return to in greater detail later in this chapter.
QueryParser is instantiated with a field name (String) and an Analyzer, which it uses to break the incoming search text into Terms.
We discuss analyzers in detail in the next chapter and then cover the interactions between QueryParser and the analyzer in section 4.1.2:
The provided field name is the default field against which all terms will be searched, unless the search.
Then, the QueryParser instance has a parse() method to allow for the simplest use:
The query String is the expression to be parsed, such as “+cat +dog”
If the expression fails to parse, a ParseException is thrown, a condition that your application should.
ParseException’s message gives a reasonable indication of why the parsing failed; however, this description may be too technical for end users.
The parse() method is quick and convenient to use, but it may not be sufficient.
There are various settings that can be controlled on a QueryParser instance, such as the default operator when multiple terms are used (which defaults to OR)
These settings also include locale (for date parsing), default phrase slop, the minimum similarity and prefix length for fuzzy queries, the date resolution, whether to lowercase wildcard queries, and various other advanced settings.
Contain the term java or junit, or both, in the default field2
Have extreme in the title field and don’t have sports in the subject field.
Contain methodology and must also contain agile and/or extreme, all in the default field.
With this broad picture of Lucene’s search capabilities, you’re ready to dive into details.
We’ll revisit QueryParser in section 3.5, after we cover the more foundational pieces.
Like the rest of Lucene’s primary API, IndexSearcher is simple to use.
The simplest way to create an IndexSearcher is by providing it a String file path to your index directory in the filesystem:
You can also pass in an instance of java.io.File, or your own Directory instance, which allows you to use Directory implementations other than FSDirectory.
If you already have an open IndexReader instance that you’d like to use for searching, you can create an IndexSearcher from that as well.
After constructing an IndexSearcher, we call one of its search methods to perform a search.
The main search methods available to an IndexSearcher instance are shown in table 3.3
This chapter only deals with search(Query, int) method, and that may be the only one you need to concern yourself with.
The other search method signatures, including the filtering and sorting variants, are covered in chapter 5, Advanced Search Techniques.
The int n parameter is how many top scoring documents to return.
Searches constrained to a subset of available documents, based on filter criteria.
Searches constrained to a subset of available documents based on filter criteria, and sorted by a custom Sort object.
Used when you have custom logic to implement for each document visited, or you’d like to collect a different subset of documents than the top N by the sort criteria.
Same as above, except documents are only accepted if they pass the filter criteria.
An IndexSearcher instance searches only the index as it existed at the time the IndexSearcher was instantiated.
If indexing is occurring concurrently with searching, newer documents indexed won’t be visible to searches.
In order to see the new documents, you must commit the changes from IndexWriter and then instantiate a new IndexSearcher.
Most of IndexSearcher’s search methods return TopDocs, which we cover next, to represent the returned results.
Now that we’ve called search(Query, n), we have a TopDocs object at our disposal which we use for efficient access to the search results.
Typically, you’ll use one of the search methods that return a TopDocs object, as shown in table 3.3
Results are ordered by relevance—in other words, by how well each document matches the query (sorting results in other ways is discussed in section 5.1)
There are only three attributes and methods on a TopDocs instance; they’re listed in table 3.4
A matching document is one with a score greater than zero, as defined by the scoring formula covered in section 3.3
The matches, by default, are sorted in decreasing score order.
But if you sort by other criteria, as described in section 5.1, it will be the max score of all matching documents even when the best scoring document isn’t in the top results by your sort criteria.
Paging through ScoreDocs is a common need, although if you find users are frequently doing a lot of paging you should revisit your design: ideally the user almost always finds the result on the first page.
Gather multiple pages worth of results on the initial search and keep the resulting ScoreDocs and IndexSearcher  instances available while the user is navigating the search results.
Requery each time the user navigates to a new page.
It turns out that requerying is most often the best solution.
Requerying at first glance seems a waste, but Lucene’s blazing speed more than compensates.
Also, thanks to the IO caching in modern operating systems, requerying will typically be fast because the necessary bits from disk will already be cached in RAM.
Frequently users don’t click past the first page of results anyway.
In order to requery, the original search is reexecuted, with a larger n, and the results are displayed beginning on the desired page.
How the original query is kept depends on your application architecture.
In a web application where the user types in an expression that is parsed with QueryParser, the original expression could be made part of the hyperlinks for navigating the pages and reparsed for each request, or the expression could be kept in a hidden HTML field or as a cookie.
Don’t prematurely optimize your paging implementations with caching or persistence.
First implement your paging feature with a straightforward requery approach; chances are you’ll find this sufficient for your needs.
We chose to discuss this complex topic early in this chapter so you’ll have a general sense of the various factors that go into Lucene scoring as you continue to read.
We’ll describe how Lucene scores document matches to a query, and then show you how to get a detailed explanation of how a certain document arrived at its score.
Without further ado, meet Lucene’s similarity scoring formula, shown in figure 3.1
It’s called the similarity scoring formula because its purpose is to measure the similarity between a query and each document that matches the query.
The score is computed for each document (d) matching each term (t) in a query (q)
Figure 3.1 Lucene uses this formula to determine a document score based on a query.
If this equation or the thought of mathematical computations scares you, you may safely skip this section.
Lucene scoring is top-notch as is, and a detailed understanding of what makes it tick isn’t necessary to take advantage of Lucene’s capabilities.
Typically, if an application presents the score to the end user, it’s best to first normalize the scores by dividing all scores by the maximum score for the query.
The larger the similarity score, the better the match of the document to the query.
By default Lucene returns documents reverse-sorted by this score, meaning the top documents are the best matching ones.
Table 3.5 describes each of the factors in the scoring formula.
Very common terms have a low idf; very rare terms have a high idf.
You may use this to statically boost certain fields and certain documents over others.
This value is computed during indexing and stored in the index norms.
Shorter fields (fewer tokens) get a bigger boost from this factor.
The coordination factor gives an AND-like boost to documents that contain more of the search terms than other documents.
Boost factors are built into the equation to let you affect a query or field’s influence on score.
Field boosts come in explicitly in the equation as the boost(t.field in d) factor, set at indexing time.
During indexing, a Document can be assigned a boost, too.
A Document boost factor implicitly sets the starting field boost of all fields to the specified value.
Fieldspecific boosts are multiplied by the starting value, giving the final value of the field boost factor.
It’s possible to add the same named field to a Document multiple times, and in such situations the field boost is computed as all the boosts specified for that field and document multiplied together.
In addition to the explicit factors in this equation, other factors can be computed on a per-query basis as part of the queryNorm factor.
Queries themselves can have an impact on the document score.
Boosting a Query instance is sensible only in a multiple-clause query; if only a single term is used for searching, boosting it would boost all matched documents equally.
In a multiple-clause boolean query, some documents may match one clause but not another, enabling the boost factor to discriminate between matching documents.
Most of these scoring formula factors are controlled and implemented as a subclass of the abstract Similarity class.
More computations are performed under the covers of DefaultSimilarity; for example, the term frequency factor is the square root of the actual frequency.
Because this is an “in action” book, it’s beyond the book’s scope to delve into the inner workings of these calculations.
In practice, it’s extremely rare to need a change in these factors.
Should you need to change these factors, please refer to Similarity’s Javadocs, and be prepared with a solid understanding of these factors and the effect your changes will have.
It’s important to note that a change in index-time boosts or the Similarity methods used during indexing, such as lengthNorm, require that the index be rebuilt for all factors to be in sync.
Let’s say you’re baffled as to why a certain document got a good score to your Query.
We’re talking about factors that rank one document higher than another based on a query; that in and of itself deserves the sophistication going on.
If you want to see how all these factors play out, Lucene provides a very helpful feature called Explanation.
IndexSearcher has an explain method, which requires a Query and a document ID and returns an Explanation object.
The Explanation  object internally contains all the gory details that factor into the score calculation.
Each detail can be accessed individually if you like; but generally, dumping out the explanation in its entirety is desired.
The .toString() method dumps a nicely formatted text representation of the Explanation.
We wrote a simple program to dump Explanations, shown here:
Using the query junit against our sample index produced the following output; notice that the most.
JUnit in Action has the term junit twice in its contents field.
The contents field in our index is an aggregation of the title and subject fields to allow a single field for searching.
Java Development with Ant has the term junit only once in its contents field.
There is also a .toHtml() method that outputs the same hierarchical structure, except as nested HTML <ul> elements suitable for outputting in a web browser.
In fact, the Explanation feature is a core part of the Nutch project (see the case study in section 10.1), allowing for transparent ranking.
Explanations are handy to see the inner workings of the score calculation, but they expend the same amount of effort as a query.
Now we’ll switch gears and show you how to create queries programmatically.
By now you have a strong foundation for getting your search application off the ground: we showed the most important ways of performing searches with Lucene.
Now, it’s time to drill down into detail on creating Lucene’s queries programmatically.
It turns out Lucene has a number of interesting queries for you to play with!  After that we get back to QueryParser and visit a number of details and interesting topics that may arise.
As you saw in section 3.2, querying Lucene ultimately requires a call to IndexSearcher’s search using an instance of Query.
Query subclasses can be instantiated directly; or, as we discussed in section 3.1.2, a Query can be constructed through the use of a parser such as QueryParser.
If your application will rely solely on QueryParser to construct Query objects, understanding Lucene’s direct API capabilities is still important because QueryParser uses them.
Even if you’re using QueryParser, combining a parsed query expression with an API-created Query is a common technique to augment, refine, or constrain a human-entered query.
For example, you may want to restrict free-form parsed expressions to a subset of the index, like documents only within a category.
Depending on your search’s user interface, you may have date pickers to select a date range, drop-downs for selecting a category, and a free-form search box.
Each of these clauses can be stitched together using a combination of QueryParser, BooleanQuery, RangeQuery, and a TermQuery.
We demonstrate building a similar aggregate query in section 5.5.4
Yet another way to create query objects is by using the XML Query Parser package, contained in Lucene’s contrib sandbox and described in detail in section XXX.
This package allows you to create an XML string describing, in great detail, the specific query you’d like to run; the package then parses that string into a Query instance.
The QueryParser expression syntax that maps to each Query type is provided.
The most elementary way to search an index is for a specific term.
A term is the smallest indexed piece, consisting of a field name and a text-value pair.
Listing 3.1 provided an example of searching for a specific term.
All documents that have the word java in a contents field are returned from searches using this.
Note that the value is case-sensitive, so be sure to match the case of terms indexed; this may not be the exact case in the original document text, because an analyzer (see chapter 5) may have indexed things differently.
TermQuerys are especially useful for retrieving documents by a key.
For example, given our book test data, the following code retrieves the single document matching the ISBN provided:
In our sample book data, isbn is unique among all documents.
A TermQuery is returned from QueryParser if the expression consists of a single word.
The expression java creates a TermQuery, just as we did with the API in testKeyword.
Terms are ordered lexicographically within the index, allowing for straightforward searching of terms within a range.
Lucene’s RangeQuery facilitates searches from a starting term through an ending term.
The beginning and ending terms may either be included or excluded.
When the RangeQuery search is executed, there are two supported approaches under the hood.
The first approach, which is the default, is to expand to an OR query of all terms within the range.
This option has two serious downsides: first, it can be very slow if the number of terms in the range is large.
This can easily happen if the values in the field are indexed with fine granularity.
Second, the relevance scores assigned to the matching documents are counterintuitive and not generally useful.
With this option, the matching documents are first enumerated into an internal bit set and then that bit set is used to match documents.
Each document is assigned a constant score equal to the Query’s boost value.
Generally this gives faster performance and is the recommended usage of RangeQuery.
If you still have performance problems, you may want to switch to TrieRangeQuery, available in Lucene’s sandbox and covered in Section 8.12.5
The third and forth arguments to construct a RangeQuery are boolean flags, indicating whether the begin and end of the range are inclusive, respectively.
Using the same dates and range, but exclusively, no book is found:
If you are searching numeric fields (int, float, etc), don’t forget to zero-pad the numbers during.
Now we move to another query that can match many terms from the index, PrefixQuery.
Searching with a PrefixQuery matches documents containing terms beginning with a specified string.
The following code demonstrates how you can query a hierarchical structure recursively with a simple PrefixQuery.
The documents contain a category keyword field representing a hierarchical structure:
Search for programming books, including subcategories #2 Search only for programming books, not subcategories.
Our PrefixQueryTest demonstrates the difference between a PrefixQuery and a TermQuery.
Books in this subcategory are found with a PrefixQuery but not with the TermQuery on the parent category.
For example, luc* is converted into a PrefixQuery using luc as the term.
See section 3.5.7 for details on how to control this setting.
Our next query, BooleanQuery, is an interesting one because it’s able to embed and combine other queries.
The various query types discussed here can be combined in complex ways using BooleanQuery.
A clause is a subquery that can be optional, required, or prohibited.
These attributes allow for logical AND, OR, and NOT combinations.
You add a clause to a BooleanQuery using this API method:
A BooleanQuery can be a clause within another BooleanQuery, allowing for sophisticated groupings.
First, here’s an AND query to find the most recent books on one of our favorite subjects, search:
Note that this could also be done with a "2004" PrefixQuery.
BooleanQuerys are restricted to a maximum number of clauses; 1,024 is the default.
This limitation is in place to prevent queries from accidentally adversely affecting performance.
A TooManyClauses exception is thrown if the maximum is exceeded.
It may seem that this is an extreme number and that constructing this number of clauses is unlikely, but under the covers Lucene does some of its own query rewriting for queries like RangeQuery and turns them into a BooleanQuery with nested optional (not required, not prohibited) TermQuerys.
The next query, PhraseQuery, differs from the queries we covered so far in that it pays attention to the positional details of multiple term occurrences.
PhraseQuery uses this information to locate documents where terms are within a certain distance of one another.
For example, suppose a field contained the phrase “the quick brown fox jumped over the lazy dog”
Without knowing the exact phrase, you can still find this document by searching for documents with fields having quick and fox near each other.
Sure, a plain TermQuery would do the trick to locate this document knowing either of those words; but in this case we only want documents that have phrases where the words are either exactly side by side (quick fox) or have one word in between (quick [irrelevant] fox)
The maximum allowable positional distance between terms to be considered a match is called slop.
Distance is the number of positional moves of terms to reconstruct the phrase in order.
Let’s take the phrase just mentioned and see how the slop factor plays out.
First we need a little test infrastructure, which includes a setUp() method to index a single document and a custom matched (String[], int) method to construct, execute, and assert a phrase query matched the test document:
Because we want to demonstrate several phrase query examples, we wrote the matched method to simplify the code.
Phrase queries are created by adding terms in the desired order.
By default, a PhraseQuery has its slop factor set to zero, specifying an exact phrase match.
With our setUp() and helper matched method, our test case succinctly illustrates how PhraseQuery behaves.
Terms added to a phrase query don’t have to be in the same order found in the field, although order does impact slop-factor considerations.
For example, had the terms been reversed in the query (fox and then quick), the number of moves needed to match the document would be three, not one.
To visualize this, consider how many moves it would take to physically move the word fox two slots past quick; you’ll see that it takes one move to move fox into the same position as quick and then two more to move fox beyond quick sufficiently to match “quick brown fox”
Figure 3.2 shows how the slop positions work in both of these phrase query scenarios, and this test case shows the match in action:
Regardless of how many terms are used for a phrase, the slop factor is the maximum total number of moves allowed to put the terms in order.
Let’s look at an example of a multiple-term phrase query:
Now that you’ve seen how phrase queries match, we turn our attention to how phrase queries affect the score.
More exact matches count for more weight than sloppier ones.
The inverse relationship with distance ensures that greater distances have lower scores.
Terms surrounded by double quotes in QueryParser parsed expressions are translated into a PhraseQuery.
The slop factor defaults to zero, but you can adjust the slop factor by adding a tilde (~) followed by an integer.
There are additional details about PhraseQuery and the slop factor in section 3.5.6
Phrases are analyzed by the analyzer passed to the QueryParser, adding another layer of complexity, as discussed in section 4.1.2
Wildcard queries let you query for terms with missing pieces but still find matches.
Two standard wildcard characters are used: * for zero or more characters, and ? for zero or one character.
You can think of WildcardQuery as a more general PrefixQuery because the wildcard doesn’t have to be at the end.
Note how the wildcard pattern is created as a Term (the pattern to match) even though it isn’t explicitly used as an exact term under the covers.
Internally, it’s used as a pattern to match terms in the index.
A Term instance is a convenient placeholder to represent a field name and an arbitrary string.
A larger prefix (characters before the first wildcard character) decreases the number of terms enumerated to find matches.
Beginning a pattern with a wildcard query forces the term enumeration to search all terms in the index for matches.
Oddly, the closeness of a wildcard match has no effect on scoring.
The last two assertions in listing 3.2, where wild and mild are closer matches to the pattern than mildew, demonstrate this.
With QueryParser, the first character of a wildcarded term may not be a wildcard character; this restriction prevents users from putting asterisk-prefixed terms into a search expression, incurring an expensive operation of enumerating all the terms.
Also, if the only wildcard character in the term is a trailing asterisk, the query is optimized to a PrefixQuery.
Wildcard terms are lowercased automatically by default, but this can be changed.
See section 3.5.7 for more on wildcard queries and QueryParser.
The Levenshtein distance algorithm determines how similar terms in the index are to a specified target term.3 Edit distance is another term for Levenshtein distance; it’s a measure of similarity between two strings, where distance is measured as the.
For example, the edit distance between three and tree is 1, because only one character deletion is needed.
Levenshtein distance isn’t the same as the distance calculation used in PhraseQuery and PhrasePrefixQuery.
The phrase query distance is the number of term moves to match, whereas Levenshtein distance is an intraterm computation of character moves.
Both documents match; the term searched for (wuzza) wasn’t indexed but was close enough to match.
FuzzyQuery uses a threshold rather than a pure edit distance.
The threshold is a factor of the edit distance divided by the string length.
Edit distance affects scoring, such that terms with less edit distance are scored higher.
Distance is computed using the formula shown in figure 3.4
FuzzyQuery enumerates all terms in an index to find terms within the allowable threshold.
Use this type of query sparingly, or at least with the knowledge of how it works and the effect it may have on performance.
For example, the FuzzyQuery from the previous example would be wuzza~ in a query expression.
Note that the tilde is also used to specify sloppy phrase queries, but the context is different.
Double quotes denote a phrase query and aren’t used for fuzzy queries.
Our final query, before we move onto QueryParser, is MatchAllDocsQuery.
MatchAllDocsQuery, as the name implies, simply matches every document in your index.
By default, it assigns a constant score, the boost of the query (default 1.0), to all documents that match, so if you use this as your toplevel query, it’s best to sort by a field other than the default relevance sort.
It’s also possible to have MatchAllDocsQuery assign as document scores the boosting recorded in the index, for a specified field, like so:
If you do this, documents are scored according to how the specified field was boosted (as described in section 2.xxx)
We’re done reviewing Lucene’s basic core Query classes!  Chapter 5 covers more advanced Query classes.
Now we’ll move onto using QueryParser to construct queries from a user’s textual query.
Although API-created queries can be powerful, it isn’t reasonable that all queries should be explicitly written in Java code.
Using a human-readable textual query representation, Lucene’s QueryParser constructs one of the previously mentioned Query subclasses.
This constructed Query instance could be a complex entity, consisting of nested BooleanQuerys and a combination of almost all the Query types mentioned, but an expression entered by the user could be as readable as this:
Whenever special characters are used in a query expression, you need to provide an escaping mechanism so that the special characters can be used in a normal fashion.
QueryParser uses a backslash (\) to escape special characters within terms.
The following sections detail the expression syntax, examples of using QueryParser, and customizing QueryParser’s behavior.
The discussion of QueryParser in this section assumes knowledge of the query.
We begin with a handy way to glimpse what QueryParser does to expressions.
Seemingly strange things can happen to a query expression as it’s parsed with QueryParser.
How can you tell what really happened to your expression? Was it translated properly into what you intended? One way to peek at a resultant Query instance is to use the toString() method.
All concrete core Query classes we’ve discussed in this chapter have a special toString() implementation.
Calling the no-arg toString() method uses an empty default field name, causing the output to explicitly use field selector notation for all terms.
The toString() methods (particularly the String-arg one) are handy for visual debugging of.
Don’t rely on the ability to go back and forth accurately between a Query.toString() representation and a QueryParser-parsed expression, though.
It’s generally accurate, but an analyzer is involved and may confuse things; this issue is discussed further in section 4.1.2
Constructing Boolean queries textually via QueryParser is done using the operators AND, OR, and NOT.
Terms listed without an operator specified use an implicit operator, which by default is OR.
The query abc xyz will be interpreted as either abc OR xyz or abc AND xyz, based on the implicit operator setting.
Placing a NOT in front of a term excludes documents matching the following term.
Negating a term must be combined with at least one nonnegated term to return documents; in other words, it isn’t possible to use a query like NOT term to find all documents that don’t contain a term.
Each of the uppercase word operators has shortcut syntax; table 3.7 illustrates various syntax equivalents.
One powerful feature of QueryParser is the ability to create nested clauses, using grouping.
Lucene’s BooleanQuery lets you construct complex nested clauses; likewise, QueryParser enables it with textual query expressions.
Let’s find all the methodology books that are either about agile or extreme methodologies.
We use parentheses to form subqueries, enabling advanced construction of BooleanQuerys:
Next, we discuss how a specific field can be selected.
QueryParser needs to know the field name to use when constructing queries, but it would generally be unfriendly to require users to identify the field to search (the end user may not need or want to know the field names)
As you’ve seen, the default field name is provided when you create the QueryParser instance.
Parsed queries aren’t restricted, however, to searching only the default field.
Using field selector notation, you can specify terms in nondefault fields.
For example, when HTML documents are indexed with the title and body areas as separate fields, the default field will likely be body.
You can group field selection over several terms using field:(a b c)
Text or date range queries use bracketed syntax, with TO between the beginning term and ending term.
The type of bracket determines whether the range is inclusive (square brackets) or exclusive (curly brackets)
Our testRangeQuery() method demonstrates both inclusive and exclusive range queries:
This inclusive range uses a field selector since the default field is subject.
Nondate range queries use the beginning and ending terms as the user entered them, without modification.
In other words, the beginning and ending terms are not analyzed.
Start and end terms must not contain whitespace, or parsing fails.
In our example index, the field pubmonth isn’t a date field; it’s text of the format YYYYMM.
If either of the two terms fails to parse as a valid date, they’re both used as is for a textual range.
The reason for this is QueryParser by default uses the old DateField class to parse dates, which internally encodes each date as a long that is then rendered in hexadecimal format.
If instead we set the proper resolution for our date field we get the expected result:
Internally, all terms are text to Lucene, and dates are represented in a lexicographically ordered text format.
Typically the client’s locale would be determined and used, rather than the default locale.
For example, in a web application, the HttpServletRequest object contains the locale set by the client browser.
You can use this locale to control the locale used by date parsing in QueryParser, as shown in listing 3.3
Text analysis is another, more important, place where such concerns are handled.
The text between the quotes is analyzed; thus the resultant PhraseQuery may not be exactly the phrase originally specified.
For example, the query "This is Some Phrase*", when analyzed by the StandardAnalyzer, parses to a PhraseQuery using the phrase “some phrase”
The StandardAnalyzer removes the words this and is because they match the default stop word list (more in section 4.3.2 on StandardAnalyzer)
A common question is why the asterisk isn’t interpreted as a wildcard query.
Keep in mind that surrounding text with double quotes causes the surrounded text to be analyzed and converted into a PhraseQuery.
The following code demonstrates both the effect of analysis on a phrase query expression and the TermQuery optimization:
The slop factor is zero unless you specify it using a trailing tilde (~) and the desired integer slop value.
Because the implicit analysis of phrases may not match what was indexed, the slop factor can be set to something other than zero automatically if it isn’t specified using the tilde notation:
A sloppy PhraseQuery, as noted, doesn’t require that the terms match in the same order.
However, a SpanNearQuery (discussed in section 5.4.3) has the ability to guarantee an in-order match.
In section 6.3.4, we extend QueryParser and substitute a SpanNearQuery when phrase queries are parsed, allowing for sloppy in-order phrase matches.
The final queries we discuss are WildcardQuery, PrefixQuery and FuzzyQuery, all of which QueryParser can create.
If a term contains an asterisk or a question mark, it’s considered a WildcardQuery.
When the term only contains a trailing asterisk, QueryParser optimizes it to a PrefixQuery instead.
Both prefix and wildcard queries are lowercased by default, but this behavior can be controlled:
Wildcards at the beginning of a term are prohibited using QueryParser, but an API-coded.
WildcardQuery may use leading wildcards (at the expense of performance)
A trailing tilde (~) creates a fuzzy query on the preceding term.
The same performance caveats that apply to WildcardQuery also apply to fuzzy queries and can be disabled with a customization similar to that discussed in section 6.3.1
A carat (^) followed by a floating-point number sets the boost factor for the preceding query.
You can apply a boost to any type of query, including parenthetical groups.
QueryParser can’t create every type of query that can be constructed using the API.
In chapter 5, we detail a handful of API-only queries that have no QueryParser expression capability.
You must keep in mind all the possibilities available when exposing free-form query parsing to an end user; some queries have the potential for performance bottlenecks, and the syntax used by the built-in QueryParser may not be suitable for your needs.
You can exert some limited control by subclassing QueryParser (see section 6.3.1)
We don’t discuss the creation of a custom query parser; however, the source code for Lucene’s QueryParser is freely available for you to borrow from.
You can often obtain a happy medium by combining a QueryParser-parsed query with API-created queries as clauses in a BooleanQuery.
For example, if users need to constrain searches to a particular category or narrow them to a date range, you can have the user interface separate those selections into a category chooser or separate date-range fields.
Most applications need only a few Lucene classes and methods to enable searching.
The most fundamental things for you to take from this chapter are an understanding of the basic query types (of which TermQuery, RangeQuery, and BooleanQuery are the primary ones) and how to access search results.
Although it can be a bit daunting, Lucene’s scoring formula (coupled with the index format discussed in appendix B and the efficient algorithms) provides the magic of returning the most relevant documents first.
Lucene’s QueryParser parses human-readable query expressions, giving rich full-text search power to end users.
QueryParser immediately satisfies most application requirements; however, it doesn’t come without caveats, so be sure you understand the rough edges.
Much of the confusion regarding QueryParser stems from unexpected analysis interactions; chapter 4 goes into great detail about analysis, including more on the QueryParser issues.
And yes, there is more to searching than we’ve covered in this chapter, but understanding the groundwork is crucial.
Analysis, in Lucene, is the process of converting field text into its most fundamental indexed representation, terms.
These terms are used to determine what documents match a query during searches.
For example, if this sentence were indexed into a field the terms might start with for and example, and so on, as separate terms in sequence.
An analyzer tokenizes text by performing any number of operations on it, which could include extracting words, discarding punctuation, removing accents from characters, lowercasing (also called normalizing), removing common words, reducing words to a root form (stemming), or changing words into the basic form (lemmatization)
This process is also called tokenization, and the chunks of text pulled from a stream of text are called tokens.
You want to throw gobs of text at Lucene and have them be richly searchable by the individual words within that text.
In order for Lucene to know what “words” are, it analyzes the text during indexing, extracting it into terms.
Choosing the right analyzer is a crucial development decision with Lucene, and one size definitely doesn’t fit all.
Language is one factor, because each has its own unique features.
Another factor to consider is the domain of the text being analyzed; different industries have different terminology, acronyms, and abbreviations that may deserve attention.
Although we present many of the considerations for choosing analyzers, no single analyzer will suffice for all situations.
It’s possible that none of the builtin analysis options are adequate for your needs, and you’ll need to invest in creating a custom analysis solution; pleasantly, Lucene’s building blocks make this quite easy.
In this chapter, we’ll cover all aspects of the Lucene analysis process, including how and where to use analyzers, what the built-in analyzers do, and how to write your own custom analyzers using the building blocks provided by the core Lucene API.
Let’s begin by seeing when and how Analyzers are used by Lucene.
Before we get into the gory details of what lurks inside an analyzer, let’s look at how an analyzer is used within Lucene.
Analysis occurs any time text needs to be converted into terms, which in Lucene’s core is at two spots: during indexing and when using QueryParser for searching.
In the following two sections, we detail how an analyzer is used in these scenarios.
In the last section we describe an important difference between parsing and analyzing a document.
If you highlight hits in your search results, which is strongly recommended as it gives a better end user experience, you may need to analyze text at that point as well.
Highlighting, available in Lucene’s sandbox, is covered in detail in section XXX.
Before we begin with any code details, look at listing 4.1 to get a feel for what the analysis process is all about.
Two phrases are analyzed, each by four of the built-in analyzers.
Each token is shown between square brackets to make the separations apparent.
During indexing, the tokens extracted during analysis are the terms indexed.
And, most important, it’s only the terms that are indexed that are searchable!
Only the tokens produced by the analyzer will be searched.
Analyzing "The quick brown fox jumped over the lazy dogs" WhitespaceAnalyzer : [The] [quick] [brown] [fox] [jumped] [over] [the] [lazy] [dogs]
SimpleAnalyzer : [the] [quick] [brown] [fox] [jumped] [over] [the] [lazy] [dogs]
The code that generated this analyzer output is shown later, in listing 4.2
Section 4.2.3 explains more of the details of what happened, but here’s a quick summary of each of these analyzers:
SimpleAnalyzer first splits tokens at non-letter characters, then lowercases each token.
StopAnalyzer is the same as SimpleAnalyzer, except it removes common words (called stop words, described more in section XXX)
It has quite a bit of logic to identify certain kinds of tokens, such as company names, email addresses, and host names.
Lucene doesn’t make the results of the analysis process visible to the end user.
Terms pulled from the original text are immediately added to the index.
It is these terms, and only these terms, that are matched during searching.
When searching with QueryParser, the analysis process happens again, on the textual parts of the search query, in order to ensure the best possible matches.
Let’s look at how the analyzer is used during indexing.
During indexing text contained in the document’s field values must be converted into tokens, as shown in Figure 4.1
In this example, we use the built-in StandardAnalyzer, one of the several available within the core Lucene library.
Each analyzed field of each document indexed with the IndexWriter instance uses the.
However, the output of the designated Analyzer dictates what is indexed.
The following code demonstrates indexing of a document where one field is analyzed and stored, and the second field is analyzed but not stored:
Both "title" and "contents" are analyzed using the Analyzer instance provided to the.
However, if an individual document has special analysis needs, the analyzer may be specified on a per-document basis, like this:
QueryParser must also use an Analyzer to parse fragments of the user’s textual query.
QueryParser is wonderful for presenting the end user with a free-form option of querying.
To do its job, of course, QueryParser uses an analyzer to break the text it encounters into terms for searching.
You must provide an analyzer when you instantiate the QueryParser:
The analyzer receives individual contiguous text pieces of the expression, not the expression as a.
QueryParser analyzes all text equally, without knowledge of how it was indexed.
This is a particularly thorny issue when you’re querying for fields that were indexed without tokenization.
Should you use the same analyzer with QueryParser that you used during indexing? The short, most accurate, answer is, “it depends.” If you stick with the basic built-in analyzers, then you’ll probably be fine.
However, when you’re using more sophisticated analyzers, quirky cases can come up in which using different analyzers between indexing and QueryParser is best.
We discuss this issue in more detail in section 4.6
Now we draw the difference between parsing and analyzing a document.
An important point about analyzers is that they’re used internally for fields enabled for analysis.
Documents such as HTML, Microsoft Word, XML, and others, contain meta-data such as author, title, last modified date, and potentially much more.
When you’re indexing rich documents, this meta-data should be separated and indexed as separate fields.
Analyzers are used to analyze a specific field at a time and break things into tokens only within that field; creating new fields isn’t possible within an analyzer.
Analyzers don’t help in field separation because their scope is to deal with a single field at a time.
In these cases, the documents should be parsed, or preprocessed, into separate blocks of text representing each field.
Now that we’ve seen where and how Lucene uses analyzers, it’s time to delve into just what an analyzer does and how it works.
In order to do understand the analysis process, we need to open the hood and tinker around a bit.
Because it’s possible that you’ll be constructing your own analyzers, knowing the architecture and building blocks provided is crucial.
Quite elegantly, it turns text into a stream of tokens enumerated by the TokenStream class.
The returned TokenStream is then used to iterate through all tokens.
Let’s start “simply” with the SimpleAnalyzer and see what makes it tick.
That method is allowed to re-use the same TokenStream that it had previously returned to the same thread.
This can save a lot of allocation and garbage collection since every field of every document otherwise needs a new TokenStream.
All of the built-in Lucene analyzers implement this method: the first time the method is called from a given thread, a new TokenStream instance is created and saved away.
Subsequent calls simply return the previous TokenStream after resetting it to the new Reader.
In the following sections, we take a detailed look at each of the major players used by analyzers, including Token and the TokenStream family, plus the Attributes that represent the components of a Token.
We’ll also show you how to visualize what an analyzer is actually doing, and describe the important of the order of Tokenizers.
Lets begin with the basic unit of analysis, the Token.
During indexing, fields designated for tokenization are processed with the specified analyzer, and each token is then written into the index.
For example, let’s analyze the text “the quick brown fox”
A token carries with it a text value (the word itself) as well as some meta-data: the start and end character offsets in the original text, a token type, and a position increment.
Figure 4.1 shows the details of the token stream analyzing this phrase with the SimpleAnalyzer.
The start offset is the character position in the original text where the token text begins, and the end offset is the position just after the last character of the token text.
These offsets are useful for highlighting matched tokens in search results, as described in 8.XXX.
The token type is a String, defaulting to "word", that you can control and use in the token-filtering process if desired.
As text is tokenized, the position relative to the previous token is recorded as the position increment value.
The position increment and start and end offsets are the only additional meta-data associated with the token carried through to the index.
The token type is discarded—it’s only used during the analysis process.
Position increment is usually 1, indicating that each word is in a unique and successive position in the field.
Position increments greater than 1 allow for gaps and can be used to indicate where words have been removed.
See section 4.7.1 for an example of stop-word removal that leaves gaps using position increments.
A token with a zero position increment places the token in the same position as the previous token.
Analyzers that inject synonyms can use a position increment of zero for the synonyms.
The effect is that phrase queries work regardless of which synonym was used in the query.
See our SynonymAnalyzer in section 4.6 for an example that uses position increments of zero.
There are two different styles of TokenStreams: Tokenizer and TokenFilter.
A good generalization to explain the distinction is that Tokenizers deal with individual characters, and TokenFilters deal with words.
Tokenizers produce a new TokenStream, while TokenFilters simply filter the tokens from a prior TokenStream.
A Tokenizer is a TokenStream that tokenizes the input from a Reader.
When you’re indexing a String, Lucene wraps the String in a StringReader for tokenization.
The second style of TokenStream, TokenFilter, lets you chain TokenStreams together.
A TokenStream is fed into a TokenFilter, giving the filter a chance to add, remove, or change the stream as it passes through.
Figure 4.3 shows the Tokenizer and TokenFilter inheritance hierarchy within Lucene.
Note the composite pattern used by TokenFilter to encapsulate another TokenStream (which could, of course, be another TokenFilter)
Table 4.1 Analyzer building blocks provided in Lucene’s core API.
CharTokenizer Parent class of character-based tokenizers, with abstract isTokenChar() method.
Also provides the capability to normalize (for example, lowercase) characters.
Tokens are limited to a maximum size of 255 characters.
KeywordTokenizer Tokenizes the entire input string as a single token.
SinkTokenizer A Tokenizer that absorbs tokens, caches them in a private list, and then can later iterate over the tokens it had previously cached.
This is used in conjunction with TeeTokenizer to "split" a Token stream.
StandardTokenizer Sophisticated grammar-based tokenizer, emitting tokens for high-level types like e-mail addresses (see section 4.3.2 for more details)
Each emitted token is tagged with a special type, some of which are handled specially by StandardFilter.
StopFilter Removes words that exist in a provided set of words.
TeeTokenFilter Splits a Token stream, by passing each token it iterates through into a SinkTokenizer, and also returning the Token unmodified to its caller.
CachingTokenFilter Saves all tokens from the input stream and can then replay the stream back over and over.
LengthFilter Only accepts tokens whose text length falls within a specified range.
Removes dots from acronyms and ’s (apostrophe followed by S) from words with apostrophes.
Tokenizers start the analysis process by churning the character input into tokens (mostly these correspond to words in the original text)
TokenFilters then take over the remainder of the analysis, initially wrapping a Tokenizer and successively wrapping nested TokenFilters.
Thus, the purpose of an analyzer is to simply define this analyzer chain (TokenStream followed by a series of TokenFilters) and implement it in the tokenStream method.
To illustrate this in code, here is the analyzer chain returned by StopAnalyzer:
Nonletter characters form token boundaries aren’t included in any emitted token.
Following this word tokenizer and lowercasing, StopFilter removes words in a stop-word list (see section 4.3.1)
Buffering is a feature that’s commonly needed in the TokenStream implementations.
Low-level Tokenizers do this to buffer up characters to form tokens at boundaries such as whitespace or nonletter characters.
TokenFilters that emit additional tokens into the stream they’re filtering must queue an.
Most of the builtin TokenFilters simply alter a single stream of input tokens in some fashion, but two of them are more interesting.
TeeTokenFilter is a filter that splits the incoming token stream into two output streams, one of which is a SinkTokenizer.
This is useful when two or more fields would like to share the same basic initial analysis steps, but differ on the final processing of the tokens.
Next we describe how to see the results of the analysis process.
It’s important to understand what various analyzers do with your text.
Seeing the effect of an analyzer is a powerful and immediate aid to this understanding.
We’ll also describe the Attribute class, which represents each element of a Token, and we’ll discuss each of the Token’s attributes: term, positionIncrement, offset, type, flags and payload.
Listing 4.2 provides a quick and easy way to get visual feedback about the four primary built-in analyzers on a couple of text examples.
AnalyzerDemo includes two predefined phrases and an array of the four analyzers we’re focusing on in this section.
Each phrase is analyzed by all the analyzers, with bracketed output to indicate the terms that would be indexed.
AnalyzerUtils passes text to an analyzer without indexing it and pulls the results in a manner similar to what happens during the indexing process under the covers of IndexWriter.
Invoke analysis process #2 Output token text surrounded by brackets.
WhitespaceAnalyzer didn’t lowercase, left in the dash, and did the bare minimum of tokenizing at whitespace boundaries.
SimpleAnalyzer left in what may be considered irrelevant (stop) words, but it did lowercase and tokenize at nonalphabetic character boundaries.
Both SimpleAnalyzer and StopAnalyzer mangled the corporation name by splitting XY&Z and removing the ampersand.
We recommend keeping a utility like this handy to see what tokens emit from your analyzers of choice.
In fact, rather than write this yourself, you can use our AnalyzerUtils or the AnalyzerDemo code for experimentation.
The AnalyzerDemo application lets you specify one or more strings from the command line to be analyzed instead of the embedded example ones:
Let’s now look deeper into what makes up a Token.
We display all token information on the example phrase using SimpleAnalyzer:
We present a similar, but simpler, visualization of token position increments in section 4.6.1, and we provide a visualization of tokens sharing the same position.
Past versions of Lucene did use a Token object, but in order to be more extensible, and to provide better analysis performance through reuse, Lucene switched to the AttributeSource API as of 2.9
This method returns true if there is a new token and false if you’ve exhausted the stream.
You obtain the attributes of interest by calling the addAttribute method, which will return a subclass of Attribute.
That method will add the requested attribute, with default values, if it’s not already present, and then return the attribute.
You then interact with that subclass to obtain the value for each token.
When incrementToken returns true, all attributes within it will have altered their state to the next token.
Typically you would obtain the attributes up front, and then iterate through the tokens, asking each attribute for its values.
For example, if you’re only interested in the position increment, you could simply do this:
Each attribute class is actually bidirectional: you use each to get and to set the value for that attribute.
This returns an AttributeSource instance that holds a private copy of all attributes as of when it was called.
Generally this is not recommended as it results in much slower performance, but for diagnostic purposes it’s fine.
By providing an extensible attribute-based API, Lucene allows you to create your own attributes.
Note that Lucene will do nothing with your attribute during indexing, so this is really only currently useful in cases where one TokenStream early in your analysis chain wishes to send information to another TokenStream later in the chain.
If you index with TermVectors, as described in section 2.8, you can store token text, offsets and position information in your index for the fields you specify.
Then, at search time, TermVectors can be used for highlighting matches in text, as discussed in section 8.7
In this case, the stored offsets hold the start and end character offset in the original text for each token, which the highlighter then uses to make each matched token stand out in the search results.
It’s also possible to re-analyze the text to do highlighting without storing TermVectors, in which case the start and end offsets are used in real-time.
TOKEN-TYPE USEFULNESS You can use the token-type value to denote special lexical types for tokens.
Under the covers of StandardAnalyzer is a StandardTokenizer that parses the incoming text into different types based on a grammar.
Analyzing the phrase “I’ll e-mail you at xyz@example.com” with StandardAnalyzer produces this interesting output:
StandardAnalyzer is the only built-in analyzer that leverages the token-type data.
The order of events can be critically important during analysis.
Each step may rely on the work of a previous step.
StopFilter does a case-sensitive look-up of each token in a set of stop words.
As an example, we first write a functionally equivalent StopAnalyzer variant; we’ll follow it with a flawed variant that reverses the order of the steps:
StopAnalyzer2 uses a LetterTokenizer feeding a LowerCaseFilter, rather than just a LowerCaseTokenizer.
We’ve added a utility method to our AnalyzerUtils that asserts tokens match an expected list:
To illustrate the importance that the order can make with token filtering, we’ve written a flawed analyzer that swaps the order of the StopFilter and the LowerCaseFilter:
The StopFilter presumes all tokens have already been lowercased and does a case-sensitive lookup.
Another test case shows that The was not removed (it’s the first token of the analyzer output), yet it was lowercased:
For example, the StandardFilter is designed to be used in conjunction with StandardTokenizer and wouldn’t make sense with any other TokenStream feeding it.
There may also be performance considerations when you order the filtering process.
Consider an analyzer that removes stop words and also injects synonyms into the token stream—it would be more efficient to remove the stop words first so that the synonym injection filter would have fewer terms to consider (see section 4.6 for a detailed example)
Now that we’ve seen in great detail all elements of the Token and Analyzer classes, let’s see which out-of-the-box analyzers Lucene provides.
Lucene includes several built-in analyzers, created by chaining together the built-in Tokenizers and TokenFilters.
StopAnalyzer  Divides text at nonletter characters, lowercases, and removes stop words.
You can see the effect of each of these analyzers in the output in section 4.2.3
WhitespaceAnalyzer and SimpleAnalyzer are trivial and we don’t cover them in more detail here.
We explore the StopAnalyzer and StandardAnalyzer in more depth because they have nontrivial effects.
Remember that an analyzer is simply a chain of an original Tokenizer and a series of TokenFilters.
There’s absolutely no reason why you must use one of Lucene’s built-in analyzers; you can easily make your own analyzer that defines your own interesting chain.
StopAnalyzer, beyond doing basic word splitting and lowercasing, also removes special words called stop words.
Stop words are words that are very common, such as the, and thus assumed to carry very little standalone meaning for searching since nearly every document will contain the word.
Embedded in StopAnalyzer is the following list of common English stop words; this list is used unless otherwise specified:
The StopAnalyzer has a second constructor that allows you to pass your own list as a String[] instead.
Under the hood, StopAnalyzer creates a StopFilter to perform the filtering.
StandardAnalyzer holds the honor as the most generally useful built-in analyzer.
A JFlex-based1 grammar underlies it, tokenizing with cleverness for the following lexical types: alphanumerics, acronyms, company names, e-mail addresses, computer host names, numbers, words with an interior apostrophe, serial numbers, IP addresses, and CJK (Chinese Japanese Korean) characters.
StandardAnalyzer also includes stop-word removal, using the same mechanism as the StopAnalyzer (identical default English list, and an optional String[] constructor to override)
Its unique effect, though, is apparent in the different treatment of text.
Let’s pause now for a bit and recap where we are.
You’re about halfway through this chapter, and by now you understand where and why Lucene performs analysis of text, and you’ve seen the internal details of how analysis is actually implemented.
You’ve seen that analysis is a chain of one Tokenizer and any number of TokenFilters, and you know how to create your own analyzer chain.
You’ve seen the nitty gritty details of how a TokenStream produces tokens.
We delved into Lucene’s builtin analyzers and token filters, and touched on some tricky topics like the importance of filter order.
At this point you have a strong foundational knowledge of Lucene’s analysis process.
What we’ll now do for the second half of the chapter is build on this base knowledge by visiting several real-world topics, use cases and challenges you’ll no doubt encounter.
Then we’ll show you how to use analysis to implement a couple frequently requested features: sounds like querying, and synonyms expansion.
Next, we create our own analyzer chain that normalizes tokens by their stems, removing stop words in the process, and discuss some challenges that result.
Finally we discuss issues that arise when analyzing different languages, and we’ll wrap up with a quick taste of how the Nutch project handles document analysis.
The fact that a Document is composed of multiple fields, with diverse characteristics, introduces some interesting requirements to the analysis process.
We’ll first consider how analysis is impacted by multivalued fields.
Next we’ll discuss how to use different analyzers for different fields.
Finally we’ll talk about skipping analysis entirely for certain fields.
Recall from chapter 2 that a Document may have more than one Field with the same name, and that Lucene logically appends the tokens of these fields sequentially during indexing.
Fortunately, your analyzer has some control over what happens at each field boundary.
This is important to ensure queries that pay attention to a Token’s position, such as phrase or span queries, do not inadvertently match across two separate field instances.
By default this is 0 (no gap), which means it acts as if the field values were directly appended to one another.
If your documents have multivalued fields, and you do index them, it’s a good idea to increase this to a large enough number (for example 100) so no positional queries, such as PhraseQuery, could ever match across the boundary.
Another frequently encountered analysis challenge is how to use a different analyzer for different fields.
During indexing, the granularity of analyzer choice is at the IndexWriter or per-Document level.
With QueryParser, there is only one analyzer use to analyze all encountered text.
Yet for many applications, where the documents have very diverse fields, it would seem that each field may deserve unique analysis.
Internally, analyzers can easily act on the field name being analyzed, since that’s passed as an argument to the tokenStream method.
The built-in analyzers don’t leverage this capability because they’re designed for general-purpose use and field names are of course application specific, but you can easily create a custom analyzer that does so.
Then, for any field that requires a different analyzer, you call the addAnalyzer method.
Any field that wasn’t assigned a specific analyzer simply falls back to the default one.
In the example above, we use SimpleAnalyzer for all fields except body, which uses StandardAnalyzer.
There are often cases when you’d like to index a field’s value without analysis.
For example, part numbers, URLs, social-security numbers, etc should all be indexed and searched as a single token.
Of course, you also want users to be able to search on these part numbers.
This is simple if your application directly creates a TermQuery.
A dilemma can arise, however, if you use QueryParser and attempt to query on an unanalyzed field, since the fact that the field was not analyzed is only known during indexing.
There is nothing special about such a field’s terms once indexed; they’re just terms.
Let’s see the issue exposed with a straightforward test case that indexes a document with an unanalyzed field and then attempts to find that document again:
So far, so good—we’ve indexed a document and can retrieve it using a TermQuery.
QueryParser analyzes each term and phrase of the query expression.
SimpleAnalyzer strips nonletter characters and lowercases, so Q36 becomes q.
Query has a nice toString() method (see section 3.5.1) to return the query as a QueryParser-like expression.
This issue of QueryParser encountering an unanalyzed field emphasizes a key point: indexing and analysis are intimately tied to searching.
It’s problematic because QueryParser analyzed the partnum field, but it shouldn’t have.
Separate your user interface such that a user selects a part number separately from free-form queries.
Generally, users don’t want to know (and shouldn’t need to know) about the field names in the index.
It’s also very poor practice to present more than one text entry box to the user.
If part numbers or other textual constructs are common lexical occurrences in the text you’re analyzing, consider creating a custom domain-specific analyzer that recognizes part numbers, and so on, and preserves them.
Subclass QueryParser and override one or both of the getFieldQuery methods to provide fieldspecific handling.
Section 8.5 covers ways to use JavaScript in a web browser for building queries.
Section 8.XXX shows you how to present a forms-based search interface that uses XML to represent the full query.
The information in this chapter provides the foundation for building domain-centric analyzers.
We’ll use Lucene’s KeywordAnalyzer to tokenize the part number as a single token.
First let’s look at the KeywordAnalyzer in action as it fixes the situation:
We apply the KeywordAnalyzer only to the partnum field, and we use the SimpleAnalyzer for all other fields.
Note that the query now has the proper term for the partnum field, and the document is found as expected.
Have you ever played the game Charades, cupping your hand to your ear to indicate that your next gestures refer to words that “sound like” the real words you’re trying to convey? Neither have we.
Suppose, though, that a high-paying client has asked you to implement a search engine accessible by J2ME-enabled devices, such as a cell phone, to help during those tough charade matches.
In this section, we’ll implement an analyzer to convert words to a phonetic root using an implementation of the Metaphone algorithm from the Jakarta Commons Codec project.
We chose the Metaphone algorithm as an example, but other algorithms are available, such as Soundex.
Let’s start with a test case showing the high-level goal of our search experience:
It seems like magic! The user searched for “kool kat”
Neither of those terms was in our original document, yet the search found the desired match.
Searches on the original text would also return the expected matches.
Because the Metaphone algorithm expects words that only include letters, the LetterTokenizer is used to feed our metaphone filter.
The tokens emitted are replaced by their metaphone equivalent, so lowercasing is unnecessary.
This new token is set with the same position offsets as the original, because it’s a replacement in the same position.
The last line before returning the token sets the token type.
Each token can be associated with a String indicating its type, giving meta-data to later filtering in the analysis process.
The StandardTokenizer, as discussed in “Token type usefulness” under section 4.2.3, tags tokens with a type that is later used by the StandardFilter.
The METAPHONE type isn’t used in our examples, but it demonstrates that a later filter could be Metaphone-token aware by calling Token’s type() method.
Unless otherwise specified, the type word is used for tokens by default.
As always, it’s good to view what an analyzer is doing with text.
Using our AnalyzerUtils, two phrases that sound similar yet are spelled completely differently are tokenized and displayed:
We get a sample of the Metaphone encoder, shown here:
Wow—an exact match! In practice, it’s unlikely you’ll want sounds-like matches except in special places; otherwise, far too.
One implementation approach to this idea could be to run all text through a sounds-like analysis and build a cross-reference lookup to consult when a correction is needed.
Now we walk through an Analyzer that can handle synonyms during indexing.
How often have you searched for “spud” and been disappointed that the results did not include “potato”? OK, maybe that precise example doesn’t happen often, but you get the idea: natural languages for some reason have evolved many ways to say the same thing.
Such synonyms must be handled during searching, or else your users won’t find their documents.
Our next custom analyzer injects synonyms of words into the outgoing token stream, but places the synonyms in the same position as the original word.
By adding synonyms during indexing, searches will find documents that may not contain the original search terms but match the synonyms of those words.
We start with the test case showing how we expect our new analyzer to work:
While working on this chapter, Erik asked his brilliant 5-year-old son, Jakob, how he would spell cool cat.
Erik imagines that a “sounds-like” feature in search engines designed for children would be very useful.
Notice that our unit test shows not only that synonyms for the word jumps are emitted from the SynonymAnalyzer but also that the synonyms are placed in the same position (increment of zero) as the original word.
Let’s see what the SynonymAnalyzer is doing; then we’ll explore the implications of position increments.
TokenFilters; in fact, this is the StandardAnalyzer wrapped with an additional filter.
See table 4.1 for more on these basic analyzer building blocks.
The final TokenFilter in the chain is the new SynonymFilter (listing 4.6), which gets to the heart of the current discussion.
The code successively pops the stack of buffered synonyms from the last streamed-in token until it’s empty.
After all previous token synonyms have been emitted, we read the next token.
We push all synonyms of the current token onto the stack.
Now we return the current (and original) token before its associated synonyms.
The position increment is set to zero, allowing synonyms to be logically in the same place as the original term.
Using an interface for this design easily allows test implementations.
It’s cruel to leave you hanging with a mock implementation, isn’t it? Actually, we’ve implemented a powerful SynonymEngine using the WordNet database.
Notice that the synonyms generated by TestSynonymEngine are one-way: for example, quick has the synonyms fast and speedy, but fast has no synonyms.
In a real production environment, you should ensure all synonyms list one another as alternate synonyms, but since we are using this for simple testing, it’s fine.
Setting the position increment seems powerful, and indeed it is.
You should only modify increments knowing of some odd cases that arise in searching, though.
Since synonyms are indexed just like other terms, TermQuery works as expected.
Also, PhraseQuery works as expected when we use a synonym in place of an original word.
We perform the analysis with a custom SynonymAnalyzer, using MockSynonym-Engine.
A TermQuery for hops succeeded, as did an exact PhraseQuery for “fox hops”
The first one creates QueryParser using our SynonymAnalyzer, and the second one using StandardAnalyzer:
Both analyzers find the matching document just fine, which is great.
With SynonymAnalyzer, "fox jumps" parses to "fox (jumps hops leaps)" With StandardAnalyzer, "fox jumps" parses to "fox jumps"
As expected, with SynonymAnalyzer, words in our query were expanded to their synonyms.
QueryParser is smart enough to notice that the tokens produced by the analyzer have zero position increment, and when that happens inside a phrase query, it creates a MultiPhraseQuery, described in section 5.3
However, this is in fact wasteful and unnecessary: we only need synonym expansion during indexing or during searching, not both.
If you choose to expand during indexing, the disk space consumed by your index will be somewhat larger, but searching may be faster since there are fewer search terms to visit.
However, since your synonyms have been baked into the index, you don’t have the freedom to quickly change them and see the impact of such changes during searching.
If instead you expand at search time, you can see fast turnaround when testing.
These are simply tradeoffs, and which option is best is your decision based on your application’s constraints!
Next we improve our AnalyzerUtils class to more easily see synonyms expansion during indexing.
We wrote a quick piece of code to see what our SynonymAnalyzer is really doing:
And we can now visualize the synonyms placed in the same positions as the original words:
The numbers here are continuous, but they wouldn’t be if the analyzer left holes (as you’ll see with the next custom analyzer)
Multiple terms shown for a single position illustrates where synonyms were added.
This analyzer removes stop words, leaving positional holes where words are removed, and also leverages a stemming filter.
The PorterStemFilter is shown in the class hierarchy in figure 4.3, but it isn’t used by any built-in analyzer.
It stems words using the Porter stemming algorithm created by Dr.
Martin Porter, and it’s best defined in his own words:
The Porter stemming algorithm (or ‘Porter stemmer’) is a process for removing the commoner morphological and inflexional endings from words in English.
Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.4
In other words, the various forms of a word are reduced to a common root form.
For example, the words breathe, breathes, breathing, and breathed, via the Porter stemmer, reduce to breath.
KStem is another stemming algorithm that has been adapted to Lucene (search Google for KStem and Lucene)
We first show how to use StopFilter to leave holes whenever it removes a word.
Then we’ll describe the full analyzer and finally we’ll talk about what to do about the missing positions.
Stopword removal brings up an interesting issue: what happens to the holes left by the words removed? Suppose you index “one is not enough”
The tokens emitted from StopAnalyzer will be one and enough, with is and not thrown away.
By default, StopAnalyzer does not account for words removed, so the result is exactly as if you indexed “one enough”
If you were to use QueryParser along with StopAnalyzer, this document would match phrase queries for “one enough”, “one is enough”, “one but not enough”, and the original “one is not enough”
Remember, QueryParser also analyzes phrases, and each of these reduces to “one enough” and matches the terms indexed.
Since StopAnalyzer does not expose this option, this is a good reason to create your own analyzer.
This custom analyzer uses a stop-word removal filter, enabled to keep positional gaps, and fed from a LowerCaseTokenizer.
The results of the stop filter are fed to the Porter stemmer.
Listing 4.8 shows the full implementation of this sophisticated analyzer.
Leaving gaps when stop words are removed makes logical sense but introduces new issues that we explore next.
As you saw with the SynonymAnalyzer, messing with token position information can cause trouble during searching.
Exact phrase matches now fail, as illustrated in our test case:
The difficulty lies deeper inside PhraseQuery and its current inability to deal with positional gaps.
All terms in a PhraseQuery must be side by side, and in our test case, the phrase it’s searching for is “over lazi” (stop word removed with remaining words stemmed)
Setting the slop to 1 allows the query to effectively ignore the single gap:
The value of the phrase slop factor, in a simplified definition for this case, represents how many stop words could be present in the original text between indexed words.
Introducing a slop factor greater than zero, however, allows even more inexact phrases to match.
With stop-word removal in analysis, doing exact phrase matches is, by definition, not possible: The words removed aren’t there, so you can’t know what they were.
The slop factor addresses the main problem with searching using stop-word removal that leaves holes; you can now see the benefit our analyzer provides, thanks to the stemming:
Both laziness and the phrase “fox jumped” matched our indexed document, allowing users a bit of flexibility in the words used during searching.
Slop is not a great solution for this problem, since it fuzzes up all phrase matching.
A better solution is to use shingles, which are compound tokens created from multiple adjacent tokens.
Lucene has a sandbox module that simplifies adding shingles to your index, described in section 8.3.2
With shingles, stop words are combined with adjacent words to make new tokens, such as the-quick.
This enables precise phrase matching, because the stop words are not in fact discarded, yet has good search performance because the number of documents containing the-quick is far fewer than the number containing the stopword the in any context.
Nutch’s document analysis, described in section 4.9, also uses shingles.
Dealing with languages in Lucene is an interesting and multifaceted issue.
How can text in various languages be indexed and subsequently retrieved? As a developer building I18N-friendly applications based on Lucene, what issues do you need to consider?
You must contend with several issues when analyzing text in various languages.
The first hurdle is ensuring that character-set encoding is done properly such that external data, such as files, are read into Java properly.
During the analysis process, different languages have different sets of stop words and unique stemming algorithms.
Perhaps accents should be removed from characters as well, which would be language dependent.
Finally, you may require language detection if you aren’t sure what language is being used.
Each of these issues is ultimately up to the developer to address, with only basic buildingblock support provided by Lucene.
However, a number of analyzers and additional building blocks such as Tokenizers and TokenStreams are available in the Sandbox (discussed in section 8.3) and elsewhere online.
We first describe the Unicode character encoding, then discuss options for analyzing non-English languages, and in particular Asian languages, which present unique challenges.
Finally we discuss options for mixing multiple languages in one index.
We begin first with a brief introduction to Unicode and character encodings.
Internally, Lucene stores all characters in the standard UTF-8 encoding.
Java frees us from many struggles by automatically handling Unicode within Strings and providing facilities for reading in external data in the many encodings.
You, however, are responsible for getting external text into Java and Lucene.
If you’re indexing files on a file system, you need to know what encoding the files were saved as in order to read them properly.
If you’re reading HTML or XML from an HTTP server, encoding issues get a bit more complex.
Encodings can be specified in an HTTP  content-type header or specified within the document itself in the XML header or an HTML  <meta> tag.
We won’t elaborate on these encoding details, not because they aren’t important, but because they’re separate issues from Lucene.
Please refer to appendix C for several sources of more detailed information on encoding topics.
We’ll proceed with the assumption that you have your text available as Unicode, and move on to the Lucene-specific language concerns.
All the details of the analysis process apply when you’re dealing with text in non-English languages.
With Western languages, where whitespace and punctuation are used to separate words, you must adjust stop-word lists and stemming algorithms to be specific to the language of the text being analyzed.
Beyond the built-in analyzers we’ve discussed, the Sandbox provides many language-specific analyzers, under contrib/analyzers.
Also freely available is the SnowballAnalyzer family of stemmers, which supports many European languages.
Asian languages, such as Chinese, Japanese, and Korean (also denoted as CJK), generally use ideograms rather than an alphabet to represent words.
These pictorial words may or may not be separated by whitespace and thus require a different type of analysis that recognizes when tokens should be split.
The only built-in analyzer capable of doing anything useful with Asian text is the StandardAnalyzer, which recognizes some ranges of the Unicode space as CJK characters and tokenizes them individually.
However, two analyzers in the Lucene Sandbox are suitable for Asian language analysis (see section 8.1 for more details on the Sandbox): CJKAnalyzer and ChineseAnalyzer.
In our sample book data, the Chinese characters for the book Tao Te Ching were added to the title.
Because our data originates in Java properties files, Unicode escape sequences are used:5
We used StandardAnalyzer for all tokenized fields in our index, which tokenizes each English word as expected (tao, te, and ching) as well as each of the Chinese characters as separate terms (tao te ching) even though there is no space between them.
Our ChineseTest demonstrates that searching by the word tao using its Chinese representation works as desired:
Java includes a native2ascii program that can convert natively encoded files into the appropriate format.
We had to ensure that the representations of the Chinese characters are encoded and read properly, and use a CJK-aware analyzer.
This demo uses AWT Labels to properly display the characters regardless of your locale and console environment.
CJKAnalyzer and ChineseAnalyzer are analyzers found in the Lucene Sandbox; they aren’t included in the core Lucene distribution.
ChineseDemo shows the output using an AWT Label component to avoid any confusion that might arise from console output encoding or limited fonts mangling things; you can see the output in figure 4.5
The CJKAnalyzer pairs characters in overlapping windows of two characters each.
By pairing characters in this manner, words are likely to be kept together (as well as disconnected characters, increasing the index size)
The ChineseAnalyzer takes a simpler approach and, in our example, mirrors the results from the built-in StandardAnalyzer by tokenizing each Chinese character.
Words that consist of multiple Chinese characters are split into terms for each component character.
The StandardAnalyzer is still the best built-in general-purpose analyzer, even accounting for CJK characters; however, the Sandbox CJKAnalyzer seems better suited for Asian language analysis.
When you’re indexing documents in multiple languages into a single index, using a per-Document analyzer is appropriate.
You may also want to add a field to documents indicating their language; this field.
In “Controlling date parsing locale” in section 3.5.5, we show how to retrieve the locale from a user’s web browser; this could be automatically used in queries.
This, like character encodings, is outside the scope of Lucene, but it may be important to your application.
We don’t cover language-detection techniques in this book, but it’s an active area of research with several implementations to choose from (see appendix C)
We don’t have the source code to Google, but we do have the open-source project Nutch, created by Lucene’s creator Doug Cutting.
Nutch takes an interesting approach to analyzing text, specifically how it handles stop words, which it calls common terms.
If all words are indexed, an enormous number of documents become associated with each common term, such as the.
Querying for the is practically a nonsensical query, given that the majority of documents contain that term.
When common terms are used in a query, but not within a phrase, such as the quick brown with no other adornments or quotes, common terms are discarded.
However, if a series of terms is surrounded by double-quotes, such as “the quick brown”, a fancier trick is played, which we detail in this section.
Nutch combines an index-time analysis bigram (grouping two consecutive words as a single token) technique with a query-time optimization of phrases.
This results in a far smaller document space considered during searching; for example, far fewer documents have the quick side by side than contain the.
Using the internals of Nutch, we created a simple example to demonstrate the Nutch analysis trickery.
The analyzer output shows how “the quick” becomes a bigram, but the word the isn’t discarded.
The bigram resides in the same token position as the:
Because additional tokens are created during analysis, the index is larger, but the benefit of this trade-off is that searches for exact-phrase queries are much faster.
And there’s a bonus: No terms were discarded during indexing.
The query output (recall from section 3.5.1 that Query’s toString() is handy) of the Lucene Query instance for the query expression "the quick brown" is.
A Nutch query expands to search in the url, anchor, title and host fields as well, with higher boosts for those fields, using the exact phrase.
The content field clause is optimized to only include the bigram of a position that contains an additional <WORD> type token.
This was a quick view of what Nutch does with indexing analysis and query construction.
Nutch continues to evolve, optimize, and tweak the various techniques for indexing and querying.
You can use the shingles sandbox package, covered in section 8.XXX, to take the same approach as Nutch.
Analysis, while only a single facet of using Lucene, is the aspect that deserves the most attention and effort.
The words that can be searched are those emitted during indexing analysis: nothing more, nothing less.
Sure, using StandardAnalyzer may do the trick for your needs, and it suffices for many applications.
Users who take analysis for granted often run into confusion later when they try to understand why searching for “to be or not to be” returns no results (perhaps due to stop-word removal)
It takes less than one line of code to incorporate an analyzer during indexing.
Many sophisticated processes may occur under the covers, such as stop-word removal and stemming of words.
Removing words decreases your index size but can have a negative impact on precision querying.
Because one size doesn’t fit all when it comes to analysis, you may need to tune the analysis process for your application domain.
Lucene’s elegant analyzer architecture decouples each of the processes internal to textual analysis, letting you reuse fundamental building blocks to construct custom analyzers.
When you’re working with analyzers, be sure to use our AnalyzerUtils, or something similar, to see first-hand how your text is tokenized.
If you’re changing analyzers, you should rebuild your index using the new analyzer so that all documents are analyzed in the same manner.
Some projects, though, need more than the basic searching mechanisms.
In this chapter, we explore the more sophisticated searching capabilities built into Lucene.
Sometimes you need fast access to a certain field’s value for every document.
Lucene’s normal inverted index can’t do this, since it optimizes instead for fast access to all documents containing a given term.
Stored fields and term vectors do let you access field values by document number; however they can be slow and are generally not recommended for more than a page worth of results.
Lucene’s field cache, an advanced API, was created to address this need.
Often your application will not use the field cache directly, but functionality you do use, such as sorting results by field values (covered in the next section), uses the field cache under the hood.
We’ll visit a number of classes in this chapter that use field cache internally, so it’s important to understand the tradeoffs of the field cache.
Let’s first see how to use field cache directly, should we need access to a field’s value for all documents.
You can easily use the field cache to load an array of native values for a given field, indexed by document number.
For example, if every document has a field called “weight”, you can get the weight for all documents like this:
Then, simply reference weights[docID] whenever you need to know a document’s weight value.
One very important restriction is that all documents must have a single value for the specified field.
Field cache can only be used on fields that have a single term.
The field cache supports many native types: byte, short, int, long, float, double, strings, StringIndex (includes sort order of the string values)
An “auto” method (getAuto) will peek at the first document in the index and attempt to guess the appropriate native type.
The first time the field cache is accessed for a given reader and field, the values for all documents are visited and loaded into memory as a single large array, and recorded into an internal cache keyed on the reader instance and the field name.
This process can be quite time consuming, for a large index.
Subsequent calls quickly return the same array from the cache.
The cache entry is not cleared until the reader is closed and completely dereferenced by your application (a WeakHashMap is used under the hood)
It’s important to factor in the memory usage of field cache.
Numeric fields require the number of bytes for the native type, multiplied by the number of documents.
For String types, each unique term is also cached for each document.
For highly unique fields, such as “title”, this can be a large amount of memory.
The StringIndex field cache, which is used when sorting by a string field, also stores an additional int array holding the sort order for all documents.
The field cache may consume quite a bit of memory, since each entry allocates an array of the native type, whose length is equal to the number of documents in the reader.
FieldCache does not clear its entries until you close your reader and remove all references to that reader from your application.
By default, Lucene sorts the documents in descending relevance score order, where the most relevant documents appearing first.
For example, for a book search you may want to display search results grouped into categories, and within each category the books should be ordered by relevance to the query.
Collecting all results and sorting them programmatically outside of Lucene is one way to accomplish this; however, doing so introduces a possible performance bottleneck if the number of results is enormous.
In this section, we explore the various ways to sort search results, including sorting by one or more field values in either ascending or descending order.
We’ll begin by showing how to specify a custom sort when searching.
Then we visit two special sort orders: relevance (the default sort) and index order.
Then we’ll sort by a field’s values, including optionally reversing the sort order.
Next we’ll see how to sort by multiple sort criteria.
We then show how to specify the field’s type or locale, which is important to ensure the sort order is correct.
Thus far we’ve covered only the basic search(Query, int) method, which returns the top N results ordered by decreasing relevance.
The sorting version of this method has the signature search(Query, Filter, int, Sort)
Filter, which we’ll cover Section 5.5, should be null if you don’t need to filter the results.
Listing 5.1 demonstrates the use of the sorting search method, you can run this by typing ant SortingExample in the book’s source code directory.
The displayResults method uses the sorting search method and displays the search results.
The examples following will use the displayResults method to illustrate how various sorts work.
The Sort object encapsulates an ordered collection of field sorting information.
We call the overloaded search method with the Sort object.
Later you’ll see a reason to look at the explanation of score.
If you’ve been running the examples using ant, that directory maps to build/index in the.
Since our sample data set consists of only a handful of documents, the sorting examples use a query that matches all documents:
All books in our collection are in this publication month range.
Next, the example runner is constructed from the sample book index included with this book’s source code:
Now that you’ve seen how to use sorting, let’s explore various ways search results can be sorted.
Lucene sorts by decreasing relevance, also called score by default.
Sorting by score relevance works by either passing null as the Sort object or using the default Sort behavior.
Each of these variants returns results in the default score order.
There is overhead involved in using a Sort object, though, so stick to using search(Query, int) if you want to sort by relevance.
The output of using Sort.RELEVANCE is as follows (notice the decreasing score column):
Score and index order are special types of sorting: The results are returned first in decreasing score order and, when the scores are identical, subsorted with a secondary sort by increasing document ID order.
Document ID order is the order in which the documents were indexed.
As an aside, you may wonder why the score of the last two books is different from the rest.
A RangeQuery by default expands into a BooleanQuery matching any of the terms in the range (see section 3.xxx)
We had the same curiosity when developing this example, and uncommenting the Explanation output in displayHits gave us the details to understand this effect.
If the order documents were indexed is relevant, you can use Sort.INDEXORDER.
Document order may be interesting for an index that you build up once and never change.
But if you need to re-index documents, document order typically will not work because newly indexed documents receive new document IDs and will be sorted last.
So far we’ve only sorted by score, which was already happening without using the sorting facility, and document order, which is probably only marginally useful at best.
Sorting by one of our own fields is really what we’re after.
Sorting by a field first requires that you follow the rules for indexing a sortable field, as detailed in section 2.9
To sort by a field, you must create a new Sort object, providing the field name:
The results now appear sorted by our category field in increasing alphabetical order.
Notice the sorted-by output: The Sort class itself automatically adds document ID as the final sort field when a single field name is specified, so the secondary sort within category is by document ID.
The default sort direction for sort fields (including relevance and document ID) is natural ordering.
Natural order is descending for relevance but increasing for all other fields.
For example, here we list books with the newest publications first:
The exclamation point in sorted by "pubmonth"!,<doc> indicates that the pubmonth field is being sorted in reverse natural order (descending publication months, newest first)
Note that the two books with the same publication month are sorted in document id order.
Sorting by multiple fields is important whenever your primary sort leaves ambiguity when there are equal values.
Implicitly we’ve been sorting by multiple fields, since the Sort object appends a sort by document ID in appropriate cases.
You can control the sort fields explicitly using an array of SortFields.
This example uses category as a primary alphabetic sort, with results within category sorted by score; finally, books with equal score within a category are sorted by decreasing publication month:
The Sort instance internally keeps an array of SortFields, but only in this example have you seen it explicitly; the other examples used shortcuts to creating the SortField array.
A SortField holds the field name, a field type, and the reverse order flag.
The type of field is automatically detected as String, int, or float based on the value of the first term in the field.
If you’re using strings that may appear as numeric in some fields, be sure to specify the type explicitly as SortField.STRING.
By search time, the fields that can be sorted on and their corresponding types are already set.
Indexing time is when the decision about sorting capabilities should be made; however, custom sorting implementations can do so at search time, as you’ll see in section 6.1
By indexing an Integer.toString or Float.toString, sorting can be based on numeric values.
In our example data, pubmonth was indexed as a String but is a valid, parsable Integer; thus it’s treated as such for sorting purposes unless specified as SortField.STRING explicitly.
Sorting by a numeric type consumes fewer memory resources than by STRING; section 5.1.9 discusses performance issues further.
It’s important to understand that you index numeric values this way to facilitate sorting on those fields, not to constrain a search on a range of values.
The numeric range query capability is covered in section 6.3.3; the padding technique will be necessary during indexing and searching in order to use numeric fields for searching.
All terms in an index are Strings; the sorting feature uses the standard Integer and Float constructors to parse the string representations.
When sorting by String values you may need to specify your own locale, which we cover next.
When you’re sorting on a SortField.STRING type, order is determined under the covers using String.compareTo by default.
However, if you need a different collation order, SortField lets you specify a Locale.
There are two overloaded SortField constructors for use when you need to specify locale:
Both of these constructors imply the SortField.STRING type because locale applies only to string-type sorting, not to numerics.
Sorting by field value (ie, anything except relevance and document order) uses the field cache to retrieve values.
This means the first query sorting by a given field and reader will be slower, and memory will be consumed holding onto the cache entries.
If the first query is too slow, it’s best to first “warm” your IndexSearcher before putting it into production, as described in 10.X.
Sorting by String, especially for fields with many unique values, will consume the most memory.
The built-in MultiPhraseQuery is definitely a niche query, but it’s potentially useful.
MultiPhraseQuery allows multiple terms per position, effectively the same as a BooleanQuery  on.
For example, suppose we want to find all documents about speedy foxes, with quick or fast followed by fox.
One approach is to do a "quick fox" OR "fast fox" query.
In our example, two documents are indexed with similar phrases.
One document with uses “the quick brown fox jumped over the lazy dog”, and the other uses “the fast fox hopped over the hound” as shown in our test setUp() method:
Knowing that we want to find documents about speedy foxes, MultiPhraseQuery lets us match phrases very much like PhraseQuery, but with a twist: each term position of the query can have multiple terms.
This has the same set of hits as a BooleanQuery consisting of multiple PhraseQuerys combined with an OR operator.
The following test method demonstrates the mechanics of using the MultiPhraseQuery API by adding one or more terms to a MultiPhraseQuery instance in order:
Any of these terms may be in first position to match #2 Only one in second position.
In testBasic(), the slop is used to match “quick brown fox” in the second search; with the default slop of zero, it doesn’t match.
For completeness, here is a test illustrating the described BooleanQuery, with a slop set for the phrase “quick fox”:
Of course, hard-coding the terms wouldn’t be realistic, generally speaking.
One possible use of a MultiPhraseQuery would be to inject synonyms dynamically into phrase positions, allowing for less precise matching.
For example, you could tie in the WordNet-based code (see section 8.6 for more on WordNet and Lucene)
QueryParser produces a MultiPhraseQuery for search terms surrounded in double quotes when the analyzer it’s using returns positionIncrement 0 for any of the tokens within the phrase:
Users may want to query for terms regardless of which field they are in.
Under the covers, it instantiates a QueryParser and parses the query expression for each field and then combines the resulting queries using a BooleanQuery.
The default operator OR is used in the simplest parse method when adding the clauses to the BooleanQuery.
The test shows that documents match based on either of those fields.
You can’t control any of the settings that QueryParser supports, and you’re stuck with the defaults such as default locale date parsing and zero-slop default phrase queries.
Generally speaking, querying on multiple fields isn’t the best practice for user-entered queries.
More commonly, all words you want searched are indexed into a contents or keywords field by combining various fields.
A synthetic contents field in our test environment uses this scheme to put author and subjects together:
We used a space (" ") between author and subjects to separate words for the analyzer.
Allowing users to enter text in the simplest manner possible without the need to qualify field names, generally makes for a less confusing user experience.
We’ll now move onto span queries, advanced queries that allow you to match based on positional constraints.
Lucene includes a whole family of queries based on SpanQuery.
A span in this context is a starting and ending token position in a field.
Recall from section 4.2.1 that tokens emitted during the analysis process include a position increment from the previous token.
This position information, in conjunction with the new SpanQuery subclasses, allow for even more query discrimination and sophistication, such as all documents where "quick fox" is near "lazy dog"
Using the query types we’ve discussed thus far, it isn’t possible to formulate such a query.
Phrase queries could get close with something like "quick fox" AND "lazy dog", but these phrases may be too distant from one another to be relevant for our searching purposes.
Span queries track more than the documents that match: The individual spans, perhaps more than one per field, are tracked.
Contrasting with TermQuery, which simply matches documents, for example, SpanTermQuery keeps track of the positions of each of the terms that match, for every document.
There are five subclasses of the base SpanQuery, shown in table 5.1
SpanTermQuery  Used in conjunction with the other span query types.
SpanFirstQuery  Matches spans that occur within the first part of a field.
We’ll discuss each of these SpanQuery types within the context of a JUnit test case, SpanQueryTest.
In order to demonstrate each of these types, a bit of setup is needed as well as some helper assert methods to make our later code clearer, as shown in listing 5.3
We index two similar phrases in a field f as separate documents and create SpanTermQuerys for several of the terms for later use in our test methods.
In addition, we add three convenience assert methods to streamline our examples.
With this necessary bit of setup out of the way, we can begin exploring span queries.
Span queries need an initial leverage point, and SpanTermQuery is just that.
Internally, a SpanQuery keeps track of its matches: a series of start/end positions for each matching document.
By itself, a SpanTermQuery matches documents just like TermQuery does, but it also keeps track of position of the same terms that appear within each document.
The brown SpanTermQuery was created in setUp() because it will be used in other tests that follow.
The dumpSpans method uses some lower-level SpanQuery APIs to navigate the spans; this lower-level API probably isn’t of much interest to you other than for diagnostic purposes, so we don’t elaborate further on it.
Each SpanQuery subclass sports a useful toString() for diagnostic purposes, which dumpSpans uses:
More interesting is the dumpSpans output from a SpanTermQuery for the:
Not only were both documents matched, but also each document had two span matches highlighted by the brackets.
The basic SpanTermQuery is used as a building block of the other SpanQuery types.
Let’s see how to match only documents where the terms of interest occur in the beginning of the field.
To query for spans that occur within the first n positions of a field, use SpanFirstQuery.
The resulting span matches are the same as the original SpanQuery spans, in this case the same dumpSpans() output for brown as seen in section 5.4.1
A PhraseQuery (see section 3.4.5) matches documents that have terms near one another, with a slop factor to allow for intermediate or reversed terms.
SpanNearQuery matches spans that are within a certain number of positions from one another, with a separate flag indicating whether the spans must be in the order specified or can be reversed.
The resulting matching spans span from the start position of the first span sequentially to the ending position of the last span.
An example of a SpanNearQuery given three SpanTermQuery objects is shown in figure 5.3
Using SpanTermQuery objects as the SpanQuerys in a SpanNearQuery is much like a PhraseQuery.
However, the SpanNearQuery slop factor is a bit less confusing than the PhraseQuery slop factor because it doesn’t require at least two additional positions to account for a reversed span.
To reverse a SpanNearQuery, set the inOrder flag (third argument to the constructor) to false.
Listing 5.4 demonstrates a few variations of SpanNearQuery and shows it in relation to PhraseQuery.
Querying for these three terms in successive positions doesn’t match either document.
Using the same terms with a slop of 4 positions still doesn’t result in a match.
With a slop of 5, the SpanNearQuery has a match.
The nested SpanTermQuery objects are in reverse order, so the inOrder flag is set to false.
A slop of only 3 is needed for a match.
Here we use a comparable PhraseQuery, although a slop of 4 still doesn’t match.
A slop of 5 is needed for a PhraseQuery to match.
We’ve only shown SpanNearQuery with nested SpanTermQuerys, but SpanNearQuery allows for any SpanQuery type.
A more sophisticated SpanNearQuery is demonstrated later in listing 5.5 in conjunction with SpanOrQuery.
The first argument to the SpanNotQuery constructor is a span to include, and the second argument is the span to exclude.
We’ve strategically added dumpSpans to clarify what is going on.
Here is the output with the Java query annotated above each:
The SpanNear query matched both documents because both have quick and fox within one position of one another.
Notice that the resulting span matches are the original included span.
The excluded span is only used to determine if there is an overlap and doesn’t factor into the resulting span matches.
Finally there is SpanOrQuery , which aggregates an array of SpanQuerys.
Our example query, in English, is all documents that have "quick fox" near "lazy dog" or that have "quick fox" near "sleepy cat"
The first clause of this query is shown in figure 5.4
This single clause is SpanNearQuery nesting two SpanNearQuerys, which each consist of two SpanTermQuerys.
Our test case becomes a bit lengthier due to all the sub-SpanQuerys being built upon (see listing 5.5)
We’ve used our handy dumpSpans a few times to allow us to follow the progression as the final OR query is built.
Here is the output, followed by our analysis of it:
Finally, these two SpanNearQuery instances are combined within a SpanOrQuery, which aggregates all matching spans.
QueryParser doesn’t currently support any of the SpanQuery types, but the surround QueryParser in the sandbox does.
Recall from section 3.4.5 that PhraseQuery is impartial to term order when enough slop is specified.
Interestingly, you can easily extend QueryParser to use a SpanNearQuery with SpanTermQuery clauses instead, and force phrase queries to only match fields with the terms in the same order as specified.
Filtering is a mechanism of narrowing the search space, allowing only a subset of the documents to be considered as possible hits.
A security filter is a powerful example, allowing users to only see search results of documents they own even if their query technically matches other documents that are off limits; we provide an example of a security filter in section 5.5.3
RangeFilter matches only documents containing terms within a specified range of terms.
QueryWrapperFilter  uses the results of query as the searchable document space for a new query.
Before you get concerned about mentions of caching results, rest assured that it’s done with a tiny data structure (a DocIdBitSet) where each bit position represents a document.
Consider, also, the alternative to using a filter: aggregating required clauses in a BooleanQuery.
In this section, we’ll discuss each of the built-in filters as well as the BooleanQuery alternative, starting with RangeFilter.
RangeFilter filters on a range of terms in a specific field.
This is actually very useful, depending on the original type of the field.
If the field is a date field, then you get a date range filter.
If it’s an integer field, you can filter by numeric range.
If the field is simply textual, for example last names, then you can filter for all names within a certain alphabetic range such as M to Q.
Having a date field, you filter as shown in testDateFilter() in Listing 5.6
We test the date RangeFilter by using an all-inclusive query, which by itself returns all documents.
The first parameter to both of the RangeFilter constructors is the name of a date field in the index.
In our sample data this field name is modified; this field is the last modified date of the source data file.
The two final boolean arguments to the constructor for RangeFilter, includeLower and includeUpper, determine whether the lower and upper terms should be included or excluded from the filter.
To filter on ranges with one end of the range specified and the other end open, just pass null for whichever end should be open:
RangeFilter provides two static convenience methods to achieve the same thing:
It achieves exactly the same filtering, but does so with a very different implementation.
It loads the StringIndex field cache entry under the hood, so it inherits the known benefits and limitations of field cache (described in section 5.1)
While it’s used in exactly the same way as RangeFilter, since the underlying implementation is very different, there are important tradeoffs.
To check the filter for each document, it maps the upper and lower bounds to their corresponding points in the sort order, and then checks each document against these bounds.
It takes yet another interesting approach for range filtering, by pre-dividing the field’s range of values into larger and larger ranges and then aggregating the ranges at search time, to achieve any desired lower and upper bound.
The tradeoff is a slight increase in index size, for likely very much faster range filtering.
Generally for medium to large indexes, TrieRangeFilter is the best option.
Let’s see how to filter by an arbitrary set of terms.
Sometimes you’d simply like to select specific terms to include in your filter.
For example, perhaps your documents have Country as a field, and your search interface presents a checkbox allowing the user to pick and choose which countries to include in the search.
Simply instantiate it with the field (String) and an array of String:
All documents that have any of the specific terms in the specified field will be accepted.
Note that the documents must have a single term value for each field.
Under the hood, this filter loads all terms for all documents into the field cache the first time it’s used during searching for a given field.
This means the first search will be slower, but subsequent searches, which re-use the cache, will be very fast.
The field cache is re-used even if you change which specific terms are included in the filter.
The second approach for filtering by terms is TermsFilter, which is included in Lucene’s sandbox and described in more detail in section XXX.
TermsFilter does not do any internal caching, and also allows filtering on fields that have more than one term.
It’s best to test both approaches for your application to see if there are any significant performance differences.
QueryFilter uses the hits of one query to constrain available documents from a subsequent search.
The result is a DocIdSet representing which documents were matched from the filtering query.
Here we’re searching for all the books (see setUp() in listing 5.6) but constraining the search using a filter for a category which contains a single book.
We explain the last assertion of testQueryFilter() shortly, in section 5.5.4
QueryWrapperFilter can even replace RangeFilter usage, although it requires a few more lines of code, isn’t nearly as elegant looking and likely has worse performance.
The following code demonstrates date filtering using a QueryWrapperFilter on a RangeQuery:
Let’s see how to use filters for applying security constraints, also known as entitlements.
Another example of document filtering constrains documents with security in mind.
Our example assumes documents are associated with an owner, which is known at indexing time.
We index two documents; both have the term info in their keywords field, but each document has a different owner:
Using a TermQuery for info in the keywords field results in both documents found, naturally.
Suppose, though, that Jake is using the search feature in our application, and only documents he owns should be searchable by him.
Quite elegantly, we can easily use a QueryWrapperFilter to constrain the search space to only documents he is the owner of, as shown in listing 5.7
Here, the filter constrains document searches to only documents owned by “jake”
Only Jake’s document is returned, using the same info TermQuery.
If your security requirements are this straightforward, where documents can be associated with users or roles during indexing, using a QueryWrapperFilter will work nicely.
However, this scenario is oversimplified for most needs; the ways that documents are associated with roles may be quite a bit more dynamic.
QueryWrapperFilter is useful only when the filtering constraints are present as field information within the index itself.
In section 6.4, we develop a more sophisticated filter implementation.
You can constrain a query to a subset of documents another way, by combining the constraining query to the original query as a required clause of a BooleanQuery.
There are a couple of important differences, despite the fact that the same documents are returned from both.
In addition, normalized document scores are unlikely to be the same.
When you’re using BooleanQuery aggregation, all documents containing the terms are factored into the equation, whereas a filter reduces the documents under consideration and impacts the inverse document frequency factor.
This test case demonstrates how to “filter” using BooleanQuery aggregation and illustrates the scoring difference compared to testQueryFilter:
The technique of aggregating a query in this manner works well with QueryParser parsed queries, allowing users to enter free-form queries yet restricting the set of documents searched by an APIcontrolled query.
Better integration of Query and Filter classes is an active topic of discussion in Lucene, so you may very well see changes in this area.
PrefixFilter matches documents containing Terms starting with a specified prefix.
We can use this to search for all books published any year in the 1900s:
Next we show how to cache a filter for better performance.
Filters cache by using the IndexReader as the key, which means searching should also be done with the same instance of IndexReader to benefit from the cache.
If you aren’t constructing IndexReader yourself, but rather are creating an IndexSearcher from a directory, you must use the same instance of IndexSearcher to benefit from the caching.
When index changes need to be reflected in searches, discard IndexSearcher and IndexReader and reinstantiate.
To demonstrate its usage, we return to the date-range filtering example.
We want to use RangeFilter, but we’d like to benefit from caching to improve performance:
We saw how to wrap a Filter as a Query.
You can also do the reverse, using ConstantScoreQuery to turn any Filter into a Query, which you can then search on.
The query matches only documents that are included in the Filter, and assigns all of them the score equal to the query boost.
The queries that have a constant score mode (PrefixQuery, RangeQuery, WildcardQuery and FuzzyQuery) all simply use a ConstantScoreQuery wrapped around the corresponding filter.
An additional filter found in the Lucene Sandbox, ChainedFilter, allows for complex chaining of filters.
Writing custom filters allows external data to factor into search constraints; however, a bit of detailed Lucene API know-how may be required to be highly efficient.
And if these filtering options aren’t enough, Lucen e adds another interesting use of a filter.
The FilteredQuery filters a query, like IndexSearcher’s search(Query, Filter, int) can, except it is itself a query: Thus it can be used as a single clause within a BooleanQuery.
Our next topic is function queries, which give you custom control over how documents are scored.
Lucene’s relevance scoring formula, which we discussed in Chapter 3, does a good job assigning relevance to each document based on how well the document matches the query.
But what if you’d like to modify or override how this scoring is done?  In section 5.1 we saw how you could change the default relevance sorting to sort by an arbitrary field, but what if you need even more flexibility?  This is where function queries come in.
For example, FieldScoreQuery derives each document’s score statically from a specific indexed field.
The field should be numbers, indexed without norms and with a single token per document.
First, include the field “score” in your documents like this:
That query matches all documents, assigning each a score according to the contents of its “score” field.
You can also use the SHORT, INT or FLOAT types.
Under the hood, this function query uses the same field cache used when you sort by a specified field, which means the first search that uses a given field will be slower as it must populate the cache.
Subsequent searches based on the same IndexReader and field are then fast.
The above example is somewhat contrived, since you could simply sort by the score field, descending, to achieve the same thing.
But function queries get more interesting when you combine them using the second type of function query, CustomScoreQuery.
This query class lets you combine a normal Lucene query with one or more ValueSourceQuerys.
A ValueSourceQuery is the super class of FieldScoreQuery, and is simply a query that matches all documents and returns an arbitrary preassigned score per document.
FieldScoreQuery is one such example, where the score is derived from an indexed field.
You could also provide values from some external source, for example a database.
We can now use the FieldScoreQuery we created above, and a CustomScoreQuery, to compute our own score:
In this case we create a normal query q by parsing the user’s search text.
We next create the same FieldScoreQuery we used above, which assigns static scores to documents according to the field score.
Finally, we create a CustomScoreQuery, overriding the customScore method to compute our score for each matching document.
In this case, we take the square root of the incoming query score and then multiply it by the static score provided by the FieldScoreQuery.
A real-world use of CustomScoreQuery is to do custom document boosting.
In applications where documents have a clear timestamp, such as searching a news feed or press releases, boosting by relevance is very useful.
The class requires you to externally pre-compute the daysAgo array, which for each doc id holds the number of days ago that the document was updated.
Here’s how we can use this to search our books index:
In this example, we populate the daysAgo array by loading each stored document and processing the pubmonth field, but you could also derive it from any other source, for example an external database.
In computing daysAgo, we pretend today is 10/1/2004 to better show the recency effect.
In a production use you should use the current day.
If instead you run the commented out line, which runs the original unboosted query, you’ll see this:
You can see that in the un-boosted query, the top 3 results were tied based on relevance.
If your architecture consists of multiple Lucene indexes, but you need to search across them using a single query with search results interleaving documents from different indexes, MultiSearcher is for you.
In high-volume usage of Lucene, your architecture may partition sets of documents into different indexes.
With MultiSearcher, all indexes can be searched with the results merged in a specified (or descendingscore) order.
Using MultiSearcher is comparable to using IndexSearcher, except that you hand it an array of IndexSearchers to search rather than a single directory (so it’s effectively a decorator pattern and delegates most of the work to the subsearchers)
Listing 5.8 illustrates how to search two indexes that are split alphabetically by keyword.
The index is made up of animal names beginning with each letter of the alphabet.
Half the names are in one index, and half are in the other.
A search is performed with a range that spans both indexes, demonstrating that results are merged together.
The first half of the alphabet is indexed to one index, and the other half is indexed to the other index.
The basic search and search with filter options are parallelized, but searching with a HitCollector has not yet been parallelized.
There are numerous other alternatives to exposing search remotely, such as through web services.
This section focuses solely on Lucene’s built-in capabilities; other implementations are left to your innovation (you can also borrow ideas from projects like Nutch; see section 10.1)
An RMI server binds to an instance of RemoteSearchable, which is an implementation of the Searchable interface just like IndexSearcher and MultiSearcher.
The server-side RemoteSearchable delegates to a concrete Searchable, such as a regular IndexSearcher instance.
Clients to the RemoteSearchable invoke search methods identically to searching through an IndexSearcher or MultiSearcher, as shown throughout this chapter.
Figure 5.5 Remote searching through RMI, with the server searching multiple indexes.
Twenty-six indexes reside under the basedir, each named for a letter of the alphabet.
A MultiSearcher over all indexes, named LIA_Multi, is created and published through RMI.
Querying through SearchServer remotely involves mostly RMI glue, as shown in SearchClient in listing 5.10
Because our access to the server is through a Remote-Searchable, which is a lower-level.
Why MultiSearcher? Because it’s a wrapper over Searchables, making it as friendly to use as IndexSearcher.
We perform multiple identical searches to warm up the JVM and get a good sample of response time.
The searchers are cached, to be as efficient as possible.
The remote Searchable is located and wrapped in a MultiSearcher.
We don’t close the searcher because it closes the remote searcher, thereby prohibiting future searches.
Doing so will prevent future searches from working because the server side will have closed its access to the index.
For demonstration purposes, we ran it on a single machine in separate console windows.
It’s interesting to note the search times reported by each type of server-side searcher.
Also, you can see the reason why we chose to run the search multiple times: The first search took much longer relative to the successive searches, which is probably due to JVM warmup.
These results point out that performance testing is tricky business, but it’s necessary in many environments.
Because of the strong effect your environment has on performance, we urge you to perform your own tests with your own environment.
Performance testing is covered in more detail in section 6.5
If you choose to expose searching through RMI in this manner, you’ll likely want to create a bit of infrastructure to coordinate and manage issues such as closing an index and how the server deals with index updates (remember, the searcher sees a snapshot of the index and must be reopened to see changes)
Most of us probably can’t envision vectors in hyperdimensional space, so for visualization purposes, let’s look at two documents that contain only the terms cat and dog.
Plotting the term frequencies of each document in X, Y coordinates looks something like figure 5.6
What gets interesting with term vectors is the angle between them, as you’ll see in more detail in section 5.7.2
Figure 5.6 Term vectors for two documents containing the terms cat and dog.
We covered how to enable indexing of term vectors in Section 2.2.1
We indexed the title, author, subject and contents fields with term vectors when indexing our book data.
Retrieving term vectors for a field in a given document by ID requires a call to an IndexReader  method:
A TermFreqVector instance has several methods for retrieving the vector information, primarily as matching arrays of Strings and ints (the term value and frequency in the field, respectively)
That class contains offset and position information for each occurrence of the terms in the document.
You can use term vectors for some interesting effects, such as finding documents “like” a particular document, which is an example of latent semantic analysis.
We’ll show how to find books similar to an existing one, as well as a proof-of-concept categorizer that can tell us the most appropriate category for a new book, as you’ll see in the following sections.
We wrap up with the TermVectorMapper classes for precisely controlling how term vectors are read from the index.
It would be nice to offer other choices to the customers of our bookstore when they’re viewing a particular book.
The alternatives should be related to the original book, but associating alternatives manually would be labor-intensive and would require ongoing effort to keep up to date.
Instead, we use Lucene’s boolean query capability and the information from one book to look up other books that are similar.
Listing 5.11 demonstrates a basic approach for finding books like each one in our sample data.
As an example, we iterate over every book document in the index and find books like each one.
Here we look up books that are like this one.
Books by the same author are considered alike and are boosted so they will likely appear before books by other authors.
Using the terms from the subject term vectors, we add each to a boolean query.
We combine the author and subject queries into a final boolean query.
We exclude the current book, which would surely be the best match given the other criteria, from consideration.
In #3, we used a different way to get the value of the author field.
It was indexed as multiple fields, in the manner (shown in more detail in section 8.4) where the original author string is a comma-separated list of author(s) of a book:
The output is interesting, showing how our books are connected through author and subject:
If you’d like to see the actual query used for each, uncomment the output lines toward the end of the docsLike.
The books-like-this example could have been done without term vectors, and we aren’t really using them as vectors in this case.
We’ve only used the convenience of getting the terms for a given field.
Without term vectors, the subject field could have been reanalyzed or indexed such that individual subject terms were added separately in order to get the list of terms for that field (see section 8.4 for discussion of how the sample data was indexed)
Our next example also uses the frequency component to a term vector in a much more sophisticated manner.
The sandbox contains a useful Query implementation, MoreLikeThisQuery, doing the same thing as our BooksLikeThis class, but more generically.
BooksLikeThis is clearly hardwired to fields like “subject” and “author”, from our books index.
But MoreLikeThis makes this generic so it works well on.
Another Sandbox package, the Highlighter, described in Section 8.7, also uses of term vectors to find term occurrences for highlighting.
Let’s see another example usage of term vectors: automatic category assignment.
The best category placement for a new book may be relatively obvious, or (more likely) several possible categories may seem reasonable.
We’ve written a bit of code that builds a representative subject vector for each existing category.
This representative, archetypical, vector is the sum of all vectors for each document’s subject field vector.
With these representative vectors pre-computed, our end goal is a calculation that can, given some subject keywords for a new book, tell us what category is the best fit.
The best category is determined by finding the closest category angle-wise in vector space to the new book’s subject.
Our code builds category vectors by walking every document in the index and aggregating book subject vectors into a single vector for the book’s associated category.
Category vectors are stored in a Map, keyed by category name.
The value of each item in the category map is another map keyed by term, with the value an Integer for its frequency:
A book’s term frequency vector is added to its category vector in addTermFreqToMap.
That was the easy part—building the category vector maps—because it only involved addition.
In the simplest two-dimensional case, as shown earlier in figure 5.6, two categories (A and B) have unique term vectors based on aggregation (as we’ve just done)
The closest category, angle-wise, to a new book’s subjects is the match we’ll choose.
Figure 5.8 shows the equation for computing an angle between two vectors.
Figure 5.8 Formula for computing the angle between two vectors.
Our getCategory method loops through all categories, computing the angle between each category and the new book.
The smallest angle is the closest match, and the category name is returned:
The angle computation takes these assumptions into account to simplify a part of the computation.
Finally, computing the angle between an array of words and a specific category is done in computeAngle, shown in listing 5.1.2
Listing 5.13 Computing term vector angles for a new book against a given category.
We multiply the square root of N by the square root of N is N.
This shortcut prevents a precision issue where the ratio could be greater than 1 (which is an illegal value for the inverse cosine function)
It requires square-root and inverse cosine calculations and may be prohibitive in high-volume indexes.
We finish our coverage of term vectors with the TermVectorMapper class.
Perhaps instead of sorting by Term, you’d like to sort the term vectors according to your own criteria.
Or maybe you’d like to only load certain terms of interest.
All of these can be done with a recent addition to Lucene, TermVectorMapper.
Table 5.ZZZ describes the methods that a concrete TermVectorMapper implementation (subclass) should implement.
Lucene includes a few public core implementations of TermVectorMapper, described in Table 5.AAA.
When you load stored fields, you likewise have specific control using FieldSelector.
We’ve talked about reading a Document from the index using an IndexReader.
Under the hood, Lucene writes these fields into the index and then IndexReader reads them.
Unfortunately, reading a Document can be fairly time consuming, especially if you need to read many of them per search and if your documents have many stored fields.
Often, a document may have one or two large stored fields, holding the actual textual content for the document, and a number of smaller “metadata” fields such as title, category, author, published date, etc.
When presenting the search results, you might only need the metadata fields and so loading the very large fields is costly and unnecessary.
LOAD_FOR_MERGE Used internally to load a field during segment merging; this skips decompressing compressed fields.
SIZE_AND_BREAK Like SIZE, but don’t load any of the remaining fields.
When loading stored fields with a FieldSelector, IndexReader steps through the fields one by one.
There are several builtin concrete classes implementing FieldSelector, described in Table 5.YYY.
You specify the String names of the fields you want to LOAD; all other fields are skipped.
While FieldSelector will save some time during loading fields, just how much time is very application dependent.
Much of the cost when loading stored fields is in seeking the file pointers to the places in the index where all fields are stored, so you may find you don’t save that much time skipping fields.
This chapter has covered some diverse ground, highlighting Lucene’s additional built-in search features.
We touched on Lucene’s field cache, which allows you to load into memory an array of a given field’s value for all documents.
Sorting is a flexible way to control the ordering of search results.
MultiPhraseQuery generalizes PhraseQuery by allowing more than one term at the same position within a phrase.
The SpanQuery family leverages term-position information for greater searching precision.
Function queries let you programmatically customize how documents are scored.
Filters constrain document search space, regardless of the query, and you can either create your own Filter (described in section 6.4), or use one of Lucene’s many built-in ones.
We saw how to wrap a Query as a Filter, and vice versa, as well as how to cache filters for fast reuse.
Lucene includes support for multiple (including parallel) and remote index searching, giving developers a head start on distributed and scalable architectures.
The term vectors enable interesting effects, such as “like this” term vector angle calculations.
Finally we showed how to fine-tune the loading of term vectors and stored fields by using TermVectorMapper and FieldSelector.
Is this the end of the searching story? Not quite.
Lucene also includes several ways to extend its searching behavior, such as custom sorting, positional payloads, filtering, and query expression parsing, which we cover in the following chapter.
In those two chapters, we explored only the built-in features.
Our first custom extension demonstrates Lucene’s custom sorting hooks, allowing us to implement a search that returns results in ascending geographic proximity order from a user’s current location.
Next, implementing your own HitCollector bypasses simple collection of the top N scoring documents; this is effectively an event listener when matches are detected during searches.
QueryParser is extensible in several useful ways, such as for controlling date parsing and numeric formatting, as well as for disabling potential performance degrading queries such as wildcard and fuzzy queries or using your own Query subclasses when creating Query instances.
Custom filters allow information from outside the index to factor into search constraints, such as factoring some information present only in a relational database into Lucene searches.
Custom sorting implementations are most useful in situations when the sort criteria can’t be determined during indexing.
An interesting idea for a custom sorting mechanism is to order search results based on geographic distance from a given location.1 The given location is only known at search time.
Thanks to Tim Jones (the contributor of Lucene’s sort capabilities) for the inspiration.
The test data is indexed as shown in listing 6.1, with each place given a name, location in X and Y coordinates, and a type.
The type field allows our data to accommodate other types of businesses and could allow us to filter search results to specific types of places.
The coordinates are indexed into a single location field as a string x, y.
The location could be encoded in numerous ways, but we opted for the simplest approach for this example.
Next we write a test that we use to assert that our sorting implementation works appropriately:
Our test has shown that the first and last documents in the returned are the ones closest and furthest from home.
Muy bien! Had we not used a sort, the documents would have been returned in insertion order, since the score of each hit is equivalent for the restaurant-type query.
The constructor is handed the base location from which results are sorted by distance.
Here we store the top N distances seen, so far.
Here we store the worst distance in the top N queue, so far.
We iterate over all the terms in the specified field.
Next, we iterate over every document containing the current term.
The compare method is used by the high-level searching API when the actual distance isn’t needed.
Record which slot is the bottom (worst) in the queue.
The value method is used by the lower-level searching API when the distance value is desired.
The sorting infrastructure within Lucene interacts with the FieldComparator API in order to sort matching documents.
For performance reasons, this API is more complex than one would otherwise expect.
In particular, the comparator is made aware of the size of the top N queue (passed as the numHits argument to newComparator) being maintained within Lucene.
In addition, the comparator is notified every time a new segment is searched (with the setNextReader method)
While searching, when a document is competitive it is inserted into the queue at a given slot, determined by Lucene.
Your comparator is asked to compare hits within the queue (compare), set the bottom (worst scoring entry) slot in the queue (setBottom), compare a hit to the bottom of the queue (compareBottom), and copy a new hit into the queue (copy)
In a homogeneous index where all documents have the same fields, this would involve computing the distance for every document.
Given these steps, it’s imperative that you’re aware of the resources utilized to sort; this topic is discussed in more detail in section 5.1.9 as well as in Lucene’s Javadocs.
Sorting by runtime information such as a user’s location is an incredibly powerful feature.
At this point, though, we still have a missing piece: What is the distance from each of the restaurants to our current location? When using the TopDocs-returning search methods, we can’t get to the distance computed.
However, a lower-level API lets us access the values used for sorting.
The exception enters with accessing custom sorting values, like the distance to each of the restaurants computed by our custom comparator source.
The signature of the method we use, on IndexSearcher, is:
TopFieldDocs  contains the total number of ScoreDocs, the SortField array used for sorting, and an array of FieldDoc (subclass of ScoreDoc) objects.
A FieldDoc encapsulates the computed raw score, document ID, and an array of Comparables with the value used for each SortField.
TopFieldDocs and FieldDoc are specific to searching with a Sort.
Rather than concerning ourselves with the details of the API, which you can get from Lucene’s Javadocs or the source code, let’s see how to really use it.
This lower-level API requires that we specify the maximum number of hits returned.
The total number of hits is still provided because all hits need to be determined to find the three best ones.
The total number of documents (up to the maximum specified) are returned.
The value of the first (and only, in this example) SortField computation is available in the first fields slot.
In most applications with full-text search, users are looking for the most relevant documents from a query.
The most common usage pattern is such that only the first few highest-scoring hits are visited.
In some scenarios, though, users want to be shown all documents (by ID) that match a query without needing to access the contents of the document; search filters, discussed in section 5.5, may use HitCollectors efficiently in this manner.
Another possible use, which we demonstrate in this section, is accessing every document’s contents from a search in a direct fashion.
Using a TopDocs-returning search method will work to collect all documents if you traverse all the results and process them manually, although you’re incurring the cost of sorting by relevence.
We show you two simple custom HitCollectors, BookLinkCollector and AllDocCollector.
We’ve developed a custom HitCollector, called BookLinkCollector, which builds a map of all unique URLs and the corresponding book titles matching a query.
The collect(int, float) method must be implemented from the HitCollector interface.
Our collector collects all book titles (by URL) that match the query.
Using a HitCollector requires the use of IndexSearcher’s search method variant as shown here:
In our example, we’re sure we want the title and URL of each document matched.
Stopping a HitCollector midstream is a bit of a hack, though, because there is no built-in mechanism to allow for this.
To stop a HitCollector, you must throw a runtime exception and be prepared to catch it where you invoke search.
It simply iterates through the results, printing the score and title of each.
Filters (see section 5.5), such as QueryFilter, can use a HitCollector to set bits on a DocIdSet.
Sometimes you’d like to simple record every single matching document for a search, and you know the number of matches will not be very large.
You simply instantiate it, pass it to the search, and then use the getHits() method to retrieve all hits.
In section 3.5, we introduced QueryParser and showed that it has a few settings to control its behavior, such as setting the locale for date parsing and controlling the default phrase slop.
QueryParser is also extensible, allowing subclassing to override parts of the query-creation process.
In this section, we demonstrate subclassing QueryParser to disallow inefficient wildcard and fuzzy queries, custom date-range handling, and morphing phrase queries into SpanNearQuerys instead of PhraseQuerys.
Although QueryParser has some quirks, such as the interactions with an analyzer, it does have extensibility points that allow for customization.
Table 6.1 details the methods designed for overriding and why you may want to do so.
These methods are responsible for the construction of either a TermQuery or a PhraseQuery.
If special analysis is needed, or a unique type of query is desired, override this method.
For example, a SpanNearQuery can replace PhraseQuery to force ordered phrase matches.
This method is used to construct a query when the term ends with an asterisk.
The term string handed to this method doesn’t include the trailing asterisk and isn’t analyzed.
Default range-query behavior has several noted quirks (see section 3.5.5)
Handle number ranges by padding to match how numbers were indexed.
Wildcard queries can adversely affect performance, so overridden methods could throw a ParseException to disallow them.
Alternatively, since the term string isn’t analyzed, special handling may be desired.
All of the methods listed return a Query, making it possible to construct something other than the current subclass type used by the original implementations of these methods.
Also, each of these methods may throw a ParseException allowing for error handling.
QueryParser also has extensibility points for instantiating each query type.
These differ from the points listed in table 6.1 in that they simply create the requested query type and return it.
Overriding these is useful if you simply want to change which Query class is used for each type of query without altering the logic of what query is constructed.
For example, if whenever a TermQuery is created by QueryParser you’d like to instantiate your own subclass of TermQuery, simply override newTermQuery.
The subclass in listing 6.5 demonstrates a custom query parser subclass that disables fuzzy and wildcard queries by taking advantage of the ParseException option.
To use this custom parser and prevent users from executing wildcard and fuzzy queries, construct an instance of CustomQueryParser and use it exactly as you would QueryParser, as shown in the following code:
With this implementation, both of these expensive query types are forbidden, giving you some peace of mind in terms of performance and errors that may arise from these queries expanding into too many terms.
Our next example of QueryParser extension shows how to tweak how RangeQuery is created.
You’ve seen in several places how dates can be handled, which amounts to their being converted into a text representation that can be ordered alphabetically.
Handling numbers is basically the same, except implementing a conversion to a text format is left up to you.
In this section, our example scenario indexes an integer id field so that range queries can be performed.
Lucene stores term information with prefix compression so that no penalty is paid for large shared prefixes like this zero padding.
This is done in our test setUp() method on the id keyword field:
The values are taken literally and aren’t padded as they were when indexed.
Fortunately we can fix this problem in our CustomQueryParser by overriding the getRangeQuery() method:
This implementation is specific to our id field; you may want to generalize it for more fields.
If the field isn’t id, it delegates to the default behavior.
The id field is treated specially, and the pad function is called just as with indexing.
The following test case shows that the range query worked as expected, and you can see the results of the padding using Query’s toString(String) method:
Our test shows that we’ve succeeded in allowing sensible-looking user-entered range queries to work as expected.
Our final QueryParser customization shows how to replace the default PhraseQuery with SpanNearQuery.
A PhraseQuery could be created from a single term if the analyzer created more than one token for it.
We delegate to QueryParser’s implementation for analysis and determination of query type.
Here we override PhraseQuery and return anything else right away.
Finally, we create a SpanNearQuery with all the terms from the original PhraseQuery.
Our test case shows that our custom getFieldQuery is effective in creating a SpanNearQuery:
Another possible enhancement would add a toggle switch to the custom query parser, allowing the inorder flag to be controlled by the user of the API.
If all the information needed to perform filtering is in the index, there is no need to write your own filter because the QueryFilter can handle it.
However, there are good reasons to factor external information into a custom filter.
Using our book example data and pretending we’re running an online bookstore, we want users to be able to search within our special hot deals of the day.
One option is to store the specials flag in an index field.
Rather than reindex documents when specials change, we opt to keep the specials flagged in our (hypothetical) relational database.
To do this right, we want it to be test-driven and demonstrate how our SpecialsFilter can pull information from an external source without even having an external source! Using an interface, a mock object, and good ol’ JUnit, here we go.
Since we won’t have an enormous amount of specials at one time, returning all the ISBNs of the books on special will suffice.
Now that we have a retrieval interface, we can write our custom filter, SpecialsFilter.
Enabled bits mean the document for that position is available to be searched against the query, and unset bits mean the document won’t be considered in the search.
Here, we fetch the ISBNs of the specials we want to enable for searching.
With the matching document found, we set its corresponding bit.
Here’s how we test our SpecialsFilter, using the same setUp() that the other filter tests used:
We use a generic query that is broad enough to retrieve all the books, making assertions easier to craft; but because our filter trimmed the search space, only the specials are returned.
With this infrastructure in place, implementing a SpecialsAccessor to retrieve a list of ISBNs from a database should be easy; doing so is left as an exercise for the savvy reader.
Note that we made an important implementation decision not to cache the DocIdSet in SpecialsFilter.
To add to the filter terminology overload, one final option is FilteredQuery.5 FilteredQuery inverts the situation that searching with a Filter presents.
Using a Filter an IndexSearcher’s search method applies a single filter during querying.
Using the FilteredQuery, though, you can apply a Filter to a particular query clause of a BooleanQuery.
This time, we want a more sophisticated query: books in an education category on special, or books on Logo.6 We couldn’t accomplish this with a direct query using the techniques shown thus far, but FilteredQuery makes this possible.
Had our search been only for books in the education category on special, we could have used the technique shown in the previous code snippet, instead.
Our test case, in listing 6.7, demonstrates the described query using a BooleanQuery with a nested TermQuery and FilteredQuery.
We’re sorry! We know that Filter, QueryFilter, FilteredQuery, and the completely unrelated TokenFilter names can be confusing.
Erik began his programming adventures with Logo on an Apple ][e.
Times haven’t changed much; now he tinkers with StarLogo on a PowerBook.
We construct a query for education books on special, which only includes Steiner’s book in this example.
We construct a query for all books with logo in the subject, which only includes Mindstorms in our sample data.
The getDocIdSet() method of the nested Filter is called each time a FilteredQuery is used in a search, so we recommend that you use a caching filter if the query is to be used repeatedly and the results of a filter don’t change.
We’ll switch to an advanced means of customizization and a relatively new feature in lucene, payloads.
Payloads are an advanced feature in Lucene that enables an application to store an arbitrary byte array for every occurrence of a term during indexing.
This byte array is entirely opaque to Lucene: it’s simply stored at each Term position, during indexing, and then can be retrieved during searching.
Otherwise the core Lucene functionality doesn’t do anything with the payload or make any assumptions about its contents.
This means you can store arbitrary encoded data that is important to your application, and then use it during searching, either to decide which documents are included in the search results, or to alter how matched documents are scored, or, both.
Let’s see how to use payloads for position-specific boosting, whereby matched documents can be boosted when the specific terms that matched were “important”
Imagine we are indexing mixed documents, where some of them are bulletins (weather warnings) while others are more ordinary.
You’d like a search for “warning” to give extra boost when it occurs in a bulletin document.
Another example is boosting terms that were bolded or italicized in the original text, or contained within a title or header tag for HTML documents.
While you could use field boosting to achieve this, that’d require you to separate out all the important terms into entirely separate fields, which is often not feasible or desired.
Payloads lets you solve this by boosting on a term by term basis within a single field.
The first step is to create an analyzer that attaches payloads to certain tokens.
It’s perfectly fine to set a null payload for some tokens.
In fact, for application where there is a common “default value”, it’s best to represent that default value as a null payload, instead of a payload with the default value encoded into it, to save space in your index.
Lucene simply records that there is no payload available at that position.
The sandbox, under contrib/analyzers, includes several useful TokenFilters, as shown in Table 6.2
These classes simply translate certain existing attributes of a Token, such as type and start/end offset, into a corresponding payload.
PayloadHelper Static methods to encode and decode ints and floats into byte array payloads.
Quite often, as is the case in our example, the logic you need to create a payload requires more.
In our case, we want to create a payload for those term occurrences that should be boosted, containing the boost score, and set no payload for all other terms.
Fortunately, it’s straightforward to create your own TokenFilter to implement such logic.
Our logic is quite simple: if the document is a bulletin, which is simplistically determined by checking whether the contents start with the prefix “Bulletin:”, then we attach a payload that encodes a float boost to any occurrence of the term “warning”
We use PayloadHelper to encode the float into a byte array.
Listing 6.8 Custom filter and analyzer to attach payloads to the token “warning” inside bulletin documents.
If document is a bulletin, and term is warning, record payload boost #2 Clear payload to get no boost.
Using this analyzer, we can get our payloads into the index.
Let’s create our own Similarity class, subclassing DefaultSimilarity, that overrides scorePayload:
We again use PayloadHelper, this time to decode the byte array back into a float.
Now that we have all the pieces, let’s pull it together into a test case as shown in Listing 6.9
The first search is a normal TermQuery, which should return the 2nd document as the top result, because it contains two occurrences of the term “warning”
Indeed, BoostingTermQuery caused the two bulletins to get much higher scores, bringing them to the top of the results!
While BoostingTermQuery is the simplest way to use payloads to alter scoring of documents, the SpanQuery classes have also been extended on an initial, experimental basis, to include payloads.
Each SpanQuery class implements the method getPayloadSpans, to retrieve all matching spans for the query, along with the payloads contained within each span that matched.
At this point, none of the SpanQuery classes make use of the payloads.
It’s up to you to subclass a SpanQuery class and override the getSpans method if you’d like to filter documents that match based on payload, or override the SpanScorer class to provide custom scoring based on the payloads contained within each matched span.
These are very advanced use cases, and only a few users have ventured into this territory, so your best bet for inspiration is to spend some quality time on Lucene’s users list.
The final Lucene API that has been extended with payloads is the TermPositions iterator.
This is an advanced internal API that allows you to step through the posting list for a specific term, retrieving each document that matched plus all positions, along with their payload, of that term’s occurrences in the document.
Note that once you’ve called getPayload() you cannot call it again until you’ve advanced to the next.
Payloads are still under active development and exploration, in order to provide more core support to.
Until the core support is fully fleshed out, you’ll need to use the extension points described here to take advantage of this powerful feature.
Next we show some tools to evaluate search performance using the JUnitPerf framework.
But how fast is it? Is it fast enough? Can you guarantee that searches are returned within a reasonable amount of time? How does Lucene respond under load?
If your project has high performance demands, you’ve done the right thing by choosing Lucene, but don’t let performance numbers be a mystery.
There are several ways Lucene’s performance can be negatively impacted by how you use it—like using fuzzy or wildcard queries or a range query, as you’ll see in this section.
We’ve been highlighting unit testing throughout the book using the basics of JUnit.
JUnitPerf, a JUnit decorator, allows JUnit tests to be measured for load and speed.
We’ve discussed how FuzzyQuery and WildcardQuery have the potential to get out of control.
In a similar fashion, RangeQuery can, too: As it enumerates all the terms in the range, it forms a BooleanQuery that can potentially be large.
The infamous Mike “addicted to the green bar” Clark has graciously donated some Lucene performance tests to us.7 Let’s examine a concrete example in which we determine that a searching performance issue is caused by how we index, and find out how we can easily fix this issue.
We rely on JUnitPerf to identify the issue and ensure that it’s fixed and stays fixed.
Let’s create a test up front to ensure that our search is returning the expected results by searching over a timestamp range that encompasses all documents indexed:
See the “about this book” section at the beginning of the book for details on obtaining the full source code.
We’ve indexed 1,000 documents and found them all using an encompassing date RangeQuery.
Our dataset is only 2,000 documents, which is in general no problem for Lucene to handle.
But, by default a RangeQuery internally rewrites itself to a BooleanQuery with a SHOULD clause for every term in the range.
This exceeds the default limit of 1,024 clauses to a BooleanQuery, which prevents queries from getting carried away.
Even so, it’s still in your interest to reduce the number of terms in the range so that the rewrite process runs as quickly as possible.
It’s unlikely we’ll need to search for documents in a range of seconds, so using this fine-grained timestamp isn’t necessary.
Since searching by day, not second, is the real goal, let’s index the documents by day instead:
Remember, terms are sorted alphabetically, so numbers need to take this into account (see section 6.3.3 for a number-padding example):
Notice that we’re using a String value for today (such as 20040715) rather than using the DateField .dateToString() method.
Regardless of whether you index by timestamp or by YYYYMMDD format, the documents all have the same year, month, and day; so in our second try at indexing a last-modified field, there is only a single term in the index, not thousands.
This is a dramatic improvement that’s easily spotted in JUnitPerf tests.
You can certainly keep a timestamp field in the document, too—it just shouldn’t be a field used in range queries unless you actually require per-second resolution.
We first run one test to warm up the JVM prior to timing.
Then, we wrap the simple test inside a TimedTest, asserting that it runs in 100 milliseconds or less.
Of course, when 2,000 documents are attempted it fails horribly with a TooManyClauses exception.
The value of today in testSearchByDay() is the current date in YYYYMMDD format.
Now we replace one line in SearchTimedTest with a testSearchByDay():
Our SearchTimedTest now passes with flying colors (see figure 6.3 for timings of SearchTest under load)
Not only can JUnitPerf decorate a test and assert that it executes in a tolerated amount of time, it can also perform load tests by simulating a number of concurrent users.
The same decorator pattern is used as with a TimedTest.
Decorating a TimedTest with a LoadTest is the general usage, as shown in listing 6.9
We wrap the basic test (ensuring that 1,000 hits are found) with a TimedTest.
Then we wrap the TimedTest in a LoadTest, which executes the TimedTest 10 times concurrently.
The results indicate that each test performed well under the 100-millisecond requirement, even running under concurrent load.
The built-in date-range handling parses DateFormat.SHORT formats into the DateField  text conversions.
It would be nice to let users enter a typical date format like 1/1/04 and have it converted to our revised date format of YYYYMMDD.
This can be done in a similar fashion to what we did in section 6.3.3 to pad integers for range queries.
The only difference between our overridden getRangeQuery and the original implementation is the use of YYYYMMDD formatting.
In addition to testing whether Lucene can perform acceptably with your environment and data, unit performance testing assists (as does basic JUnit testing) in the design of your code.
In this case, you’ve seen how our original method of indexing dates was less than desirable even though our first unit test succeeded with the right number of results.
Only when we tested with more data or with time and load constraints did an issue present itself.
However, we wouldn’t be able to hide a performance test failure.
We strongly encourage you to adopt unit testing in your projects and to continue to evolve the testing codebase into performance unit testing.
As you can tell from the code examples in this book, we are highly test-centric, and we also use tests for learning purposes by exploring APIs.
Lucene itself is built around a strong set of unit tests, and it improves on a regular basis.
Custom sorting is straightforward and useful when the built-in sorting by relevance or field values is not sufficient.
Custom HitCollector implementations let you efficiently do what you want with each search hit as it’s found, while custom Filters allow you to pull in any external information to construct a filter.
By extending QueryParser you can refine how it constructs queries, in order to prevent certain kinds of queries or alter how each Query is constructed.
Finally we showed how the advanced payloads functionality can be used for refined control over which terms in a document are more important than others, based on their positions.
One of the more mundane yet vital steps when building a search application is extracting text from the documents you need to index.
You might be lucky to have an application whose content is already in textual format or whose documents are always the same format, such as XML files or as rows in a database.
If you are unlucky, you must instead accept the surprisingly wide plethora of document formats that are popular today such as Outlook, Word, Excel, PowerPoint, Visio, Flash, PDF, Open Office, RTF, HTML and even archive file formats like Tar and Zip.
Even seemingly textual formats, like XML or HTML, present challenges as you must take care not to accidentally include any tags or JavaScript sources, etc.
The plain text format might seem easiest of all, yet frequently it is difficult to determine its character encoding.
In the past it was necessary to "go it alone": track down your own document filters, one by one, and interact with their unique and interesting APIs in order to extract the text you need.
Fortunately, we now have a nice framework called Tika which handles most of the work for you.
Development continues at a rapid pace, and it's expected there will be.
Tika is actually a framework that hosts plugin parsers for each supported document type.
The framework presents the same standard API to the application for extracting text and metadata from a document, and under the hood the plugin parser interacts with the external library using the custom API exposed by that library.
This lets your application use the same uniform API regardless of document type.
When you need to extract text from a document, Tika finds the right parser for the document (details on this shortly)
Being a framework, Tika doesn't do any of the actual document filtering itself.
Rather, it relies on external open-source projects and libraries to do the heavy lifting.
There is support for many common document formats, and new formats are added frequently, so check online for the latest list.
In addition to extracting the body text for a document, Tika also extracts metadata values for most document types.
However, not all parsers can extract metadata, and when they do, they may extract to different metadata keys than you expect.
In general the area of metadata extraction is still in flux in Tika, so it’s best to test parsing some samples of your documents to understand what metadata is exposed.
Table 7.1: Supported document formats and the library used to parse them.
Metadata Constant Description RESOURCE_KEY_NAME The name of the file or resource that contains the document.
The parser implementation may set this property if the file format contains the canonical name of the file (for example the Gzip format has a slot for the file name)
A client application can set this property based on an HTTP Content-Type header, for example.
The declared content type may help the parser to correctly interpret the document.
The parser implementation sets this property to the content type according to which the document was parsed.
The declared content type may help the parser to correctly interpret the document.
The parser implementation sets this property to the content type according to which the document was parsed.
The parser implementation sets this property if the document format contains an explicit title field.
The parser implementation sets this property if the document format contains an explicit author field.
Let’s drill down into how Tika models a document’s logical structure, and what concrete API is used to expose this.
Tika uses the XHTML (Extensible Hypertext Markup Language) standard to model all documents, regardless of their original format.
With XHTML, a document is cast to this logical structure:
This is the logical structure of an XHTML document, but how does Tika actually deliver that to your application? The answer is SAX (Simple API for XML), another well established standard used by XML parsers.
This is a very scalable approach for parsing XML documents since it enables the application to pick and choose what should be done with each element, as it is encountered.
Arbitrarily large documents can be processed with minimal consumption of RAM.
Tika reads the bytes for the document from the InputStream, but will not close it.
The document parser then decodes the bytes, translates the document into the logical XHTML structure, and.
The final parameter, metadata, is used bidirectionally: input details, such as specified Content-Type (from an HTTP Server) or filename (if known) are set before invoking parse, and then any metadata encountered while Tika is processing the document will be recorded and returned.
You can see Tika itself is simply a conduit: it doesn't do anything with the document text it encounters except invoke the ContentHandler.
It's then up to your application to provide a ContentHandler that actually does something of interest with the resulting elements and text.
However, Tika includes some helpful utility classes that implement ContentHandler for common cases.
If you know for certain which document type you are dealing with, you can create the right parser (for example, PDFParser, OfficeParser, HtmlParser, etc) directly and then invoke its parse method.
If you are unsure of the document’s type, Tika provides an AutoDetectParser, which is a Parser implementation that uses various heuristics to determine the document's type and apply the correct parser.
Tika tries to autodetect things like document format and the encoding of the character set (for text/plain documents)
Still if you have pre-existing information about your documents, such as the original filename (containing a possibly helpful extension) or the character encoding, it's best if you provide this information via the Metadata input so Tika may make use this.
It’s time to get our feet wet!  Let’s walk through the installation process for Tika.
The source code with this book includes the 0.3 release of Tika, in the lib directory, but likely you’re staring at a newer release.
Building Tika from sources is also straightforward, although you should check Getting Started on the Tika website for any changes since this was written.
Then run "mvn install" from within the Tika source directory you unpacked above.
That command will download a bunch of dependencies into your Maven area, compile Tika's sources, run tests, and finally produce these two build artifacts in the subdirectory "target":
This uses the http://retrotranslator.sourceforge.net software, run on the first build artifact.
If all goes well, you’ll see "BUILD SUCCESSFUL" printed at the end.
Now that we’ve built Tika, it’s time to finally extract some text!  We’ll start with Tika’s built-in text extraction tool.
Tika comes with a simple built-in tool allowing you to extract text from documents in the local filesystem or via URL.
This tool creates an AutoDetectParser to filter the document, and then provides a few options for interacting with the results.
The tool can run either with a dedicated graphical user interface (GUI), or in a command-line only mode that can be chained together, using pipes, with other command-line tools.
This brings up a simple GUI window, to which you can drag and drop files in order to test how the filters work with them.
Figure 7.1 shows the window after dragging a draft of this chapter (as a Microsoft Word document) onto the window.
The window has multiple tabs showing different text extracted during filtering:
Plain text shows only the text and whitespace parts, extracted from the XHTML document.
While the GUI tool is a great way to quickly test Tika on a document, it’s often more useful to use the commandline only invocation, for example like this:
This tool accepts various command-line options to change its behavior:
This corresponds to the Plain text tab from the GUI.
You could use Tika’s command-line tool as the basis of your text extraction solution.
But if you need more control over which parts of the text are used, or which metadata fields to keep, you’ll need to use Tika’s programmatic API, which we cover next.
We’ve seen Tika’s simple parse API, which is the core of any text extraction based on Tika.
But what about the rest of the text extraction process?  How can you build a Lucene document from a SAX ContentHandler?  That’s what we’ll now do in this section.
You have a source for the document, which you must open as an InputStream.
Then you create an appropriate ContentHandler for your application, or use one of the utility classes provided with Tika.
Finally, you build the Lucene Document instance from the metadata and text encountered by the ContentHandler.
Let’s make this all concrete: recall that the Indexer tool from Chapter 1 has the limitation that it can only index plain text files (with the extension .txt)
TikaIndexer, shown in Listing 7.1, now fixes that!  Let’s walk through the approach:
Subclass the original Indexer and override two methods: the acceptFile method is changed to always return true, so that we attempt to index all files encountered; the getDocument method is changed to use Tika to extract the text.
In getDocument, we create a Metadata instance and record the filename in it so AutoDetectParser can use the file's extension to aid in choosing the right parser.
Use the utility class BodyContentHandler to get the text from the body of the document.
This example will work very well, but there are a few things you should fix before using it for real in production:
If there was a problem reading the bytes from the InputStream, you'll hit an IOException.
You may hit class loader exceptions if the required parser could not be located or instantiated.
Be more selective about which metadata fields you want in our index, and how you'd like to index them.
Right now TikaIndexer simply appends together all text from the document into the "contents" field, by adding more than one instance of that field name to the.
Add any custom logic to filter out known “uninteresting” portions of text documents, for example standard headers and footer text that appear in all documents.
If your document’s text could be very large in size, consider using the ParsingReader utility class (described next in section 7.4.1) instead.
Since Tika is advancing so quickly, it’s likely by the time you read this there is a good out-of-the-box integration of Lucene and Tika, so be sure to check at http://lucene.apache.org/tika.
As you can see, it’s quite simple using Tika’s programmatic APIs to extract text and build a Lucene document.
In our example, we used the parse API from AutoDetectParser, but Tika also provides some utility APIs that might be a useful alternate path for your application.
Since Lucene’s Field can index text directly from a Reader, this is a very simple way to index the text with Lucene.
When created, it spawns a background thread to parse the document, using the BodyContentHandler.
The resulting text is written to a PipedWriter (from java.io), and then a corresponding PipedReader is returned back to you.
Because of this streaming implementation, the full text of the document is never materialized at once.
Instead, the text is created as the Reader consumes it, with a small shared buffer.
This means even documents that parse to an exceptionally large amount of text will use very little memory during filtering.
During creation, ParsingReader also attempts to process all metadata for the document, so after it’s created but before indexing the document you should call the getMetadata() method and add any important metadata to your document.
Note that this is a new feature available starting with Tika’s 0.3 release; prior releases don’t set the metadata until after text is read from the Reader.
This class may be a great fit for your application.
However, because a thread is spawned for every document, and because PipedWriter and PipedReader are used, it’s likely net indexing throughput is slower than if you simply materialize the full text yourself up front (eg with StringBuilder)
Still, if materializing the full text up front is out of the question, because your documents may be unbounded in size, then ParsingReader is a real life saver.
Tika’s AutoDetectParser first determines the mime type of the document, through various heuristics, and then uses that mime type to look up the appropriate parser.
To do that lookup, Tika uses an instance of TikaConfig, which is a simple class that loads the mapping of mime type to parser class via an XML file.
The default TikaConfig class can be obtained with the static getDefaultConfig method, which in turn simply loads the file tika-config.xml that comes with Tika.
Since this is an XML file, you can easily open it with your favorite text editor to see which mime types Tika can presently handle.
We also used TikaConfig’s getParsers method in Listing 7.1 to list the mime types.
If you’d like to change which parser is used for a given mime-type, or, add your own parser to handle a certain mime-type, simply create your own corresponding XML file, and instantiate your own TikaConfig from that file.
This wraps up our coverage of using Tika for extracting text.
Let’s change gears now and consider some of Tika’s limitations.
Being very new, Tika has a few known challenges that it’s working through.
Some of these issues are simply a byproduct of its design and won’t change with time without major changes, while others are solvable problems and likely resolved by the time you read this.
The first challenge is loss of document structure in certain situations.
In general, some documents may have far richer structure than the simple standard XHTML model used by Tika.
In our example, addressbook.xml has rich structure, containing 2 entries each with rich specific fields.
But Tika regularizes this down to a fixed XHTML structure, thus losing some information.
If you need to make use of this structure, you're better off interacting directly with an XML parser.
Another limitation is the astounding number of dependencies when using Tika.
If you use the standalone jar, this results in a very large number of classes in that jar.
If you’re not using the standalone JAR, then you’ll need man JAR files on your CLASSPATH.
In part this is simply because Tika relies on numerous external packages to do the actual parsing.
But it’s also because these external libraries often do far more than Tika requires.
For example PDFBox and Apache POI understand document fonts, layouts, embedded graphics, etc., and are able to create new documents in the binary format or modify existing documents.
Tika only requires a small portion of this (the "extract text" part), yet these libraries don't typically factor that out as a standalone component.
As a result, numerous excess classes and JARs end up on the CLASSPATH which could cause problems if they conflict with other JARs in your application.
Another challenge is certain document parsers, such as Microsoft’s OLE2 Compound Document Format, require full random access to the document’s bytes, which InputStream doesn’t expose.
In such cases Tika currently copies all bytes from the stream into a temporary file, which is then opened directly for random access.
A future improvement, possibly already done by the time you read this, will allow you to pass a random access stream directly to Tika (if your document is already stored and accessible via a random access file), to avoid this unnecessary copy.
While Tika is our favorite way to extract text from documents, there are some interesting alternatives.
The Aperture open-source project, hosted by SourceForge at http://aperture.sourceforge.net, has support for a wide variety of document formats and is able to extract text content and metadata.
Furthermore, while Tika focuses only on text extraction, Aperture also provides crawling support, meaning it can connect to file systems, Web servers, IMAP mail servers, Outlook and iCal file and crawl for all documents within these systems.
There are also commercial document filtering libraries, such as Stellent’s filters (now part of Oracle) and KeyView filters (now part of Autonomy)
While these are closed solutions, and could be fairly expensive to license, they may be a fit for your application.
Finally, there are numerous individual open-source parsers out there for handling document types.
It’s entirely possible your document type already has a good open-source parser that simply hasn’t yet been integrated with Tika.
If you find one, you should consider building the Tika plugin for it and donating it back, or even simply calling attention to the parser on Tika’s developers mailing list.
There are a great many popular document formats in the world.
In the past, extracting text from these documents was a real sore point in building a search application.
But today, we have Tika, which makes text extraction surprisingly simple.
We’ve seen Tika’s command-line tool, which could be the basis of a quick integration with your application, as well as an example using Tika’s APIs that with some small modifications could easily be the core of text extraction for your search application.
Using Tika to handle text extraction allows you to spend more time on the truly important parts of your search application.
In the next chapter we’ll look at ports of Lucene to other programming languages and environments.
You’ve built an index, but can you browse or query it without writing code? Absolutely! In this chapter, we’ll discuss three tools to do this.
Do you need analysis beyond what the built-in analyzers provide? Several specialized analyzers for many languages are available in Lucene’s Sandbox.
How about providing term highlighting in search results? We’ve got that, too!
This chapter examines third-party (non-core-Lucene) software as well as several Sandbox projects.
Apache hosts a separate subversion directory, contrib, where add-ons to Lucene are kept.
Deliberate care was taken with the design of Lucene to keep the core source code cohesive yet extensible.
We’re taking the same care in this book by keeping an intentional separation between what is in the core of Lucene and the tools and extensions that have been developed to augment it.
In an effort to accommodate the increasing contributions to the Lucene project that are above and beyond the core codebase, a contrib directory was created to house them.
The Sandbox is continually evolving, making it tough to write about concretely.
We’ll cover the stable pieces and allude to the other interesting.
We encourage you, when you need additional Lucene pieces, to consult the Sandbox repository and familiarize yourself with what is there—you may find that one missing piece you need.
And in the same vein, if you’ve developed Lucene pieces and want to share the maintenance efforts, contributions are more than welcome.
Table 8.1 lists the current major contents of the Sandbox with pointers to where each is covered in this book.
Luke Graphical interface to interact with an index Section 8.2.2
Memory indices Create custom memory-based indexes for fast searching Section 8.10
WordNet Utility to build a synonym index from WordNet database Section 8.6
There are a few more Sandbox components than those we cover in this chapter.
Refer to the Sandbox directly to dig around and to see any new goodies since this was printed.
The benchmark package is so useful we dedicate a separate appendix (D) to it.
We begin with some useful tools for peeking into your Lucene index.
Now what? Wouldn’t it be nice to browse the index and perform ad hoc queries? You will, of course, write Java code to integrate Lucene into your applications, and you could fairly easily write utility code as a JUnit test case, a command-line utility, or a web application to interact with the index.
Thankfully, though, some nice utilities have already been created to let you interact with Lucene file system indexes.
We’ll explore three such utilities, each unique and having a different type of interface into an index:
Rather than write code to interact with an index, it can be easier to do a little command-line tap dancing for ad-hoc searches or to get a quick explanation of a score.
Lucli lets you scroll through a history of commands and reexecute a previously entered command to enhance its usability.
Example: count foo explain: Explanation that describes how the document scored against query.
Example: explain foo help: Display help about commands index: Choose a different lucene index.
Example index my_index info: Display info about the current Lucene index.
Example: info optimize: Optimize the current index quit: Quit/exit the program search: Search the current index.
Example: search foo terms: Show the first 100 terms in this index.
Supply a field name to only show terms in a specific field.
Example: terms tokens: Does a search and shows the top 10 tokens for each document.
Lucli is a fairly simple tool, but it has enough functionality to be very useful, especially if you’re running through a limited terminal connection and unable to run the full user-interface that tools like Luke require.
This gem provides an intimate view inside a file system–based index from an attractive desktop Java application (see figure 8.1)
We highly recommend having Luke handy when you’re developing with Lucene because it allows for ad-hoc querying and provides insight into the terms and structure in an index.
Luke has become a regular part of our Lucene development toolkit.
Its tabbed and well integrated user interface allows for rapid browsing and experimentation.
Luke can force an index to be unlocked when opening, optimize an index, and also delete and undelete documents, so it’s really only for developers or, perhaps, system administrators.
You can launch Luke via Java WebStart from the Luke web site or install it locally.
In any event, it requires JRE 1.5 or later to run.
It’s a single JAR file that can be launched directly (by double-clicking from a file-system browser, if your system supports that) or running java –jar luke.jar from the command line.
The usual issues of Lucene version and index compatibility apply.
Luke’s interface is nicely interconnected so that you can jump from one view to another in the same context.
The Tools menu provides options to optimize the current index, undelete any documents flagged for deletion, and switch the index between compound and standard format.
OVERVIEW: SEEING THE BIG PICTURE Luke’s Overview tab shows the major pieces of a Lucene index, including the number of fields, documents, and terms (figure 8.3)
The top terms in one or more selected fields are shown in the “Top ranking terms” pane.
Double-clicking a term opens the Documents tab for the selected term, where you can browse all documents containing that term.
Right-clicking a term brings up a menu with three options: “Show all term docs” opens the Search tab for that term so all documents appear in a list, “Browse term docs”
Figure 8.3 Luke: index overview, allowing you to browse fields and terms.
Browsing by document number is straightforward; you can use the arrows to navigate through the documents sequentially.
The table at the bottom of the screen shows all stored fields for the currently selected document.
Browsing by term is trickier; you can go about it several ways.
Clicking First Term navigates the term selection to the first term in the index.
You can scroll through terms by clicking the Next Term button.
The number of documents containing a given term is shown as the “Doc freq of this term” value.
To select a specific term, type all but the last character in the text box, click Next Term, and navigate forward until you find the desired term.
Just below the term browser is the term document browser, which lets you navigate through the documents containing the term you selected.
The First Doc button selects the first document that contains the selected term; and, as when you’re browsing terms, Next Doc navigates forward.
Another feature of the Documents tab is the “Copy text to Clipboard” feature.
All fields shown, or the selected fields, may be copied to the clipboard.
For example, copying the entire document to the clipboard places the following text there:
Luke can only work within the constraints of a Lucene index, and unstored fields don’t have the text available in its original form.
The terms of those fields, of course, are navigable with Luke, but those fields aren’t available in the document viewer or for copying to the clipboard (for example, our contents field in this case)
Clicking the Show All Docs button shifts the view to the Search tab with a search on the selected term, such that all documents containing this term are displayed.
If a field’s term vectors have been stored, the Field’s Term Vector button displays a window showing terms and frequencies.
One final feature of the Documents tab is the “Reconstruct & Edit” button.
Clicking this button opens a document editor allowing you to edit (delete and re-add) the document in the index or add a new document.
Luke reconstructs fields that were tokenized but not stored, by aggregating in position order all the terms that were indexed.
Reconstructing a field is a potentially lossy operation, and Luke warns of this when you view a reconstructed field (for example, if stop words were removed or tokens were stemmed during the analysis process then the original value cannot be reconstructed)
You can also use the Search tab manually, entering QueryParser expression syntax along with your choice of Analyzer and default field.
Click Search when the expression and other fields are as desired.
The bottom table shows all the documents from the search hits, as shown in figure 8.6
Figure 8.6 Searching: an easy way to experiment with QueryParser.
Double-clicking a document shifts back to the Documents tab with the appropriate document preselected.
Luke shows all analyzers it finds in the classpath, but only analyzers with no-arg constructors may be used with Luke.
Luke also provides insight into document scoring with the explanation feature.
To view score explanation, select a result and click the Explanation button; an example is shown in figure 8.7
The total index size is also shown, as you can see in figure 8.8
Figure 8.8 Luke’s Files view shows how big an index is.
Analyzer Tool has the same purpose as the AnalyzerDemo developed in section 4.2.3, showing the results of the analysis process on a block of text.
As an added bonus, highlighting a selected token is a mere buttonclick away, as shown in figure 8.9
Scripting Luke lets interactively run JavaScript code accessing Luke’s internals.
Custom Similarity allows you to code up your own Similarity implementation in JavaScript, which is then compiled and accessible in the Search panel.
Vocabulary Analysis Tool and Zipf distribution are two tools that show graphs of term statistics from the index.
Consult the Luke documentation and source code for information on how to develop your own plug-in.
Julien Nioche is the creator of Lucene Index Monitor (LIMO).2 It’s available online at http://limo.sourceforge.net/
In addition, a rudimentary document browser lets you scroll through documents sequentially.
When you cannot use Luke, because the index is on a remote server and not accessible to your local computer, LIMO is a good fallback since it runs as a webapp on the server.
Figure 8.10 shows the initial page, where you can select one or more preconfigured indexes.
Expand the WAR file in the Tomcat webapps/limo webapps directory.
Start the web container and point your browser to the appropriate URL (http://localhost:8080/limo, if you are using Tomcat and you’re on the same computer)
The version of LIMO that we used embeds Lucene 2.0; if you need to use a newer version of Lucene than LIMO embeds, replace the Lucene JAR in WEB-INF/lib by removing the existing file and adding a newer one.
The first step you must do is enter the path and name for the index you wish to browse.
These paths are simply saved into the limo.properties file under webapps/limo.
Once you’ve entered one or more indices, you can select the one you’d like to browse.
Click the Prev and Next links to navigate through the documents.
All the fields are shown on the right, indicating whether they are stored and/or indexed.
Next, you can run a search by entering your search text into the text box, selecting the default field to search as well as the analyzer you’d like to use, and then clicking the Search Index button.
This brings you to the search results page, with highlighting, as shown in Figure 8.12
On the far right of each search hit are three links: view lets you view that document in the document browsing page; explain provides an explanation of the score for that hit, in the bottom left of the page; and reconstruct rebuilds the document from the index, in the bottom right of the page.
You may want to have LIMO installed on a secured Tomcat instance on a production server.
Being able to get a quick view of how many documents are in an index, whether it’s locked, and when it was last updated can be helpful for monitoring purposes.
Also, using the LIMO JSP pages as a basis for building your own custom monitoring view could be a time saver.
Because LIMO functions as a web application and doesn’t allow any destructive operations on an index, it provides a handy and safe way to peek into a remote index.
We now switch to the numerous options in the sandbox for analysis.
And the Sandbox doesn’t disappoint in this area: It houses numerous language-specific analyzers, a few related filters and tokenizers, and the slick Snowball algorithm analyzers.
Breaks characters of a single word into a series of character ngrams.
This can be useful for spell correction and live auto completion.
Two different TokenFilters that decompose compound words you find in many Germanic languages to the word parts.
There are two approaches (one using hyphenation based grammar to detect word parts; the other using a word-based dictionary)
Similar to StandardTokenizer, except it adds further specialization to process the Wikipedia-specific tokens that appear in the XML export of the Wikipedia corpus.
A SinkTokenizer (see Section 4.XXX) that only accepts tokens within a certain range.
A SinkTokenizer (see Section 4.XXX) that only accepts tokens of a specific type as returned by Token.type()
Tokenizers that create shingles (n-grams from multiple tokens) from another TokenStream.
TokenFilters that carry over token attributes as payloads; these are described in section 6.5
The Brazilian and French analyzers use language-specific stemming and custom stop-word lists.
The Czech analyzer uses standard tokenization, but also incorporates a custom stop word list.
We demonstrate analysis of Chinese characters in section 4.8.3, illustrating how these two analyzers work.
Each of these analyzers, including the SnowballAnalyzer discussed in the next section, lets you customize the stop-word list just as the StopAnalyzer does (see section 4.3.1)
Most of these analyzers do quite a bit in the filtering process.
If the stemming or tokenization is all you need, borrow the relevant pieces, and construct your own custom analyzer from the parts here.
We’ll give special attention here to the snowball analyzers and shingle and ngram filters.
The SnowballAnalyzer deserves special mention because it serves as a driver of an entire family of stemmers for different languages.
In fact, the snowball project in Lucene’s Sandbox has a build process that can pull the definitions from Dr.
One of the test cases demonstrates the result of the English stemmer stripping off the trailing ming from stemming and the s from algorithms:
SnowballAnalyzer has two constructors; both accept the stemmer name, and one specifies a String[] stop-word list to use.
These exact names are the valid argument values for the name argument to the SnowballAnalyzer constructors.
If your project demands stemming, we recommend that you give the Snowball analyzer your attention first since an expert in the stemming field developed it.
And, as already mentioned but worth repeating, you may want to use the clever piece of this analyzer (the SnowballFilter) wrapped in your own custom analyzer implementation.
Several sections in chapter 4 discuss writing custom analyzers in great detail.
However, while the ngram tokenizers operate on letters, shingles operate on whole words.
For example the sentence “please divide this sentence into shingles” might be tokenized into the shingles “please divide”, “divide this”, “this sentence”, “sentence into” and “into shingles”
Why would you ever want to do such a thing?  One common reason is to speed up phrase searches, especially for phrases involving common terms.
For example, consider a search for the exact phrase “Wizard of Oz”
Since the word “of” is incredibly common, including it in the phrase search will require.
Lucene to visit and filter out a great many occurrences that do not match the phrase, which is costly.
If, instead, you had indexed the tokens “wizard of” and “of oz”, those tokens occur far less frequently and your phrase search can run very quickly.
The Nutch search engine, covered in Section 4.9, creates shingles for exactly this reason.
You could also simply discard such common terms as stop words, using a StopFilter during analysis.
But then you must take care to add slop into your phrase searches, as covered in section 4.7.3
Unlike shingles which correctly provide an exact match to your search, the slop solution is able to match other phrases.
Another interesting use of shingles is document clustering in order to group similar or near-duplicate documents together.
This is important for large collections of documents where duplicates may accidentally sneak in, which happens frequently when crawling for content through Web servers that construct documents dynamically.
Often slightly different URLs can yield the same underlying content, perhaps with a different header added in.
Much like using term vectors to find similar documents (Section XXX) the approach is to represent each document by its salient shingles and then search for other documents that have similar shingles with similar frequency.
The ngram filters take a single token and emit a series of letter ngram tokens, which are combinations of N adjacent letters into a single token.
Note that each larger ngram series is positioned after the previous series.
A more natural approach would be to have the ngram’s position be set to the character position where it had started in the word, but unfortunately at this time there’s no option to do this (it is however a known limitation, so by the time you read this it may be fixed)
The EdgeNGramFilter is similar, except it only generates ngrams anchored to the start or end of the word.
Depending on your needs, you may want JAR binary distributions of these analyzers or raw source code from which to borrow ideas.
Section 8.10 provides details on how to access the Sandbox SVN repository.
Within the repository, the Snowball analyzer resides in contrib/snowball; the other analyzers discussed here are in contrib/analyzers.
There are no external dependencies for these analyzers other than Lucene itself, so they are easy to incorporate.
A test program called TestApp is included for the Snowball project.
Only the Snowball stemmer itself is used with rudimentary text splitting at whitespace.
Our next package extends ant with tasks to control indexing.
A natural integration point with Lucene incorporates document indexing into a build process.
As part of Java Development with Ant (Hatcher and Loughran, Manning Publications, 2002), Erik created an Ant task to index a directory of file-based documents.
This code has since been enhanced and is maintained in the Sandbox.
Why index documents during a build process? Imagine a project that is providing an embedded help system with search capability.
The documents are probably static for a particular version of the system, and having a read-only index created at build-time fits perfectly.
For example, what if the Ant, Lucene, and other projects had a domain-specific search on their respective web sites? It makes sense for the searchable documentation to be the latest release version; it doesn’t need to be dynamically updated.
We first describe the <index> task, then show how to create a custom document handler, and finally show how to install this package.
Parent of index directory #2 Root directory of documents to index.
The Ant integration is Ant 1.6 Antlib compatible, as seen with the xmlns specification.
Only path and modified are fixed fields; the others come from the document handler.
Contents Text Complete contents of .txt files; parsed <body> of HTML files.
It’s very likely that the default document handler is insufficient for your needs.
JTidy is currently used to extract HTML content for indexing.
We use <typedef> because we need an additional dependency added to the classpath for our document handler.
If we didn’t need a custom document handler, the <typedef> would be unnecessary.
We use a custom document handler to process files differently.
Here we hand our document handler a configuration property, basedir.
At this point, the two document-handling frameworks are independent of one another, although they’re similar and can be easily merged.
The folder hierarchy serves as meta-data also, specifying the book categories.
The base directory points to data and is stripped off in the document handler as shown in listing 8.4
Figure 8.13 Sample data directory structure, with the file path specifying a category.
To write a custom document handler, pick one of the two interfaces to implement.
If you don’t need any additional meta-data from the Ant build file, implement DocumentHandler, which has the following single method returning a Lucene Document instance:
Configuration options are passed using a single <config> subelement with arbitrarily named attributes.
The <config> attribute names become the keys to the properties.
We base the category on the relative path from the base data directory, ensuring that forward slashes are used as separators.
Here we pull each field from the values in the .properties file.
We add each field to the Document instance; note the different types of fields used.
The contents field is an aggregate field: We can search a single field containing both the author and subject.
When you use a custom document handler, in addition to the fields the handler creates, the <index> task automatically adds path and modified fields.
These two fields are used for incremental indexing, allowing only newly modified files to be processed.
The build file can also control the analyzer and merge factor.
Currently, only analyzers that have a no-argument constructor can be used with <index>; this rules out using the SnowballAnalyzer directly, for example.
There are several interesting possibilities, thanks to the flexibility of the <index> task, such as indexing documentation in multiple languages.
You could use the <index> task multiple times in a build process to build a separate index for each language, or you could write them all to the same index and use a different analyzer for each language.
The Lucene JAR, JTidy’s JAR, and the JAR of the <index> task itself are required.
See section 8.10 for elaboration on how to obtain JARs from the Sandbox component, and refer to Ant’s documentation and Manning’s Java Development with Ant for specifics on working with Ant.
Let’s see next how to build a search UI with the JavaScript utilities sandbox package.
Integrating Lucene into an application often requires placing a search interface in a web application.
QueryParser is handy, and it’s easy to expose a simple text box allowing the user to enter a query; but it can be friendlier for users to see query options separated into fields, such as a date-range selection in conjunction with a text box for free-text searching.
The JavaScript utilities in the Sandbox assist with browser-side usability in constructing and validating sophisticated expressions suitable for QueryParser.
We’ll describe how a query is created and validated, how to escape special characters, and finally how to use the JavaScript sources.
As we’ve explored in several previous chapters, exposing QueryParser directly to end users can lead to confusion.
If you’re providing a web interface to search a Lucene index, you may want to consider using the nicely done JavaScript query constructor and validator in the Sandbox, originally written by fellow Lucene developer Kelvin Tan.
The javascript Sandbox project includes a sample HTML file that mimics Google’s advanced searching options, as shown in figure 8.14
The query constructor supports all HTML fields including text and hidden fields, radio buttons, and single and multiple selects.
Each HTML field must have a corresponding HTML field named with the suffix Modifier, controlling how the terms are added to the query.
The modifier field can be a hidden field to prevent a user from controlling it, as in the case of the text fields in figure 8.12
The constructed query is placed in an HTML field (typically a hidden one), which is handed to QueryParser on the server side.
The query validator uses regular expressions to do its best approximation of what is acceptable to QueryParser.
Both JavaScript files allow customization with features like debug mode to alert you to what is happening, modifier field suffixes, specifying whether to submit the form upon construction, and more.
The Java-Script files are well documented and easy to drop into your own environment.
At the time of this writing, the javascript Sandbox was being enhanced.
Rather than show potentially out-of-date HTML, we refer you to the examples in the Sandbox when you need this capability.
The characters must be escaped if they’re used in a field name or as part of a term (see section 3.5 for more details on QueryParser escape characters)
You should use the query escaper only on fields or strings that should not contain any Lucene special characters already.
For example, it would be incorrect to escape a query built with the query constructor, since any parentheses and operators it added would be subsequently escaped.
A system developed at Princeton University’s Cognitive Science Laboratory, driven by Psychology Professor George Miller, illustrates the net of synonyms.8 WordNet represents word forms that are interchangeable, both lexically and semantically.
Google’s define feature (type define: word as a Google search, and see for yourself) often refers users to the online WordNet system, allowing you to navigate word interconnections.
Figure 8.14 shows the results of searching for search at the WordNet site.
Interestingly, this is the same George Miller who reported on the phenomenon of seven plus or minus two chunks in immediate memory.
Figure 8.15 Caught in the WordNet: word interconnections for search.
What does all this mean to developers using Lucene? With Dave Spencer’s contribution to Lucene’s Sandbox, the WordNet synonym database can be churned into a Lucene index.
This allows for rapid synonym lookup—for example, for synonym injection during indexing or querying (see section 8.6.2 for such an implementation)
We first see how to build an index containing WordNet’s synonyms, then how to use these synonyms during analysis, and finally an unusual example of what you can do with a WordNet index.
Obtain the binary (or build from source; see section 8.10) of the Sandbox WordNet package.
It should produce a subdirectory, prolog, that has many files.
Build the synonym index using the Syns2Index program from the command line.
The first parameter points to the wn_s.pl file and the second argument specifies the path where the Lucene index will be created:
The Syns2Index program converts the WordNet Prolog synonym database into a standard Lucene index with an indexed field word and unindexed fields syn for each document.
A second utility program in the WordNet Sandbox area lets you look up synonyms of a word.
Here is a sample lookup of a word near and dear to our hearts:
Synonyms found for "search": explore hunt hunting look lookup research seek.
To use the synonym index in your applications, borrow the relevant pieces from SynLookup, as shown in listing 8.5
The SynLookup program was written for this book, but it has been added into the WordNet Sandbox codebase.
The custom SynonymAnalyzer from section 4.6 can easily hook into WordNet synonyms using the SynonymEngine interface.
We use the AllDocCollector from section 6.XXX to keep all synonyms.
Interestingly, WordNet synonyms do exist for jump and dog (see the lucli output in listing 8.1), but only in singular form.
These are issues that need to be addressed based on your environment.
This emphasizes again the importance of the analysis process and the fact that it deserves your attention.
We’ve apparently befuddled or outfoxed the WordNet synonym database because the synonyms injected for fox don’t relate to the animal noun we intended.
With the ubiquity of mobile devices and their shrinking size, we need clever text-input methods.
Each word, its T9 equivalent, and the text length of the word are indexed, as shown here:
T9 is an input method that maps each numeric button to multiple letters of the alphabet.
A series of numbers logically corresponds to a subset of sensible words.
Many thanks to Dave Engler for building the base Swing application framework.
We omit norms and set omitTf on the word and length fields, since they are only used for sorting.
For the t9 field we omit norms because every field has the same length.
The t9 method is not shown, but it can be obtained from the book’s source code distribution (see the “About this book” section)
The query uses a boosted TermQuery on the exact digits (to ensure that exact matches come first) and wildcard queries matching words with one or two more characters more.
The search results are sorted first by score, then by length, and finally alphabetically within words of the same length:
A unit of astronomical length based on the distance from Earth at which stellar parallax is 1 second of arc;
The status bar displays the time the search took (often under 5ms)
Giving end users some context around hits from their searches is friendly and, more important, useful.
Each hit, as shown in Figure 1.1, includes up to three lines of the matching document highlighting the terms of the query.
Often a brief glimpse of the surrounding context of the search terms is enough to know if that result is worth investigating further.
Like spell correction, covered in Section 8.11, the Web search engines have established this feature as a baseline requirement that all other search engines are expected to have.
What’s commonly referred to as highlighting in fact consists of two separate functions.
First is dynamic fragmenting, which means picking a few choice sentences out of a large text that best match the search query.
Some search applications skip this step, and instead fallback on a static abstract or summary for each document, but generally that gives a worse user experience because it’s static.
The second function is highlighting, whereby specific words in context of their surrounding text are called out, often with bolding and a colored background, so the user’s eyes can quickly jump to specific words that matched.
For example, you may apply highlighting to a title field without deriving fragments from it, because you always want to present the full title.
Or, for a field that has a large amount of text, you would first fragment it and then apply the highlighting.
Thanks to Mark Harwood’s contribution, the Sandbox includes infrastructure to fragment and highlight text based on a Lucene query.
Figure 8.17 is an example of using Highlighter on part of the text from this section, based on a term query for highlighting.
The source code for this is shown in Listing 8.7
We begin by an overview of the components used during highlighting, then show a simple example of highlighter in action, including how to use cascading style sheets to control the mechanics of highlighting.
We wrap up showing you how to highlight actual search results.
The Highlighter code is a sophisticated and flexible utility, and is well factored to break out the separate steps necessary during fragmentation and highlighting.
Figure 8.19 shows the steps used by the Highlighter class to compute the highlighted text.
Typically you would store the full text as a stored field in the index, but if you have an alternate external store, for example a database, that works fine as well.
Just be sure that source can deliver the text for N documents per search quickly enough.
To create the TokenStream, you could simply re-analyze the text.
The convenient TokenSources class in the Highlighter package has various static convenience methods that will extract a TokenStream from an index using whichever of these sources is appropriate.
You can also create your own TokenStream separately if you’d like.
Generally, term vectors will give you the fastest performance, but they do consume additional space in the index.
Highlighter relies on the start and end offset of each Token from the token stream, to locate the exact character slices to highlight in the original input text.
The core Lucene analyzers all set the offsets properly, so this normally is not a problem unless you’ve created your own analyzer.
The next component, Fragmenter, breaks the original text into small pieces called fragments.
NullFragmenter is one concrete class implementing this interface that simply returns the entire string as a single fragment.
This is appropriate for title fields and other short text fields, where you wish to show the full text.
SimpleFragmenter is another concrete class that breaks the text up into fixed-size fragments by character length, with no effort to spot sentence boundaries.
You can specify how many characters per fragment (the default is 100)
However, this fragmenter is a little too simple: it does not take into account positional constraints of the query when creating the fragments, which means for phrase queries and span queries, a matching span will easily be broken across two fragments.
You’ll have to pass in a SpanScorer (see next section) so it knows where the span matches are.
If you don’t set a Fragmenter on your Highlighter instance, it uses SimpleFragmenter by default.
Although it does not exist currently in the Highlighter package, a good implementation of.
Fragmenter would be one that attempts to produce fragments on sentence boundaries.
Highlighter then takes each fragment produced by the fragmenter and passes each to the Scorer.
To do this, Highlighter asks the Scorer, a Java interface, to score each fragment.
The Highlighter package provides two concrete implementations: QueryScorer, which scores each fragment based on how many terms from the provided Query appear in the fragment, and SpanScorer, which attempts to only assign scores to actual term occurrences that contributed to the match for the document.
QueryScorer uses the terms from the query; it extracts them from primitive term, phrase, and Boolean queries and weighs them based on their corresponding boost factor.
A query must be rewritten in its most primitive form for QueryScorer to be happy.
For example, wildcard, fuzzy, prefix, and range queries rewrite themselves to a BooleanQuery of all the matching terms.
SpanScorer extracts matching spans for the query and then uses these spans to score each fragment.
Fragments that did not in fact match the query, even if they contain a subset of the terms from the query, receive a score of 0.0
If you use the simpler QueryScorer, you’ll find that a PhraseQuery can show fragments that do not actually show the entire phrase, which is terribly disconcerting and trust eroding to the end user.
Note, however, that because SpanScorer is specific to each matching document, since it enumerates the specific matching spans, it must be instantiated for every document you need to highlight.
Because of these benefits, it’s strongly recommended that you use SpanScorer instead of QueryScorer.
At this point Highlighter chooses the best scoring fragments to present to the user.
There are two concrete implementations: DefaultEncoder, which is used by default in Highlighter, simply does nothing with the text.
Once encoder is done, the final step is to format the fragments for presentation.
The default constructor will use the <b> (bold) HTML tag.
GradientFormatter uses different shades of background color to indicate how strong each hit was, using the <font> HTML tag.
You can of course also create your own class implementing the Formatter API.
Now that you understand the logical steps of the highlighting process, let’s look at some concrete examples.
The simplest example of Highlighter returns the best fragment, surrounding each matching term with HTML <B> tags:
String text = "The quick brown fox jumps over the lazy dog";
In this simple example, our text was a fixed string and we derived a TokenStream by using SimpleAnalyzer.
To successfully highlight terms, the terms in the Query need to match Tokens emitted from the TokenStream.
The same text must be used to generate the TokenStream as is used for the original text to highlight.
SpanScorer requires you to wrap the TokenStream in a CachingTokenFilter as it needs to process the tokens more than once.
In this simple example, since the text is so small, the fragmenter is pointless as the entire text will become the one and only fragment; for example, we could have simply used NullFragmenter instead.
Finally, we create Highlighter, set our fragmenter, then ask it to for the best scoring fragment.
Next we show how to use Cascading Style Sheets to control how highlighting is done.
Using <B> tags to surround text that will be rendered by browsers is a reasonable default.
Fancier styling should be done with cascading style sheets (CSS) instead.
Note that this is a contrived example, since the content to be highlighted is a static string in the source code.
In our first example, only the best fragment was returned, but Highlighter shines in returning multiple fragments.
We highlight the best three fragments, separating them with an ellipsis (…)
Finally we write the highlighted HTML to a file, as shown in figure 8.17
In neither of our examples did we perform a search and highlight actual hits.
This brings up an important issue when dealing with the Highlighter: where to get the text to highlight in a real search application? This is addressed in the next section.
Whether to store the original field text in the index is up to you (see section 2.2.1 for field indexing options)
If the original text isn’t stored in the index (generally because of size considerations), you’ll have to retrieve the text to be highlighted from its original source.
Take care to ensure that the retrieved text is always identical to the text that had been indexed.
If the original text is stored with the field, it can be retrieved directly from the Document obtained from the search, as shown in the following piece of code:
Under the hood, this method first tries to retrieve the term vectors from the index.
Otherwise, the analyzer you pass in is used to re-analyze the text.
Whether to index term vectors or re-analyze the text is an application dependent decision: run your own tests to measure the difference in run time and index size for each approach.
In our case, since we did index the title field with term vectors in the books index, term vectors are used to create the token stream.
Next we show a useful means of cascading more than one filter together.
Using a search filter, as we’ve discussed in section 5.5, is a powerful mechanism for selectively narrowing the document space to be searched by a query.
The Sandbox contains an interesting meta-filter in the misc project, contributed by Kelvin Tan, which chains other filters together and performs AND, OR, XOR, and ANDNOT bit operations between them.
It’s slightly involved to demonstrate ChainedFilter because it requires a diverse enough dataset to showcase how the various scenarios work.
In addition to the test index, setUp defines an all-encompassing query and some filters for our examples.
The query searches for documents owned by either bob or sue; used without a filter, it will match all 500 documents.
An all-encompassing DateFilter is constructed, as well as two QueryFilters, one to filter on owner bob and the other for sue.
Using a single filter nested in a ChainedFilter has no effect beyond using the filter without ChainedFilter, as shown here with two of the filters:
The real power of ChainedFilter comes when we chain multiple filters together.
The default operation is OR, combining the filtered space as shown when filtering on bob or sue:
Rather than increase the document space, AND can be used to narrow the space:
The testAND test case shows that the dateFilter is AND’d with the bobFilter, effectively restricting the search space to documents owned by bob since the dateFilter is all encompassing.
In other words, the intersection of the provided filters is the document search space for the query.
Filter bit sets can be XOR’d (exclusively OR’d, meaning one or the other, but not both):
The dateFilter XOR’d with bobFilter effectively filters for owner sue in our test data.
And finally, the ANDNOT operation allows only documents that match the first filter but not the second filter to pass through:
In testANDNOT, given our test data, all documents in the date range except those owned by sue are available for searching, which narrows it down to only documents owned by bob.
Depending on your needs, the same effect can be obtained by combining query clauses into a BooleanQuery or using FilteredQuery (see section 6.4.1)
Keep in mind the performance caveats to using filters; and, if you’re reusing filters without changing the index, be sure you’re using a caching filter.
The Chandler project (http://chandlerproject.org) is an ongoing effort to build an open-source Personal Information Manager.
Chandler aims to manage diverse types of information such as email, instant messages, appointments, contacts, tasks, notes, web pages, blogs, bookmarks, photos, and much more.
As you suspected, search is a crucial component to the Chandler infrastructure.
The Chandler codebase uses Python primarily, with hooks to native code where necessary.
We’re going to jump right to how the Chandler developers use Lucene; refer to the Chandler site for more details on this fascinating project.
Andi Vajda, one of Chandler’s key developers, created PyLucene to enable full access to Lucene’s APIs from Python.
PyLucene is an interesting port of Lucene to Python; we cover it in full detail in section 9.6
Chandler’s underlying repository uses Oracle’s Berkeley DB in a vastly different way than a traditional relational database, inspired by RDF and associative databases.
Andi created a Lucene directory implementation that uses Berkeley DB as the underlying storage mechanism.
An interesting side-effect of having a Lucene index in a database is the transactional support it provides.
Andi donated his implementation to the Lucene project, and it’s maintained in the contrib/db/bdb area of the Sandbox.
Berkeley DB, at release 4.7.25 as of this writing, is written in C, but provides full Java API access via JNI.
Berkeley DB also has a “Java edition”, which is written entirely in Java, so no JNI access is required and the code exists in a single JAR file.
The example below shows how to use the Java edition version of Berkeley DB but the API for the original Berkeley DB is very similar.
We provide the corresponding examples for both indexing and searching with the source code that comes with this book.
JEDirectory, which is a Directory implementation that stores its files in the Berkeley DB Java Edition, is more involved to use than the built-in RAMDirectory and FSDirectory.
Once you have an instance of JEDirectory, using it with Lucene is no different than using the built-in Directory implementations.
In Section 2.10 we showed how to use RAMDirectory to load an index entirely in RAM.
This is especially convenient if you have a pre-built index living on disk and you’d like to slurp the whole thing into RAM for faster searching.
However, because RAMDirectory still treats all data from the index as files, there is actually significant overhead during searching for Lucene to decode this file structure for every query.
This is where two interesting Sandbox contributions come in: MemoryIndex and InstantiatedIndex.
MemoryIndex, contributed by Wolfgang Hoschek, is a very fast RAM-only index designed to test whether a single document matches a query.
It’s only able to index and search a single document.
You instantiate the MemoryIndex, and then use its addField method to add the document’s fields into it.
Then, use its search methods to search with an arbitrary Lucene query.
This method returns a float relevance score, where 0.0 means there was no match.
InstantiatedIndex, contributed by Karl Wettin, is similar, except it’s able to index and search multiple documents.
You first create an InstantiatedIndex, which is analogous to RAMDirectory in that it is the common store that a writer and reader share.
Under the hood, both of these contributions represents all aspects of a Lucene index using linked inmemory Java data structures, instead of separate index files like RAMDirectory.
This makes searching much faster than RAMDirectory, at the expense of more RAM consumption.
In many cases, especially if the index is small, the documents you’d like to search have very high turnover, short index to search delay is required, and you have plenty of RAM, one of these classes may be a perfect fit.
Spell correction is something users now take for granted in today’s search engines.
Enter a mis-spelled word into Google and you’ll get back a helpful “Did you mean…?” with your typo corrected as a hyperlink.
You can then conveniently click it to express “yes, indeed, I did!”
Google’s spell correction is so effective that you can rely on it to correct your typos!  Spelling correction is such a wonderfully simple and intuitive must-have feature to the end user.
But, as a developer, just how do you implement it?  Fortunately, Lucene has the spellchecker Sandbox contribution, created by David Spencer, for just this purpose.
Web search engines spend a lot of energy tuning their spell correction algorithms, and it shows.
Generally you get a very good experience, and indeed this sets the very high bar for how all the world’s search applications are expected to behave.
Let’s walk through the typical steps during spell correction, including generation of possible suggestions, selection of the best one for each mis-spelled word, presenting the choice to the user, and some other possible extensions to try.
Along the way we’ll see how the spellchecker sandbox package tackles each.
You might assume the very first step is to decide whether or not spell correction is even necessary.
But that’s hard to determine up front, and it’s usually more effective to simply always run through the steps and then use the score of each potential suggestion to decide whether they should be presented to the user.
The first step is to generate a raw set of possible suggestions.
The spellchecker module works with one term at a time, so if the query has multiple terms you’ll have to consider each, separately (though see “Further things to try” below for some ideas on handling multi-term queries)
You could try to find a generic known accurate dictionary, but it’s hard to find such dictionaries that will exactly match your search domain and it’s even harder to keep such a dictionary current over time.
A more powerful means of deriving a dictionary is to use your search index to gather all unique terms seen during indexing from a particular field.
You could use a phonetic approach, such as the “sounds like” matching we explored in section 4.5
Another approach, which is the one used by spellchecker, is to use letter ngrams to identify similar words.
A letter ngram is all sub-sequences of N letters in length, where N varies between a min and max size.
Using this approach, the ngrams for all words in the dictionary are indexed into a separate spellchecker index.
This is usually a fast operation, and so the application’s indexing process would rebuild the entire spell check index whenever the main index is updated.
Next, imagine the user searches for “letuce”, whose ngrams are shown in Table 8.21
To find the suggestions, the ngrams for “letuce” are used to run a search against the spellcheck index.
Since many of the ngrams are shared (“let”, “tuc”, “uce” and “tuce”) the correct word “lettuce” will be returned with a high relevance score.
Listing 8.9 shows how to do so, using the terms from an existing lucene index.
Create SpellChecker on its directory #2 Open IndexReader containing words to add to spell dictionary.
Add all words from the specified fields into the spell checker index.
From the first step, using the letter ngram approach, we can now generate a set of suggestions for each term in the user’s query.
Unfortunately, you don’t have the luxury of showing many spell suggestions to the user.
Typically you can either present no choice (if you determine all terms in the query seem properly spelled, or there were no good spelling candidates found), or a single suggestion, back to the user.
While the ngram approach is good for enumerating potential respellings, its relevance ranking is generally not good for selecting the best one.
Typically, a different distance metric is used to resort the suggestions, according to how similar each is to the original term.
One common metric is the Levenshtein metric, which we used in Section 3.4.7 to search for similar terms.
This is the default metric used by SpellChecker, and generally it works well.
You can also use the JaroWinkler distance (see http://en.wikipedia.org/wiki/Jaro-Winkler), which is provided in the spellchecker package, or you could implement your own string similarity metric.
The final step is to present the respelled option to the user.
Finally, once you have your single best respelling candidate, you first need to decide if it’s good enough for presenting to the user.
SpellChecker does not return the distance between each suggestion and the original user’s term, though you could simply recomputed that by calling the getDistance method on the StringDistance you are using.
SpellChecker also has an alternative suggestSimilar method that takes additional arguments in order to restrict the suggestions to those terms that are more frequent than the original term; this way you will only present a suggested respelling if it occurred more frequently than the original term, which is one good way to decide whether a candidate is worth presenting.
It also has a setAccuracy method to set the minimum relevance of each suggestion.
You could also run the original search, and then if it returns 0, or very few results, try the respelled search to see if it returns more, and use that to bias the decision.
Next, assuming you have a suggestion worth presenting, what exactly should your application do? One option, if you are very confident of the respelling, is to automatically respell the term.
But be sure to clearly tell the user at the top of the search results that this was done, and give the user a quick link to forcefully do the original search.
Alternatively, you could search exactly as the user requested but present a “Did you mean…” with the respelling.
Finally, you could search for both the original query plus the respelled query, or’d together perhaps with different boosts.
Typically a search application will choose up-front one of these options.
But modern Web search engines seem to make this choice dynamically, per query, depending on some measure of confidence of the respelling.
Go ahead and try some searches in http://google.com and http://search.live.com.
For example, on Google a search for the mis-spelled “Levenstein distance” will silently also search for the proper spelling “Levenshtein distance” plus the original mis-spelled term.
If instead you search for “hippopatomus” (misspelled), Google will faithfully search for your mis-spelled term, which has thousands.
Curiously, the same two searches on http://search.live.com have the opposite behavior.
Of course by the time you read this likely you’ll see different behavior.
We’ll end this important topic with some further ideas to explore.
Implementing spell correction is in fact very challenging, and we’ve touched on a few of the issues above.
But you may want to explore some of the possible improvements below in your application.
If you find success with one of these, or something else, please donate it back if possible:
If you have high search traffic, consider using the terms from your user’s queries to help rank the best suggestion.
In applications whose dictionary changes quickly with time, such as a news search engine for current events, this is especially compelling.
Instead of respelling each term separately, consider factoring in the other terms to bias the suggestions of each term.
One way is to compute term co-occurrence statistics up front for every pair of terms X and Y, to measure how many documents contain both terms X and Y.
Then, when sorting the suggestions take these statistics into account with the other terms in the user’s query.
If a user enters the misspelled query “harry poter” you’d really like to suggest “harry potter” instead of other choices like “harry spotter”
The dictionary you use for spell correction is very important.
When you use terms from an existing index, you can easily import mis-spellings from the content you had indexed if the content is “dirty”
You can also accidentally import terms that you may never want to suggest, for example SKU numbers or stock ticker symbols.
Try to filter such terms out, or only accepting terms that occurred above a certain frequency.
If you high search traffic, you can train your spellchecker according to how users click on the “Did you mean…” link, biasing future suggestions based on how users have accepted suggestions in the past.
If your search application has entitlements (restricting which content a user can see based on her entitlement) then take care to keep the spellchecker dictionary separate for different user classes.
A single global dictionary can accidentally “leak” information across entitlement classes which could cause real problems.
The spellcheck module currently relies entirely on the StringDistance score for this, but you could imagine improving this by combining StringDistance with the frequency of this tem in the index to get a better confidence.
The MoreLikeThis class captures all the logic for finding similar documents to an existing document.
In Section 5.7.1 we saw the BooksLikeThis example to accomplish the same functionality, but MoreLikeThis is more general and will work with any Lucene index.
Listing 8.10 shows how to do the same thing as BooksLikeThis using MoreLikeThis.
The approach is exactly the same: enumerate terms from the provided document and build up a Query to find similar documents.
MoreLikeThis is more flexible: if you give it a docID and an IndexReader instance, it will iterate through any field that is stored or has indexed term vectors, to locate the terms for that document.
For stored fields it must re-analyze the text, so be sure to set the analyzer first if the default StandardAnalyzer is not appropriate.
MoreLikeThis is able to find similar documents to an arbitrary String or the contents of a provided File or URL, as well.
Remember MoreLikeThis will usually return the exact same document back (if your search was based on a document in the index), so be sure to filter it out in your presentation.
Sets the string distance metric used to rank the suggestions #3 Generate respelled candidates.
It allows you to build a query by adding in arbitrary text, which is analyzed by default with StandardAnalyzer.
The tokens derived from that analysis are then “fuzzed” using the same process that FuzzyQuery uses.
Finally, from these terms the most differentiating terms are selected and searched on.
This query can be a useful alternative when end users are unfamiliar with the standard QueryParser boolean search syntax.
The BoostingQuery allows you to run a primary Query, but selectively demote search results matching a second Query.
All documents matching negativeQuery alone will not be included in the results.
All documents matching positiveQuery alone will be included with their original score.
Finally, all documents matching both will have their score demoted by the specified factor.
BoostingQuery is similar to creating a boolean query with the negativeQuery added as a NOT clause, except instead of excluding outright those documents matching negativeQuery, BoostingQuery includes those documents with a weaker score.
TermsFilter is a filter that matches any arbitrary set of terms you specify.
It’s like a RangeFilter that does not require the terms to be in a contiguous sequence.
You simply construct the TermsFilter, one by one add the terms you’d like to filter on by calling the addTerm method, and then use that filter when searching.
An example might be a collection of primary keys from a database query result or perhaps a choice of "category" labels picked by the end user.
TrieRangeQuery first appeared in Lucene 2.9, contributed by Uwe Schindler.
While this is precisely the same functionality as RangeQuery, TrieRangeQuery takes a drastically different approach: it indexes additional tokens derived from the number that represent successively larger ranges the number falls within.
This range aggregation at indexing time increases the size of the index somewhat, since additional tokens are present, but results in far faster range searches, especially when your numeric field is finegrained and you need to filter on large ranges.
In such situations RangeQuery is typically unacceptably slow, while TrieRangeQuery is exceptionally fast.
At search time, the requested range is translated into an equivalent union of these pre-indexed ranges, which typically results in far fewer terms that need to be searched.
Fortunately, all this magic happens under the hood and is exposed with a simple API.
When you run the tests, it produces this output (in addition to the tests passing):
As you can see, for this simple index, adding the trie fields almost doubled the size of the index.
But, fear not!  In a real index, that has additional text fields and a larger set of documents, the percentage increase in index size will be much smaller.
Normally the amazing gain in search performance more than makes up for this.
The first one is a normal Lucene index, which we use for running RangeQuery.
You can see that we had to zero-pad the numbers, using DecimalFormat, to ensure their lexicographic sort matches the numeric sort, as described in section 2.8
As you can see, we handle our numeric field like a tokenized text by supplying a special TokenStream (see [chapter about analyzers]) to the Field constructor to index our long value:
The precisionStep controls how many ranges should be created; a larger value makes fewer ranges, which reduces the size of the index but makes searching slower.
Just set the new numeric value using setValue(long value) and consume the stream again.
When you need to query a range at search time, simply create LongTrieRangeQuery with the same arguments that you’d pass to RangeQuery, except for a new integer argument (precisionStep) passed after the field name.
Just like precisionStep used during indexing, the larger this value is, the slower the searching will be as more ranges will need to be visited.
In general, any type that can be represented without loss of precision as an int or long can be handled in this way.
For example, if you need to index java.util.Date values, you can use the getTime() method of java.util.Date to translate it into a long.
However, because the tokens in the index are encoded, you must use a custom parser to decode them back to their original values.
Once you have the SortField, create a Sort instance from it (possibly involving other fields if you are performing a multi-field sort) and then run your search.
Under the hood, TrieUtils will parse the encoded values back to their original values for sorting.
If you are curious, set VERBOSE to true in the test to see just what terms are added to your document.
As you can see, the field values are unreadable!  Don’t worry, that’s expected.
All that really matters is that the Trie queries and SortField parsers know how to read them.
You’ll see similar values if you look at your index with Luke, for example.
Our next package, XML Query Parser, is another option for building your search user interface.
The standard Lucene QueryParser is ideal for creating the classic "onebox" search interface provided by web search engines, such as Google.
However, many search applications are more complex than this and require a custom search form to capture criteria with widgets such as:
All of the criteria from these HTML form elements must be brought together somehow to form a Lucene search request.
Figure 8.22: Three common options for building a Lucene query from a search user interface.
The standard QueryParser syntax can only be used to instantiate a limited range of Lucene's available queries and filters.
Option 2 embeds all of the query logic in Java code, where it can be hard to read or maintain.
There are many examples that show it’s desirable to avoid using Java code to assemble complex collections of objects.
Often a domain-specific text file provides a cleaner syntax and eases maintenance.
Further examples include Spring configuration files, XUL frameworks, Ant build files or Hibernate database mappings.
We’ll start with a brief example, then show a full example of how XmlQueryParser is used, and end with options for extending XmlQueryParser with new Query types.
Here’s a simple example XML query that combines a Lucene query and filter, enabling you to express a Lucene Query without any Java code:
XmlQueryParser parses such XML and produces a Query object for you, and the sandbox includes a full DTD to formally specify the out-of-the-box tags, as well as full HTML documentation, including examples, for all tags.
But how can you produce this XML from a Web search UI in the first place?  There are various approaches but one simple approach is to use the Extensible Stylesheet Language (XSL) to define query templates as text files that can be populated with user input at run-time.
This example is derived from the web demo available in the XmlQueryParser sandbox sources.
Figure 8.23 An example search user interface for a job search site.
Consider the Web-based form user interface shown in Figure 8.23
Let’s create a servlet that can handle this “job search” form.
The good news is this code should also work, unchanged, with your own choice of form.
Our method (not shown here) simply opens a standard IndexSearcher and caches this in our servlet's instance data.
This class will be used later to help construct queries.
Having initialized our servlet we now add code to handle search requests:
The XML document is then passed to the query parser to create a Query object for use in searching.
The remainder of the method is typical Servlet code used to package results and pass them on to a JSP for display.
Having set up our servlet we can now take a closer look at the custom query logic we need for our “job search” and how this is expressed in the “query.xsl” query template.
The XSL language in the query template allows us to perform the following operations:
Test for the presence of input values with “if” statements.
Loop around sections of content using “for each” statements We will not attempt to document all of the XSL language here but clearly the shortlist above lets us.
The XSL statements that control the construction of our query clauses can be differentiated from the query clauses because they are all prefixed with the <xsl: tag.
The above template conditionally outputs clauses depending on the presence of user input.
The logic behind each of the clauses is as follows:
Job Type: as a field with only two possible values (“permanent” or “contract”) this can be an expensive query clause to run because a search will typically match half of all the documents in our search index.
If our index is very large then this can involve reading millions of  document ids from the disk.
For this reason we use a cached filter for these search terms.
Any filter can be cached in memory for reuse simply by wrapping it in a <CachedFilter> tag.
Job description: being a  free-text field the standard Lucene query syntax is useful for allowing the user to express their criteria.
The contents of the <UserQuery> tag are passed to a standard Lucene QueryParser to interpret the user's search.
Job location: like the job type field, the job location field is a field with a limited choice of values which benefit from caching as a filter.
Unlike the job type field however, multiple choices of field value can be selected for a location.
We use a BooleanFilter to OR multiple Filter clauses together.
Job Salary: job salaries are handled as a RangeFilter clause.
The input field from the search form requires some manipulation in the XSL template before it can be used.
The salary range value arrives from our search form as a single string value such as “90-100”
Before we can construct a Lucene request we must split this into an Upper and Lower value and make sure both values are zero-padded to comply with Lucene's requirement for these to be lexicographically ordered.
Fortunately these operations can be performed using built-in XSL functions.
Adding support for new tags in the query syntax or changing the classes that support the existing tags is a relatively simple task.
As an example, we will add support for a new XML tag to simplify the creation of date-based filters.
For example, in our job search application we might want to add a filter using syntax like this:
Each tag in the XML syntax has an associated “Builder” class which is used to parse the content.
The Builders are registered simply by adding the object with the name of the tag it supports to the parser.
So in order to register a new builder for the above “Ago” tag we would need to include a line like the following in the initialization method of our servlet:
The AgoFilterBuilder class is a simple object which is used to parse any XML tags with the value “Ago”
For those familiar with the XML DOM interface the code presented below should be straight-forward.
Our AgoFilterBuilder is called by the XML parser every time an “Ago” tag is encountered and it is expected to return a Lucene Filter object given an XML DOM element.
The class DOMUtils simplifies the code involved in extracting parameters.
Our AgoFilterBuilder reads the “to”, “from” and “timeUnit” attributes using DOMUtils to provide default values if no attributes are specified.
Our code simplifies application logic for specifying “to” and “from” values by swapping the values if they are out of order.
An important consideration in coding Builder classes is that they should be thread-safe.
For this reason our class creates a SimpleDateFormat object for each request rather than holding a single object in instance data as SimpleDateFormat is not thread-safe.
Our Builder is relatively simple because the XML tag does not permit any child queries or filters to be nested inside it.
Next we look at an alternate QueryParser that can produce span queries.
As we saw in section 5.XXX, span queries offer some advanced possibilities for positional matching.
The Surround QueryParser defines an advanced textual language to describe span queries.
Let’s walk through an example to get a sense of the query language accepted by the Surround QueryParser.
Suppose a meteorologist wants to find documents on “temperature inversion”
In the documents this “inversion” can also be expressed as “negative gradient”, and each word can occur in various inflected forms.
But this will not match the following text, because there's nothing to match “gradient”:
This shows the power of spans: they allow word combinations in proximity (“negative gradient”) to be.
You’ll notice the Surround syntax is very different from Lucene’s built-in QueryParser.
The parentheses for the prefix form gave the name Surround to the language, as they surround the underlying Lucene spans.
The 3n operator is used in infix notation, meaning it’s written in-between the two subqueries.
If you replace “n” with “w” then an ordered SpanNearQuery is created.
Continuing the example, suppose the meteorologist wants to find documents that match the above “negative gradient” and two more concepts, "low pressure", and "rain"
In the documents these concepts can be also expressed in plural or verb form and by synonyms such as "depression" for "low pressure" and "precipitation" for "rain"
Also all three concepts should occur at most 50 words away from each other:
When the temperature has a negative height gradient above a depression no precipitation is expected.
But it will not match this text because the word "gradient" is in wrong place (further than 3 positions away), leading to improved precision in query results:
When the temperature has a negative height above a depression no precipitation gradient is expected.
When no proximity is used, the Surround QueryParser produces the same boolean and term queries as the built-in QueryParser.
In proximity subqueries, wildcards and "or" map to SpanOrQuery, and single terms map to SpanTermQuery.
Due to limitations of the Lucene spans package, the operators "and", "not" and "^" cannot be used in subqueries of the proximity operators.
It should be noted that the Lucene spans package is generally not as efficient as the phrase queries used by the standard query parser.
And the more complex the query, the higher its execution time.
Because of this, it is recommended to provide the user with the possibility to use filters.
Unlike the standard QueryParser, the Surround parser does not use an analyzer.
This means that the user will have to know precisely how terms are indexed.
For indexing texts to be queried by the Surround language it is recommended to use a lowercasing analyzer that removes only the most frequently occurring punctuations.
Using analyzers this way gives very good control over the query results, at the expense of having to use more wildcards during searching.
With the possibility of nested proximity queries, the need to know precisely what is indexed, the need to use parentheses, commas and wildcards, and the preference for additional use of filters, the Surround query language is not intended for the casual user.
However, for those users that are willing to spend more effort on their queries so they can save time by having higher precision results, this query language can be a good fit.
For a more complete description of the Surround query language, have a look at the README.txt file that comes with the source code.
To use Surround, make sure that the surround contrib package is on the CLASSPATH and follow the example java code to obtain a normal Lucene query:
And, of course, extensions to this query language are welcome.
Video search, medical search, image search, news, sports etc.: each of these is referred to as a vertical search.
One that stands out is local search: Local search is the use of specialized search techniques that allow users to submit geographically constrained searches against a structured database of local business listings13
Lucene now contains a sandbox package to enable local search, called spatial lucene.
If you need to find 'shoe stores' that exist within 10 miles of location X, then spatial lucene will do that.
While by no means a full GIS (geographical information system) solution, spatial lucene supports these functions:
Radial based searching, for example "show me only restaurants within 2 miles from a specified location"
Sorting by distance, so locations closer to a specified origin are sorted first.
Boosting by distance, so locations closer to a specified origin receive a larger boost.
The real challenge with spatial search is that for every query that arrives, a different origin is required.
Life would be very simple if the origin were fixed, as we could compute and store all distances in the index.
But because distance is a very dynamic value, changing with every query as the origin changes, spatial Lucene must take a far more dynamic approach that requires special logic during indexing as well as searching.
We’ll visit this logic here, as well as touch on some of the performance consideration implied by spatial Lucene’s approach.
Let’s first see how to index documents for spatial search.
In order to use spatial lucene, you must first geo-code locations in your documents.
This means a textual location, such as "77 Massachusetts Ave" or “the Louvre” must be translated into its corresponding latitude and longitude.
This process must be done outside of spatial lucene, which only operates on locations represented as latitudes and longitudes.
Now what does spatial lucene do with each location?  One simple way approach would be to load each document's location, compute its distance on the fly, and use that for filtering, sorting or boosting.
This approach will work, but it results in rather poor performance.
Instead, spatial Lucene implements interesting transformations during indexing, including both projection and hierarchical tries and grids, that allow for faster searching.
This is similar to having a light shine through a transparent globe and 'projected' onto a flat canvas.
By unfolding the globe into a flat surface the methods for selecting bounding boxes are much more uniform.
The first is the sinusoidal projection (http://en.wikipedia.org/wiki/Sinusoidal_projection), which keeps an even spacing of the projection.
However, it will cause a distortion of the image, giving it a "pinched" look.
The second projection is the Mercator projection (http://en.wikipedia.org/wiki/Mercator_projection), used because it gives a regular rectangular view of the globe.
However it does not correctly scale to certain areas of the planet.
If for example you look at a global projection of the Earth on Google maps, and compare it to the spherical projection in Google Earth, you will see Greenland in Google maps rectangular projection about the size of North America, whereas in Google Earth, it's about 1/3 the size.
Spatial lucene has a built-in implementation for the Sinusoidal projection, which we use in our example.
The next step is to map each location to a series of grid boxes.
Tiers divide the 2d grid into smaller and smaller square grid boxes, where each grid box is assigned a unique id; as each tier gets higher the grid boxes become finer.
This allows quick retrieval of locations stored at different levels of granularity.
For instance, imagine you have 1 million documents representing different parts of the US, and want every document that has a location in the West Coast of the US.
If you were storing just the raw document locations, you would have to iterate through every one of those million documents to if its location is inside your search radius.
Two term retrievals versus 1 million iterations is a major cost and time savings.
Listing 8.XXX shows how to index documents with spatial lucene.
String latField = "lat"; String lngField = "lon"; String tierPrefix = "_localTier";
The most important part is the loop that creates the tiers for each location to be indexed.
This provides a very rapid method for looking up values in an area, and finding its nearest neighbors.
Once you have your data indexed, you’ll need to retrieve it.
Below we will create a method to perform a normal text search that filters and sorts according to distance from a specific origin.
As the radius filter has performed the distance calculations already, pass in the filter to our DistanceSortSource to reuse the results.
For instance, to filter for all locations on the West Coast of the US, which is a fairly arbitrary request, a minimal bounding box could suffice in which case you would leave needPrecise as false.
If you need precisely filtered results, or you intend to sort by distance, you must specify true.
Note that the field name (“foo” in our example) is completely unused because DistanceSortSource provides the sorting information.
Let’s put the finishing touches on it, combining what we’ve done so far with some actual spatial data, seen in Listing 8.XXX.
We’ve added an addData method, to enroll a bunch of bars, clubs and restaurants into the index, plus main function that creates the index and then does a search for the nearest restaurant.
You can run this by entering “ant SpatialLucene” at the command prompt.
As our final topic, let’s look at the performance of spatial Lucene.
Unlike standard text search, which relies heavily on an inverted index where duplication in words actually reduces the size of an index and improves retrieval time, spatial locations have a tendency to be very unique.
The introduction of a Cartesian grid with tiers provides the ability to bucketize the locations into non-unique grids of different size, thus improving retrieval time.
However calculating distance still relies on visiting individual locations in the index.
Memory consumption can be high storing unique fields for both latitude and longitude.
There is a trade off in the additional processing overhead though for encoding and decoding the.
The more results you have the more distance calculations you will need to perform.
Distribution and multithreading helps here; the more concurrent work you can spread across threads and cpus, the quicker the response.
Caching really doesn't help here, although spatial Lucene does cache overlapping locations, as usually the center location of your search can change more frequently than your search term.
Thus the key is spread the load as evenly as possible.
Do not index all of your data by regions as you will find an uneven distribution of load.
Cities will generally have more data than suburbs, thus taking more processing time.
Furthermore, more people will search for results in cities versus suburbs.
Table 8.XXX shows the results: Number of results is the number of documents that are returned by the query; Time to find the results is the amount of time for the boundary box calculation without the precise distance calculation; Time to calculate distance is the additional time required to get the precise result.
See http://en.wikipedia.org/wiki/Geohash for a good description of what a geohash is.
Number of Results Time to find results Time to filter by distance.
It's clear from the above table that large sets of spatial data can be retrieved from the index rapidly:
However, a significant amount of time is consumed calculating all the precise result distances to filter out any that might exist outside of the radius and to enable sorting.
We’ll now finish up this chapter by showing you to build the sandbox packages.
Many of the packages from the sandbox repository are included in the standard Lucene releases, under the contrib directory.
Each package generally has its own jar files for the classes and the javadocs.
Still, some packages are not part of the build and release process.
Further, there may be recent improvements not yet released that you’d like to use.
To handle these cases you’ll need to access the source code and build the packages yourself.
Fortunately, this is straightforward!  You can easily obtain Lucene’s source code directly via Apache’s SVN access and either build the JAR files and incorporate the binaries into your project or copy the desired source code into your project and build it directly into your own binaries.
Using a Subversion client (see http://subversion.tigris.org), follow the instructions provided at the Apache site: http://wiki.apache.org/lucene-java/SourceRepository.
Specifically, this involves executing the following command from the command line:
In your current directory, you’ll now have a subdirectory named lucene-trunk.
Under that directory is a contrib directory where all the goodies discussed here, and more, reside.
You’ll need Ant 1.6.x or later in order to run the Sandbox build files.
At the root of the lucene-trunk directory is a build.xml file.
From the command line, with the current directory lucene-trunk, execute:
Most of the components will build and create a distributable JAR file in the build subdirectory.
Now is also a good time to execute ant test, which runs all core and contrib unit tests, to confirm all of Lucene’s tests are passing.
Some components, such as javascript, aren’t currently integrated into this build process, so you need to copy the necessary files into your project.
Some outdated contributions are still there as well (these are the ones we didn’t mention in this chapter), and additional contributions will probably arrive after we’ve written this.
Each contrib subdirectory, such as analyzers and ant, has its own build.xml file.
To build a single component, set your current working directory to the desired component’s directory and execute ant.
This is still a fairly crude way of getting your hands on these add-ons to Lucene, but it’s useful to have direct access to the source.
You may want to use the Sandbox for ideas and inspiration, not necessarily for the exact code.
Someone has no doubt encountered the same situation you’re struggling with.
The Sandbox and the other resources listed on the Lucene web site should be your first stops.
One widely used package is highlighter, enabling you to extract summaries for each hit, and highlight the terms that matched the user’s query.
This functionality is incredibly important and difficult to implement yourself!
Another very important package is spellchecker, for detecting mis-spelled words, generating possible suggestions, and sorting the suggestions to select the best one.
Several useful tools (Limo, lucli and Luke) let you peek into and index and see all sorts of details.
Spatial lucene is a delightful package allowing you to add geographic distance filters and sorting to your search application.
Rounding out our coverage are a number of interesting packages.
WordNet’s synonyms can be easily incorporated into your indexing process.
XmlQueryParser aims to simplify creation of a rich search userinterface.
The surround QueryParser enables a rich query language for span queries.
The Javascript package gives browser control over query construction validation.
Fast in-memory indices can be created using either MemoryIndex or InstantiatedIndex.
We saw a number of interesting new Query implementations, including a generic MoreLikeThis class for finding documents similar to a provided original.
A great many analyzers offer support for many languages, shingles and ngrams.
We saw a nice ant integration, enabling you to build a Lucene index as part of your build process.
You can easily store your index in a Berkeley DB (BDB) directory, giving you all the features of BDB such as full transactions.
If you end up rolling up your sleeves and creating something new and generally useful, please consider donating it to the Sandbox or making it available to the Lucene community.
We’re all more than grateful for Doug Cutting’s generosity for open-sourcing Lucene itself.
By also contributing, you benefit from a large number of skilled developers who can help review, debug, and maintain it; and, most important, you can rest easy knowing you have made the world a better place!
Today, Lucene is the de facto standard open-source IR library.
Although Java is certainly a very popular programming language, not everyone uses it.
What do you do if you love Lucene but not Java?  Fear not: you are in good company!  Luckily, a number of options are available for accessing Lucene functionality from different languages.
In this chapter we discuss different options for accessing Lucene's functionality from programming languages other than Java.
We’ll provide brief examples of the ports’ use, but keep in mind that each port is an independent project with its own mailing lists, documentation, tutorials, user, and developer community that will be able to provide more detailed information.
We begin with our rather loose interpretation of what a port is.
Possibly higher chance of bugs Likely to be less compatible with Lucene java.
A JVM is embedded into the native language’s runtime, and a wrapper is used to expose Lucene’s API.
Port is fast, so lower release delay, since only Lucene’s APIs need to be exposed 100% compatibility with Lucene.
Heavier, since two runtime environments are running side by side.
Client/Server A separate process, perhaps on a separate machine, runs Lucene Java and exposes a standard protocol for access.
Clients are very fast to build Solr provides functionality beyond Lucene and is very actively developed 100% compatibility with Lucene.
Much heavier weight since you now have a whole server to manage.
A native port translates all of Lucene's sources into the target runtime environment.
Lucene.Net, which rewrites all of Lucene in C#, is a good example.
Another example is KinoSearch, which provides Lucene-like functionality with a C core and Perl bindings.
Since C or C++ is the accepted extensions language for many dynamic languages, such as Perl and Python, we count this as a native port.
A reverse native port is the mirror image of a native port: the target runtime environment has been ported to run on a JVM.
You write programs in your target language, such as Ruby, but the environment that runs your programs runs on a JVM and therefore has full access to any Java APIs, including Lucene.
The local wrapper approach runs a JVM under the hood, side by side with the “normal” runtime for the target language, and then only the APIs that need exposing are wrapped to the target environment.
Finally, in the client/server approach, Lucene is running in a separate process, perhaps on another computer, and is accessible using a standard network-based protocol.
The server could be just the JVM, as is the case with the PHP Bridge, or it may be a full server like Solr, which implements an XML over HTTP API for accessing Lucene and provides additional functionality beyond Lucene such as distributed search and faceted navigation.
Clients are then developed, in multiple programming languages, to interact with the server over the network, using the target language.
There are numerous differences between these approaches, which we visit next.
Each approach has important tradeoffs, also summarized in Table 9.1
The native port has the advantage of running only code for the target environment, within a single process.
It’s perhaps the cleanest, technically, and most lightweight approach, because a single runtime environment is running all code.
But the downside is the cost of maintaining this port as Lucene’s sources improve with time, which means longer release delay, higher chances that the port will differ from Lucene in API and index file format and a higher risk that the project will be abandoned, as the efforts to continuously port source changes are significant.
The native port is also likely to have substantially different performance characteristics, depending on whether the target environment is faster or slower than the JVM.
The reverse native port is a compelling option, assuming the runtime environment itself does not have problems running the target language.
By using JRuby, you write ruby code that has access to any Java code, but will generally lose access to Ruby extensions that are implemented in C.
This option is also lightweight at runtime, since it runs in a single process and with a single (JVM) runtime environment.
The wrapper approach is similarly a single process, but it embeds a JVM (to run the compiled Java bytecode from Lucene) as well as running the target environment’s runtime, side by side, so it's somewhat heavier weight.
The important tradeoff is that much less work is required to stay current with Lucene’s releases: only the API changes need to be ported, and not Lucene’s entire implementation, so the work is in proportion to the net API “surface area” and the release delay can be much less.
With PyLucene in particular, which auto-generates the wrapper code using JCC, the delay is essentially zero because the computer does all the work!  If only other wrappers could use JCC.
Because a separate server runs and exposes Lucene’s APIs via a standard network protocol, you can now share this server amongst multiple clients, possibly with different programming languages.
But one potential downside is you now must manage a new standalone process or server, entirely different from your main application.
Having so many different approaches for ports seem daunting at first, but in reality this gives a lot of flexibility to people who create the ports, which in turn gives you more options to choose from.
If your application is already server-centric, and you’re in love with PHP, then the client/server model (Solr as server and SolrPHP client) is a nobrainer.
In fact, server based applications often require a client/server search architecture so that multiple frontend computers can share access to the search server(s)
At the other end of the spectrum, if you’re coding up a C++ desktop application and you can’t afford a separate server let alone a separate process, choose a native port like CLucene.
Often it’s one person driving the port, and if they lose interest or can’t afford the ongoing time, the port slowly dies.
New ports, with new approaches, may surface and attract more interest.
While we do our best to describe all of Lucene’s ports, today, very likely by the time you read this there will be other compelling options.
Be sure to do your due diligence, by searching the Web, an asking questions on the user’s lists, etc, before making your final decision.
Although each port tries to remain in sync with the latest Lucene version, they all necessarily lag behind Lucene’s releases.
Furthermore, most of the ports are relatively young, and from what we could gather, there are little developer community overlaps.
Each port takes some and omits some of the concepts from Lucene, but they all mimic its architecture.
Each port has its own web site, mailing lists, and everything else that typically goes along with open-source projects.
Each port also has its own group of founders and developers.
There is also little communication between the ports’ developers and Lucene’s developers, although we’re all aware of each project’s existence.
With this said, let’s look at each port, starting with Solr.
Solr, which is covered in detail in Chapter XXX, is a client/server architecture exposing access from many programming languages.
In a nutshell, Solr is a server wrapper around Lucene.
It provides a standard XML over HTTP interface for interacting with Lucene’s APIs, and also layers on further functionality not normally available in Lucene, such as distributed search, faceted navigation and a field schema.
Because Solr “translates” Lucene’s Java-only API into a very friendly network protocol, it’s very easy to.
For this reason, of all approaches for accessing Lucene from other languages, Solr offers the least porting effort.
Solr has a delightful diversity of clients, shown in Table 9.3
If you need to access Lucene from an exotic language, chances are there is already at least one Solr client.
And if there isn’t, it’s very easy to create one!  Solr is very actively developed and of course has excellent compatibility with Lucene since it uses Lucene under-the-hood.
If your application can accept the addition of a standalone server, Solr is likely a very good fit.
Although his studies aren’t in a technology-related field, he has strong interest in Information Retrieval.
While Ben was the original creator, many other active developers now participate in this port.
Ben was kind enough to provide this overview of CLucene.
CLucene is a native port to C++: the file format and API are exactly the same as Lucene 1.9, in its latest stable release.
The unstable release is actively working towards compatibility with Lucene’s 2.3.1 release, and development is proceeding on a source code branch towards making this a stable release.
Despite being officially marked unstable, the 2.3.1 branch seems to be quite stable and is commonly used, although the APIs are still changing as of this writing.
The distribution package of CLucene includes many of the same components as Lucene, such as tests and demo examples.
CLucene contains wrappers that allow it to be used with other programming languages.
Currently there are wrappers for PHP, .NET (read-only), and a Dynamic Link Library (DLL) that can be shared between different programs, and separately developed wrappers for Python and Perl.
CLucene was initially developed in Microsoft Visual Studio, but now it also compiles in GCC, MinGW32, and (reportedly) the Borland C++ compiler (although no build scripts are currently being distributed)
The CLucene team is making use of SourceForge’s multiplatform compile farm to ensure that CLucene compiles and runs on as many platforms as possible.
The activity on the CLucene developers’ mailing lists indicates that support for AMD64 architecture and FreeBSD is being added.
This means that code written in Java can be converted to C++ fairly easily.
The drawback is that CLucene doesn’t follow the generally accepted C++ coding standards.
However, due to the number of classes that would have to redesigned, CLucene continues to follow a “Javaesque” coding standard.
This approach also allows much of the code to be converted using macros and scripts.
The CLucene wrappers for other languages, which are included in the distribution, all have different APIs.
Listing 9.1 shows a command-line program that illustrates the indexing and searching API and its use.
This program first indexes several documents with a single contents field.
Following that, it runs a few searches against the generated index and prints out the search results for each query.
Many applications have to deal with characters outside the ASCII range.
CLucene was originally written to be as fast and lightweight as possible.
In the interest of speed, the decision was made not to incorporate any external libraries for string handling and reference counting.
Linux suffers from a lack of good Unicode support, and since CLucene doesn’t use external libraries, Linux builds had to be built without Unicode.
This led to CLucene using the _UNICODE pre-processor directive: When it’s specified, the Unicode characters are used; otherwise, non-Unicode (narrow) characters are used.
However, support for Unicode is included in CLucene and can be enabled at compile-time.
Future version may also solve this problem by optionally including a Unicode library.
According to a couple of reports captured in the archives of the Lucene Developers mailing list, CLucene indexes documents faster than Lucene.
We haven’t done any benchmarks ourselves because doing so would require going back to version 1.2 of Lucene (not something a new Lucene user would do)
Although the CLucene port has been around for a while and has an active user mailing list, we haven’t been able to locate many actual CLucene users to list here.
This could be due to the fact that the CLucene development team is.
The distribution package of Lucene.Net consists of the same components as the distribution package of Lucene.
It includes the source code, tests, and a few demo examples.
Although it’s written in C#, Lucene.Net exposes an API that is nearly identical to that of Lucene.
Consequently, code written for Lucene can be ported to C# with minimal effort.
This compatibility also allows .NET developers to use documentation for the Java version, such as this book.
The difference is limited to the Java and C# naming styles.
Whereas Java’s method names begin with lowercase letters, the .NET version uses the C# naming style in which method names typically begin with uppercase letters.
That is to say, an index created by Lucene can be read by Lucene.Net and vice versa.
Of course, as Lucene evolves, indexes between versions of Lucene itself may not be portable, so this compatibility is currently limited to Lucene version 1.4
The developers of Lucene.Net don’t have any performance numbers at this time, and they’re focused on adding features to their port to ensure it stays as close to Lucene as possible.
However, it would be safe to assume that Lucene.Net’s performance is similar to that of its precursor; according to Lucene.Net’s author, its performance was comparable to that of Lucene.
One interesting user of Lucene.Net is Beagle (http://beagle-project.org/Main_Page), a search tool for searching your personal information space, including local files, email, images, calendar entries, addressbook entries, etc.
Its design is just like Solr: there is a dedicated daemon process that exposes a network API, and then clients are available in various programming languages (currently at least C#, C and Python)
Beagle seems to be well adopted by Linux desktop environments as their standard local search implementation, whereby Beagle runs under Mono.
Larry Wall has stated one of his goals in Perl is to offer many ways to accomplish a given task.
Larry would be proud, as there are quite a few choices for accessing Lucene’s functionality from Perl.
After that we touch on Lucy, which is still under active development and hasn’t had any releases yet but is nevertheless interesting.
KinoSearch, created and actively maintained by Marvin Humphrey, is a C and Perl “loose port” of Lucene.
This means its approach, at a high level, is similar to Lucene, but the architecture, APIs and index file format are not identical.
The summary of its current status is shown in Table 9.6
Marvin took the time to introduce interesting innovations to KinoSearch while porting to Perl and C; some of these innovations have inspired corresponding improvements back to Lucene, which is one of the delightful and natural “cross fertilization” effects of open source development.
Web site http://www.rectangular.com/kinosearch/ Development Status Alpha (though widely used and quite stable)
Last stable release 0.163 Matching Lucene release N/A (loose port)
KinoSearch is technically in the alpha stage of its development, but in practice is nevertheless extremely.
Development and users lists are active, and developers (mostly Marvin) are working toward the 1.0 first stable release.
It’s hard to gauge usage, but at least two wellknown web sites, Slashdot.org and Eventful.com, use it.
When users find issues and post questions to the mailing lists, Marvin is always very responsive.
KinoSearch also learned important lessons from an earlier port of Lucene to Perl, PLucene.
PLucene, which has stopped development, suffered from performance problems, likely because it was implemented entirely in Perl; KinoSearch instead wraps Perl bindings around a C core.
This allows the C core to do all the “heavy lifting”, which results in much better performance.
Early testing of KinoSearch showed its indexing performance to be close to Lucene’s 1.9.1 release.
However both KinoSearch and Lucene have changed quite a bit since then, so it’s not clear how they compare today.
Probably the largest architectural difference is that KinoSearch requires you to specify field definitions up front when you first create the index (similarly to how you create a database table)
The fields in documents then must match this pre-set schema.
This allows KinoSearch to make internal simplifications, which gain performance, but at the cost of full document flexibility that is available in Lucene.
For example, there is only one class, InvIndexer, for making changes to an index (whereas Lucene has two classes for doing so, IndexWriter and, somewhat confusingly, IndexReader)
Next we look at Lucy, something of a followon to KinoSearch.
It plans to be a loose native port of Lucene to C, with a design that makes it simple to wrap the C code with APIs in different dynamic languages, with the initial focus on Perl and Ruby.
Lucy was started by the creator of KinoSearch, Marvin Humphrey, and the creator of Ferret (see section XXX below), David Balmain.
Unfortunately, David became unavailable and Marvin has now folded Lucy’s approach back into KinoSearch main development repository.
It may very well be that Lucy is realized as the 2.0 release of KinoSearch.
Like Ferret and KinoSearch, Lucy is inspired by Lucene and derives much of its design from those two projects, aiming to achieve the best of both.
Eventually other programming languages should be able to wrap Lucy’s C core.
There are other ways to access Lucene’s functionality from Perl.
There are at least 2 clients for Solr: Solr.pm (available at http://search.cpan.org/perldoc?Solr and separately developed from the Solr effort), and SolPerl which is developed and distributed with Solr.
If you have a strong preference for API and index compatible ports, and don’t like that KinoSearch is a “loose” port, have a look at CLucene’s Perl bindings, which is also a native port of Lucene but with matching APIs and index file formats.
Let’s move next to another popular dynamic language starting with the letter P.
This section was gratefully contributed by the creator of PyLucene, Andi Vajda.
PyLucene takes the “local wrapper” approach, by adding Python bindings to the actual Lucene source code.
The PyLucene Python extension, a Python module called “lucene”, is machine-generated by a package called JCC, also included with the PyLucene sources.
Once it knows that API, it generates the appropriate C++ code that enables access to that API from Python through JNI (Java’s Native Interface), using C++ as the common “bridge” language.
Because JCC auto-generates all wrappers by inspecting Lucene’s JAR file, the release latency is near zero.
It was recently (Jan 2009) folded into Apache as a sub-project of Lucene.
Hence all public APIs in all public classes available from Lucene are available from PyLucene.
Warning: once you’ve used Lucene from Python, it can be very hard to go back to using Java!
As far as its structure is concerned, the API is virtually the same, which makes it easy for users of Lucene to learn how to use PyLucene.
Another convenient side effect is that all existing Lucene documentation can be used for programming with PyLucene.
The latest and greatest from Lucene is usually available via PyLucene a few days after a release.
The performance of PyLucene should be very similar to that of Lucene since the actual Lucene code is running in an embedded Java VM in-process.
The Python/Java barrier is crossed via the Java Native Interface (JNI) and is reasonably fast.
Virtually all of the source code generated by JCC for PyLucene is C++
That code uses the Python VM for exposing Lucene objects to the Python interpreter but none of the PyLucene code itself is interpreted Python.
It has had a number of users over the years.
Some Linux distributions, such as Debian, are now beginning to distribute PyLucene and JCC.
Currently, the PyLucene developer/user mailing list has about 160 members.
Lucene issues while using PyLucene are usually handled on the Lucene user mailing list.
While PyLucene is our favorite option for using Lucene from Python, there are other choices with different tradeoffs:
If you prefer a native port, CLucene offers Python bindings.
Beagle, described briefly in section 9.4.4, also includes Python bindings.
As you’ve seen, there are a number of ways to access Lucene from Python, the most popular being PyLucene.
Fortunately, you can access Lucene from Ruby in various ways.
The most popular port is Ferret, summarized in Table 9.9
Activity Development stopped but active users Last stable release 0.11.6
Although independently developed, Ferret takes the same approach as KinoSearch, as a loose port of Lucene to C and Ruby.
The C core does the heavy lifting, while the Ruby API exposes access to that core.
Ferret was created by David Balmain, who has written a dedicated book about Ferret.
There is also an acts_as_ferret plugin for Ruby on Rails.
User reports have shown Ferret’s performance to be quite good, comparable at least to Lucene’s 1.9 release.
Even though development appears to have ended, usage of Ferret still appears to be strong, especially for acts_as_ferret, although there are reports of still open serious issues on the most recent release, so you should tread carefully.
SolRuby is Solr’s Ruby client, allowing you to add, update and delete documents, as well as issue queries.
Solr also provides a modified JSON response format that produces valid Ruby source code as the string response, which can be directly eval’d in Ruby even without the SolRuby client.
Finally, Erik has developed Solr Flare, which is a feature rich Rails plugin that provides even more functionality than acts_as_solr.
There is even a Common Lisp port called Montezuma, at http://code.google.com/p/montezuma.
Development seems to have stopped, after an initial burst of activity.
In fact, Montezuma is a port of Ferret, which in turn is a port of Lucene to Ruby (described in section 9.7)
Finally, another compelling option is to use JRuby, which is a reverse port of the Ruby language to run on a JVM.
The one downside to JRuby is that it cannot run a Ruby extension that’s implemented in C.
Let’s switch gears now to another popular Web application dynamic language, PHP.
There are several interesting options if you’d like to use PHP.
The first option is to use Solr with its PHP client, SolPHP, which is a client/server solution.
As is the case for Ruby, Solr has a response format that produces valid PHP code, which can simply be eval’d in PHP.
The second option is CLucene’s PHP bindings, which are included with CLucene’s release, which is a pure native port.
It includes a pure native port of Lucene to PHP 5, described at http://framework.zend.com/manual/en/zend.search.lucene.html, enabling you to easily add full search to your web application.
There are some reports of slow performance during indexing, though this may have been resolved by more recent releases so you should certainly test for yourself.
Earlier releases did not support Unicode content, but this has since been fixed.
Zend Framework may be a good fit for your application if you want a pure PHP solution, but if you don’t require a native port and you’d like a lighter weight solution instead, then PHP Bridge may be a good option.
The PHP/Java Bridge, hosted at http://php-java-bridge.sourceforge.net/pjb/index.php, is technically a client/server solution.
Normal Java Lucene runs in a standalone process, possibly on a different computer, and then the PHP runtime can invoke methods on Java classes through the PHP Bridge.
It can also bridge to a running .NET process, so you could also use PHP to access Lucene.Net, for example.
The release WAR that you download from the above site even includes examples of indexing and searching with Lucene.
Since this is just a client/server wrapper around Lucene, you can tap directly into the latest release of Lucene.
Performance should be close to Lucene’s performance, except for the overhead of invoking methods over the bridge.
In this chapter, we discussed the four different approaches that Lucene’s ports use, and we visited all existing Lucene ports known to us: CLucene, Lucene.Net, Pylucene, Solr and its many clients, KinoSearch, Ferret, the upcoming Lucy, and numerous PHP options.
We looked at their APIs, supported features, Lucene compatibility, development and user activity and performance as compared to Lucene, as well as some of the users of each port.
The future may bring additional Lucene ports; the Lucene developers keep a list on the Lucene Wiki at http://wiki.apache.org/lucene-java/LuceneImplementations.
As you can see, there are a great many ways to access Lucene from environments other than Java, each with their own tradeoffs.
While this may seem daunting, if you are trying to decide which of these to use, it’s actually a great sign of Lucene’s popularity and maturity that so many people have created all these different options.
By covering the Lucene ports, we have stepped outside the boundaries of core Lucene.
In the next chapter we’ll go even further by examining several interesting Lucene case studies.
You've seen diverse examples of how to use Lucene for indexing and searching, including many advanced use cases.
Here we change gears and cover practical, hands-on administrative aspects of Lucene.
Some say administrative details are a mundane and necessary evil, but at least one of your beloved authors would beg to differ!  A well tuned Lucene application is like a well maintained car: it will operate for years without problems, requiring only a small, informed investment on your part.
You can take pride in that!  This chapter gives you all the tools you need to keep your Lucene application in tip top shape.
Lucene has great out-of-the-box performance, but for some demanding applications, this is still not good enough.
Fear not!  There are many fun ways to tune for performance.
Adding threads to your application is often very effective, but the added complexity can make it tricky.
We'll show you some simple drop-in classes that hide this complexity.
Most likely you can tune Lucene to get the performance you need.
Beyond performance, people are often baffled Lucene's consumption of resources like disk space, file descriptors and memory.
Keeping tabs on this consumption, over time, as your index grows and application evolves, is necessary to prevent sudden catastrophic problems.
Fortunately, Lucene's use of these resources is simple to predict once you understand how.
Of course, what good is great performance if you have no more search index?  Despite all your preventative efforts, things will eventually go very wrong (thank you Murphy's Law), and restoring from backup will be your only option.
As of 2.3, properly backing up your index, even while you are still adding documents to it, is simple.
You have no excuse to delay!  Just a little bit of planning ahead will save you a lot of trouble later.
So, roll your sleeves up: it's time to get your hands dirty!  Let's jump right in with performance tuning.
Many applications achieve awesome performance with Lucene, out of the box.
But, you may find that as your index grows larger, and as you add new features to your application, or even as your web site gains popularity and must handle higher and higher traffic, performance could eventually become an issue.
Before jumping into specific metrics, there are some best practices that you should always follow regardless of what specific tuning you want to do:
Run a Java profiler, or collect your own rough timing using System.nanoTime, to verify your performance problem is in fact Lucene and not your application.
For many applications, loading the document from a database or file system, filtering the raw document into plain text, and tokenizing that text, is time consuming.
During searching, rendering the results from Lucene might be time consuming.
Run your JVM with the -server switch, which generally configures the JVM for faster net throughput over time but a possibly higher startup cost.
Lucene is always getting better: performance is improved, bugs are fixed, and new features are added.
In 2.3 in particular there were numerous optimizations to indexing.
The Lucene development community has a clear commitment to backwards compatibility: it is strictly kept across minor releases (2.x) but not necessarily across major releases.
A new minor release should just be a drop-in, so go ahead and try it!
Local file systems are generally quite a bit faster than remote ones.
If you are concerned about local hard drives crashing, use a RAID array with redundancy.
In any event, be sure to make backups of your index (see 10.4): someday, something will inevitably go horribly wrong.
Share a single instance for a long time and re-open only when necessary.
Modern computers have amazing concurrency in CPU, IO and RAM, and that concurrency is only increasing with time.
Be sure Lucene is not using so much memory that your computer is forced to constantly swap or the JVM is forced to constantly GC.
Budget enough memory, CPU and file descriptors for your peak usage.
This is typically when you are opening a new searcher during peak traffic perhaps while indexing a batch of documents.
These best practices will take you a long ways towards better performance.
It could be, after following these steps, you are done: congratulations!  If not, don't fear: there are still many options to try.
For each of these we show how to tune Lucene.
But first, be sure your application really does need faster performance from Lucene.
Performance tuning can be a time consuming and, frankly, rather addictive affair.
It can also add complexity to your application, which may introduce bugs, making your application more costly to maintain.
Ask yourself, honestly (use a mirror, if necessary): could your time be better spent improving the user interface or tuning relevance?  You can always improve performance by simply rolling out more or faster hardware, so always consider that option first.
Never sacrifice user experience in exchange for performance: keeping users happy, by providing the best experience humanly and computerly possible, should always be your top priority.
These are the costs of performance tuning so before you even start make sure you do need really better performance.
So you still have your heart set on tuning performance?  No problem: read on!
First, set up a simple repeatable test that allows you to measure the specific metrics you want to improve.
Without this you can't know if you're actually improving things.
Try to use true documents and searches from your search logs, if available.
If you see high variance on each run you may want to run the test 3 or more times and discard the outliers (keeping the middle result)
Finally, take an open minded iterative approach: performance tuning is empirical and often.
Let the computer tell you what works and what doesn't.
Make one change at a time, test it, and keep it only if the metric really improved.
Some changes will unexpectedly degrade performance, so don't keep those ones!  Make a list of ideas to try, and sort them according to your best estimate of "bang for the buck": those changes that are quick to test and could be the biggest win should be tested first.
Once you've improved your metric enough, stop and move onto other important things!  You can always come back to your list later and keep iterating.
More than likely someone has already encountered and solved something similar to your problem and your question can lead to healthy discussion on how Lucene could be improved.
For our testing throughout this chapter we will use the framework in contrib/benchmark, described in more detail in Appendix D.
This is an excellent tool for creating and running repeatable performance tests.
It already has support for multiple runs of each test, changing Lucene configuration parameters, measuring metrics, and printing summary reports of the full test run.
There are a large set of built-in tasks and document sources to choose from.
You simply write algorithm (.alg) file, using a simple custom scripting language, to describe the test.
That prints great details about the metrics for each step of your test.
Algorithm files also make it simple for others to reproduce your test results: you just send it to them and they run it!  Let's look at specific metrics that you may need to tune.
First, because Lucene periodically merges segments, when you run two indexing tests with different settings it's quite possible for each resulting index to end in a different merge state.
It's not really fair to compare metrics from these two tests because in the first case Lucene did more work to make the index more compact.
To work around this, you could set mergeFactor to an enormous number, to turn off merging entirely.
This will make the tests at least comparable, but just remember that the resulting numbers are not accurate in an absolute sense, because in a real application you cannot turn off merging.
Of course,  this is only meaningful if you are not trying to compare the cost of merging in the first place.
The second issue is to make sure your tests include the time it takes to call close on the IndexWriter.
During close, IndexWriter flushes documents, may start new merges, and waits for any background merges to finish.
Try to write your algorithm files so that the CloseIndex task is included in the report.
It's important to understand which metric you need to improve, and which are less important, because optimizing one metric is often at the expense of others.
Index-to-search delay: the time from when a document is added to the index until your users can actually see it in their search results.
You want this number to be no more than 1 second.
There are many cumulative delays in a Web search application, so be sure to measure all steps before and after the Lucene search is actually executed to be sure it's really Lucene that needs tuning.
Search throughput: how many searches per second can your application can handle.
Which metric is important depends on your application, and can vary with time.
Index-to-search delay is the elapsed time from when you add or update a document in the index until users can actually search that document.
Because a reader always presents the index as of the "point in time" when it was opened, the only way to reduce index-to-search delay is to close your writer and reopen your reader.
Unfortunately, these operations are fundamentally costly: they consume IO, CPU, and memory.
The larger your index, and the more fields accessed through the FieldCache API or used for sorting, the more costly reopening the reader is.
As a result, frequently reopening your reader necessarily degrades other metrics like indexing and search throughput.
On the bright side, many applications only require high indexing throughput while creating the initial index or doing bulk updates.
During this time, the index-to-search latency does not matter because no searching is happening.
But then once the index is built and in-use in the application, the rate of document turnover is often low, while index-to-search latency becomes important.
While this makes reopen faster, the creation of the IndexSearcher is still a very costly operation.
Here are the steps to follow to reopen a reader:
While not strictly necessary, it's best to do this after closing your writer, or at a time when you're certain the writer is idle (not performing segment merges, eg during optimize)
Otherwise, opening a reader during this time will hold open segment files for segments that otherwise may have been deleted, thus consuming unexpectedly more disk space.
While this is happening, keep your old searcher alive to answer incoming searches.
Once the new searcher is ready, direct new searches to it, but follow-on searches (e.g., another page of results for a previously run search) back to the original searcher.
This is to ensure the user does not see results suddenly shift while paging through search results.
However, keeping readers open consumes more resources (file descriptors, RAM, disk space)
Once all search sessions have completed, or a preset timeout periods has passed, on the old searcher, close it.
When your have multiple threads doing searches, this re-opening sequence is tricky since you cannot close the old searcher until all threads are done.
See 10.2.2 for a useful utility class to handle this tracking for you.
Measure the cost of re-opening a new searcher and use this to determine how frequently your application should do so.
Figure 10.1 Steps to test indexing throughput on Wikipedia articles.
Indexing throughput measures how many documents per second you are able to add to your index, which determines how much time it will take to build and update your index.
In the benchmark framework there are several built-in document sources we could choose from, including the Reuter's corpus (ReutersDocMaker), Wikipedia articles (EnwikiDocMaker) and a simple document source that recursively finds all *.txt files under a directory (DirDocMaker)
We'll use Wikipedia as the document source for all of our tests.
This is a large and diverse collection so it makes for a good real-world test.
For your own tests, create a document source implementing the DocMaker interface, and then use it for all of your testing.
To minimize the cost of document construction, let's first pre-process the Wikipedia XML content into a single large text file that contains one article per line.
We will be following the steps shown in Figure 10.1
There is a built-in WriteLineDoc task for exactly this purpose, which works for simple DocMakers that produce only title, date and body fields (this includes both EnwikiDocMaker and ReutersDocMaker)
You can also download a snapshot of this file from here:
This algorithm uses the built-in EnwikiDocMaker, which knows how to parse the XML format from Wikipedia, to produce one document at a time.
Sit back and enjoy the sound of hard drives seeking away doing all the hard work for you!  That is, if you are not using a solid-state drive.
Great!  Wasn't that easy? Now that we're done with the one-time setup, let's run a real test, using the efficient LineDocMaker as our document maker.
This algorithm builds an index with the first 200,000 Wikipedia articles, three times, using StandardAnalyzer.
At the end it prints a one-line summary of each run.
If you were building a real index for Wikipedia, you should use an analyzer base on the Wikipedia tokenizer under contrib/wikipedia.
This tokenizer understands the custom elements of Wikipedia's document format such as [[Category:…]]
Since we are only measuring indexing throughput here, StandardAnalyzer is fine for our purposes.
You should see something like this as your final report:
Discarding the slowest and fastest run, our baseline indexing throughput is 557.3 documents/second.
Not too shabby!  As of Lucene 2.3, the "out of the box" default indexing throughput has improved substantially.
Here are some specific things to try to further improve your application's indexing throughput:
This could be the single biggest impact change you can make, especially if your computer's hardware has a lot of concurrency.
Set IndexWriter to flush by memory usage and not document count.
This is the default as of 2.3, but if your application still calls setMaxBufferedDocs then change it to setRAMBufferSizeMB instead.
Make sure you don't go so high such that the JVM is forced to GC too frequently, or the computer is forced to start swapping (see 10.3.3)
Use the option ram.flush.mb in your algorithm to change the size of IndexWriter's RAM buffer.
Set compound=false in your algorithm to turn off compound file format.
If it's not necessary for a reader to see the changes to the index as you go, using autoCommit=false may improve throughput especially if you use large stored fields and term vectors.
As of 2.3, a Field allows you to change its value.
If your documents are highly regular (most are), then create a single Document instance and hold onto its Field instances.
Change only the Field values, and then call addDocument with the same Document instance.
Make sure your own analyzers and filters are re-using a single Token instance by defining the nextToken(Token) API.
For that Token, use the termBuffer API to get the internal char[] and change that array directly with your term text.
As of 2.3, all of the core analyzers use these APIs.
Higher values mean less merging cost while indexing, but slower searching since the index will have more segments.
Beware: if you make this too high, and if compound file format is turned off, you can hit file descriptor limits on your OS (see 10.3.2)
As of 2.3, segment merging is done in the background during indexing, so this is an automatic way to take advantage of concurrency.
You may actually see faster performance with a low mergeFactor, especially if you optimize the index in the end.
Test high and low values in your application and let the computer tell you which is best: you might be surprised!
This method optimizes your index down to maxNumSegments (instead of always 1 segment) which can greatly reduce the cost of optimizing, while still making your searches quite a bit faster.
If your searching performance is acceptable without optimizing then consider never optimizing.
Do not use the older addIndexes methods as these make extra calls to optimize.
Test the speed of creating the documents and just tokenizing them by using the ReadTokens task in your algorithm.
This task steps through each field of the document, and tokenizes it using the specified analyzer.
This is an excellent way to measure the document construction and tokenization cost, alone.
Run this algorithm to tokenize this first 200K docs from Wikipedia using StandardAnalyzer:
This number is actually very low, because we are using LineDocMaker as our document source.
In a real application, creating, filtering, and tokenizing the document would be much more costly.
Try it with your own DocMaker! Let's combine the suggestions above.
We'll index the same 200,000 documents from Wikipedia, but changing the settings to try to improve indexing throughput.
Wow, the performance is even better: 899.7 documents per second!  Of course in your testing you should test each of these changes above, one at a time, and keep only those that help.
Well there you have it!  As we've seen, Lucene's "out of the box" indexing throughput is excellent.
But with some simple tuning ideas, you can make it even better.
Search latency and throughput are two sides of one coin: changes that improve search latency will also improve your search throughput, on the same hardware.
The best way to measure your search latency and throughput is with a standalone load testing tool, such as Grinder.
These tools do a great job simulating multiple users and reporting latency and throughput results.
They also test your application end-to-end, which is what a real user experiences when using your web site.
Try to use real searches from real users when running search performance tests.
If possible, cull search logs to get all searches, and run them in the same order that they came from the search logs.
Use multiple threads to simulate multiple users, and verify you are fully utilizing the computer's concurrency.
Include follow-on searches, like clicking through pages, in the test.
The more "real world" your test is, the more accurate your test results are.
For example, if you create your own small set of hand-crafted searches for testing, and run these over and over, you can easily see unexpectedly excellent performance because the OS has loaded all required bytes from disk into its IO cache.
To fix this, you may be tempted to flush the IO cache before each test, which is possible.
But then you're going too far in the other direction, by penalizing your results too heavily, since in your real application the IO cache would help your performance.
Verify that rendering the results returned by Lucene is fast.
Be sure you are using enough threads to fully utilize the computer's hardware.
Increase the thread count until throughput no longer improves before search latency starts getting worse.
If you have enough RAM and enough file descriptors, consider using more than one instance of IndexSearcher.
There are some places where IndexReader is not fully concurrent, and many threads sharing a single instance could bottleneck.
Warm up your searchers before using them on real searches.
The first time a certain sort is used, it must populate the FieldCache.
Pre-warm the searching by issuing one search for each of the sort fields that may be used (see 10.1.2)
Use FieldCache instead of stored fields, if you can afford the RAM.
FieldCache pulls all stored fields into RAM, whereas stored fields must go back to disk for every document.
Populating a FieldCache is resource consuming (CPU and IO), but done only once per field the first time it's accessed.
Use TermVectorMapper to carefully select only the parts of the term vectors that you actually need.
If you must load stored fields, use FieldSelector to restrict to exactly those fields that you need.
Use lazy field loading for large fields so that the contents of the field are only loaded when actually requested.
You have to find the right balance for your application.
Moore's law lives on, but instead of giving us faster clock speeds, we get more CPU cores.
Hard drives accept many IO requests at once and re-order them to make more efficient use of the disk heads.
Even solid state disks do the same, and then go further by using multiple channels to concurrently access the raw storage.
Then, there is concurrency across these resources: when one thread is stuck waiting for an IO request to complete, another thread can use the CPU, and you will gain concurrency.
Therefore, it's critical to use threads for indexing and searching.
It's like buying a Corvette and driving it no faster than 20 mph!  Likely, switching to using threads is the single change you can make that will increase performance the most.
You'll have to empirically test, to find the right number of threads for your application and tradeoff search or indexing latency and throughput.
Generally, at first, as you add more threads, you'll see latency stay about the same, but throughput will improve.
Then when you hit the right number of threads, adding more will not improve throughput, and may hurt it somewhat due to context switching costs, but will increase the latency.
Unfortunately, there is the dark side to threads, which if you've explored them in the past you've discovered: they add substantial complexity to your application.
Testing is difficult because the threads are scheduled at different times by the JVM every time you run a test.
Yes, they are! Lucene has been carefully designed to work very well with many threads.
Lucene is thread safe: sharing IndexSearcher, IndexReader, IndexWriter, etc, across multiple threads is perfectly fine.
Lucene is also thread friendly: synchronized code is minimized such that multiple threads can make full use of the hardware's concurrency.
You can choose a merge scheduler in your algorithm by setting the merge.scheduler property.
In this section we show you how to leverage threads during indexing and searching, and provide a couple of drop-in classes to make it simple to gain concurrency.
The class simplifies multithreaded indexing because all details of these threads are hidden from you.
It's also a drop-in for anywhere you are currently using the IndexWriter class, though you may need to modify it if you need to use one of IndexWriter’s expert constructors.
Just specify how many threads to use, and the size of the queue, when you instantiate the class.
Test different values to find the sweet spot for your application, but a good rule of thumb for numThreads is one plus the number of CPU cores in your computer that you'd like to consume on indexing, and then 4*numThreads for maxQueueSize.
As you use more threads for indexing, you'll find that a larger RAM buffer size should help even more, so be sure to test different combinations of number of threads and RAM buffer size to reach your best performance.
Check process monitor tools, like top or ps on unix, or Task Manager on Windows, to verify that CPU utilization is near 100%
Listing 10.1 Utility class to use multiple threads to index documents.
The class overrides the addDocument and updateDocument methods: when one of these is called, a Job is created and added to the work queue in the thread pool.
If the queue in the thread pool is not full, then control immediately returns back to the caller.
Otherwise, the caller's thread is used to immediately execute the Job.
In the background, a worker thread wakes up, takes jobs from the front of the work queue, and does the real work.
When you use this class, you cannot re-use Document or Field instances, because you can't control precisely when a Document is done being indexed.
The class overrides close and rollback methods, to first shutdown the thread pool to ensure all adds and updates in the queue have completed.
Create a new algorithm, derived from the baseline algorithm from 10.2.3, with only these changes:
You should see it finish quite a bit faster than the original baseline!  Now you can just drop this class in wherever you now use IndexWriter and take advantage of concurrency.
Fortunately, a modern web or application server handles most of the threading issues for you: it maintains a firstin-first-out request queue, and a thread pool to service requests from the queue.
This means much of the hard work is already done!  All you have to do is create a Query based on the details in the user's request, invoke your IndexSearcher, and render the results.
Be sure you tune the size of the thread pool to make full use of the computer's concurrency.
Also, tune the maximum allowed size of the request queue for searching: when your web site is suddenly popular and far too many searches per second are arriving, you want new requests to quickly receive an HTTP 500 Server Too Busy error, instead of waiting in the request queue forever.
This also ensures that your application gracefully recovers once the traffic settles down again.
There is one tricky aspect that the application server will not handle for you: reopening your searcher when your index has changed.
Since an IndexReader only sees the index as of the point in time when it was opened, once there are changes to the index you must reopen your IndexReader to search them.
Unfortunately, this is frequently a costly operation, consuming CPU and IO resources.
Yet, for some applications, minimizing index-tosearch delay is worth that cost, which means you'll have to reopen your searcher frequently.
Beyond that, you may want to keep the old searcher around for long enough for all search sessions (original search plus all follow on actions like clicking through pages) to finish or expire.
For example, consider a user who is stepping through page after page of results, where each page is a new search on your server.
If you suddenly swap in a new searcher in between pages then the documents assigned to each page could shift, causing the user to see duplicate results across pages, or, to miss some results.
This erodes your user's trust which is pretty much the kiss of death for your application.
Prevent this by sending new pages for a previous search back to the original searcher, when possible.
Listing 10.2 shows a useful utility class, SearcherManager, that hides the tricky details of re-opening your searcher in the presence of multiple threads.
Instantiate this class once in your application, naming it searcherManager.
Every call to get must be matched with a corresponding call to release, ideally using a try/finally clause.
Note that this class does not actually do any re-opening on its own.
Instead, you must call maybeReopen every so often according to your application's needs.
For example, a good time to call this is after you've closed the IndexWriter.
Reopening your reader while an IndexWriter is still open could tie up extra disk space because segment merges might be in progress.
Once optimize is done, you should definitely reopen to free up the disk space and get faster searches.
If your index is not too large, it’s fine to call maybeReopen during a search request.
The call will reopen the searcher, if necessary, using the foreground thread, thus adding latency to the unlucky search that hits it.
If your index is large, then it’s better to call maybeReopen from a dedicated background thread.
You should also create a subclass that implements the warm method to run the targetted initial searches against the new searcher before it's made available for general searching (see 10.1.2.3)
Like all software, Lucene requires certain precious resources to get its job done.
A computer has a limited supply of things like disk storage, file descriptors and memory.
Understanding how Lucene uses resources and what you can do to control this lets you keep your search application healthy.
You might assume Lucene's disk usage is simply proportional to the total size of all documents you've added, but you'll be surprised to see that often, this is far from the truth.
Similarly, Lucene's usage of simultaneous open file descriptors is unexpected: changes to a few Lucene configuration options can drastically change the number of open files.
Finally, to manage Lucene's memory consumption, you'll see why it's not always best to give Lucene access to all memory on the computer.
We start with everyone's favorite: how much disk space does Lucene require?  Next we describe Lucene’s open file descriptor usage, and finally, memory usage.
An index with only a single pure indexed, typical text field will be about 1/3rd of the total size of the original text.
But at the other extreme, an index that has stored fields and term vectors with offsets and positions, with numerous deleted documents plus an open reader on the index, with an optimize running, can easily consume 10X the total size of the original text!  This wide range and seeming unpredictability makes it exciting to manage disk usage for a Lucene index.
Figure 10.3 shows the disk usage over time while indexing all documents from Wikipedia, finishing with an optimize call.
Rather than increasing gradually with time, as you add documents to the index, disk usage will suddenly ramp up during a merge and then quickly fall back again once the merge has finished, creating a saw tooth pattern.
The size of this jump corresponds to how large the merge was (the net size of all segments being merged)
How can you manage disk space when it has such wild swings?  Fortunately, there is a method to this madness.
Once you understand what's happening under the hood, you can predict and understand Lucene's disk usage.
It's important to differentiate transient disk usage, while the index is being built (Figure 10.3), versus final disk usage, when the index is completely built and optimized to one.
Here is a coarse formula to estimate the final size based on the size of all text from the documents:
For example, documents with unusually diverse or unique terms, like a large spreadsheet that contains many unique product SKUs, will use more disk space.
Figure 10.3 Disk usage while building an index of all Wikipedia documents, with optimize called in the end.
As the index gets larger, the size of each saw tooth will get larger as bigger merges are being done.
Large merges also take longer to complete and will therefore tie up disk space for more time.
When you optimize the index, down to one segment, the final merge is the largest merge possible and will require 1X the final size of your index in temporary disk space.
Here are other things that will affect transient disk usage:
Open readers prevent deletion of those files they are using.
You should have only one open reader at a time, except when you are re-opening it.
With autoCommit=false, disk usage will be higher because segments referenced by the starting commit point (when the writer was opened), as well as those referenced by the current (in memory) commit point are not deleted.
If you frequently replace documents but do not run optimize then the space used by old copies of the deleted documents won't be reclaimed until those segments are merged.
The more segments in your index, the more disk space will be used than if those segments were merged.
This means a high mergeFactor will result in more disk space being used.
Given the same net amount of text, many small documents will result in a larger index than fewer large documents.
Do not open a new reader while optimize, or any other merges, are running as this will hold references to segments that would otherwise be deleted.
Do open a new reader after making changes with IndexWriter.
If you don't, then the reader is holding references to files that IndexWriter may have deleted, due to merging.
If you are running a backup (see 10.4) then the files in the snapshot being copied will also consume disk space, until you release the snapshot.
Note that on UNIX you may think disk space has been freed because the writer has deleted the old segments files, but in fact the files still consume disk space as long as those files are held open by an IndexReader.
Windows does not allow deletion of open files so you'll still see the files when you look at the directory.
Don't be fooled! So how can you make sense of all of this?  A good rule of thumb is to measure the total size of your index, called that X.
Suppose you're happily tuning your application to maximize indexing throughput.
You cranked up mergeFactor and got awesome speedups, so you want to push it even higher.
Unfortunately, there is a secret cost to these changes: you are drastically increasing how many files Lucene must hold open at once.
At first you're ecstatic about your changes, since everything seems fine.
Then, as you add more documents index grows, Lucene will need more and more open files when one day -- BOOM! -- you hit the dreaded "Too many open files" IOException, and the OS stops you dead in your tracks.
Faced with this silent and sudden risk, how can you possibly tune for the best indexing performance, while staying under this limit? Fortunately, there is hope!  With a few simple steps you can take control of the situation.
When you run the test, it will always fail and then tell you how many files it was able to open before the OS cut it off.
Next, increase the limit to the maximum allowed by the OS.
The exact command for doing so varies according to OS and shell (hello Google my old friend)
Run the test again to make sure you've actually increased the limit.
Finally, monitor how many open files your JVM is actually using.
You'll have to add File Handles as a column, using the View.
The sysinternals tools from Microsoft also include useful utilities like Process Monitor to see which specific files are held open by which processes.
To get more specifics about which files Lucene is opening, and when, use the class in Listing 10.3
This class is a drop-in replacement for FSDirectory that adds tracking of open files.
It reports whenever a file is opened or closed, for reading or writing, and lets you retrieve the current total count of open files.
Compile the code and ensure the class file is on your CLASSPATH.
Figure 10.4 File descriptor consumption while building an index of Wikipedia articles.
You can see that it follows a peaky pattern, with very low usage when flushing segments and rather high usage while merges are running since the writer holds open files for all segments being merged plus the new segment being created.
This means mergeFactor, which sets the number of segments to merge at a time, directly controls the open file count during idnexing.
Unlike indexing, where peak open file count is a simple multiple of mergeFactor, searching can require many more open files.
For each segment in the index, the reader must hold open all files for that segment.
During reopen, if the index has changed substantially because a merge has completed, the open file count will at first peak very high, because during this time both the old and new reader are in fact open.
Once the old reader is closed, the usage drops down, in proportion to how many segments are in the index.
As the index gets larger, the usage increases, though it is not a straight line because sometimes the reader catches the index soon after a large merge has finished.
Armed with your new knowledge about open file consumption, here are some simple tips to keep them under control while still enjoying your indexing performance gains:
The less often the writer flushes a segment, the fewer segments there will be in the index.
This is a very big reduction on peak open file count.
This is often when you are opening and warming a new reader, before you've closed the old one.
If you run indexing and searching from a single JVM, you must add up the peak open file count for both.
The peak often occurs when several concurrent merges are running and you are reopening your reader.
If possible, close your writer before reopening your reader to prevent this "perfect storm"
Double check that all other code also running in the same JVM is not using too many open files.
If it is, consider running a separate JVM for it.
If you find you are still running out of file descriptors far earlier than you'd expect, make sure you are really closing your old IndexReader instances.
Striking the right balance between performance, and the dreaded open file limit, feels like quite an art.
But now that you understand how Lucene uses open files, how to test and increase the limit on your OS, and how to measure exactly which files are held open by Lucene, you have all the tools you need to strike that perfect balance.
It's now more science than art!  We'll move next to another challenging resource: memory usage.
You've surely hit OutOfMemoryError in your Lucene application in the past?  If you haven't, then you will, especially when many of the ways to tune Lucene for performance also increase its memory usage.
So you thought: no problem, just increase the JVMs' heap size, and move on.
You do that, and things seem fine, but little do you know you actually really hurt the performance of your application because the computer has started swapping memory to disk.
And perhaps a few weeks later you hit the same error again.
What's going on?  How can you control this devious error and still keep your performance gains?
Managing memory usage is especially exciting, because there are two very different levels of memory.
First, you must control how the JVM uses memory from the OS.
Second, you must control how Lucene uses memory from the JVM.
Once you understand these levels you'll have no trouble preventing memory errors and maximizing your performance, at the same time.
You manage the JVM by telling it how large its heap should be.
The option -Xms size sets the starting size of the heap and the option -Xmx size sets the maximum allowed size of the heap.
In a production server environment, you should set both of these sizes to the same value, so the JVM does not spend time growing and shrinking the heap.
The heap size should be large enough to give Lucene the RAM that it needs, but not so large that you force the computer to swap excessively.
Generally you shouldn't just give all RAM to the JVM: it's beneficial to leave excess RAM free to allow the OS to use as its IO cache.
How can you tell if the computer is excessively swapping?  Here are some clues:
Check for high numbers in these columns (say greater than 50)
Ordinary interactive processes, like a shell or command prompt, or a text editor, or Windows explorer, are.
The numbers tell you how much RAM the computer is using for its IO cache.
Note that modern OSs happily swap out processes that seem idle in order to use the RAM for the IO cache.
But, excessive swapping, especially while you are indexing or searching, is not good.
In order to manage how Lucene, in turn, uses memory from the JVM, you first need to measure how much memory Lucene needs.
There are various ways, but the simplest is to specify the -verbose:gc and XX:+PrintGCDetails options when you run java, and then look for the size of the total heap after collection.
This is useful because it excludes the memory consumed by garbage objects that are not yet collected.
If your Lucene application needs to use up nearly all of the memory allocated for the JVM's maximum heap size, it may cause excessive GC, which will slow things down.
If you use even more memory than that, you'll eventually hit OutOfMemoryError.
During indexing, the RAM required by Lucene is entirely dictated by the size of the buffer used by IndexWriter, which you can control with setRAMBufferMB.
Don't set this too low as it will slow down indexing throughput.
While a segment merge is running, a small amount of additional RAM is required.
Here are some tips to reduce RAM usage during searching:
Try not to load String fields as these are far more memory consuming than int, long and float.
The first time a search is sorted by a given field, its values are loaded into the FieldCache.
Norms encode index-time boosting, which combines field boost, doc boost and length boost, into a single byte per document.
Even documents without this field consume one byte because the norms are stored as a single contiguous array.
This quickly works out to a lot of RAM if you have many indexed fields.
Often norms are not actually a big contributor to relevance scoring.
For example, if your field values are all similar in length (e.g., a title field), and you are not using field or document boosting, then norms are not necessary.
Be sure to fully rebuild your index once you turn them off because if even a single document in your index had norms enabled for that field then all documents in the same segment will still have norms allocated.
Use a single text field: it's better to combine multiple text fields into a single indexed, tokenized field and search only that field.
This will also make searching faster and may improve your search relevance.
Use Luke to look at the terms in your index and verify these are legitimate terms that users may search on.
It's easy to accidentally index binary documents, which can produce a great many bogus binary terms that would never be used for searching.
These terms cause all sorts of problems once they get into your index, so it's best to catch them early by skipping or properly filtering the binary content.
If your index has an unusually large number of legitimate terms, e.g.
Accidentally keeping a reference to past instances can very quickly exhaust RAM and file descriptors.
Use a Java memory profiler to see what's using so much RAM.
Be sure to test your RAM requirements during searching while you are re-opening a new reader because this will be your peak usage.
Let's go back and re-run our fastest Wikipedia indexing algorithm, intentionally using a heap size that's too small to see what happens if you don't tune memory usage appropriately.
This time let's give it only 145 MB heap size (anything below this will likely hit OutOfMemoryError)
Run the algorithm, adding -Dtask.mem=145M, and you should see something like this:
You can see the importance of giving Lucene enough RAM!
So, it's 2 AM, and you're having a pleasant dream about all the users who love your Lucene search application when, suddenly, you wake up to the phone.
It's an emergency call saying your search index is corrupt.
No problem, you answer: restore from the last backup! You do back up your search index, right?
Things will inevitably go wrong: a power supply fails, a hard drive crashes, your RAM becomes random.
These events can suddenly render your index completely unusable, almost certainly at the worst possible time.
Your final line of protection against such failures is a periodic backup of the index along with simple, accessible steps to restore it.
The simplest way to backup an index is to close your writer and make a copy of all files in the index directory.
During the copy, which could take a very long time for a large index, you cannot have a writer open.
Many applications cannot accept such a long downtime in their indexing.
When a reader is open on the index, you will copy more files than needed, if the reader is holding some files open that are no longer referenced.
Finally, the IO load of the copy can slow down searching.
You might be tempted to throttle the copy rate to compensate for this, but that will increase your downtime.
No wonder so many people just skip backups entirely, cross their fingers, and hope for the best!
Fortunately, as of Lucene 2.3, there is now a simple answer: you can easily make a "live backup" of the index, such that you create a consistent backup image, with just the files referenced by the most recent commit point, without closing your writer.
No matter how long the copying takes, you can still make updates to the index.
Your backup program can take as long as it needs to copy the files.
You could throttle its IO or set it to low process priority to make sure it does not interfere with ongoing searching or indexing.
You can spawn a sub-process to run rsync, tar, robocopy or your favorite backup utility, giving it the list of files to copy.
This can also be used to mirror a snapshot of the index to other computers.
When you want to do a backup, just do this:
Inside the try block, all files referenced by the commit point will not be deleted by the writer, even if the writer.
While this snapshot is kept alive, the files that belong to it will hold space on disk.
So while a backup is running your index will use more disk space than it normally would (assuming the writer is continuing to commit changes to the index)
Once you're done, call release to allow the writer to delete these files the next time it flushes or is closed.
This means you can do an incremental backup by simply comparing file names.
You do not have to look at the contents of each file, not its last modified timestamp, because, once a file is written, it will not be changed.
The only exception to this is the file segments.gen, which is overwritten on every commit, and so you should always copy this file.
This means if you close your writer and open a new one, the snapshot will be deleted.
So you cannot close your writer until the backup has completed.
This is also easy to fix: you could store and load the current snapshot on disk and then protect it on opening a new writer.
This would allow the backup to keep running even if the original writer is closed and new one opened.
Believe it or not, that's all there is to it!  Now we move onto restoring your index.
In addition to doing periodic backups, you should have simple steps on hand to quickly restore the index from backup and restart your application.
Here are the steps to follow when restoring an index:
Close any existing readers and writers on the index directory, so the file copies will succeed.
On Windows, if there are still processes using those files, you won't be able to overwrite them.
Copy all files from your backup into the index directory.
Be certain this copy does not hit any errors, like a disk full, because that is a sure way to corrupt your index.
Speaking of corruption, let's talk next about common errors you may hit with Lucene.
Documents already committed to the index will be intact, and the index will be consistent.
The same is true if the JVM crashes, or hits an unhandled exception, or is explicitly killed.
But first be certain there is in fact no writer writing to that directory!
One common pitfall is to iterate through the hits of a search after the underlying reader has been closed.
This is not allowed because those hits need the reader to remain open in order to load documents.
So maybe you've seen an odd, unexpected exception in your logs, or, maybe the computer crashed while indexing.
Nervously, you bring your Lucene application back up, and all seems to be fine, so you just shrug and move onto the next crisis.
But you can't escape the sinking sensation and burning question deep in your mind: is it possible my index is now corrupt?  Suddenly a month or two later, more strange exceptions start appearing.
Unfortunately, there are certain known situations that can lead to index corruption.
If this happens to you, try to get to the root cause of the corruption.
Accidentally allowing two writers to write to the same index at the same time.
But if you use a different LockFactory inappropriately, or if you incorrectly removed a write.lock that was in fact a real lock, that could allow two writers at once.
If you have a step in your indexing process where an index is copied from one place to another, an error in that copying can easily corrupt the target index.
It's even possible you've hit a previously undiscovered bug in Lucene!  Take your case to the lists, or open an issue with as much detail as possible about what led to the corruption.
While you can't eliminate these risks, you can be proactive in detecting index corruption.
But all sorts of other unexplained exceptions are also possible.
To proactively test your index for corruption, here are two things to try:
This causes Lucene to do more thorough checking at many points during indexing and searching, which could catch corruption sooner than you would otherwise.
This tool runs a thorough check of every segment in the index, and reports detailed statistics, and any corruption, for each.
If you find your index is corrupt, first try to restore from backups.
This can easily happen since corruption may take a long time to detect.
What can you do, besides rebuilding your full index from scratch?  Fortunately, there is one final resort: use the CheckIndex tool to repair it.
When all else fails, your final resort is the CheckIndex tool.
In addition to printing details of your index, this tool can repair your index if you add the -fix command-line option:
Note that this completely removes all documents that were contained in that segment, so use this option with caution and make a backup copy of your index first! You should use this tool just to get your search operational again, on an emergency basis.
Once you are back up, then you should rebuild your index to recover the lost documents.
We've covered many important hands-on topics in this chapter!  Think of this chapter like your faithful swiss-army knife: you now have the necessary tools under your belt to deal with all the important, real-world aspects of Lucene administration.
Lucene has great out-of-the-box performance, and now you know how to further tune that performance for specific metrics that are important to your application, using the powerful and extensible contrib/benchmark framework.
Unfortunately, tuning for one metric often comes at the expense of others, so you should decide up front which metric is most important to you.
You've seen how crucial it is to use threads during indexing and searching to take advantage of the concurrency in modern computers, and now you have a couple drop-in classes that make this painless.
Backing up and index is a surprisingly simple operation that no longer requires stopping your IndexWriter.
Lucene's consumption of disk, file descriptors, and memory is no longer a mystery and is well within your control.
Index corruption is not something to fear, since you know what might cause it and you know how to detect and repair a corrupted index.
The common errors that happen are easy to understand and fix.
So far, we have treated the Lucene index more or less as a black box and have concerned ourselves only with its logical view.
Although you don’t need to understand index structure details in order to use Lucene, you may be curious about the “magic.” Lucene’s index structure is a case study in itself of highly efficient data structures and clever arrangement to maximize performance and minimize resource usage.
You may see it as a purely technical achievement, or you can view it as a masterful work of art.
There is something innately beautiful about representing rich structure in the most efficient manner possible.
Consider the information represented by fractal formulas or DNA as nature’s proof.
In this appendix, we’ll look at the logical view of a Lucene index, where we’ve fed documents into Lucene and retrieved them during searches.
Then, we’ll expose the inner structure of Lucene’s inverted index.
B.1 Logical index view Let’s first take a step back and start with a quick review of what you already know about Lucene’s index.
From the perspective of a software developer using Lucene API, an index can be considered a black box represented by the abstract Directory class.
When indexing, you create instances of the Lucene Document class and populate it with Fields that consist of name and value pairs.
When searching, you again use the abstract Directory class to represent the index.
You pass that Directory to the IndexSearcher class and then find Documents that match a given query by passing search terms encapsulated in the Query object to one of IndexSearcher’s search methods.
The results are matching Documents represented by the Hits object.
Figure B.1 The logical, black-box view of a Lucene index.
We have also used Indexer, a program for indexing text files, shown in listing 1.1
Recall that we specified several arguments when we invoked Indexer from the command line and that one of those arguments was the directory in which we wanted Indexer to create a Lucene index.
What does that directory look like once Indexer is done running? What does it contain? In this section, we’ll peek into a Lucene index and explain its structure.
Before we start, though, you should know that the index file format often changes between releases.
It’s free to change without breaking backwards compatibility because the classes that access the index can detect when they are interacting with an older format index and act accordingly.
Lucene supports two index structures: multifile indexes and compound indexes.
Multifile indexes use quite a few files to represent the index, while compound indexes use a special file, much like an archive such as a zip file, to hold multiple index files in a single file.
Let’s look at each type of index structure, starting with multifile.
B.2.1 Understanding the multifile index structure If you look at the index directory created by our Indexer, you’ll see a number of files whose names may seem random at first.
These are index files, and they look similar to those shown here:
In this example index, a number of files start with the prefix _2, followed by various extensions.
Index files that belong to the same segment share a common prefix and differ in the suffix.
In the previous example index, the index consisted of a single segment whose files started with _2:
You can think of a segment as a subindex, although each segment isn’t a fully independent index.
As you can see in figure B.2, each segment contains one or more Lucene Documents, the same ones.
By now you may be wondering what function segments serve in a Lucene index; what follows is the answer to that question.
This process makes additions efficient because it minimizes physical index modifications.
If this index were to be optimized using the default Lucene indexing parameters, all 34 of its documents would be merged in a single segment.
One of Lucene’s strengths is that it supports incremental indexing, which isn’t something every IR library is capable of.
Whereas some IR libraries need to reindex the whole corpus when new data is added to their index, Lucene does not.
After a document has been added to an index, its content is immediately made searchable.
In IR terminology, this important feature is called incremental indexing.
Lucene supports incremental indexing makes Lucene suitable for environments that deal with large bodies of information where complete reindexing would be unwieldy.
Because new segments are created as new Documents are indexed, the number of segments, and hence index files, varies while indexing is in progress.
Once an index is fully built, the number of index files and segments remains steady.
If any index file is modified or removed by anything other than Lucene itself, the index becomes corrupt, and the only option is a complete reindexing of the original data.
On the other hand, you can add random files to a Lucene index directory without corrupting the index.
For instance, if we add a file called random.txt to the index directory, as shown here, Lucene ignores that file, and the index doesn’t become corrupt:
As you may have guessed from its name, the segments file stores the name and certain details of all existing index segments.
Every time an IndexWriter commits a change to the index, the generation (the _3 in the above listing) of the segments file is incremented.
Before accessing any files in the index directory, Lucene consults this file to figure out which index files to open and read.
Lucene also limits itself to files with known extensions, such as .fdt, .fdx, and other extensions shown in our example, so even saving a file with a segment prefix, such as _2.txt, won’t throw Lucene off.
Of course, polluting an index directory with non-Lucene files is strongly discouraged.
The exact number of files that constitute a Lucene index and each segment varies from index to index and depends on the number of fields the index contains.
However, every index contains a single segments file and a single segments.gen file.
The segments.gen file is always 20 bytes and contains the suffix (generation) of the current segments as a redundant way for Lucene to determine the most recent commit.
B.2.2 UNDERSTANDING THE COMPOUND INDEX STRUCTURE When we described multifile indexes, we said that the number of index files depends on the number of indexed fields present in the index.
We also mentioned that new segments are created as documents are added to an index; since a segment consists of a set of index files, this results in a variable and possibly large number of files in an index directory.
Although the multifile index structure is straightforward and works for most scenarios, it isn’t suitable for environments with large number of indexes and other environment where using Lucene results in a large number of index files.
Most, if not all, contemporary operating systems limit the number of files in the system that can be opened at one time.
Recall that Lucene creates new segments as new documents are added, and every so often it merges them to reduce the number of index files.
However, while the merge procedure is executing, the number of index files temporarily increases.
If Lucene is used in an environment with lots of indexes that are being searched or indexed simultaneously, it’s possible to reach the limit of open files set by the operating system.
This can also happen with a single Lucene index if the index isn’t optimized or if other applications are running simultaneously and keeping many files open.
Lucene’s use of open file handles depends on the structure and state of an index.
Section 10.XXX presents formulas for calculating the number of open files, as well as ideas to reduce that number.
Instead of having to open and read 10 files from the index, as in the multifile index, Lucene must open only three files when accessing this compound index, thereby consuming fewer system resources.
The compound index reduces the number of index files, but the concept of segments, documents, fields, and terms still applies.
The difference is that a compound index contains a single .cfs file per segment, whereas each segment in a multifile index contains consists of seven different files.
The compound structure encapsulates individual index files in a single .cfs file.
Pleasantly, you aren’t locked into the multifile or compound format.
After indexing, you can still switch from one format to another, although this will only affect newly written segments.
B.2.3 CONVERTING FROM ONE INDEX STRUCTURE TO THE OTHER It’s important to note that you can switch between the two described index structures at any point during indexing.
Similarly, you can convert the structure of an existing index without adding more documents to it.
For example, you may have a multifile index that you want to convert to a compound one, to reduce the number of open files used by Lucene.
To do so, open your index with IndexWriter, specify the compound structure, optimize the index, and close it:
Note that the third IndexWriter parameter is false to ensure that the existing index isn’t destroyed.
B.3 Choosing the index structure Although switching between the two index structures is simple, you may want to know beforehand how many open files resources Lucene will require when accessing your index.
If you’re designing a system with multiple simultaneously indexed and searched indexes, you’ll most definitely want to take out a pen and a piece of paper and do some simple math with us now.
B.3.1 Calculating the number of open files Let’s consider a multifile index first.
A multifile index contains seven index files for each segment (10 if any fields have term vectors indexed) and a single segments plus a segments.gen file for the whole index.
Also assume that term vectors are used, these indexes aren’t optimized and that each has nine segments that haven’t been merged into a single segment yet, as is often the case during indexing.
Although today’s computers can usually handle this many open files, most come with a preconfigured limit that is much lower.
In section 10.3.2, we go into more detail about how to measure and manage file descriptor.
Next, let’s consider the same 100 indexes, but this time using the compound structure.
Only a single file with a .cfs extension is created per segment, in addition to a single segments and segments.gen file for the whole index.
Therefore, if we use the compound index instead of the multifile one, the number of open files is reduced to 900:
The lesson here is that if you need to develop Lucene-based software that will run in environments with a large number of Lucene indexes with a number of indexed fields, you should consider using a compound index.
Of course, you can use a compound index even if you’re writing a simple application that deals with a single Lucene index.
B.3.2 INDEXING AND SEARCHING PERFORMANCE Performance is another factor you should consider when choosing the index structure.
Creating an index with a compound structure is generally 5–10% slower than creating an equivalent multifile index.
This is because when writing a new segment the IndexWriter first writes the multifile format and then creates the compound file.
Section 10.1.2 goes into more detail about tuning and measuring indexing performance.
This performance difference and the difference in the amount of system resources the two index structures use are their only notable differences.
All Lucene’s features work equally well with either type of index.
B.4 Inverted index Lucene uses a well-known index structure called an inverted index.
Quite simply, and probably unsurprisingly, an inverted index is an inside-out arrangement of documents such that terms take center stage.
Let’s dissect our sample book data index to get a deeper glimpse at the files in an index Directory.
Regardless of whether you’re working with a RAMDirectory, an FSDirectory, or any other Directory implementation, the internal structure is a group of files.
FSDirectory literally represents an index as a file-system directory, as described earlier in this appendix.
The compound file mode (added in Lucene 1.3) adds an additional twist regarding the files in a Directory.
When an IndexWriter is set for compound file mode, the “files” are written to a single .cfs file, which alleviates the common issue of running out of file handles.
See the section “Compound index files” in this appendix for more information on the compound file mode.
B.4.1 Inside the index The Lucene index format is detailed in all its gory detail on the Lucene web site at http://jakarta.apache.org/lucene/docs/fileformats.html.
It would be painful for us, and tedious for you, if we repeated this detailed information here.
Rather, we have chosen to summarize the overall file structure using our sample book data as a concrete example.
Our summary glosses over most of the intricacies of data compression used in the actual data representations.
This extrapolation is helpful in giving you a feel for the structure instead of getting caught up in the minutiae (which, again, are detailed on the Lucene web site)
Figure B.3 represents a slice of our sample book index.
The slice is of a single segment (in this case, we had an optimized index with only a single segment)
A segment is given a unique filename prefix (_c in this case)
The following sections describe each of the files shown in figure B.3 in more detail.
Each field is flagged to indicate options that were used while indexing:
Does it have payloads? The order of the field names in the .fnm file is determined during indexing and isn’t necessarily.
Each field is assigned a unique integer, the field number, according to the order in this file.
That field number, instead of the string name, is used in other Lucene files to save space.
In our sample index, only the subject field is vectored.
The url field was added as a Field.Index.NO field, which is neither indexed nor vectored.
Terms are ordered first alphabetically, according to the UTF16 java character, by field name and then by value within a field.
Each term entry contains its document frequency: the number of documents that contain this term within the segment.
Figure B.3 shows only a sampling of the terms in our index, one or more from each field.
Note that the url field is missing because it was added as an unindexed field, which is stored only and not available as terms.
Not shown is the .tii file, which is a cross-section of the .tis file designed to be kept in physical memory for random access to the .tis file.
For each term in the .tis file, the .frq file contains entries for each document containing the term.
In our sample index, Java Development with Ant (document ID 5) has the value “junit” once in the contents field.
JUnit in Action has the value “junit” twice, provided once by the title and once by the subject.
Our contents field is an aggregation of title, subject, and author.
The frequency of a term in a document factors into the score calculation (see section 3.3) and typically boosts a document’s relevance for higher frequencies.
For each document listed in the .frq file, the positions (.prx) file contains entries for each occurrence of the term within a document.
The position information is used when queries demand it, such as phrase queries and span queries.
Position information for tokenized fields comes directly from the token position increments designated during analysis.
Figure B.3 shows three positions, for each occurrence of the term junit.
In the case of document 5, the field value (after analysis) is “java development ant apache jakarta ant build tool junit java development erik hatcher steve loughran”
We used the StandardAnalyzer; thus stop words (with in Java Development with Ant, for example) are removed and aren’t accounted for in positional information (see section 4.7.3 for more on stop word removal and positional information)
The .fdx file contains simple index information, which is uses to resolve document number to.
We’re indebted to Luke, the fantastic index inspector, for allowing us to easily gather some of the data provided about the index structure.
The .fdt file actually contains the contents of the fields that were stored.
The .tvf file is the largest and actually stores the specific terms, sorted alphabetically, and their frequencies, plus the optional offsets and positions for the terms.
The .tvd file lists which fields had term vectors for a given document and indexes byte offsets into the .tvf file so specific fields can be retrieved.
Finally, the .tvx file has index information, which resolves document number into the byte positions in the .tvf and .tvd files.
Each document has one byte in this file, which encodes the combination of the document’s boost, that field’s boost, and a normalization factor based on the overall length of the content in that field.
B.5 Summary The rationale for the index structure is two-fold: maximum performance and minimum resource utilization.
For example, if a field isn’t indexed it’s a very quick operation to dismiss it entirely from queries based on the indexed flag of the .fnm file.
The .tii file, cached in RAM, allows for rapid random access into the term dictionary .tis file.
Phrase and span queries need not look for positional information if the term itself isn’t present.
Streamlining the information most often needed, and minimizing the number of file accesses during searches is of critical concern.
These are just some examples of how well thought out the index structure design is.
If this sort of low-level optimization is of interest, please refer to the Lucene index file format details on the Lucene web site, where details we have glossed over here can be found.
The contrib/benchmark package is a very useful framework for running repeatable performance tests.
By creating a short script, called an "algorithm" (file.alg), you tell benchmark what test to run and how to report its results.
We briefly used benchmark in Chapter 10 to measure Lucene’s indexing performance.
Benchmark is quite new and will improve over time, so always check the javadocs.
You might be tempted to just create your own testing framework.
Likely you’ve done so already many times in the past.
But there are some important reasons to use benchmark instead:
Since an algorithm file is a simple text file, it’s easily and quickly shared with others so they can reproduce your results.
This is vitally important in cases where you are trying to track down a performance anomaly and you'd like to understand the source.
Whereas for your own testing framework often there are numerous dependencies and perhaps access to a database that holds that would have to be transferred for someone else to run the test.
The benchmark framework already has builtin support for common standard sources of documents (eg, Reuters, Wikipedia, Trec)
With your own test, it’s easy to accidentally create performance overhead in the test code itself, or even a bug, which skews results.
The benchmark package, thanks to being open source, is well-debugged and well-tuned so it's less likely to hit this.
Thanks to a great many builtin tasks, you can create many algorithms without writing any Java code.
By writing a few lines (the algorithm file) you can craft any test you'd like.
You only have to change your script and re-run if you want to test something else.
Benchmark has multiple extension points, to easily customize the source of documents, source of queries, how the metrics are reported in the end.
You can also create your own tasks, as we did in Section 10.XXX.
Benchmark already gathers important metrics, like run time, documents per second, and memory usage, saving you from having to instrument these in your custom test code.
Let’s get our feet wet now and run a basic algorithm.
D.1 Running an algorithm Save the following lines to a file, test.alg:
As you can probably guess, this algorithm indexes the entire Reuter’s corpus, 3 times, and then reports the.
Those steps include creating a new index, adding all Reuter’s documents, and closing the index.
Remember, when testing indexing performance it's important to include the time to close the index since necessary time consuming things happen during close()
For example, Lucene waits for any still-running background merges to finish, and then syncs all newly written files in the index.
If you've implemented any custom tasks, you'll need to include the classpath to your compiled sources by adding this to the ant command line:
Ant first runs a series of dependency targets, for example making sure all sources are compiled and downloading, and unpacking the Reuters corpus.
Finally it runs your task and produces something like this under the "run-task" output:
First, benchmark prints all the settings you are running with.
It's best to look this over and verify the settings.
For each round, it includes the number of records (added documents in this case), records per second, elapsed seconds, and memory usage.
The average used memory is computed by subtracting freeMemory() from totalMemory()
What exactly is a "record"?  In general, most tasks count as +1 on the record count.
To prevent the record count from incrementing, you prefix the task with a "-" as we have done above for CreateIndex and CloseIndex.
This allows us to include the cost (time & memory usage) of creating and closing the index yet correctly amortize that total cost across all added documents.
So that was pretty simple, right?  From this you could probably poke around and make your own algorithms.
But to really shine, you’ll need to know the full list of settings and tasks that are available.
D.2 Parts of an algorithm file Let's dig into the different parts of an algorithm file.
This file is a simple text file, where comments begin with the “#” character, and whitespace is generally not significant.
Usually the settings, which bind global names to their values, appear at the top.
Next comes the heart of the algorithm, which expresses the  series of tasks to run, and in what order.
Finally, there is usually one or more reporting tasks at the very end to generate the final summary.
For example compound=false will open the IndexWriter with setUseCompoundFile set to false.
Often you want to run a series of rounds where each round uses different combination of settings.
One example would be to measure the performance impact of changing RAM buffer sizes during index.
Be sure to consult the online documentation for an up to date list.
If true, the doc.maker will reset itself upon running out of documents and just keep producing documents forever.
Otherwise, it will stop when it has made one pass through its documents.
If true then fields are indexed with term vectors enabled.
If  true, the builtin document sources will additionally store the body as UTF-8 encoded bytes.
Used by certain document sources as the root directory for finding document files in the filesystem.
Used by LineDocMaker and WriteLineFile as the file for single line documents.
If true then a single shared instance of Document, and a single shared instance of Field for each field in the document, is re-used.
This gains performance by avoiding allocation and GC costs, however if you create a custom tasks that adds documents to an index using private threads you will need to turn this off.
The normal parallel task sequence, which also uses threads, may leave  this at true because the single instance is per thread.
This is the file that contains one text query per line.
How often to print the progress line, as measured by number of docs created.
How often to print the progress line, as measured by number of docs deleted.
If true, the queries returned by the query maker are printed.
Set this to a lower number to limit how many tasks log.
How often to print the progress line, as measured in number of docs analyzed.
D.2.1 Document Maker The setting doc.maker defines the class to use for generating documents, for algorithms that consume documents with the AddDoc task.
This class is instantiated once, globally, and then all tasks will pull documents from this source.
You can also create your own document source by subclassing DocMaker.
However, take care to make your class thread safe since multiple threads will share a single instance of your DocMaker.
DirDocMaker Recursively walks a root directory (specified as docs.dir setting), opening any file ending with extension .txt and yields the.
The first line of each file should contain the date; the second line should contain the title and the rest of the document is the body.
LineDocMaker Opens a single file, specified as the setting docs.file, and reads one document per line.
Generally this DocMaker has far less overhead in creating documents than the others since it minimizes the IO cost by only working with a single file.
Unlike other DocMakers, this DocMaker does not accept a parameter specifying the max size of the body field.
Instead, you should pre-create a line file, using the WriteLineDoc task, with the target document size.
Generates documents directly from the large XML export provided by http://wikipedia.org.
The ant task "get-files" retrieves and unpacks the Reuters corpus.
Documents are created as *.txt files under the output directory work/reuters-out.
The setting docs.dir, defaulting to work/reuters-out, specifies the root location of the unpacked corpus.
This assumes you have already unpacked the Trec into the directory set by docs.dir.
SimpleDocMaker Trivial doc maker to be used only for testing.
This generates a single document with a small fixed English text.
D.2.2 Query maker The query.maker  setting determines which class to use for generating queries.
ReutersQueryMaker Generates a small fixed set of 10 queries that roughly match the Reuters corpus.
Now we’ll talk about the available control structures in an algorithm, which is really the “glue” that allows you to take builtin tasks and combine them in interesting ways.
A parallel sequence runs the enclosed tasks with as many threads as there are tasks, with each task running in its own thread.
Repeating a task multiple times is achieved by appending :N to the end.
Use * to pull all documents from the doc maker.
Repeating a task for a specified amount of time is achieved by appending :Xs to the end.
This defines a single AddDoc call "My Name", and then runs that task 1000 times.
AddDoc takes a numeric parameter indicing the size of the added document, in characters.
The body of each document from the docmaker will be truncated to this size, with the leftover being prepended to the next document.
This requires that the doc maker supports changing the document size.
DeleteDocs takes a numeric parameter indicating the document number to be deleted.
SetProp takes a name,value, and changed the named property to the specified value.
SearchTravRetTAsk and SearchTravTask take a numeric parameter which is the required traversal size.
Rate limiting: in addition to specifying how many times a task or task sequence should be repeated, you can.
Do this by adding : N : R after the task.
Disable counting for a task: each tasks contributes to the records count that is used for reporting at the end.
Sometimes you do not want to include the count of a task in your final report.
D.4 Builtin tasks We’ve discussed the settings and the control structures, or glue, that allows you to combine tasks into larger sequences of tasks.
Make sure that your new task class name is suffixed by Task.
Report tasks run after this point will only include statistics from tasks run after this task.
This command makes most sense at the end of an outermost sequence.
All tasks that start will record this new round count and their statistics would be aggregated under that new round count.
In addition, NewRound moves to the next setting if the setting specified different settings per round.
Note that if you have more rounds than number of settings, it simply wraps around to the first setting again.
ResetInputs Re-initializes the document and query sources back to the start.
For example, it's a good idea to insert this call after NewRound to make sure your document source feeds the exact same documents for each round.
This is only necessary when you are not  running your document source to exhaustion.
ResetSystemErase Reset all index and input data, and call System.gc()
You must call CreateIndex to create a new index after this call, if you intend to add documents to an index.
ResetSystemSoft Just like ResetSystemErase, except the index and Directory are not erased.
This is useful for testing performance of opening an existing index for searching or updating.
You can then use the AddDoc task to add documents to the index.
You can then use the AddDoc task to add documents to the index.
This task optionally takes an integer parameter, which is the maximum number of segments to optimize.
This task takes an optional Boolean parameter specifying whether Lucene should wait for running merges to complete.
OpenReader Create an IndexReader and IndexSearcher, available for the search tasks.
If a Read task is invoked,  it will use the currently open reader.
If no reader is open, it will open its own reader, perform its task, and then close the reader.
This enables testing of various scenarios: sharing a reader, searching with a "cold" reader, with a "warm" reader, etc.
The read tasks affected by this are: Warm, Search, SearchTrav (search and traverse) and SearchTravRet (search, traverse and retrieve)
Note that each of the 3 search tasks maintains its own queryMaker instance.
This task takes a single parameter, which is a comma separated list of class names.
Each time this task is executed, it will switch to the next analyzer in its list, rotating back to the start if it his the end.
If the reader is already opened (with the OpenReader task), it's searched.
Otherwise a new reader is opened, searched, and hten closed.
This task simply issues the search but does not traverse the results.
This task takes an optional  integer parameter, which is the number of Hits to visit.
If no parameter is specified, the full result set is visited.
This task returns as its count the number of documents visited.
SearchTravRet search an index and traverse and retrieve the results.
Like SearchTrav, except for each hit visited the corresponding document is also retrieved from the index.
Like SearchTrav, excep this task takes an optional comma-separated string parameter specifying which fields of the document should be retrieved.
This task taskes a comma-seperated parameter list to control details of the highlighting.
Settings are normally globally set in your algorithm file, but this task can be used to change a setting at a specific point.
All tasks executed after this point will use the new setting.
Warm warms up a previously opened searcher by retrieving all documents in the index.
Note that in a real application, this is not sufficient as you would also want to pre-populate the FieldCache if you are using it, and possibly issue initial searches for commonly searched for terms.
DeleteDoc delete a document by document ID, or by incrementing step size to compute the document ID to be deleted.
If the parameter is non-negative then this is a fixed document ID to delete.
ReadTokens this tasks tests the performance of just the analyzer's tokenizer.
It simply reads the next document from the document maker and fully tokenizes all of its fields.
As the count this task returns the number of tokens encountered.
This is a useful task to measure cost of document retrieval and tokenization.
By subtracting this cost from the time spent building an index you can get a rough measure of what the actual indexing cost is.
WriteLineDocTask used to create a line file that can then be used by LineDocMaker.
D.4.1 Creating and using line files Line files are simple text files that contain one document per line.
Indexing documents from a line file incurs quite a bit less overhead than other approaches such as one file per document, or pulling files from a database, etc.
This is important if you are trying to measure performance of just the core indexing process.
If instead you are trying to measure indexing performance from your specific document source then you should not use a line file! The benchmark framework provides a simple task, WriteLineDocTask, to create line files.
Using this you can translate any document source into a line file.
However, the one limitation is that each document only has a date, title, and body field.
For example, use this algorithm to translate the Reuter's corpus into a single line file.
D.4.2 Builtin reporting tasks Report tasks generate a summary report at the end of the algorithm, showing how many records per second were achieved, how much memory was used, one line per task or task sequence that gathered statistics.
The Report tasks themselves are not measured and not reported.
If needed, additional reports can be added by extending the abstract class ReportTask, and by manipulating the statistics data in Points and TaskStats.
RepSelectByPref prefix all records for tasks whose name start with prefix.
RepSumByPref prefix all records for tasks whose name start with prefixWord aggregated by their full task name.
RepSumByNameRound name all statistics, aggregated by name and by round.
See more about rounds in the NewRound task description below.
RepSumByPrefRound prefix similar to RepSumByNameRound,  except only tasks whose name starts with prefixWord are included.
D.5 Evaluating search quality How do you test the relevance or quality of your search application?  Relevance testing is crucial because, at the end of the day, your users will not be satisfied if they don’t get relevant results.
Many small changes to how you use Lucene, from the analyzer chain, to which fields you index, to how you build up a Query, to how you customize scoring, can have large impacts on relevance.
Being able to properly measure such effects allows you to make changes that improve your relevance.
Yet, despite being the most important aspect of a search application, quality is devilishly difficult to pin down.
You could run a controlled user trial, or you can play with the application yourself.
What do you look for?  Besides checking if the returned documents are relevant, there are many other things to check: are the excerpts accurate?  Is the right metadata presented?  Is the UI easily consumed on quick glance?  No wonder so few applications are tuned for their relevance! That said, if you’d like to objectively measure the relevance of returned documents, you’re in luck: recent additions to the benchmark framework, under the quality package, allow you to do so.
These classes provide concrete implementations based on the formats from the TREC corpus, but you can also implement your own.
You’ll need a “ground truth” transcribed set of queries, where each query lists the documents that are relevant to it.
This approach is entirely binary: a given document from the index is either relevant or not.
D.5.1 Precision and recall Precision and recall are standard metrics in the information retrieval community for objectively measuring relevance of search results.
Precision measures what subset of the documents returned for each query were relevant.
Recall measures what percentage of the relevant documents for that query was actually returned.
In a properly configured search application, these two measures are naturally at odds with one another.
Let’s say, on one extreme, you only show the user the very best (top 1) document matching their query.
With such an approach, your precision will typically be high, because the first result has a good chance of being relevant, while your recall would be very low, because if there are many relevant documents for a given query you have only returned one of them.
The precision will necessarily drop because most likely you are now allowing some non-relevant documents into the result set.
But recall should increase because each query should return a larger subset of its relevant documents.
Still, you’d like the relevant documents to be higher up in the ranking.
This measure computes precision at each of the N cutoffs, where N ranges from 1 to a maximum value, and then takes the average.
So this measure is higher if your search application generally returns relevant documents earlier in the result set.
Mean average precision, or MAP, then measures the mean of average precision across a set of queries.
A related measure, mean reciprocal rank or MRR, measures 1/M where M is the first rank that had a relevant document.
You want both of these numbers to be as high as possible!
Listing D.1 shows how to use the quality package to compute precision and recall.
Currently, in order to measure search quality, you must write your own Java code (ie, there are no builtin tasks for doing so, that would allow you to solely use an algorithm file)
The queries to be tested are represented as an array of QualityQuery instances.
The TrecTopicsReader knows how to read the TREC topic format, into QualityQuery instances, but you could also implement your own.
Next, the ground truth is represented with the simple Judge interface.
Finally, QualityBenchmark tests the queries by running them against a provided IndexSearcher.
It returns and array of QualityStats, one each for each of the queries.
When you run the code in Listing D.1, by entering ant PrecisionRecall at the command line within the book’s source code directory, it will produce something like this:
Precision then gets worse beyond the top 3 results because any further document is incorrect.
Recall is perfect (1.0) because all three correct documents were returned.
For ideas on how to improve relevance, have a look at this recently added page on the Lucene wiki:
D.5 Errors If you make a mistake in writing your algorithm, which is in fact very easy to do, you’ll see a somewhat cryptic exception like this:
D.6 Summary As we’ve seen, the benchmark package is a powerful framework for quickly creating performance tests and for evaluating your search application for precision and recall.
It saves you tons of time because all of the normal overhead in creating a performance test is handled for you.
Combine this with the large library of built-in tasks for common indexing and searching operations, plus extensibility to add your own report, task or a document or query source, and you’ve got one very useful tool under your belt.
Please post comments or corrections to the Author Online forum at http://www.manning-sandbox.com/forum.jspa?forumID=451
Chapter 10: Lucene performance tuning and administration Apples and Oranges.
