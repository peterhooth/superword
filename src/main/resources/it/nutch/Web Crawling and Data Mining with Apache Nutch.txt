Perform web crawling and apply data mining in your application.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the authors, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Zakir Laliwala is an entrepreneur, an open source specialist, and a hands-on CTO at Attune Infocom.
Attune Infocom provides enterprise open source solutions and services for SOA, BPM, ESB, Portal, cloud computing, and ECM.
At Attune Infocom, he is responsible for product development and the delivery of solutions and services.
He explores new enterprise open source technologies and defines architecture, roadmaps, and best practices.
He has provided consultations and training to corporations around the world on various open source technologies such as Mule ESB, Activiti BPM, JBoss jBPM and Drools, Liferay Portal, Alfresco ECM, JBoss SOA, and cloud computing.
He was an adjunct faculty at Dhirubhai Ambani Institute of Information and Communication Technology (DA-IICT), and he taught Master's degree students at CEPT.
He has published many research papers on web services, SOA, grid computing, and the semantic web in IEEE, and has participated in ACM International Conferences.
He serves as a reviewer at various international conferences and journals.
He has also published book chapters and written books on open source technologies.
Abdulbasit Shaikh has more than two years of experience in the IT industry.
He has a lot of experience in open source technologies.
He has also delivered projects and training on open source technologies.
He has a very good knowledge of cloud computing, such as AWS and Microsoft Azure, as he has successfully delivered many projects in cloud computing.
He is a very enthusiastic and active person when he is working on a project or delivering a project.
He is totally focused on open source technologies, and he is very much interested in sharing his knowledge with the open source community.
He is the President of SHMsoft, a provider of Hadoop applications for various verticals.
He is a co-founder of the Hadoop Illuminated training and consulting firm, and the co-author of the open source Hadoop Illuminated book.
He has authored and coauthored a number of books and patents.
I would like to acknowledge the help of my colleagues, in particular Sujee Maniyam, and last but not least, my multitalented family.
His current interests lie in the areas of databases and distributed systems.
He received his Bachelor's degree from the College of Engineering Guindy, Anna University, Chennai and has two years of work experience.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Preface Apache Nutch is an open source web crawler software that is used for crawling websites.
It provides facilities for parsing, indexing, and scoring filters for custom implementations.
This book is designed for making you comfortable in applying web crawling and data mining in your existing application.
It will demonstrate real-world problems and give the solutions to those problems with appropriate use cases.
This book will demonstrate all the practical implementations hands-on so readers can perform the examples on their own and make themselves comfortable.
The book covers numerous practical implementations and also covers different types of integrations.
What this book covers Chapter 1, Getting Started with Apache Nutch, covers the introduction of Apache Nutch, including its installation, and guides you for crawling, parsing, and creating plugins with Apache Nutch.
By the end of this chapter, you will be able to install Apache Nutch in your own environment, and also be able to crawl and parse websites.
Additionally, you will be able to create a Nutch plugin.
It also covers how sharding can take place with Apache Nutch using Apache Solr as a search tool.
By the end of this chapter, you will be able to deploy Apache Solr on a server that contains the data crawled by Apache Nutch and also be able to perform sharding using Apache Nutch and Apache Solr.
You will also be able to integrate AJAX with your running Apache Solr instance.
By the end of this chapter, you will be able to set up Apache Nutch running on Apache Hadoop in your own environment and also be able to perform crawling in Apache Nutch using Eclipse.
By the end of this chapter, you will be able to integrate Apache Nutch with Apache Accumulo as well as with MySQL.
After that, you can perform crawling using Apache Nutch on Apache Accumulo and also on MySQL.
You can also get the results of your crawled pages on Accumulo as well as on MYSQL.
You can integrate Apache Solr too, as we have discussed before, and get your crawled pages indexed onto Apache Solr.
What you need for this book You will require the following software to be installed before starting with the book:
Who this book is for This book is for those who are looking to integrate web crawling and data mining into their existing applications as well as for the beginners who want to start with web crawling and data mining.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "clicking the Next button moves you to the next screen"
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/ support.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Apache Nutch is a very robust and scalable tool for web crawling; it can be integrated with the scripting language Python for web crawling.
You can use it whenever your application contains huge data and you want to apply crawling on your data.
This chapter covers the introduction to Apache Nutch and its installation, and also guides you on crawling, parsing, and creating plugins with Apache Nutch.
It will start from the basics of how to install Apache Nutch and then will gradually take you to the crawling of a website and creating your own plugin.
By the end of this chapter, you will be comfortable playing with Apache Nutch as you will be able to configure Apache Nutch yourself in your own environment and you will also have a clear understanding about how crawling and parsing take place with Apache Nutch.
Additionally, you will be able to create your own Nutch plugin.
Introduction to Apache Nutch Apache Nutch is open source WebCrawler software that is used for crawling websites.
You can create your own search engine like Google, if you understand Apache Nutch clearly.
It will provide you with your own search engine, which can increase your application page rank in searching and also customize your application searching according to your needs.
It facilitates parsing, indexing, creating your own search engine, customizing search according to needs, scalability, robustness, and ScoringFilter for custom implementations.
ScoringFilter is a Java class that is used while creating the Apache Nutch plugin.
We can run Apache Nutch on a single machine as well as on a distributed environment such as Apache Hadoop.
We can find broken links using Apache Nutch and create a copy of all the visited pages for searching over, for example, while building indexes.
We can find web page hyperlinks in an automated manner.
Apache Nutch can be integrated with Apache Solr easily and we can index all the web pages that are crawled by Apache Nutch to Apache Solr.
We can then use Apache Solr for searching the web pages which are indexed by Apache Nutch.
Apache Solr is a search platform that is built on top of Apache Lucene.
It can be used for searching any type of data, for example, web pages.
Installing and configuring Apache Nutch In this section, we are going to cover the installation and configuration steps of Apache Nutch.
So we will first start with the installation dependencies in Apache Nutch.
After that, we will look at the steps for installing Apache Nutch.
Finally, we will test Apache Nutch by applying crawling on it.
In the latter, Apache Nutch developers create a crawl script that will do crawling for us by just running that script; there is no need to type commands step-by-step.
There may be more differences but I have covered just one.
I have used Apache Nutch 2.2.1 because it is the latest version at the time of writing this book.
The steps for installation and configuration of Apache Nutch are as follows:
Here $NUTCH_HOME is the directory where your Apache Nutch resides.
HBase is the Apache Hadoop database that is distributed, a big data store, scalable, and is used for storing large amounts of data.
You should use Apache HBase when you want real-time read/write accessibility of your big data.
Here, we will use Apache HBase for storing data, which is crawled by Apache Nutch.
Then we can log in to our database and access it according to our needs.
It will extract all the files in the respective folder.
Otherwise you might face an issue while running Apache HBase.
Find the database in which all the data related to HBase will reside:
Make sure that the HBasegora-hbase dependency is available in ivy.xml.
Put the following configuration into the ivy.xml file: <!-- Uncomment this to use HBase as Gora backend.
Make sure that HBaseStore is set as the default data store in the gora.
Go there and type the following command from your terminal: ant runtime.
This will build your Apache Nutch and create the respective directories in the Apache Nutch's home directory.
It is needed because Apache Nutch 2.x is only distributed as source code.
The tree structure of the generated directories would be as shown in the following diagram:
The preceding diagram shows the directory structure of Apache Nutch, which we built in the preceding step.
The build directory contains all the required JAR files that Apache Nutch has downloaded at the time of building.
The conf directory contains all the configuration files which are required for crawling.
The docs directory contains the documentation that will help the user to perform crawling.
The ivy directory contains the required configuration files in which the user needs to add certain configurations for crawling.
The runtime directory contains all the necessary scripts which are required for crawling.
The src directory contains all the Java classes on which Apache Nutch has been built.
Ant is the tool which is used for building your project and which will resolve all the dependencies of your project.
It will fetch the required JAR files from the Internet by running the build.xml file that is required for running Ant.
So when you type ant at runtime, it will search for the build.xml file in the directory from where you have hit this command, and once found, it will fetch all the required JAR files that you have mentioned in build.xml.
You have to install Ant if it is not installed already.
You can refer to http://www.ubuntugeek.com/howto-install-ant-1-8-2-using-ppa-on-ubuntu.html for a guide to the installation of Ant.
HBase is running properly, go to the home directory of Hbase.
If everything goes well, you will get an output as follows:
Now you should be able to use it by going to the bin directory of Apache Nutch.
The local directory contains all the configuration files which are required to perform crawling.
The runtime directory contains the local directory and the deploy directory.
Verifying your Apache Nutch installation Once Apache Nutch is installed, it is important to check whether it is working up to the mark or not.
The steps for verifying Apache Nutch installation are as follows:
Go to the local directory of Apache Nutch from your terminal.
If everything is successful, you will get the output as follows:
Run the following command if you see a Permission denied message: chmod +x bin/nutch.
On your Mac system, you can run the following command or add it to ~/.bashrc.
Crawling your first website We have now completed the installation of Apache Nutch.
It's now time to move to the key section of Apache Nutch, which is crawling.
Crawling is driven by the Apache Nutch crawling tool and certain related tools for building and maintaining several data structures.
It includes web database, the index, and a set of segments.
Once Apache Nutch has indexed the web pages to Apache Solr, you can search for the required web page(s) in Apache Solr.
The nutch-site.xml file is the configuration file from where Apache Nutch will fetch the necessary details at the time of crawling.
We will define different properties in this file, as you will see in the following code snippet.
Create a directory called urls inside it by following these steps:
Find the command for creating the urls directory as follows: #mkdir –p urls.
If you are a Windows user, the following command should be used: #mkdir urls.
The following  command will take you inside the urls directory:
Now create the seed.txt file inside the urls directory and put the following content: http://nutch.apache.org/
You can put n number of URLs but one URL per line.
You can comment by putting # at the start of the line.
An example would be as follows: # Your commented text is here.
So whenever crawling is performed for the URL, Apache Nutch will match the respective URL that we are putting inside seed.txt, with the pattern defined in this file for that URL and crawl accordingly.
As you will see shortly, we have applied crawling on http://nutch.apache.org and we have set the pattern inside this file.
It can be used for searching any type of data, for example, web pages.
It's a very powerful searching mechanism and provides full-text search, dynamic clustering, database integration, rich document handling, and much more.
Apache Solr will be used for indexing URLs which are crawled by Apache Nutch and then one can search the details in Apache Solr crawled by Apache Nutch.
This will extract all the files of Apache Solr in the respective folder.
To open this file, go to the root directory from your terminal and type the following command: gedit ~/.bashrc.
This classpath variable is required for Apache Solr to run.
When you start Apache Solr, it will search for this variable inside your .bashrc file for locating your Apache Solr and it will give an error if something goes wrong in the configuration.
You will find this directory located in your Apache Solr's home directory.
Type the following command to start Apache SOLR: java -jar start.jar.
Verify Apache Solr installation by hitting the following URL on your browser:
You will get the image of Running Apache Solr on your browser, as shown in the following screenshot:
So once Apache Nutch finishes with crawling and indexing URLs to Apache Solr, you can search for particular documents on Apache Solr and get the expected results.
You will find this directory in your Apache Solr's home directory.
Crawling your website using the crawl script Apache Nutch 2.2.1 comes with the crawl script facility which does crawling by just executing one single script.
In the earlier version, we had to manually perform the steps of generating data, fetching data, parsing data, and so on for performing crawling.
The steps for crawling your website using the crawl script are as follows:
Go to the home directory of HBase from your terminal.
You will find this directory located at the same location where your HBase resides.
If you get the following output, it means HBase is already started.
Now go to the local directory of Apache Nutch from your terminal and perform some operations by typing the following command.
TestCrawl: This is the crawl data directory which will be automatically created inside Apache Hbase with the name TestCrawl_Webpage, which will contain information on all the URLs which are crawled by Apache Nutch.
This is the number of iterations, which will tell Apache Nutch in how many iterations this crawling will end.
The crawl script has a lot of parameters to be set; it would be good to understand the parameters before setting up big crawls.
Because you can use these parameters according to your requirements, you have to first study these parameters and then apply them.
Crawling the Web, the CrawlDb, and URL filters Crawling the Web is already explained above.
When a user invokes a crawling command in Apache Nutch 1.x, CrawlDB is generated by Apache Nutch which is nothing but a directory and which contains details about crawling.
Instead, Apache Nutch keeps all the crawling data directly in the database.
In our case, we have used Apache HBase, so all crawling data would go inside Apache HBase.
The following are details of how each function of crawling works.
A crawling cycle has four steps, in which each is implemented as a Hadoop MapReduce job:
First of all, the job of an Injector is to populate initial rows for the web table.
The InjectorJob will initialize crawldb with the URLs that we have provided.
We need to run the InjectorJob by providing certain URLs, which will then be inserted into crawlDB.
Then the GeneratorJob will use these injected URLs and perform the operation.
The table which is used for input and output for these jobs is called webpage, in which every row is a URL (web page)
The row key is stored as a URL with reversed host components so that URLs from the same TLD and domain can be kept together and form a group.
In most NoSQL stores, row keys are sorted and give an advantage.
Using specific rowkey filtering, scanning will be faster over a subset, rather than scanning over the entire table.
Let's define each step in depth so that we can understand crawling step-by-step.
Apache Nutch contains three main directories, crawlDB, linkdb, and a set of segments.
If it is fetched, crawlDB contains the details when it was fetched.
The linkdatabase or linkdb contains all the links to each URL which will include source URL and also the anchor text of the link.
A set of segments is a URL set, which is fetched as a unit.
A crawl_generate job will be used for a set of URLs to be fetched.
A crawl_fetch job will contain the status of fetching each URL.
A content will contain the content of rows retrieved from every URL.
InjectorJob The Injector will add the necessary URLs to the crawlDB.
You need to provide URLs to the InjectorJob either by downloading URLs from the Internet or by writing your own file which contains URLs.
Let's say you have created one directory called urls that contains all the URLs needed to be injected in crawlDB; the following command will be used for performing the InjectorJob:
After performing this job, you will have a number of unfetched URLs inside your database (crawlDB)
GeneratorJob Once we are done with the InjectorJob, it's time to fetch the injected URLs from CrawlDB.
So for fetching the URLs, you need to perform the GeneratorJob first.
FetcherJob The job of the fetcher is to fetch the URLs which are generated by the GeneratorJob.
Here I have provided input parameters—this means that this job will fetch all the URLs that are generated by the GeneratorJob.
You can use different input parameters according to your needs.
ParserJob After the FetcherJob, the ParserJob is to parse the URLs that are fetched by FetcherJob.
I have used input parameters—all of which will parse all the URLs fetched by the FetcherJob.
You can use different input parameters according to your needs.
DbUpdaterJob Once the ParserJob has completed its task, we need to update the database by providing results of the FetcherJob.
This will update the respective databases with the last fetched URLs.
I have provided input parameters, all of which will update all the URLs that are fetched by the FetcherJob.
You can use different input parameters according to your needs.
After performing this job, the database will contain updated entries of all the initial pages and the new entities which correspond to the newly discovered pages that are linked from the initial set.
Invertlinks Before applying indexing, we need to first invert all the links.
After this we will be able to index incoming anchor text with the pages.
Indexing with Apache Solr At the end, once crawling has been performed by Apache Nutch, you can index all the URLs that are crawled by Apache Nutch to Apache Solr, and after that you can search for the particular URL on Apache Solr.
The following command will be used for indexing with Apache Solr:
Parsing and parse filters Parsing is a task or process by which a parse object is created and populated within the data.
Parsing contains the parsed text of each URL, the outlink URLs used to update crawldb, and outlinks and metadata parsed from each URL.
Parsing is also done by crawl script, as explained earlier; to do it manually, you need to first execute inject, generate, and fetch commands, respectively:
The preceding commands are the individual commands required for parsing.
To perform these, go to the Apache Nutch home directory.
The HtmlParseFilter permits one to add additional metadata to HTML parses.
Webgraph Webgraph is a component which is used for creating databases.
It will create databases for inlinks, outlinks, and nodes that are used for holding the number of outlinks and inlinks to a URL and for the current score of the URL.
Webgraph will run once all the segments are fetched and ready to be processed.
If you only type #bin/nutch webgraph, it will show you the usage as follows:
Loops Loops is used for determining spam sites by determining link cycles in a Webgraph.
So once the Webgraph is completed, we can start the process of link analysis.
An example of a link cycle is, P is linked to Q, Q is linked R, R is linked S, and then again S is linked to P.
Due to its large expense and time and space requirements, it cannot be run on more than four levels.
It helps the LinkRank program to identify spam sites, which can then be discounted in later LinkRank programs.
There can be another way to perform this function with a different algorithm.
It is just placed here for the purpose of completeness.
LinkRank LinkRank is used for performing an iterative link analysis.
It is used for converging to stable global scoring for each URL.
It starts with a common scoring for each URL like PageRank.
It creates a global score for each and every URL based on the number of incoming links and also scores for those links and total outgoing links from the page.
It is an iterative process and scores converge after a given number of iterations.
It differs from PageRank in the way that links internal to a website and reciprocal links in between websites could be ignored.
Unlike the OPIC scoring, the LinkRank program doesn't track scores from one processing time to another.
Both Webgraph and link scores are recreated on each processing run.
So we do not have any problems in increasing scores.
LinkRank wants the Webgraph program to be completed successfully and stores the output scoring of each URL in the node database of the Webgraph.
ScoreUpdater After completing the LinkRank program and link analysis, your scores must be updated inside the crawl database working with the current Apache Nutch functionality.
The ScoreUpdater program stores the scores in the node database of the Webgraph and updates them inside crawldb.
A scoring example This example runs the new scoring and indexing systems from start to end.
The package contains multiple programs that will build web graphs, performing a stable convergent link analysis, and updating crawldb with those scores.
For doing scoring, go to the local directory from the terminal.
Webgraph will be used on larger web crawls to create web graphs.
The following options are interchangeable with their corresponding configuration options:
But by default, if you are doing only crawling of pages inside a domain or inside a set of subdomains, all the outlinks will be ignored and you come up having an empty Webgraph.
The Apache Nutch plugin The plugin system displays how Nutch works and allows us to customize Apache Nutch to our personal needs in a very flexible and maintainable manner.
But writing an own plugin will be a challenge at one point or another.
But simply imagine you would like to add a new field to the index by doing some custom analysis of a parsed web page content to Solr as an additional field.
The Apache Nutch plugin example This example will focus on the urlmeta plugin.
Its aim is to provide a comprehensive knowledge of the Apache Nutch plugin.
This example covers the integral components required to develop and use a plugin.
As you can see, inside the plugin directory located at $NUTCH_HOME/src/, the folder urlmeta contains the following:
A build.xml file that tells Ant how to build your plugin.
An ivy.xml file containing either the description of the dependencies of a.
A /src directory containing the source code of our plugin with the directory structure is shown in the hierarchical view, as follows:
Now we need to configure the plugin.xml, build.xml, and ivy.xml files.
Modifying plugin.xml Your plugin.xml file should look like the following:
The preceding code defines ID, name, version, and provider name of your plugin.
The preceding code defines the library, which is a JAR file and export command of your plugin.
The preceding code is used for importing nutch-extension points for your plugin.
The preceding code is used for defining extension ID, extension name, extension point, implementation ID, and implementation class for your plugin.
The preceding configuration is written to tell Apache Nutch about your plugin and register your plugin with Apache Nutch.
In its simplest form, the preceding configuration looks as follows:
The preceding code will be used for building your plugin using Ant.
Describing dependencies with the ivy module The ivy.xml file is used to describe the dependencies of the plugin on other libraries:
The preceding configuration contains either the description of the dependencies of a module, its published artifacts and its configurations, or else the location of another file which specifies this information.
IndexingFilter is the extension point and an interface for adding metadata into the search index.
If the document being indexed has a recommended metatag, this extension adds a Lucene text field to the index called recommended with the content of that metatag.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.
Using your plugin with Apache Nutch So the plugin has already been created.
For that you need to make certain configurations with Apache Nutch.
It will configure your plugin with Apache Nutch and after that you are able to use it as and when required.
For that you need to edit your nutch-site.xml file by in putting the following content, which you will find in $NUTCH_HOME/conf.
Compiling your plugin And the last step of creating the Apache Nutch plugin is to compile your plugin.
Once it is compiled, your newly created plugin will be indexed to Apache Solr and then you can search for the field, which is added as your plugin on Apache Solr; Apache Solr will give you the result as a separate document.
So you need to compile and deploy your plugin by Ant.
For that, you need to edit build.xml by in putting the following content.
Both scoring and indexing extensions will be used, which will enable us to search for metatags within our Solr index.
You can go to Apache Solr on the browser by typing localhost:<PORT> and do an all query search.
You will get your plugin indexed into the Apache Solr as follows:
The following screenshot shows that the plugin has been successfully created:
Understanding the Nutch Plugin architecture The following diagram illustrates the Apache Nutch plugin architecture which will show you how Apache Nutch functions.
It will elaborate on searcher, indexer, web DB, and fetcher.
It will provide you with the flow, how data is flowing from one stage to the next.
Therefore, understanding this architecture is a must to understanding the functioning of Apache Nutch.
So, have a look at the image below and you will get an overview of Apache Nutch plugin architecture.
I have also explained each component in detail, which will help you to understand them in depth:
Enter some text into the QueryString box and click on Search.
If your query matches any result, you should see an XML file containing the indexed pages of your websites.
Indexer: This creates the inverted index from which the searcher extracts results.
Web DB: Web DB stores the document contents for indexing and later summarization by the searcher, along with information such as the link structure of the document space and the time each document was last fetched.
Fetcher: Fetcher requests web pages, parses them, and extracts links from them.
Let's discuss briefly what you have learned in this chapter.
Then we moved on to the installation and verification of Apache Nutch and saw how crawling takes place with Apache Nutch.
Then we had an introduction to Apache Solr and saw how Apache Solr can be integrated with Apache Nutch to index the data crawled by Apache Nutch.
We covered crawling, crawling the web, and CrawlDB, as well as parsing and parse filters.
And finally we saw how the Apache Nutch plugin can be created and also learned the Apache Nutch plugin architecture.
Now let's see how deployment of Apache Nutch takes place and how we can apply sharding on Apache Solr in the next chapter.
Deployment, Sharding, and AJAX Solr with Apache Nutch We have discussed the installation of Apache Nutch, crawling websites, and creating a plugin with Apache Nutch in the first chapter.
This chapter covers deployment of Apache Solr on a particular server, such as Apache Tomcat, Jetty.
Also, this chapter covers how sharding can take place with Apache Nutch using Apache Solr as a searcher.
In this chapter, we are going to cover following topics:
By the end of this chapter, you will be able to deploy Apache Solr on a server which contains the data crawled by Apache Nutch.
You will also be able to perform sharding with data of Apache Nutch using Apache Solr.
You will also be able to integrate AJAX with your running Apache Solr instance.
Deployment of Apache Solr This part covers all the necessary steps which are required for performing deployment of Apache Solr.
Introduction of deployment Installing, testing, and implementing a computer system or application is called deployment.
This term can be used for any installation and testing, such as setting up a new network in an enterprise, installing a server farm, or implementing a new application on a distributed computing network.
So after successful deployment of Apache Solr on Apache Tomcat, it's quite possible that by starting Apache Tomcat, Apache Solr will automatically start.
It's a very important process because once Apache Nutch has crawled all the web pages for you and indexed them to Apache Solr, then the process needs to be started which will deploy Apache Solr on Apache Tomcat.
After successful deployment of Apache Solr with Apache Tomcat, you can iterate on Apache Solr over a browser and can start searching on web pages which are crawled by Apache Nutch.
In short, you can make your own search engine by customizing Apache Solr.
This is a very basic need for any web-based application because every application needs this type of scenario.
You can integrate the deployed Apache Solr with your running application and get the benefit out of it.
But the question is what is the need for that? I will give you an answer.
Let's say you have crawled a number of websites using Apache Nutch.
So, Apache Nutch has crawled a lot of pages for you.
Now what? Here is when this deployment of Apache Solr comes into the picture.
I will cover the sharding part in a later section.
It's a kind of mechanism that divides the total number of URLs to different shards for bringing efficiency in Apache Solr searching.
So, it's a very good concept that we are going to discuss in a further section.
But for now, I can say that it's very much compulsory for Apache Solr to be deployed on any server as Apache Tomcat.
These platforms were released by Oracle Corporation in the form of binary products aimed at Java developers on Solaris, Linux, Mac OS X, or Windows.
So be sure you have JDK installed on your system.
It will help you to set up the JDK and JAVA_HOME variable.
It implements the Java Servlet and JavaServer Pages (JSP) specifications provided by Sun Microsystems and also provides a pure Java HTTP web server environment for Java code to run.
It includes tools for configuration and management, but you can configure it by editing the XML configuration files.
The following steps are given for the set up of Apache Tomcat:
Download the Apache Tomcat server from its official website at http://tomcat.apache.org.
You can stop Apache Tomcat by executing the following commands:
The other method for stopping Apache Tomcat is by typing the following command on the terminal where your Apache Tomcat is currently running:
The following steps are given for setting up Apache Solr:
Make a folder with the name SOLR_HOME in the respective folder of your choice.
Let's say I have created the SOLR_HOME directory in /usr/local.
So, we have set up Apache Solr in the SOLR_HOME directory.
We have also copied all the necessary files to this directory.
Copy these two directories and paste them in the Solr_Home directory.
If Solr has already started and if lib directory is not copied, you will get an exception as the server error filterstart.
The collection1 and bin directories contain the necessary configuration files.
So when you run Apache Solr, if these JARs are not present, then you will not get any logs.
The log4j.properties file contains the required properties for the logging configuration.
We are doing this for running Apache Solr in Tomcat.
So, this WAR file will be deployed into Apache Tomcat when Apache Tomcat starts for the first time, and it will create the directory called solr-4.3.0 inside the webapps directory, which is called an application of Apache Solr.
If Apache Tomcat has not started, then just start it as mentioned in the preceding Setting up Tomcat section.
So this configuration will tell Apache Tomcat where Apache Solr resides and it will use that path of Apache Solr whenever Apache Tomcat starts.
Running Solr on Tomcat We are done with all the configurations of Apache Solr for deploying it on Apache Tomcat.
Now, it's time to actually run Apache Solr on Apache Tomcat and check whether it's functioning properly or not.
You need to make sure that all the logs related to Apache Solr need to appear on the Apache Tomcat console when you start working on Apache Solr on the browser.
If it's not appearing, then you need to check your configuration once again for what you have missed:
If successful, you will get an output like the following:
Check the logs on the Apache Tomcat console to see whether they are appearing or not.
The following screenshot will be the output of Apache Tomcat logs:
If not, then you need to check your configuration once again to find out what you have missed.
Sharding using Apache Solr This section is going to describe all the necessary steps which are required to perform sharding using Apache Solr.
Introduction to sharding When your data becomes too large for a single node, you can break it down into a number of sections by creating one or more shards.
Each shard will be a portion of the logical index or core.
A shard is the terminology which means splitting a core data over a number of servers or nodes.
Take an example of representing each state; you might have a shard available for data which represents each state or it might be different categories which you want to make them search independently, but they are actually often combined.
Load balancing comes into the picture whenever load increases in your application.
Load means that the number of requests/hits is increased and your application is not able to handle that many requests.
At this time, load balancing is required which will balance your application load and divide the load among machines, so that your application can run smoothly.
Division of a load happens by transferring an incoming request to different machines.
Whenever any request comes to the master machine, it will transfer that request to any slave machine.
The master machine is not responsible for handling all requests.
Load will decrease from the master machine and it will be divided among the slave machines.
Scalability means whenever load increases, the number of servers increases.
Your application must be scalable enough whenever load increases on it.
So, scalability is required for handling load on your application.
So, it's very important functionality of Apache Solr which is highly used in real-time applications.
If users are limited in performing indexing on your application, then you can apply sharding.
But if the number of users is increased, then sharding might not be a good option.
In case many users are performing indexing simultaneously, then sharding isn't the answer to handling high traffic.
This will distribute complete copies of the master server to one or more slave servers.
The job of the master server will be to update the index and all the queries will be handled by the slave servers.
This division will make Apache Solr scalable and Apache Solr can provide quick responses against queries over large search volumes.
Whenever load increases, more and more servers will be required to handle this load.
This involves a master-slave configuration in which one server would be the master Solr server and the rest are the slave Solr servers.
So, the master server will divide that load among the slave Solr severs whenever load increases on it.
In this case, we would like to set up with multiple Solr instances.
For configuring, we are going to use the schema.xml file which resides inside $SOLR_ HOME which is a directory where your Apache Solr resides.
The following figure shows how load is divided from the master machine to the slave machines:
A request will be first handled by the master server.
The master server will decide to which slave server the request needs to be sent.
Then, the master server will send the particular request to the particular slave server.
Update commands are sent to any server with distributed indexing organized properly.
Whenever a new document needs to be added or deleted, then that request would be forwarded to that slave machine which has the support of a hash function of the distinctive document ID.
The commit and deleteByQuery commands are sent to each server in shards.
Use of sharding with Apache Nutch We are going to see how sharding takes place with Apache Nutch using Apache Solr.
At this point, you must be aware of deployment of Apache Solr on Apache Tomcat.
Let's say you have crawled one million URLs and indexed them to Apache Solr.
Now to make Apache Solr efficient in searching, you need to apply some mechanism on it.
Sharding will divide the traffic by dividing the total number of URLs into different shards, which makes Apache Solr efficient in searching and, in addition, Apache Solr gives the higher throughput while searching.
So in real-time applications, all these components will be integrated and this makes your application efficient in a web environment.
Distributing documents across shards As explained previously, the total number of documents will be divided among a number of shards and each shard will be responsible for a certain amount of documents.
You can get all the documents indexed on every shard of your server.
Solr doesn't embody out of the box support for distributed indexing.
But our technique will be straightforward and you can send documents to any shard for indexing.
By using this approach, you will get an idea that a document needs to be updated or deleted.
Sharding Apache Solr indexes Now, we'd like to pick the right quantity of shards.
A group that is supposed to be designed is one among those variables that we must have before the final deployment.
After making our collection, we can't modify the number of shards.
Of course this comes with consequences—if we have chosen the number of shards incorrectly, we might find ourselves with a very low shards count.
The only way to go would be making a brand new assortment with the correct number of shards and re-index our knowledge.
With the release of Apache Solr 4.3, we are currently ready to split shards of our collections.
Single cluster Now we tend to check the new shard-split functionality.
We try and run a tiny low and single cluster containing a single Apache Solr instance with embedded ZooKeeper and use the instance assortment given with Apache Solr.
If successful, you will get an output like the following:
For starting embedded ZooKeeper, the following argument will be provided: -DzkRun.
For defining the number of shards, the following argument will be provided: -DnumShards=1
You can increase the number of shards by providing an incremented value in the following argument: –DnumShards.
For starting Apache Solr, the following argument will be provided:
In the preceding command, –jar is an argument for defining JAR and start.jar is the actual JAR which will start Apache Solr.
The preceding command will index all XML files which reside in the exampledocs directory of Apache Solr.
Apache Solr needs these XML files to perform sharding on them.
As you can see, we have got 32 documents in our collection.
Splitting shards with Apache Nutch In this section, we will divide a whole shard into a number of shards.
So, the total number of documents will be divided into this number of shards.
And after that instead of having one collection, we will have a different number of shards.
Cleaning up with Apache Nutch First of all in order to see the data in new shards, we need to run the commit command against our collection.
Splitting cluster shards After splitting a shard cluster, we will get an output on Apache Solr on the browser as follows:
The preceding screenshot shows the single shard cluster before splitting.
You can see that traffic is passing from only one shard.
You will see in the following screenshot, which shows the multiple shard clusters, in which traffic is going from multiple shards.
As you see in the first diagram on this page, we have one collection point before cluster shard splitting.
Every shard will contain a portion of documents from the original shard1
So this is how the division of documents is done in Apache Solr sharding.
Checking statistics of sharding with Apache Nutch Now, it's time to see the output on Apache Solr on the browser.
It will show you the statistics of every shard such as how many documents each shard contains and the details related to that shard will be displayed.
For checking statistics of the main shard, that is shard1, use the following URL:
This division of documents is handled by Apache Solr internally.
So, we might not be able to predict which shard contains how many documents.
The final test with Apache Nutch This is the final step for sharding with Apache Solr.
You have to check whether your documents are available in the shards created by the SPLITSHARD action.
Use the following URL to check the availability of documents in shards:
So just check the output, whether the documents which you indexed to Apache Solr are appearing or not.
So that's the end of the sharding with Apache Nutch using Apache Solr.
It's a very useful technique when you work in real-time applications.
So the basic agenda behind this implementation is that you should use this technique in your real-time applications where you have crawled millions of web pages using Apache Nutch and indexed them to Apache Solr.
Apply sharding on them and get the best result from Apache Solr.
As such, you'll use the library whether or not you develop jQuery, MooTools, Prototype, Dojo, or any other framework.
You will have to define only a manager object that extends the provided abstract manager object, and define the function executeRequest() in the object.
We need to understand it very clearly to integrate this with our running Apache Solr.
The Parameter Store is the model, that stores the Solr parameters and thus, the state of the appliance.
The Manager is the controller; it connects to the Parameter Store and asks for the required details, sends requests to Solr, and delegates the response to the widgets for rendering.
Applying AJAX Solr on Reuters' data Reuters is actually a dataset on which we are going to apply the AJAX Solr mechanism.
Before we begin, we tend to write the Hypertext Mark-up Language (HTML) to the JavaScript widgets that can be integrated with each other.
In order to proceed further, this HTML can typically be the non-JavaScript version of your search interface that we would like to improve with unobtrusive JS.
Running AJAX Solr If you want to integrate AJAX with your Apache Solr, you need to do a certain configuration.
So, it's a very useful technique in real-time applications that will load your data on Apache Solr without refreshing the whole web page.
The following steps are given for integrating AJAX Solr with your running local instance of Apache Solr:
Find the solrUrl field in reuters.js and set your running Apache Solr URL there which will be http://localhost:8983/solr by default if you haven't changed anything.
Find facet.field in reuters.js and update it with the fields you want to facet.
Remove all facet.date parameters unless you have used the date field to facet in your Apache Solr.
You can update or remove tag cloud, autocomplete, country code, and calendar widgets from reuters.js.
You can set the associated Solr fields for the tag cloud by changing the value of var fields in reuters.js.
Replace all occurrences of doc.text with doc.content in the template method.
So remove all occurrences of doc.dateline + ' ' +
Now you can enjoy playing with AJAX on Apache Solr.
So this ends the integration of AJAX with your running instance of Apache Solr.
In this, we started with an introduction on deployment and then we moved over to the installation steps and got an idea of JDK, Apache Tomcat, and what the prerequisites for Apache Solr deployment are.
In this, we started with an introduction on sharding and then we covered how sharding takes place on Apache Solr.
And finally, we saw a very key concept called AJAX Solr which can be highly used in real-time applications.
In the next chapter, we will discuss how Apache Nutch can be integrated with Apache Hadoop and how Apache Nutch can be integrated with Eclipse in the same way.
We have also covered integrating AJAX with our running Apache Solr instance.
In this chapter, we will see how we can integrate Apache Nutch with Apache Hadoop, and we will also see how we can integrate Apache Nutch with Eclipse.
Apache Hadoop is a framework which is used for running our applications in a cluster environment.
Eclipse will be used as an Integrated Development Environment (IDE) for performing crawling operations with Apache Nutch.
We will discuss in detail about this in the coming sections.
So, we will first start with the integration of Apache Nutch with Apache Hadoop.
And then, we will move towards the integration of Apache Nutch with Eclipse.
In this chapter, we are going to cover the following topics:
By the end of this chapter, you will be able to set up Apache Nutch on Apache Hadoop in your own environment.
You will also be able to perform crawling in Apache Nutch by using Eclipse.
Then, we will have some basic introduction of integration of Apache Nutch with Apache Hadoop.
Lastly, we will move over to the configuration part of Apache Hadoop and Apache Nutch.
We will also see how we can deploy Apache Nutch on multiple machines.
Introducing Apache Hadoop Apache Hadoop is designed for running your application on servers where there will be a lot of computers, one of them will be the master computer and the rest will be the slave computers.
Master computers are the computers that will direct slave computers for data processing.
This is the reason why Apache Hadoop is used for processing huge amounts of data.
The process is divided into the a number of slave computers, which is why Apache Hadoop gives the highest throughput for any processing.
So, as data increases, you will need to increase the number of slave computers.
Apache Nutch can be easily integrated with Apache Hadoop, and we can make our process much faster than running Apache Nutch on a single machine.
So, the process will be much faster and we will get the highest amount of throughput.
Installing Apache Hadoop and Apache Nutch In this section, we will see how we can configure Apache Hadoop and Apache Nutch in our own environment.
After installing these, you can perform the crawling operation in Apache Nutch, which will be run on the Apache Hadoop cluster environment.
You need to check out the newest version of Nutch from the source after downloading.
As an alternative, you can pick up a stable release from the Apache Nutch website.
Setting up Apache Hadoop with the cluster Apache Hadoop with Cluster setup does not require a huge hardware to be purchased to run Apache Nutch and Apache Hadoop.
It is designed in such a way that it makes the maximum use of hardware.
So, we are going to use six computers to set up Apache Nutch with Apache Hadoop.
My computer configuration in the cluster will have Ubuntu 10.04 installed.
To start our master node we use the reeshucluster01 computer.
By master node, I mean that this will run the Hadoop services, which will coordinate with the slave nodes (the rest of the computers)
And it's the machine on which we will perform our crawl.
We have to set up Apache Hadoop on each of the above clusters.
The steps for setting up Apache Hadoop on these clusters are described in the following sections.
Java is used everywhere—in laptops, data centers, game consoles, scientific supercomputers, cell phones, Internet, and so on.
If your cluster configuration has Java 7 installed, you need to change it accordingly.
The steps for installing Java 6 are given as follows.
Execute the downloaded file, which is prepended by the path to it.
The binary code license is displayed, and you are prompted to agree to its terms.
Delete the bin file if you want to save disk space.
The preceding screenshot will show you that your JDK is successfully installed and your Java is running correctly.
The steps for downloading Apache Hadoop are given as follows:
If you're using Windows, you will have to use an archive program, such as WinZip or WinRar, for extracting.
I have configured Apache Hadoop by taking a separate new user on my system as it's a good practice to do it.
I have created a new user and given permission to Apache Hadoop to allow only this user to access Apache Hadoop.
It's for security purposes (so that no other user can access Apache Hadoop)
Not even the root user is able to access Apache Hadoop.
You have to log in as this user to access Apache Hadoop.
The following command will add a new group called hadoop: $ sudo addgroup hadoop.
The following command will add new user called hduser, and then adds this user to the hadoop group:
Configuring SSH Apache Hadoop needs SSH, which stands for Secure Shell, to manage its nodes, that is, remote machines and your local machine.
It is used to log into the remote system or the local system, and also performs necessary operations on a particular machine.
So, you need to configure this in your local environment as well as in your remote environment for running Apache Hadoop.
The commands and steps for configuring SSH are given as follows:
The following command will be used for logging into the hduser: user@ubuntu:~$ su - hduser.
It will ask for a password, if one is set.
Enter the password and you will be logged into that user.
The following command will be used for generating a key.
This key will be used to provide authentication at the time of login.
Make sure you are logged in as an hduser before firing the following command.
You will get a result as follows: Generating public/private rsa key try.
The key fingerprint is: The key's random art image is:
The second line will make an RSA key pair with an empty password.
It's required for authentication at the time of SSH login.
The final step is to test the SSH setup by connecting to your local machine as the hduser.
The following command will be used for testing: hduser@ubuntu:~$ ssh localhost.
You will get an output as follows: The authenticity of host 'localhost (::1)' can't be established.
Are you sure you want to continue connecting (yes/no)? Just type yes and press enter.
You will get as follows: Warning: Permanently added 'localhost' (RSA) to the list of known hosts.
It is also the communication protocol which provides the location and an identification for routers and computers on networks.
For disabling IPv6, open sysctl.conf (which you will find in /etc) and put the following configuration at the end of the file:
To check whether IPv6 is enabled or not on your machine, the following command will be used:
The following commands will be used to change the owner of all the files to the hduser user and the hadoop group cluster.
So, we will assign permission to Apache Hadoop such that only hduser is able to access Apache Hadoop and not any other user:
The following command will take you to the directory where Apache Hadoop resides: $ cd /usr/local.
The following command will give hduser permission to access Apache Hadoop: $ sudo chown -R hduser:hadoop hadoop.
Open the bashrc file by going to the root directory, and type the following command:
You need to also create a location where your Apache Hadoop and Java will reside.
It will set the Apache Hadoop and Java to our classpath, which is required for this configuration.
Put the following configuration at the end of the file:
Required ownerships and permissions In this section, we are going to cover configuration of data files, the network ports of Apache Hadoop on which Apache Hadoop listens, and so on.
This setup will use Hadoop Distributed File System (HDFS), though there is only a single machine in our cluster.
Apache Hadoop will use the tmp directory for its operations.
Therefore, you should not get surprised if, at some later point, you see Hadoop making the required directory mechanically on HDFS.
The following command will allow the /app/hadoop/tmp directory to be accessed only by hduser:
This configuration is required for Apache Hadoop to perform its operations:
Since we are working with the Apache Hadoop filesystem, we've pointed this to the hadoop master or name node.
This will provide information about our master node to Apache Hadoop.
The dfs.replication property will tell Apache Hadoop about the number of replications to be used.
It means that there is no replication, that is, it is using only a single machine in a cluster.
It should usually be three or more—in fact you should have a minimum in that range of operating nodes in your Hadoop cluster.
The dfs.name.dir property will be used as the Apache Nutch name directory.
The dfs.data.dir data directory is used by Apache Nutch for its operations.
You must create these directories manually and give the proper path of those directories.
The dfs.name.dir property is the directory used by the name node for storing, following, and coordinating the data for the info nodes.
This should often be expected to be similar on each node.
When a client wants to govern a file in the filesystem, it will contact the name node.
The name node will indicate which data node to contact for accepting the file.
The name node is the organizer and will store what blocks area unit on what computers, and what must be replicated to completely different data nodes.
They are storing the particular files, serving them up on request, and so on.
If you're running a name node and a data node on the same PC, it will still act over sockets as if the information node was on a different PC.
This is often only on the tracker and not on the mapreduce hosts.
Formatting the HDFS filesystem using the NameNode HDFS is a directory used by Apache Hadoop for storage purposes.
So, it's the directory that stores all the data related to Apache Hadoop.
NameNode manages the filesystem's metadata and DataNode actually stores the data.
Whenever there are very large clusters, this is when the configuration needs to be tuned.
The first step for getting your Apache Hadoop started is formatting the Hadoop filesystem, which is implemented on top of the local filesystem of your cluster (which will include only your local machine if you have followed)
The HDFS directory will be the directory that you specified in hdfs-site.xml with the property dfs.data.
To format the filesystem, go to the respective directory where your Apache Hadoop resides by terminal.
Make sure that you are logged in as a hduser before hitting the following command:
If all succeeds, you will get an output as follows:
The preceding screenshot shows that your HDFS directory is formatted successfully.
Starting your single-node cluster Now, we are done with the setup of the single-node cluster of Apache Hadoop.
It's time to start Apache Hadoop and check whether it is running up to the mark or not.
So, run the following command for starting your single-node cluster.
Make sure you are logged in as hduser before hitting the following command:
If all succeeds, you will get the output as follows:
The preceding screenshot shows  all the components which have started; they're listed one by one.
Once started, you need to check whether all the components are running perfectly or not.
The preceding screenshot shows the number of components running in Apache Hadoop.
JobTracker: This is a component that will keep track of the number of jobs running in Apache Hadoop and divide each job into the number of tasks that are performed by TaskTracker.
TaskTracker: This is used for performing tasks given by JobTracker.
So, each task tracker has multiple tasks to be performed.
And once it is completed with a particular task, it will inform the JobTracker.
That's how JobTracker will get an update that tasks are being performed in the desired manner.
Namenode: This keeps track of the directories created inside HDFS.
The responsibility of Namenode is to transfer data to Datanodes.
Rather, it will transfer all the data to the DataNode.
So, whenever any Namenode fails, we can back up our data from SecondaryNamenode.
DataNode: This is the component which actually stores the data transferred from NameNode.
So, the responsibility of DataNode is to store all the data of Apache Hadoop.
It's a command that is a part of Sun Java since v1.5.0
Just check on your browser by hitting the following URL:
Stopping your single-node cluster If you want to stop your running cluster, hit the following command.
Make sure you are logged in as a hduser before hitting this command.:
If all succeeds, you will get an output as follows:
The preceding screenshot shows the number of components in Apache Hadoop that are being stopped.
So that's all for installation of Apache Hadoop on a single machine.
Now, we will move over to setting up the deployment architecture of Apache Nutch.
Setting up the deployment architecture of Apache Nutch We have to set up Apache Nutch on each of the machines that we are using.
So, we have to set up Apache Nutch on all of the machines.
If there are a small number of machines in our cluster configuration, we can set it up manually on each machine.
But when there are more machines, let's say we have 100 machines in our cluster environment, we can't set it up on each machine manually.
For that, we require a deployment tool such as Chef or at least distributed SSH.
You can refer to http://www.opscode.com/chef/ for getting familiar with Chef.
I will just demonstrate running Apache Hadoop on Ubuntu for a single-node cluster.
If you want to run Apache Hadoop on Ubuntu for a multi-node cluster, I have already provided the reference link.
Once we are done with the deployment of Apache Nutch to a single machine, we will run this start-all.sh script that will start the services on the master node and data nodes.
This means that the script will begin the Hadoop daemons on the master node.
So, we are able to login to all the slave nodes using the SSH command as explained, and this will begin daemons on the slave nodes.
The start-all.sh script expects that Apache Nutch should be put on the same location on each machine.
It is also expecting that Apache Hadoop is storing the data at the same file path on each machine.
The start-all.sh script that starts the daemons on the master and slave nodes are going to use password-less login by using SSH.
Installing Apache Nutch Download Apache Nutch from http://nutch.apache.org/downloads.html and extract the contents of the Apache Nutch package to your preferred location.
You need to assign permission to Apache Nutch so that only hduser can access it.
The commands that will be used are given as follows:
The following command will take you to the local directory: $ cd /usr/local.
The following command will assign permission to the nutch directory that can be accessed only by hduser: $ sudo chown -R hduser:hadoop nutch.
To open this file, go to the root directory from your terminal.
Put the following configuration at  the end of the file.
Key points of the Apache Nutch installation We need to rebuild Apache Nutch by using the ant command.
And, we also need to set the classpath in hadoop-env.sh, which you will find in $HADOOP_HOME/conf by putting the following configuration into it:
After this, go to the $NUTCH_HOME directory and type the following command for rebuilding Apache Nutch:
Now, we can start Apache Hadoop and perform the tasks of Apache Nutch on that.
It's time to start up Apache Hadoop on a single node and check whether it's working properly or not.
If you want to stop all components, the following command should be used:
It will perform crawling on the Apache Hadoop cluster and give us the result.
The steps for performing this job are given as follows:
This file will contain the list of URLs to be crawled on the Apache Hadoop Cluster.
The preceding command will create the seed.txt file and open it up.
You can enter n number of URLs, but one URL per line.
The following command will be used for adding this directory.
For checking whether it's correctly put or not, type the following command.
It will list all the directories that are inside the given directory.
So, any web URL that ends with apache.org will be crawled.
Make sure that you are logged in as an hduser before hitting the following commands:
This is the directory which we put in the HDFS.
So, only those URLs will be crawled from the urls directory.
You can give any integer value to it, but with the condition that topN should be greater than or equal to the total number of URLs in the urls directory.
So, there are many arguments which you can apply according to your needs.
The preceding screenshot is showing that Apache Hadoop crawled URLs for you.
You can also keep track of your crawling from the browser by opening the Jobtracker component of Apache Hadoop.
It will show you the statistics of the running jobs, and also the number of tasks per job.
If all succeeds, you will get an output as follows:
The preceding screenshot will show you the number of jobs and their statuses for the crawling that you have performed.
You can also check the detailed statistics of your tasks per job by opening the Tasktracker component of Apache Hadoop.
If all succeeds, you will get an output as follows:
The preceding screenshot is showing the task tracker that will keep track of the number of tasks for the particular job.
You could take the dump of the crawled URLs by copying the directory from HDFS, which contains data about all the crawled URLs.
Copy that directory and paste it to your preferred location for backup.
Now we will move over to Apache nutch configuration with eclipse in the next section.
So, just like we performed crawling in Apache Nutch using the command line, we can perform crawling using Java API.
So, we need to perform crawling from the command line.
We can use eclipse for all the operations of crawling that we are doing from the command line.
Instructions are provided for fixing a development environment for Apache Nutch with Eclipse IDE.
It's supposed to give a comprehensive starting resource for configuring, building, crawling, and debugging of Apache Nutch.
The prerequisites for Apache Nutch integration with Eclipse are given as follows:
All the required components are available from them Eclipse Marketplace.
Download the IvyDE plugin for Eclipse from the following link: http://ant.apache.org/ivy/ivyde/download.cgi.
Installation and building Apache Nutch with Eclipse Here, we will define the installation steps for configuring Apache Nutch with Eclipse.
The steps for configuring Apache Nutch with Eclipse are given as follows:
Get the latest source code of Apache Nutch by using SVN, which is a subversion repository.
For Apache Nutch 2.x, the following command will be used:
You need to decide which data store you are going to use for this integration.
Some of the choices of storage classes are given as follows:
Otherwise, you have to search for your datastore and uncomment it if it is commented accordingly.
Otherwise you have to find out that line for your datastore and in put it accordingly.
Now, we shall move to the Crawling in Eclipse section.
Crawling in Eclipse In this section, we will see how we can import Apache Nutch into Eclipse and perform crawling.
In the next window, set the root directory to the place where you have done the checkout of Apache Nutch 2.1.1, and then click on Finish.
You are currently seeing a new project named 2.1.1, which is being added within the workspace.
Wait for some time until Eclipse refreshes its SVN cache and builds its workspace.
You'll get the status at the end point of the corner of the Eclipse window, as shown in the next screenshot:
Sadly, Eclipse will take one more build of the workspace, but this time it won't take much time.
How to create an Eclipse launcher? Let's start with the inject operation.
For Apache version 1.x: Set the main class value as org.apache.
In the Arguments tab for program arguments, give the path of the input dir, which has the seed URLs.
If everything was done perfectly, then you will see the inject operation progressing on the console as shown in the following screenshot:
If you want to find out the Java class related to any command, just go inside the src/bin/nutch script; at the bottom, you will find a switch-case code with a case corresponding to each command.
The important classes corresponding to the crawl cycle are given as follows:
In the same way, you can perform all the jobs that are listed in the preceding table.
You can take the respective Java class of a particular job and run that within Eclipse.
So that's how crawling occurs in Apache Nutch using Eclipse.
So, now we have successfully integrated Apache Nutch with Eclipse.
Let's go to the Summary section now and revise what you have learned from this chapter.
In that, we have covered an introduction to Apache Hadoop, what do we mean by integrating Apache Nutch with Apache Hadoop, and what are the benefits of that.
Then, we moved on to the configuration steps, and we configured Apache Nutch with Apache Hadoop successfully.
We also performed a crawling job by installing Apache Nutch on a machine, and confirmed the output—Apache Hadoop cluster is running properly and is performing the crawling job correctly.
Then, we started with integration of Apache Nutch with Eclipse.
We also had a little introduction to what is integration of Apache Nutch with Eclipse.
Then, we looked at the configuration of Apache Nutch with Eclipse.
We have successfully integrated Apache Nutch with Eclipse and performed one InjectorJob example.
Now, let's see how we can integrate Apache Nutch with Gora, Accumulo, and MySQL in the next chapter.
First, we will start with the integration of Apache Nutch with Apache Accumulo using Gora and then we will move on to the integration of Nutch with MySQL using Gora.
By the end of this chapter, you will be able to integrate Nutch with Apache Accumulo as well as with MySQL.
After that, you can perform crawling using Apache Nutch on Apache Accumulo and also on MySQL, and you can get the results of your crawled pages on Accumulo as well as on MYSQL.
You can also perform the integration of Apache Solr as discussed earlier and get your crawled pages indexed onto the Apache Solr.
Introduction to Apache Accumulo Accumulo is basically used as the data store for storing data the same way we use different databases such as MySQL and Oracle.
The key point of Apache Accumulo is that it runs on the Apache Hadoop Cluster environment, which is a very good feature of Accumulo.
With Accumulo sorted, the distributed key/value store could be a strong, scalable, high-performance information storage and retrieval system.
Apache Accumulo features some novel improvement on the BigTable design in the form of cell-based access management and the server-side programming mechanism, which will perform modification in the key/value pairs at varied points within the data management process.
Main features of Apache Accumulo The following are the two main features of Accumulo:
Security at cell level: Apache Accumulo extends a BigTable data model, adding a new component called Column Visibility to the key.
This component stores a logical combination of security labels that need to be satisfied at query time in order for the key and value to be returned as a part of the user request.
This enables data of varying security levels to be stored within the same table and allows users to see only those keys and values for which they are authorized.
Programming at the server side: In addition to the security at the cell level, Apache Accumulo provides a programming mechanism at the server side called Iterators, which enables users to perform extra process at the table server.
The range of operations that may be applied is equivalent to those that can be implemented within a MapReduce Combiner function, which produces an aggregate value for several key/value pairs.
Introduction to Apache Gora Apache Gora's open source framework provides an in-memory data model and persistence for large data.
Apache Gora supports persisting to column stores, key-value stores, document stores, and RDBMS, and analyzing the data with extensive Apache Hadoop MapReduce support.
Supported data stores Apache Gora presently supports the following data stores:
Use of Apache Gora Although there are many excellent ORM frameworks for relational databases and data modeling, data stores in NoSQL are profoundly different from their relative cousins.
Data model agnostic frameworks such as JDO aren't easy for use cases, where one has to use the complete power of data models in column stores.
Gora fills the gap giving users an easy-to-use in-memory data model plus persistence for large data framework, providing data store-specific mappings and also in-built Apache Hadoop support.
Integration of Apache Nutch with Apache Accumulo In this section, we are going to cover the integration process for integrating Apache Nutch with Apache Accumulo.
Apache Accumulo is basically used for a huge data storage.
So, a potential use of integrating Apache Nutch with Apache Accumulo is when our application has huge data to process and we want to run our application in a cluster environment.
At such times, we can use Apache Accumulo for data storage purposes.
As Apache Accumulo only runs with Apache Hadoop, the maximum use of Apache Accumulo would be in a cluster-based environment.
We will first start with the configuration of Apache Gora with Apache Nutch.
After this, we will do the installation and configuration of Apache Accumulo.
Following this, we will test Apache Accumulo, and at the end we will see the process of crawling with Apache Nutch on Apache Accumulo.
So, you have to get Apache Nutch as explained before.
Modify the ivy.xml file that you will find in $NUTCH_HOME, which will be the path where your Apache Nutch resides.
By default, your Maven repository resides inside your home directory.
That's why I have given the path in the root property.
If you have changed the path, you will have to make changes in the directory accordingly.
This is the class that will process the data crawled by Apache Nutch and store it inside the table.
The preceding configuration will create a table called webpage, which is used for storing your data crawled by Apache Nutch.
This table will be automatically created at the time of crawling.
Whatever fields are defined by the <field> tag, those many number of fields will be created with the same name provided in the name attribute.
Hence, Apache Nutch is using this dependency while building the code.
If you have not configured it yet, it will give an error at the time of building Apache Nutch.
The preceding command will build Apache Nutch and set up the necessary configuration for this integration.
This is compulsory whenever you do any changes in the Apache Nutch configuration.
Now, let's move to our next section, which shows how to set up Apache Hadoop and ZooKeeper.
Setting up Apache Hadoop and Apache ZooKeeper As Apache Hadoop has already been covered earlier, I am not going to cover that again.
Apache ZooKeeper is a centralized service, which is used for maintaining configuration information and provides distributed synchronization, naming, and group services.
All of these services are used by distributed applications in one manner or another.
All these services are provided by the ZooKeeper, so you don't have to write these services from scratch.
You can use these services for implementing consensus, management, group, leader election, and presence protocols, and you can also build it for your own requirements.
You can refer to http://www.covert.io/post/18414889381/accumulo-nutchand-gora for any queries related to the setup.
So, I will not cover this part here; instead, I will directly start Apache Hadoop.
Apache ZooKeeper for the first time, the simple thing to do is run it in the standalone mode with the single ZooKeeper server.
You can try this on a development machine for example.
Apache ZooKeeper requires Java 6 to run, so you have to make sure that you have it installed.
There is no need for Cygwin to run Apache ZooKeeper on Windows since Windows versions of Apache ZooKeeper scripts are available.
Windows supports only as a development platform and not as a production platform.
The following command will be used for unpacking and giving permission to ZooKeeper: # cd /usr/local.
Now, set up a classpath entry inside your .bashrc file for Apache ZooKeeper by adding the following configuration.
The following commands will be used for creating this directory: #mkdir  -p /app/zookeeper.
It will give the output as a bunch of logging messages, which is fine.
Press Enter and then you must be inside a shell.
You must see a single line of output (followed again by a prompt) that looks as follows:
The following screenshot shows that your ZooKeeper is running successfully:
The preceding screenshot shows only the last lines as the output.
If you face any error, you need to check the whole output.
If no errors occur, you have configured Apache ZooKeeper properly.
When you run the zkCli.sh command for the first time and if you see stack traces as follows:
Installing and configuring Apache Accumulo The numbers of tools required for Apache Accumulo are as follows:
The following are the steps for the installation and configuration of Apache Accumulo:
You can download Apache Accumulo from http://accumulo.apache.org/downloads/ and unpack it in a suitable location.
The following commands will be used for unpacking it and giving access permission to it: $ cd /usr/local.
To open this file, go to your root directory from the terminal and type the following command: gedit ~/.bashrc.
Now set up a classpath entry in your .bashrc file for Apache Accumulo by including the following configuration into it:
This is the configuration on which tablet servers and loggers will run.
This is the configuration of machines where the master server will run.
The following will be the path of the Accumulo log directory.
The preceding paths would be different for different users based on their system settings.
Warning!!! Your instance secret is still set to the default, this is not secure.
You will also need to edit your secret in your configuration file by adding the property.
Here, I set my instance name to inst and my password to root.
You can do this in the same way, or make sure that you set the right configuration parameters later.
If all succeeds, it will give the output as shown in the following screenshot:
After completing this, you will be able to open http://127.0.0.1:50095/ in your web browser, and you will see a page as shown in the following screenshot:
The preceding screenshot shows that your Apache Accumulo is running on the browser successfully.
Now, before starting crawling, you have to start the Apache Accumulo server.
If all succeeds, you will get the output as shown in the following screenshot:
The preceding screenshot shows that the server in Apache Accumulo is running successfully.
So, we are ready to run Apache Nutch web crawler.
If the urls directory isn't created already, you have to create that directory.
You may see some log messages print to the console, but hopefully no stack traces.
If you are seeing stack traces, you must go back and check your configuration to make sure they match with the ones we made earlier.
After the crawler has completed its run, you are able to explore it using the Apache Accumulo shell.
If all succeeds, you will get an output as follows:
To check the table, you need to look inside the table with the following commands:
Now, let's move to our next section that shows how to integrate Apache Nutch with MySQL.
After the integration, you can crawl web pages in Apache Nutch that will be stored in MySQL.
So, you can go to MySQL and check your crawled web pages and also perform the necessary operations.
We will start with the introduction of MySQL and then we will cover the benefits of integrating MySQL with Apache Nutch.
After that, we will see the configuration of MySQL with Apache Nutch, and finally, we will perform crawling with Apache Nutch on MySQL.
Introduction to MySQL MySQL is a relational database which is used for data storage.
You can store any type of data (text, numeric, and alphanumeric)
MySQL is used to store your applications data and retrieve it whenever your applications need it.
Now let's see some of the reasons for integrating MySQL with Apache Nutch.
Benefits of integrating MySQL with Apache Nutch MySQL provides rich querying functionality which other NoSQL stores don't provide.
Whichever web pages are crawled by Apache Nutch need to be stored in MySQL.
You can use MySQL as a data store with Apache Nutch.
As many other data stores are also available, you can use MySQL in the same way.
It's a very good data storage option with Apache Nutch.
So, that's the benefits of integrating MySQL with Apache Nutch.
Now, let's find out about  configuration of MySQL with Apache Nutch.
Configuring MySQL with Apache Nutch In this section, we are going to cover the configuration steps that are required for configuring MySQL with Apache Nutch:
Install MySQL Server and MySQL client from the Ubuntu software center, or type the following command: #sudo apt-get install mysql-server mysql-client.
The innodb options help in dealing with the small, primary key size restriction of MySQL.
The character and the collation settings are for handling Unicode correctly.
The max_allowed_packet settings is optional and only necessary for very large sizes.
You need to restart your machine for changes to take effect.
Check whether MySQL is running by entering the following command: sudo netstat -tap | grep mysql.
We need to set up the Nutch database in MySQL manually because the current Apache Nutch/Apache Gora/MySQL generated database schema is set to Latin.
Log in to MySQL by entering the command using the MySQL ID and password which you have set for MySQL: mysql –u <username> -p.
Just type your password, and you will be logged in to MySQL.
Create the nutch database by entering the following command: CREATE DATABASE nutch;
Hit Enter and then to select the nutch database, enter the following command: use nutch;
Now, let's understand how to crawl with Apache Nutch on MySQL.
Crawling with Apache Nutch on MySQL In this section, we are going to integrate Apache Nutch with MySQL, and we will then perform the crawling operation and check whether our crawled web pages come to MySQL database or not.
This allows selection of non-English language as the default one to retrieve.
It is a useful setting for search engines built for certain national groups.
Install Ant if it's not installed already from the Ubuntu software center or by typing the following command: sudo apt-get install ant.
If all succeeds, a table will be created in MySQL with the name TestCrawl.
MySQL Workbench with the following command: sudo apt-get install mysql-workbench.
Now you can use these records and perform whatever operations you require.
We saw in detail how we can configure Apache Accumulo with the help of all these mentioned components.
We have covered the configuration part of Apache Accumulo and also performed crawling using Apache Nutch on Apache Accumulo.
After that, we started with the integration of Apache Nutch with MySQL, where we covered the introduction of MySQL.
Then, we covered how MySQL can be integrated with Apache Nutch.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cutting-edge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licenses, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Get up and running with the basic techniques of web scraping using PHP.
Learn something new in an Instant! A short, fast, focused guide delivering immediate results.
Build a re-usable scraping class to expand on for future projects.
Scrape, parse, and save data from any website with ease.
Over 100 recipes to discover new ways to work with Apache's Enterprise Search Server.
Apache Solr to make your search engine quicker and more effective.
Deal with performance, setup, and configuration problems in no time.
Discover little-known Solr functionalities and create your own modules to customize Solr to your company's needs.
Learn exciting new ways to buid efficient, high performance enterprise search repostories for Big Data using Hadoop and Solr.
Solr work on Big Data as well as the benefits and drawbacks.
Indexer extension program Scoring extension program Using your plugin with Apache Nutch Compiling your plugin.
